job_title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Data Scientist - 25 To 30 lacs. Remote,Markelytics Limited,1 - 3 years,25-30 Lacs P.A.,[],"We only hire from Top Tier Universities including IITs, BITS, DCE/NSIT, ISI, Top NITs etc.\n\nThis role is only suitable for candidates with between 1 to 4 years experience. People with more than 4 years of experience need not apply.\n\nThis is an URGENT requirement. We are hiring for a UK based Fintech company (name is kept confidential). The company is seeking an early stage Data Scientist to join the team and support the design, development, and Machine Learning and AI use-cases. Your work will directly support strategic initiatives and improve business outcomes.\n\n\nJob Summary:\n\nWe are seeking a motivated Data Scientist to join our Data and AI team. The ideal candidate will assist in analysing complex datasets, building predictive models, and contributing to data-driven decision-making. This entry-level role is perfect for someone eager to apply their technical skills and grow in a collaborative, innovative environment.\n\n\nKey Responsibilities:\n\nCollect, clean, and preprocess structured and unstructured data from various sources\nPerform exploratory data analysis (EDA) to identify trends, patterns, and insights\nDevelop, test, and deploy basic machine learning models under senior team guidance\nCreate visualisations and dashboards to communicate findings to stakeholders\nCollaborate with cross-functional teams (e.g., product, engineering) to support business objectives\nAssist in maintaining data pipelines and ensuring data quality\nStay updated on industry trends and emerging tools in data science\n\n\nQualifications:\n\nBachelors degree in data science, Computer Science, Statistics, Mathematics, or a related field\n1-3 years of experience in data science or a related role (internships or academic projects count)\nProficiency in Python for data analysis (e.g., pandas, NumPy, scikit-learn)\nStrong analytical and problem-solving skills\nExcellent communication skills to explain technical concepts to non-technical audiences.\nUnderstanding of machine learning algorithms (e.g., regression, classification, clustering).\n\n\nNice to Have:\n\nInternship or project experience in data science or analytics\nExposure to Gen AI architectures (RAG, MCP etc.)\nExperience working with cloud platforms (e.g., AWS, GCP, Azure)\nExperience with SQL for data querying\nKnowledge of version control (e.g., Git)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Python', 'Algebra', 'Data Wrangling', 'Probability', 'Data Analysis', 'Mathematics', 'Data Processing', 'machine learning', 'Statistics']",2025-06-11 05:54:40
Data Scientist - L3,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\n\n\n\nDo\n1. Demand generation through support in Solution development\na. Support Go-To-Market strategy\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n3. Team Management\na. Talent Management\ni. Support on boarding and training to enhance capability & effectiveness\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Demand generation# PoC supported\n2.Revenue generation through deliveryTimeliness, customer success stories, customer use cases\n3.Capability Building & Team Management# Skills acquired\n\n\n\n\n\n\nMandatory Skills: Data Analysis.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'ML deployment', 'Deep learning models', 'Solution development', 'Talent Management', 'Machine Learning']",2025-06-11 05:54:42
Data Scientist - L3,Wipro,3 - 5 years,Not Disclosed,['Ramdurg'],"Role Purpose\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\nDo\n1. Demand generation through support in Solution development\na. Support Go-To-Market strategy\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n3. Team Management\na. Talent Management\ni. Support on boarding and training to enhance capability & effectiveness\nDeliver\n\nNo.Performance ParameterMeasure\n1.Demand generation# PoC supported\n2.Revenue generation through deliveryTimeliness, customer success stories, customer use cases\n3.Capability Building & Team Management# Skills acquired\n\n\nMandatory Skills: Data Analysis.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'python', 'team management', 'natural language processing', 'scikit-learn', 'ml deployment', 'machine learning', 'data engineering', 'artificial intelligence', 'sql', 'deep learning', 'tensorflow', 'data science', 'predictive modeling', 'statistical modeling', 'ml']",2025-06-11 05:54:44
Data Scientist,Dwplacesolutions,3 - 5 years,Not Disclosed,['Bengaluru'],We are seeking an experienced Data Scientist to join our team.\nThe ideal candidate will have a strong background in developing and deploying\nconversational AI solutions using Large Language Models (LLMs) and RASA\nframework.,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Machine Learning', 'Tensorflow', 'R', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Chatbot', 'Deep Learning', 'Python']",2025-06-11 05:54:45
Hdfc Bank - Digital Banking - Data Scientist - Generative AI,Hdfc Bank,4 - 9 years,Not Disclosed,['Mumbai (All Areas)'],"Role - Digital Banking-Data Scientist-Digital Experience Analytics\n\nLocation - Mumbai\nGrade - Deputy Manager to Senior Manager\nMinimum 4 years experience\n\nJob Purpose:\nThe Data Scientist will design, develop and deploy advanced AI models to drive digital transformation, enhance customer experience, optimize operations and mitigate risks. This role requires a sound understanding of AI/ML techniques and their application to challenges such as customer engagement and process automation.",,,,"['Data Science', 'Artificial Intelligence', 'Machine Learning', 'Generative AI', 'Risk Analytics', 'Data Analytics']",2025-06-11 05:54:47
Data Scientist,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Senior Data Scientist is an advanced subject matter expert, tasked with taking accountability in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nResponsible for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources\nProvides meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nDirects scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualizes the output of the models.\nResponsible for ensuring that industry standards best practices are applied to development activities.\nKnowledge and Attributes:\nAdvanced understanding of data modelling, statistical methods and machine learning techniques.\nStrong ability to thrive in a dynamic, fast-paced environment.\nStrong quantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nAdvanced understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nStrong ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nStrong ability to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\nAcademic Qualifications and Certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming certification preferred.\nAgile certification preferred.\nRequired Experience:\nAdvanced demonstrated experience in a data science position in a corporate environment and/or related industry.\nAdvanced demonstrated experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nAdvanced demonstrated experience in programming languages (R, Python, etc.).\nAdvanced demonstrated experience working with and creating data architectures.\nAdvanced demonstrated experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nAdvanced demonstrated experience visualizing and/or presenting data for stakeholder use and reuse across the business.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'R', 'data modelling', 'data mining', 'statistical modelling', 'machine learning', 'Python', 'SQL']",2025-06-11 05:54:48
GenAI Data Scientist,EXL,2 - 5 years,Not Disclosed,"['Noida', 'Gurugram', 'Bengaluru']","Job Summary\nWe are seeking a highly skilled Generative AI Data Scientist to design, develop, and deploy generative AI applications and models that can generate meaningful insights, creative business domain content, or solve domain-specific challenges. The ideal candidate will combine expertise in data science, machine learning, AI and natural language processing (NLP) with a passion for innovation and real-world applications.\nAs part of the EXL Digital AI R&D Innovation team, you will work as a Data Scientist in the Innovation unit supporting all EXL Business Units. You will be exposed to hundreds of different enterprise GenAI business use cases and help our clients create value architecting GenAI applications and solutions.",,,,"['GenAI', 'NLP', 'LLM', 'Natural Language Processing']",2025-06-11 05:54:50
Data Scientist,THERMAX,2 - 3 years,Not Disclosed,['Pune'],Job Details:\nWe are seeking a highly motivated and enthusiastic Junior Data Scientist with 2-3 years of experience to join our data science team. This role offers an exciting opportunity to contribute to both traditional Machine Learning projects for our commercial IoT platform and cutting-edge Generative AI initiatives.\n\nExperience,,,,"['Tensorflow', 'Manufacturing', 'Machine Learning', 'IOT', 'Python', 'Pytorch', 'Data Science', 'Aiml', 'Scikit-Learn', 'Ml']",2025-06-11 05:54:51
Data Scientist (Immediate To 15 days NP),Sais It Services,5 - 8 years,10-18 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities :\n\nRequirements:\nStrong expertise in NLP, text summarization, semantic search, and LLM APIs.\nPractical experience with Amazon Bedrock, OpenAI, or Hugging Face transformers.\nFamiliar with prompt tuning and few-shot learning.\nPython (pandas, langchain, boto3, NumPy, etc.)",,,,"['Open AI', 'NLP', 'Design and Development', 'LLM APIs', 'Python', 'call transcripts', 'Amazon Bedrock']",2025-06-11 05:54:53
Data Scientist @ bangalore,MKS Vision,7 - 12 years,Not Disclosed,"['Hyderabad', 'Coimbatore']","MKS Vision Pvt Ltd\n\nAbout us:\nMKS Vision is a full spectrum of Information Technology and engineering service provider. We exist to provide increased efficiencies and flexibility that accelerate business performance by adapting the latest cutting-edge technologies for our customers. Our services bring tangible benefits to our customers. MKS Vision will assist you in adopting global services.\nWebsite: https://www.mksvision.com/\nJob Location: Coimbatore/Hyderabad\nRisk Data Scientist\nKnowledge of lending industry analytical processes related to (credit underwriting, collections, etc.)\nExperienced in the data science lifecycle (model specification, development, deployment, and validation)\nProficient in the use of modeling and machine learning techniques (logistic regression, gradient boosting, etc.), in SAS, Python or R\nStrong working exp in Power BI.\nProficient in SQL for data extraction, manipulation and cleanup, and the development of modeling datasets for development\nExperience in conducting data studies and retro studies using internal and external data for model validation\nExperience in developing project presentations across the project lifecycle (project specification, development, conclusions, and recommendations)\nExperience in development and maintenance of model documentation\nKnowledge of lending data systems and data structures (credit applications, loan origination, collections, payments, dialers, credit bureau data)\nAbility to generate analytical insights form model data, including identification of candidate variables, and development of new features.\nPreferred minimum 7+ years of experience, BS degree on computer science, management information systems, statistics, data science, etc., or similar experience.",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'SAS', 'Credit Risk Modelling', 'Python', 'SQL', 'R', 'Power Bi', 'Risk Analytics', 'ETL', 'Data Standards', 'Risk Modeling', 'Credit Risk Analysis']",2025-06-11 05:54:54
Data Scientist - ML,Client Of EDGE,8 - 13 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Delhi / NCR']","Our client is a India's marquee global technology company. They are an international flag-bearer of technical and managerial excellence. With offices around the globe, the company has a comprehensive presence across multiple segments of the IT product and service industries.\nWe are Seeking to identify a Data Scientist Engineer role, responsible for Leading the design, development, and deployment of advanced machine learning models and algorithms. Software engineering experience with Strong knowledge of machine learning frameworks (e.g., TensorFlow, PyTorch) and generative AI libraries., Understanding of C++ programming principles.\nLead the design, development, and deployment of advanced machine learning models and algorithms.\nTrain and fine-tune generative models on large datasets, optimizing model performance and efficiency.\nExperience of developing and deploying solutions on Nvidia or Intel software stacks will be an added advantage.\nExcellent problem-solving skills and the ability to work on complex, unstructured challenges.\nProficient Understanding of distributed Computing Principles. Working knowledge of frameworks such as accelerate, deepspeed, etc.\nExperience with hiring, mentoring and leading teams of ML engineers, data engineers, etc.\nConduct in-depth data analysis, feature engineering, and data pre-processing to extract meaningful insights.\nDevelop and execute data strategies to collect, process, and store data effectively.\nWork closely with data engineers and architects to ensure data availability and quality.\nCollaborate with cross-functional teams to develop AI-powered solutions that address business challenges and opportunities.\nEnsure the successful integration of AI models into production systems.\nStay up-to-date with the latest trends and advancements in data science and AI.\nDrive research initiatives to explore and implement innovative techniques and technologies.\nLead research initiatives to develop novel AI techniques and technologies.\nCommunicate findings, insights, and project progress to non-technical stakeholders in a clear and understandable manner.\nPublications or contributions to the field of CV, NLP or generative AI are a plus.\nYour Profile:\nAn Engineer with 5+ years of experience in Leading the design, development, and deployment of advanced machine learning models and algorithms.\nUnderstanding of C++ programming.\nStrong knowledge of machine learning frameworks (e.g., TensorFlow, PyTorch) and generative AI libraries.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['C++', 'Machine Learning', 'Data Scientist', 'AI', 'Development', 'Deployment']",2025-06-11 05:54:56
Specialist Data Scientist,NICE,8 - 11 years,Not Disclosed,['Pune'],"So, what’s the role all about?\nNICE provides state-of-the-art enterprise level AI and analytics for all forms of business communications between speech and digital.   We are a world class research team developing new algorithms and approaches to help companies with solving critical issues such as identifying their best performing agents, preventing fraud, categorizing customer issues, and determining overall customer satisfaction.  If you have interacted with a major contact center in the last decade, it is very likely we have processed your call. \nThe research group partners with all areas of NICE’s business to scale out the delivery of new technology and AI models to customers around the world that are tailored to their company, industry, and language needs.",,,,"['python', 'confluence', 'natural language processing', 'presentation skills', 'big data technologies', 'pyspark', 'microsoft azure', 'bert', 'machine learning', 'sql', 'tensorflow', 'data science', 'gcp', 'pytorch', 'machine learning algorithms', 'aws', 'big data', 'communication skills', 'statistics', 'jira']",2025-06-11 05:54:58
Data Scientist(0217),A Reputed Organization,5 - 10 years,Not Disclosed,"['Kolkata', 'Pune', 'Bengaluru']","Lead the design, development, and deployment of advanced machine learning models and algorithms.\nTrain and fine-tune generative models on large datasets, optimizing model performance and efficiency.\nStrong knowledge of machine learning frameworks (e.g., TensorFlow, PyTorch) and generative AI libraries., Understanding of C++ programming principles.\nExperience of developing and deploying solutions on Nvidia or Intel software stacks will be an added advantage.\nExcellent problem-solving skills and the ability to work on complex, unstructured challenges.\nProficient Understanding of distributed Computing Principles. Working knowledge of frameworks such as accelerate, deepspeed, etc.\nExperience with hiring, mentoring and leading teams of ML engineers, data engineers, etc.\nConduct in-depth data analysis, feature engineering, and data preprocessing to extract meaningful insights.\nDevelop and execute data strategies to collect, process, and store data effectively.\nWork closely with data engineers and architects to ensure data availability and quality.\nCollaborate with cross-functional teams to develop AI-powered solutions that address business challenges and opportunities.\nEnsure the successful integration of AI models into production systems.\nStay up-to-date with the latest trends and advancements in data science and AI.\nDrive research initiatives to explore and implement innovative techniques and technologies.\nLead research initiatives to develop novel AI techniques and technologies.\nCommunicate findings, insights, and project progress to non-technical stakeholders in a clear and understandable manner.\nPublications or contributions to the field of CV, NLP or generative AI are a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'C++', 'Data Scientist', 'AI']",2025-06-11 05:54:59
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-11 05:55:01
Lead Data Scientist,R Systems International,8 - 10 years,Not Disclosed,"['Noida', 'Pune', 'Chennai']","Job Title: Lead Data Scientist\n\nWe are seeking a highly skilled and experienced Lead Data Scientist to join our dynamic team. In this role, you will be responsible for leading data-driven projects, mentoring junior data scientists, and guiding the organization in making strategic decisions based on data insights.\n\nKey Responsibilities:\n- Develop and implement advanced statistical models and algorithms to analyze complex data sets.\n- Collaborate with cross-functional teams to identify business opportunities and translate them into data-driven solutions.\n- Mentor and oversee a team of data scientists, providing guidance on best practices and techniques in data analysis and modeling.\n- Communicate findings and insights to stakeholders through presentations, reports, and visualizations.\n- Stay current with industry trends and emerging technologies in data science and analytics.\n- Design and implement experiments to validate models and hypotheses.\n- Ensure the quality and integrity of data throughout the analytic process.\n\nQualifications:\n- Master's. in Computer Science, Statistics, Mathematics, or a related field.\n- Proven experience as a Data Scientist, with a strong portfolio of successful projects.\n- Expertise in programming languages such as Python or R, as well as experience with machine learning frameworks.\n- Strong knowledge of statistical analysis and modeling techniques.\n- Excellent problem-solving skills and the ability to work with large and complex data sets.\n- Strong communication skills, with the ability to convey technical concepts to non-technical stakeholders.\n- Experience in leading and managing teams is a plus.\n\nWe offer a competitive salary, comprehensive benefits, and the opportunity to work in a collaborative and innovative environment. If you are passionate about data science and eager to make a significant impact, we would love to hear from you.",,,,"['algorithms', 'python', 'modeling', 'data analysis', 'mathematics', 'data processing', 'data pipeline', 'machine learning', 'data collection', 'analytics', 'r', 'data science', 'computer science', 'predictive modeling', 'science', 'machine learning algorithms', 'programming', 'reporting', 'statistics', 'communication skills']",2025-06-11 05:55:03
"Data Scientist with EDA, Python, SQL and Business Analysis",Cognizant,8 - 11 years,Not Disclosed,['Hyderabad'],"Job Summary\nWe are seeking a highly skilled Data Scientist with Business analysis with good hands on in Python , SQL , EDA Data Visualization with 3 to 12+ years of experience\nKey Responsibilities:\nPerform exploratory data analysis (EDA) to uncover trends, patterns, and insights.\nDevelop and maintain dashboards and reports to visualize key business metrics.\nCollaborate with cross-functional teams to gather requirements and deliver data-driven solutions.",,,,"['python', 'eda', 'data analysis', 'data management', 'modeling', 'analytical', 'data manipulation', 'query', 'predictive analytics', 'microsoft azure', 'business analysis', 'cloud platforms', 'business analytics', 'machine learning', 'dashboards', 'sql', 'analytics', 'gcp', 'statistical modeling', 'data visualization', 'aws']",2025-06-11 05:55:04
Hiring - Data Scientist Lead,AGS Health,6 - 10 years,13-23 Lacs P.A.,['Ahmedabad'],Hi\n\nImmediate opening for Data Scientist Lead\n\nLocation : Ahmedabad ( Onsite only )\n\nExperience : 6 + years,,,,"['Tensorflow', 'Deep Learning', 'Pytorch', 'Data Science', 'NLP', 'Natural Language Processing', 'LLM', 'Machine Learning', 'Scikit-Learn', 'Nltk', 'Keras', 'Spacy', 'Python']",2025-06-11 05:55:06
Senior Data Scientist,Fastenal,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Description:\nWe are seeking a highly skilled and experienced Senior Data Scientist to join our team. The ideal candidate will have a strong background in data science, machine learning, and statistical analysis. As a Senior Data Scientist, you will be responsible for leading data-driven projects, developing predictive models, and providing actionable insights to drive business decisions.\n\nKey Responsibilities:",,,,"['Data Science', 'SQL', 'Pyspark', 'R', 'Python']",2025-06-11 05:55:08
Senior Associate Data Scientist,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Data Scientist is a seasoned subject matter expert, tasked with participating in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nAccountable for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources.\nAccountable for providing meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nAccountable for performing analysis using programming languages or statistical packages such as Python, pandas etc.\nDesigns scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualize the output of the models.\nCreates documentation around processes and procedures and manages code reviews.\nAccountable for ensuring that industry standards best practices are applied to development activities.\nKnowledge and Attributes:\nSeasoned in data modelling, statistical methods and machine learning techniques.\nAbility to thrive in a dynamic, fast-paced environment.\nQuantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nGood understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nAbility to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nAbility to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\nAble to apply data science principles through a business lens.\nDesire to create strategies and solutions that challenge and expand the thinking of peers and business stakeholders.\nAcademic Qualifications and Certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming (Python) certification preferred.\nAgile certification preferred.\nRequired Experience:\nSeasoned experience in a data science position in a corporate environment and/or related industry.\nSeasoned experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nSeasoned experience in programming languages (Python, etc.).\nSeasoned experience working in databases (MySQL, Microsoft SQL Server, Azure Synapse, MongoDB)\nSeasoned experience working with and creating data architectures.\nSeasoned experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nSeasoned experience visualizing and/or presenting data for stakeholder use and reuse across the business.\nSeasoned experience on working with API (creating and using APIs)\nAutomation experience using Python scripting, UIPath, Selenium, PowerAutomate.\nSeasoned experience working on Linux operating system (Ubuntu)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure Synapse', 'Microsoft SQL Server', 'MySQL', 'Python scripting', 'PowerAutomate', 'Linux operating system', 'MongoDB', 'Selenium', 'Python', 'UIPath']",2025-06-11 05:55:09
Senior Data Scientist,"NTT DATA, Inc.",2 - 6 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Senior Data Scientist is an advanced subject matter expert, tasked with taking accountability in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nKey responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nResponsible for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources\nProvides meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nDirects scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualizes the output of the models.\nResponsible for ensuring that industry standards best practices are applied to development activities.\n\nTo thrive in this role, you need to have:\nAdvanced understanding of data modelling, statistical methods and machine learning techniques.\nStrong ability to thrive in a dynamic, fast-paced environment.\nStrong quantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nAdvanced understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nStrong ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nStrong ability to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\n\nAcademic qualifications and certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming certification preferred.\nAgile certification preferred.\n\nRequired experience:\nAdvanced demonstrated experience in a data science position in a corporate environment and/or related industry.\nAdvanced demonstrated experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nAdvanced demonstrated experience in programming languages (R, Python, etc.).\nAdvanced demonstrated experience working with and creating data architectures.\nAdvanced demonstrated experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nAdvanced demonstrated experience visualizing and/or presenting data for stakeholder use and reuse across the business.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'data analytics', 'natural language processing', 'data modeling', 'data mining', 'statistical modeling', 'data architecture', 'machine learning']",2025-06-11 05:55:11
Data Engineer - Python/SQL,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Continuous Integration, Deployment & Monitoring of Software100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan\n2.Quality & CSATOn-Time Delivery, Manage software, Troubleshoot queries,Customer experience, completion of assigned certifications for skill upgradation\n3.MIS & Reporting100% on time MIS & report generation\nMandatory Skills: Python for Insights.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'module development', 'Data Engineering', 'software development', 'software programs', 'SQL']",2025-06-11 05:55:12
Data Analyst,PwC India,4 - 8 years,Not Disclosed,"['Bengaluru', 'Mumbai (All Areas)']","If Interested, please apply in the given link : https://forms.office.com/r/h5Qzqnb1Kr\n\nJob Title: Data Analyst (4 - 8 Years Experience)\nLocation: Bengaluru/ Mumbai\nType: Full-Time\nAbout the Role:\nWe are on the lookout for a sharp, self-driven Data Analyst with a strong command of SQL, Python, and relational databases. If solving complex data problems, building efficient data pipelines, and collaborating across teams excites you - youll thrive in this role.",,,,"['Python', 'SQL', 'Data analyst']",2025-06-11 05:55:14
Data Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\nDeliver\n\nNo Performance Parameter Measure\n1. Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Tableau.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Tableau', 'data warehouses', 'data integration', 'Data modelling']",2025-06-11 05:55:15
"Lead Data Scientist, Operations || Mumbai || Max 38 LPA",Argus India Price Reporting Services,5 - 10 years,20-35 Lacs P.A.,"['Mumbai Suburban', 'Navi Mumbai', 'Mumbai (All Areas)']","Lead Data Scientist, Operations\nMumbai, India\n\nAbout Argus:\n\nArgus is the leading independent provider of market intelligence to the global energy and commodity markets. We offer essential price assessments, news, analytics, consulting services, data science tools and industry conferences to illuminate complex and opaque commodity markets.\nHeadquartered in London with 1,500 staff, Argus is an independent media organisation with 30 offices in the worlds principal commodity trading hubs.\nCompanies, trading firms and governments in 160 countries around the world trust Argus data to make decisions, analyse situations, manage risk, facilitate trading and for long-term planning. Argus prices are used as trusted benchmarks around the world for pricing transportation, commodities and energy.\nFounded in 1970, Argus remains a privately held UK-registered company owned by employee shareholders and global growth equity firm General Atlantic.\nWhat were looking for:\nJoin our Generative AI team to lead a new group in India, focused on creating and maintaining AI-ready data. As the point of contact in Mumbai, you will guide the local team and ensure seamless collaboration with our global counterparts. Your contributions will directly impact the development of innovative solutions used by industry leaders worldwide, supporting text and numerical data extraction, curation, and metadata enhancements to accelerate development and ensure rapid response times. You will play a pivotal role in transforming how our data are seamlessly integrated with AI systems, paving the way for the next generation of customer interactions.\n\nWhat will you be doing:\n\nLead and Develop the Team: Oversee a team of data scientists in Mumbai. Mentoring and guiding junior team members, fostering their professional growth and development.\nStrategic Planning: Develop and implement strategic plans for data science projects, ensuring alignment with the company's goals and objectives.\nAI-Ready Data Development: Design, develop, and maintain high-quality AI-ready datasets, ensuring data integrity, usability, and scalability to support advanced Generative AI models.\nAdvanced Data Processing: Drive hands-on efforts in complex data extraction, cleansing, and curation for diverse text and numerical datasets. Implement sophisticated metadata enrichment strategies to enhance data utility and accessibility for AI systems.\nAlgorithm Implementation & Optimization: Implement and optimize state-of-the-art algorithms and pipelines for efficient data processing, feature engineering, and data transformation tailored for LLM and GenAI applications.\nGenAI Application Development: Apply and integrate frameworks like LangChain and Hugging Face Transformers to build modular, scalable, and robust Generative AI data pipelines and applications.\nPrompt Engineering Application: Apply advanced prompt engineering techniques to optimize LLM performance for specific data extraction, summarization, and generation tasks, working closely with the Lead's guidance.\nLLM Evaluation Support: Contribute to the systematic evaluation of Large Language Models (LLMs) outputs, analysing quality, relevance, and accuracy, and supporting the implementation of LLM-as-a-judge frameworks.\nRetrieval-Augmented Generation (RAG) Contribution: Actively contribute to the implementation and optimization of RAG systems, including working with embedding models, vector databases, and, where applicable, knowledge graphs, to enhance data retrieval for GenAI.\nTechnical Leadership: Act as a technical leader and subject matter expert for junior data scientists, providing guidance on best practices in coding and PR reviews, data handling, and GenAI methodologies.\nCross-Functional Collaboration: Collaborate effectively with global data science teams, engineering, and product stakeholders to integrate data solutions and ensure alignment with broader company objectives.\nOperational Excellence: Troubleshoot and resolve data-related issues promptly to minimize potential disruptions, ensuring high operational efficiency and responsiveness.\nDocumentation & Code Quality: Produce clean, well-documented, production-grade code, adhering to best practices for version control and software engineering.\n\nSkills and Experience:\n\nLeadership Experience: Proven track record in leading and mentoring data science teams, with a focus on strategic planning and operational excellence.\nAcademic Background: Advanced degree in AI, statistics, mathematics, computer science, or a related field.\nProgramming and Frameworks: 5+ years of hands-on experience with Python, TensorFlow or PyTorch, and NLP libraries such as spaCy and Hugging Face.\nGenAI Tools: 2+ years of Practical experience with LangChain, Hugging Face Transformers, and embedding models for building GenAI applications.\nPrompt Engineering: Deep expertise in prompt engineering, including prompt tuning, chaining, and optimization techniques.\nLLM Evaluation: Experience evaluating LLM outputs, including using LLM-as-a-judge methodologies to assess quality and alignment.\nRAG and Knowledge Graphs: Practical understanding and experience using vector databases. In addition, familiarity with graph-based RAG architectures and the use of knowledge graphs to enhance retrieval and reasoning would be a strong plus.\nCloud: 2+ years of experience with Gemini/OpenAI models and cloud platforms such as AWS, Google Cloud, or Azure. Proficient with Docker for containerization.\nData Engineering: Strong understanding of data extraction, curation, metadata enrichment, and AI-ready dataset creation.\nCollaboration and Communication: Excellent communication skills and a collaborative mindset, with experience working across global teams.\n\nWhats in it for you:\n\nCompetitive salary\nHybrid Working Policy (3 days in Mumbai office/ 2 days WFH once fully inducted)\nGroup healthcare scheme\n18 days annual leave\n8 days of casual leave\nExtensive internal and external training\n\nHours:\n\nThis is a full-time position operating under a hybrid model, with three days in the office and up to two days working remotely.\nThe team supports Argus key business processes every day, as such you will be required to work on a shift-based rota with other members of the team supporting the business until 8pm. Typically support hours run from 11am to 8pm with each member of the team participating up to 2/3 times a week.\n\nFor more details about the company and to apply please make sure you send your CV and cover letter via our website: www.argusmedia.com/en/careers/open-positions\nBy submitting your job application, you automatically acknowledge and consent to the collection, use and/or disclosure of your personal data to the Company. Argus is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, sexual orientation, gender identity, disability or veteran status.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Huggingface', 'Langchain', 'Spacy', 'Python', 'TensorFlow', 'Pytorch']",2025-06-11 05:55:17
"Senior Data Scientist, Operations || Mumbai || 29 LPA",Argus India Price Reporting Services,5 - 10 years,20-25 Lacs P.A.,"['Mumbai Suburban', 'Navi Mumbai', 'Mumbai (All Areas)']","Senior Data Scientist, Operations\nMumbai, India\nAbout Argus:\n\nArgus is the leading independent provider of market intelligence to the global energy and commodity markets. We offer essential price assessments, news, analytics, consulting services, data science tools and industry conferences to illuminate complex and opaque commodity markets.\nHeadquartered in London with 1,500 staff, Argus is an independent media organisation with 30 offices in the worlds principal commodity trading hubs.\nCompanies, trading firms and governments in 160 countries around the world trust Argus data to make decisions, analyse situations, manage risk, facilitate trading and for long-term planning. Argus prices are used as trusted benchmarks around the world for pricing transportation, commodities and energy.\nFounded in 1970, Argus remains a privately held UK-registered company owned by employee shareholders and global growth equity firm General Atlantic.\n\nWhat were looking for:\nJoin our Generative AI team as a Senior Data Scientist, reporting directly to the Lead Data Scientist in India. You will play a crucial role in building, optimizing, and maintaining AI-ready data infrastructure for advanced Generative AI applications. Your focus will be on hands-on implementation of cutting-edge data extraction, curation, and metadata enhancement techniques for both text and numerical data. You will be a key contributor to the development of innovative solutions, ensuring rapid iteration and deployment, and supporting the Lead in achieving the team's strategic goals.\n\nWhat will you be doing:\nAI-Ready Data Development: Design, develop, and maintain high-quality AI-ready datasets, ensuring data integrity, usability, and scalability to support advanced Generative AI models.\nAdvanced Data Processing: Drive hands-on efforts in complex data extraction, cleansing, and curation for diverse text and numerical datasets. Implement sophisticated metadata enrichment strategies to enhance data utility and accessibility for AI systems.\nAlgorithm Implementation & Optimization: Implement and optimize state-of-the-art algorithms and pipelines for efficient data processing, feature engineering, and data transformation tailored for LLM and GenAI applications.\nGenAI Application Development: Apply and integrate frameworks like LangChain and Hugging Face Transformers to build modular, scalable, and robust Generative AI data pipelines and applications.\nPrompt Engineering Application: Apply advanced prompt engineering techniques to optimize LLM performance for specific data extraction, summarization, and generation tasks, working closely with the Lead's guidance.\nLLM Evaluation Support: Contribute to the systematic evaluation of Large Language Models (LLMs) outputs, analysing quality, relevance, and accuracy, and supporting the implementation of LLM-as-a-judge frameworks.\nRetrieval-Augmented Generation (RAG) Contribution: Actively contribute to the implementation and optimization of RAG systems, including working with embedding models, vector databases, and, where applicable, knowledge graphs, to enhance data retrieval for GenAI.\nTechnical Mentorship: Act as a technical mentor and subject matter expert for junior data scientists, providing guidance on best practices in coding and PR reviews, data handling, and GenAI methodologies.\nCross-Functional Collaboration: Collaborate effectively with global data science teams, engineering, and product stakeholders to integrate data solutions and ensure alignment with broader company objectives.\nOperational Excellence: Troubleshoot and resolve data-related issues promptly to minimize potential disruptions, ensuring high operational efficiency and responsiveness.\nDocumentation & Code Quality: Produce clean, well-documented, production-grade code, adhering to best practices for version control and software engineering.\n\nSkills and Experience:\nAcademic Background: Advanced degree in AI, statistics, mathematics, computer science, or a related field.\nProgramming and Frameworks: 2+ years of hands-on experience with Python, TensorFlow or PyTorch, and NLP libraries such as spaCy and Hugging Face.\nGenAI Tools: 1+ years Practical experience with LangChain, Hugging Face Transformers, and embedding models for building GenAI applications.\nPrompt Engineering: Deep expertise in prompt engineering, including prompt tuning, chaining, and optimization techniques.\nLLM Evaluation: Experience evaluating LLM outputs, including using LLM-as-a-judge methodologies to assess quality and alignment.\nRAG and Knowledge Graphs: Practical understanding and experience using vector databases. In addition, familiarity with graph-based RAG architectures and the use of knowledge graphs to enhance retrieval and reasoning would be a strong plus.\nCloud: 2+ years of experience with Gemini/OpenAI models and cloud platforms such as AWS, Google Cloud, or Azure. Proficient with Docker for containerization.\nData Engineering: Strong understanding of data extraction, curation, metadata enrichment, and AI-ready dataset creation.\nCollaboration and Communication: Excellent communication skills and a collaborative mindset, with experience working across global teams.\n\nWhats in it for you:\nCompetitive salary\nHybrid Working Policy (3 days in Mumbai office/ 2 days WFH once fully inducted)\nGroup healthcare scheme\n18 days annual leave\n8 days of casual leave\nExtensive internal and external training\n\nHours:\nThis is a full-time position operating under a hybrid model, with three days in the office and up to two days working remotely.\nThe team supports Argus key business processes every day, as such you will be required to work on a shift-based rota with other members of the team supporting the business until 8pm. Typically support hours run from 11am to 8pm with each member of the team participating up to 2/3 times a week.\n\nFor more details about the company and to apply please make sure you send your CV and cover letter via our website: www.argusmedia.com/en/careers/open-positions\nBy submitting your job application, you automatically acknowledge and consent to the collection, use and/or disclosure of your personal data to the Company. Argus is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, sexual orientation, gender identity, disability or veteran status.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pytorch', 'Artificial Intelligence', 'LangChain', 'hugging face', 'Spacy', 'Tensorflow']",2025-06-11 05:55:18
Data Analyst,"NTT DATA, Inc.",1 - 4 years,Not Disclosed,['Bengaluru'],"Req ID: 327898\n\nWe are currently seeking a Data Analyst to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesKey Responsibilities:\nConduct in-depth revenue analysis to identify trends and opportunities for growth.\nPerform P&L attribution, analyzing variances and providing detailed insights to stakeholders.\nExecute Independent Price Verification (IPV) processes to ensure the accuracy and consistency of financial data.\nLead data reconciliation efforts by identifying and resolving discrepancies in financial datasets.\nCollaborate with finance, operations, and IT teams to develop efficient processes and reporting mechanisms.\nDocument business requirements, workflows, and processes, translating them into technical specifications where necessary.\nDevelop and maintain financial models to support decision-making processes.\nMonitor financial performance and prepare detailed reports for senior management.\n\nMinimum Skills RequiredQualifications:\nBachelor's degree in Finance, Economics, Business, or a related field (Master""™s preferred).\nProven experience as a Business Analyst in the finance domain, with expertise in revenue, P&L attribution, IPV, and data reconciliation.\nStrong knowledge of financial principles, data analysis, and reporting tools.\nProficiency in data analytics platforms such as Excel, SQL, or Tableau.\nExcellent problem-solving skills and attention to detail.\nStrong communication and interpersonal skills for effective stakeholder management.\nAbility to work in a fast-paced, deadline-driven environment.\nPreferred\n\nSkills:\n\nCertification in Business Analysis (e.g., CBAP, CCBA) or related field is a plus.\nFamiliarity with financial systems and accounting software.\nExperience in Agile or Scrum methodologies.""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'data analysis', 'data reconciliation', 'stakeholder management', 'reporting tools', 'cbap', 'business analysis', 'variance analysis', 'ipv', 'sql', 'business requirement analysis', 'revenue', 'tableau', 'workflow analysis', 'scrum', 'technical specifications', 'agile']",2025-06-11 05:55:20
Data Analyst,"NTT DATA, Inc.",1 - 4 years,Not Disclosed,['Bengaluru'],"Req ID: 327858\n\nWe are currently seeking a Data Analyst to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob DutiesKey ResponsibilitiesConduct in-depth revenue analysis to identify trends and opportunities for growth. Perform P&L attribution, analyzing variances and providing detailed insights to stakeholders. Execute Independent Price Verification (IPV) processes to ensure the accuracy and consistency of financial data. Lead data reconciliation efforts by identifying and resolving discrepancies in financial datasets. Collaborate with finance, operations, and IT teams to develop efficient processes and reporting mechanisms. Document business requirements, workflows, and processes, translating them into technical specifications where necessary. Develop and maintain financial models to support decision-making processes. Monitor financial performance and prepare detailed reports for senior management. Minimum Skills RequiredQualificationsBachelor's degree in Finance, Economics, Business, or a related field (Master""™s preferred). Proven experience as a Business Analyst in the finance domain, with expertise in revenue, P&L attribution, IPV, and data reconciliation. Strong knowledge of financial principles, data analysis, and reporting tools. Proficiency in data analytics platforms such as Excel, SQL, or Tableau. Excellent problem-solving skills and attention to detail. Strong communication and interpersonal skills for effective stakeholder management. Ability to work in a fast-paced, deadline-driven environment. Preferred\n\nSkills:\nCertification in Business Analysis (e.g., CBAP, CCBA) or related field is a plus. Familiarity with financial systems and accounting software. Experience in Agile or Scrum methodologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'data analysis', 'data reconciliation', 'stakeholder management', 'reporting tools', 'cbap', 'revenue analysis', 'business analysis', 'ipv', 'sql', 'business requirement analysis', 'revenue', 'tableau', 'workflow analysis', 'scrum', 'technical specifications', 'agile']",2025-06-11 05:55:22
Data Scientist - Computational Scientist / Applied Mathematician,Gitcs India,5 - 7 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role: Data Scientists - Computational Scientist / Applied Mathematician\nLocation: Bangalore\nMust Have:\nMSc/PhD in Physics, Stats, Math, Comp Bio, CS, etc.\nStrong Python + Jupyter skills\nExperience with real-world biological data\nEager to learn biology & make real patient impact\nBonus: Genomics, sequencing tech, cloud (AWS/GCP), Rust/C++, Snakemake, ML models\nWork with biologists, chemists & ML scientists to analyze experiments & build predictive tools.\n\nAbout Role\nYou have strong mathematical foundations and a passion for understanding & analyzing data. You have a solid understanding of how to analyze noisy, real-world, data. In addition to a strong background in statistics, you are also capable of programming a computer to perform data analysis tasks. You want to impact people by treating genetic diseases and are capable of learning biology that is crucial to the data analysis task.\nYou enjoy close-knit teamwork and a highly interdisciplinary intellectual environment where you will work side-by-side with both computational and bench scientists. You continually learn and challenge yourself with scientific pursuits. You take pride in rigorous pursuit of science.\n\nRequired Qualifications\nMSc or PhD in a quantitative discipline such as physics, statistics, economics, applied mathematics, computer science, computation genomics / biology or related field\nFamiliarity with analyzing real world data and quantifying uncertainty\nGood understanding of Python is a required, knowledge of Rust / C++ are nice to have\nVery comfortable with Juptyer ecosystem\nAbility to independently master unfamiliar topics, especially in biological sciences and data analysis\nDesire to make a difference to patients with rare disease\n\nHelpful Qualifications\nStrong foundation in algorithms of scientific computing: steepest descent, Metropolis-Hastings, Runge-Kutta, Krylov space methods, etc\nFluent and comfortable working across local and cloud environments and tools (AWS, GCP, kubernetes, docker)\nUnderstanding of sequencing technologies (Illumina, PacBio, Oxford Nanopore)\nKnowledge of standard software tools in transcriptomics, genomics, NGS assays & biological data analysis\nUnderstanding of Linux, make, snakemake, git, etc.\nExposure in distributed computing (Spark, Dask etc.)",Industry Type: Biotechnology,Department: Research & Development,"Employment Type: Full Time, Permanent","['Cloud Platforms', 'Research Analysis', 'Python', 'C++', 'Rust', 'Docker', 'Jupyter Ecosystem', 'GCP', 'Statistical Analysis Software', 'Bioinformatics', 'AWS', 'Machine Learning']",2025-06-11 05:55:23
Associate Data Scientist,ZIGRAM,3 - 6 years,Not Disclosed,['Gurugram'],"Role & responsibilities\n\nHighly focused individual with self-driven attitude\nProblem solving and logical thinking to automate and improve internal processes\nUsing various tools such as SQL and Python for managing the various requirements for different data asset projects.\nAbility to diligently involve in activities like Data Cleaning, Retrieval, Manipulation, Analytics and Reporting\nUsing data science and statistical techniques to build machine learning models and deal with textual data.\nKeep up-to-date knowledge of the industry and related markets\nAbility to multitask, prioritize, and manage time efficiently\nUnderstand needs of the hiring organization or client in order to target solutions to their benefit\nAdvanced speaking and writing skills for effective communication\nAbility to work in cross functional teams demonstrating high level of commitment and coordination\nAttention to details and commitment to accuracy for the desired deliverable\nShould demonstrate and develop a sense of ownership towards the assigned task\nAbility to keep sensitive business information confidential\nContribute, positively and extensively towards building the organizational reputation, brand and\noperational excellence\n\nPreferred candidate profile\n\n3-6 years of relevant experience in data science\nAdvanced knowledge of statistics and basics of machine learning\nExperienced in dealing with textual data and using natural language processing techniques\nAbility to conduct analysis to extract actionable insights\nTechnical skills in Python (Numpy, Pandas, NLTK, transformers, Spacy), SQL and other programming languages for dealing with large datasets\nExperienced in data cleaning, manipulation, feature engineering and building models\nExperienced in the end-to-end development of a data science project\nStrong interpersonal skills and extremely resourceful\nProven ability to complete assigned task according to the outlined scope and timeline\nGood language, communication and writing skills in English\nExpertise in using tools like MS Office, PowerPoint, Excel and Word\nGraduate or Post-graduate from a reputed college or university",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Natural Language Processing', 'Machine Learning', 'Statistics', 'Random Forest', 'Data Science', 'Logistic Regression', 'Word', 'Mathematics', 'Powerpoint', 'MS Office', 'Deep Learning', 'Numpy', 'Analytics', 'SQL', 'Nltk', 'data base', 'English', 'Excel', 'Pandas', 'Transformers', 'Spacy', 'Python']",2025-06-11 05:55:25
Lead Data Scientist,Tothr,8 - 12 years,20-22.5 Lacs P.A.,"['Chennai', 'Bengaluru']","Experience working closely with other data scientists, data engineers' software engineers, data managers and business partners.\n7+ years in designing, planning, prototyping, productionizing, maintaining\nknowledge in Python, Go, Java,\nSQL knowledge",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Statistical Modeling', 'Python Development', 'Machine Learning', 'Deep Learning', 'Generative Ai', 'Advance Sql']",2025-06-11 05:55:26
Sr. Data Scientist - Chennai,Teamplus Staffing Solution,10 - 17 years,25-40 Lacs P.A.,['Chennai( T Nagar )'],"LLMs - OpenAI, Gemini, CoPilot etc.\nGood Knowledge of RAG Pipeline Architectures\nFine/Prompt/Instruction Tuning of LLMs\nmachine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn).\ncloud services (GCP, Azure, AWS).",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Scientist', 'Azure', 'Artificial Intelligence', 'CoPilot', 'LLM', 'Machine Learning', 'Deep Learning', 'Pytorch', 'GCP', 'RAG', 'OpenAI', 'Keras', 'AWS', 'Gemini']",2025-06-11 05:55:28
Associate Lead / Lead Data Scientist,Ignitho,6 - 10 years,Not Disclosed,['Chennai( Sholinganallur )'],"Job Title: Lead Data Scientist\nLocation: Chennai\nReports To: CEO\nJob Summary:\nWe are seeking a highly skilled and experienced Data Scientist with 6+ years of hands-on experience in data science, analytics, and stakeholder engagement. The ideal candidate should have strong expertise in Python, Tableau, Snowflake, Machine Learning, Statistical testing and should be comfortable driving business insights through storytelling and daily interactions with stakeholders.\nKey Responsibilities:\nDesign, build, and deploy scalable machine learning models to solve complex business problems\nWrite and optimize complex SQL queries, particularly on the Snowflake platform\nDevelop insightful dashboards and visualizations using Tableau\nConduct data exploration, cleaning, and transformation using Python & R and relevant libraries (Pandas, NumPy, Scikit-learn, etc.)\nPerform A/B testing & other statistical techniques\nTranslate analytical insights into clear, compelling stories and recommendations for stakeholders\nCollaborate cross-functionally with product, engineering, and business teams to understand data needs and deliver solutions\nPresent findings and recommendations to both technical and non-technical stakeholders regularly\nRequirements:\n6+ years of professional experience in data science or advanced analytics\nStrong proficiency in Python for data manipulation, analysis, and modelling\nProven experience in building dashboards and reports using Tableau\nExpertise in writing complex SQL queries, especially on Snowflake\nSolid understanding of machine learning techniques, statistical tests and model deployment best practices\nExcellent communication and storytelling skills to convey data-driven insights\nComfortable working closely with stakeholders daily to gather requirements and present findings",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Tableau', 'Machine Learning', 'Python', 'SQL', 'Pandas', 'Scikit-Learn', 'Numpy', 'Ml']",2025-06-11 05:55:30
Lead Data Engineer,Acuity Knowledge Partners,8 - 13 years,20-25 Lacs P.A.,"['Pune', 'Bangalore Rural', 'Gurugram']","Desired Skills and experience\n9+ years of experience in software development with a focus on data projects using Python, PySpark, and associated frameworks.\nProven experience as a Data Engineer with experience in Azure cloud.\nExperience implementing solutions using Azure cloud services, Azure Data Factory, Azure Lake Gen 2, Azure Databases, Azure Data Fabric, API Gateway management, Azure Functions.",,,,"['Data Engineering', 'Python', 'Pyspark', 'ETL', 'SQL']",2025-06-11 05:55:31
Senior Data Scientist,IDS Infotech,5 - 6 years,Not Disclosed,['Chandigarh'],"We are seeking a highly skilled and motivated Senior Data Scientist with 56 years of experience to drive the development of intelligent AI systems.\nThis role requires extensive hands-on experience with Large Language Models (LLMs), strong background in Agentic AI, Machine Learning, and Python programming.\nYou will work on designing autonomous agents, building scalable ML pipelines, and integrating advanced LLM-powered solutions into real-world products.\nKey Responsibilities:\nArchitect and implement agentic AI systems that use LLMs for autonomous reasoning, planning, and multi-step task execution.\nLead the development, fine-tuning, evaluation, and deployment of Large Language Models (LLMs) using frameworks like Hugging Face Transformers, LangChain, LLM orchestration tools, and vector databases.\nDevelop ML models using supervised, unsupervised, and reinforcement learning techniques, and integrate them into production environments.\nDesign and build end-to-end machine learning pipelines, including data ingestion, feature engineering, training, and deployment.\nOptimize model performance and latency in real-world applications and implement model monitoring and retraining strategies.\nCollaborate with cross-functional teams including product, engineering, and business to translate AI capabilities into product features.\nMentor junior data scientists and contribute to the team's technical excellence and innovation.\nRequired Skills & Experience:\n56 years of professional and relevant experience in data science, AI, or machine learning roles.\nProven hands-on experience with LLMs, including fine-tuning, prompt engineering, and RAG (Retrieval-Augmented Generation) pipelines.\nDeep expertise in Python and ML libraries such as scikit-learn, PyTorch, TensorFlow, and transformers.\nStrong understanding of agentic AI principles, autonomous agents, and task orchestration.\nExperience with cloud platforms (AWS, GCP, or Azure) and scalable infrastructure for deploying AI models.\nExposure to API development, ML model serving, and integration with real-time systems.\nExcellent communication and collaboration skills in cross-functional environments.\nEducational Qualification:\nBachelor of Technology (B.Tech) in Computer Science, Data Science, Artificial Intelligence, or a related field from a recognized institution.\nPreferred Qualifications:\nContributions to open-source AI/ML projects or research publications related to LLMs or agent-based systems.\nFamiliarity with LangChain, AutoGPT, CrewAI, or other agent orchestration frameworks.\nExperience with vector databases (e.g., FAISS, Pinecone) and knowledge of semantic search.\nUnderstanding of multi-agent collaboration, goal decomposition, and planning architectures.",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'ML model serving', 'Azure', 'PyTorch', 'CrewAI', 'GCP', 'LLM orchestration', 'AWS', 'Machine Learning', 'Python', 'TensorFlow']",2025-06-11 05:55:33
Data Engineer _Technology Lead,Broadridge,6 - 10 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nAnalyzes and solve problems using technical experience, judgment and precedents\nProvides informal guidance to new team members\nExplains complex information to others in straightforward situations\n1. Data Engineering and Modelling:\nDesign & Develop Scalable Data Pipelines: Leverage AWS technologies to design, develop, and manage end-to-end data pipelines with services like .",,,,"['Star Schema', 'Snowflake', 'AWS', 'Apache Airflow']",2025-06-11 05:55:35
Senior Data Scientist,Straive,5 - 10 years,Not Disclosed,"['Hyderabad', 'Gurugram', 'Bengaluru']","Role & responsibilities\nRequires 5-8 years of proven experience in banking/payments/other domains\nStrong experience in developing Machine Learning models, Python & SQL\nExperience working with pre-trained models, awareness of state-of-art in embeddings and applicability for use cases\nDetailed oriented with a proactive mindset towards problem-solving\nExcellent communication and presentation skills with the ability to convey complex information clearly and concisely",,,,"['Machine Learning', 'Python', 'SQL', 'Xgboost', 'Neural Networks', 'Random Forest']",2025-06-11 05:55:36
"Mid Data Science & AIML, GenAI Lead/Engineer","NTT DATA, Inc.",3 - 6 years,Not Disclosed,['Bengaluru'],"Req ID: 312501\n\nWe are currently seeking a Mid Data Science & AIML, GenAI Lead/Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob DutiesJob TitleData Science & AIML, GenAI Lead/Engineer\n\nKey Responsibilities:\n""¢ Develop and implement traditional machine learning algorithms.\n""¢ Deploy at least one model in a production environment.\n""¢ Write and maintain Python code for data science and machine learning projects.\n\nMinimum Skills RequiredPreferred Qualifications:\n""¢ Knowledge of Deep Learning (DL) techniques.\n""¢ Experience working with Generative AI (GenAI) and Large Language Models (LLM).\n""¢ Exposure to Langchain.\n\n""‹""‹""‹""‹""‹""‹""‹",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'artificial intelligence', 'deep learning', 'data science', 'algorithms', 'natural language processing', 'scikit-learn', 'dl', 'aiml', 'numpy', 'sql', 'tensorflow', 'r', 'predictive modeling', 'statistical modeling', 'machine learning algorithms', 'statistics']",2025-06-11 05:55:38
Data Engineer Senior Consultant,"NTT DATA, Inc.",8 - 13 years,Not Disclosed,['Bengaluru'],"Req ID: 327863\n\nWe are currently seeking a Data Engineer Senior Consultant to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob DutiesResponsibilitiesLead the development of backend systems using Django. Design and implement scalable and secure APIs. Integrate Azure Cloud services for application deployment and management. Utilize Azure Databricks for big data processing and analytics. Implement data processing pipelines using PySpark. Collaborate with front-end developers, product managers, and other stakeholders to deliver comprehensive solutions. Conduct code reviews and ensure adherence to best practices. Mentor and guide junior developers. Optimize database performance and manage data storage solutions. Ensure high performance and security standards for applications. Participate in architecture design and technical decision-making. Minimum Skills RequiredQualificationsBachelor's degree in Computer Science, Information Technology, or a related field. 8+ years of experience in backend development. 8+ years of experience with Django. Proven experience with Azure Cloud services. Experience with Azure Databricks and PySpark. Strong understanding of RESTful APIs and web services. Excellent communication and problem-solving skills. Familiarity with Agile methodologies. Experience with database management (SQL and NoSQL).\n\nSkills:\nDjango, Python, Azure Cloud, Azure Databricks, Delta Lake and Delta tables, PySpark, SQL/NoSQL databases, RESTful APIs, Git, and Agile methodologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['web services', 'rest', 'python', 'backend development', 'django', 'azure cloud services', 'pyspark', 'jquery', 'sql', 'git', 'asp.net', 'ssrs', 'web api', 'wcf', 'big data', 'agile methodology', 'azure databricks', 'c#', 'ssas', 'data engineering', 'azure cloud', 'sql server', 'javascript', 'nosql', 'linq', 'ssis']",2025-06-11 05:55:40
Lead Data Engineer,"NTT DATA, Inc.",5 - 10 years,Not Disclosed,['Bengaluru'],"Req ID: 306669\n\nWe are currently seeking a Lead Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n Position Overview  We are seeking a highly skilled and experienced Lead Data/Product Engineer to join our dynamic team. The ideal candidate will have a strong background in streaming services and AWS cloud technology, leading teams and directing engineering workloads. This is an opportunity to work on the core systems supporting multiple secondary teams, so a history in software engineering and interface design would be an advantage.\n\n\n\n Key Responsibilities  \n\nLead and direct a small team of engineers engaged in\n\n- Engineering reuseable assets for the later build of data products\n\n- Building foundational integrations with Kafka, Confluent Cloud and AWS\n\n- Integrating with a large number of upstream and downstream technologies\n\n- Providing best in class documentation for downstream teams to develop, test and run data products built using our tools\n\n- Testing our tooling, and providing a framework for downstream teams to test their utilisation of our products\n\n- Helping to deliver CI, CD and IaC for both our own tooling, and as templates for downstream teams\n\n\n\n Required Skills and Qualifications  \n\n\n\n- Bachelor's degree in Computer Science, Engineering, or related field\n\n- 5+ years of experience in data engineering\n\n- 3+ years of experience with real time (or near real time) streaming systems\n\n- 2+ years of experience leading a team of data engineers\n\n- A willingness to independently learn a high number of new technologies and to lead a team in learning new technologies\n\n- Experience in AWS cloud services, particularly Lambda, SNS, S3, and EKS, API Gateway\n\n- Strong experience with Python\n\n- Strong experience in Kafka\n\n- Excellent understanding of data streaming architectures and best practices\n\n- Strong problem-solving skills and ability to think critically\n\n- Excellent communication skills to convey complex technical concepts both directly and through documentation\n\n- Strong use of version control and proven ability to govern a team in the best practice use of version control\n\n- Strong understanding of Agile and proven ability to govern a team in the best practice use of Agile methodologies\n\n\n\n Preferred Skills and Qualifications  \n\n   \n\n- An understanding of cloud networking patterns and practises\n\n- Experience with working on a library or other long term product\n\n- Knowledge of the Flink ecosystem\n\n- Experience with terraform\n\n- Experience with CI pipelines\n\n- Ability to code in a JVM language\n\n- Understanding of GDPR and the correct handling of PII\n\n- Knowledge of technical interface design\n\n- Basic use of Docker",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'version control', 'kafka', 'cloud formation aws', 'agile', 'jvm', 'cloud services', 'api gateway', 'eks', 'data engineering', 'docker', 'sql', 'java', 'lambda expressions', 'aws cloud', 'devops', 'real time pcr', 'sns', 'terraform', 'software engineering', 'aws', 'agile methodology', 'interface design']",2025-06-11 05:55:41
Senior GenAI Data Engineer,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,"['New Delhi', 'Chennai', 'Bengaluru']","Your day at NTT DATA\nSenior GenAI Data Engineer\nWe are seeking an experienced Senior Data Engineer to join our team in delivering cutting-edge Generative AI (GenAI) solutions to clients. The successful candidate will be responsible for designing, developing, and deploying data pipelines and architectures that support the training, fine-tuning, and deployment of LLMs for various industries. This role requires strong technical expertise in data engineering, problem-solving skills, and the ability to work effectively with clients and internal teams.\nWhat you'll be doing\nKey Responsibilities:\nDesign, develop, and manage data pipelines and architectures to support GenAI model training, fine-tuning, and deployment\nData Ingestion and Integration: Develop data ingestion frameworks to collect data from various sources, transform, and integrate it into a unified data platform for GenAI model training and deployment.\nGenAI Model Integration: Collaborate with data scientists to integrate GenAI models into production-ready applications, ensuring seamless model deployment, monitoring, and maintenance.\nCloud Infrastructure Management: Design, implement, and manage cloud-based data infrastructure (e.g., AWS, GCP, Azure) to support large-scale GenAI workloads, ensuring cost-effectiveness, security, and compliance.\nWrite scalable, readable, and maintainable code using object-oriented programming concepts in languages like Python, and utilize libraries like Hugging Face Transformers, PyTorch, or TensorFlow\nPerformance Optimization: Optimize data pipelines, GenAI model performance, and infrastructure for scalability, efficiency, and cost-effectiveness.\nData Security and Compliance: Ensure data security, privacy, and compliance with regulatory requirements (e.g., GDPR, HIPAA) across data pipelines and GenAI applications.\nClient Collaboration: Collaborate with clients to understand their GenAI needs, design solutions, and deliver high-quality data engineering services.\nInnovation and R&D: Stay up to date with the latest GenAI trends, technologies, and innovations, applying research and development skills to improve data engineering services.\nKnowledge Sharing: Share knowledge, best practices, and expertise with team members, contributing to the growth and development of the team.\nRequirements:\nBachelors degree in computer science, Engineering, or related fields (Master's recommended)\nExperience with vector databases (e.g., Pinecone, Weaviate, Faiss, Annoy) for efficient similarity search and storage of dense vectors in GenAI applications\n5+ years of experience in data engineering, with a strong emphasis on cloud environments (AWS, GCP, Azure, or Cloud Native platforms)\nProficiency in programming languages like SQL, Python, and PySpark\nStrong data architecture, data modeling, and data governance skills\nExperience with Big Data Platforms (Hadoop, Databricks, Hive, Kafka, Apache Iceberg), Data Warehouses (Teradata, Snowflake, BigQuery), and lakehouses (Delta Lake, Apache Hudi)\nKnowledge of DevOps practices, including Git workflows and CI/CD pipelines (Azure DevOps, Jenkins, GitHub Actions)\nExperience with GenAI frameworks and tools (e.g., TensorFlow, PyTorch, Keras)\nNice to have:\nExperience with containerization and orchestration tools like Docker and Kubernetes\nIntegrate vector databases and implement similarity search techniques, with a focus on GraphRAG is a plus\nFamiliarity with API gateway and service mesh architectures\nExperience with low latency/streaming, batch, and micro-batch processing\nFamiliarity with Linux-based operating systems and REST APIs\nLocation: Delhi or Bangalore\nWorkplace type:\nHybrid Working",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GenAI', 'hive', 'continuous integration', 'kubernetes', 'ci/cd', 'pyspark', 'data architecture', 'sql', 'docker', 'tensorflow', 'git', 'data modeling', 'gcp', 'devops', 'linux', 'jenkins', 'pytorch', 'keras', 'hadoop', 'bigquery', 'python', 'microsoft azure', 'data engineering', 'data bricks', 'data governance', 'aws']",2025-06-11 05:55:43
Data Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\nDeliver\n\nNo Performance Parameter Measure\n1. Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Tableau.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Tableau', 'data warehouses', 'data integration', 'Data modelling']",2025-06-11 05:55:45
Data Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure\n1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Database Architecting.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'data validation', 'data mining', 'data warehousing', 'business analytics', 'dashboards', 'data integration']",2025-06-11 05:55:46
Data Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\n\n\n\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n\n\n\n\nMandatory Skills: Database Architecting.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Database Architecting', 'data analysis', 'data mining', 'business analytics', 'reporting tools']",2025-06-11 05:55:48
Data Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Chennai'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\n\n\n\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n\n\n\n\nMandatory Skills: Google BigQuery.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Google BigQuery', 'analytics reporting', 'data analysis', 'data mining', 'business analytics', 'bigquery', 'data integration']",2025-06-11 05:55:50
Data/ML Ops Engineer,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Additional Career Level Description:\n\n\nKnowledge and application:\nSeasoned, experienced professional; has complete knowledge and understanding of area of specialization.\nUses evaluation, judgment, and interpretation to select right course of action.\n\n\n\nProblem solving:\nWorks on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\nResolves and assesses a wide range of issues in creative ways and suggests variations in approach.\n\n\n\nInteraction:\nEnhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\nWorks with others outside of own area of expertise, with the ability to adapt style to differing audiences and often advises others on difficult matters.\n\n\n\nImpact:\nImpacts short to medium term goals through personal effort or influence over team members.\n\n\n\nAccountability:\nAccountable for own targets with work reviewed at critical points.\nWork is done independently and is reviewed at critical points.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ML Ops', 'python', 'spark', 'big data', 'data engineering', 'artificial intelligence', 'ml', 'sql']",2025-06-11 05:55:51
Data Analyst,"NTT DATA, Inc.",3 - 8 years,Not Disclosed,['Pune'],"Req ID: 324676\n\nWe are currently seeking a Data Analyst to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nExtract, transform, and load (ETL) data from various sources, ensuring data quality, integrity, and accuracy.\n\nPerform data cleansing, validation, and preprocessing to prepare structured and unstructured data for analysis.\n\nDevelop and execute queries, scripts, and data manipulation tasks using SQL, Python, or other relevant tools.\n\nAnalyze large datasets to identify trends, patterns, and correlations, drawing meaningful conclusions that inform business decisions.\n\nCreate clear and concise data visualizations, dashboards, and reports to communicate findings effectively to stakeholders.\n\nCollaborate with clients and cross-functional teams to gather and understand data requirements, translating them into actionable insights.\n\nWork closely with other departments to support their data needs.\n\nCollaborate with Data Scientists and other analysts to support predictive modeling, machine learning, and statistical analysis.\n\nContinuously monitor data quality and proactively identify anomalies or discrepancies, recommending corrective actions.\n\nStay up-to-date with industry trends, emerging technologies, and best practices to enhance analytical techniques.\n\nAssist in the identification and implementation of process improvements to streamline data workflows and analysis.\n\nBasic Qualifications:\n\n3 + years of proficiency in data analysis tools such as [Tools - e.g., Excel, SQL, R, Python].\n\n3+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nUndergraduate or Graduate degree preferred\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nStrong proficiency in data analysis tools such as Python, SQL, Talend (any ETL).\n\nExperience with data visualization tools like PowerBI.\n\nExperience with cloud data platforms .\n\nFamiliarity with ETL (Extract, Transform, Load) processes and tools.\n\nKnowledge of machine learning techniques and tools.\n\nExperience in a specific industry (e.g., financial services, healthcare, manufacturing) can be a plus.\n\nUnderstanding of data governance and data privacy regulations.\n\nAbility to query and manipulate databases and data warehouses.\n\nExcellent analytical and problem-solving skills.\n\nStrong communication skills with the ability to explain complex data insights to non-technical stakeholders.\n\nDetail-oriented with a commitment to accuracy.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'data analytics', 'data engineering', 'analysis tools', 'software engineering', 'python', 'data manipulation', 'talend', 'power bi', 'data warehousing', 'machine learning', 'dashboards', 'sql', 'data cleansing', 'data quality', 'r', 'predictive modeling', 'data visualization', 'etl']",2025-06-11 05:55:53
Data Analyst - People & Culture,"NTT DATA, Inc.",3 - 8 years,Not Disclosed,['Gurugram'],"Key Responsibilities:\nDesign, develop, and maintain Power BI dashboards and reports to support People & Culture initiatives.\nCollaborate with stakeholders to understand business needs and translate them into analytical solutions.\nExtract, clean, and consolidate data from multiple systems (e.g., HRIS, S/Sheets, payroll, engagement platforms).\nAnalyse trends and provide insights on key people metrics\nEnsure data accuracy and integrity, and work closely with P&C and other business areas to ensure data accuracy.\nCommunicate complex findings in a clear and actionable manner to both technical and non-technical audiences.\nSupport ongoing improvements in data processes, systems, and reporting capabilities.\nContribute to building a strong data-driven culture within the People & Culture function.\nMay act as project lead on local projects to work on HR system-related process improvements via project plans and schedules, including documentation of scope and requirements, and reporting of project status.\n\n\nKey :\n3+ years of experience in a Data Analyst role, ideally within an HR, People Analytics, or People Experience environment.\nAdvanced Power BI skills ""“ including DAX, data modelling, and report/dashboard development.\nProven ability to work with large and complex datasets from various systems and sources.\nStrong analytical mindset with attention to detail and a passion for turning data into insights.\nExperience working in fast-paced, matrixed, or global organisations is highly desirable.\nExcellent communication skills with the ability to present data in a user-friendly and impactful way.\nSelf-starter with the ability to work independently as part of a remote / offshore team.\nStrong project management skills\n\n\n\nPreferences:\nPrior experience of HR role supporting UK\nPrior experience of HR Master Data maintenance in SuccessFactors or SAP.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'sap', 'power bi', 'data modeling', 'dax', 'master data', 'data analysis', 'c', 'management skills', 'hris', 'dashboard development', 'bi', 'documentation', 'pivot table', 'vlookup', 'dashboards', 'sql', 'successfactors', 'power bi dashboards', 'advanced excel', 'data maintenance']",2025-06-11 05:55:55
Sr. Executive Data Engineering Analytics,IndiGo,5 - 10 years,Not Disclosed,['Gurugram'],"Role & responsibilities\nDevelop in Python, create responsive dashboards, and manage large datasets.\nDesign and deploy Power BI reports based on business needs.\nApply machine learning, deep learning, and statistical analysis (e.g., classification, regression, sentiment analysis, time series).\nTranslate technical concepts for non-technical stakeholders.\nDesign and implement Big Data platform components (batch/stream processing, memory cache, SQL query layer, rule engine).\nBuild scalable data solutions.\nConduct root cause analysis, troubleshoot applications, and support configurations.\nAutomate processes and reporting within the Operations Control Center (OCC).\nCollaborate with leadership to solve business problems and define objectives.\nRecommend and implement automated solutions.\nPerform data analysis and apply statistical methods for decision-making.\n\nPreferred candidate profile\n\nEducation: Bachelors or Master’s in Computer Science, Engineering, or related field. (Mathematics/Statistics)\nExperience: 5–10 years in data engineering & analytics.\nSkills:\nPython (hands-on)\nPower BI (dashboarding)\nSQL/SSMS (data storage and extraction)\nPower Automate / Power Apps (nice to have)",Industry Type: Travel & Tourism,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'SQL', 'SSMS', 'Power Bi', 'Power Automate']",2025-06-11 05:55:56
BFSI Data and Analytics Project Lead - CITI Bank,"NTT DATA, Inc.",12 - 17 years,Not Disclosed,['Bengaluru'],"Req ID: 326459\n\nWe are currently seeking a BFSI Data and Analytics Project Lead - CITI Bank to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesData & Analytics Project Lead with over 12+ years of experience in BFSI Domain\nThe Data and Analytics Delivery Manager will oversee the successful delivery of the Client's data and analytics projects, ensuring our clients derive maximum value from their data assets. This leadership role involves setting strategy, managing delivery teams, collaborating across functions, and upholding data governance and quality standards. The ideal candidate brings strong technical and business acumen to build and execute data-driven strategies aligned with the Client's mission of transforming their business with data-driven insights.\nThe core responsibilities for the job include the following:\nProject and Program Oversight:\n""¢ Oversee end-to-end delivery of complex data and analytics projects, ensuring timely, high-quality, and cost-effective outcomes.\n""¢ Establish project governance, risk management, and quality assurance standards for effective project delivery.\n""¢ Monitor project portfolios and allocate resources to optimize productivity across multiple client engagements.\n\nStakeholder Collaboration and Engagement:\n""¢ Serve as the primary liaison between data delivery teams, sales, product, and client-facing teams to ensure client needs are met.\n""¢ Present data strategies, project status, and insights effectively to both technical and non-technical stakeholders, fostering alignment.\n""¢ Drive collaboration with product management and engineering teams to align on data needs and operational goals.\n\nInnovation and Technology Adoption:\n""¢ Stay abreast of the latest trends in GenAI, Agentic AI in data engineering, data science, machine learning, and AI to enhance the Client's data capabilities.\n""¢ Must have experience in Cloud Modernization, DWH, Datalake project execution\n""¢ Drive the adoption of advanced analytics tools and technologies to improve data delivery efficiency and solution impact.\n""¢ Assess and recommend new tools, platforms, and partners to continuously improve data solutions.\nTeam Development and Leadership:\n""¢ Recruit, mentor, and retain a high-performing data and analytics team, fostering a culture of collaboration and continuous improvement.\n""¢ Set performance goals, conduct regular evaluations, and provide ongoing feedback to support team growth.\n\nMinimum Skills Required:\n""¢ Educational BackgroundBachelor's or Master's degree in Data Science, Computer Science, Business Administration, or a related field.\n""¢ Experience15+ years of experience in data and analytics, including at least 5 years in a leadership role with a proven track record in delivery management.\n""¢ Technical ProficiencyDeep understanding of data warehousing, data visualization, data governance, data trust and big data tools (SQL, Python, R, Tableau, Power BI, and cloud platforms like AWS, Azure, or Google Cloud).\n""¢ Must have experience in Cloud Modernization, DWH, Datalake project execution\n""¢ BFSI KnowledgeMandatory to have worked in BFSI projects delivered Data & Analytics projects to BFSI clients\n""¢ Project Management ExpertiseStrong background in Agile, Scrum, or other project management methodologies.\n""¢ Leadership and CommunicationExcellent interpersonal and communication skills, with a demonstrated ability to lead, influence, and engage stakeholders at all levels.\n""¢ Analytical and Problem-Solving\n\nSkills:\nStrong analytical mindset with a track record of delivering actionable insights from complex data\nThe Data and Analytics Delivery Manager will oversee the successful delivery of the Client's data and analytics projects, ensuring our retail clients derive maximum value from their data assets. This leadership role involves setting strategy, managing delivery teams, collaborating across functions, and upholding data governance and quality standards. The ideal candidate brings strong technical and business acumen to build and execute data-driven strategies aligned with the Client's mission of transforming retail with data-driven insights.""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data governance', 'scrum', 'agile', 'data visualization', 'big data', 'advanced analytics', 'python', 'data analytics', 'data warehousing', 'power bi', 'microsoft azure', 'project management process', 'machine learning', 'sql', 'tableau', 'r', 'bfsi', 'data science', 'gcp', 'project execution', 'aws']",2025-06-11 05:55:58
"Tcs is hiring For Azure Data Engineer (ADF, Python, Pyspark,DataBricks",Tata Consultancy Services,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Role & responsibilities\nStrong understanding of Azure environment (PaaS, IaaS) and experience in working with Hybrid model\nAt least 1 project experience in Azure Data Stack that involves components like Azure Data Lake, Azure Synapse Analytics, Azure Data Factory, Azure Data Bricks, Azure Analysis Service, Azure SQL DWH\nStrong hands-on SQL/T-SQL/Spark SQL and database concepts",,,,"['Azure Databricks', 'Azure Data Factory', 'Pyspark', 'Python Data', 'Microsoft Azure', 'Devops', 'Python', 'SQL']",2025-06-11 05:55:59
Associate Data Engineer,"NTT DATA, Inc.",1 - 3 years,Not Disclosed,"['New Delhi', 'Chennai', 'Bengaluru']","Your day at NTT DATA\nWe are seeking an experienced Data Engineer to join our team in delivering cutting-edge Generative AI (GenAI) solutions to clients. The successful candidate will be responsible for designing, developing, and deploying data pipelines and architectures that support the training, fine-tuning, and deployment of LLMs for various industries. This role requires strong technical expertise in data engineering, problem-solving skills, and the ability to work effectively with clients and internal teams.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesign, develop, and manage data pipelines and architectures to support GenAI model training, fine-tuning, and deployment\nData Ingestion and Integration: Develop data ingestion frameworks to collect data from various sources, transform, and integrate it into a unified data platform for GenAI model training and deployment.\nGenAI Model Integration: Collaborate with data scientists to integrate GenAI models into production-ready applications, ensuring seamless model deployment, monitoring, and maintenance.\nCloud Infrastructure Management: Design, implement, and manage cloud-based data infrastructure (e.g., AWS, GCP, Azure) to support large-scale GenAI workloads, ensuring cost-effectiveness, security, and compliance.\nWrite scalable, readable, and maintainable code using object-oriented programming concepts in languages like Python, and utilize libraries like Hugging Face Transformers, PyTorch, or TensorFlow\nPerformance Optimization: Optimize data pipelines, GenAI model performance, and infrastructure for scalability, efficiency, and cost-effectiveness.\nData Security and Compliance: Ensure data security, privacy, and compliance with regulatory requirements (e.g., GDPR, HIPAA) across data pipelines and GenAI applications.\nClient Collaboration: Collaborate with clients to understand their GenAI needs, design solutions, and deliver high-quality data engineering services.\nInnovation and R&D: Stay up to date with the latest GenAI trends, technologies, and innovations, applying research and development skills to improve data engineering services.\nKnowledge Sharing: Share knowledge, best practices, and expertise with team members, contributing to the growth and development of the team.\n\nBachelors degree in computer science, Engineering, or related fields (Masters recommended)\nExperience with vector databases (e.g., Pinecone, Weaviate, Faiss, Annoy) for efficient similarity search and storage of dense vectors in GenAI applications\n5+ years of experience in data engineering, with a strong emphasis on cloud environments (AWS, GCP, Azure, or Cloud Native platforms)\nProficiency in programming languages like SQL, Python, and PySpark\nStrong data architecture, data modeling, and data governance skills\nExperience with Big Data Platforms (Hadoop, Databricks, Hive, Kafka, Apache Iceberg), Data Warehouses (Teradata, Snowflake, BigQuery), and lakehouses (Delta Lake, Apache Hudi)\nKnowledge of DevOps practices, including Git workflows and CI/CD pipelines (Azure DevOps, Jenkins, GitHub Actions)\nExperience with GenAI frameworks and tools (e.g., TensorFlow, PyTorch, Keras)\nNice to have:\nExperience with containerization and orchestration tools like Docker and Kubernetes\nIntegrate vector databases and implement similarity search techniques, with a focus on GraphRAG is a plus\nFamiliarity with API gateway and service mesh architectures\nExperience with low latency/streaming, batch, and micro-batch processing\nFamiliarity with Linux-based operating systems and REST APIs",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Iceberg', 'Faiss', 'PySpark', 'Kafka', 'Pinecone', 'GitHub Actions', 'Snowflake', 'Apache Hudi', 'AWS', 'Azure DevOps', 'Python', 'Azure', 'BigQuery', 'Hadoop', 'Annoy', 'Teradata', 'SQL', 'Jenkins', 'Hive', 'Cloud Native platforms', 'GCP', 'Delta Lake', 'Databricks', 'Weaviate']",2025-06-11 05:56:01
AWS Databricks Data Engineer,Tata Consultancy Services,6 - 11 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\n\nData Engineer, Expertise in AWS, Databricks and Pyspark",,,,"['Pyspark', 'databricks', 'AWS', 'data engineer', 'Aws Databricks']",2025-06-11 05:56:03
Senior Data Engineer,Impetus Technologies,5 - 10 years,Not Disclosed,['United Arab Emirates'],"The Opportunity:We are seeking a highly motivated and technically strong Module Lead Software Engineer with significant expertise in Python, PySpark, and Palantir Foundry. In this role, you will be responsible for the end-to-end technical ownership, design, and delivery of a specific module or component within our enterprise data platform. You will combine hands-on development with technical leadership, ensuring the highest standards of code quality, performance, and reliability.\n\nKey Responsibilities:\n\nModule Technical Leadership & Ownership: Take full technical ownership of a specific module or component within the data platform on Palantir Foundry. This includes defining its technical roadmap, architecture, design patterns, and ensuring its integration into the broader data ecosystem.\nHands-on Development and Complex Problem Solving: Act as a lead individual contributor, developing sophisticated data pipelines, transformations, and applications using Python and PySpark within Palantir Foundry's various tools (e.g., Code Workbook, Pipeline Builder). Tackle the most challenging technical problems and implement core functionalities for the module.\nQuality Assurance and Best Practices Advocacy: Drive and enforce high standards for code quality, test coverage, documentation, and operational excellence within your module. Conduct rigorous code reviews, provide constructive feedback, and mentor engineers within your immediate scope to elevate their technical skills.\nCross-Functional Collaboration and Module Integration: Collaborate extensively with other module leads, architects, data scientists, and business stakeholders to ensure seamless integration of your module's deliverables. Proactively identify and manage technical dependencies and ensure the module aligns with overall project goals and architectural vision.\n• Performance Optimization and Troubleshooting: Continuously monitor and optimize the performance of your module's data pipelines and applications. Efficiently troubleshoot and resolve complex technical issues, data quality concerns, and system failures specific to your module.\n\nRequired Qualifications:\n\nExperience: 6-8 years of progressive experience in software development with a strong focus on data engineering.\nPython Proficiency: Expert-level proficiency in Python, including advanced programming concepts, data structures, and performance optimization techniques.\nPySpark Expertise: Strong experience with PySpark for large-scale distributed data processing, transformations, and analytics.\nPalantir Foundry: Proven, hands-on experience designing, developing, and deploying solutions within Palantir Foundry is essential.\nDeep familiarity with Foundry's data integration capabilities, Code Workbook, Pipeline Builder, Data Health checks, and Ontology modeling.\nExperience with Foundry's approach to data governance and versioning.\nSQL Skills: Excellent SQL skills for complex data querying, manipulation, and optimization.\nData Warehousing/Lakes: Solid understanding of data warehousing concepts, data lake architectures, and ETL/ELT principles.\nCloud Platforms: Experience with at least one major cloud platform (AWS, Azure, GCP), particularly with data-related services.\nVersion Control: Strong experience with Git and collaborative development workflows.\n\nPreferred Qualifications (Nice-to-Have):\n\nExperience mentoring or leading small technical teams/pods.\nFamiliarity with containerization technologies (Docker, Kubernetes).\nExperience with streaming data technologies (e.g., Kafka, Kinesis).\nUnderstanding of CI/CD pipelines for data solutions.\nKnowledge of data governance, data quality, and metadata management best practices.\nExperience in [specific industry, e.g., Financial Services, Manufacturing, Healthcare].",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Palantir Foundry', 'Python', 'Spark', 'Data Warehousing', 'SQL']",2025-06-11 05:56:04
Data Engineer,"NTT DATA, Inc.",1 - 4 years,Not Disclosed,['Bengaluru'],"Req ID: 321498\n\nWe are currently seeking a Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob Duties""¢ Work closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.\n""¢ Work closely with Data modeller to ensure data models support the solution design\n""¢ Develop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.\n""¢ Analysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.\n""¢ Develop documentation and artefacts to support projects\n\nMinimum Skills Required""¢ ADF\n""¢ Fivetran (orchestration & integration)\n""¢ SQL\n""¢ Snowflake DWH",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'data engineering', 'sql', 'oracle adf', 'etl', 'hive', 'python', 'data analysis', 'oracle', 'informatica powercenter', 'amazon redshift', 'talend', 'data warehousing', 'power bi', 'plsql', 'tableau', 'data modeling', 'spark', 'etl tool', 'technical specifications', 'hadoop', 'informatica', 'unix']",2025-06-11 05:56:05
Data Engineer Advisor,"NTT DATA, Inc.",8 - 13 years,Not Disclosed,['Bengaluru'],"Req ID: 327859\n\nWe are currently seeking a Data Engineer Advisor to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob DutiesResponsibilitiesLead the development of backend systems using Django. Design and implement scalable and secure APIs. Integrate Azure Cloud services for application deployment and management. Utilize Azure Databricks for big data processing and analytics. Implement data processing pipelines using PySpark. Collaborate with front-end developers, product managers, and other stakeholders to deliver comprehensive solutions. Conduct code reviews and ensure adherence to best practices. Mentor and guide junior developers. Optimize database performance and manage data storage solutions. Ensure high performance and security standards for applications. Participate in architecture design and technical decision-making. Minimum Skills RequiredQualificationsBachelor's degree in Computer Science, Information Technology, or a related field. 8+ years of experience in backend development. 8+ years of experience with Django. Proven experience with Azure Cloud services. Experience with Azure Databricks and PySpark. Strong understanding of RESTful APIs and web services. Excellent communication and problem-solving skills. Familiarity with Agile methodologies. Experience with database management (SQL and NoSQL).\n\nSkills:\nDjango, Python, Azure Cloud, Azure Databricks, Delta Lake and Delta tables, PySpark, SQL/NoSQL databases, RESTful APIs, Git, and Agile methodologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['web services', 'rest', 'python', 'backend development', 'django', 'azure cloud services', 'css', 'pyspark', 'jquery', 'sql', 'git', 'asp.net', 'html', 'web api', 'mvc', 'wcf', 'agile methodology', 'azure databricks', 'c#', 'entity framework', 'azure cloud', 'javascript', 'sql server', 'nosql', 'angular', 'linq', '.net']",2025-06-11 05:56:07
Data Architect Sr. Advisor,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Bengaluru'],"Req ID: 323777\n\nWe are currently seeking a Data Architect Sr. Advisor to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'python', 'client engagement', 'ai solutions', 'aiml', 'data architecture', 'machine learning', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'gcp cloud', 'ml']",2025-06-11 05:56:09
Big Data Engineer - Python+ PySpark + Spark,Hexaware Technologies,9 - 12 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Experience - 9 years - 12 years\nLocation - Mumbai / Chennai / Bangalore / Pune\n\nDevelop and maintain scalable data pipelines using PySpark and Spark SQL for processing large datasets efficiently.\nWrite clean, reusable, and optimized code in Python for data manipulation, analysis, and automation tasks.",,,,"['PySpark', 'Spark', 'Python', 'SQL']",2025-06-11 05:56:10
Sr. AWS Databricks Data Engineer,Tata Consultancy Services,6 - 11 years,Not Disclosed,"['Kolkata', 'Pune', 'Chennai']","Role & responsibilities\n\nData Engineer, Expertise in AWS, Databricks and Pyspark",,,,"['AWS', 'Data Bricks', 'Pyspark', 'Data engineer', 'Aws Databricks']",2025-06-11 05:56:11
Lead Data Architect,"NTT DATA, Inc.",7 - 12 years,Not Disclosed,['Bengaluru'],"We are currently seeking a Lead Data Architect to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n Position Overview  We are seeking a highly skilled and experienced Data Architect to join our dynamic team. The ideal candidate will have a strong background in designing and implementing data solutions using AWS infrastructure and a variety of core and supplementary technologies. This role requires a deep understanding of data architecture, cloud services, and the ability to drive innovative solutions to meet business needs.\n\n\n\n Key Responsibilities  \n\n- Architect end-to-end data solutions using AWS services, including Lambda, SNS, S3, and EKS, Kafka and Confluent, all within a larger and overarching programme ecosystem\n\n- Architect data processing applications using Python, Kafka, Confluent Cloud and AWS\n\n- Ensure data security and compliance throughout the architecture\n\n- Collaborate with cross-functional teams to understand business requirements and translate them into technical solutions\n\n- Optimize data flows for performance, cost-efficiency, and scalability\n\n- Implement data governance and quality control measures\n\n- Ensure delivery of CI, CD and IaC for NTT tooling, and as templates for downstream teams\n\n- Provide technical leadership and mentorship to development teams and lead engineers\n\n- Stay current with emerging technologies and industry trends\n\n\n\n Required Skills and Qualifications  \n\n\n\n- Bachelor's degree in Computer Science, Engineering, or related field\n\n- 7+ years of experience in data architecture and engineering\n\n- Strong expertise in AWS cloud services, particularly Lambda, SNS, S3, and EKS\n\n- Strong experience with Confluent\n\n- Strong experience in Kafka\n\n- Solid understanding of data streaming architectures and best practices\n\n- Strong problem-solving skills and ability to think critically\n\n- Excellent communication skills to convey complex technical concepts to both technical and non-technical stakeholders\n\n- Knowledge of Apache Airflow for data orchestration\n\n\n\n Preferred Qualifications  \n\n\n\n- An understanding of cloud networking patterns and practises\n\n- Experience with working on a library or other long term product\n\n- Knowledge of the Flink ecosystem\n\n- Experience with Terraform\n\n- Deep experience with CI/CD pipelines\n\n- Strong understanding of the JVM language family\n\n- Understanding of GDPR and the correct handling of PII\n\n- Expertise with technical interface design\n\n- Use of Docker\n\n\n\n Responsibilities  \n\n- Design and implement scalable data architectures using AWS services, Confluent and Kafka\n\n- Develop data ingestion, processing, and storage solutions using Python and AWS Lambda, Confluent and Kafka\n\n- Ensure data security and implement best practices using tools like Synk\n\n- Optimize data pipelines for performance and cost-efficiency\n\n- Collaborate with data scientists and analysts to enable efficient data access and analysis\n\n- Implement data governance policies and procedures\n\n- Provide technical guidance and mentorship to junior team members\n\n- Evaluate and recommend new technologies to improve data architecture",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'cloud services', 'data architecture', 'kafka', 'ci cd pipeline', 'jvm', 'python', 'confluent', 'aws iam', 'airflow', 'ci/cd', 'eks', 'aws lambda', 'apache flink', 'docker', 'apache', 'lambda expressions', 'aws cloud', 'data governance', 'sns', 'terraform', 'aws', 'interface design']",2025-06-11 05:56:13
Data Engineering Specialist,Accenture,4 - 8 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","Job Summary:\nWe are:\nSales Excellence. Sales Excellence at Accenture empowers our people to compete, win and grow. We provide everything they need to grow their client portfolios, optimize their deals and enable their sales talent, all driven by sales intelligence.\nThe team will be aligned to the Client Success, which is a new function to support Accenture’s approach to putting client value and client experience at the heart of everything we do to foster client love. Our ambition is that every client loves working with Accenture and believes we’re the ideal partner to help them create and realize their vision for the future – beyond their expectations.\nYou are:\nA builder at heart – curious about new tools and their usefulness, eager to create prototypes, and adaptable to changing paths. You enjoy sharing your experiments with a small team and are responsive to the needs of your clients.\nThe work:\nThe Center of Excellence (COE) enables Sales Excellence to deliver best-in-class service offerings to Accenture leaders, practitioners, and sales teams.\nAs a member of the COE Analytics Tools & Reporting team, you will help in building and enhancing data foundation for reporting tools and Analytics tool to provide insights on underlying trends and key drivers of the business.\n\nRoles & Responsibilities:\nCollaborate with the Client Success, Analytics COE, CIO Engineering/DevOps team, and stakeholders to build and enhance Client success data lake.\nWrite complex SQL scripts to transform data for the creation of dashboards or reports and validate the accuracy and completeness of the data.\nBuild automated solutions to support any business operation or data transfer.\nDocument and build efficient data model for reporting and analytics use case.\nAssure the Data Lake data accuracy, consistency, and timeliness while ensuring user acceptance and satisfaction.\nWork with the Client Success, Sales Excellence COE members, CIO Engineering/DevOps team and Analytics Leads to standardize Data in data lake.\nProfessional & Technical Skills:\nBachelor’s degree or equivalent experience in Data Engineering, analytics, or similar field.\nAt least 4 years of professional experience in developing and managing ETL pipelines.\nA minimum of 2 years of GCP experience.\nAbility to write complex SQL and prepare data for dashboarding.\nExperience in managing and documenting data models.\nUnderstanding of Data governance and policies.\nProficiency in Python and SQL scripting language.\nAbility to translate business requirements into technical specification for engineering team.\nCuriosity, creativity, a collaborative attitude, and attention to detail.\nAbility to explain technical information to technical as well as non-technical users.\nAbility to work remotely with minimal supervision in a global environment.\nProficiency with Microsoft office tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GCP', 'Python', 'SQL', 'Data Engineering', 'ETL']",2025-06-11 05:56:14
Cloud Data Engineer,PwC India,3 - 8 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Description:\n\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\n\nKey Responsibilities:",,,,"['AWS OR Azure', 'Azure Data Engineer OR AWS Data Engineer', 'Azure', 'AWS']",2025-06-11 05:56:16
Senior Azure Data Engineer (Only Immediate Join),Adecco,7 - 12 years,15-30 Lacs P.A.,['Bengaluru'],"Position : Senior Azure Data Engineer (Only Immediate Joiner)\nLocation : Bangalore\nMode of Work : Work from Office\nExperience : 7 years relevant experience\nJob Type : Full Time (On Roll)\n\nJob Description\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills.\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions.\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution.\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nInterested candidates kindly share your CV and below details to usha.sundar@adecco.com\n1) Present CTC (Fixed + VP) -\n2) Expected CTC -\n3) No. of years experience -\n4) Notice Period -\n5) Offer-in hand -\n6) Reason of Change -\n7) Present Location -",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'SCALA', 'Azure Data Lake', 'Databricks', 'Stream Analytics', 'Azure Data Engineering', 'Azure Databricks', 'Data factory', 'Streaming data', 'Data Bricks', 'SQL']",2025-06-11 05:56:17
Data Ingest Engineer,"NTT DATA, Inc.",6 - 10 years,Not Disclosed,['Pune'],"Req ID: 323909\n\nWe are currently seeking a Data Ingest Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesThe Applications Development Technology Lead Analyst is a senior level position responsible for establishing and implementing new or revised application systems and programs in coordination with the Technology team.\nThis is a position within the Ingestion team of the DRIFT data ecosystem. The focus is on ingesting data in a timely , complete, and comprehensive fashion while using the latest technology available to Citi. The ability to leverage new and creative methods for repeatable data ingestion from a variety of data sources while always questioning ""is this the best way to solve this problem"" and ""am I providing the highest quality data to my downstream partners"" are the questions we are trying to solve.\n\nResponsibilities:\n""¢ Partner with multiple management teams to ensure appropriate integration of functions to meet goals as well as identify and define necessary system enhancements to deploy new products and process improvements\n""¢ Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards\n""¢ Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint\n""¢ Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation\n""¢ Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals\n""¢ Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions\n""¢ Serve as advisor or coach to mid-level developers and analysts, allocating work as necessary\n""¢ Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.\n\nMinimum Skills Required""¢ 6-10 years of relevant experience in Apps Development or systems analysis role\n""¢ Extensive experience system analysis and in programming of software applications\n""¢ Application Development using JAVA, Scala, Spark\n""¢ Familiarity with event driven applications and streaming data\n""¢ Experience with Confluent Kafka, HDFS, HIVE, structured and unstructured database systems (SQL and NoSQL)\n""¢ Experience with various schema and data types -> JSON, AVRO, Parquet, etc.\n""¢ Experience with various ELT methodologies and formats -> JDBC, ODBC, API, Web hook, SFTP, etc.\n""¢ Experience working within the Agile and version control tool sets (JIRA, Bitbucket, Git, etc.)\n""¢ Ability to adjust priorities quickly as circumstances dictate\n""¢ Demonstrated leadership and project management skills\n""¢ Consistently demonstrates clear and concise written and verbal communication",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['application software', 'system analysis', 'java', 'project management', 'applications programming', 'hive', 'scala', 'jdbc', 'bitbucket', 'sql', 'parquet', 'git', 'spark', 'data ingestion', 'json', 'debugging', 'api', 'jira', 'avro', 'odbc', 'elt', 'data engineering', 'nosql', 'app development', 'kafka', 'sftp', 'agile']",2025-06-11 05:56:19
Job opening For Manager Data Science @ GlobalData-Bengaluru,Globaldata,8 - 12 years,Not Disclosed,['Bengaluru( Koramangala )'],"Hello,\n\nGreetings from GlobalData!!!\n\nJob opening for Data Science Manager role @ GD-Bengaluru\nJob Criteria :-\nQualification: B.Tech/BE/MCA/M.Tech/M.Sc-(Computers)",,,,"['Natural Language Processing', 'Machine Learning', 'Generative Ai Tools', 'Large Language Model', 'Python', 'Tensorflow', 'Artificial Intelligence', 'Statistics', 'Deep Learning', 'Data Science', 'Agentic AI', 'Pytorch', 'Pandas']",2025-06-11 05:56:20
BFSI Data and Analytics Delivery Manager,"NTT DATA, Inc.",18 - 23 years,Not Disclosed,['Bengaluru'],"Req ID: 326457\n\nWe are currently seeking a BFSI Data and Analytics Delivery Manager to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesData & Analytics Delivery Manager with over 18+ years of experience in BFSI Domain\nThe Data and Analytics Delivery Manager will oversee the successful delivery of the Client's data and analytics projects, ensuring our clients derive maximum value from their data assets. This leadership role involves setting strategy, managing delivery teams, collaborating across functions, and upholding data governance and quality standards. The ideal candidate brings strong technical and business acumen to build and execute data-driven strategies aligned with the Client's mission of transforming their business with data-driven insights.\nThe core responsibilities for the job include the following:\nProject and Program Oversight:\n""¢ Oversee end-to-end delivery of complex data and analytics projects, ensuring timely, high-quality, and cost-effective outcomes.\n""¢ Establish project governance, risk management, and quality assurance standards for effective project delivery.\n""¢ Monitor project portfolios and allocate resources to optimize productivity across multiple client engagements.\n\nStakeholder Collaboration and Engagement:\n""¢ Serve as the primary liaison between data delivery teams, sales, product, and client-facing teams to ensure client needs are met.\n""¢ Present data strategies, project status, and insights effectively to both technical and non-technical stakeholders, fostering alignment.\n""¢ Drive collaboration with product management and engineering teams to align on data needs and operational goals.\n\nInnovation and Technology Adoption:\n""¢ Stay abreast of the latest trends in GenAI, Agentic AI in data engineering, data science, machine learning, and AI to enhance the Client's data capabilities.\n""¢ Must have experience in Cloud Modernization, DWH, Datalake project execution\n""¢ Drive the adoption of advanced analytics tools and technologies to improve data delivery efficiency and solution impact.\n""¢ Assess and recommend new tools, platforms, and partners to continuously improve data solutions.\nTeam Development and Leadership:\n""¢ Recruit, mentor, and retain a high-performing data and analytics team, fostering a culture of collaboration and continuous improvement.\n""¢ Set performance goals, conduct regular evaluations, and provide ongoing feedback to support team growth.\n\nMinimum Skills Required:\n""¢ Educational BackgroundBachelor's or Master's degree in Data Science, Computer Science, Business Administration, or a related field.\n""¢ Experience15+ years of experience in data and analytics, including at least 5 years in a leadership role with a proven track record in delivery management.\n""¢ Technical ProficiencyDeep understanding of data warehousing, data visualization, data governance, data trust and big data tools (SQL, Python, R, Tableau, Power BI, and cloud platforms like AWS, Azure, or Google Cloud).\n""¢ Must have experience in Cloud Modernization, DWH, Datalake project execution\n""¢ BFSI KnowledgeMandatory to have worked in BFSI projects delivered Data & Analytics projects to BFSI clients\n""¢ Project Management ExpertiseStrong background in Agile, Scrum, or other project management methodologies.\n""¢ Leadership and CommunicationExcellent interpersonal and communication skills, with a demonstrated ability to lead, influence, and engage stakeholders at all levels.\n""¢ Analytical and Problem-Solving\n\nSkills:\nStrong analytical mindset with a track record of delivering actionable insights from complex data\nThe Data and Analytics Delivery Manager will oversee the successful delivery of the Client's data and analytics projects, ensuring our retail clients derive maximum value from their data assets. This leadership role involves setting strategy, managing delivery teams, collaborating across functions, and upholding data governance and quality standards. The ideal candidate brings strong technical and business acumen to build and execute data-driven strategies aligned with the Client's mission of transforming retail with data-driven insig""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data governance', 'scrum', 'agile', 'data visualization', 'big data', 'advanced analytics', 'python', 'data analytics', 'data warehousing', 'power bi', 'microsoft azure', 'project management process', 'machine learning', 'sql', 'tableau', 'r', 'bfsi', 'data science', 'gcp', 'project execution', 'aws']",2025-06-11 05:56:22
Lead Data Analyst-Business Intelligence,Tresvista Financial Services,6 - 10 years,Not Disclosed,['Bengaluru'],"Roles and Responsibilities\nArchitect and incorporate an effective Data framework enabling end to end Data Solution.\nUnderstand business needs, use cases and drivers for insights and translate them into detailed technical specifications.\nCreate epics, features and user stories with clear acceptance criteria for execution and delivery by the data engineering team.\nCreate scalable and robust data solution designs that incorporate governance, security and compliance aspects.\nDevelop and maintain logical and physical data models and work closely with data engineers, data analysts and data testers for successful implementation of them.\nAnalyze, assess and design data integration strategies across various sources and platforms.\nCreate project plans and timelines while monitoring and mitigating risks and controlling progress of the project.\nConduct daily scrum with the team with a clear focus on meeting sprint goals and timely resolution of impediments.\nAct as a liaison between technical teams and business stakeholders and ensure.\nGuide and mentor the team for best practices on Data solutions and delivery frameworks.\nActively work, facilitate and support the stakeholders/ clients to complete User Acceptance Testing ensure there is strong adoption of the data products after the launch.\nDefining and measuring KPIs/KRA for feature(s) and ensuring the Data roadmap is verified through measurable outcomes\n\nPrerequisites\n5 to 8 years of professional, hands on experience building end to end Data Solution on Cloud based Data Platforms including 2+ years working in a Data Architect role.\nProven hands on experience in building pipelines for Data Lakes, Data Lake Houses, Data Warehouses and Data Visualization solutions\nSound understanding of modern Data technologies like Databricks, Snowflake, Data Mesh and Data Fabric.\nExperience in managing Data Life Cycle in a fast-paced, Agile / Scrum environment.\nExcellent spoken and written communication, receptive listening skills, and ability to convey complex ideas in a clear, concise fashion to technical and non-technical audiences\nAbility to collaborate and work effectively with cross functional teams, project stakeholders and end users for quality deliverables withing stipulated timelines\nAbility to manage, coach and mentor a team of Data Engineers, Data Testers and Data Analysts. Strong process driver with expertise in Agile/Scrum framework on tools like Azure DevOps, Jira or Confluence\nExposure to Machine Learning, Gen AI and modern AI based solutions.\n\nExperience\nTechnical Lead Data Analytics with 6+ years of overall experience out of which 2+ years is on Data architecture.\n\nEducation\nEngineering degree from a Tier 1 institute preferred.\n\nCompensation\nThe compensation structure will be as per industry standards",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Bricks', 'Data Lake', 'Data Warehousing', 'Python', 'Business Intelligence', 'Databricks Engineer', 'Machine Learning', 'Redshift Aws', 'Snowflake', 'Data Visualization', 'ETL', 'Data Mesh']",2025-06-11 05:56:23
Data Engineer - Streamsets,Wipro,4 - 6 years,Not Disclosed,['Pune'],"Role Purpose\n\nConsultants are expected to complete specific tasks as part of a consulting project with minimal supervision. They will start to build a core areas of expertise and will contribute to client projects typically involving in-depth analysis, research, supporting solution development and being a successful communicator. The Consultant must achieve high personal billability.\n\n\n\nResponsibilities\n\nAs aDeveloper, Analyze, design and develop components, tools and custom features using Databricks and streamsets as per business needs.\n\nAnalyze, create and develop technical design to determine business functional and non-functional requirements & processes and review them with the technology leads and architects.\n\nWork collaboratively with all the teams as required to build a data model and setup things in Databricks andstreamsets as appropriate to transform the data and transfer it as appropriate.\n\nDevelop solutions to publish/subscribe Kafka topics.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'data bricks', 'web services', 'asp.net', 'api', 'requirement analysis']",2025-06-11 05:56:25
Senior Data Engineer,S&P Global Market Intelligence,6 - 11 years,Not Disclosed,['Gurugram'],"\n\nAbout the Role: \n\nGrade Level (for internal use):\n10\n \n\nPosition summary \n\n Our proprietary software-as-a-service helps automotive dealerships and sales teams better understand and predict exactly which customers are ready to buy, the reasons why, and the key offers and incentives most likely to close the sale. Its micro-marketing engine then delivers the right message at the right time to those customers, ensuring higher conversion rates and a stronger ROI. \n\n \n\nWhat You'll Do \n\n You will be part of our Data Platform & Product Insights data engineering team. As part of this agile team, you will work in our cloud native environment to \n\n Build & support data ingestion and processing pipelines in cloud. This will entail extraction, load and transformation of big data from a wide variety of sources, both batch & streaming, using latest data frameworks and technologies \n\n Partner with product team to assemble large, complex data sets that meet functional and non-functional business requirements, ensure build out of Data Dictionaries/Data Catalogue and detailed documentation and knowledge around these data assets, metrics and KPIs. \n\n Warehouse this data, build data marts, data aggregations, metrics, KPIs, business logic that leads to actionable insights into our product efficacy, marketing platform, customer behaviour, retention etc. \n\n Build real-time monitoring dashboards and alerting systems. \n\n Coach and mentor other team members. \n\n\n \n\nWho you are \n\n 6+ years of experience in Big Data and Data Engineering. \n\n Strong knowledge of advanced SQL, data warehousing concepts and DataMart designing. \n\n Have strong programming skills in SQL, Python/ PySpark etc. \n\n Experience in design and development of data pipeline, ETL/ELT process on-premises/cloud. \n\n Experience in one of the Cloud providers GCP, Azure, AWS. \n\n Experience with relational SQL and NoSQL databases, including Postgres and MongoDB. \n\n Experience workflow management toolsAirflow, AWS data pipeline, Google Cloud Composer etc. \n\n Experience with Distributed Versioning Control environments such as GIT, Azure DevOps \n\n Building Docker images and fetch/promote and deploy to Production. Integrate Docker container orchestration framework using Kubernetes by creating pods, config Maps, deployments using terraform. \n\n Should be able to convert business queries into technical documentation. \n\n Strong problem solving and communication skills. \n\n Bachelors or an advanced degree in Computer Science or related engineering discipline. \n\n\n \n\nGood to have some exposure to \n\n Exposure to any Business Intelligence (BI) tools like Tableau, Dundas, Power BI etc. \n\n Agile software development methodologies. \n\n Working in multi-functional, multi-location teams \n\n  \n \n\nGrade10  \n \n\nLocationGurugram \n \n\nHybrid Modeltwice a week work from office  \n \n\nShift Time12 pm to 9 pm IST  \n What You'll Love About Us Do ask us about these! \n\n \n\nTotal Rewards. Monetary, beneficial and developmental rewards! \n\n \n\nWork Life Balance. You can't do a good job if your job is all you do! \n\n \n\nPrepare for the Future.Academy we are all learners; we are all teachers! \n\n \n\nEmployee Assistance Program. Confidential and Professional Counselling and Consulting. \n\n \n\nDiversity & Inclusion. HeForShe! \n\n \n\nInternal Mobility.\n\nGrow with us! \n\n  \n  \n  \n  \n\nAbout automotiveMastermind\n\nWho we are:\n\nFounded in 2012, automotiveMastermind is a leading provider of predictive analytics and marketing automation solutions for the automotive industry and believes that technology can transform data, revealing key customer insights to accurately predict automotive sales. Through its proprietary automated sales and marketing platform, Mastermind, the company empowers dealers to close more deals by predicting future buyers and consistently marketing to them. automotiveMastermind is headquartered in New York City. For more information, visit automotivemastermind.com.\n\nAt automotiveMastermind, we thrive on high energy at high speed. Were an organization in hyper-growth mode and have a fast-paced culture to match. Our highly engaged teams feel passionately about both our product and our people. This passion is what continues to motivate and challenge our teams to be best-in-class. Our cultural values of Drive and Help have been at the core of what we do, and how we have built our culture through the years. This cultural framework inspires a passion for success while collaborating to win.\n\nWhat we do:\n\nThrough our proprietary automated sales and marketing platform, Mastermind, we empower dealers to close more deals by predicting future buyers and consistently marketing to them. In short, we help automotive dealerships generate success in their loyalty, service, and conquest portfolios through a combination of turnkey predictive analytics, proactive marketing, and dedicated consultative services.\n\nWhats In It For\n\nYou\n\nOur Purpose:\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\n\nOur Values:\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of\n\nintegrity in all we do, bring a spirit of\n\ndiscovery to our work, and collaborate in close\n\npartnership with each other and our customers to achieve shared goals.\n\nBenefits:\n\nWe take care of you, so you cantake care of business. We care about our people. Thats why we provide everything youand your careerneed to thrive at S&P Global.\n\nHealth & WellnessHealth care coverage designed for the mind and body.\n\n\n\nContinuous LearningAccess a wealth of resources to grow your career and learn valuable new skills.\n\nInvest in Your FutureSecure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n\nFamily Friendly PerksIts not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n\nBeyond the BasicsFrom retail discounts to referral incentive awardssmall perks can make a big difference.\n\nFor more information on benefits by country visithttps://spgbenefits.com/benefit-summaries",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['advance sql', 'python', 'pyspark', 'sql', 'data warehousing concepts', 'kubernetes', 'microsoft azure', 'data warehousing', 'power bi', 'relational sql', 'elt', 'data engineering', 'business intelligence', 'azure devops', 'docker', 'nosql', 'tableau', 'git', 'postgresql', 'gcp', 'agile', 'big data', 'aws', 'mongodb']",2025-06-11 05:56:26
Technical Lead - Sr. Data Engineer,Edgematics Consulting,8 - 13 years,Not Disclosed,['Pune'],"About This Role :\n\nWe are looking for a talented and experienced Data Engineer with Tech Lead with hands-on expertise in any ETL Tool with full knowledge about CI/CD practices with leading a team technically more than 5 and client facing and create Data Engineering, Data Quality frameworks. As a tech lead must ensure to build ETL jobs, Data Quality Jobs, Big Data Jobs performed performance optimization by understanding the requirements, create re-usable assets and able to perform production deployment and preferably worked in DWH appliances Snowflake / redshift / Synapse\n\nResponsibilities\nWork with a team of engineers in designing, developing, and maintaining scalable and efficient data solutions using Any Data Integration (any ETL tool like Talend / Informatica) and any Big Data technologies.\nDesign, develop, and maintain end-to-end data pipelines using Any ETL Data Integration (any ETL tool like Talend / Informatica) to ingest, process, and transform large volumes of data from heterogeneous sources.\nHave good experience in designing cloud pipelines using Azure Data Factory or AWS Glues/Lambda.\nImplemented Data Integration end to end with any ETL technologies.\nImplement database solutions for storing, processing, and querying large volumes of structured and unstructured and semi-structured data\nImplement Job Migrations of ETL Jobs from Older versions to New versions.\nImplement and write advanced SQL scripts in SQL Database at medium to expert level. \nWork with technical team with client and provide guidance during technical challenges.\nIntegrate and optimize data flows between various databases, data warehouses, and Big Data platforms.\nCollaborate with cross-functional teams to gather data requirements and translate them into scalable and efficient data solutions.\nOptimize ETL, Data Load performance, scalability, and cost-effectiveness through optimization techniques.\nInteract with Client on a daily basis and provide technical progress and respond to technical questions.\nImplement best practices for data integration.\nImplement complex ETL data pipelines or similar frameworks to process and analyze massive datasets.\nEnsure data quality, reliability, and security across all stages of the data pipeline.\nTroubleshoot and debug data-related issues in production systems and provide timely resolution.\nStay current with emerging technologies and industry trends in data engineering technologies, CI/CD, and incorporate them into our data architecture and processes.\nOptimize data processing workflows and infrastructure for performance, scalability, and cost-effectiveness.\nProvide technical guidance and foster a culture of continuous learning and improvement.\nImplement and automate CI/CD pipelines for data engineering workflows, including testing, deployment, and monitoring.\nPerform migration to production deployment from lower environments, test & validate\n\nMust Have Skills\nMust be certified in any ETL tools, Database, Cloud.(Snowflake certified is more preferred)\nMust have implemented at least 3 end-to-end projects in Data Engineering.\nMust have worked on performance management optimization and tuning for data loads, data processes, data transformation in big data\nMust be flexible to write code using JAVA/Scala/Python etc. as required\nMust have implemented CI/CD pipelines using tools like Jenkins, GitLab CI, or AWS CodePipeline.\nMust have managed a team technically of min 5 members and guided the team technically.\nMust have the Technical Ownership capability of Data Engineering delivery.\nStrong communication capabilities with client facing.\nBachelor's or Master's degree in Computer Science, Engineering, or a related field.\n5 years of experience in software engineering or a related role, with a strong focus on Any ETL Tool, database, integration.\nProficiency in Any ETL tools like Talend , Informatica etc for Data Integration for building and orchestrating data pipelines.\nHands-on experience with relational databases such as MySQL, PostgreSQL, or Oracle, and NoSQL databases such as MongoDB, Cassandra, or Redis.\nSolid understanding of database design principles, data modeling, and SQL query optimization.\nExperience with data warehousing, Data Lake , Delta Lake concepts and technologies, data modeling, and relational databases.",Industry Type: IT Services & Consulting,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'ETL', 'Azure Aws', 'Data Management', 'Big Data', 'Ci/Cd', 'Data Integration', 'Data Quality', 'Data Pipeline', 'Data Warehousing', 'Data Modeling', 'Data Governance']",2025-06-11 05:56:28
Lead AWS Glue Data Engineer,Allegis Group,8 - 13 years,Not Disclosed,[],"Lead AWS Glue Data Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\n\nWe are seeking a skilled Lead AWS Data Engineer with 8+ years of strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena.\nImplement data migration and transformation processes using AWS DMS and Glue.\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\n\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDS', 'Glue', 'DMS', 'Lambda', 'Redshift', 'Athena']",2025-06-11 05:56:30
Data & AI Technical Solution Architects,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Bengaluru'],"Req ID: 323775\n\nWe are currently seeking a Data & AI Technical Solution Architects to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works""",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'solution design', 'gcp', 'gcp cloud', 'ml']",2025-06-11 05:56:31
Senior Data Engineer,Client of Hiresquad Resources,5 - 8 years,22.5-30 Lacs P.A.,"['Noida', 'Hyderabad', 'Bengaluru']","Role: Data Engineer\nExp: 5 to 8 Years\nLocation: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\nNP: Immediate to 15 Days (Try to find only immediate joiners)\n\n\nNote:\nCandidate must have experience in Python, Kafka Streams, Pyspark, and Azure Databricks.\nNot looking for candidates who have only Exp in Pyspark and not in Python.\n\n\nJob Title: SSE Kafka, Python, and Azure Databricks (Healthcare Data Project)\nExperience:  5 to 8 years\n\nRole Overview:\nWe are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n\nKey Responsibilities:\nLead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\nArchitect scalable data streaming and processing solutions to support healthcare data workflows.\nDevelop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\nEnsure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\nCollaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\nTroubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\nMentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\nStay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\n\nRequired Skills & Qualifications:\n4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\nExpertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\nExperience with Azure Databricks (or willingness to learn and adopt it quickly).\nHands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\nProficiency in SQL, NoSQL databases, and data modeling for big data processing.\nKnowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\nExperience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\nStrong analytical skills, problem-solving mindset, and ability to lead complex data projects.\nExcellent communication and stakeholder management skills.\n\n\n\nEmail: Sam@hiresquad.in",Industry Type: Medical Services / Hospital,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Azure Databricks', 'Python', 'Etl Pipelines', 'Data Engineering', 'Data Streaming', 'Healthcare Data', 'python scripts', 'schema registry', 'SQL Database', 'Nosql Databases', 'Kafka Streams', 'kafka connect']",2025-06-11 05:56:33
Big Data Engineer,Grid Dynamics,6 - 11 years,Not Disclosed,['Bengaluru'],"NOTE: We are only looking for candidates who can join Immediately to available to join in 15 days\nExperience level- 6+ years\nLocation: Bangalore (Candidates who are currently in Bangalore can apply)\n\nQualifications we are looking for\nMaster/Bachelor degree in Computer Science, Electrical Engineering, Information Systems or other technical discipline; advanced degree preferred.\nMinimum of 7+ years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies.\nMinimum 4+ years of experience in Hadoop using Core Java Programming, Spark, Scala, Hive and Go lang\nExpertise in Object Oriented Programming Language Java\nExperience using CI/CD Process, version control and bug tracking tools.\nExperience in handling very large data volume in Real Time and batch mode.\nExperience with automation of job execution and validation\nStrong knowledge of Database concepts\nStrong team player.\nStrong communication skills with proven ability to present complex ideas and document in a clear and concise way.\nQuick learner; self-starter, detailed and in-depth.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Big data engineer', 'Spark', 'Java', 'Core Java', 'Hive', 'CI', 'Hadoop', 'Big Data', 'Java Development', 'Big Data Technologies', 'Ci/Cd']",2025-06-11 05:56:34
Collibra Data Governance Engineer,Allegis Group,6 - 11 years,Not Disclosed,[],"Collibra Data Governance Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\nRequired Skills\n5+ years of experience in data governance and/or metadata management.\nHands-on experience with Collibra Data Governance Center (Collibra DGC), including workflow configuration, cataloging, and operating model customization.\nStrong knowledge of metadata management, data lineage, and data quality principles.\nHands-on experience with Snowflake\nFamiliarity with data integration tools and AWS cloud platform\nExperience with SQL and working knowledge of relational databases.\nUnderstanding of data privacy regulations (e.g., GDPR, CCPA) and compliance frameworks.\nPreferred Skills\nCertifications such as Collibra Certified Solution Architect.\nExperience integrating Collibra with tools like Snowflake, Tableau or other BI/analytics platforms.\nExposure to DataOps, MDM (Master Data Management), and data governance frameworks like DAMA-DMBOK.\nStrong communication and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Collibra', 'Metadata', 'Data Governance']",2025-06-11 05:56:36
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Chennai'],"Req ID: 324631\n\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-11 05:56:37
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Chennai'],"Req ID: 324632\n\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-11 05:56:39
"Senior Data Engineer (Snowflake, DBT)",Allegis Global Solutions (AGS),5 - 10 years,Not Disclosed,[],"Senior Data Engineer (Snowflake, DBT, Azure)\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\nJob Details\nTechnical Expertise:\nStrong proficiency in Snowflake architecture, including data sharing, partitioning, clustering, and materialized views.\nAdvanced experience with DBT for data transformations and workflow management.\nExpertise in Azure services, including Azure Data Factory, Azure Data Lake, Azure Synapse, and Azure Functions.\nData Engineering:\nProficiency in SQL, Python, or other relevant programming languages.\nStrong understanding of data modeling concepts, including star schema and normalization.\nHands-on experience with ETL/ELT pipelines and data integration tools.\n\nPreferred Qualifications:\nCertifications in Snowflake, DBT, or Azure Data Engineering.\nFamiliar with data visualization tools like Power BI or Tableau.\nKnowledge of CI/CD pipelines and DevOps practices for data workflows.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Build Tool', 'Azure']",2025-06-11 05:56:41
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Pune'],"Req ID: 324653\n\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-11 05:56:42
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Pune'],"Req ID: 324609\n\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-11 05:56:44
Senior AWS Data Engineer,Sightspectrum,4 - 7 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Must-Have Qualifications:\nAWS Expertise: Strong hands-on experience with AWS data services including Glue, Redshift, Athena, S3, Lake Formation, Kinesis, Lambda, Step Functions, EMR, and CloudWatch.\nETL/ELT Engineering: Deep proficiency in designing robust ETL/ELT pipelines with AWS Glue (PySpark/Scala), Python, dbt, or other automation frameworks.\nData Modeling: Advanced knowledge of dimensional (Star/Snowflake) and normalised data modeling, optimised for Redshift and S3-based lakehouses.\nProgramming Skills: Proficient in Python, SQL, and PySpark, with automation and scripting skills for data workflows.\nArchitecture Leadership: Demonstrated experience leading large-scale AWS data engineering projects across teams and domains.\nPre-sales & Consulting: Proven experience working with clients, responding to technical RFPs, and designing cloud-native data solutions.\nAdvanced PySpark Expertise: Deep hands-on experience in writing optimized PySpark code for distributed data processing, including transformation pipelines using DataFrames, RDDs, and Spark SQL, with a strong grasp of lazy evaluation, catalyst optimizer, and Tungsten execution engine.\nPerformance Tuning & Partitioning: Proven ability to debug and optimize Spark jobs through custom partitioning strategies, broadcast joins, caching, and checkpointing, with proficiency in tuning executor memory, shuffle configurations, and leveraging Spark UI for performance diagnostics in large-scale data workloads (>TB scale).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS Data Engineer', 'SQL', 'ETL', 'Python', 'Airflow', 'Shell Scripting', 'Elt', 'S', 'Glue', 'Amazon Redshift', 'Redshift Aws', 'AWS', 'Athena']",2025-06-11 05:56:46
Data Analyst - Contract 3 months,RWS Group,5 - 8 years,22.5-30 Lacs P.A.,[],"Role & responsibilities -\nDevelop Sisense Report for Certified HAI (HSBC) to provide key business insights and form part of the QBR\nSupport with business reporting on the Evolve roll-out (LXD)\nImplement automated email triggers with reporting capabilities to track engagement and effectiveness.\nGenerate and maintain HAI reports in Salesforce for sales and customer service teams.\nCreate a HAI customer report in Salesforce for sales enablement and customer insights.\nDevelop and optimize the Marketing dashboard to track campaign performance.\nBuild a PJM management dashboard for project performance monitoring.\nProvide ongoing support for data-driven decision-making through continuous insights.\nAutomate customer surveys to streamline feedback collection.\nImplement a customer feedback review system within dashboards to assess quotes and service quality. \nMonitor conversion rates to improve lead generation and sales performance.\nGenerate customer heatmaps to analyze engagement and behavior trends.\n\nPreferred candidate profile\nProficiency in developing reports and dashboards using tools like Sisense and Salesforce, with a strong ability to deliver business insights that support quarterly business reviews (QBRs) and strategic initiatives.\nExperience in creating, managing, and optimizing reports and dashboards within Salesforce, specifically tailored for sales enablement, customer service, and performance monitoring.\nAbility to design and maintain marketing dashboards to effectively track and analyze campaign performance metrics.\nSkilled in building project management dashboards (e.g., PJM) to support performance analysis and reporting for project teams.\nExperience in implementing automated workflows, such as email triggers with embedded reporting, to track user engagement and process efficiency.\nAbility to automate surveys and develop customer feedback dashboards to assess service quality and support data-driven improvement.\nProven capability in delivering continuous insights and analytical support for strategic decision-making across multiple business units.\nCompetence in monitoring and analyzing conversion rates, customer behavior trends, and engagement heatmaps to inform sales and marketing strategies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Project management dashboard', 'Sisense', 'Salesforce']",2025-06-11 05:56:47
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-11 05:56:49
Staff Data Engineer - Machine Learning,Netradyne,5 - 8 years,22.5-35 Lacs P.A.,['Bengaluru'],"Role and Responsibilities:\n\nYou will be embedded within a team of machine learning engineers and data scientists; responsible for building and productizing generative AI and deep learning solutions. You will:\nDesign, develop and deploy production ready scalable solutions that utilizes GenAI, Traditional ML models, Data science and ETL pipelines\nCollaborate with cross-functional teams to integrate AI-driven solutions into business operations.\nBuild and enhance frameworks for automation, data processing, and model deployment.\nUtilize Gen-AI tools and workflows to improve the efficiency and effectiveness of AI solutions.\nConduct research and stay updated with the latest advancements in generative AI and related technologies.\nDeliver key product features within cloud analytics.\n\nRequirements:\n\nB. Tech, M. Tech or PhD in Computer Science, Data Science, Electrical Engineering, Statistics, Maths, Operations Research or related domain.\nStrong programming skills in Python, SQL and solid fundamentals in computer science, particularly in algorithms, data structures, and OOP.\nExperience with building end-to-end solutions on AWS cloud infra.\nGood understanding of internals and schema design for various data stores (RDBMS, Vector databases and NoSQL).\nExperience with Gen-AI tools and workflows, and large language models (LLMs).\nExperience with cloud platforms and deploying models at scale.\nStrong analytical and problem-solving skills with a keen attention to detail.\nStrong knowledge of statistics, probability, and estimation theory.\n\nDesired Skills:\n\nFamiliarity with frameworks such as PyTorch, TensorFlow and Hugging Face.\nExperience with data visualization tools like Tableau, Graphana, Plotly-Dash.\nExposure to AWS services like Kinesis, SQS, EKS, ASG, lambda etc.\nExpertise in at least one popular Python web-framework (like FastAPI, Django or Flask).\nExposure to quick prototyping using Streamlit, Gradio, Dash etc.\nExposure to Big Data processing (Snowflake, Redshift, HDFS, EMR)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'AWS', 'Generative Artificial Intelligence', 'Python', 'Big Data Technologies']",2025-06-11 05:56:51
Data Engineer - Analyst,FedEx,3 - 5 years,Not Disclosed,"['Gurugram', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities :\n\nDesign,develop, and maintain ETL workflows using Ab Initio.\nManage and support critical data pipelines and data sets across complex,high-volume environments.\nPerform data analysis and troubleshoot issues across Teradata and Oracle data sources.\nCollaborate with DevOps for CI/CD pipeline integration using Jenkins, and manage deployments in Unix/Linux environments.\nParticipate in Agile ceremonies including stand-ups, sprint planning, and roadmap discussions.\nSupport cloud migration efforts, including potential adoption of Azure,Databricks, and PySparkbased solutions.\nContribute to project documentation, metadata management (LDM, PDM), onboarding guides, and SOPs\n\n\n\nPreferred candidate profile\n\n3 years of experience in data engineering, with proven expertise in ETL development and maintenance.\nProficiency with Ab Initio tools (GDE, EME, Control Center).\nStrong SQL skills, particularly with Oracle or Teradata.\nSolid experience with Unix/Linux systems and scripting.\nFamiliarity with CI/CD pipelines using Jenkins or similar tools.\nStrong communication skills and ability to collaborate with cross-functional teams.",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Ab Initio', 'Data Modeling', 'ETL', 'Cicd Pipeline', 'Informatica', 'Data Warehousing', 'Databricks', 'Teradata', 'Oracle', 'SQL']",2025-06-11 05:56:52
Sr Data Engineer - Remote,Teamplus Staffing Solution,5 - 10 years,12-18 Lacs P.A.,"['Pune', 'Bengaluru', 'Delhi / NCR']","SQL, SNOWFLAKE, TABLEAU\nSQL, SNOWFLAKE,DBT, Datawarehousing\nSQL, SNOWFLAKE, Python, DBT, Datawarehousing\nSQL, SNOWFLAKE, Datawarehousing, any ETL tool(preffered is Matillion)\nSQL, SNOWFLAKE, TABLEAU",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Matillion', 'SNOWFLAKE', 'Data Warehousing', 'TABLEAU', 'SQL', 'dbt', 'ETL', 'Python']",2025-06-11 05:56:54
DataBricks - Data Engineering,Wipro,5 - 8 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Continuous Integration, Deployment & Monitoring of Software100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan\n2.Quality & CSATOn-Time Delivery, Manage software, Troubleshoot queries,Customer experience, completion of assigned certifications for skill upgradation\n3.MIS & Reporting100% on time MIS & report generation\nMandatory Skills: DataBricks - Data Engineering.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['DataBricks', 'module development', 'software development life cycle', 'Data Engineering', 'software development', 'quality assurance']",2025-06-11 05:56:56
Data Engineer KL-BL,PureSoftware Pvt Ltd,7 - 12 years,Not Disclosed,"['Bengaluru', 'Malaysia']","Core Competences Required and Desired Attributes:\n  Bachelor's degree in computer science, Information Technology, or a related field.\nProficiency in Azure Data Factory, Azure Databricks and Unity Catalog, Azure SQL Database, and other Azure data services.\nStrong programming skills in SQL, Python and PySpark languages.\nExperience in the Asset Management domain would be preferable.\nStrong proficiency in data analysis and data modelling, with the ability to extract insights from complex data sets.\nHands-on experience in Power BI, including creating custom visuals, DAX expressions, and data modelling.\nFamiliarity with Azure Analysis Services, data modelling techniques, and optimization.\nExperience with data quality and data governance frameworks with an ability to debug, fine tune and optimise large scale data processing jobs.\nStrong analytical and problem-solving skills, with a keen eye for detail.\nExcellent communication and interpersonal skills, with the ability to work collaboratively in a team environment.\nProactive and self-motivated, with the ability to manage multiple tasks and deliver high-quality results within deadlines.\n\nRoles and Responsibilities\nCore Competences Required and Desired Attributes:\n  Bachelor's degree in computer science, Information Technology, or a related field.\nProficiency in Azure Data Factory, Azure Databricks and Unity Catalog, Azure SQL Database, and other Azure data services.\nStrong programming skills in SQL, Python and PySpark languages.\nExperience in the Asset Management domain would be preferable.\nStrong proficiency in data analysis and data modelling, with the ability to extract insights from complex data sets.\nHands-on experience in Power BI, including creating custom visuals, DAX expressions, and data modelling.\nFamiliarity with Azure Analysis Services, data modelling techniques, and optimization.\nExperience with data quality and data governance frameworks with an ability to debug, fine tune and optimise large scale data processing jobs.\nStrong analytical and problem-solving skills, with a keen eye for detail.\nExcellent communication and interpersonal skills, with the ability to work collaboratively in a team environment.\nProactive and self-motivated, with the ability to manage multiple tasks and deliver high-quality results within deadlines.",Industry Type: Not mentioned,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['azure databricks', 'python', 'data services', 'data analysis', 'modeling', 'analytical', 'languages', 'catalog', 'pyspark', 'datafactory', 'interpersonal skills', 'microsoft azure', 'power bi', 'azure data factory', 'data engineering', 'sql', 'sql azure', 'data modeling', 'azure analysis', 'programming', 'communication skills']",2025-06-11 05:56:57
Data Analyst-Having Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Analyst\n\nLocation: Bangalore\nExperience: 8 - 15 Yrs\nType: Full-time\n\nRole Overview\n\nWe are seeking a skilled Data Analyst to support our platform powering operational intelligence across airports and similar sectors. The ideal candidate will have experience working with time-series datasets and operational information to uncover trends, anomalies, and actionable insights. This role will work closely with data engineers, ML teams, and domain experts to turn raw data into meaningful intelligence for business and operations stakeholders.\n\nKey Responsibilities\n\nAnalyze time-series and sensor data from various sources\nDevelop and maintain dashboards, reports, and visualizations to communicate key metrics and trends.\nCorrelate data from multiple systems (vision, weather, flight schedules, etc) to provide holistic insights.\nCollaborate with AI/ML teams to support model validation and interpret AI-driven alerts (e.g., anomalies, intrusion detection).\nPrepare and clean datasets for analysis and modeling; ensure data quality and consistency.\nWork with stakeholders to understand reporting needs and deliver business-oriented outputs.\n\n\nQualifications & Required Skills\n\nBachelors or Masters degree in Data Science, Statistics, Computer Science, Engineering, or a related field.\n5+ years of experience in a data analyst role, ideally in a technical/industrial domain.\nStrong SQL skills and proficiency with BI/reporting tools (e.g., Power BI, Tableau, Grafana).\nHands-on experience analyzing structured and semi-structured data (JSON, CSV, time-series).\nProficiency in Python or R for data manipulation and exploratory analysis.\nUnderstanding of time-series databases or streaming data (e.g., InfluxDB, Kafka, Kinesis).\nSolid grasp of statistical analysis and anomaly detection methods.\nExperience working with data from industrial systems or large-scale physical infrastructure.\n\n\nGood-to-Have Skills\n\nDomain experience in airports, smart infrastructure, transportation, or logistics.\nFamiliarity with data platforms (Snowflake, BigQuery, Custom-built using open-source).\nExposure to tools like Airflow, Jupyter Notebooks and data quality frameworks.\nBasic understanding of AI/ML workflows and data preparation requirements.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Kafka', 'SQL', 'airports', 'InfluxDB', 'Airflow', 'structured Data', 'time-series', 'JSON', 'Tableau', 'Grafana', 'R', 'AI/ML', 'Kinesis', 'Snowflake', 'time-series databases', 'Data Preparation', 'Python', 'smart infrastructure', 'BigQuery', 'streaming data', 'Power BI', 'CSV', 'transportation', 'logistic', 'reporting tools']",2025-06-11 05:56:59
Data Engineer,R Systems International,4 - 6 years,15-25 Lacs P.A.,['Noida'],"We are looking for a highly experienced Senior Data Engineer with deep expertise in Snowflake to lead efforts in optimizing the performance of our data warehouse to enable faster, more reliable reporting. You will be responsible for improving query efficiency, data pipeline performance, and overall reporting speed by tuning Snowflake environments, optimizing data models, and collaborating with Application development teams.\n\nRoles and Responsibilities",,,,"['hive', 'analytical', 'report generation', 'bi', 'data warehousing', 'data pipeline', 'sql', 'plsql', 'star schema', 'optimization', 'spark', 'data ingestion', 'hadoop', 'etl', 'big data', 'reporting', 'snow flake schema', 'snowflake', 'python', 'sql queries', 'performance tuning', 'elt', 'data engineering', 'kafka', 'clustering', 'informatica']",2025-06-11 05:57:01
Data Engineer,Bebo Technologies,4 - 9 years,Not Disclosed,['Chandigarh'],"Design, build, and maintain scalable and reliable data pipelines on Databricks, Snowflake, or equivalent cloud platforms.\nIngest and process structured, semi-structured, and unstructured data from a variety of sources including APIs, RDBMS, and file systems.\nPerform data wrangling, cleansing, transformation, and enrichment using PySpark, Pandas, NumPy, or similar libraries.\nOptimize and manage large-scale data workflows for performance, scalability, and cost-efficiency.\nWrite and optimize complex SQL queries for transformation, extraction, and reporting.\nDesign and implement efficient data models and database schemas with appropriate partitioning and indexing strategies for Data Warehouse or Data Mart.\nLeverage cloud services (e.g., AWS S3, Glue, Kinesis, Lambda) for storage, processing, and orchestration.\nUse orchestration tools like Airflow, Temporal, or AWS Step Functions to manage end-to-end workflows.\nBuild containerized solutions using Docker and manage deployment pipelines via CI/CD tools such as Azure DevOps, GitHub Actions, or Jenkins.\nCollaborate closely with data scientists, analysts, and business stakeholders to understand requirements and deliver data solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'python', 'Snowflake', 'Data Bricks', 'sql']",2025-06-11 05:57:03
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-11 05:57:04
Data Engineer,7dxperts,5 - 8 years,15-20 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n3+ years of experience in Spark, Databricks, Hadoop, Data and ML Engineering.\n3+ Years on experience in designing architectures using AWS cloud services & Databricks.\nArchitecture, design and build Big Data Platform (Data Lake / Data Warehouse / Lake house) using Databricks services and integrating with wider AWS cloud services.\nKnowledge & experience in infrastructure as code and CI/CD pipeline to build and deploy data platform tech stack and solution.\nHands-on spark experience in supporting and developing Data Engineering (ETL/ELT) and Machine learning (ML) solutions using Python, Spark, Scala or R languages.\nDistributed system fundamentals and optimising Spark distributed computing.\nExperience in setting up batch and streams data pipeline using Databricks DLT, jobs and streams.\nUnderstand the concepts and principles of data modelling, Database, tables and can produce, maintain, and update relevant data models across multiple subject areas.\nDesign, build and test medium to complex or large-scale data pipelines (ETL/ELT) based on feeds from multiple systems using a range of different storage technologies and/or access methods, implement data quality validation and to create repeatable and reusable pipelines\nExperience in designing metadata repositories, understanding range of metadata tools and technologies to implement metadata repositories and working with metadata.\nUnderstand the concepts of build automation, implementing automation pipelines to build, test and deploy changes to higher environments.\nDefine and execute test cases, scripts and understand the role of testing and how it works.\n\nPreferred candidate profile\nBig Data technologies Databricks, Spark, Hadoop, EMR or Hortonworks.\nSolid hands-on experience in programming languages Python, Spark, SQL, Spark SQL, Spark Streaming, Hive and Presto\nExperience in different Databricks components and API like notebooks, jobs, DLT, interactive and jobs cluster, SQL warehouse, policies, secrets, dbfs, Hive Metastore, Glue Metastore, Unity Catalog and ML Flow.\nKnowledge and experience in AWS Lambda, VPC, S3, EC2, API Gateway, IAM users, roles & policies, Cognito, Application Load Balancer, Glue, Redshift, Spectrum, Athena and Kinesis.\nExperience in using source control tools like git, bit bucket or AWS code commit and automation tools like Jenkins, AWS Code build and Code deploy.\nHands-on experience in terraform and Databricks API to automate infrastructure stack.\nExperience in implementing CI/CD pipeline and ML Ops pipeline using Git, Git actions or Jenkins.\nExperience in delivering project artifacts like design documents, test cases, traceability matrix and low-level design documents.\nBuild references architectures, how-tos, and demo applications for customers.\nReady to complete certifications",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Bricks', 'Python', 'ML', 'ML Engineering', 'Pyspark', 'MLops', 'Ci Cd Pipeline', 'GIT', 'Machine Learning', 'SQL']",2025-06-11 05:57:06
"Senior Data Engineer (Exp into Azure Databricks,Pyspark, SQL)",Adecco India,7 - 12 years,22.5-30 Lacs P.A.,"['Pune', 'Bengaluru']","Job Role & responsibilities:-\n\nUnderstanding operational needs by collaborating with specialized teams\nSupporting key business operations. This involves architecture designing, building and deploying data systems, pipelines etc\nDesigning and implementing agile, scalable, and cost efficiency solution on cloud data services.\nLead a team of developers, implement Sprint planning and executions to ensure timely deliveries\n\nTechnical Skill, Qualification & experience required:-\n\n7-10 years of experience in Azure Cloud Data Engineering, Azure Databricks, datafactory , Pyspark, SQL,Python\nHands on experience in Data Engineer, Azure Databricks, Data factory, Pyspark, SQL\nProficient in Cloud Services Azure\nArchitect and implement ETL and data movement solutions.\nMigrate data from traditional database systems to Cloud environment\nStrong hands-on experience for working with Streaming dataset\nBuilding Complex Notebook in Databricks to achieve business Transformations.\nHands-on Expertise in Data Refinement using Pyspark and Spark SQL\nFamiliarity with building dataset using Scala.\nFamiliarity with tools such as Jira and GitHub\nExperience leading agile scrum, sprint planning and review sessions\nGood communication and interpersonal skills\nComfortable working in a multidisciplinary team within a fast-paced environment\n\n* Immediate Joiners will be preferred only",Industry Type: Insurance,Department: Other,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineer', 'Azure data engineer', 'Data Bricks', 'SQL', 'Data Engineering', 'Python']",2025-06-11 05:57:08
Data Engineer,Talent Aspire,2 - 7 years,Not Disclosed,"['Chandigarh', 'Bengaluru', 'Remote']","As the Data Engineer, you will play a pivotal role in shaping our data infrastructure and\nexecuting against our strategy. You will ideate alongside engineering, data and our clients to\ndeploy data products with an innovative and meaningful impact to clients. You will design, build,\nand maintain scalable data pipelines and workflows on AWS. Additionally, your expertise in AI\nand machine learning will enhance our ability to deliver smarter, more predictive solutions.\nKey Responsibilities\nCollaborate with other engineers, customers to brainstorm and develop impactful data\nproducts tailored to our clients.\nLeverage AI and machine learning techniques to integrate intelligent features into our\nofferings.\nDevelop, and optimize end-to-end data pipelines on AWS\nFollow best practices in software architecture and development.\nImplement effective cost management and performance optimization strategies.\nDevelop and maintain systems using Python, SQL, PySpark, and Django for front-end\ndevelopment.\nWork directly with clients and end-users and address their data needs\nUtilize databases and tools including and not limited to, Postgres, Redshift, Airflow, and\nMongoDB to support our data ecosystem.\nLeverage AI frameworks and libraries to integrate advanced analytics into our solutions.\nQualifications\n\nExperience:\nMinimum of 3 years of experience in data engineering, software development, or\nrelated roles.\nProven track record in designing and deploying AWS cloud infrastructure\nsolutions\nAt least 2 years in data analysis and mining techniques to aid in descriptive and\ndiagnostic insights\nExtensive hands-on experience with Postgres, Redshift, Airflow, MongoDB, and\nreal-time data workflows.\n\nTechnical Skills:\nExpertise in Python, SQL, and PySpark\nStrong background in software architecture and scalable development practices.\nTableau, Metabase or similar viz tools experience\nWorking knowledge of AI frameworks and libraries is a plus.\nLeadership & Communication:\nDemonstrates ownership and accountability for delivery with a strong\ncommitment to quality.\nExcellent communication skills with a history of effective client and end-user\nengagement.\nStartup & Fintech Mindset:\nAdaptability and agility to thrive in a fast-paced, early-stage startup environment.\nPassion for fintech innovation and a strong desire to make a meaningful impact\non the future of finance.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'and PySpark', 'Django', 'AI frameworks', 'Python', 'SQL']",2025-06-11 05:57:09
"Tech Lead- Data Engineer (SQL, Data Lake, Azure Data Factory)",Global Technology Company @ Pune,6 - 10 years,25-30 Lacs P.A.,"['Pune', 'Bengaluru', 'Mumbai (All Areas)']","Minimum of 6 yrs of Data Engineering Exp\nMust be an expert in SQL, Data Lake, Azure Data Factory, Azure Synapse, ETL, Databricks\nMust be an expert in data modeling, writing complex queries in SQL\nAbility to convert SQL code to PySpark\n\nRequired Candidate profile\nExp with SQL, Python, data modelling, data warehousing & dimensional modelling concept\nFamiliarity with data governance, data security & Production deployments using Azure DevOps CICD pipelines.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Synapse', 'Pyspark', 'Azure Data factory', 'Data Lake', 'Azure Services', 'Databricks', 'ETL', 'SQL']",2025-06-11 05:57:11
Data Engineer,Wissen Infotech,5 - 10 years,8-18 Lacs P.A.,"['Bengaluru', 'Mumbai (All Areas)']","Key Responsibilities\n• Design, develop, and optimize data pipelines using Python and AWS services such as Glue, Lambda, S3, EMR, Redshift, Athena, and Kinesis.\n• Implement ETL/ELT processes to extract, transform, and load data from various sources into centralized repositories (e.g., data lakes or data warehouses).\n• Collaborate with cross-functional teams to understand business requirements and translate them into scalable data solutions.\n• Monitor, troubleshoot, and enhance data workflows for performance and cost optimization.\n• Ensure data quality and consistency by implementing validation and governance practices.\n• Work on data security best practices in compliance with organizational policies and regulations.\n• Automate repetitive data engineering tasks using Python scripts and frameworks.\n• Leverage CI/CD pipelines for deployment of data workflows on AWS.\n\nRequired Skills and Qualifications\n\n• Professional Experience: 5+ years of experience in data engineering or a related field.\n• Programming: Strong proficiency in Python, with experience in libraries like pandas, pySpark, or boto3.\n• AWS Expertise: Hands-on experience with core AWS services for data engineering, such as AWS Glue for ETL/ELT, S3 for storage.\n• Redshift or Athena for data warehousing and querying.\n• Lambda for serverless compute.\n• Kinesis or SNS/SQS for data streaming.\n• IAM Roles for security. • Databases: Proficiency in SQL and experience with relational (e.g., PostgreSQL, MySQL) and NoSQL (e.g., DynamoDB) databases. • Data Processing: Knowledge of big data frameworks (e.g., Hadoop, Spark) is a plus. • DevOps: Familiarity with CI/CD pipelines and tools like Jenkins, Git, and CodePipeline. • Version Control: Proficient with Git-based workflows. • Problem Solving: Excellent analytical and debugging skills. Optional Skills • Knowledge of data modeling and data warehouse design principles. • Experience with data visualization tools (e.g., Tableau, Power BI). • Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'ETL', 'AWS', 'Python', 'SQL', 'Snowflake', 'Hadoop', 'SCALA', 'Big Data', 'Spark', 'Aws Glue']",2025-06-11 05:57:13
"Data Engineer - Hadoop, Spark, Python, Data bricks",Damco Solutions,4 - 9 years,Not Disclosed,['Coimbatore'],"Position Name: Data Engineer\nLocation: Coimbatore (Hybrid 3 days per week)\nWork Shift Timing: 1.30 pm to 10.30 pm (IST)\nMandatory Skills: Hadoop, Spark, Python, Data bricks\nGood to have: Java/Scala\n\nThe Role:\n• Designing and building optimized data pipelines using cutting-edge technologies in a cloud environment to drive analytical insights.\n• Constructing infrastructure for efficient ETL processes from various sources and storage systems.\n• Leading the implementation of algorithms and prototypes to transform raw data into useful information.\n• Architecting, designing, and maintaining database pipeline architectures, ensuring readiness for AI/ML transformations.\n• Creating innovative data validation methods and data analysis tools.\n• Ensuring compliance with data governance and security policies.\n• Interpreting data trends and patterns to establish operational alerts.\n• Developing analytical tools, programs, and reporting mechanisms.\n• Conducting complex data analysis and presenting results effectively.\n• Preparing data for prescriptive and predictive modeling.\n• Continuously exploring opportunities to enhance data quality and reliability.\n• Applying strong programming and problem-solving skills to develop scalable solutions.\n\nRequirements:\n• Experience in the Big Data technologies (Hadoop, Spark, Nifi, Impala).\n• Hands-on experience designing, building, deploying, testing, maintaining, monitoring, and owning scalable, resilient, and distributed data pipelines.\n• High proficiency in Scala/Java and Spark for applied large-scale data processing\n• Expertise with big data technologies, including Spark, Data Lake, and Hive.\n• Solid understanding of batch and streaming data processing techniques.\n• Proficient knowledge of the Data Lifecycle Management process, including data collection, access, use, storage, transfer, and deletion.\n• Expert-level ability to write complex, optimized SQL queries across extensive data volumes.\n• Experience on HDFS, Nifi, Kafka.\n• Experience on Apache Ozone, Delta Tables, Databricks, Axon(Kafka), Spring Batch, Oracle DB\n• Familiarity with Agile methodologies.\n• Obsession for service observability, instrumentation, monitoring, and alerting.\n• Knowledge or experience in architectural best practices for building data lakes\n\n\nInterested candidates can share their resume at Neesha1@damcogroup.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'Hadoop', 'Data bricks', 'Spark', 'Hdfs', 'Impala', 'Apache Nifi', 'date lake', 'Hive', 'java', 'SCALA', 'Big data', 'oracle DB']",2025-06-11 05:57:14
Data Engineer - SAS Migration,Crisil,2 - 4 years,Not Disclosed,['Mumbai'],"The SAS to Databricks Migration Developer will be responsible for migrating existing SAS code, data processes, and workflows to the Databricks platform. This role requires expertise in both SAS and Databricks, with a focus on converting SAS logic into scalable PySpark and Python code. The developer will design, implement, and optimize data pipelines, ensuring seamless integration and functionality within the Databricks environment. Collaboration with various teams is essential to understand data requirements and deliver solutions that meet business needs",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'pyspark', 'data warehousing', 'data migration', 'sql', 'spark', 'gcp', 'mysql', 'hadoop', 'bigquery', 'big data', 'etl', 'python', 'sas', 'teradata', 'airflow', 'microsoft azure', 'data engineering', 'sql server', 'dataproc', 'data bricks', 'cloud data flow', 'kafka', 'migration', 'sqoop', 'data flow']",2025-06-11 05:57:16
Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location),Allegis Group,5 - 10 years,Not Disclosed,[],"Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location)\nJob Location : Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\n\n5 -20 years of relevant hands on development experience. And 4+ years as Azure Data Engineering role\nProficient in Azure technologies like ADB, ADF, SQL(capability of writing complex SQL queries), ADB, PySpark, Python, Synapse, Delta Tables, Unity Catalog\nHands on in Python, PySpark or Spark SQL\nHands on in Azure Analytics and DevOps\nTaking part in Proof of Concepts (POCs) and pilot solutions preparation\nAbility to conduct data profiling, cataloguing, and mapping for technical design and construction of technical data flows\nExperience in business processing mapping of data and analytics solutions",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Analytics', 'Azure Data Engineering', 'Azure Databricks', 'Devops', 'Python', 'Azure Data Factory', 'Pyspark', 'Azure', 'Adb']",2025-06-11 05:57:17
Data Engineer,Kanini Software Solutions,12 - 20 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","We are looking for a skilled Data Engineer to join our growing data team. The ideal candidate will be responsible for designing, building, and maintaining data pipelines and infrastructure to support data analytics and business intelligence needs. A strong foundation in cloud data platforms, data transformation tools, and programming is essential.\nKey Responsibilities:\nDesign and implement scalable data pipelines using Azure Data Lake and dbt.\nIngest and transform data from various sources including databases, APIs, flat files, JSON, and XML.",,,,"['Pyspark', 'Azure', 'Snowflake']",2025-06-11 05:57:19
Data Engineer - Associate,FedEx,1 - 2 years,Not Disclosed,"['Gurugram', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities :\n\nDevelop and maintain data workflows using Ab Initio tools.\nAnalyze data, troubleshoot issues, and resolve defects within data pipelines.\nParticipate in Agile ceremonies including daily stand-ups, sprint planning, and reviews.\nApply CI/CD practices using tools like Jenkins and work within Unix/Linux environments.\nMaintain documentation including metadata definitions, onboarding materials, and SOPs.\nContribute to modernization and cloud-readiness efforts, particularly leveraging Azure and Databricks.\n\n\n\nPreferred candidate profile\n\n1-2 years of hands-on experience in data engineering, ETL development, or data analytics.\nExperience with SQL and working knowledge of Unix/Linux systems.\nFamiliarity with ETL tools such as Ab Initio.\nAbility to analyze, debug, and optimize data workflows.\nExposure to CI/CD pipelines and version control practices.\nStrong analytical thinking, attention to detail, and documentation skills.\nComfortable working in Agile development environments",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Ab Initio', 'ETL', 'Unix', 'Linux', 'Informatica', 'Data Modeling', 'Data Warehousing', 'SQL']",2025-06-11 05:57:20
Field Engineer,Genpact,0 - 5 years,Not Disclosed,['Bengaluru'],"Genpact (NYSE: G) is a global professional services and solutions firm delivering outcomes that shape the future. Our 125,000+ people across 30+ countries are driven by our innate curiosity, entrepreneurial agility, and desire to create lasting value for clients. Powered by our purpose the relentless pursuit of a world that works better for people – we serve and transform leading enterprises, including the Fortune Global 500, with our deep business and industry knowledge, digital operations services, and expertise in data, technology, and AI.\nInviting applications for the role of Field Data Engineer!\nIn this role - The team developing industry leading X-ray generation subsystems (ie. tubes and generators) is looking for highly motivated engineers to join them in the efforts to monitor and maintain the large installed base of the imaging scanners that use these X-ray generation subsystem in the field.",,,,"['Field Engineering', 'Field Support']",2025-06-11 05:57:22
Data Science - Director job opening at GlobalData(Hyderabad),Globaldata,15 - 20 years,Not Disclosed,['Hyderabad( Kondapur )'],"R\nHello,\n\nUrgent job openings for Data Science - Director @ GlobalData(Hyderebad)\n\nJob Description given below please go through to understand the requirement.\n\nif requirement is matching to your profile & interested to apply please share your updated resume @ mail id (m.salim@globaldata.com).",,,,"['Data Science', 'Pytorch', 'Generative Ai Tools', 'Large Language Model', 'Python', 'Tensorflow', 'Natural Language Processing', 'Deep Learning']",2025-06-11 05:57:23
Data Visualization Expert - Quick sight,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Chennai'],"We are currently seeking a Data Visualization Expert - Quick sight to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\n\n\n What awaits you/ Job Profile  \n\n   \nDesign and develop data visualizations using Amazon QuickSight to present complex data in clear and understandable Dashboards.\nCreate interactive dashboards and reports that allow end-users to explore data and draw meaningful conclusions.\nWork on Data preparation and ensure the good quality data is used in Visualization.\nCollaborate with data analysts and business stakeholders to understand data requirements, gather insights, and transform raw data into actionable visualizations.\nEnsure that the data visualizations are user-friendly, intuitive, and aesthetically pleasing. Optimize the user experience by incorporating best practices.\nIdentify and address performance bottlenecks in data queries and visualization.\nEnsure compliance with data security policies and governance guidelines when handling sensitive data within QuickSight.\nProvide training and support to end-users and stakeholders on how to interact with Dashboards.\nSelf-Managing and explore the latest technical development and incorporate in the project.\nExperience in analytics, reporting and business intelligence tools.\nUsing the Agile Methodology, attending daily standups and use of the Agile tools.\nLead Technical discussions with customers to find the best possible solutions.\n\n What should you bring along  \n\n   \n\n Must Have  \nOverall experience of 2-5 years in Data visualization development.\nMinimum of 2 years in QuickSight and 1-2 years in other BI Tools like Tableau, PowerBI, Qlik\nGood In writing complex SQL Scripting, Dataset Modeling.\nHands on in AWS -Athena, RDS, S3, IAM, permissions, Logging and monitoring Services.\nExperience working with various data sources and databases like Oracle, mySQL, S3, Athena.\nAbility to work with large datasets and design efficient data models for visualization.\nPrior experience in working in Agile, Scrum/Kanban working model.\n\n Nice to Have  \nKnowledge on Data ingestion and Data pipeline in AWS.\nKnowledge Amazon Q or AWS LLM Service to enable AI integration\n\n Must have skill  \n\nQuick sight, Tableau, SQL , AWS\n\n Good to have skills  \n\nQlikview ,Data Engineer, AWS LLM\n\n\n\n\n\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'data visualization', 'quicksight', 'tableau', 'aws', 'hive', 'bi', 'data warehousing', 'dashboards', 'bi tools', 'iam', 'spark', 'kanban', 'ssrs', 'mysql', 'hadoop', 'etl', 'python', 'oracle', 'data analysis', 'power bi', 'amazon rds', 'sql server', 'qlikview', 'sql scripting', 'scrum', 'athena', 'agile', 'ssis']",2025-06-11 05:57:25
Data Visualization Expert - Quick sight,"NTT DATA, Inc.",4 - 5 years,Not Disclosed,['Chennai'],"We are currently seeking a Data Visualization Expert - Quick sight to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\n\n\n What awaits you/ Job Profile  \n\n\n\n Location Bangalore and Chennai, Hybrid mode,Immediate to 10 Days Notice period \nDevelop reports using Amazon Quicksight\nData Visualization DevelopmentDesign and develop data visualizations using Amazon Quicksight to present complex data in a clear and understandable format. Create interactive dashboards and reports that allow end-users to explore data and draw meaningful conclusions.\nData AnalysisCollaborate with data analysts and business stakeholders to understand data requirements, gather insights, and transform raw data into actionable visualizations.\nDashboard User Interface (UI) and User Experience (UX)Ensure that the data visualizations are user-friendly, intuitive, and aesthetically pleasing. Optimize the user experience by incorporating best practices in UI/UX design.\nData IntegrationWork closely with data engineers and data architects to ensure seamless integration of data sources into Quicksight, enabling real-time and up-to-date visualizations.\nPerformance OptimizationIdentify and address performance bottlenecks in data queries and visualization rendering to ensure quick and responsive dashboards.\nData Security and GovernanceEnsure compliance with data security policies and governance guidelines when handling sensitive data within Quicksight.\nTraining and DocumentationProvide training and support to end-users and stakeholders on how to interact with and interpret visualizations effectively. Create detailed documentation of the visualization development process.\nStay Updated with Industry TrendsKeep up to date with the latest data visualization trends, technologies, and best practices to continuously enhance the quality and impact of visualizations.\nUsing the Agile Methodology, attending daily standups and use of the Agile tools\nCollaborating with cross-functional teams and stakeholders to ensure data security, privacy, and compliance with regulations.\nusing Scrum/Kanban\nProficiency in Software Development best practices - Secure coding standards, Unit testing frameworks, Code coverage, Quality gates.\nAbility to lead and deliver change in a very productive way\nLead Technical discussions with customers to find the best possible solutions.\nW orking closely with the Project Manager, Solution Architect and managing client communication (as and when required)\n\n What should you bring along  \n\n\n\nMust Have\nPerson should have relevant work experience in analytics, reporting and business intelligence tools.\n4-5 years of hands-on experience in data visualization.\nRelatively 2-year Experience developing visualization using Amazon Quicksight.\nExperience working with various data sources and databases.\nAbility to work with large datasets and design efficient data models for visualization.\n\n\n\nNice to Have\nAI Project implementation and AI methods.\n\n Must have technical skill  \n\nQuick sight , SQL , AWS\n\n Good to have Technical skills  \n\nTableau, Data Engineer\n\n\n\n",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'data visualization', 'quicksight', 'software development', 'aws', 'hive', 'amazon redshift', 'unit testing', 'data warehousing', 'dashboards', 'business intelligence', 'spark', 'kanban', 'hadoop', 'etl', 'python', 'data analysis', 'ux', 'power bi', 'sql server', 'tableau', 'scrum', 'athena', 'agile', 'ssis']",2025-06-11 05:57:27
Data Science Manager job opening at GlobalData(Hyd),Globaldata,10 - 15 years,Not Disclosed,['Hyderabad( Kondapur )'],"Hello,\n\nUrgent job openings for Data Science Manager role @ GlobalData(Hyd).\n\nJob Description given below please go through to understand the requirement.\nif requirement is matching to your profile & interested to apply please share your updated resume @ mail id (m.salim@globaldata.com).",,,,"['Artificial Intelligence', 'Natural Language Processing', 'Machine Learning', 'Deep Learning', 'Data Science', 'Tensorflow', 'Predictive Modeling', 'Azure', 'Power Bi', 'Tableau', 'NLP', 'Hadoop Spark', 'AWS', 'Python']",2025-06-11 05:57:29
Data Analyst,Talent Hire It Solutions,5 - 10 years,Not Disclosed,"['Kochi', 'Thiruvananthapuram']","Collaborate with business stakeholders to understand data needs and translate them into analytical\nrequirements.\nAnalyze large datasets to uncover trends, patterns, and actionable insights.\nDesign and build dashboards and reports using Power BI.\nPerform ad-hoc analysis and develop data-driven narratives to support decision-making.\nEnsure data accuracy, consistency, and integrity through data validation and quality checks.\nBuild and maintain SQL queries, views, and data models for reporting purposes.\nCommunicate findings clearly through presentations, visualizations, and written summaries.\nPartner with data engineers and architects to improve data pipelines and architecture.\nContribute to the definition of KPIs, metrics, and data governance standards.\n\nJob Specification / Skills and Competencies\nBachelors or Master’s degree in Statistics, Mathematics, Computer Science,\nEconomics, or a related field.\n5+ years of experience in a data analyst or business intelligence role.\nAdvanced proficiency in SQL and experience working with relational databases (e.g.,\nSQL Server, Redshift, Snowflake).\n\nJob Description\n\n2\n\nHands-on experience in Power BI.\nProficiency in Python, Excel and data storytelling.\nUnderstanding of data modelling, ETL concepts, and basic data architecture.\nStrong analytical thinking and problem-solving skills.\nExcellent communication and stakeholder management skills\nTo adhere to the Information Security Management policies and procedures.\n\nSoft Skills Required\nMust be a good team player with good communication skills\nMust have good presentation skills\nMust be a pro-active problem solver and a leader by self\nManage & nurture a team of data engineersRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SQL', 'Power BI', 'Amazon Athena', 'Python']",2025-06-11 05:57:30
Senior Data Engineer,Egon Zehnder,3 - 8 years,Not Disclosed,"['Noida', 'New Delhi', 'Gurugram']","Role & responsibilities\n\nAs a Data Engineer, you will be responsible for establishing and optimizing the flow of data throughout the organization while ensuring its security.\nExecuting end-to-end data pipeline, from designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nThe Data Engineer is expected to help in driving the database architecture and design for Egon Zehnder large-scale, Intranet and Internet-based applications.\nThe Data Engineer should research new tools and technologies and come up with recommendations on how they can be used in Egon Zehnder applications.\nThe Data Engineer will also be expected to actively participate in design and implementation of projects bearing a high degree of technical complexity and/or scalability and performance significance.\nIdentifying data sources, both internal and external, and working out a plan for data management that is aligned with organizational data strategy.\nCollaborate cross-functionally with roles such as IT infrastructure, Digital/application development, and Legal to identify and highlight gaps and risks around Cybersecurity and Data Protection such as GDPR.\n\nExperience & Key Competencies\nEngineering Degree or equivalent.\n3+ years of experience of SQL writing and data modelling skills, with a solid understanding of data technologies including RDBMS, No-SQL databases.\nExperience with one or more of the following databases: SQL Server, MySQL, PostgreSQL, Oracle, Couch-base, Redis, Elastic Search or other NoSQL technologies.\nProven experience in building ETL/ELT pipelines, preferably from SQL to Azure services\nArchitecture experience of making ETL operations such as Medallion Architecture etc.\nFamiliarity with CI/CD practices for data pipelines and version control using Git\nExperience in integrating new/replacing vendor products in existing ecosystem.\nExperience in migrating data structured, unstructured data to Data lake\nExperience or understanding of performance engineering both at system level and database level.\nExperience or understanding of Data Governance, Data Quality, Data Issue Management.\nWork closely with the implementation teams of the various product lines to ensure that data architectural standards and best practices are being followed consistently across all Egon Zehnder applications\nEnsure compliance with all regulations, policies, and procedures.\nEscalate issues/risks pro-actively to appropriate stakeholders.\nRegularly communicate status and challenges to team members and management.\nSelf-driven with keenness to master, suggest and work with different technologies & toolsets.\nExcellent communication skills and interpersonal skills suitable for a diverse audience with ability to communicate in a positive friendly and effective manner with technical or non-technical users/customers\nExcellent and resourceful problem-solving skills, adaptable and willingness to learn.\nGood analysis skills - to be able to join the dots across multiple applications and interfaces between them.\n\n\nPreferred candidate profile\n\nExperience with one or more of the following databases: SQL Server, MySQL, PostgreSQL, Oracle, Couch-base, Redis, Elastic Search or other NoSQL technologies.\nExperience in Data factory and Data Lake technologies.\nRich Experience in data modelling techniques and creating various data models.\nExperience in Azure cloud services and architecture patterns.\nUnderstanding of RESTful APIs for data distribution.\nUnderstanding of deployment architecture and infrastructure in both on-prem and cloud hosting environments\nExcellent oral and written communication with an ability to articulate complex systems to multiple teams.\nSelf-motivation and the ability to work under minimal supervision\n\nBenefits\nBenefits which make us unique\nAt EZ, we know that great people are what makes a great firm. We value our people and offer employees a comprehensive benefits package. Learn more about what working at Egon Zehnder can mean for you!\nBenefits Highlights:\n•       5 Days working in a Fast-paced work environment\n•       Work directly with the senior management team\n•       Reward and Recognition\n•       Employee friendly policies\n•       Personal development and training\n•       Health Benefits, Accident Insurance\nPotential Growth for you!\nWe will nurture your talent in an inclusive culture that values diversity. You will be doing regular catchups with your Manager who will act as your career coach and guide you in your career goals and aspirations.\nLocation\nThe position is based at Egon Zehnders KCI office in Gurgaon, Plot no. 29, Institutional Area Sector 32.\nEZIRS Commitment to Diversity & Inclusion\nEgon Zehnder Information Research & Services (EZIRS) aims for a diverse workplace and strive to continuously lead with our firm values. We respect personal values of every individual irrespective of race, national or social origin, gender, religion, political or other opinion, disability, age and sexual orientation as warranted by basic rights enshrined in the UN Declaration of Human Rights. We believe diversity of our firm is central to the success and enables us to deliver better solutions for our clients. We are committed to creating an inclusive environment and supportive work environment, where everyone feels comfortable to be themselves and treated with dignity and respect and there is no unlawful discrimination related to employment, recruitment, training, promotion or remuneration.\nEgon Zehnder is an Equal Opportunity Employer\nEgon Zehnder provides equal employment opportunities to all applicants and employees without regard to race, color, creed, religion, sex, sexual orientation, gender identity, marital status, citizenship status, age, national origin, disability, or any other legally protected status and to affirmatively seek to advance the principles of equal employment opportunity.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL Server', 'PostgreSQL', 'MySQL', 'Couch-base', 'Redis', 'Oracle', 'Elastic Search or other NoSQL technologies']",2025-06-11 05:57:32
Snowflake Data Engineer / Database Lead,TechStar Group,9 - 14 years,15-20 Lacs P.A.,['Hyderabad'],"Job Description:\nSQL & Database Management: Deep knowledge of relational databases (PostgreSQL), cloud-hosted data platforms (AWS, Azure, GCP), and data warehouses like Snowflake.\nETL/ELT Tools: Experience with SnapLogic, StreamSets, or DBT for building and maintaining data pipelines. / ETL Tools Extensive Experience on data Pipelines\nData Modeling & Optimization: Strong understanding of data modeling, OLAP systems, query optimization, and performance tuning.\nCloud & Security: Familiarity with cloud platforms and SQL security techniques (e.g., data encryption, TDE).\nData Warehousing: Experience managing large datasets, data marts, and optimizing databases for performance.\nAgile & CI/CD: Knowledge of Agile methodologies and CI/CD automation tools.\n\n\nRole & responsibilities\nBuild the data pipeline for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud database technologies.\nWork with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data needs.\nWork with data and analytics experts to strive for greater functionality in our data systems.\nAssemble large, complex data sets that meet functional / non-functional business requirements.\n– Ability to quickly analyze existing SQL code and make improvements to enhance performance, take advantage of new SQL features, close security gaps, and increase robustness and maintainability of the code.\n– Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery for greater scalability, etc.\n– Unit Test databases and perform bug fixes.\n– Develop best practices for database design and development activities.\n– Take on technical leadership responsibilities of database projects across various scrum teams.\nManage exploratory data analysis to support dashboard development (desirable)\n\n\nRequired Skills:\n– Strong experience in SQL with expertise in relational database(PostgreSQL preferrable cloud hosted in AWS/Azure/GCP) or any cloud-based Data Warehouse (like Snowflake, Azure Synapse).\n– Competence in data preparation and/or ETL/ELT tools like SnapLogic, StreamSets, DBT, etc. (preferably strong working experience in one or more) to build and maintain complex data pipelines and flows to handle large volume of data.\n– Understanding of data modelling techniques and working knowledge with OLAP systems\n– Deep knowledge of databases, data marts, data warehouse enterprise systems and handling of large datasets.\n– In-depth knowledge of ingestion techniques, data cleaning, de-dupe, etc.\n– Ability to fine tune report generating queries.\n– Solid understanding of normalization and denormalization of data, database exception handling, profiling queries, performance counters, debugging, database & query optimization techniques.\n– Understanding of index design and performance-tuning techniques\n– Familiarity with SQL security techniques such as data encryption at the column level, Transparent Data Encryption(TDE), signed stored procedures, and assignment of user permissions\n– Experience in understanding the source data from various platforms and mapping them into Entity Relationship Models (ER) for data integration and reporting(desirable).\n– Adhere to standards for all database e.g., Data Models, Data Architecture and Naming Conventions\n– Exposure to Source control like GIT, Azure DevOps\n– Understanding of Agile methodologies (Scrum, Itanban)\n– experience with NoSQL database to migrate data into other type of databases with real time replication (desirable).\n– Experience with CI/CD automation tools (desirable)\n– Programming language experience in Golang, Python, any programming language, Visualization tools (Power\nBI/Tableau) (desirable).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'Snowflake Developer', 'SQL', 'Snowflake Sql', 'Snowflake Data Engineer', 'Snowsql', 'Snowflake', 'Schema', 'Data Engineer', 'Data Sharing', 'Snowpipe', 'Streams']",2025-06-11 05:57:34
Senior Data Engineer,Atidiv,5 - 8 years,10-17 Lacs P.A.,[],"Were Hiring! | Senior Data Engineer (Remote)\n\nLocation: Remote |\nShift: US - CST Time |\nDepartment: Data Engineering\n\nAre you a data powerhouse who thrives on solving complex data challenges? Do you love working with Python, AWS, and cutting-edge data tools? If yes, Atidiv wants YOU!",,,,"['SQL', 'Snowflake', 'Python', 'Airflow', 'Pyspark', 'Kafka', 'Lamda', 'Ci/Cd', 'EMR', 'Aws Glue', 'Lambda Aws', 'Cd Tools', 'Glue', 'Kinesis', 'Redshift Aws', 'DBT', 'aws']",2025-06-11 05:57:35
Senior Data Engineer,A leading Bank of India,8 - 13 years,Not Disclosed,['Mumbai (All Areas)'],"Role & responsibilities:\nDesign, optimize, and manage complex SQL queries across large Oracle databases.\nBuild and maintain metadata layers (table schema, DDL, column descriptions, relationships).\nDevelop and fine-tune solutions for converting natural language to SQL using various python libraries or other solutions.\nBuild and integrate chatbot interfaces using Python (Streamlit, FastAPI, Flask). Preferred is Flask.\nHave handled feature building for creating dynamic query agents.\nImplement user query handling, ambiguity detection, and feedback loops.\nDesign and test proof-of-concept (PoC) solutions to solve business queries using Python.\nBuild modular components (e.g., SQL generators, data sanitizers, security layers).\nCollaborate with business and business analysts to convert ideas into working prototypes.\nImplement query safety checks to prevent injection and unauthorized data access.\nMaintain logs and audit trails for executed queries and system usage.\n\nPreferred candidate profile:\n\nStrong experience in data engineering, analytics, or backend systems.\nExpert-level SQL skills (especially Oracle SQL).\nStrong Python programming skills, including libraries like pandas, sqlalchemy,strea, flask, etc.\nExperience with one or more NLP libraries: OpenAI GPT (function calling), LangChain, HuggingFace Transformers.\nKnowledge of database metadata modeling and handling DDL for large-scale systems.\nExperience with REST APIs and building microservices in Python.\nExposure to modern LLM tools like DSPy, sqlcoder, Text2SQL, or LangChain SQL Agent.\nFamiliarity with RDBMS performance tuning and optimization strategies.\nKnowledge of visualization tools like Plotly, Dash, or Streamlit for result rendering.\nUnderstanding of RBAC and secure data access practices.",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Banking domain', 'Natural Language Processing', 'Large Language Model', 'Chatbot Development', 'Python', 'Tensorflow', 'Oracle SQL', 'Artificial Intelligence', 'FASTapi', 'Machine Learning', 'Deep Learning', 'Numpy', 'Scikit-Learn', 'Pytorch', 'Django', 'Pandas', 'Aiml', 'Flask']",2025-06-11 05:57:37
Data & AI Technical Solution ArchitectsData & AI Technical Solution,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Pune'],"Title : Data & AI Technical Solution ArchitectsData & AI Technical Solution Architects\n\nReq ID: 323749\n\nWe are currently seeking a Data & AI Technical Solution ArchitectsData & AI Technical Solution Architects to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'needs assessment', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'gcp', 'gcp cloud', 'ml']",2025-06-11 05:57:39
Data Analyst -Python,Sopra Steria,3 - 5 years,Not Disclosed,['Chennai'],"Experience working in large Software Development Teams\nKnowledge and experience in Agile Delivery mechanisms \nWork with business stakeholders, SCRUM masters, Designers and testers in SCRUM team.\nProficient in English language with ability to lead stakeholder conversations.\nExperience in generating insights through data and articulating stories addressing business problems.\nTotal Experience Expected: 6-8 years\n",,,,"['data mining', 'vlookup', 'sql', 'analytics', 'data science', 'advanced excel', 'data visualization', 'technical skills', 'python', 'macros', 'data analysis', 'data analytics', 'sas', 'insights', 'predictive analytics', 'business analysis', 'machine learning', 'excel', 'tableau', 'r', 'vba', 'predictive modeling', 'scrum', 'agile', 'statistics']",2025-06-11 05:57:40
Senior Data Engineer,Straive,6 - 10 years,Not Disclosed,['Mumbai (All Areas)'],Senior Data Engineer\nYou will have the following responsibilities:\nDesign\nAnalyse relevant internally and externally sourced data (raw data) to generate BI and Advanced\nAnalytics datasets based on your stakeholders requirements,,,,"['AWS', 'Postgresql', 'Snowflake', 'Data Warehousing', 'ETL', 'Aws Rds Oracle']",2025-06-11 05:57:42
Data & AI Technical Solution Architects,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Pune'],"Req ID: 323754\n\nWe are currently seeking a Data & AI Technical Solution Architects to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'solution design', 'gcp', 'gcp cloud', 'ml']",2025-06-11 05:57:43
Data & AI Technical Solution Architects,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Hyderabad'],"Req ID: 323774\n\nWe are currently seeking a Data & AI Technical Solution Architects to join our team in Hyderabad, Telangana (IN-TG), India (IN).\n\n""Job DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works""",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'solution design', 'gcp', 'gcp cloud', 'ml']",2025-06-11 05:57:45
Senior Data Engineer,Tredence,4 - 8 years,Not Disclosed,['Pune'],"About Tredence:\nTredence is a global data science solutions provider founded in 2013 by Shub Bhowmick, Sumit Mehra, and Shashank Dubey focused on solving the last-mile problem in AI. Headquartered in San Jose, California, the company embraces a vertical-first approach and an outcome-driven mindset to help clients win and accelerate value realization from their analytics investments. The aim is to bridge the gap between insight delivery and value realization by providing customers with a differentiated approach to data and analytics through tailor-made solutions. Tredence is 1,800-plus employees strong with offices in San Jose, Foster City, Chicago, London, Toranto, and Bangalore, with the largest companies in retail, CPG, hi-tech, telecom, healthcare, travel, and industrials as clients.",,,,"['azure databricks', 'python', 'azure data lake', 'rdbms', 'data management', 'performance tuning', 'analytical', 'data', 'pyspark', 'data warehousing', 'azure data factory', 'data engineering', 'tools', 'artificial intelligence', 'sql', 'plsql', 'unix shell scripting', 'spark', 'etl', 'communication skills', 'agile methodology']",2025-06-11 05:57:47
Senior Data Engineer,TechAffinity,6 - 10 years,Not Disclosed,['Chennai'],"We are looking for a highly skilled Senior Data Engineer with strong expertise in Data Warehousing & Analytics to join our team. The ideal candidate will have extensive experience in designing and managing data solutions, advanced SQL proficiency, and hands-on expertise in Python.\nKey Responsibilities:\nDesign, develop, and maintain scalable data warehouse solutions.\nWrite and optimise complex SQL queries for data extraction, transformation, and Reporting.\nDevelop and automate data pipelines using Python.\nWork with AWS cloud services for data storage, processing, and analytics.\nCollaborate with cross-functional teams to provide data-driven insights and solutions.\nEnsure data integrity, security, and performance optimisation.\n\nRequired Skills & Experience:\n6-10 years of experience in Data Warehousing & Analytics.\nStrong proficiency in writing complex SQL queries with deep understanding of query optimization, stored procedures, and indexing.\nHands-on experience with Python for data processing and automation.\nExperience working with AWS cloud services.\nAbility to work independently and collaborate with teams across different time zones.\nGood to Have:\nExperience in the SAS domain and understanding of financial data structures.\nHands-on experience with reporting tools like Power BI or Tableau.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power Bi', 'Tableau', 'SQL', 'Python', 'Data Warehousing Concepts', 'SAS', 'Azure Cloud', 'ETL', 'Aws Cloud Services', 'AWS', 'financial data structures.']",2025-06-11 05:57:48
Senior Data Engineer,Parkar Global Technologies,6 - 10 years,Not Disclosed,['Pune'],"About Position:\nWe are looking for a Senior Data Engineer to play a key role in building, optimizing, and maintaining our Azure-based data platform, which supports IoT data processing, analytics, and AI/ML applications. As part of our Data Platform Team, you will design and develop scalable data pipelines, implement data governance frameworks, and ensure high-performance data processing to drive digital transformation across our business.\n\nResponsibilities:\nData Pipeline Development: Design, build, and maintain high-performance, scalable ETL/ELT pipelines using Azure Data Factory, Databricks, and ADLS.\nData Platform Enhancement: Contribute to the development and optimization of our Azure-based data platform, ensuring efficiency, reliability, and security.\nIoT & High-Volume Data Processing: Work with large-scale IoT and operational datasets, optimizing data ingestion, transformation, and storage.\nData Governance & Quality: Implement data governance best practices, ensuring data integrity, consistency, and compliance.\nPerformance Optimization: Improve query performance and storage efficiency for analytics and reporting use cases.\nCollaboration: Work closely with data scientists, architects, and business teams to ensure data availability and usability.\nInnovation & Automation: Identify opportunities for automation and process improvements, leveraging modern tools and technologies.\n\nRequirement:\n6+ years of experience in data engineering with a focus on Azure cloud technologies.\nStrong expertise in Azure Data Factory, Databricks, ADLS, and Power BI.\nProficiency in SQL, Python, and Spark for data processing and transformation.\nExperience with IoT data ingestion and processing, handling high-volume, real-time data streams.\nStrong understanding of data modeling, lakehouse architectures, and medallion frameworks.\nExperience in building and optimizing scalable ETL/ELT processes.\nKnowledge of data governance, security, and compliance frameworks.\nExperience with monitoring, logging, and performance tuning of data workflows.\nStrong problem-solving and analytical skills with a platform-first mindset.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'Azure Data Lake', 'Azure Devops', 'SQL']",2025-06-11 05:57:50
Senior Web Crawler & Data Extraction Engineer,Netgraph Networking,5 - 10 years,Not Disclosed,[],"Summary\n\nTo enhance user profiling and risk assessment, we are building web crawlers to collect relevant user data from third-party sources, forums, and the dark web. We are seeking a Senior Web Crawler & Data Extraction Engineer to design and implement these data collection solutions.\n\nJob Responsibilities\n\nDesign, develop, and maintain web crawlers and scrapers to extract data from open web sources, forums, marketplaces, and the dark web.\nImplement data extraction pipelines that aggregate, clean, and structure data for fraud detection and risk profiling.\nUse Tor, VPNs, and other anonymization techniques to safely crawl the dark web while avoiding detection.\nDevelop real-time monitoring solutions for tracking fraudulent activities, data breaches, and cybercrime discussions.\nOptimize crawling speed and ensure compliance with website terms of service, ethical standards, and legal frameworks.\nIntegrate extracted data with fraud detection models, risk scoring algorithms, and cybersecurity intelligence tools.\nWork with data scientists and security analysts to develop threat intelligence dashboards from collected data.\nImplement anti-bot detection evasion techniques and handle CAPTCHAs using AI-driven solvers where necessary.\nStay updated on OSINT (Open-Source Intelligence) techniques, web scraping best practices, and cybersecurity trends.\n\nRequirements\n5+ years of experience in web crawling, data scraping, or cybersecurity data extraction.\nStrong proficiency in Python, Scrapy, Selenium, BeautifulSoup, Puppeteer, or similar frameworks.\nExperience working with Tor, proxies, and VPNs for anonymous web scraping.\nDeep understanding of HTTP protocols, web security, and bot detection mechanisms.\nExperience parsing structured and unstructured data from JSON, XML, and web pages.\nStrong knowledge of database management (SQL, NoSQL) for storing large-scale crawled data.\nFamiliarity with AI/ML-based fraud detection techniques and data classification methods.\nExperience working with cybersecurity intelligence sources, dark web monitoring, and OSINT tools.\nAbility to implement scalable, distributed web crawling architectures.\nKnowledge of data privacy regulations (GDPR, CCPA) and ethical data collection practices.\n\nNice to Have\nExperience in fintech, fraud detection, or threat intelligence.\nKnowledge of natural language processing (NLP) for analyzing cybercrime discussions.\nFamiliarity with machine learning-driven anomaly detection for fraud prevention.\nHands-on experience with cloud-based big data solutions (AWS, GCP, Azure, Elasticsearch, Kafka).",Industry Type: FinTech / Payments,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Scrapy', 'Web Crawling', 'Data Extraction', 'Web Scraping', 'Python', 'Automation', 'BeautifulSoup', 'Data Scraping', 'Selenium', 'Python Development']",2025-06-11 05:57:51
Data Analyst,Tothr,3 - 5 years,6-12 Lacs P.A.,[],"We are seeking a dynamic candidate with strong data analytical capabilities, advanced Excel proficiency, and excellent English communication skills and bring innovative ideas to the table, and contribute actively toward optimizing internal workflows.\n\nRequired Candidate profile\nExcellent Communication Skill.\nPrior experience in handling vendors or external partners Familiarity with eCommerce workflows and operations",Industry Type: Miscellaneous,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Interactive Reports', 'Vendor Management', 'Advanced Excel', 'excellent Verbal and written communication in English']",2025-06-11 05:57:53
Junior Data Engineer,Talent Corner Hr Services,1 - 5 years,7-10 Lacs P.A.,['Bengaluru( JP Nagar )'],"We are looking for a Junior Data Engineer with 1–3 years of experience, primarily focused on database management and data processing using MySQL. The candidate will support the data engineering team in maintaining reliable data pipelines",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MySQL', 'Query Optimization', 'Data Processing']",2025-06-11 05:57:54
Data Engineer,Sorice Solutions,4 - 8 years,6-12 Lacs P.A.,['Chennai'],"Location: Chennai\n\n\nRole & responsibilities\n\n\nBackend data model and data architecture, data migration, using python/java to build data ingestion pipelines and data validation processes, develop task schedulers for different data sources",Industry Type: Miscellaneous,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Migration', 'Data Modeling', 'Python', 'SQL']",2025-06-11 05:57:55
Data Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will responsible for designing, building, maintaining, analyzing, and interpreting data to provide actionable insights that drive business decisions. This role involves working with large datasets, developing reports, supporting and executing data governance initiatives, and visualizing data to ensure data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in the design and development of the data pipeline.\nCreate data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nContribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions.\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks.\nCollaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications and Experience\nMasters degree and 1 to 3 years of experience in Computer Science, IT, or related field OR\nBachelors degree and 3 to 5 years of experience in Computer Science, IT, or related field OR\nDiploma and 7 to 9 years of experience in Computer Science, IT, or related field\nMust-Have Skills:\nHands-on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing.\nProficiency in data analysis tools (e.g., SQL) and experience with data visualization tools.\nExcellent problem-solving skills and the ability to work with large, complex datasets.\nPreferred Qualifications:\nGood-to-Have Skills:\nExperience with ETL tools such as Apache Spark, and various Python packages related to data processing, machine learning model development.\nStrong understanding of data modeling, data warehousing, and data integration concepts.\nKnowledge of Python/R, Databricks, SageMaker, cloud data platforms.\nProfessional Certifications:\nCertified Data Engineer / Data Analyst (preferred on Databricks or cloud environments).\nCertified Data Scientist (preferred on Databricks or Cloud environments).\nMachine Learning Certification (preferred on Databricks or Cloud environments).\nSoft Skills:\nExcellent critical-thinking and problem-solving skills.\nStrong communication and collaboration skills.\nDemonstrated awareness of how to function in a team setting.\nDemonstrated presentation skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'SageMaker', 'R', 'data modeling', 'data warehousing', 'cloud data platforms', 'Databricks', 'ETL', 'data integration', 'Python']",2025-06-11 05:57:57
Data Engineer,Tekskills India pvt ltd,7 - 9 years,8-15 Lacs P.A.,['Hyderabad'],"Role & Responsibilities Role Overview: We are seeking a talented and forward-thinking Data Engineer for one of the large financial services GCC based in Hyderabad with responsibilities that include designing and constructing data pipelines, integrating data from multiple sources, developing scalable data solutions, optimizing data workflows, collaborating with cross-functional teams, implementing data governance practices, and ensuring data security and compliance.\n\nTechnical Requirements: • Proficiency in ETL, Batch, and Streaming Process • Experience with BigQuery, Cloud Storage, and CloudSQL • Strong programming skills in Python, SQL, and Apache Beam for data processing • Understanding of data modeling and schema design for analytics • Knowledge of data governance, security, and compliance in GCP • Familiarity with machine learning workflows and integration with GCP ML tools • Ability to optimize performance within data pipelines\n\nFunctional Requirements: • Ability to collaborate with Data Operations, Software Engineers, Data Scientists, and Business SMEs to develop Data Product Features • Experience in leading and mentoring peers within an existing development team • Strong communication skills to craft and communicate robust solutions • Proficient in working with Engineering Leads, Enterprise and Data Architects, and Business Architects to build appropriate data foundations • Willingness to work on contemporary data architecture in Public and Private Cloud environments This role offers a compelling opportunity for a seasoned Data Engineering to drive transformative cloud initiatives within the financial sector, leveraging unparalleled experience and expertise to deliver innovative cloud solutions that align with business imperatives and regulatory requirements. Qualification o Engineering Grad / Postgraduate CRITERIA o Proficient in ETL, Python, and Apache Beam for data processing efficiency. o Demonstrated expertise in BigQuery, Cloud Storage, and CloudSQL utilization. o Strong collaboration skills with cross-functional teams for data product development. o Comprehensive knowledge of data governance, security, and compliance in GCP. o Experienced in optimizing performance within data pipelines for efficiency.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'GCP', 'Apache beam', 'Bigquery', 'Cloud sql', 'Cloudstorage', 'Python']",2025-06-11 05:57:58
Consultant-Data Engineer (Only from Pharma/Lifescience/Biotech domain),Chryselys,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description for Consultant - Data Engineer\nAbout Us:\nChryselys is a Pharma Analytics & Business consulting company that delivers data-driven insights leveraging AI-powered, cloud-native platforms to achieve high-impact transformations.\nWe specialize in digital technologies and advanced data science techniques that provide strategic and operational insights.\nWho we are:\nPeople - Our team of industry veterans, advisors and senior strategists have diverse backgrounds and have worked at top tier companies.\nQuality - Our goal is to deliver the value of a big five consulting company without the big five cost.\nTechnology - Our solutions are Business centric built on cloud native technologies.\nKey Responsibilities and Core Competencies:\n•      You will be responsible for managing and delivering multiple Pharma projects.\n•      Leading a team of atleast 8 members, resolving their technical and business related problems and other queries.\n•      Responsible for client interaction; requirements gathering, creating required documents, development, quality assurance of the deliverables.\n•      Good collaboration with onshore and Senior folks.\n•      Should have fair understanding of Data Capabilities (Data Management, Data Quality, Master and Reference Data).\n•      Exposure to Project management methodologies including Agile and Waterfall.\n•      Experience working in RFPs would be a plus.\nRequired Technical Skills:\n•      Proficient in Python, Pyspark, SQL\n•      Extensive hands-on experience in big data processing and cloud technologies like AWS and Azure services, Databricks etc.\n•      Strong experience working with cloud data warehouses like Snowflake, Redshift, Azure etc.\n•      Good experience in ETL, Data Modelling, building ETL Pipelines.\n•      Conceptual knowledge of Relational database technologies, Data Lake, Lake Houses etc.\n•      Sound knowledge in Data operations, quality and data governance.\nPreferred Qualifications:\n•      Bachelors or master’s Engineering/ MCA or equivalent degree.\n•      6-13 years of experience as Data Engineer, with atleast 2 years in managing medium to large scale programs.\n•      Minimum 5 years of Pharma and Life Science domain exposure in IQVIA, Veeva, Symphony, IMS etc.\n•      High motivation, good work ethic, maturity, self-organized and personal initiative.\n•      Ability to work collaboratively and providing the support to the team.\n•      Excellent written and verbal communication skills.\n•      Strong analytical and problem-solving skills.\nLocation\n•      Preferably Hyderabad, India",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Data Bricks', 'Python', 'SQL', 'Data Engineering', 'Microsoft Azure', 'Data Lake', 'Data Warehousing']",2025-06-11 05:58:00
Azure Data Engineer,Management Consulting,5 - 10 years,6-15 Lacs P.A.,['Bengaluru'],"Urgent Hiring _ Azure Data Engineer with a leading Management Consulting Company @ Bangalore Location.\n\nStrong expertise in Databricks & Pyspark while dealing with batch processing or live (streaming) data sources.\n4+ relevant years of experience in Databricks & Pyspark/Scala\n7+ total years of experience\nGood in data modelling and designing.\n\nCtc- Hike Shall be considered on Current/Last Drawn Pay\n\nApply - rohita.robert@adecco.com\n\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Databricks', 'Airflow', 'Streaming Data', 'Elt', 'Data Bricks', 'Redshift', 'SQL', 'PyTest', 'Azure Data Engineer', 'Datafactory', 'Synapse', 'Glue', 'Stream Analytics', 'Kinesis', 'SCALA', 'SonarQube', 'Data Modeling']",2025-06-11 05:58:02
Tech. PM - Data Engineering-Data Analytics@ Gurgaon/Blore_Urgent,A global leader in delivering innovative...,5 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Title - Technical Project Manager\n\nLocation - Gurgaon/ Bangalore\n\nNature of Job - Permanent\n\nDepartment - data analytics\n\nWhat you will be doing\n\n\nDemonstrated client servicing and business analytics skills with at least 5 - 9 years of experience as data engineer, BI developer, data analyst, technical project manager, program manager etc.\nTechnical project management- drive BRD, project scope, resource allocation, team\ncoordination, stakeholder communication, UAT, Prod fix, change requests, project governance\nSound knowledge of banking industry (payments, retail operations, fraud etc.)\nStrong ETL experience or experienced Teradata developer\nManaging team of business analysts, BI developers, ETL developers to ensure that projects are completed on time\nResponsible for providing thought leadership and technical advice on business issues\nDesign methodological frameworks and solutions.\n\n\nWhat were looking for\n\n\nBachelors/masters degree in computer science/data science/AI/statistics, Certification in Gen AI. Masters degree Preferred.\nManage multiple projects, at a time, from inception to delivery\nSuperior problem-solving, analytical, and quantitative skills\nEntrepreneurial mindset, coupled with a “can do” attitude\nDemonstrated ability to collaborate with cross-functional, cross-border teams and coach / mentor colleagues.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Technical Project Manager', 'Data Engineering', 'multiple projects', 'Technical project management', 'Data Analytics', 'project scope', 'ETL Pipeline', 'team coordination', 'resource allocation', 'Prod fix', 'drive BRD', 'program manager', 'Big data']",2025-06-11 05:58:03
NLP Data Engineer - Risk Insights & Monitoring,MNC IT,10 - 12 years,25-30 Lacs P.A.,"['Pune', 'Mumbai (All Areas)']","Design and implement state-of-the-art NLP models, including but not limited to text classification, semantic search, sentiment analysis, named entity recognition, and summary generation.\nconduct data preprocessing, and feature engineering to improve model accuracy and performance.\nStay updated with the latest developments in NLP and ML, and integrate cutting-edge techniques into our solutions.\ncollaborate with Cross-Functional Teams: Work closely with data scientists, software engineers, and product managers to align NLP projects with business objectives.\ndeploy models into production environments and monitor their performance to ensure robustness and reliability.\nmaintain comprehensive documentation of processes, models, and experiments, and report findings to stakeholders.\nimplement and deliver high quality software solutions / components for the Credit Risk monitoring platform.\nleverage his/her expertise to mentor developers; review code and ensure adherence to standards.\napply a broad range of software engineering practices, from analyzing user needs and developing new features to automated testing and deployment\nensure the quality, security, reliability, and compliance of our solutions by applying our digital principles and implementing both functional and non-functional requirements\nbuild observability into our solutions, monitor production health, help to resolve incidents, and remediate the root cause of risks and issues\nunderstand, represent, and advocate for client needs\nshare knowledge and expertise with colleagues , help with hiring, and contribute regularly to our engineering culture and internal communities.\nExpertise -\nBachelor of Engineering or equivalent.\nIdeally 8-10Yrs years of experience in NLP based applications focused on Banking / Finance sector.\nPreference for experience in financial data extraction and classification.\nInterested in learning new technologies and practices, reuse strategic platforms and standards, evaluate options, and make decisions with long-term sustainability in mind.\nProficiency in programming languages such as Python & Java. Experience with frameworks like TensorFlow, PyTorch, or Keras.\nIn-depth knowledge of NLP techniques and tools, including spaCy, NLTK, and Hugging Face.\nExperience with data handling and processing tools like Pandas, NumPy, and SQL.\nPrior experience in agentic AI, LLMs ,prompt engineering and generative AI is a plus.\nBackend development and microservices using Java Spring Boot, J2EE, REST for implementing projects with high SLA of data availability and data quality.\nExperience of building cloud ready and migrating applications using Azure and understanding of the Azure Native Cloud services, software design and enterprise integration patterns.\nKnowledge of SQL and PL/SQL (Oracle) and UNIX, writing queries, packages, working with joins, partitions, looking at execution plans, and tuning queries.\nA real passion for and experience of Agile working practices, with a strong desire to work with baked in quality subject areas such as TDD, BDD, test automation and DevOps principles\nExperience in Azure development including Databricks , Azure Services , ADLS etc.\nExperience using DevOps toolsets like GitLab, Jenkins",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['data engineer', 'Natural Language Processing', 'Python', 'Java']",2025-06-11 05:58:05
Senior Big Data Engineer,Meritus Management Service,6 - 8 years,20-30 Lacs P.A.,"['Nagpur', 'Pune']","Build and maintain scalable Big Data pipelines using Hadoop, PySpark, and SQL for batch and real-time processing.\nCollaborate with cross-functional teams to transform, optimize, and secure large datasets while ensuring data quality and performance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Python', 'Big Data']",2025-06-11 05:58:06
Azure Data Engineer,Our client is a multinational professio...,7 - 12 years,Not Disclosed,['Bengaluru'],"Urgently Hiring for Senior Azure Data Engineer\n\nJob Location- Bangalore\nMinimum exp - 7yrs- 11yrs\n\nKeywords Databricks, Pyspark, SCALA, SQL, Live / Streaming data, batch processing data\n\nShare CV Mohini.sharma@adecco.com\nOR Call 9740521948\n\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills.\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions.\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution.\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nShare CV Mohini.sharma@adecco.com\nOR Call 9740521948",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'Pyspark', 'SCALA', 'SQL']",2025-06-11 05:58:08
Data Engineer 2,Uplers,3 - 8 years,Not Disclosed,['Bengaluru'],"About the Role:\nAs a Data Engineer, you will be part of the Data Engineering team with this role being inherently multi-functional, and the ideal candidate will work with Data Scientist, Analysts, Application teams across the company, as well as all other Data Engineering squads at Wayfair. We are looking for someone with a love for data, understanding requirements clearly and the ability to iterate quickly. Successful candidates will have strong engineering skills and communication and a belief that data-driven processes lead to phenomenal products.\n\nWhat you'll do:\nBuild and launch data pipelines, and data products focussed on SMART Org.\nHelping teams push the boundaries of insights, creating new product features using data, and powering machine learning models.\nBuild cross-functional relationships to understand data needs, build key metrics and standardize their usage across the organization.\nUtilize current and leading edge technologies in software engineering, big data, streaming, and cloud infrastructure\n\nWhat You'll Need:\nBachelor/Master degree in Computer Science or related technical subject area or equivalent combination of education and experience 3+ years relevant work experience in the Data Engineering field with web scale data sets.\nDemonstrated strength in data modeling, ETL development and data lake architecture.\nData Warehousing Experience with Big Data Technologies (Hadoop, Spark, Hive, Presto, Airflow etc.).\nCoding proficiency in at least one modern programming language (Python, Scala, etc)\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing and query performance tuning skills of large data sets.\nIndustry experience as a Big Data Engineer and working along cross functional teams such as Software Engineering, Analytics, Data Science with a track record of manipulating, processing, and extracting value from large datasets.\nStrong business acumen. Experience leading large-scale data warehousing and analytics projects, including using GCP technologies Big Query, Dataproc, GCS, Cloud Composer, Dataflow or related big data technologies in other cloud platforms like AWS, Azure etc.\nBe a team player and introduce/follow the best practices on the data engineering space.\nAbility to effectively communicate (both written and verbally) technical information and the results of engineering design at all levels of the organization.\n\nGood to have :\nUnderstanding of NoSQL Database exposure and Pub-Sub architecture setup.\nFamiliarity with Bl tools like Looker, Tableau, AtScale, PowerBI, or any similar tools.\n\nPS: This role is with one of our clients who is a leading name in Retail Industry.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Data Engineering', 'Cloud Platform', 'Hive', 'GCP', 'Bigquery', 'Hadoop', 'SCALA', 'Big Data Technologies', 'Etl Development', 'Spark', 'Python']",2025-06-11 05:58:09
Data Engineer,DATA ENGINEER,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Title: Data Engineer\nExperience: 5+ Years\nLocation: Hyderabad (Onsite)\nAvailability: Immediate Joiners Preferred\nJob Description:\nWe are seeking an experienced Data Engineer with a strong background in Java, Spark, and Scala to join our dynamic team in Hyderabad. The ideal candidate will be responsible for building scalable data pipelines, optimizing data processing workflows, and supporting data-driven solutions for enterprise-grade applications. This is a full-time onsite role.\nKey Responsibilities:\nDesign, develop, and maintain robust and scalable data processing pipelines.\nWork with large-scale data using distributed computing technologies like Apache Spark.\nDevelop applications and data integration workflows using Java and Scala.\nCollaborate with cross-functional teams including Data Scientists, Analysts, and Product Managers.\nEnsure data quality, integrity, and security in all data engineering solutions.\nMonitor and troubleshoot performance and data issues in production systems.\nMust-Have Skills:\nStrong hands-on experience with Java, Apache Spark, and Scala.\nProven experience working on large-scale data processing systems.\nSolid understanding of distributed systems and performance tuning.\nGood-to-Have Skills:\nExperience with Hadoop, Hive, and HDFS.\nFamiliarity with data warehousing concepts and ETL processes.\nExposure to cloud data platforms is a plus.\nDesired Candidate Profile:\n5+ years of relevant experience in data engineering or big data technologies.\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration skills.\nAbility to work independently in a fast-paced environment.\nAdditional Details:\nWork Mode: Onsite (Hyderabad)\nEmployment Type: Full-time\nNotice Period: Immediate joiners highly preferred, candidates serving notice period.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'SCALA', 'Spark', 'Hive', 'Hadoop', 'Kafka']",2025-06-11 05:58:11
Scientist - Molecular Biology,Biocon Biologics Limited,4 - 6 years,Not Disclosed,['Bengaluru'],"Molecular Biology activities like plasmid and genomic DNA preparation, PCR amplification, restriction digestion, molecular cloning, sequence confirmation, involved in vector construction for stable and transient gene expression.\nRoutine cell bank preparation of microbial and mammalian cells.\nAnalytical skills related to protein estimation and characterization like, ELISA, SDS PAGE/Western Blot, Octet and HPLC.\nStandard cell culture activity such as vial thaw, subculturing, transfection, single cell cloning, fed batch, monoclonality assurance.\nExperience in bacterial and yeast expression systems preferred.\nRoutine documentation in electronic notebook. Preparation of SOP/GM and development reports.\nIndustrial experience of 4-6 years in cell line development.",Industry Type: Pharmaceutical & Life Sciences,Department: Research & Development,"Employment Type: Full Time, Permanent","['Transfection', 'Molecular Biology', 'Molecular Cloning', 'Cloning', 'Protein Expression', 'SDS-Page', 'Recombinant DNA Technology', 'Molecular Biology Techniques', 'DNA Extraction', 'Electrophoresis', 'PCR', 'ELISA']",2025-06-11 05:58:12
"Data Engineer (Python, Kafka Stream, Pyspark, and Azure Databricks.)",Hire Squad,5 - 8 years,20-30 Lacs P.A.,"['Noida', 'Hyderabad', 'Bengaluru']","Looking for Data Engineers, immediate joiners only, for Hyderabad, Bengaluru and Noida Location.\n\n*Must have experience in Python, Kafka Stream, Pyspark, and Azure Databricks.*\n\nRole and responsibilities:\nLead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\nArchitect scalable data streaming and processing solutions to support healthcare data workflows.\nDevelop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\nEnsure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\nCollaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\nTroubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\nMentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\nStay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\n\nPreferred candidate profile :\n5+ years of experience in data engineering, with strong proficiency in Kafka and Python.\nExpertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\nExperience with Azure Databricks (or willingness to learn and adopt it quickly).\nHands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\nProficiency in SQL, NoSQL databases, and data modeling for big data processing.\nKnowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\nExperience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\nStrong analytical skills, problem-solving mindset, and ability to lead complex data projects.\nExcellent communication and stakeholder management skills.\n\n\n\nInterested, call:\nRose (9873538143 / WA : 8595800635)\nrose2hiresquad@gmail.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Databricks', 'Kafka Streams', 'Python', 'Etl Pipelines', 'development', 'Data Engineering', 'cloud-based data platforms', 'implementation', 'Data Streaming', 'Lead the design', 'processing solutions', 'Hitrust', 'large-scale data processing', 'ELT pipelines', 'HIPAA', 'real-time data streaming']",2025-06-11 05:58:14
Cloud Data Engineer,Acesoft,7 - 9 years,19-20 Lacs P.A.,['Bengaluru'],"Hi all,\nWe are hiring for the role Cloud Data Engineer\nExperience: 7 - 9 years\nLocation: Bangalore\nNotice Period: Immediate - 15 Days\nSkills:\nOverall, 7 to 9 years of experience in cloud data and analytics platforms such as AWS, Azure, or GCP\n• Including 3+ years experience with Azure cloud Analytical tools is a must\n• Including 5+ years of experience working with data & analytics concepts such as SQL, ETL, ELT, reporting and report building, data visualization, data lineage, data importing & exporting, and data warehousing\n• Including 3+ years of experience working with general IT concepts such as integrations, encryption, authentication & authorization, batch processing, real-time processing, CI/CD, automation\n• Advanced knowledge of cloud technologies and services, specifically around Azure Data Analytics tools\no Azure Functions (Compute)\no Azure Blob Storage (Storage)\no Azure Cosmos DB (Databases)\no Azure Synapse Analytics (Databases)\no Azure Data Factory (Analytics)\no Azure Synapse Serverless SQL Pools (Analytics)\no Azure Event Hubs (Analytics- Realtime data)\n• Strong coding skills in languages such as\no SQL\no Python\no PySpark\n• Experience in data streaming technologies such as Kafka or Azure Event Hubs\n• Experience in handling unstructured streaming data is highly desired\n• Knowledge of Business Intelligence Dimensional Modelling, Star Schemas, Slowly Changing Dimensions\n• Broad understanding of data engineering methodologies and tools, including Data warehousing, DevOps/DataOps, Data ingestion, ELT/ETL and Data visualization tools\n• Knowledge of database management systems, data modelling, and data warehousing best practices\n• Experience in software development on a team using Agile methodology\n• Knowledge of data governance and security practices\n\nIf you are interested drop your resume at mojesh.p@acesoftlabs.com\nCall: 9701971793",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Cloud Analytics', 'Azure Cloud', 'Data Warehousing', 'AWS', 'Python', 'Azure Data Analytics tools', 'authentication & authorization', 'PySpark', 'batch processing', 'Elt', 'sql', 'automation', 'encryption', 'Azure Cosmos DB', 'real-time processing', 'CI/CD', 'etl', 'integrations']",2025-06-11 05:58:16
Lead Data Engineer ( Exp: 6+ Years ),Atyeti,6 - 11 years,Not Disclosed,"['Hyderabad', 'Pune']",Job Description :\n\nStrong experience on Python programming.\nExperience on Databricks.\nExperience on Database like SQL\nPerform database performance tuning and optimization. Databricks Platform\nWork with Databricks platform for big data processing and analytics.,,,,"['Pyspark', 'ETL', 'Data Bricks', 'Python', 'SQL']",2025-06-11 05:58:17
"Snowflake Data Engineer-Cortex AI, Aws / azure","Location - Kolkata, Hyderabad, Bangalore...",7 - 12 years,20-35 Lacs P.A.,"['Kolkata', 'Hyderabad', 'Bengaluru']","Skill set\nSnowflake, AWS, Cortex AI, Horizon Catalog\nor\nSnowflake, AWS, (Cortex AI or Horizon Catalog)\nor\nSnowflake, Azure, Cortex AI, Horizon Catalog\nOr\nSnowflake, Azure, (Cortex AI or Horizon Catalog)\nPreferred Qualifications:\nBachelors degree in Computer Science, Data Engineering, or a related field.\nExperience in data engineering, with at least 3 years of experience working with Snowflake.\nProven experience in Snowflake, Cortex AI/ Horizon Catalog focusing on data extraction, chatbot development, and Conversational AI.\nStrong proficiency in SQL, Python, and data modeling.\nExperience with data integration tools (e.g., Matillion, Talend, Informatica).\nKnowledge of cloud platforms such as AWS or Azure, or GCP.\nExcellent problem-solving skills, with a focus on data quality and performance optimization.\nStrong communication skills and the ability to work effectively in a cross-functional team.\nProficiency in using DBT's testing and documentation features to ensure the accuracy and reliability of data transformations.\nUnderstanding of data lineage and metadata management concepts, and ability to track and document data transformations using DBT's lineage capabilities.\nUnderstanding of software engineering best practices and ability to apply these principles to DBT development, including version control, code reviews, and automated testing.\nShould have experience building data ingestion pipeline.\nShould have experience with Snowflake utilities such as SnowSQL, SnowPipe, bulk copy, Snowpark, tables, Tasks, Streams, Time travel, Cloning, Optimizer, Metadata Manager, data sharing, stored procedures and UDFs, Snowsight.\nShould have good experience in implementing CDC or SCD type 2\nProficiency in working with Airflow or other workflow management tools for scheduling and managing ETL jobs.\nGood to have experience in repository tools like Github/Gitlab, Azure repo",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Kafka', 'horizon Catalog', 'cortex AI', 'Python', 'aws', 'azure']",2025-06-11 05:58:19
"Google Cloud Platform Data Engineer -GCP,BigQuery,SQL, Cloud Function",Tredence,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Role - GCP Data Engineer\nExperience:4+ years\nPreferred - Data Engineering Background\nLocation - Bangalore, Chennai, Pune, Gurgaon, Kolkata\nRequired Skills - GCP DE Experience, Big query, SQL, Cloud compressor/Python, Cloud functions, Dataproc+pyspark, Python injection, Dataflow+PUB/SUB\n\nHere is the job description for the same -\nJob Requirement:\nHave Implemented and Architected solutions on Google Cloud Platform using the components of GCP\nExperience with Apache Beam/Google Dataflow/Apache Spark in creating end to end data pipelines.\nExperience in some of the following: Python, Hadoop, Spark, SQL, Big Query, Big Table Cloud Storage, Datastore, Spanner, Cloud SQL, Machine Learning.\nExperience programming in Java, Python, etc.\nExpertise in at least two of these technologies: Relational Databases, Analytical Databases, NoSQL databases.\nCertified in Google Professional Data Engineer/ Solution Architect is a major Advantage",,,,"['Pubsub', 'GCP', 'Bigquery', 'Google Cloud Platforms', 'SQL', 'Data Flow', 'Dataproc']",2025-06-11 05:58:20
Azure Data Engineer,Tech Mahindra,5 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Company Name: Tech Mahindra\nExperience: 5-8 Years\nLocation: Bangalore/Hyderabad (Hybrid Model)\nInterview Mode: Virtual\nInterview Rounds: 2 Rounds\nNotice Period: Immediate to 30 days\nGeneric Responsibilities :\nDesign, develop, and maintain large-scale data pipelines using Azure Data Factory (ADF) to extract, transform, and load data from various sources into Azure Databricks.\nCollaborate with cross-functional teams to gather requirements and design solutions for complex business problems.\nDevelop SQL queries and stored procedures to optimize database performance and troubleshoot issues in Azure Databricks.\nEnsure high availability, scalability, and security of the deployed solutions by monitoring logs, metrics, and alerts.\nGeneric Requirements :\n5-8 years of experience in designing and developing large-scale data engineering projects on Microsoft Azure platform.\nStrong expertise in Azure Data Factory (ADF), Azure Databricks, SQL Server Management Studio (T-SQL).\nExperience working with big data technologies such as Hadoop Distributed File System (HDFS), Spark Core/Scala programming languages.",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Azure Data Lake', 'Azure Data Services', 'Data Bricks', 'SQL']",2025-06-11 05:58:22
Associate Data Engineer | Pricing | Full Time Contract 18 Months,Argus India Price Reporting Services,1 - 3 years,5-11 Lacs P.A.,['Mumbai (All Areas)'],"Associate Data Engineer - (Fixed Term Contract)\nMumbai\nJob Purpose:\nDue to the continued growth of the business and the importance of the data we use on a daily basis, we are currently looking for a for a junior data engineer to join our global Data team in Mumbai.\nYou will work closely with internal clients of the data team to support and maintain R (incl. R Shiny), Excel and database-based processes for gathering data, calculating prices and producing the reports and data feeds. The work also involves writing robust automated processes in R.\nThe team supports Argus key business processes every day, as such you will be required to work on a shift-based rota with other members of the team supporting the business until 8pm. Typically support hours run from 11pm to 8pm with each member of the team participating 2/3 times a week.\n\nKey Responsibilities:\nSupport and development of data processing systems\nClient support with queries relating to\nintegration of Argus data and metadata into client systems\ndata validation\nprovision of data\nData and systems support to Argus staff\nProject development\nMaintenance and development of existing systems\nmetadata modification\ndata cleansing\ndata checking\nRequired Skills and Experience:\nSignificant recent experience of developing tools using R (incl., R Shiny applications) in a commercial (work) environment.\nGood knowledge and experience of SQL, preferably in Oracle or MySQL, including stored procedures, functions, and triggers.\nExperience with version control systems (e.g., Git) and Unit Testing in R is required.\nAbility to work both as part of a team and autonomously\nExcellent communication skills.\n\nDesired Skills and Experience:\nBS degree in Computer Science, Mathematics, Business, Engineering or related field.\nExperience in visualisation techniques is desirable.\nAny experience working with energy markets and commodities data is highly desirable\nExperience developing production-grade scalable applications in R.\n\nPersonal Attributes:\nAbility to interact with non-technical people in plain language\nInnovative thinker with good problem-solving abilities and attention to detail\nNumerically proficient\nSelf-motivated with ability to work independently, prioritising tasks to meet deadlines\nCustomer service focused.\n\nBenefits:\nCompetitive salary\nFlexible Working Policy\nGroup healthcare scheme\n18 days annual leave\n8 days casual leave\nExtensive internal and external training",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['R', 'R Shiny', 'Unit Testing', 'Version Control Systems Svn', 'SQL']",2025-06-11 05:58:23
Automation & Data Engineer,Vyometra Global Llp,2 - 3 years,3.6-3.96 Lacs P.A.,['Bengaluru'],"Work with ops teams to digitize manual processes.\nCapture data via PLCs/IoT, build backend (Python, Flask/Django), SQL DB & frontend dashboard.\nTrack OEE, downtime, rejections. Deploy, maintain & contribute to future product roadmap.",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Automation', 'Automation', 'Machine Learning', 'Javascript', 'Python', 'Ai Techniques', 'Artificial Intelligence', 'Software Testing', 'Flask Web Framework', 'Numpy', 'Scikit-Learn', 'English', 'Pandas', 'Data Analysis', 'SQL Database', 'Ai Builder', 'Flask']",2025-06-11 05:58:25
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,5 - 9 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n5+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-11 05:58:27
Data Engineer,Get Your Job,3 - 8 years,9.5-18 Lacs P.A.,"['Hyderabad', 'Gurugram']","3-8 Years exp.\nJob Location – Gurgaon and Hyderabad\nWork Mode – Hybrid (3-4 Days work from office)\nNotice Period – Immediate to 30 Days Official NP, OR 45 days serving NP candidates, ONLY.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Hive', 'hadoop', 'Python', 'Spark', 'SQL']",2025-06-11 05:58:28
Data Engineer,Emiza Supply Chain Services,2 - 6 years,Not Disclosed,['Mumbai (All Areas)( Vidya Vihar West )'],"Role & responsibilities\n\nKey Responsibilities\n\nDesign, build, and maintain scalable data pipelines and ETL/ELT processes.\nIntegrate data from various internal and external sources (e.g., ERP, WMS, APIs).\nOptimize and monitor data flows for performance and reliability.\nCollaborate with data analysts, software developers, and business teams to understand data requirements.\nEnsure data quality, consistency, and security across the data lifecycle.\nSupport reporting, dashboarding, and data science initiatives with clean and structured data.\nMaintain data documentation and metadata repositories.\n\nPreferred candidate profile\n\nBachelor's or Masters degree in Computer Science, Engineering, or a related field.\n2+ years of experience in a data engineering or similar role.\nStrong proficiency in SQL and working with relational databases (e.g., PostgreSQL, MySQL).\nExperience with big data technologies like Spark, Hadoop, or similar is a plus.\nHands-on experience with ETL tools (e.g., Apache Airflow, Talend, DBT).\nProficiency in Python or Scala for data processing.\nFamiliarity with cloud platforms (AWS, GCP, or Azure), especially with data services like S3, Redshift, BigQuery, etc.\nKnowledge of APIs and data integration concepts.",Industry Type: Courier / Logistics,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Etl Pipelines', 'Data Engineering', 'Hadoop', 'Cloud Platform', 'Elt', 'Data Pipeline', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-11 05:58:29
Hiring For Azure Data Engineer with MNC client-FTE-Hyd/Bangalore/Pune,The It Mind Services,4 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","JOB DESCRIPTION:\n\n• Strong experience in Azure Datafactory,Databricks, Eventhub, Python,PySpark ,Azure Synapse and SQL\n• Azure Devops experience to deploy the ADF pipelines.\n• Knowledge/Experience with Azure cloud stack.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Azure Synapse', 'Eventhub', 'Azure Datafactory', 'Databricks', 'Python', 'PySpark', 'Azure Devops']",2025-06-11 05:58:31
Data Engineer,Meritus Management Service,5 - 10 years,10-20 Lacs P.A.,"['Nagpur', 'Pune']","We are looking for a skilled Data Engineer to design, build, and manage scalable data pipelines and ensure high-quality, secure, and reliable data infrastructure across our cloud and on-prem platforms.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Synapse Analytics', 'SQL', 'Azure Data Factory', 'Python', 'API Integration', 'Postgresql', 'Data Bricks', 'Scripting', 'SCALA', 'Data Lake', 'MongoDB', 'Data Warehousing', 'Data Modeling', 'ETL', 'Azure Devops']",2025-06-11 05:58:33
Senior Data Science,IDS Infotech,4 - 8 years,Not Disclosed,['Chandigarh'],"About the Role:\nWe are seeking a highly skilled and motivated Senior Data Scientist with 56 years of experience to drive the development of intelligent AI systems. This role requires extensive hands-on experience with Large Language Models (LLMs), strong background in Agentic AI, Machine Learning, and Python programming.\nYou will work on designing autonomous agents, building scalable ML pipelines, and integrating advanced LLM-powered solutions into real-world products.\nKey Responsibilities:",,,,"['GEN AI', 'Natural Language Processing', 'Aiml', 'LLM', 'Deep Learning', 'Artificial Intelligence', 'Machine Learning']",2025-06-11 05:58:34
Data Engineer,reycruit,7 - 12 years,35-40 Lacs P.A.,['Hyderabad'],"Looking for 8+ years\nPython+Azure/Aws cloud is mandatory\n1st round- virtual\n2nd round- F2F\nMust have 7+ years of relevant experience- should have hands-on experience with ETL/ELT processes, cloud-based data solutions, and big data technologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Pipeline', 'Big Data', 'Elt', 'ETL']",2025-06-11 05:58:35
Python Developer/ Python Data Engineer (Python+ ETL+ Pandas),Atyeti,5 - 8 years,Not Disclosed,['Hyderabad'],"Role & responsibilities\nB.Tech or M.Tech in Computer Science, or equivalent experience.\n5+ years of experience working professionally as a Python Software Developer.\nOrganized, self-directed, and resourceful.\nExcellent written and verbal communication skills.\nExpert in python & pandas.\nExperience in building data pipelines, ETL and ELT processes.",,,,"['Pandas', 'ETL', 'Python', 'SQL']",2025-06-11 05:58:37
Data Engineer,Revature,1 - 2 years,Not Disclosed,['Chennai'],"The ideal candidate should have a strong background in SQL, BigQuery, and Google Cloud Platform (GCP), with hands-on experience in developing reports and dashboards using Looker Studio, Looker Standard, and LookML. Excellent communication skills and the ability to work collaboratively with cross-functional teams are essential for success in this role.\nKey Responsibilities:\nDesign, develop, and maintain dashboards and reports using Looker Studio and Looker Standard.\nDevelop and maintain LookML models, explores, and views to support business reporting requirements.\nOptimize and write advanced SQL queries for data extraction, transformation, and analysis.\nWork with BigQuery as the primary data warehouse for managing and analyzing large datasets.\nCollaborate with business stakeholders to understand data requirements and translate them into scalable reporting solutions.\nImplement data governance, access controls, and performance optimizations within the Looker environment.\nPerform root-cause analysis and troubleshooting for reporting and data issues.\nMaintain documentation for Looker projects, data models, and data dictionaries.\nStay updated with the latest Looker and GCP features and best practices.",,,,"['SQL', 'Python', 'Bigquery', 'Gcp Cloud']",2025-06-11 05:58:38
Openings For Data Engineer,Creative Solutions,5 - 10 years,4-9 Lacs P.A.,['Hyderabad'],"Skills: ADF,ETL,Rest API\nMicrosoft Power Platform / Snowflake Certification is a Plus\nPower Bi / Talend and Integration\nPower Apps\nSAP S4 Hana Abap,Odata Rest and Soap\nJob LOcation : Hyderabad",Industry Type: Software Product,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['ADF', 'power APPS', 'Azure Data Factory', 'Power Bi', 'Sap S4 Hana', 'ETL DEveloper', 'Talend']",2025-06-11 05:58:40
Python Data Engineer,GAVS Technologies,4 - 6 years,5.5-15.5 Lacs P.A.,['Pune'],"Skill Expectations\nMust-Have Skills:\nStrong hands-on experience in Python development\nExperience working with Fast API\nData migration and data engineering experience (ETL, pipelines, transformations)\nExperience in web scraping and data extraction techniques\nExperience working with GCP",,,,"['Data Extraction', 'Web Scraping', 'Python', 'GCP', 'Fast Api']",2025-06-11 05:58:42
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,5 - 9 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n5 to 9+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-11 05:58:44
"Data engineer with Gen AI- Balewadi, pune- hybrid",Indian MNC,5 - 10 years,15-30 Lacs P.A.,['Pune( Balewadi )'],"Role & responsibilities\nWe are seeking a skilled Data Engineer with advanced expertise in Python, PySpark, Databricks, and Machine Learning, along with a working knowledge of Generative and Agentic AI. This role is critical in ensuring data integrity and driving innovation across enterprise systems. You will design and implement ML-driven solutions to enhance Data Governance & Data Privacy initiatives through automation, self-service capabilities, and scalable, AI-enabled innovation.\nKey Responsibilities:\nImplement ML and Generative/Agentic AI solutions to optimize Data Governance processes.\nDesign, develop, and maintain scalable data pipelines using Python, PySpark, and Databricks.\nDevelop automation frameworks to support data quality, lineage, classification, and access control.\nDevelop and deploy machine learning models to uncover data patterns, detect anomalies, and enhance data governance and privacy compliance\nCollaborate with data stewards, analysts, and governance teams to build self-service data capabilities.\nWork with Databricks, Azure Data Lake, AWS, and other cloud-based data platforms for data engineering.\nBuild, configure, and integrate APIs for seamless system interoperability.\nEnsure data integrity, consistency, and compliance across systems and workflows.\nIntegrate AI models to support data discovery, metadata enrichment, and intelligent recommendations.\nOptimize data architecture to support analytics, reporting, and governance use cases.\nMonitor and improve the performance of ML/AI components in production environments.\nStay updated with emerging AI and data engineering technologies to drive continuous innovation.\nTechnical Skills:\nStrong programming skills in Python, PySpark, SQL for data processing and automation.\nExperience with Databricks and Snowflake (preferred) for building and maintaining data pipelines.\nExperience with Machine Learning model development and Generative/Agentic AI frameworks (e.g. LLMs, Transformers, LangChain) especially in the Data Management space\nExperience working with REST APIs & JSON for service integration\nExperience working with cloud-based platforms such as Azure, AWS, or GCP\nPower BI dashboard development experience is a plus.\nSoft Skills:\nStrong problem-solving skills and attention to detail.\nExcellent communication and collaboration abilities, with experience working across technical and business teams",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Generative Ai', 'Azure Databricks', 'Ml']",2025-06-11 05:58:45
Data Engineer,Forbes Global 2000 IT Services Firm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Title: Big Data Engineer Java & Spark\nLocation: Hyderabad\nWork Mode: Onsite (5 days a week)\nExperience: 5 to 10 Years\nJob Summary:\nWe are hiring an experienced Big Data Engineer with strong expertise in Java, Apache Spark, and Big Data technologies. You will be responsible for designing and implementing scalable data pipelines that support real-time and batch processing for data-driven applications.\nKey Responsibilities:\nDevelop and maintain scalable batch and streaming data pipelines using Java and Apache Spark\nWork with Hadoop, Hive, Kafka, and HDFS to manage and process large datasets\nCollaborate with data analysts, scientists, and other engineering teams to understand data requirements\nOptimize Spark jobs and ensure performance and reliability in production\nMaintain data quality, governance, and security best practices\nRequired Skills:\n510 years of hands-on experience in data engineering or related roles\nStrong programming skills in both Java\nExpertise in Apache Spark for data processing and transformation\nGood understanding of Big Data frameworks: Hadoop, Hive, Kafka, HDFS\nExperience with distributed systems and large-scale data processing\nFamiliarity with cloud platforms such as AWS, GCP, or Azure\nGood to Have:\nExperience with workflow orchestration tools like Airflow or NiFi\nKnowledge of containerization (Docker, Kubernetes)\nExposure to CI/CD pipelines and version control (e.g., Git)\nEducation:\nBachelors or Masters degree in Computer Science, Engineering, or related field\nWhy Join Us:\nBe part of a high-impact data engineering team\nWork on modern data platforms with the latest open-source tools\nStrong tech culture with career growth opportunities",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spark', 'Hive', 'Hadoop']",2025-06-11 05:58:47
CMMS Data Engineer,Idexcel,3 - 6 years,Not Disclosed,['Vadodara'],"Role & responsibilities\n\nProfessional Summary:\nExperience: Extensive experience in major oil & gas industry equipment including pumps, compressors, turbines, piping, storage tanks, and vessels. Proficient in understanding master data and Bill of Materials (BoM) structures.\nKnowledge: In-depth knowledge of equipment classification, codes, and characteristics. Skilled in reviewing materials classification, codes, and characteristics.",,,,"['SAP PM', 'Material Management', 'SAP MM', 'Data Management', 'CMMS', 'SAP', 'MDM', 'Master Data Management']",2025-06-11 05:58:48
Data Engineer,Lance Labs,7 - 12 years,Not Disclosed,"['Noida', 'Chennai']","Deployment, configuration & maintenance of Databricks clusters & workspaces\nSecurity & Access Control\nAutomate administrative task using tools like Python, PowerShell &Terraform\nIntegrations with Azure Data Lake, Key Vault & implement CI/CD pipelines\n\nRequired Candidate profile\nAzure, AWS, or GCP; Azure experience is preferred\nStrong skills in Python, PySpark, PowerShell & SQL\nExperience with Terraform\nETL processes, data pipeline &big data technologies\nSecurity & Compliance",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Databricks', 'SQL', 'Terraform', 'Python', 'Powershell', 'Ci/Cd', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Azure Data Lake', 'ETL', 'AWS', 'Data Governance', 'Azure Devops']",2025-06-11 05:58:49
Data Engineer,MNC,4 - 8 years,Not Disclosed,['Hyderabad'],"Job Title: Software Engineer -Data Engineer\nPosition: Software Engineer\nExperience: 4-6 years (Less YOE will be Rejected)\nCategory: Software Development/ Engineering\nShift Timings: 1:00 pm to 10:00 pm\nMain location: Hyderabad\nWork Type: Work from office\nNotice Period: 0-30 Days\nSkill: Python, Pyspark, Data Bricks\nEmployment Type: Full Time\n\n• Bachelor's in Computer Science, Computer Engineering or related field\nRequired qualifications to be successful in this role\nMust have Skills:\n• 3+ yrs. Development experience with Spark (PySpark), Python and SQL.\n• Extensive knowledge building data pipelines\n• Hands on experience with Databricks Devlopment\n• Strong experience with\n• Strong experience developing on Linux OS.\n• Experience with scheduling and orchestration (e.g. Databricks Workflows,airflow, prefect, control-m).\n\nGood to have skills:\n• Solid understanding of distributed systems, data structures, design principles.\n• Agile Development Methodologies (e.g. SAFe, Kanban, Scrum).\n• Comfortable communicating with teams via showcases/demos.\n• Play key role in establishing and implementing migration patterns for the Data Lake Modernization project.\n• Actively migrate use cases from our on premises Data Lake to Databricks on GCP.\n• Collaborate with Product Management and business partners to understand use case requirements and reporting.\n• Adhere to internal development best practices/lifecycle (e.g. Testing, Code Reviews, CI/CD, Documentation) .\n• Document and showcase feature designs/workflows.\n• Participate in team meetings and discussions around product development.\n• Stay up to date on industry latest industry trends and design patterns.\n• 3+ years experience with GIT.\n• 3+ years experience with CI/CD (e.g. Azure Pipelines).\n• Experience with streaming technologies, such as Kafka, Spark.\n• Experience building applications on Docker and Kubernetes.\n• Cloud experience (e.g. Azure, Google).\n\nInterested Candidates can drop your Resume on Mail id :- "" tarun.k@talent21.in """,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Bricks', 'Python', 'Pyspark', 'SQL', 'ETL', 'Airflow', 'Azure Pipelines', 'Kafka', 'Design', 'Docker', 'Azure Cloud', 'Control-M', 'Cicd Pipeline', 'Modernization', 'testing', 'Documentation', 'Workflow', 'Code Review', 'Gcp Cloud', 'Product Development', 'Agile Methodology', 'GIT', 'Linux', 'GCP', 'Data Lake', 'Kubernetes']",2025-06-11 05:58:51
Data Engineer,MNC,4 - 9 years,Not Disclosed,['Hyderabad'],"Job Title: Software Engineer -Data Engineer\nPosition: Software Engineer\nExperience: 4-9 years\nCategory: Software Development/ Engineering\nShift Timings: 1:00 pm to 10:00 pm\nMain location: Hyderabad\nWork Type: Work from office\nNotice Period: 0-30 Days\nSkill: Python, Pyspark, Data Bricks\nEmployment Type: Full Time\n\n• Bachelor's in Computer Science, Computer Engineering or related field\nRequired qualifications to be successful in this role\nMust have Skills:\n• 3+ yrs. Development experience with Spark (PySpark), Python and SQL.\n• Extensive knowledge building data pipelines\n• Hands on experience with Databricks Devlopment\n• Strong experience with\n• Strong experience developing on Linux OS.\n• Experience with scheduling and orchestration (e.g. Databricks Workflows,airflow, prefect, control-m).\n\nGood to have skills:\n• Solid understanding of distributed systems, data structures, design principles.\n• Agile Development Methodologies (e.g. SAFe, Kanban, Scrum).\n• Comfortable communicating with teams via showcases/demos.\n• Play key role in establishing and implementing migration patterns for the Data Lake Modernization project.\n• Actively migrate use cases from our on premises Data Lake to Databricks on GCP.\n• Collaborate with Product Management and business partners to understand use case requirements and reporting.\n• Adhere to internal development best practices/lifecycle (e.g. Testing, Code Reviews, CI/CD, Documentation) .\n• Document and showcase feature designs/workflows.\n• Participate in team meetings and discussions around product development.\n• Stay up to date on industry latest industry trends and design patterns.\n• 3+ years experience with GIT.\n• 3+ years experience with CI/CD (e.g. Azure Pipelines).\n• Experience with streaming technologies, such as Kafka, Spark.\n• Experience building applications on Docker and Kubernetes.\n• Cloud experience (e.g. Azure, Google).\n\nInterested Candidates can drop your Resume on Mail id :- "" kalyan.v@talent21.in """,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Bricks', 'Python', 'Pyspark', 'SQL', 'ETL', 'Airflow', 'Azure Pipelines', 'Kafka', 'Design', 'Docker', 'Azure Cloud', 'Control-M', 'Cicd Pipeline', 'Modernization', 'testing', 'Documentation', 'Workflow', 'Code Review', 'Gcp Cloud', 'Product Development', 'Agile Methodology', 'GIT', 'Linux', 'GCP', 'Data Lake', 'Kubernetes']",2025-06-11 05:58:53
Director Data Science,Astar Data,10 - 17 years,Not Disclosed,['Bengaluru'],"Sigmoid enables business transformation using data and analytics, leveraging real-time insights to make accurate and fast business decisions, by building modern data architectures using cloud and open source. Some of the worlds largest data producers engage with Sigmoid to solve complex business problems. Sigmoid brings deep expertise in data engineering, predictive analytics, artificial intelligence, and DataOps. Sigmoid has been recognized as one of the fastest growing technology companies in North America, 2021, by Financial Times, Inc. 5000, and Deloitte Technology Fast 500.\nOffices: New York | Dallas | San Francisco | Lima | Bengaluru\nThe below role is for our Bengaluru office.\n\nWhy Join Sigmoid?\n• Sigmoid provides the opportunity to push the boundaries of what is possible by seamlessly\ncombining technical expertise and creativity to tackle intrinsically complex business\nproblems and convert them into straight-forward data solutions.\n• Despite being continuously challenged, you are not alone. You will be part of a fast-paced\ndiverse environment as a member of a high-performing team that works together to\nenergize and inspire each other by challenging the status quo\n• Vibrant inclusive culture of mutual respect and fun through both work and play\nRoles and Responsibilities:\n• Convert broad vision and concepts into a structured data science roadmap, and guide a\nteam to successfully execute on it.\n• Handling end-to-end client AI & analytics programs in a fluid environment. Your role will be a\ncombination of hands-on contribution, technical team management, and client interaction.\n• Proven ability to discover solutions hidden in large datasets and to drive business results\nwith their data-based insights\n• Contribute to internal product development initiatives related to data science.\n• Drive excellent project management required to deliver complex projects, including\neffort/time estimation.\n• Be proactive, with full ownership of the engagement. Build scalable client engagement level\nprocesses for faster turnaround & higher accuracy\n• Define Technology/ Strategy and Roadmap for client accounts, and guides implementation\nof that strategy within projects\n• Manage the team-members, to ensure that the project plan is being adhered to over the\ncourse of the project\n• Build a trusted advisor relationship with the IT management at clients and internal accounts\nleadership.\nMandated Skills:\n• A B-Tech/M-Tech/MBA from a top tier Institutepreferably in a quantitativesubject\n• 10+ years of hands-onexperience in applied Machine Learning, AI and analytics\n• Experience of scientific programming in scripting languages like Python, R, SQL, NoSQL,\nSpark with ML tools & Cloud Technology (AWS, Azure, GCP)\n• Experience in Python libraries such as numpy, pandas, scikit-learn, tensor-flow, scrapy, BERT\netc. Strong grasp of depth and breadth of machine learning, deep learning, data mining, and\nstatistical concepts and experience in developing models and solutions in these areas\n• Expertise with client engagement, understanding complex problem statements, and offering\nsolutions in the domains of Supply Chain, Manufacturing, CPG, Marketing etc.\nDesired Skills:\nDeep understanding of ML algorithms for common use cases in both structured and\nunstructured data ecosystems.\nComfortable with large scale data processing and distributed computing\nProviding required inputs to sales, and pre-sales activities\nA self-starter who can work well with minimalguidance\nExcellent written and verbal communication skills",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Machine Learning', 'Algorithm Development', 'Pattern Recognition', 'Opencv', 'Image Processing', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Computer Vision', 'Deep Learning']",2025-06-11 05:58:54
Azure Data Engineer - Remote,Software Company,4 - 8 years,8-13 Lacs P.A.,[],"Azure Cloud Technologies, Azure Data Factory, Azure Databricks (Advance Knowledge), PySpark, CI/CD Pipeline (Jenkins, GitLab CVCD or Azure DevOps), Data Ingestion, SOL\ndesigning, developing, & optimizing scalable data solutions.\n\nRequired Candidate profile\nAzure Databricks, Azure Data Factory expertise, PySpark proficiency, Big Data CI/CD, Troubleshoot, Jenkins, Gitlab CI/ CD, Data Pipeline Development & Deployment",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Azure Develops', 'Azure Data Engineer', 'Azure Data Factory', 'Jenkins', 'Azure Cloud Technologies', 'PySpark', 'Azure Databricks', 'GitLab CI/CD', 'Data Injection, SOL', 'GitLab CVCD']",2025-06-11 05:58:56
Data Engineer Graph – Research Data and Analytics,Amgen Inc,2 - 4 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be part Researchs Semantic Graph Team is seeking a qualified individual to design, build, and maintain solutions for scientific data that drive business decisions for Research. The successful candidate will construct scalable and high-performance data engineering solutions for extensive scientific datasets and collaborate with Research partners to address their data requirements. The ideal candidate should have experience in the pharmaceutical or biotech industry, leveraging their expertise in semantics, taxonomies, and linked data principles to ensure data harmonization and interoperability. Additionally, this individual should demonstrate robust technical skills, proficiency with data engineering technologies, and a thorough understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nDesign, develop, and implement data pipelines, ETL/ELT processes, and data integration solutions\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks\nDevelop and maintain semantic data models for biopharma scientific data, data dictionaries, and other documentation to ensure data accuracy and consistency\nOptimize large datasets for query performance\nCollaborate with global multi-functional teams including research scientists to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nCollaborate with Data Architects, Business SMEs, Software Engineers and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve [complex] data-related challenges\nAdhere to standard processes for coding, testing, and designing reusable code/component\nExplore new tools and technologies that will help to improve ETL platform performance\nParticipate in sprint planning meetings and provide estimations on technical implementation\nMaintain comprehensive documentation of processes, systems, and solutions\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. T\nBasic Qualifications and Experience:\nDoctorate Degree OR Masters degree with 2- 4years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 4- 6years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 7- 9 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\n\n\nPreferred Qualifications and Experience:\n4+ years of experience in designing and supporting biopharma scientific research data analytics (software platforms)\n\n\nFunctional Skills:\nMust-Have Skills:\nProficiency in SQL and Python for data engineering, test automation frameworks (pytest), and scripting tasks\nHands on experience with data technologies and platforms, such as Databricks, workflow orchestration, performance tuning on big data processing.\nExcellent problem-solving skills and the ability to work with large, complex datasets\n\n\nGood-to-Have Skills:\nA passion for tackling complex challenges in drug discovery with technology and data\nExperience with system administration skills, such as managing Linux and Windows servers, configuring network infrastructure, and automating tasks with shell scripting. Examples include setting up and maintaining virtual machines, troubleshooting server issues, and ensuring data security through regular updates and backups.\nSolid understanding of data modeling, data warehousing, and data integration concepts\nSolid experience using RDBMS (e.g. Oracle, MySQL, SQL server, PostgreSQL)\nKnowledge of cloud data platforms (AWS preferred)\nExperience with data visualization tools (e.g. Dash, Plotly, Spotfire)\nExperience with diagramming and collaboration tools such as Miro, Lucidchart or similar tools for process mapping and brainstorming\nExperience writing and maintaining user documentation in Confluence\nUnderstanding of data governance frameworks, tools, and standard processes\n\n\nProfessional Certifications:\nDatabricks Certified Data Engineer Professional preferred\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analytics', 'PostgreSQL', 'MySQL', 'ETL', 'ELT', 'Oracle', 'SQL server', 'AWS']",2025-06-11 05:58:58
Azure Data Engineers For Pune IT Companies urgent,International IT Companies,3 - 8 years,9-16 Lacs P.A.,['Pune'],"We are looking for a skilled Azure Data Engineer to design, develop, optimize data pipelines for following\n1, SQL+ETL+AZURE+Python+Pyspark+Databricks\n2, SQL+ADF+ Azure\n3, SQL+Python+Pyspark\n- Strong proficiency in SQL for data manipulation querying\n\nRequired Candidate profile\n- Python and PySpark for data engineering tasks.\n- Exp with Databricks for big data processing analytics.\n- Knowledge of data modeling, warehousing, governance.\n- CI/CD pipelines for data deployment.\n\nPerks and benefits\nPerks and Benefits",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure', 'PySpark', 'SQL', 'ETL', 'Python', 'Data Transformation', 'Business Intelligence', 'ADF', 'Data Engineering', 'Data Pipelines', 'Big Data', 'AI', 'Machine Learning', 'Analytics', 'Cloud Computing', 'Data Security', 'Databricks', 'Data Governance']",2025-06-11 05:59:00
Cloud Data Engineer,Vesz Consultancy Services,5 - 10 years,9-15.6 Lacs P.A.,['Chennai'],"SQL, Python, Spark\nAWS Glue, Lambda, Step Functions, Azure Data Factory / Data bricks\nData validation, transformation, and quality assurance.\nBuilding and maintaining automated data pipelines, for data integrity\nWorking with large datasets\nGood Comm.",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Cloud', 'Spark', 'AWS', 'Python', 'Jenkins', 'Github', 'Postgresql']",2025-06-11 05:59:01
"Sustainable, Client and Regulatory Reporting Data Product Owner",Capital Markets,15 - 20 years,Not Disclosed,['Bengaluru'],"Hiring, Sustainable, Client and Regulatory Reporting Data Product Owner - ISS Data (Associate Director)\nAbout your team\n\nThe Technology function provides IT services that are integral to running an efficient run-the business operating model and providing change-driven solutions to meet outcomes that deliver on our business strategy. These include the development and support of business applications that underpin our revenue, operational, compliance, finance, legal, marketing and customer service functions. The broader organisation incorporates Infrastructure services that the firm relies on to operate on a day-to-day basis including data centre, networks, proximity services, security, voice, incident management and remediation.\nThe Technology group is responsible for providing Technology solutions to the Investment Solutions & Services business (which covers Investment Management, Asset Management Operations & Distribution business units globally)\n\nThe Technology team supports and enhances existing applications as well as designs, builds and procures new solutions to meet requirements and enable the evolving business strategy.\nAs part of this group, a dedicated Data Programme team has been mobilised as a key foundational programme to support the execution of the overarching Investment Solutions and Service strategy.\n\nAbout your role\nThe Investment Reporting Data Product Owner role is instrumental in the creation and execution of a future state data reporting product to enable Regulatory, Client, Vendor, Internal & MI reporting and analytics. The successful candidate will have an in- depth knowledge of all data domains that represent institutional clients , the investment life cycle , regulatory and client reporting data requirements.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned with our cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our internal business stakeholders and our clients.\n\nKey Responsibilities\n\nLeadership and Management:\nLead the ISS distribution, Client Propositions, Sustainable Investing and Regulatory reporting data outcomes defining the data roadmap and capabilities and supporting the execution and delivery of the data solutions as a Data Product lead within the ISS Data Programme.\nLine management responsibilities for junior data analysts within the chapter, coaching, influencing and motivating them for high performance.\nDefine the data product vision and strategy with end-to-end thought leadership.\nLead and define the data product backlog , documentation, enable peer-reviews, analysis effort estimation, maintain backlog, and support end to end planning.\nBe a catalyst of change for driving efficiencies, scale and innovation.\n\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering.\n\nCoordination and Communication:\nSenior management level communication to influence senior tech and business stakeholders globally, get alignment on the roadmaps.\nCoordinate with internal and external teams to communicate with those impacted by data flows.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\n\nAbout you\nThe Investment Reporting Data Product Owner role is instrumental in the creation and execution of a future state data reporting product to enable Regulatory, Client, Vendor, Internal & MI reporting and analytics. The successful candidate will have an in- depth knowledge of all data domains that represent institutional clients , the investment life cycle , regulatory and client reporting data requirements.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned with cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our internal business stakeholders and our clients.\n\nKey Responsibilities\n\nLeadership and Management:\nLead the ISS distribution, Client Propositions, Sustainable Investing and Regulatory reporting data outcomes defining the data roadmap and capabilities and supporting the execution and delivery of the data solutions as a Data Product lead within the ISS Data Programme.\nLine management responsibilities for junior data analysts within the chapter, coaching, influencing and motivating them for high performance.\nDefine the data product vision and strategy with end-to-end thought leadership.\nLead and define the data product backlog , documentation, enable peer-reviews, analysis effort estimation, maintain backlog, and support end to end planning.\nBe a catalyst of change for driving efficiencies, scale and innovation.\n\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering.\n\nCoordination and Communication:\nSenior management level communication to influence senior tech and business stakeholders globally, get alignment on the roadmaps.\nCoordinate with internal and external teams to communicate with those impacted by data flows.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\n\nYour Skills and Experience\n\nStrong leadership and senior management level communication, internal and external client management and influencing skills.\nAt least 15 years of proven experience as a senior business/technical/data analyst within technology and/or business change delivering data led business outcomes within the financial services/asset management industry.\n5-10 years as a data product owner adhering to agile methodology, delivering data solutions using industry leading data platforms such as Snowflake, State Street Alpha Data, Refinitiv Eikon, SimCorp Dimension, BlackRock Aladdin, FactSet etc.\nOutstanding knowledge of Client life cycle covering institutional & wholesale with a focus on CRM data, Transfer agency data.\nVery good understanding of the data generated by investment management processes and how that is leveraged in Go-to market capabilities such as client reporting, Sales, Marketing.\nExcellent knowledge of regulatory environment with a focus on European regulations and ESG specific ones such as MIFID II, EMIR, SFDR.\nWork effortlessly in different operating models such as insourcing, outsourcing and hybrid models.\nAutomation mindset that can drive efficiencies and quality in the reporting landscape.\nKnowledge of industry standard data calcs for fund factsheets, Institutional admin and investment reports would be an added advantage.\nIn Depth expertise in data and calculations across the investment industry covering the below.\nClient Specific data: This includes institutional and wholesale client, account and channels data, client preferences and data sets needed for client analytics. Knowledge of Salesforce desirable.\nTransfer Agency & Platform data: This includes granular client holdings at various levels, client transactions and relevant ref data. Knowledge of role of TPAs as TA and integrating external feeds/products with strategic inhouse data platforms.\nInvestment data: This includes investment life cycle data covering data domains such as trading, ABOR, IBOR, Security and fund reference.\nShould possess Problem Solving, Attention to detail, Critical thinking.\nTechnical Skills: Hands on SQL, Advanced Excel, Python, ML (optional) and knowledge of end-to-end tech solutions involving data platforms.\nKnowledge of data management, data governance, and data engineering practices\nHands on experience with data modelling techniques such as dimensional, data vault.\nWillingness to own and drive things, collaboration across business and tech stakeholders.",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Product Management,"Employment Type: Full Time, Permanent","['Data Transformation', 'ESG Framework', 'Snowflake', 'Asset Management', 'Product Owner', 'Product Manager', 'MIFID II', 'alphastate street', 'SQL', 'EMIR', 'Data Quality', 'Data Analysis', 'charles river', 'Agile', 'UK Regulatory Reporting', 'data roadmap', 'Capital Market Operations', 'Aladdin', 'SFDR.', 'Python', 'ML']",2025-06-11 05:59:03
Data Architect / Engagement Lead,Ignitho,7 - 10 years,Not Disclosed,['Chennai( Sholinganallur )'],"Job Title: Data Architect / Engagement Lead\nLocation: Chennai\nReports To: CEO\n\nAbout the Company:\nIgnitho Inc. is a leading AI and data engineering company with a global presence, including US, UK, India, and Costa Rica offices.\nVisit our website to learn more about our work and culture: www.ignitho.com.\nIgnitho is a portfolio company of Nuivio Ventures Inc., a venture builder dedicated to developing Enterprise AI product companies across various domains, including AI, Data Engineering, and IoT.\nLearn more about Nuivio at: www.nuivio.com.\n\nJob Summary:\nAs the Data Architect and Engagement Lead, you will define the data architecture strategy and lead client engagements, ensuring alignment between data solutions and business goals. This dual role blends technical leadership with client-facing responsibilities.\n\nKey Responsibilities:\nDesign scalable data architectures, including storage, processing, and integration layers.\nLead technical discovery and requirements gathering sessions with clients.\nProvide architectural oversight for data and AI solutions.\nAct as a liaison between technical teams and business stakeholders.\nDefine data governance, security, and compliance standards.\n\nRequired Qualifications:\nBachelors or Masters in computer science, Information Systems, or similar.\n7+ years of experience in data architecture, with client-facing experience.\nDeep knowledge of data modelling, cloud data platforms (Snowflake / BigQuery/ Redshift / Azure), and orchestration tools.\nExcellent communication, stakeholder management, and technical leadership skills.\nFamiliarity with AI/ML systems and their data requirements is a strong plus.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Data Modeling', 'Azure Cloud', 'Bigquery', 'Redshift Aws', 'Artificial Intelligence', 'Snowflake', 'Machine Learning']",2025-06-11 05:59:05
Data Engineering Manager,Amgen Inc,3 - 5 years,Not Disclosed,['Hyderabad'],"We are seeking a seasoned Engineering Manager (Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives. You will be responsible for the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional Skills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharma domain companies.\nExperience in designing and maintaining data pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, or scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development & DataOps automation, logging frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have Skills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\nDoctorate Degree with 3-5 + years of experience in Computer Science, IT or related field\nOR\nMasters degree with 6 - 8 + years of experience in Computer Science, IT or related field\nOR\nBachelors degree with 10 - 12 + years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Neo4J', 'HIPAA', 'Stardog', 'Databricks', 'Marklogic', 'AWS', 'SOX', 'GDPR']",2025-06-11 05:59:06
Data Architect,Coforge,11 - 16 years,Not Disclosed,"['Noida', 'Greater Noida', 'Delhi / NCR']","-Data Architect Department:\nData & Analytics The Data Architect having more than 14 years of experience and should play a pivotal role in designing, developing, and governing scalable data architectures to support enterprise-wide data integration, analytics, and reporting.\nThis role will focus on creating unified data models, optimizing data pipelines, and ensuring compliance with regulatory standards (GDPR) using cloud-based platforms.\nThe ideal candidate is a strategic thinker with deep expertise in data modeling, cloud data platforms, and governance.",,,,"['Data Migration', 'Data Warehousing', 'Data Modeling', 'Informatica', 'SSIS', 'ETL Tool']",2025-06-11 05:59:08
Engineer - Investment Data Platform - 3+ Years - Pune,Crescendo Global,3 - 6 years,Not Disclosed,['Pune'],"Engineer - Investment Data Platform - 3+ Years - Pune\n\nWe are hiring a skilled Engineer to join the Investment Data Platform in Pune in Financial services. If you're passionate about data software and engineering and delivering high-quality software solutions using Azure and .Net technologies, this opportunity is for you.\n\nLocation: Pune\n\nYour Future Employer: Our client is a leading financial services firm with a global presence. They are committed to creating an inclusive and diverse workplace where all employees feel valued and have the opportunity to reach their full potential.\n\nResponsibilities:\nDeveloping and maintain software solutions aligned with business outcomes.\nCollaborating within agile teams to review user stories and implement features.\nMaintaining existing data platform artefacts and contribute to continuous improvement.\nBuilding scalable, robust software adhering to data engineering best practices.\nSupporting development of data ingestion, modeling, transformation, and deployment pipelines.\n\nRequirements:\n3+ years of experience in software engineering and 2+ years in data engineering.\nProficiency in C#, .Net Framework, SQL; exposure to Python, Java, PowerShell, or JavaScript.\nExperience with Azure Data Factory, CI/CD pipelines, and DevOps principles.\nStrong interpersonal and communication skills.\nBachelor's degree in computer science, engineering, finance or related field\n\nWhat is in it for you:\nJoin a high-performing team at a global investment leader\nExposure to cutting-edge Azure data platform technologies\nCompetitive compensation with hybrid work flexibility\n\nReach us: If you think this role is aligned with your career, kindly write to me along with your updated CV at aayushi.goyal@crescendogroup.in for a confidential discussion on the role.\n\nDisclaimer: Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging and memorable job search and leadership hiring experience. Crescendo Global does not discriminate based on race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n\nNote: We receive a lot of applications daily, so it may not be possible to respond to each one individually. Please assume that your profile has not been shortlisted if you don't hear from us in a week. Thank you for your understanding.\n\nScammers can misuse Crescendo Globals name for fake job offers. We never ask for money, purchases, or system upgrades. Verify all opportunities at www.crescendo-global.com and report fraud immediately. Stay alert!\n\nProfile Keywords: Azure Data Engineering, C# Developer, .Net Engineer, SQL Data Engineer, DevOps Data, Databricks Jobs, Data Ingestion Engineer, Financial Services Tech Jobs, Asset Management IT, Financial Services",Industry Type: Financial Services (Asset Management),Department: Other,"Employment Type: Full Time, Permanent","['C#', 'dot net', 'SQL', 'Financial markets', 'asset management process', 'Azure Databricks', 'Azure DevOps']",2025-06-11 05:59:09
Aws Data Engineer,Binary Infoways,5 - 8 years,18-30 Lacs P.A.,['Hyderabad'],"AWS Data Engineer with Glue, Terraform, Business Intelligence (Tableau) development\n* Design, develop & maintain AWS data pipelines using Glue, Lambda & Redshift\n* Collaborate with BI team on ETL processes & dashboard creation with Tableau",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Tableau', 'Glue', 'Terraform', 'AWS', 'Business Intelligence', 'Ci/Cd', 'Aws Glue', 'GIT', 'Aws Lambda', 'Amazon Redshift', 'Redshift Aws', 'Spark', 'Splunk', 'Kubernetes']",2025-06-11 05:59:11
Azure Data lead,Infyjob Technology Services,7 - 12 years,Not Disclosed,['Chennai'],"candidate must have 7 year relevant experience in this following key skills\nazure data bricks\nAzure data factory\npython\nSQL\nonly we looking immediate joiners, even only give first priority for tier one companies\n\nonly chennai loaction available\n\nRequired Candidate profile\nAzure Data Engineer, Azure Solutions Architect , azure data factory",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'SQL', 'Python']",2025-06-11 05:59:12
Data Engineer (AWS Databricks),Fortune India 500 IT Services Firm,5 - 7 years,15-22.5 Lacs P.A.,['Chennai'],"Role & responsibilities :\n\nJob Description:\n\nPrimarily looking for a Data Engineer (AWS) with expertise in processing data pipelines using Data bricks, PySpark SQL on Cloud distributions like AWS\nMust have AWS Data bricks ,Good-to-have PySpark, Snowflake, Talend\n\nRequirements-\n\n• Candidate must be experienced working in projects involving\n• Other ideal qualifications include experiences in\n• Primarily looking for a data engineer with expertise in processing data pipelines using Databricks Spark SQL on Hadoop distributions like AWS EMR Data\nbricks Cloudera etc.\n• Should be very proficient in doing large scale data operations using Databricks and overall very comfortable using Python\n• Familiarity with AWS compute storage and IAM concepts\n• Experience in working with S3 Data Lake as the storage tier\n\n• Any ETL background Talend AWS Glue etc. is a plus but not required\n• Cloud Warehouse experience Snowflake etc. is a huge plus\n• Carefully evaluates alternative risks and solutions before taking action.\n• Optimizes the use of all available resources\n• Develops solutions to meet business needs that reflect a clear understanding of the objectives practices and procedures of the corporation department and business unit\n\n• Skills\n• Hands on experience on Databricks Spark SQL AWS Cloud platform especially S3 EMR Databricks Cloudera etc.\n• Experience on Shell scripting\n• Exceptionally strong analytical and problem-solving skills\n• Relevant experience with ETL methods and with retrieving data from dimensional data models and data warehouses\n• Strong experience with relational databases and data access methods especially SQL\n• Excellent collaboration and cross functional leadership skills\n• Excellent communication skills both written and verbal\n• Ability to manage multiple initiatives and priorities in a fast-paced collaborative environment\n• Ability to leverage data assets to respond to complex questions that require timely answers\n• has working knowledge on migrating relational and dimensional databases on AWS Cloud platform\nSkills\nMandatory Skills: Apache Spark, Databricks, Java, Python, Scala, Spark SQL.\n\n\nNote : Need only Immediate joiners/ Serving notice period.\n\nInterested candidates can apply.\n\nRegards,\nHR Manager",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineer', 'Aws Databricks', 'SQL', 'Data Engineering', 'Azure Databricks', 'ETL', 'Talend', 'Python']",2025-06-11 05:59:14
Data Architect,.,7 - 12 years,20-35 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Job Description\nWe are seeking a highly skilled Azure Data Engineer with strong expertise in Data Architecture, PySpark/Python, Azure Databricks, and data streaming solutions. The ideal candidate will have hands-on experience in designing and implementing large-scale data pipelines, along with solid knowledge of data governance and data modeling.\nKey Responsibilities\nDesign, develop, and optimize PySpark/Python-based data streaming jobs on Azure Databricks.\nBuild scalable and efficient data pipelines for batch and real-time processing.\nImplement data governance policies, ensuring data quality, security, and compliance.\nDevelop and maintain data models (dimensional, relational, NoSQL) to support analytics and reporting.\nCollaborate with cross-functional teams (data scientists, analysts, and business stakeholders) to deliver data solutions.\nTroubleshoot performance bottlenecks and optimize Spark jobs for efficiency.\nEnsure best practices in CI/CD, automation, and monitoring of data workflows.\nMentor junior engineers and lead technical discussions (for senior/managerial roles).\nMandatory Skills & Experience\n5+ years of relevant experience as a Data Engineer/Analyst/Architect (8+ years for Manager/Lead positions).\nExpert-level proficiency in PySpark/Python and Azure Databricks (must have worked on real production projects).\nStrong experience in building and optimizing streaming data pipelines (Kafka, Event Hubs, Delta Lake, etc.).\n4+ years of hands-on experience in data governance & data modeling (ER, star schema, data vault, etc.).\nIn-depth knowledge of Azure Data Factory, Synapse, ADLS, and SQL/NoSQL databases.\nExperience with Delta Lake, Databricks Workflows, and performance tuning.\nFamiliarity with data security, metadata management, and lineage tracking.\nExcellent communication skills (must be able to articulate technical concepts clearly).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure Databricks', 'Data Modeling', 'Data Governance', 'Python', 'ETL']",2025-06-11 05:59:16
Urgent Hiring For Data Engineer with Product based MNC Pune,Peoplefy,7 - 12 years,Not Disclosed,['Pune'],Greetings from Peoplefy Infosolutions !!!\n\nWe are hiring for one of our reputed MNC client based in Pune.\nWe are looking for candidates with 7 + years of experience in below skills -\n\nPrimary skills :\nUnderstanding of AI ML in DE\nPython\nData Engineers\nDatabase -Big query or Snowflake\n\n\nInterested candidates for above position kindly share your CVs on chitralekha.so@peoplefy.com with below details -\n\nExperience :\nCTC :\nExpected CTC :\nNotice Period :\nLocation :,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Aiml', 'ETL', 'Python', 'Bigquery', 'AWS']",2025-06-11 05:59:17
Azure Data Engineer,Meritus Management Service,4 - 7 years,10-20 Lacs P.A.,['Pune'],"Experience in designing, developing, implementing, and optimizing data solutions on Microsoft Azure. Proven expertise in leveraging Azure services for ETL processes, data warehousing and analytics, ensuring optimal performance and scalability.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Pipeline', 'Synapse Analytics', 'Azure Databricks', 'Spark', 'Python', 'Pyspark', 'Ci/Cd', 'azure']",2025-06-11 05:59:19
Data Modelling and Data Visualization Specialist (Power BI),Krish Services Group,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Description Data Modelling and Data Visualization Specialist (Power BI)\n\n\nCompany Description - Krish is committed to enabling customers to achieve their technological goals by delivering solutions that combine the right technology, people, and costs. Our approach emphasizes building long-term relationships while ensuring customer success through tailored solutions, leveraging the expertise and integrity of our consultants and robust delivery processes.",,,,"['Power Bi', 'SSRS', 'SSIS', 'SQL Azure', 'Azure Data Factory', 'Azure Databricks', 'SQL Server', 'Tableau', 'Power Query', 'SQL', 'Onpremise', 'Azure Data Lake', 'Data Visualization', 'Dax', 'Data Modeling', 'ETL']",2025-06-11 05:59:21
Snowflake Data Engineer,Epam Systems,5 - 10 years,Not Disclosed,['Chennai'],"Key Skills:\nSnowflake (Snow SQL, Snow PLSQL and Snowpark)\nStrong Python\nAirflow/DBT\nAny DevOps tools\nAWS/Azure Cloud Skills\n\nRequirements:\nLooking for engineer for information warehouse\nWarehouse is based on AWS/Azure, DBT, Snowflake.\nStrong programming experience with Python.\nExperience with workflow management tools like Argo/Oozie/Airflow.\nExperience in Snowflake modelling - roles, schema, databases\nExperience in data Modeling (Data Vault).\nExperience in design and development of data transformation pipelines using the DBT framework.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Python', 'Azure Cloud', 'AWS', 'SQL']",2025-06-11 05:59:23
Data Engineer with GCP,Egen (Formerly SpringML),4 - 6 years,Not Disclosed,['Hyderabad( Nanakramguda )'],"Job Overview:\n\nWe are looking for a skilled and motivated Data Engineer with strong experience in Python programming and Google Cloud Platform (GCP) to join our data engineering team. The ideal candidate will be responsible for designing, developing, and maintaining robust and scalable ETL (Extract, Transform, Load) data pipelines. The role involves working with various GCP services, implementing data ingestion and transformation logic, and ensuring data quality and consistency across systems.",,,,"['GCP', 'Python', 'Azure Data Factory', 'Cloud Functions', 'IAM', 'Bigquery', 'Snowflake', 'Google Cloud Storage', 'SQL Server', 'Oracle', 'Data Integration']",2025-06-11 05:59:24
Gcp Data Engineer,One Of the MNC company,6 - 8 years,Not Disclosed,"['Hyderabad', 'Pune', 'Mumbai (All Areas)']","Role & responsibilities\n\nDevelop, implement, and optimize ETL/ELT pipelines for processing large datasets efficiently.\n• Work extensively with BigQuery for data processing, querying, and optimization.\n• Utilize Cloud Storage, Cloud Logging, Dataproc, and Pub/Sub for data ingestion, storage, and event-driven processing.\n• Perform performance tuning and testing of the ELT platform to ensure high efficiency and scalability.\n• Debug technical issues, perform root cause analysis, and provide solutions for production incidents.\n• Ensure data quality, accuracy, and integrity across data pipelines.\n• Collaborate with cross-functional teams to define technical requirements and deliver solutions.\n• Work independently on assigned tasks while maintaining high levels of productivity and efficiency.\nSkills Required:\n• Proficiency in SQL and PL/SQL for querying and manipulating data.\n• Experience in Python for data processing and automation.\n• Hands-on experience with Google Cloud Platform (GCP), particularly:\no BigQuery (must-have)\no Cloud Storage\no Cloud Logging\no Dataproc\no Pub/Sub\n• Experience with GitHub and CI/CD pipelines for automation and deployment.\n• Performance tuning and performance testing of ELT processes.\n• Strong analytical and debugging skills to resolve data and pipeline issues efficiently.\n• Self-motivated and able to work independently as an individual contributor.\n• Good understanding of data modeling, database design, and data warehousing concepts.\n\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'ETL', 'PL/SQL', 'SQL']",2025-06-11 05:59:26
Business Analyst,Capgemini,6 - 11 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","We are hiring experienced Business Analysts / Data Analysts with a strong background in Wholesale Loans, Lending Business, Capital Markets, Finance, or Risk Reporting. If you have expertise in SQL, Data Lineage, and Functional Design, wed love to connect with you!\n\nKey Responsibilities:\n\nAnalyze data requirements and identify disparate data sources for consolidation and distribution\nDocument functional specifications and collaborate with technology teams for implementation\nReview logical and conceptual data models to align with business requirements\nGather business requirements, produce business specifications, and create process flow diagrams\nPerform data tracing, lineage efforts, and validate solution implementations\nProvide production deployment support and investigate data quality issues\nWork with stakeholders to ensure completeness and accuracy of data models\n\nKey Skills & Requirements:\n\nSubject matter expertise in Wholesale Loans / Lending / Capital Markets / Finance / Risk Reporting\nProficiency in SQL, Data Analysis, and Database Management\nStrong documentation, analytical, and business modeling skills\nHands-on experience with MS Office (Excel, Visio, PowerPoint, Word)\nExperience in Data Tracing, Lineage, and Functional Design\nKnowledge of logical and physical data models\nNotice Period: Preferred 0 to 30 days joiners",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Capital Market', 'Business Analytics', 'SQL']",2025-06-11 05:59:27
MIS Analyst,Phonepe,3 - 6 years,Not Disclosed,['Bengaluru'],"Role & responsibilities:\n1. Data Collection and Integration: Gather and integrate data from multiple internal and external sources.\nEnsure data accuracy, integrity, and consistency across various data streams.\nDevelop and maintain databases and data systems necessary for projects and department functions.\n2. Data Analysis and Reporting: Analyze complex data sets to identify trends, patterns, and insights\n. Prepare detailed business views and reports, highlighting key metrics and performance indicators.\nCreate dashboards and visualizations to present data in an understandable and actionable manner.\n3. Business Metrics and Health Monitoring: Define and track key performance indicators (KPIs) to monitor business health.\nDevelop methodologies for measuring and reporting on business performance.\nRegularly update and maintain reports and dashboards to reflect current business status.\n4. Representation and Communication:\nRepresent the team in various forums, including management meetings, strategy sessions, and cross-functional working groups.\nCommunicate findings, insights, and recommendations effectively to stakeholders at all levels. Ensure that the team's work is visible and understood across the organization.\n5. Collaboration and Stakeholder Management: Work closely with different departments to understand their data needs and provide necessary support.\nCollaborate with IT and data engineering teams to ensure seamless data flow and integration. Foster strong relationships with key stakeholders to facilitate effective communication and collaboration.\n6. Continuous Improvement:\nStay updated with the latest industry trends, tools, and technologies in data analytics and business intelligence.\nPropose and implement process improvements to enhance data quality and reporting efficiency.\nParticipate in professional development opportunities to expand skillset and knowledge bas\n\n\nPreferred candidate profile",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Excel', 'MIS', 'data', 'powerbi', 'Management Information System', 'data analyst', 'sql']",2025-06-11 05:59:29
Analyst,Merck Life Science,1 - 3 years,2-4 Lacs P.A.,['Bengaluru'],"• Recommend products based on customer needs and research interests.\n• Recommend products by manually searching in the Merck and other related websites as required.\n• Do manual web searches by understanding appropriate product description of the vendor products and match the technical details from Sigma Aldrich and Merck product portfolio.\n• Prepare batch search lists and process using Competitive Intelligence Application (CIA) and other resources to provide competitor cross-references to marketing, sales, product management and other internal needs.\n• Maintain/upload cross-references in CIA and other internal databases.\nESSENTIAL JOB FUNCTIONS\n• Recommend high quality and accurate Merck Products to internal/external customers.\n• Maintain all process records/data on a real time basis.\n• Contribute to company/department operations and process improvement efforts.\n• Perform Queue Management for Tender Management Queue using SFDC.\n• Perform quality checks for the team as and when required.\nBASIC QUALIFICATIONS\nEducation: MSc. Microbiology/Biochemistry/Biotechnology/Chemistry\nExperience: 2-4 years with Sales/Marketing, R&D or Technical Services background (Broad on focus Life Science Market)\nTechnical Skills / Competencies:\n• Good knowledge of MS-Access, MS-Excel\n• Good knowledge of Life Science/Chemistry product data\n• Understanding of Salesforce.com (SFDC) Navigation\n• Proficient in Merck Website search\nBehavioral Competencies:\n• Limited Interpersonal, Problem Solving, Planning & Execution.\n• Basic time management skills.\n• Basic oral and written communication skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Excellent Communication', 'MS-Excel', 'MS-Access', 'Life Sciences', 'Pharmaceutical']",2025-06-11 05:59:31
Applied Scientist I at Fintech Platform,Talent 24/7,2 - 4 years,22.5-27.5 Lacs P.A.,[],"Preferred candidate profile\nProficient in Java, C++, Python, or similar languages.\nExperience with SQL and relational databases (e.g., Oracle, Data Warehouse).\nHands-on in building ML models or algorithms for real-world use.\nBackground in ML, deep learning, NLP, computer vision, or data science.\nKnowledge of deep learning architecture, training optimization, and model pruning.",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Computer Vision', 'Python']",2025-06-11 05:59:32
Business Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\n\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\nDo\n\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\n\na.Periodic cadence with delivery team to\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\n\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\nDeliver\n\nNo.\n\nPerformance Parameter\n\nMeasure\n1. Customer Engagement and Delivery Management PCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2. Knowledge Management No. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\n\nMandatory\n\nSkills:\nAI Cognitive.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'Knowledge Management', 'Agile-Scrum', 'flow diagrams', 'change request', 'user stories']",2025-06-11 05:59:34
Business Analyst,Zestlogic,0 - 1 years,Not Disclosed,['Pune'],"We are looking for a BE, MBA fresher for a Business Analyst role\nInterested candidates can mail or call - hr@zestlogics.com, 8897767267.",Industry Type: IT Services & Consulting,Department: Sales & Business Development,"Employment Type: Full Time, Permanent","['Decision Making', 'Time Management', 'Business Analysis', 'Communication Skills', 'Problem Analysis', 'Business Management Skills', 'Business Communication', 'Presentation Skills', 'Strategic Thinking', 'Excel', 'Written Communication', 'Organization Skills', 'Strong Analytical Skills']",2025-06-11 05:59:36
Data Science Instructor,TransOrg,1 - 4 years,Not Disclosed,['Gurugram'],"Pickl.AI (TransOrgs education brand) is looking for an instructor who is technically immersed in data science/data engineering as subjects. We are looking for a creative instructor who wants to accelerate their exposure to many areas in Machine Learning, loves wearing multiple hats and can take full ownership of their work.\n\nResponsibilities:\n• Design and deliver data science and data engineering training programs to students at Pickl.AI partner institutions\n• Teach and mentor students on topics such as data analysis, machine learning, statistics, data visualisation, and other relevant topics\n• Create and develop instructional materials, including lesson plans, presentations, assignments, and assessments\n• Keep up-to-date with the latest developments in data science and incorporate new and emerging trends into the curriculum\n• Include hands-on and relevant case studies in the topics that you are teaching\n• Provide guidance and support to students throughout their learning journey, including answering questions and providing feedback on assignments and projects\n• Collaborate with other instructors and team members to continuously improve the curriculum and training programs\n• Participate in meetings and training sessions to enhance instructional skills and techniques\n• Maintain accurate and up-to-date records of student attendance, progress, and grades\n\nRequirements:\n• Master's degree or Ph.D. in Computer Science, Data Science, Statistics, or a related field would be preferred\n• Excellent knowledge and understanding of data science concepts, techniques, and tools\n• Strong presentation and communication skills\n• Ability to work independently and in a team environment\n• Experience in teaching or mentoring students in a classroom or online setting is a plus\n• Passion for teaching and helping others learn.\n\nAbout the company: TransOrg Analytics has over a decade of specialization in machine learning and data science consulting. Pickl.AI is the education brand of TransOrg Analytics. Visit us at https://pickl.ai and www.transorg,com for details",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'tutoring', 'Education', 'Statistics', 'Machine Learning']",2025-06-11 05:59:38
Business Analyst,Phonepe,2 - 4 years,5-7 Lacs P.A.,['Bengaluru'],"Role & respon PhonePe is an Indian digital payments and financial services company headquartered in Bengaluru, Karnataka, India. It was founded in December 2015, by Sameer Nigam, Rahul Chari and Burzin Engineer. The PhonePe app, based on the Unified Payments Interface, went live in August 2016.\n\nRoles and Responsibilities:\n\n- Preparing and maintain reports showcasing daily/weekly/monthly performance reports\n- Coordination with Stakeholders and maintain reports\nReports will be around the following:\nDaily business trends at various levels\n- Monitoring the effectiveness of trade communication\n- Tracking the goodness of various consumer offers\n- Track efficiency of various process changes\n- Track effectiveness of new initiative implementation\n- Adhoc Analysis\nCoordinating with cross functional stake holders to drive execution of high growth projects.\nAnalyse and assist with reporting on all key business parameters including\nPreparing and maintain reports showcasing daily/weekly/monthly performance reports etc\nTracking efficacies of all initiatives under the business unit\nWorking with external partners to drive effective business relationship as per requirements\nSkills/Abilities:\n\n- 3-5 years of experience in MIS \n- Effective verbal, written communication and presentation skills\n- Should have basic business acumen and proactive in understanding the need of stakeholders\n- Must have zeal to learn and have positive problem-solving aptitude.\n- Must have good knowledge of MS office\n\nQualification:\n\n- 3-5 years of experience in similar role.\nMust have done professional course in MS-excel, Access, SQL",Industry Type: FinTech / Payments,Department: Other,"Employment Type: Full Time, Permanent","['Excel', 'Power Bi', 'Business Analytics', 'Data Analysis', 'Bi Tools', 'Data Visualization', 'Dashboarding', 'Data Analytics', 'Data Reporting', 'SQL']",2025-06-11 05:59:39
Business Analyst,Wipro,6 - 10 years,22.5-37.5 Lacs P.A.,['Bengaluru'],"Preferred candidate profile :\n\nBusiness Analyst (Backbase) Experience – 6 to 8 years\nMandatory Skills: Banking – Online/Mobile Banking and Digital Channels\nShould have advanced experience on Omni Channel Solution knowledge at a Retail and/or Corporate Banking environment with relevant industry/functional knowledge\nShould have worked in various Omni-channel platforms like Mobile Banking, CRM, Mobile Internet , Payments, Payment Innovation/New Channels, ATM and Kiosk solution among others",,,,"['Banking Sector', 'Banking Process', 'Internet Banking', 'Phone Banking', 'Online Banking', 'Mobile Banking']",2025-06-11 05:59:41
Senior AI Scientist,Confidential,4 - 9 years,40-80 Lacs P.A.,"['Bengaluru', 'Mumbai (All Areas)']","Design & develop end-to-end machine learning models & GenAI workflows\nFetch & process data from BigQuery & various sources for model development\nBuild & deploy models using Python & frameworks like Scikit Learn, XGBoost/CatBoost, TensorFlow & Gemini\n\nRequired Candidate profile\n4+ years of experience in the Data Science domain and should have hands on exp in Python, Machine Learning and/or Generative AI (Artificial Intelligence)\nThis is an Individual contributor role",Industry Type: NBFC,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Generative Ai', 'Artificial Intelligence', 'data scientist', 'Machine Learning', 'Python']",2025-06-11 05:59:42
"Senior Engineer, Database",Nagarro,3 - 5 years,Not Disclosed,['Chennai'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal experience 3+ years.\nHands-on working experience as a DBA managing large, mission-critical MySQL databases.\nStrong understanding of Relational Database Management Systems (RDBMS).\nProficiency in SQL (CRUD operations, Views, Materialized Views).\nDesign, develop, and optimize PL/SQL code including Packages, Procedures, Functions, Triggers, and advanced Exception Handling.\nExperience with PL/SQL code optimization techniques.\nExperience working with DB2 database engine.\nIntermediate knowledge of Unix and Shell Scripting.\nHands-on experience with IBM Data Studio.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers\nIdentifying different solutions and being able to narrow down the best option that meets the client’s requirements\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design documents explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDBMS', 'Database', 'PLSQL', 'Db2 Engine']",2025-06-11 05:59:44
Data Engineer_ Immediate Joiner Required,Healthcare technology services,6 - 7 years,15-25 Lacs P.A.,"['Pune', 'Chennai']","Role: Data Engineer\nExperience: 6-9Years\nRelevant Experience in Data Engineer: 6+ Years\nNotice Period: Immediate Joiners Only\nJob Location: Pune and Chennai\n\nKey Responsibilities:\n\nMandatory Skill: Spark,SQL and Python\n\nMust Have:\nRelevant experience of 6-9 years as a Data Engineer\nExperience in programming language like Python\nGood Understanding of ETL (Extract, Transform, Load) concepts\nGood analytical and problem-solving skills\nKnowledge of a ticketing tool like JIRA/SNOW\nGood communication skills to interact with Customers on issues & requirements.\n\nReach us:If you are interested in this position and meet the above qualifications, please reach out to me directly at swati@cielhr.com and share your updated resume highlighting your relevant experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Spark', 'Python', 'SQL']",2025-06-11 05:59:45
Snowflake Data Architect,Kasmo Digital,10 - 16 years,Not Disclosed,['Hyderabad'],"Required Skills & Qualifications:\n10-12 years of experience in data architecture, data warehousing, and cloud technologies.\nStrong expertise in Snowflake architecture, data modeling, and optimization.\nSolid hands-on experience with cloud platforms: AWS, Azure, and GCP.\nIn-depth knowledge of SQL, Python, PySpark, and related data engineering tools.\nExpertise in data modeling (both dimensional and normalized models).\nStrong experience with data integration, ETL processes, and pipeline development.\nCertification in Snowflake, AWS, Azure, or related cloud technologies.\nExperience working with large-scale data processing frameworks and platforms.\nExperience in data visualization tools and BI platforms (e.g., Tableau, Power BI).\nExperience in Agile methodologies and project management.\nStrong problem-solving skills with the ability to address complex technical challenges.\nExcellent communication skills and ability to work collaboratively with cross-functional teams.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Visualization', 'Data Modeling', 'Data Warehousing', 'SQL', 'Data Architecture', 'Python']",2025-06-11 05:59:47
MDM Data Science Manager,Amgen Inc,10 - 14 years,Not Disclosed,['Hyderabad'],"We are seeking an accomplished and visionary Data Scientist/ GenAI Lead to join Amgens Enterprise Data Management team.\nAs MDM Data Science/Manager, you will lead the design, development, and deployment of Generative AI and ML models to power data-driven decisions across business domains.\nThis role is ideal for an AI practitioner who thrives in a collaborative environment and brings a strategic mindset to applying advanced AI techniques to solve real-world problems.To succeed in this role, the candidate must have strong AI/ML, Data Science, GenAI experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have AI/ML, data science and GenAI experience on technologies like (PySpark/PyTorch, TensorFlow, LLM, Autogen, Hugging FaceVectorDB,Embeddings, RAGsetc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nDrive development of enterprise-level GenAI applications using LLM frameworks such as Langchain, Autogen, and Hugging Face.\nArchitect intelligent pipelines using PySpark, TensorFlow, and PyTorch within Databricks and AWS environments.\nImplement embedding models andmanage VectorStores for retrieval-augmented generation (RAG) solutions.\nIntegrate and leverage MDM platforms like Informatica and Reltio to supply high-quality structured data to ML systems.\nUtilize SQL and Python for data engineering, data wrangling, and pipeline automation.\nBuild scalable APIs and services to serve GenAI models in production.\nLead cross-functional collaboration with data scientists, engineers, and product teams to scope, design, and deploy AI-powered systems.\nEnsure model governance, version control, and auditability aligned with regulatory and compliance expectations.\nBasic Qualifications and Experience:\nMasters degree with 8 - 10 years of experience in Data Science, Artificial Intelligence, Computer Science, or related fields OR\nBachelors degree with 10 - 14 years of experience in Data Science, Artificial Intelligence, Computer Science, or related fields OR\nDiploma with 14 - 16 years of hands-on experience in Data Science, AI/ML technologies, or related technical domains\nFunctional Skills:\nMust-Have Skills:\n10+ years of experience working in AI/ML or Data Science roles, including designing and implementing GenAI solutions.\nExtensive hands-on experience with LLM frameworks and tools such as Langchain, Autogen, Hugging Face, OpenAI APIs, and embedding models.\nStrong programming background with Python, PySpark, and experience in building scalable solutions using TensorFlow, PyTorch, and SK-Learn.\nProven track record of building and deploying AI/ML applications in cloud environments such as AWS.\nExpertise in developing APIs, automation pipelines, and serving GenAI models using frameworks like Django, FastAPI, and DataBricks.\nSolid experience integrating and managing MDM tools (Informatica/Reltio) and applying data governance best practices.\nGuide the team on development activities and lead the solution discussions\nMust have core technical capabilities in GenAI, Data Science space\nGood-to-Have Skills:\nPrior experience in Data Modeling, ETL development, and data profiling to support AI/ML workflows.\nWorking knowledge of Life Sciences or Pharma industry standards and regulatory considerations.\nProficiency in tools like JIRA and Confluence for Agile delivery and project collaboration.\nFamiliarity with MongoDB, VectorStores, and modern architecture principles for scalable GenAI applications.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nData Science and ML Certification\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Science', 'AZURE', 'AI/ML', 'PyTorch', 'Data Analysis', 'ETL', 'AWS', 'SQL', 'TensorFlow']",2025-06-11 05:59:48
"Sr Software Eng: Generative AI, Go/Python, AWS, Kubernetes 7-12 Yrs",Cisco,7 - 12 years,Not Disclosed,['Bengaluru'],"Meet The Team\nThe Cisco AI Software & Platform Group drives the development of groundbreaking generative AI applications. We empower Cisco's diverse product portfolio, spanning networking and security, with intelligent assistants and agents. We work on pioneering technologies that proactively defend against threats, safeguard critical business assets, and simplify security operations. Fueled by a passion for AI/ML, we strive to create a secure future for businesses. Our collaborative and passionate team thrives with tackling sophisticated challenges and delivering innovative solutions.",,,,"['Golang', 'Generative Ai', 'AWS', 'Python', 'Kubernetes', 'Java']",2025-06-11 05:59:50
Gen AI- Sr. Engineer/Lead,Iris Software,7 - 12 years,Not Disclosed,"['Pune', 'Chennai', 'Delhi / NCR']","Programming Skills: Advanced proficiency in Python, with experience in AI/ML frameworks.\nAzure DevOps: Expertise in version control systems (e.g., Git) and Azure DevOps tools for automation and monitoring.\nCI/CD Pipelines: Proven ability to design and implement continuous integration and deployment workflows.\nInfrastructure as Code: Experience with ARM templates or Terraform for provisioning cloud infrastructure.\nContainerization: Strong knowledge of Docker and Kubernetes for application deployment and orchestration.\nExperience:\n8+ years of experience in software development, with 3+ years in AI/ML or Generative AI projects.\nDemonstrated experience in deploying and managing AI applications in production environments.\nProven track record of implementing DevOps best practices and automation strategies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen AI', 'Azure', 'Devops', 'Python', 'Sql']",2025-06-11 05:59:51
Data Architect,Opus Technologies,10 - 16 years,35-50 Lacs P.A.,['Pune'],"Role & responsibilities\nDefine and evolve data engineering & analytics offerings aligned with payments domain needs (e.g., transaction analytics, fraud detection, customer insights).\nLead reference architecture creation for data modernization, real-time analytics, and cloud-native data platforms (e.g., Azure Synapse, GCP BigQuery).\nBuild reusable components, PoCs, and accelerators for ingestion, transformation, data quality, and governance.\nSupport pre-sales engagements with solution design, estimation, and client workshops.\nGuide delivery teams on data platform implementation, optimization, and security.\nMentor and upskill talent pool through learning paths and certifications.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Spark', 'Python', 'Azure Data Factory']",2025-06-11 05:59:52
Cerner Analyst,Tech Mahindra,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Urgent Requirement .... Tech Mahindra Hiring\n\nEXP: 4 to 12yrs\nShift: 3:30pm to 12:30am / 5.30pm to 2.30am / 6.30pm to 3.30am\nWork Location : Bangalore / Hyderabad / Pune\nWork Model : Hybrid (12 days in a month)\n\nPlease find below JD details.\n\n•        Cerner Application support, Incident resolution, Implementation of Cerner Millennium Projects.\n•        Experience in configuring and troubleshooting CERNER solution functionalities/Components.\n•        Perform complex troubleshooting investigations and documenting notes and knowledge articles.\n•        Gather requirements and determine scope of work and plan for on time delivery.\n•        Ability to work self-sufficiently on assigned time sensitive tasks.\n•        Develop and maintain good relationship with peers and client, provide timely feedback to encourage success.\n•        Strong communication skills with excellent interpersonal skills both in written and verbal correspondence.\n•        Ability to learn and adapt to changing landscape and acquire new skills with technology advancement and to work, coordinate with global teams.\n•        Readiness towards work at odd hours/on-call and weekends as and when needed.\n\n\n\nBuild & Configuration Experience required in following Cerner Applications :\n\nCCL reporting\nMS4\nFSI\nCharges\nPatient accounting\nSecurity\nProvider, Ambulatory\nRegistration\nCerner Rules (Custom Solution Development)\nScheduling\nSoarian Financials\nInvision\nHIM\n\n\nShare profiles to mm00828174@techmahindra.com",Industry Type: IT Services & Consulting,Department: Healthcare & Life Sciences,"Employment Type: Full Time, Permanent","['HIM', 'Patient Accounting', 'Invision', 'Soarian', 'Registration', 'Scheduling', 'MS4']",2025-06-11 05:59:54
Master Data Management Data Architect,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible for designing, building, maintaining, analyzing, and interpreting data deliver actionable insights that drive business decisions. This role involves working with large datasets, developing reports, supporting and driving data governance initiatives, and visualizing data to ensure data is accessible, reliable, and efficiently managed. The ideal candidate has deep technical skills and provides administration support for Master Data Management (MDM) and Data Quality platform, including solution architecture, inbound/outbound data integration (ETL), Data Quality (DQ), and maintenance/tuning of match rules.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing\nCollaborate and communicate with MDM Developers, Data Architects, Product teams, Business SMEs, and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve complex data-related challenges\nAdhere to standard processes for coding, testing, and designing reusable code/component\nParticipate in sprint planning meetings and provide estimations on technical implementation\nAs a SME, work with the team on MDM related product installation, configuration, customization and optimization\nResponsible for the understanding, documentation, maintenance, and additional creation of master data related data-models (conceptual, logical, and physical) and database structures\nReview technical model specifications and participate in data quality testing\nCollaborate with Data Quality & Governance Analyst and Data Governance Organization to monitor and preserve the master data quality\nCreate and maintain system specific master data data-dictionaries for domains in scope\nArchitect MDM Solutions, including data modeling and data source integrations from proof-of-concept through development and delivery\nDevelop the architectural design for Master Data Management domain development, base object integration to other systems and general solutions as related to Master Data Management\nDevelop and deliver solutions individually or as part of a development team\nApproves code reviews and technical work\nMaintains compliance with change control, SDLC and development standards\nContribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience.\nPreferred Qualifications:\nExpertise in architecting and designing Master Data Management (MDM) solutions.\nPractical experience with AWS Cloud, Databricks, Apache Spark, workflow orchestration, and optimizing big data processing performance.\nFamiliarity with enterprise source systems and consumer systems for master and reference data, such as CRM, ERP, and Data Warehouse/Business Intelligence.\nAt least 2 to 3 years of experience as an MDM developer using Informatica MDM or Reltio MDM, along with strong proficiency in SQL.\n\n\nGood-to-Have Skills:\nExperience with ETL tools such as Apache Spark, and various Python packages related to data processing, machine learning model development.\nGood understanding of data modeling, data warehousing, and data integration concepts.\nExperience with development using Python, React JS, cloud data platforms.\nCertified Data Engineer / Data Analyst (preferred on Databricks or cloud environments).\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Business Intelligence', 'Data Warehouse', 'cloud data platforms', 'Databricks', 'ETL', 'React JS', 'Python']",2025-06-11 05:59:56
Cloud Solution Delivery Sr Advisor,"NTT DATA, Inc.",5 - 10 years,Not Disclosed,['Bengaluru'],"Req ID: 306668\n\nWe are currently seeking a Cloud Solution Delivery Sr Advisor to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n Position Overview  We are seeking a highly skilled and experienced Lead Data Engineer to join our dynamic team. The ideal candidate will have a strong background in implementing data solutions using AWS infrastructure and a variety of core and supplementary technologies; leading teams and directing engineering workloads. This role requires a deep understanding of data engineering, cloud services, and the ability to implement high quality solutions.\n\n\n\n Key Responsibilities  \n\nLead and direct a small team of engineers engaged in\n\n- Engineer end-to-end data solutions using AWS services, including Lambda, S3, Snowflake, DBT, Apache Airflow\n\n- Cataloguing data\n\n- Collaborate with cross-functional teams to understand business requirements and translate them into technical solutions\n\n- Providing best in class documentation for downstream teams to develop, test and run data products built using our tools\n\n- Testing our tooling, and providing a framework for downstream teams to test their utilisation of our products\n\n- Helping to deliver CI, CD and IaC for both our own tooling, and as templates for downstream teams\n\n- Use DBT projects to define re-usable pipelines\n\n\n\n\n\n Required Skills and Qualifications  \n\n\n\n- Bachelor's degree in Computer Science, Engineering, or related field\n\n- 5+ years of experience in data engineering\n\n- 2+ years of experience inleading a team of data engineers\n\n- Experience in AWS cloud services\n\n- Expertise with Python and SQL\n\n- Experience of using Git / Github for source control management\n\n- Experience with Snowflake\n\n- Strong understanding of lakehouse architectures and best practices\n\n- Strong problem-solving skills and ability to think critically\n\n- Excellent communication skills to convey complex technical concepts to both technical and non-technical stakeholders\n\n- Strong use of version control and proven ability to govern a team in the best practice use of version control\n\n- Strong understanding of Agile and proven ability to govern a team in the best practice use of Agile methodologies\n\n\n\n Preferred Skills and Qualifications  \n\n- An understanding of Lakehouses\n\n- An understanding of Apache Iceberg tables\n\n- An understanding of data cataloguing\n\n- Knowledge of Apache Airflow for data orchestration\n\n- An understanding of DBT\n\n- SnowPro Core certification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'cloud services', 'data engineering', 'sql', 'agile', 'snowflake', 'kubernetes', 'github', 'aws iam', 'version control', 'solution delivery', 'microsoft azure', 'docker', 'ansible', 'git', 'apache', 'java', 'lambda expressions', 'aws cloud', 'devops', 'linux', 'jenkins', 'aws', 'agile methodology']",2025-06-11 05:59:57
Machine Learning Scientist,Glynac,2 - 7 years,6-12 Lacs P.A.,['Bengaluru'],Responsibilities:\n* Develop machine learning models using PyTorch.\n* Optimize model performance through data analysis and experimentation.\n* Collaborate with cross-functional teams on product development.\n\n\nWork from home,Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Pytorch', 'Artificial Intelligence', 'Natural Language Processing']",2025-06-11 05:59:59
Business Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\nDeliver\nNo.Performance ParameterMeasure\n1.Customer Engagement and Delivery Management\nPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2.Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: Agile DevSecOps Consulting.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'DevSecOps', 'documentation', 'Agile', 'Customer Engagement', 'Delivery Management']",2025-06-11 06:00:01
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Hyderabad'],"Role Purpose\n\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\nDo\n\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\n\na.Periodic cadence with delivery team to\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\n\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\nDeliver\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Customer Engagement and Delivery Management PCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated. 2. Knowledge Management No. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\n\nMandatory\n\nSkills:\nAgile-Scrum.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'Knowledge Management', 'Agile-Scrum', 'flow diagrams', 'change request', 'user stories']",2025-06-11 06:00:03
Business Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1. Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2. Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: Telco Processes.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Telco Processes', 'User Acceptance Testing', 'RFP/ RFI', 'Project management', 'Integration Testing', 'Business Analysis']",2025-06-11 06:00:04
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Pune'],"Role Purpose\n\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\nDo\n\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\n\na.Periodic cadence with delivery team to\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\n\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\nDeliver\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Customer Engagement and Delivery Management PCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated. 2. Knowledge Management No. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\n\nMandatory\n\nSkills:\neCommerce Consulting.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['eCommerce Consulting', 'Knowledge Management', 'Agile-Scrum', 'flow diagrams', 'change request', 'user stories']",2025-06-11 06:00:06
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Pune'],"Role Purpose\n\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\nDo\n\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\n\na.Periodic cadence with delivery team to\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\n\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\nDeliver\n\nNo.\n\nPerformance Parameter\n\nMeasure\n1. Customer Engagement and Delivery Management PCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2. Knowledge Management No. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\n\nMandatory\n\nSkills:\nAgile-Scrum.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'Knowledge Management', 'Agile-Scrum', 'flow diagrams', 'change request', 'user stories']",2025-06-11 06:00:08
Technical Lead - L1,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\n\nDo\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\n\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLAs\n\n\n\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure\n1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT\n2Team ManagementProductivity, efficiency, absenteeism\n3Capability developmentTriages completed, Technical Test performance\nMandatory Skills: Data Engineering Full Stack.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Team Management', 'technical support', 'Technical leadership', 'troubleshooting', 'operational excellence']",2025-06-11 06:00:10
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1. Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2. Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: eCommerce Consulting.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['eCommerce Consulting', 'User Acceptance Testing', 'RFP/ RFI', 'Project management', 'Integration Testing', 'Business Analysis']",2025-06-11 06:00:11
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Mumbai'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1. Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2. Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: Business Analysis.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'User Acceptance Testing', 'RFP/ RFI', 'Project management', 'Integration Testing']",2025-06-11 06:00:13
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1. Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2. Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: Agile-Scrum.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Agile', 'User Acceptance Testing', 'RFP/ RFI', 'Project management', 'Integration Testing', 'Scrum', 'Business Analysis']",2025-06-11 06:00:14
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1. Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2. Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: ServiceNow Core.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ServiceNow Core', 'User Acceptance Testing', 'RFP/ RFI', 'Project management', 'Integration Testing', 'Business Analysis']",2025-06-11 06:00:16
Technology Lead,Infosys,5 - 8 years,Not Disclosed,['Bengaluru'],"Responsibilities\nJob Responsibilities:\n* Lead the development of Gen AI solutions, including design, implementation, and deployment of AI models and systems\n* Collaborate with cross-functional teams to identify business problems and develop AI-powered solutions to drive growth and efficiency\n* Design and implement scalable and efficient AI solutions, leveraging technologies such as LangChain, Agentic AI, RAG, Event driven architecture using Kafka etc.\n* Develop and maintain large-scale AI systems, ensuring high performance, reliability, and security\n* Lead and mentor a team of AI developers, providing guidance and expertise to ensure high-quality deliverables\n* Stay up-to-date with the latest advancements in Gen AI, Prompt Engineering, and related technologies, applying this knowledge to drive innovation and improvement\n* Develop and maintain technical documentation, including architecture diagrams, design documents, and technical guides\n* Participate in code reviews, ensuring high-quality code and adherence to coding standards and best practices\nTechnical and Professional Requirements:\nPreferred Qualifications:* Experience with Agentic Frameworks such LangGraph, AutoGen, CrewAI* Experience with cloud-based AI platforms, such as AWS or Azure* Knowledge of containerization technologies, such as Docker* Familiarity with agile development methodologies, such as Scrum or Kanban* Experience with AI-related tools and frameworks, such as TensorFlow or PyTorch* Strong understanding of software design patterns, principles, and best practices* Experience with DevOps practices, including continuous integration and continuous deployment (CI/CD)* Certification in AI, machine learning, or related fields, such as Certified Data Scientist or Certified AI Engineer\nPreferred Skills:\nTechnology->Artificial Intelligence->Artificial Intelligence - ALL\nTechnology->Machine Learning->GoLearn\nTechnology->Machine Learning->Generative AI\nAdditional Responsibilities:\nRequired Qualifications:* Bachelor's or Master's degree in Computer Science, Engineering, or related field (B.E/B.Tech/M.E/M.Tech/MCA)* At least 5-8 years of experience in software development, with a minimum of 2 years of experience in Gen AI* Strong proficiency in LangChain, Python, Gen AI, Agentic AI, and Prompt Engineering* Excellent communication, teamwork, problem-solving, and leadership skillsTech Skill: LangChain, Python, Fast/Flask API, Gen AI, Agentic AI, Advanced Prompt Engineering, Machine Learning, SQL, KafkaSoft Skill: Communication, Team Work, Problem Solving\nEducational Requirements\nBachelor of Engineering\nService Line\nInformation Systems\n* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AI models', 'coding', 'Kanban', 'Azure', 'PyTorch', 'Docker', 'Kafka', 'CI/CD', 'Scrum', 'machine learning', 'AWS', 'TensorFlow']",2025-06-11 06:00:17
Business Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1. Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2. Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: Payments and Cards-Awards.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Payments and Cards', 'User Acceptance Testing', 'RFP/ RFI', 'Project management', 'Integration Testing', 'Business Analysis']",2025-06-11 06:00:19
Business Analyst,IGT Solutions,6 - 11 years,20-27.5 Lacs P.A.,['Gurugram'],"Role & responsibilities:\nExperience in Aviation Domain including Flight Operations and OCC.\nReasonable understanding of crew Management, Flight Operations, Training Planning , Manpower Planning, Fleet Planning & Tracking, Crew Planning & Tracking.\nResponsible for:\nExamining and evaluating current systems.\nIdentifying system requirements.\nLiaising with users to track additional requirements and features.\nLearn and help in software development and documentation and provide test data to Development team.\nCreating BRDs, ICD, business solutions design document.\nFeasibility of solutions\nKnowledge of business information systems.\nExcellent analytical skills.\nA good problem-solving attitude.\nShould have self-learning attitude.\nResponsible for defining business problem, business objectives and key business requirements,\nModelling requirements to restate and clarify them.\nStudy requirements feasibility to determine the requirements are technically, operationally, and economically viable.\nTrade off requirements to determine most feasible requirement alternative.\nAssessing requirements for associated risks and constraints.\nPrioritize requirements based on business need and development capacity available within Digital.\nShould understand the SDLC process and able to create Use cases, scenarios, and test data in Operations systems.\nResponsible for driving the requirements process, scheduling meetings, merging input, influencing stakeholders, and ensuring decisions are made.\nSupport the solution manager with scoping and baselining of solution and BAU requirements with respect to Systems.\nHands on experience with user acceptance testing and working knowledge of all phases of SDLC.\nCoordinate the development of all approved versions of business and functional specifications.\nExpertise with requirement and project related documentation and ability to build and manage document repository. Knowledge of document management standards preferred\nAbility to lead and support cross functional business teams as and when required as part of business solutioning exercise.\nAbility to work independently and liaise with cross functional teams.\nAny Certification will be a plus.\nAbility to work with senior level stakeholders of large organization.\nWork collaboratively with development partners and complete deliverables.",,,,"['Manpower Planning', 'crew Management', 'Flight Operations', 'Project Management', 'Business Analysis', 'Project', 'Word', 'Excel', 'Fleet Planning & Tracking', 'Crew Planning & Tracking', 'Visio', 'OCC']",2025-06-11 06:00:21
Business Analyst,.,4 - 7 years,Not Disclosed,['Pune'],"Role Overview:\n\nThis hybrid role sits within the Distribution Data Stewardship Team and combines operational and technical responsibilities to ensure data accuracy, integrity, and process optimization across sales reporting functions.\n\nKey Responsibilities:\n\nSupport sales reporting inquiries from sales staff at all levels.\nReconcile omnibus activity with sales reporting systems.\nAnalyze data flows to assess impact on commissions and reporting.\nPerform data audits and updates to ensure integrity.\nLead process optimization and automation initiatives.\nManage wholesaler commission processes, including adjustments and manual submissions.\nOversee manual data integration from intermediaries.\nExecute territory alignment changes to meet business objectives.\nContribute to team initiatives and other responsibilities as assigned.\nGrowth Opportunities:\n\nExposure to all facets of sales reporting and commission processes.\nOpportunities to develop project and relationship management skills.\nPotential to explore leadership or technical specialist roles within the firm.\nQualifications:\n\nBachelors degree in Computer Engineering or a related field.\n4–7 years of experience with Python programming and automation.\nStrong background in SQL and data analysis.\nExperience in relationship/customer management and leading teams.\nExperience working with Salesforce is a plus.\nRequired Skills:\n\nTechnical proficiency in Python and SQL.\nStrong communication skills and stakeholder engagement.\nHigh attention to data integrity and detail.\nSelf-directed with excellent time management.\nProject coordination and documentation skills.\nProficiency in MS Office, especially Excel.",Industry Type: Investment Banking / Venture Capital / Private Equity,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Advanced Python', 'Brd', 'FRD', 'Complex Queries', 'Advance Sql', 'Python Scripting', 'Python', 'SQL']",2025-06-11 06:00:22
Business Analyst,Software Company,5 - 8 years,Not Disclosed,['Hyderabad'],Business Analyst :\n\nMust have Strong Communication with Good and quick understanding of Product Vision and Domain.\nAWS Cloud knowledge is good to have\n\nLocation : Hyderabad\nExperience - 5 to 8\nNotice Period : Immediate joiners,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'Product Vision', 'Aws Cloud', 'Requirement Gathering', 'User Stories', 'product domain', 'Requirement Analysis', 'Product Ownership']",2025-06-11 06:00:24
Business Analyst,Global Banking Organization,12 - 15 years,Not Disclosed,['Pune'],"Key Skills: Lending, Business Analyst, Lending Operations, Business Analysis, Retail, Retail Banking Operations\nRoles and Responsibilities:\nUnderstand and manage the full project lifecycle within banking and financial institutions.\nApply change adoption techniques to support transformation initiatives.\nExecute projects using Agile methodology with tools like Clarity, JIRA, MS Project, and Confluence.\nAnalyze and drive changes that benefit the bank, its customers, and stakeholders in the retail banking space.\nProvide expertise in lending products and retail banking concepts.\nSupport the design and enhancement of products, propositions, rewards, and partnerships.\nManage and support large, complex, multi-country retail banking projects.\nEngage with senior stakeholders to influence and drive project decisions.\nSolve problems efficiently while adhering to tight deadlines in fast-paced environments.\nTrack and report project status, risks, issues, and milestones.\nMaintain excellent communication and interpersonal relationships with all project stakeholders.\nOversee budget and financial management of projects.\nPromote consistency by sharing best practices across teams for managing complex, multi-market projects.\nSkills Required:\n10+ years of experience in project management in the banking and financial institutions.\nIn depth understanding of the project lifecycle.\nProficient in change adoption techniques.\nIn depth knowledge and experience of project execution in agile framework with extensive hands-on experience in tools such as Clarity/ JIRA/ MS Project/ confluence etc.\nStrong understanding of Retail banking & how change drives benefits for bank, customers and other stakeholders.\nStrong understanding of Retail banking concept specifically lending products.\nGood understating of products & propositions, rewards and partnerships.\nRelevant experience of working in complex retail banking projects across countries or regions.\nAbility to interact and influence senior stakeholders to drive decisions in achieving desired project outcome.\nProblem solving ability with adherence to stringent timelines in fast paced environment.\nExperience in project tracking (setting up project plan, managing risk and issue log, tollgates, reporting and governance)\nOutstanding communication and interpersonal skills.\nProficient in financial and budget management.\nDriving consistency & lead team/s by sharing best practices around how complex & multi market projects are defined, managed, and monitored.\nWhat additional skills will be good to have?\nPMP / Prince 2 / MSP/ PgMP/ Leading SAFe Certifications\nEducation: Bachelor's Degree in related field",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Retail', 'Lending', 'Business Analyst', 'Lending Operations', 'Retail Banking Operations', 'Business Analysis']",2025-06-11 06:00:26
Business Analyst,Global Banking Organization,12 - 15 years,Not Disclosed,['Pune'],"Key Skills: Business Analyst, Financial Accounting, Account Management, Business Analysis, Accounting, Retail Banking Operations\nRoles and Responsibilities:\nGather business requirements and support design thinking for business, operations, and tech projects.\nWork in Agile teams and manage the full requirement lifecycle.\nUnderstand and improve account opening and servicing processes.\nSupport mobile journeys for account opening and management.\nCollaborate with cross-functional teams and follow HSBC's work practices.\nPlan and manage change processes to ensure smooth transitions.\nCommunicate effectively with team members and stakeholders.\nCreate clear documentation and process flow diagrams.\nGuide and support other business analysts.\nSkills Required:\n12+ years of experience in business analysis and most recent experience preferably in the banking and financial domain.\nStrong business analysis, requirements gathering and design thinking skills with a mix of business, operations and technology focused projects.\nAgile expertise, requirement life cycle management and traceability, experience in digital transformation projects in a global banks/consulting firm/ Financial industry\nStrong understanding of Customer Account opening and Servicing journeys.\nStrong domain experience in Mobile STP journeys for Account opening / Account management.\nAdapt and adhere to the HSBCs ways of working and collaborate with array of stakeholders effectively and inclusively and liaison with cross functional team for programme execution.\nExperience in Change management and Change Adoption processes - Plan and implement change intervention to enable smooth transition and embed changes and transition to business as usual, from requirements gathering, communications through to training the final user.\nEffective communication, inter-personal and negotiating skills.\nKnowledge of MS Office and business analysis tools and techniques, knowledge of JIRA and Confluence tools.\nCreate required artefacts and to expected standard (e.g. Behavior Driven Development (BDD) in user stories, end-to-end flow diagrams with touchpoints, Confluence documentation for requirements traceability).\nDemonstrate Leadership, support, coaching and development for Analysts.\nLead planning analysis activity with optimal use of resources to help define and track metrics and KPIs for the product.\nPromote the Scaled delivery approach for multimarket implementation and use of customer, product, and operational procedural insights to optimise experience and propositions.\nEducation: Bachelor's Degree in related field",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analyst', 'Accounting', 'Financial Accounting', 'Account Management', 'Business Analysis', 'Retail Banking Operations']",2025-06-11 06:00:27
Business Analyst,Global Banking Organization,8 - 12 years,Not Disclosed,['Pune'],"Key Skills: Business Analyst, Digital Channels, Business Analysis, Jira, Digital Transformation, Agile\nRoles and Responsibilities:\nAnalyze and gather business requirements, utilizing design thinking for business, operations, and technology-focused projects.\nWork with Agile methodology and manage the full requirement lifecycle and traceability.\nSupport digital transformation projects, especially within global banks, consulting firms, or the financial industry.\nLeverage asset management, transformation skills, and knowledge of SQL for process mapping and data analysis.\nAdapt and adhere to Company's work practices while collaborating with stakeholders and cross-functional teams for program execution.\nManage change adoption and ensure smooth transitions by planning and implementing change interventions, from requirements gathering to user training.\nDemonstrate excellent communication, interpersonal, and negotiation skills.\nUtilize MS Office and business analysis tools like JIRA and Confluence for project management and documentation.\nCreate artifacts to expected standards (e.g., Behavior Driven Development (BDD) in user stories, end-to-end flow diagrams, and Confluence documentation for requirements traceability).\nLead, coach, and develop analysts in the team.\nPlan and track project analysis activity, ensuring the optimal use of resources and defining metrics and KPIs for the product.\nPromote scaled delivery approaches for multi-market implementations and optimize customer, product, and operational procedures.\nSkills Required:\n8+ years of experience in business analysis and most recent experience preferably in the banking and financial domain.\nStrong business analysis, requirements gathering and design thinking skills with a mix of business, operations and technology focused projects.\nAgile expertise, requirement life cycle management and traceability, experience in digital transformation projects in a global banks/consulting firm/ Financial industry\nAsset management, Transformation skills (BA skills, Process Mapping, Data analysis and knowledge of SQL, Agile- experience with Agile Ways of Working, JIRA, Confluence)\nAdapt and adhere to the COMPANYs ways of working and collaborate with array of stakeholders effectively and inclusively and liaison with cross functional team for programme execution.\nExperience in Change management and Change Adoption processes - Plan and implement change intervention to enable smooth transition and embed changes and transition to business as usual, from requirements gathering, communications through to training the final user.\nEffective communication, inter-personal and negotiating skills.\nKnowledge of MS Office and business analysis tools and techniques, knowledge of JIRA and Confluence tools.\nCreate required artefacts and to expected standard (e.g. Behavior Driven Development (BDD) in user stories, end-to-end flow diagrams with touchpoints, Confluence documentation for requirements traceability).\nDemonstrate Leadership, support, coaching and development for Analysts.\nLead planning analysis activity with optimal use of resources to help define and track metrics and KPIs for the product.\nPromote the Scaled delivery approach for multimarket implementation and use of customer, product, and operational procedural insights to optimise experience and propositions.\nCertified Business Analysis Professional (CBAP) If not held, you would be expected to work towards the qualification\nAgile Certifications\nDesign Thinking\nEducation: Bachelor's Degree in related field",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analyst', 'Digital Transformation', 'Jira', 'Digital Channels', 'Business Analysis', 'Agile']",2025-06-11 06:00:28
Cloud Solution Delivery Lead Consultant,"NTT DATA, Inc.",8 - 13 years,Not Disclosed,['Bengaluru'],"We are currently seeking a Cloud Solution Delivery Lead Consultant to join our team in bangalore, Karntaka (IN-KA), India (IN).\n\n\nData Engineer Lead Robust hands-on experience with industry standard tooling and techniques, including SQL, Git and CI/CD pipelinesmandiroty Management, administration, and maintenance with data streaming tools such as Kafka/Confluent Kafka, Flink Experienced with software support for applications written in Python & SQL Administration, configuration and maintenance of Snowflake & DBT Experience with data product environments that use tools such as Kafka Connect, Synk, Confluent Schema Registry, Atlan, IBM MQ, Sonarcube, Apache Airflow, Apache Iceberg, Dynamo DB, Terraform and GitHub Debugging issues, root cause analysis, and applying fixes Management and maintenance of ETL processes (bug fixing and batch job monitoring)Training & Certification ""¢\nApache Kafka Administration\n\nSnowflake Fundamentals/Advanced Training\n""¢ Experience 8 years of experience in a technical role working with AWSAt least 2 years in a leadership or management role\n\n\n",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'ci/cd', 'python', 'kafka', 'aws', 'hive', 'hibernate', 'sql', 'microservices', 'spring', 'git', 'apache', 'java', 'spark', 'debugging', 'hadoop', 'snowflake', 'github', 'mq series', 'dynamo db', 'sonarqube', 'airflow', 'solution delivery', 'apache flink', 'spring boot', 'kafka streams', 'terraform']",2025-06-11 06:00:30
Hiring For Gen AI !!,HCLTech,5 - 10 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']","RESPONSIBILITIES:\nDevelop and contribute to end-to-end architecture of highly scalable, distributed machine learning solutions for AI/ML/DL/NLP platforms.\nContribute to the research and development of advanced generative AI models such as LLMs, SLM’s including GANs, VAEs, autoregressive models, and novel architectures.\nDeployment of generative AI models, frameworks, and algorithms into scalable REST API services.",,,,"['Generative Ai', 'Artificial Intelligence', 'Natural Language Processing', 'Machine Learning', 'Deep Learning']",2025-06-11 06:00:32
Technical Lead - L1,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\n\n\n\n\nDo\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\n\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLAs\n\n\n\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n\nDeliver\nNoPerformance ParameterMeasure\n1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT\n2Team ManagementProductivity, efficiency, absenteeism\n3Capability developmentTriages completed, Technical Test performance\nMandatory Skills: Data Engineering Full Stack.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Team Management', 'technical support', 'Technical leadership', 'troubleshooting', 'operational excellence']",2025-06-11 06:00:33
Artificial Intelligence Developer,Infosys,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Thiruvananthapuram']","Role & responsibilities\nArchitect and implement AI/ML solutions tailored to client-specific business problems using Gen AI (e.g., LLMs like GPT, Gemini, Mistral) and traditional ML models (e.g., scikit-learn, XGBoost).\nCollaborate with client stakeholders to understand requirements, define problem statements, and translate them into scalable AI/ML solutions.\nPresent demos and proof-of-concepts (POCs) to clients, showcasing the value of AI/ML in real-world scenarios\nMentor and guide a cross-functional team of data scientists, ML engineers, and developers\nDrive best practices in model development, deployment, and monitoring\nStay abreast of the latest advancements in Gen AI and ML, and evaluate their applicability to client use cases\nContribute to internal knowledge bases and reusable solution accelerators\nPreferred candidate profile\nStrong programming skills in Python and experience with ML libraries (e.g., TensorFlow, PyTorch, scikit-learn).\nHands-on experience with Gen AI platforms and LLMs (e.g., OpenAI, Gemini, LLaMA, Mistral).\nProven track record of deploying ML models in production environments (cloud/on-prem).\nExperience with API development, data pipelines, and dashboarding tools like Streamlit.\nFamiliarity with DevOps and MLOps practices for model lifecycle management.\nExcellent communication and stakeholder management skills.\nTools:\nLLM, SLM, Vector DB, Graph DB, Airflow, MLFlow, MLOps tools, NLP, KG, ML models (Regression, Clustering, Classification etc), LangChain, LangGraph, AutoGen",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Agentic Ai', 'Knowledge Graphs', 'Generative Ai', 'Machine Learning', 'Deep Learning', 'RAG', 'Multimodal models', 'Computer Vision']",2025-06-11 06:00:35
Technical Lead - L1,Wipro,5 - 8 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\n\n\n\n\nDo\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\n\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLAs\n\n\n\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure\n1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT\n2Team ManagementProductivity, efficiency, absenteeism\n3Capability developmentTriages completed, Technical Test performance\nMandatory Skills: DataBricks - Data Engineering.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'DataBricks', 'hive', 'python', 'technical leadership', 'team management', 'spark', 'troubleshooting', 'hadoop', 'big data', 'sql']",2025-06-11 06:00:37
GEN AI Engineer,HCLTech,3 - 8 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']",Skill Needed\nJob:\nDefine the Agentic Function for Industrial Quality Inspection\nWorkflow design\nBring up the LLM and AI baseline framework -open source (Llava),,,,"['GEN AI', 'RAG', 'LLM', 'Open source', 'ML', 'Tensorflow']",2025-06-11 06:00:39
Proactive Hiring For Databricks,HCLTech,5 - 10 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']","Responsibilities\nLead the design, development, and implementation of big data solutions using Apache Spark and Databricks.\nArchitect and optimize data pipelines and workflows to process large volumes of data efficiently.\nUtilize Databricks features such as Delta Lake, Databricks SQL, and Databricks Workflows to enhance data processing and analytics capabilities.",,,,"['apache spark', 'Databricks Engineer', 'SQL']",2025-06-11 06:00:40
Business Analyst,R Systems International,4 - 8 years,20-25 Lacs P.A.,['Noida'],"We are seeking a skilled and detail-oriented Business Analyst with expertise in payroll systems to support the seamless migration of payroll data from an existing platform to a new system. This role involves collaborating with HR, IT, and external vendors to ensure data integrity, compliance, and operational continuity throughout the migration process.\n\nRoles and Responsibilities\nPayroll Data Analysis & Mapping: Collaborate with OSC and GPMS teams to analyze current payroll data structures and define mapping for OSC to GPMS. Ensure that all data fields are accurately translated and validated.",,,,"['data analysis', 'software testing', 'data validation', 'test case execution', 'documentation', 'test cases', 'business analysis', 'data integrity', 'data migration', 'transformation', 'technical hiring', 'data cleansing', 'system', 'system integration', 'payroll processing', 'recruitment', 'payroll']",2025-06-11 06:00:42
Senior Principal Engineer - IT Business Analysis,Mercer,5 - 8 years,11-16 Lacs P.A.,['Mumbai (All Areas)'],"Job Description For Posting\nWe are seeking a talented individual to join our team at Marsh .This role will be based in Mumbai .This is a hybrid role that has a requirement of working at least three days a week in the office.\n\nSenior Principal Engineer - IT Business Analysis\n\nWe will count on you to:\nBe a highly motivated team player working within MMC Agile culture, within a specified Agile framework of Scrum or KANBAN and maintain a willingness for continuously improving your agile mindset. \nWork with the Product Owner to communicate the product vision, roadmap, value, and MVP to the Agile team to enable empathy and a shared understanding thereby helping the team to formulate an appropriate solution. \nCollaborate with the Pod Leadership and Product Owner to create Personas, Story Maps, and a Release Plan for the project.\nWork with the Product Owner to communicate the product vision, roadmap, value, and MVP to the agile teams to enable empathy and a shared understanding thereby helping the team to formulate an appropriate solution.\nWork in partnership with the Product Owner and agile teams in the creation and maintenance of Product Backlog Items ensuring that Epics and User Stories are continuously prioritized and aligned to the Product Roadmap and MVP.\nFacilitate refinement sessions with the Agile Teams and Business to sufficiently detail out User Stories, to include dependencies.\nFacilitate the Sprint Review ceremony by working with the agile teams, Product Owner, Business and Customer to review, assess and adapt the latest product increment by incorporating customer insights and feedback into the Product Backlog.\nActively work with Pod Leadership by running assessments processes for the POD and provide constructive feedback towards continuously improving the Systems Analyst function, standards and processes in Global Technology.\nFacilitate discussions and collaborate with data engineers, data architects, technical experts and AI engineers to integrate AI solutions into application design.\nDemonstrate an awareness of MMC and Mercer Technical, Security and Process Standards and work with the team to incorporate them in product delivery through the software development life cycle. \n\nWhat you need to have:\nHighly motivated candidate who is inquisitive, a rapid learner that is comfortable working as part of a remote team. \nPossess strong communication skills with the capability of working collaboratively within the organization, regardless of boundaries.\nAn effective communicator for both technical and business-oriented audiences.  \nDemonstrated requirements gathering skills showcasing the capture of customer needs and business drivers using a variety of techniques into product backlog items such as Epics and User Stories.\nProven quantitative, analytical, and problem-solving skills.\nAbility to find resolutions regarding own work methods requiring minimal direction.  \nBe willing to respond to emergent changes rather than focused on existing plans. \nFamiliarity with AI concepts and technologies, including machine learning, natural language processing, and data analytics, while ensuring compliance with AI governance frameworks. Leverage these technologies to enhance user experiences and improve decision-making processes within applications.\n\nWhat makes you stand out?\nAbility to craft effective prompts that optimize AI responses, enhancing the functionality of AI-driven applications for problem-solving, analysis, and content generation.\nStay open to learning about emerging AI tools and methodologies.\nAn Agile Mindset with an in-depth understanding of Agile Principles.\nPrior experience as an IT Systems Analyst or IT Business Analyst working as part of an agile team using an Agile Workflow tool such as JIRA or TFS. \nAgile Certification, such as CSM, PSM, CSPO, PSPO, PMI-ACP, Certified SAFe Practitioner, Azure AI Fundamentals is desirable.\n\n\nMarsh McLennan (NYSE: MMC) is a global leader in risk, strategy and people, advising clients in 130 countries across four businesses: Marsh, Guy Carpenter, Mercer and Oliver Wyman. With annual revenue of $24 billion and more than 90,000 colleagues, Marsh McLennan helps build the confidence to thrive through the power of perspective. For more information, visit marshmclennan.com, or follow on LinkedIn and X.\n\nMarsh McLennan is committed to embracing a diverse, inclusive and flexible work environment. We aim to attract and retain the best people and embrace diversity of age, background, caste, disability, ethnic origin, family duties, gender orientation or expression, gender reassignment, marital status, nationality, parental status, personal or social status, political affiliation, race, religion and beliefs, sex/gender, sexual orientation or expression, skin color, or any other characteristic protected by applicable law.\n\nMarsh McLennan is committed to hybrid work, which includes the flexibility of working remotely and the collaboration, connections and professional development benefits of working together in the office. All Marsh McLennan colleagues are expected to be in their local office or working onsite with clients at least three days per week. Office-based teams will identify at least one anchor day” per week on which their full team will be together in person.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'Use Cases', 'Requirement Gathering']",2025-06-11 06:00:43
Opening For Senior Machine Learning Engineer with Fareportal,Fareportal,2 - 5 years,Not Disclosed,['Gurugram'],"Title: Senior Machine Learning Engineer\nLocation: Gurgaon, IN\nType: (Hybrid, In-Office)\nJob Description\n\nWho We Are:\nFareportal is a travel technology company powering a next-generation travel concierge service. Utilizing its innovative technology and company owned and operated global contact centres, Fareportal has built strong industry partnerships providing customers access to over 500 airlines, a million lodgings, and hundreds of car rental companies around the globe. With a portfolio of consumer travel brands including CheapOair and OneTravel, Fareportal enables consumers to book-online, on mobile apps for iOS and Android, by phone, or live chat. Fareportal provides its airline partners with access to a broad customer base that books high-yielding international travel and add-on ancillaries.",,,,"['Containerization', 'Machine Learning', 'Python', 'azure', 'sql', 'api', 'nosql', 'deployment']",2025-06-11 06:00:45
Director - Algorithm Development,Applied Materials,15 - 20 years,Not Disclosed,['Bengaluru'],"Who We Are\n\nApplied Materials is the global leader in materials engineering solutions used to produce virtually every new chip and advanced display in the world. We design, build and service cutting-edge equipment that helps our customers manufacture display and semiconductor chips- the brains of devices we use every day. As the foundation of the global electronics industry, Applied enables the exciting technologies that literally connect our world- like AI and IoT. If you want to work beyond the cutting-edge, continuously pushing the boundaries of""science and engineering to make possible""the next generations of technology, join us to Make Possible® a Better Future.\n\nWhat We Offer\n\nLocation:\nBangalore,IND\nAt Applied, we prioritize the well-being of you and your family and encourage you to bring your best self to work. Your happiness, health, and resiliency are at the core of our benefits and wellness programs. Our robust total rewards package makes it easier to take care of your whole self and your whole family. Were committed to providing programs and support that encourage personal and professional growth and care for you at work, at home, or wherever you may go. Learn more about our benefits .\n\nYoull also benefit from a supportive work culture that encourages you to learn, develop and grow your career as you take on challenges and drive innovative solutions for our customers.""We empower our team to push the boundaries of what is possible""”while learning every day in a supportive leading global company. Visit our Careers website to learn more about careers at Applied.\n\nJob Expectations\n\nThe candidate will be responsible for leading a team of data scientists who provide analytics services for Applied Materials installed base. The deliverables include developing new service capabilities, piloting them, and commercializing them in partnership with Engineering and Service Product Managers. The team will work with field engineers and product engineering teams to understand the requirements, bring forward creative ideas, develop proofs-of-concept, architect, design, develop, and modify algorithms into production code, provide production support, and train end-users. The skill sets in the team include descriptive statistical analysis, predictive statistical analysis using AI/ML, data visualization and analytics process automation, data cleansing, complex image processing, and text processing. Candidate should be willing to learn and adopt semiconductor industry as their career domain.\n\nKey Responsibilities\n\nResponsible for managing completion of assignments, projects and programs to support Applieds service business. Scope of algorithm development includes research, design, code development, implementation, and proliferation. Execute projects as needed to support the business. Review and monitor progress to milestones on development programs. Develop roadmaps for algorithmic development programs. Oversee algorithmic concept and feasibility for algorithmic modules, including problem statement definition, data gathering, literature review, concept selection, risks, and implementation constraints. Oversee documentation of algorithmic development and deployment, including integration into required systems, user testing, and user training. Oversee software and hardware implementation. Interact with internal and external customers to define gaps, identify opportunities, define program scope and deliverables, and proliferate solutions to the user base. Present to management for project reviews, interact with project stakeholders, run regular cadence meetings and work in alignment with team and organization goals. Responsible for technical development of team, objective setting, and performance management. Develop growth plan for the team, including identification of new areas of impact.\n\n\n\nPreferred programming and data science skills includePython, C++, Unix, Image Processing, Deep Learning, AI/ML, NLP, GenAI, Text Mining, Database Design and Management, Web Scraping, GPU Optimization. Proficient in business processes and software such as Microsoft Word/ Excel/ Powerpoint/ Teams, Atlassian JIRA and Confluence. Highly organized and detail-oriented. Ability to build and maintain positive and productive inter-departmental working relationships. Ability to work in a cross-functional organization and multitask on multiple projects. Drive team members to deliver programs on time and on budget. Excellent oral and written communication, organizational, analytical, and interpersonal skills. Interest in building a career in the semiconductor industry.\n\nFunctional Knowledge\n\nDemonstrates comprehensive understanding of concepts and principles within own job family and knowledge of other related job families.\n\nBusiness Expertise\n\nApplies in-depth understanding of how own discipline integrates within the segment/function.\n\nLeadership\n\nManages multiple related teams, sets organizational priorities and allocates resources.\n\nProblem Solving\n\nIdentifies and resolves complex technical, operational and organizational problems.\n\nImpact\n\nImpacts the business results of a team or area by supporting and funding of projects, products, services and/or technologies and developing policies and plans.\n\nGuided by business unit, department or sub-functional business plans.\n\nInterpersonal Skills\n\nInfluences others internally and externally, including senior management.\n\nPosition requires understanding of Applied Materials global Standards of Business Conduct and compliance with these standards at all times. This includes demonstrating the highest level of ethical conduct reflecting Applied Materials core values.\n\nEducation\n\nBachelors, Masters, or Ph.D. Degree in Computer Science, Mathematics, or Engineering with a concentration in data science or AI/ML.\n\nExperience\n\n15 years of experience\n\nComputer Science/ Mathematics/ Engineering background with\n\n15 years of experience in performing statistical analysis, designing and developing Image Processing/ Computer Vision Algorithms, handling and analyzing large volumes of data. Semiconductor background is an added advantage. Prior team leadership experience is required.\n\nAdditional Information\n\nTime Type:\nFull time\n\nEmployee Type:\nAssignee / Regular\n\nTravel:\nYes, 20% of the Time\n\nRelocation Eligible:\nYes\nApplied Materials is an Equal Opportunity Employer. Qualified applicants will receive consideration for employment without regard to race, color, national origin, citizenship, ancestry, religion, creed, sex, sexual orientation, gender identity, age, disability, veteran or military status, or any other basis prohibited by law.",Industry Type: Electronic Components / Semiconductors,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['natural language processing', 'artificial intelligence', 'deep learning', 'statistics', 'jira', 'image processing', 'web scraping', 'algorithms', 'c++', 'python', 'confluence', 'atlassian', 'iot', 'java', 'data science', 'computer vision', 'jenkins', 'text mining', 'performance management', 'unix', 'ml']",2025-06-11 06:00:46
Engineer,"NTT DATA, Inc.",1 - 4 years,Not Disclosed,['Bengaluru'],"Req ID: 327976\n\nWe are currently seeking a Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n Key Responsibilities: \n\nServe as the primary technical expert on DataRobot""™s AI platform, providing deep technical guidance and support.\n\nCollaborate with data scientists, AI engineers, and business stakeholders to implement and optimize DataRobot solutions.\n\nTroubleshoot and resolve complex issues related to DataRobot configuration, deployment, and monitoring.\n\nOnboard and configure existing SAS-based and DataRobot Prime v6 Python models into DataRobot v8.0.21.\n\nIngest LST report files from SAS and upload metrics into DataRobot.\n\nExecute onboarded models with historical data and generate monitoring outputs within DataRobot.\n\nCompare current Power BI dashboard metrics with DataRobot outputs and develop equivalent visualizations within the platform.\n\nLead the upgrade of DataRobot from version 8 to version 11 on Azure, working closely with technology partners.\n\nUnderstand and configure necessary Azure services (e.g., AKS, storage, networking) to support DataRobot deployment.\n\nDebug and resolve AKS container setup/configuration issues and propose scalable solutions.\n\nProvide training and support to internal teams on DataRobot functionalities and best practices.\n\nDevelop and maintain comprehensive documentation including user guides, technical manuals, and best practice guidelines.\n\nStay current with advancements in AI/ML and integrate relevant innovations into DataRobot solutions.\n\nWork through the Shield process and manage Jira stories for PreDev, PostDev, Prod, and RTx environments.\n\n Required Skills & Qualifications: \n\nProven experience with DataRobot platform, especially in enterprise-scale deployments.\n\nStrong understanding of Azure cloud services, including AKS, networking, and storage.\n\nHands-on experience with containerized deployments and Kubernetes troubleshooting.\n\nProficiency in Python and familiarity with SAS model structures.\n\nExperience with model monitoring, performance metrics, and visualization tools like Power BI.\n\nExcellent problem-solving skills and ability to work independently in a fast-paced environment.\n\nStrong communication skills for cross-functional collaboration and training delivery.\n\nFamiliarity with enterprise deployment processes, including Jira-based workflow management.\n\n\nPreferred Qualifications:\n\nExperience working in regulated environments such as banking or finance.\n\nKnowledge of U.S. Bank""™s Shield process or similar enterprise governance frameworks.\n\nPrior involvement in AI/ML model lifecycle management and MLOps practices",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure cloud services', 'kubernetes', 'artificial intelligence', 'ml', 'python', 'aks', 'networking', 'jquery', 'providing training', 'asp.net', 'debugging', 'web api', 'html', 'mvc', 'wcf', 'jira', 'c#', 'sas', 'entity framework', 'power bi', 'microsoft azure', 'javascript', 'sql server', 'linq', 'troubleshooting', '.net']",2025-06-11 06:00:48
Principal AI Architect,Wabtec,10 - 15 years,Not Disclosed,['Bengaluru'],"How will you make a difference?\n\nWe are seeking a collaborative and highly motivated Principal AI Architect to lead our AI team, drive innovation, and enhance customer experiences through impactful artificial intelligence solutions. As a member of the Wabtec IT Data & Analytics (DnA) Team, you will be responsible for:\n\nProviding strategic leadership and direction in the development, articulation and implementation, of a comprehensive AI/ML/ Data Transformation Roadmap for Wabtec aligned with the overall business objectives.\nWorking with other AI champions in Wabtec evaluating AI tools/ technologies/ frameworks, champion adoption in different projects and demonstrate value for business and customers.\nActively collaborating with various stakeholders to align AI initiatives in Cloud Computing environments (e.g., AWS, Azure, OCI) with business goals.\nProviding technical oversight on AI projects to drive performance output to meet KPI metrics in Productivity and Quality.\nServing as contact and interface with external partners and industry leaders for collaborations in AI/LLM/ Generative AI.\nArchitecting and deploying scalable AI solutions that integrate seamlessly with existing business and IT infrastructure.\nDesign and architect AI as a service, to enable collaboration btw multiple teams in delivering AI solutions\nOptimizing state-of-the-art algorithms in distributed environments\nCreate clear and concise communications/recommendations for senior leadership review related to AI strategic business plans and initiatives.\nStaying abreast of advancements in AI, machine learning, and data science to continuously innovate and improve solutions and bring the external best practices for adoption in Wabtec\nImplementing best practices for AI designing, testing, deployment, and maintenance\nDiving deep into complex business problems and immerse yourself in Wabtec data & outcomes.\nMentoring a team of data scientists, fostering growth and performance.\nDeveloping AI governance frameworks with ethical AI practices and ensuring compliance with data protection regulations and ensuring responsible AI development.\n\nWhat do we want to know about you?\n\nYou must have:\nThe minimum qualifications for this role include:\n\nPh.D., M.S., or Bachelor's degree in Statistics, Machine Learning, Operations Research, Computer Science, Economics, or a related quantitative field\n5+ years of experience developing and supporting AI products in a production environment with 12+ years of proven relevant experience\n8+ years of experience managing and leading data science teams initiatives at enterprise level\nProfound knowledge of modern AI and Generative AI technologies\nExtensive experience in designing, implementing, and maintaining AI systems\nEnd-to-end expertise in AI/ML project lifecycle, from conception to large-scale production deployment\nProven track record as an Architect with cloud computing environments (e.g., AWS, Azure, OCI) and distributed computing platforms, including containerized deployments using technologies such as Amazon EKS (Elastic Kubernetes Service)\nExpertise with Hands-On experience into Python, AWS AI tech-stack (Bedrock Services, Foundation models, Textract, Kendra, Knowledge Bases, Guard rails, Agents etc.), ML Flow, Image Processing, NLP/Deep Learning, PyTorch /TensorFlow, LLMs integration with applications.\n\nPreferred qualifications for this role include:\nProven track record in building and leading high-performance AI teams, with expertise in hiring, coaching, and developing engineering leaders, data scientists, and ML engineers\nDemonstrated ability to align team vision with strategic business goals, driving impactful outcomes across complex product suites for diverse, global customers\nStrong stakeholder management skills, adept at influencing and unifying cross-functional teams to achieve successful project outcomes\nExtensive hands-on experience with enterprise-level Python development, PyData stack, Big Data technologies, and machine learning model deployment at scale\nProficiency in cutting-edge AI technologies, including generative AI, open-source frameworks, and third-party solutions (e.g., OpenAI)\nMastery of data science infrastructure and tools, including code versioning (Git), containerization (Docker), and modern AI/ML tech stacks\nPreferred: AWS with AWS AI services.\n\nWe would love it if you had:\n\nFluent with experimental design and the ability to identify, compute and validate the appropriate metrics to measure success\nDemonstrated success working in a highly collaborative technical environment (e.g., code sharing, using revision control, contributing to team discussions/workshops, and collaborative documentation)\nPassion and aptitude for turning complex business problems into concrete hypotheses that can be answered through rigorous data analysis and experimentation\nDeep expertise in analytical storytelling and stellar communications skills\nDemonstrated success mentoring junior teammates & helping develop peers\n\nWhat will your typical day look like?\n\nStakeholder Engagement: Collaborate with our Internal stakeholders to understand their needs, update on a specific project progress, and align our AI initiatives with business goals.\nUse Generative AI and machine learning techniques and build LLM Models & fine-tuning, Image processing, NLP, model integration with new/existing applications, and improve model performance/accuracy along with cost effective solutions.\nSupport AI Team: Guide and mentor the AI team, resolving technical issues and provide suggestions.\nReporting & Strategy: Generate and present reports to senior leadership, develop strategic insights, and stay updated on industry trends.\nBuilding AI roadmap for Wabtec and discussion with senior leadership\nTraining, Development & Compliance: Organize training sessions, manage resources efficiently, ensure data accuracy, security, and compliance with best practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Cloud Architecture', 'Eks', 'Machine Learning', 'Aiml']",2025-06-11 06:00:49
Digital Project Manager Lead Consultant,"NTT DATA, Inc.",3 - 4 years,Not Disclosed,['Pune'],"Req ID: 303766\n\nWe are currently seeking a Digital Project Manager Lead Consultant to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nRESPONSIBILITIES\n\n\nLead and manage AI projects from inception to completion, ensuring successful delivery within scope, timeline, and budget.\n\n\nDevelop comprehensive project plans, including timelines, resource allocation, and risk management strategies for AI and machine learning initiatives.\n\n\nCollaborate with a diverse range of stakeholders, including data scientists, engineers, and business leaders, to align project objectives with organizational goals.\n\n\nOversee the development and deployment of AI models, ensuring alignment with project requirements and performance metrics.\n\n\nIdentify, analyze, and address project issues and risks, implementing corrective actions as needed to keep projects on track.\n\n\nDrive process improvements by integrating new technologies and best practices into project management processes.\n\n\nFacilitate clear and effective communication across project teams and stakeholders, providing regular updates and ensuring transparency throughout the project lifecycle.\n\n\nEnsure AI projects adhere to data privacy regulations, ethical guidelines, and industry standards.\n\n\nHandle project operations ""“ Resource Rampup, People Management, Trainings\n\n\nWorking with COE to create new GenAI accelerators\n\n\nLocationPune\n\n\nSkills, Knowledge, and Experience\n\n\nTechnical Handson skills in GenAI is must. Should be able to provide technical guidance to the team when required\n\n\nMinimum 3-4 years of experience managing AI or technology projects, with a proven track record of delivering complex projects on time and within budget.\n\n\nBachelor""™s degree in Computer Science, Data Science, or a related field; advanced certification (e.g., PMP, Agile) is a plus.\n\n\nStrong portfolio demonstrating successful AI projects, showcasing ability to manage project scope, resources, and stakeholder expectations effectively.\n\n\nFamiliarity with AI and machine learning concepts, technologies, and tools, with the ability to understand and manage technical aspects of projects.\n\n\nProficiency in project management methodologies (e.g., Agile, Scrum) and tools (e.g., Jira, MS Project).\n\n\nExperience with data analytics and performance monitoring tools relevant to AI projects.\n\n\n\nUnderstanding of how AI solutions can drive business value and impact organizational strategy.\n\n\nKnowledge of ethical considerations and compliance requirements related to AI and data privacy.\n\n\nStrong understanding of programming or data science tools (e.g., Python, R) for better communication with technical teams & Azure GENAI\n\n\nExceptional communication skills, with the ability to convey complex AI concepts and project details to both technical and non-technical stakeholders.",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['python', 'project management process', 'machine learning', 'artificial intelligence', 'data science', 'project operations', 'risk management', 'project management', 'data analytics', 'ms project', 'microsoft azure', 'budgeting', 'r', 'pmp', 'resource allocation', 'scrum', 'agile', 'jira']",2025-06-11 06:00:51
Cloud Engineer,"NTT DATA, Inc.",9 - 14 years,Not Disclosed,['Bengaluru'],"Req ID: 324901\n\nWe are currently seeking a Cloud Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nAt NTT DATA, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company""™s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring, the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA and for the people who work here.\n\n\n\nPreferred Experience\nSolid understanding of cloud computing, networking, and storage principles with focus on Azure. Should have strong delivery knowledge and experience around cloud adoption and workload management in public cloud (IaaS and PaaS platforms)\nWilling to work on multiple cloud platforms. Understanding of the customer strategy, business, technical, security and compliance requirements.\nExpertise in Cloud Infrastructure Networking, Security, IAM, Data Security and leveraging the right solutions in these areas is required.\nSolid scripting and automation experience (DevOps & scripting). Implementation experience on Infrastructure as Code (IaC) using tools like ARM templates and/or Terraform.\nGood experience on emerging technologies, including Container bases orchestration, AKS, AI services etc.,\nInnovative self-starter, willing to learn, test, implement new technologies on existing and new public cloud providers.\nCollaborate with different teams ""“ Data Engineering, Data Analytics, InfoSec, Network/firewall, etc.\nClear understanding of automation.\nDefine requirements and evaluate existing architectures, Various technical solutions, products, Partners against our technical requirements ""“ technical, NFR such as operational resilience, scalability, reliability, performance.\nRegular interactions with senior management or executive levels on matters concerning several functional areas and/or customers are also part of the job\nAssist with the design and implementation of best governance practices for design, security, development, usability, cost optimization / control.\nKnowledge of best practices and market trends pertaining to Cloud and overall industry to provide thought leadership (seminars, whitepapers etc.,) and mentor team to build necessary competency\nCollaborating in the creation work/design documents with team assistance. Able to work independently in a project scenario and do POCs.\nShould be able to manage & mentor the junior team members from L2 / L1.\nAble to work on On-Call rotations.\nExperience handling P1/P2 calls and escalations\nAble to write quality KB articles, Problem Management articles, and SOPs/Runbooks.\nPassion for delivering timely and outstanding customer service\nGreat written and oral communication skills with internal and external customers\n\n\n\nBasic Qualifications\nAt least 9+ years of overall operational experience.\n6+ years of Azure experience\n3+ years of experience working in a diverse cloud support environment in a 24*7 production support model\n2+ years DevOps/scripting/APIs experience\n\n\nPreferred Certifications\nAzure Administrator Associate/Solution Architect Certifications.\nMicrosoft DevOps / Terraform certifications is an added advantage.\nAWS SysOps Administrator/Developer/Solutions Architect Associate Certifications\nFour Years degree on Information Technology degree or equivalent experience",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['data security', 'networking', 'iam', 'cloud infrastructure', 'cloud computing', 'container', 'public cloud', 'aks', 'microsoft azure', 'cloud platforms', 'cloud support', 'workload management', 'production support', 'firewall', 'devops', 'paas', 'terraform', 'api', 'iaas', 'arm templates', 'aws']",2025-06-11 06:00:53
Senior Consultant/Engagement Manager - Technology Consulting,Tiger Analytics,7 - 12 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","About the role\n\n\nAs a Technology Consulting, you will be a key member of the teams working on end-to-end AI and analytics solutions being built for our clients. While your primary focus will be on the data engineering and application engineering components of the overall solution, you will collaborate with internal data science teams for the ML aspects. As the person responsible for the overall success of the program, you will:",,,,"['Program Management', 'Data Engineering', 'Technology Consulting', 'ETL', 'RFP', 'Delivery Lead', 'Azure', 'SOW']",2025-06-11 06:00:54
Digital Engineering Staff Engineer,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Req ID: 310007\n\nWe are currently seeking a Digital Engineering Staff Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nData Modeler\n\n\n\nPosition Overview: The Data Modeler will be responsible for designing and implementing data models that support the organization's data management and analytics needs.\n\nThis role involves collaborating with various stakeholders to understand data sources, relationships, and business requirements, and translating them into effective data structures.\n\n\n\nKey Responsibilities:\n\n\nCollaborate with Business Analysts: Understand different data sources and their relationships.\n\n\nPrepare Conformed Dimension Matrix: Identify different grains of facts, finalize dimensions, and harmonize data across sources.\n\n\nCreate Data Models: Develop Source to Target Mapping (STMs) documentation and custom mappings (both technical and non-technical).\n\n\nInclude Transformation Rules: Ensure STMs include pseudo SQL queries for transformation rules.\n\n\nCoordinate Reviews: Work with Data Architects, Product Owners, and Enablement teams to review and approve models, STMs, and custom mappings.\n\n\nEngage with Data Engineers: Clarify any questions related to STMs and custom mappings.\n\n\n\nRequired Technical\n\nSkills:\n\n\n\nProficiency in SQL: Strong understanding of SQL and database management systems.\n\n\nData Modeling Tools: Familiarity with tools such as ERwin, IBM InfoSphere Data Architect, or similar.\n\n\nData Warehousing Concepts: Solid knowledge of data warehousing principles, ETL processes, and OLAP.\n\n\nData Governance and Compliance: Understanding of data governance frameworks and compliance requirements.\n\n\n\nKey Competencies:\n\n\nAnalytical\n\nSkills:\nAbility to analyze complex data sets and derive meaningful insights.\n\n\nAttention to Detail: Ensure accuracy and consistency in data models.\n\n\nCommunication\n\nSkills:\nEffectively collaborate with stakeholders and articulate technical concepts to non-technical team members.\n\n\nProject Management\n\nSkills:\nAbility to prioritize tasks, manage timelines, and coordinate with cross-functional teams.\n\n\nContinuous Learning and Adaptability: Commitment to ongoing professional development and adaptability to changing business needs and technologies.\n\n\n\nAdditional :\n\n\nProblem-Solving Abilities: Innovative solutions to data integration, quality, and performance challenges.\n\n\nKnowledge of Data Modeling Methodologies: Entity-relationship modeling, dimensional modeling, normalization techniques.\n\n\nFamiliarity with Business Intelligence Tools: Enhance ability to design data structures that facilitate data analysis and visualization.\n\n\n\nPreferred Qualifications:\n\n\nExperience in SDLC: Understanding of all phases of the Software Development Life Cycle.\n\n\nCertifications: Relevant certifications in data modeling, data warehousing, or related fields.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database management system', 'data warehousing', 'sql', 'olap', 'etl process', 'c#', 'ibm infosphere', 'dimensional modeling', 'erwin', 'business intelligence', 'javascript', 'sql server', 'software development life cycle', 'linq', 'java', 'data modeling', 'asp.net', 'data structures', 'etl', 'sdlc']",2025-06-11 06:00:56
Campaign Management Lead,Indegene,8 - 10 years,11-18 Lacs P.A.,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","We are a technology-led healthcare solutions provider. We are driven by our purpose to enable healthcare organizations to be future-ready. We offer accelerated, global growth opportunities for talent thats bold, industrious, and nimble. With Indegene, you gain a unique career experience that celebrates entrepreneurship and is guided by passion, innovation, collaboration, and empathy. To explore exciting opportunities at the convergence of healthcare and technology, check out www.careers.indegene.com\nLooking to jump-start your career?\nWe understand how important the first few years of your career are, which create the foundation of your entire professional journey. At Indegene, we promise you a differentiated career experience. You will not only work at the exciting intersection of healthcare and technology but also will be mentored by some of the most brilliant minds in the industry. We are offering a global fast-track career where you can grow along with Indegenes high-speed growth.\nWe are purpose-driven.  We enable healthcare organizations to be future ready and our customer obsession is our driving force. We ensure that our customers achieve what they truly want. We are bold in our actions, nimble in our decision-making, and industrious in the way we work.\n\nCampaign Management Lead - SFMC\nYou will be responsible for: \nLead the design and implementation of scalable SFMC solutions.\nDevelop and optimize data models, audience segmentation, and automation strategies.\nWork closely with marketing, sales, and IT teams to gather business requirements.\nProvide best practice recommendations for marketing automation.\nLead and mentor SFMC developers, consultants, and administrators.\nWork with REST/SOAP APIs, FTPs, and external data sources.\nConduct code reviews and enforce development best practices.\nExperience in Email, Mobile and web studio.\nLead design and development of customer journeys on marketing automation platforms.\nAbility to understand clients business objective and come up with a good campaign plan proposal that includes various options for audience creation, journey and content.\nAsk right questions and help clients come up with unambiguous campaign objectives. \nPresent information in a crisp and business friendly manner (good presentation skills PowerPoint).\nCollaborate with project management, content, data engineering and analytics teams in designing campaigns, finalizing assets and other operational activities.\nLead and nurture a team of campaign developers while maintaining operational excellence across all email/campaign projects.\nDrive innovation and thought leadership in the field of marketing automation and multi-channel marketing\nCritically analysing campaigns and identifying gaps in technical set up. \nLeverage past experience and come up with robust QA processes for campaign testing. \nExpertise in creating meaningful cross-channel campaign performance dashboards (in SFMC, Adobe, or externally) to report integrated campaign performance\nHelp business in deriving insights based on metrics reported in campaign dashboards\n\nYour impact: \nAbout you: \nDemonstrate good understanding of multi-channel marketing campaigns business processes (pharma experience would be a big plus). \nIs a very good communicator/articulator who is comfortable is switching between business and technical conversations. \nAbility to look at bigger picture (while talking to clients) and at the same time be detail-oriented (while working with internal team).\nShould be SME in audience management, content builder, journey builder and performance management of MCM campaign (email + other integrated channels such as Tele, Website and Social) \nGood understanding of domain and IP management for Email Engines. \nKnowledge of SPAM filters, tackling SPAM issues, Domain warm up, IP warm up, Spam filter criteria in major ESPs.\nStrong experience in SQL and excel is a MUST \nHas experience in managing a campaign execution technical team (a team of SFMC tech guys)\nAbility to set up campaign development and QA processes is preferred.\nMust have:\n8 years experience towards multiple marketing automation platforms like SFMC and Adobe campaigns, Marketo, etc. \nCertified ""SFMC Cloud Consultant"".\nIndegene is proud to be an Equal Employment Employer and is committed to the culture of Inclusion and Diversity. We do not discriminate on the basis of race, religion, sex, colour, age, national origin, pregnancy, sexual orientation, physical ability, or any other characteristics. All employment decisions, from hiring to separation, will be based on business requirements, the candidates merit and qualification. We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, national origin, gender identity, sexual orientation, disability status, protected veteran status, or any other characteristics.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation Studio', 'api', 'Sfmc', 'Mobile Studio', 'Email Studio', 'Marketing Automation', 'Journey Builder', 'Soap And Rest Api']",2025-06-11 06:00:57
Software Applications Development Engineer,"NTT DATA, Inc.",5 - 10 years,Not Disclosed,"['Mumbai', 'Hyderabad', 'Bengaluru']","Your day at NTT DATA\nThe Software Applications Development Engineer is a seasoned subject matter expert, responsible for developing new applications and improving upon existing applications based on the needs of the internal organization and or external clients.\nWhat you'll be doing\nYrs. Of Exp: 5 Yrs.\n\nData Engineer-\nWork closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.\nWork closely with Data modeller to ensure data models support the solution design\nDevelop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.\nAnalysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.\nDevelop documentation and artefacts to support projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Application Development', 'Data Engineering', 'Snowflake', 'ETL', 'Fivetran', 'SQL']",2025-06-11 06:00:59
Business Analyst _Payment Swift _MT/MX/GPP_6-14Yrs,Leading Software Technologies,6 - 11 years,15-30 Lacs P.A.,"['Pune', 'Chennai', 'Bengaluru']","Strong understanding of Global Payments Processing and E2E Payment workflows, SWIFT Cross boarder processing (MT & MX) & ISO 20022 Message Processing, Clearing & Settlement Systems process, Payments Interface data mapping and solution design,E2E imp\n\nRequired Candidate profile\nDomestic clearing like NEFT, ECS, ACH, CHAPS / FEDWIRE / CHIPS / Target2, RTGS, RTP schemes like NPP/FAST/IMPS/InstaPay etc.,SEPA (Direct Debits, Credit Transfers, Mandate Management, Instant Payments",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GPP', 'MT', 'MX', 'Business Analysis', 'Swift Messaging', 'Visa Installation service', 'VIS', 'NEFT', 'Aquiring', 'SEPA', 'ACH', 'Chips', 'User Stories', 'Payment Domain', 'CHAPS', 'ECS', 'AMEX global specification', 'APACS', 'Requirement Analysis', 'FedWire']",2025-06-11 06:01:01
Software Engineer,Intelliswift software,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Description:\nWe are looking for a skilled Palantir Foundry Developer with strong hands-on experience in data engineering using PySpark and SQL. The ideal candidate should be proficient in designing, building, and maintaining scalable data pipelines and integrating with Palantir Foundry environments.\nKey Skills:\nPalantir Foundry (Mandatory)\nPySpark,\nAdvanced SQL and Data Modelling\nData Pipeline Development and Optimization\nETL Processes, Data Transformation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Palantir Foundry', 'SQL']",2025-06-11 06:01:03
Ml Engineer,Ltimindtree,6 - 9 years,Not Disclosed,"['Pune', 'Bengaluru', 'Mumbai (All Areas)']","Job Title: Data Scientist\n\nLooking for someone with 5-8 years of experience manipulating data sets and building statistical models\n\nDesired Skills:\n\nExperience using statistical computer languages (R, Python, etc.) to manage data and draw insights from large data sets.\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, XGBoost, KNN, SVM, ANN, etc.).\nKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.\nKnowledge and experience in statistical and data mining techniques: GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.\nExperience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modelling, clustering, decision trees, neural networks, etc.\nExperience visualizing/presenting data for stakeholders.\nExperience with Snowflake will be an added advantage.\nExperience in deployment of machine learning models using cloud technologies.\n\nRoles and responsibilities:\n\n1. Work with stakeholders to identify opportunities for leveraging data to drive business solutions.\n2. Mine and analyse data from databases to drive optimization and improvement of product development and business strategies.\n3. Assess the effectiveness and accuracy of new data sources and data gathering techniques.\n4. Develop custom models and algorithms to apply to data sets.\n5. Use predictive modelling to increase and optimize customer experiences.\n6. Coordinate with different functional teams to implement models and monitor outcomes.\n7. Analyse large amounts of information to discover trends and patterns.\n8. Build predictive models and machine-learning algorithms.\n9. Present information using data visualization techniques .\n10. Good to have a knowledge of ML lifecycle and model governance.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'R', 'Python']",2025-06-11 06:01:04
Hiring For Business Analyst - US Shift,Digile Technologies,9 - 14 years,Not Disclosed,[],"Role & responsibilities\nPosition Overview\nWe are seeking experienced Business Analysts to help transform Professional Services operations. This team will support strategic documentation and process mapping efforts that will guide future-state road mapping across three key functions: Sales, Consulting Delivery, and Fulfillment. The ideal candidates will bring strong analytical capabilities, proficiency with tools such as Lucid chart or equivalent, and a passion for driving operational excellence.\nKey Responsibilities\nCollaborate with cross-functional stakeholders across Sales, Delivery, and Fulfillment functions to document current processes and identify areas for improvement.\nMap and analyze business processes across Lead-to-Cash, Order-to-Cash, Hire-to-Retire, and Onboarding value streams.\nCreate clear, comprehensive, and visually effective process documentation using Lucid chart or similar tools.\nSupport the development of future-state process flows to enable digital transformation and operational scalability.\nWork with internal and external-facing teams (500+ internal employees, 5,500+ contractors) to understand needs and capture pain points.\nCoordinate across global teams (India, Brazil, and U.S.) to ensure documentation efforts are aligned and time-zone compatible.\nProvide inputs to the broader change management and transformation planning initiatives.\nQualifications\n5+ years of experience as a Business Analyst, preferably in a technology or professional services environment.\nStrong process mapping and documentation skills; experience with Lucid chart, Visio, or similar tools required.\nFamiliarity with enterprise systems such as NetSuite, Salesforce, Hi Bob, or similar platforms.\nExperience documenting and analyzing end-to-end business processes, especially in scaling environments.\nExcellent communication and collaboration skills across distributed global teams.\nAbility to work independently in a fast-paced, ambiguous environment.\nPreferred Qualifications\nExperience supporting large-scale business transformation or ERP/CRM implementation projects.\nPrior exposure to professional services delivery models or managed services organizations.\nKnowledge of SaaS business models and metrics.\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['enterprise system', 'Lucidchart', 'Business analyst']",2025-06-11 06:01:06
Machine Learning Engineer,Whats On India Media,3 - 8 years,Not Disclosed,"['Mumbai', 'Gurugram', 'Bengaluru']","Nielsen is seeking an organized, detail oriented, team player, to join the Engineering team in the role of Software Machine Learning Engineer. Nielsen's Audience Measurement Engineering platforms support the measurement of television viewing in more than 30 countries around the world. The Software Engineer will be responsible to define, develop, test, analyze, and deliver technology solutions within Nielsen's Collections platforms.\nRequired Skills\nBachelor's degree in Computer Science or equivalent degree.\n3+ years of software experience\nExperience with Machine learning frameworks and models. Pytorch experience preferred\nStrong understanding of statistical analysis and mathematical data manipulation\nWork with web technology including Java, Python, JavaScript, React/Redux, Kotlin.\nFollow best practices for software development and deployment\nUnderstanding of relational database, big data, and experience in SQL\nProficient at using GIT, GitFlow, JIRA, Gitlab and Confluence.\nStrong analytical and problem solving skills.\nOpen-minded and passionate to learn and grow technology skills\nStrong sense of accountability\nSolution-focused and ability to drive change within the organization\nExperience in writing unit/integration tests including test automation.\nStrong testing and debugging abilities, functional, analytical and technical abilities, ability to find bugs, attention to detail, troubleshooting\nAdditional Useful Skills\nA fundamental understanding of the AWS ecosystem (EC2, S3, EMR, Lambda, etc)\nExperienced in building RESTful APIs.\nExperience in writing unit/integration tests including test automation.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Ml Algorithms', 'Python', 'Pytorch']",2025-06-11 06:01:07
Ml Engineer,Sightspectrum,5 - 10 years,1-6 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Description:\n\n\n\nStrong experience in ETL development, data modeling, and managing data in large-scale environments. - Proficient in AWS services including SageMaker, S3, Glue, Lambda, and CloudFormation/Terraform. - Hands-on expertise with MLOps best practices, including model versioning, monitoring, and CI/CD for ML pipelines.\n\n- Proficiency in Python and SQL; experience with Java is a plus for streaming jobs.\n- Deep understanding of cloud infrastructure automation using Terraform or similar IaC tools. - Excellent problem-solving skills with the ability to troubleshoot data processing and deployment issues.\n- Experience in fast-paced, agile development environments with frequent delivery cycles.\n- Strong communication and collaboration skills to work effectively across cross-functional team Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MLOps', 'Aws Sagemaker', 'Rasa', 'Serverless', 'Ml']",2025-06-11 06:01:09
Developer - L4,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Continuous Integration, Deployment & Monitoring of Software100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan\n2.Quality & CSATOn-Time Delivery, Manage software, Troubleshoot queries,Customer experience, completion of assigned certifications for skill upgradation\n3.MIS & Reporting100% on time MIS & report generation\nMandatory Skills: DataBricks - Data Engineering.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['DataBricks', 'module development', 'software development', 'Data Engineering', 'report generation', 'MIS', 'CI/CD', 'SDLC']",2025-06-11 06:01:10
Business Analyst ( Merchandise),Corporate in B2C Retail Sector,3 - 8 years,4-5 Lacs P.A.,['Mumbai (All Areas)'],"Role & responsibilities\n\n\n• Create and manage a dynamic assortment of fashion products that maximize profitability and\nensure efficient stock levels.\n• Utilize analytical skills to accurately forecast inventory, budget, and pricing.\n• Monitor and analyze product performance to identify opportunities for improvement.\n• Collaborate with the buying and merchandising teams to ensure product availability meets\ncustomer demand.\n• Develop promotional strategies and promotional calendars that feature company products.\nTrack and analyze customer buying trends to inform future buying decisions.\n• Collaborate with internal teams to ensure accurate product data is up to date.\n• Monitor inventory levels to ensure an optimal balance between stock availability and\ncustomer demand.\n• Evaluate and negotiate with vendors to ensure competitive pricing.\n• Work cross-functionally to ensure all merchandise planning goals are met.",Industry Type: Textile & Apparel (Fashion),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Assortment Planning', 'Pricing', 'Budgeting', 'Forecasting', 'Inventory', 'Analytical', 'Budget Analysis', 'Product Data', 'Monitoring']",2025-06-11 06:01:12
Analyst,Vipsa Talent Solutions,1 - 4 years,9-13 Lacs P.A.,['Bengaluru'],"1-3 yrs PE portfolio management, DCF valuation, LP/GP reporting, real estate valuations, financial modeling.",Industry Type: Financial Services (Asset Management),Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['Private Equity', 'DCF', 'Portfolio Reporting', 'Valuation Reporting', 'Financial Modelling', 'Real Estate']",2025-06-11 06:01:14
Technical Lead-Analytics,3i Infotech,5 - 10 years,Not Disclosed,"['Navi Mumbai', 'Bengaluru', 'Greater Noida']","Analytics Tech Lead Job Description\nAnalytics Tech Lead with a 6 years of relevant experience, who will be responsible for designing, developing, and maintaining Machine Learning use cases, data visualization solutions. Experience in Banking Industry and a good understanding of Core Banking systems/flow is preferred.\nResponsibilities\n•         Experienced in develop Machine Learning Model use cases using Python and other ML tools. \n•         Enabling clients with AI/ML solutions that seamlessly cater their business needs of data-driven decision results. I\nExperienced in designing data exploration & storyline-based analytics dashboard/wireframes/prototypes using Tableau\nAdvanced Tableau development skills, including experience with Tableau Desktop and Server latest version and expert knowledge of Tableau software optimization and configuration.\nExperience working with Tableau server and user management, performance tuning, security in Tableau server.\nImplemented data modelling, data blending, and custom query joins to enhance dashboard interactivity and functionality. \nExperience with Structured Query Language,  data analysis, data profiling, data validation.\nWork with cross-functional teams, including business analysts and data engineers to understand requirements and develop solutions.\nPrepare technical documentation on the deliverables.\nSkills and Qualifications\nBachelors degree in computer science or Master of Computer Applications with 6+ years of relevant experience.\nAI/ML tool  Certification \nExcellent written, verbal and presentation skills to foster clarity and predictability.\nExperience working with a global team in different time zones.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Machine Learning', 'Analytics', 'Python', 'Ml']",2025-06-11 06:01:15
Opening For DevOps Engineer,Randstad Digital,2 - 6 years,8-10 Lacs P.A.,['Chennai'],"Job Type - Third Party Payroll\nPayroll Company Name - Randstad Digital\nPosition - DevOps Engineer\nExperience - 3+Years\nLocation - Chennai only\nNotice Period - Immediate to 15days\nInterview Type - Virtual (Final Round Face2Face Interview)\nWork Mode - 5 days office\n\n\nIf you're interested, kindly share your updated resume and the following details to this below email ID or LinkedIn\n\n\nEmail Id: gayathri.nambi@randstaddigital.com\nLinkedIn: https://www.linkedin.com/in/gayathri-alagia-nambi-0827921a5/\n\n\nFull Name:\nPan Number:\nExperience:\nRelevant Experience:\nNotice period:\nCTC:\nExpected CTC:\nCurrent company:\nPayroll Company:\nLocation:\nPreferred location:\nOffer in hand : (Y/N)\nReason for Job Change:\n\nKey Responsibilities\nCollaborate closely with Product teams, Software, and Data engineers to maintain and develop tools and infrastructure, driving innovation and optimising existing and new product value streams.\nLead and advise on the modernisation of end-to-end ETL and real-time streaming pipelines, refining development processes, and introducing relevant technologies and tools.\nWork extensively with AWS cloud platforms and tools, applying Infrastructure as Code (IaC) practicesparticularly with Terraformand prioritising both cost optimisation and secure network configurations.\nEnsure robust security by implementing and managing cybersecurity frameworks and tools, including [specific security tools currently in use], to protect systems against threats.\nApply network knowledge to optimise and secure infrastructure, ensuring efficient communication and data flow across systems.\nImplement a proactive patch management strategy to keep all systems updated with the latest security patches.\nAdhere to change management protocols and version control standards, ensuring a secure code repository with reliable backup strategies for key intellectual property. (maybe we can rephrase it better)\nEffectively present and explain advanced technical designs to both technical and non-technical stakeholders.\n\nWhat we are looking for:\nEssential\nBachelor’s degree in software, network, or cybersecurity (or demonstrable equal experience)\nAt least 2 years’ experience in a DevOps Engineer role\nProven experience or certification in AWS\nProven experience in troubleshooting and debugging SQL, SNS, SQS, Kinesis, C# (.net Core), S3, FTP, AWS Lambda. HTTP level REST API experience (and previous API standards, e.g SOAP)\nProven experience in operating system administration (on both Wintel and Linux).\nProven experience in networking components and systems, including webservers, firewalls and network routing.\nFamiliarity with automation tooling (such as Jenkins) and runtime integrated analysis tooling (such as New Relic)\nFamiliarity with the design, development and maintenance of best-in-class analytical capabilities, including data warehousing (Redshift, OpenSearch, Athena, SQL, etc)\nFamiliarity of architectural design patterns for micro-services leveraging relational and big data technologies\nDisciplined, self-starter attitude driven to improve systems and processes and a willingness to learn\nExcellent documentation of processes\nProficiency in written and spoken English\nDesirable\nProvisioning new technical assets (e.g EC2 build), Kubernetes, Docker and associated virtualisation or containerisation technology.\nComplete familiarity with Agile development process\nExcellent documentation of processes",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['DevOps', 'Terraform', 'Docker', 'AWS', 'Kubernetes']",2025-06-11 06:01:17
AI Engineer,HCLTech,10 - 14 years,Not Disclosed,['Noida'],"Seniority: Senior\nDescription & Requirements\nPosition Summary\nThe Senior AI Engineer with GenAI expertise is responsible for developing advanced technical solutions, integrating cutting-edge generative AI technologies. This role requires a deep understanding of modern technical and cloud-native practices, AI, DevOps, and machine learning technologies, particularly in generative models. You will support a wide range of customers through the Ideation to MVP journey, showcasing leadership and decision-making abilities while tackling complex challenges.\nKey Responsibilities\nTechnical & Engineering Leadership\nDevelop solutions leveraging GenAI technologies, integrating advanced AI capabilities into cloud-native architectures to enhance system functionality and scalability.\nLead the design and implementation of GenAI-driven applications, ensuring seamless integration with microservices and container-based environments.\nCreate solutions that fully leverage the capabilities of modern microservice and container-based environments running in public, private, and hybrid clouds.\nContribute to HCL thought leadership across the Cloud Native domain with an expert understanding of open-source technologies (e.g., Kubernetes/CNCF) and partner technologies.\nCollaborate on joint technical projects with partners, including Google, Microsoft, AWS, IBM, Red Hat, Intel, Cisco, and Dell/VMware.\nService Delivery\nEngineer innovative GenAI solutions from ideation to MVP, ensuring high performance and reliability within cloud-native frameworks.\nOptimize AI models for deployment in cloud environments, balancing efficiency and effectiveness to meet client requirements and industry standards.\nAssess existing complex solutions and recommend appropriate technical treatments to transform applications with cloud-native/12-factor characteristics.\nRefactor existing solutions to implement a microservices-based architecture.\nInnovation & Initiative\nDrive the adoption of cutting-edge GenAI technologies within cloud-native projects, spearheading initiatives that push the boundaries of AI integration in cloud services.\nEngage in technical innovation and support HCLs position as an industry leader.\nAuthor whitepapers, blogs, and speak at industry events.\nMaintain hands-on technical credibility, stay ahead of industry trends, and mentor others.\nClient Relationships\nProvide expert guidance to clients on incorporating GenAI and machine learning into their cloud-native systems, ensuring best practices and strategic alignment with business goals.\nConduct workshops and briefings to educate clients on the benefits and applications of GenAI, establishing strong, trust-based relationships.\nPerform a trusted advisor role, contributing to technical projects (PoCs and MVPs) with a strong focus on technical excellence and on-time delivery.\nMandatory Skills & Experience\nA passionate developer with 10+ years of experience in Java, Python, Node.js, and Spring programming, comfortable working as part of a paired/balanced team.\nExtensive experience in software development, with significant exposure to AI/ML technologies.\nExpertise in GenAI frameworks: Proficient in using GenAI frameworks and libraries such as LangChain, OpenAI API, Gemini, and Hugging Face Transformers.\nPrompt engineering: Experience in designing and optimizing prompts for various AI models to achieve desired outputs and improve model performance.\nStrong understanding of NLP techniques and tools, including tokenization, embeddings, transformers, and language models.\nProven experience developing complex solutions that leverage cloud-native technologiesfeaturing container-based, microservices-based approaches; based on applying 12-factor principles to application engineering.\nExemplary verbal and written communication skills (English).\nPositive and solution-oriented mindset.\nSolid experience delivering Agile and Scrum projects in a Jira-based project management environment.\nProven leadership skills and the ability to inspire and manage teams.\nDesired Skills & Experience\nMachine Learning Operations (MLOps): Experience in deploying, monitoring, and maintaining AI models in production environments using MLOps practices.\nData engineering for AI: Skilled in data preprocessing, feature engineering, and creating pipelines to feed AI models with high-quality data.\nAI model fine-tuning: Proficiency in fine-tuning pre-trained models on specific datasets to improve performance for specialized tasks.\nAI ethics and bias mitigation: Knowledgeable about ethical considerations in AI and experienced in implementing strategies to mitigate bias in AI models.\nKnowledgeable about vector databases, LLMs, and SMLs, and integrating with such models.\nProficient with Kubernetes and other cloud-native technologies, including experience with commercial Kubernetes distributions (e.g., Red Hat OpenShift, VMware Tanzu, Google Anthos, Azure AKS, Amazon EKS, Google GKE).\nDeep understanding of core practices including DevOps, SRE, Agile, Scrum, XP, Domain-Driven Design, and familiarity with the CNCF open-source community.\nRecognized with multiple cloud and technical certifications at a professional level, ideally including AI/ML specializations from providers like Google, Microsoft, AWS, Linux Foundation, IBM, or Red Hat.\nVerifiable Certification\nAt least one recognized cloud professional / developer certification (AWS/Google/Microsoft)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['AI engineering', 'VMware', 'Java', 'Azure', 'Data engineering', 'AI models', 'Node.js', 'NLP', 'Azure AKS', 'Machine Learning Operations', 'AWS', 'Kubernetes', 'Python']",2025-06-11 06:01:19
Business Analyst,TechStar Group,7 - 12 years,10-15 Lacs P.A.,"['Navi Mumbai', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities :\n\nBusiness Analyst - 7 to 10 years experience in Capital markets or global treasury.\nKnowledge of capital markets & financial instruments Equities, Derivatives, Fixed Income, FX.\nUnderstanding of Investment Banking and Asset Management Front, Middle & Back Office functions.\nAsset management experience of Trading Compliance rule coding & testing is strongly preferred, but other experience within Asset Management industry is acceptable Financial industry knowledge within investments and distribution highly desired (i.e., understanding of investment process, capital markets, fixed income, equities Sound knowledge on Database SQL querying.\nShould be a strong team player Excellent communication skills - written & verbal.\nShould be able to create good documentation and correspond on functionalities & issues concisely.\nShould be able to articulate well during discussions.\nShould be able to work with tight deadlines Confident of interacting with business users and various stakeholders.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MS Excel', 'Word', 'Visio', 'PowerPoint']",2025-06-11 06:01:20
Sr. Project Manager,Useready,15 - 18 years,30-40 Lacs P.A.,"['Mohali', 'Bengaluru']","Job Summary:\nWe are seeking an experienced and detail-oriented Technical Project Manager, with strong interpersonal skills to lead and manage Data, Business Intelligence (BI), and Analytics initiatives across single and multiple client engagements. The ideal candidate will have a solid background in data project delivery, knowledge of modern cloud platforms, and familiarity with tools like Snowflake, Tableau, and Power BI. Understanding of AI and machine learning projects is a strong plus.\nThis role requires strong communication and leadership skills, with the ability to translate complex technical requirements into actionable plans and ensure successful, timely, and high-quality delivery with attention to details.\nKey Responsibilities:\nProject & Program Delivery\nManage end-to-end, the full lifecycle of data engineering and analytics, projects including data platform migrations, dashboard/report development, and advanced analytics initiatives.\nDefine project scope, timelines, milestones, resource needs, and deliverables in alignment with stakeholder objectives.\nManage budgets, resource allocation, and risk mitigation strategies to ensure successful program delivery.\nUse Agile, Scrum, or hybrid methodologies to ensure iterative delivery and continuous improvement.\nMonitor performance, track KPIs, and adjust plans to maintain scope, schedule, and quality.\nExcellence in execution and ensure client satisfaction\nClient & Stakeholder Engagement\nServe as the primary point of contact for clients and internal teams across all data initiatives.\nTranslate business needs into actionable technical requirements and facilitate alignment across teams.\nConduct regular status meetings, monthly and quarterly reviews, executive updates, and retrospectives.\nManage Large teams\nAbility to manage up to 50+ resources working on different projects for different clients.\nWork with practice and talent acquisition teams for resourcing needs\nManage P & L\nManage allocation, gross margin, utilization etc effectively\nTeam Coordination\nLead and coordinate cross-functional teams including data engineers, BI developers, analysts, and QA testers.\nEnsure appropriate allocation of resources across concurrent projects and clients.\nFoster collaboration, accountability, and a results-oriented team culture.\n  Data, AI and BI Technology Oversight\nManage project delivery using modern cloud data platforms\nOversee BI development using Tableau and/or Power BI, ensuring dashboards meet user needs and follow visualization best practices. Conduct UATs\nManage initiatives involving ETL/ELT processes, data modeling, and real-time analytics pipelines.\nEnsure compatibility with data governance, security, and privacy requirements.\nManage AL ML projects\nData & Cloud Understanding\nOversee delivery of solutions involving cloud data platforms (e.g., Azure, AWS, GCP), data lakes, and modern data stacks.\nSupport planning for data migrations, ETL processes, data modeling, and analytics pipelines.\nBe conversant in tools such as Power BI, Tableau, Snowflake, Databricks, Azure Synapse, or BigQuery.\nRisk, Quality & Governance\nIdentify and mitigate risks related to data quality, project timelines, and resource availability.\nEnsure adherence to governance, compliance, and data privacy standards (e.g., GDPR, HIPAA).\nMaintain thorough project documentation including charters, RACI matrices, RAID logs, and retrospectives.\nQualifications:\n  Bachelor’s degree in Computer Science, Information Systems, Business, or a related field.\nCertifications (Preferred):\nPMP, PRINCE2, or Certified ScrumMaster (CSM)\nCloud certifications (e.g., AWS Cloud Practitioner, Azure Fundamentals, Google Cloud Certified)\nBI/analytics certifications (e.g., Tableau Desktop Specialist, Power BI Data Analyst Associate, DA-100)\nMust Have Skills:\nStrong communication skills\nStrong interpersonal\nAbility to work collaboratively\nExcellent Organizing skills\nStakeholder Management\nCustomer Management\nPeople Management\nContract Management\nRisk & Compliance Management\nC-suite reporting\nTeam Management\nResourcing\nExperience using tools like JIRA, MS Plan etc.\nDesirable Skills:\n15 years of IT experience with 8+ years of proven project management experience, in delivering data, AI Ml, BI / analytics-focused environments.\nExperience delivering projects with cloud platforms (e.g., Azure, AWS, GCP) and data platforms like Snowflake.\nProficiency in managing BI projects preferably Tableau and/or Power BI.\nKnowledge or hands on experience on legacy tools is a plus.\nSolid understanding of the data lifecycle including ingestion, transformation, visualization, and reporting.\nComfortable using PM tools like Jira, Azure DevOps, Monday.com, or Smartsheet.\nExperience managing projects involving data governance, metadata management, or master data management (MDM).",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['delivery', 'bi projects', 'project management', 'data', 'interpersonal skills', 'microsoft azure', 'power bi', 'aiml', 'machine learning', 'business intelligence', 'artificial intelligence', 'tableau', 'stakeholder management', 'gcp', 'leadership', 'project delivery', 'scrum', 'agile', 'organizing', 'aws', 'communication skills']",2025-06-11 06:01:22
Business Analyst,ThoughtFocus,5 - 10 years,20-30 Lacs P.A.,['Bengaluru( Mahadevpura )'],"Only Immediate joiners who can join in 0-10 days\n5+ Years of Experience.\nBachelor's degree in Business, Finance, or a related field.\nCapable of delivering on multiple competing priorities with little supervision.\nExcellent verbal and written communication skills.\nThis role requires good client-facing skills.\nProven experience as a Business Analyst in the alternative asset and capital market .",,,,"['Capital Market', 'Hedge Funds', 'Private Equity', 'Investment Banking', 'Fund Accounting']",2025-06-11 06:01:23
LLMOps Engineer,HCLTech,8 - 10 years,Not Disclosed,['Noida'],"Position Summary\nLLMOps(Large language model operations) Engineer will play a pivotal role in building and maintaining the infrastructure and pipelines for our cutting-edge Generative AI applications, establishing efficient and scalable systems for LLM research, evaluation, training, and fine-tuning. Engineer will be responsible for managing and optimizing large language models (LLMs) across various platforms This position is uniquely tailored for those who excel in crafting pipelines, cloud infrastructure, environments, and workflows. Your expertise in automating and streamlining the ML lifecycle will be instrumental in ensuring the efficiency, scalability, and reliability of our Generative AI models and associated platform. LLMOps engineers expertise will ensure the smooth deployment, maintenance, and performance of these AI platforms and powerful large language models.\n\nYou will follow Site Reliability Engineering & MLOps principles and will be encouraged to contribute your own best practices and ideas to our ways of working.\nReporting to the Head of Cloud Native operations, you will be an experienced thought leader, and comfortable engaging senior managers and technologists. You will engage with clients, display technical leadership, and guide the creation of efficient and complex products/solutions.\nKey Responsibilities\nTechnical & Architectural Leadership\n\nContribute to the technical delivery of projects, ensuring a high quality of work that adheres to best practices, brings innovative approaches and meets client expectations. Project types include following (but not limited to):\nSolution architecture, Proof of concepts (PoCs), MVP, design, develop, and implementation of ML/LLM pipelines for generative AI models, data management & preparation for fine tuning, training, deployment, and monitoring.\nAutomate ML tasks across the model lifecycle.\nContribute to HCL thought leadership across the Cloud Native domain with an expert understanding of advanced AI solutions using Large Language Models (LLM) & Natural Language Processing (NLP) techniques and partner technologies.\nCollaborate with cross-functional teams to integrate LLM and NLP technologies into existing systems.\nEnsure the highest levels of governance and compliance are maintained in all ML and LLM operations.\nStay abreast of the latest developments in ML and LLM technologies and methodologies, integrating these innovations to enhance operational efficiency and model effectiveness.\nCollaborate with global peers from partner ecosystems on joint technical projects. This partner ecosystem includes Google, Microsoft, Nvidia, AWS, IBM, Red Hat, Intel, Cisco, and Dell VMware etc.\nService Delivery\n\nProvide a technical hands-on contribution. Create scalable infra to support enterprise loads (distributed GPU compute, foundation models, orchestrating across multiple cloud vendors, etc.)\nEnsuring the reliable and efficient platform operations.\nApply data science, machine learning, deep learning, and natural language processing methods to analyse, process, and improve the models data and performance.\nUnderstanding of Explainability & Biased Detection concepts.\nCreate and optimize prompts and queries for retrieval augmented generation and prompt engineering techniques to enhance the models capabilities and user experience w.r.t Operations & associated platforms.\nClient-facing influence and guidance, engaging in consultative client discussions and performing a Trusted Advisor role.\nProvide effective support to HCL Sales and Delivery teams.\nSupport sales pursuits and enable HCL revenue growth.\nDefine the modernization strategy for client platform and associated IT practices, create solution architecture and provide oversight of the client journey.\nInnovation & Initiative\n\nAlways maintain hands-on technical credibility, keep in front of the industry, and be prepared to show and lead the way forward to others.\nEngage in technical innovation and support HCLs position as an industry leader.\nActively contribute to HCL sponsorship of leading industry bodies such as the CNCF and Linux Foundation.\nContribute to thought leadership by writing Whitepapers, blogs, and speaking at industry events.\nBe a trusted, knowledgeable internal innovator driving success across our global workforce.\nClient Relationships\n\nAdvise on best practices related to platform & Operations engineering and cloud native operations, run client briefings and workshops, and engage technical leaders in a strategic dialogue.\nDevelop and maintain strong relationships with client stakeholders.\nPerform a Trusted Advisor role.\nContribute to technical projects with a strong focus on technical excellence and on-time delivery.\nMandatory Skills & Experience\nExpertise in designing and optimizing machine-learning operations, with a preference for LLMOps.\nProficient in Data Science, Machine Learning, Python, SQL, Linux/Unix shell scripting.\nExperience on Large Language Models and Natural Language Processing (NLP), and experience with researching, training, and fine-tuning LLMs. Contribute towards fine-tune Transformer models for optimal performance in NLP tasks, if required.\nImplement and maintain automated testing and deployment processes for machine learning models w.r.t LLMOps.\nImplement version control, CI/CD pipelines, and containerization techniques to streamline ML and LLM workflows.\nDevelop and maintain robust monitoring and alerting systems for generative AI models ensuring proactive identification and resolution of issues.\nResearch or engineering experience in deep learning with one or more of the following: generative models, segmentation, object detection, classification, model optimisations.\nExperience implementing RAG frameworks as part of available-ready products.\nExperience in setting up the infrastructure for the latest technology such as Kubernetes, Serverless, Containers, Microservices etc.\nExperience in scripting programming to automate deployments and testing, worked on tools like Terraform and Ansible. Scripting languages like Python, bash, YAML etc.\nExperience on CI/CD opensource and enterprise tool sets such as Argo CD, Jenkins.\nExperience with the GitHub/DevOps Lifecycle\nExperience in at least one of the Observability solutions (Prometheus, EFK stacks, ELK stacks, Grafana, Dynatrace, AppDynamics)\nExperience in at-least one of the clouds for example - Azure/AWS/GCP\nSignificant experience on microservices-based, container-based or similar modern approaches of applications and workloads.\nYou have exemplary verbal and written communication skills (English). Able to interact and influence at the highest level, you will be a confident presenter and speaker, able to command the respect of your audience.\nDesired Skills & Experience\nBachelor level technical degree or equivalent experience; Computer Science, Data Science, or Engineering background preferred; masters degree desired.\nExperience in LLMOps or related areas, such as DevOps, data engineering, or ML infrastructure.\nHands-on experience in deploying and managing machine learning and large language model pipelines in cloud platforms (e.g., AWS, Azure) for ML workloads.\nFamiliar with data science, machine learning, deep learning, and natural language processing concepts, tools, and libraries such as Python, TensorFlow, PyTorch, NLTK etc.\nExperience in using retrieval augmented generation and prompt engineering techniques to improve the models quality and diversity to improve operations efficiency. Proven experience in developing and fine-tuning Language Models (LLMs).\nStay up-to-date with the latest advancements in Generative AI, conduct research, and explore innovative techniques to improve model quality and efficiency.\nThe perfect candidate will already be working within a System Integrator, Consulting or Enterprise organisation with 8+ years of experience in a technical role within the Cloud domain.\nDeep understanding of core practices including SRE, Agile, Scrum, XP and Domain Driven Design. Familiarity with the CNCF open-source community.\nEnjoy working in a fast-paced environment using the latest technologies, love Labs dynamic and high-energy atmosphere, and want to build your career with an industry leader.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLMOps', 'Architectural Leadership', 'Machine Learning', 'Unix shell scripting', 'SQL', 'Data Science', 'NLP', 'Linux', 'Terraform', 'Ansible', 'CI/CD', 'technical delivery', 'AWS', 'Python']",2025-06-11 06:01:25
Business Analyst,TechStar Group,7 - 12 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","The resource should have the below skills:\nBusiness Analyst - 7 to 10years' experience in Capital markets or global treasury.\nKnowledge of capital markets & financial instruments Equities, Derivatives, Fixed Income, FX.\nUnderstanding of Investment Banking and Asset Management  Front, Middle & Back Office functions.\nAsset management experience of Trading Compliance rule coding & testing is strongly preferred, but other experience within Asset Management industry is acceptable Financial industry knowledge within investments and distribution highly desired (i.e., understanding of investment process, capital markets, fixed income, equities Sound knowledge on Database SQL querying.\nShould be a strong team player Excellent communication skills - written & verbal.\nShould be able to create good documentation and correspond on functionalities & issues concisely.\nShould be able to articulate well during discussions.\nShould be able to work with tight deadlines Confident of interacting with business users and various stakeholders.\nResponsibility: Should be able to work with tight deadlines • Confident of interacting with business users and various stakeholders. • Skilled at using MS Excel, Word, PowerPoint & Visio.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Capital Market', 'Business Analyst', 'Investment Banking']",2025-06-11 06:01:26
Snowflake - Senior Technical Lead,Sopra Steria,6 - 10 years,Not Disclosed,['Noida'],"Position: Snowflake - Senior Technical Lead\nExperience: 8-11 years\nLocation: Noida/ Bangalore\nEducation: B.E./ B.Tech./ MCA\nPrimary Skills: Snowflake, Snowpipe, SQL, Data Modelling, DV 2.0, Data Quality, AWS, Snowflake Security\nGood to have Skills: Snowpark, Data Build Tool, Finance Domain  \nPreferred Skills",,,,"['data', 'scala', 'administration', 'data warehousing', 'snowpipe', 'sql', 'star schema', 'cloud', 'scripting', 'security', 'java', 'data modeling', 'gcp', 'data vault', 'etl', 'architecture', 'snowflake', 'python', 'performance tuning', 'talend', 'microsoft azure', 'cloud platforms', 'javascript', 'data quality', 'build', 'aws', 'informatica']",2025-06-11 06:01:28
Senior Power Bi Developer,Course5 Intelligence,6 - 10 years,12-20 Lacs P.A.,['Bengaluru'],"Job Summary\nWe are looking for a skilled and detail-oriented Power BI Developer to join our team. The ideal candidate will be responsible for designing, developing, and maintaining business intelligence dashboards and reports that help drive data-driven decision-making across the organization.\nJob Responsibilities\nDevelop and maintain Power BI dashboards, reports, and visualizations based on business requirements\nPerform data modeling, data analysis, and implement DAX queries for enhanced insights\nConnect Power BI to various data sources (SQL Server, Excel, cloud data platforms, etc.)\nWork closely with stakeholders to understand reporting needs and translate them into technical solutions\nOptimize Power BI datasets and dashboards for performance and usability\nEnsure data accuracy, reliability, and consistency across all reports and dashboards.\nCollaborate with data engineers and analysts to streamline data pipelines and reporting logic\nImplement role-level security (RLS) in Power BI.\n\nRequired Skills & Qualifications:\n• Proven experience (4+ years) working with Power BI in a professional setting\n• Strong proficiency in DAX, Power Query (M language), and data modeling\n• Experience working with SQL and relational databases\n• Knowledge of ETL processes, data warehousing, and performance tuning\n• Strong analytical and problem-solving skills\n• Excellent communication and collaboration abilities • Experience with Power BI Service (publishing, workspace management, gateway configuration)\n• Bachelors degree in Computer Science, Information Systems, or a related field",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Dax Queries', 'Power Bi', 'Data Warehousing', 'ETL', 'SQL', 'Power Bi Reports', 'Power Bi Dashboards', 'Power Bi Desktop']",2025-06-11 06:01:30
ETL and BDX developer,"NTT DATA, Inc.",10 - 15 years,Not Disclosed,['Bengaluru'],"Req ID: 321918\n\nWe are currently seeking a ETL and BDX developer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""¢ Develop and maintain Power BI dashboards, reports, and datasets.\n\n""¢ Collaborate with stakeholders to gather and analyse business requirements.\n\n""¢ Design robust and scalable data models using Power BI and underlying data sources.\n\n""¢ Write complex DAX expressions for calculated columns, measures, and KPIs.\n\n""¢ Optimize performance of Power BI reports and data models.\n\n""¢ Integrate Power BI with other data sources (SQL Server, Excel, Azure, SharePoint, etc.).\n\n""¢ Implement row-level security and data access control.\n\n""¢ Automate data refresh schedules and troubleshoot refresh failures.\n\n""¢ Mentor junior developers and conduct code reviews.\n\n""¢ Work closely with data engineering teams to ensure data accuracy and integrity.\n\n""¢ Exp working on Power Query and data flows.\n\n""¢ Strong in writing SQL queries\n\n\n\nTotal Exp7 ""“ 10 Yrs.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql queries', 'power bi', 'microsoft azure', 'sql server', 'power query', 'power bi reports', 'ssas', 'bi', 'sharepoint', 'data engineering', 'dashboards', 'sql', 'data modeling', 'power bi dashboards', 'ssrs', 'dax', 'code review', 'etl', 'ssis', 'etl development', 'data flow', 'msbi']",2025-06-11 06:01:31
"Lead, Application Development",S&P Global Market Intelligence,10 - 13 years,Not Disclosed,"['Hyderabad', 'Gurugram', 'Ahmedabad']","\n\nAbout the Role: \n\nGrade Level (for internal use):\n11\n\nS&P Global EDO\n\nThe\n\nRole: Lead- Software Engineering IT- Application Development.\n\nJoin Our Team:Step into a dynamic team at the cutting edge of data innovation! Youll collaborate daily with talented professionals from around the world, designing and developing next-generation data products for our clients. Our team thrives on a diverse toolkit that evolves with emerging technologies, offering you the chance to work in a vibrant, global environment that fosters creativity and teamwork.\n\nThe Impact:As a Lead Software Developer at S&P Global, youll be a driving force in shaping the future of our data products. Your expertise will streamline software development and deployment, aligning cutting-edge solutions with business needs. By ensuring seamless integration and continuous delivery, youll enhance product capabilities, delivering high-quality systems that meet the highest standards of availability, security, and performance. Your work will empower our clients with impactful, data-driven solutions, making a real difference in the financial world.\n\nWhats in it for You:\n\n\nCareer Development: Build a rewarding career with a global leader in financial information and analytics, supported by continuous learning and a clear path to advancement.\n\n\nDynamic Work Environment: Thrive in a fast-paced, forward-thinking setting where your ideas fuel innovation and your contributions shape groundbreaking solutions.\n\n\nSkill Enhancement: Elevate your expertise on an enterprise-level platform, mastering the latest tools and techniques in software development.\n\n\nVersatile Experience: Dive into full-stack development with hands-on exposure to cloud computing, Bigdata, and revolutionary GenAI technologies.\n\n\nLeadership Opportunities: Guide and inspire a skilled team, steering the direction of our products and leaving your mark on the future of technology at S&P Global.\n\n\nResponsibilities:\nArchitect and develop scalable Bigdata and cloud applications, harnessing a range of cloud services to create robust, high-performing solutions.\nDesign and implement advanced CI/CD pipelines, automating software delivery for fast, reliable deployments that keep us ahead of the curve.\nTackle complex challenges head-on, troubleshooting and resolving issues to ensure our products run flawlessly for clients.\nLead by example, providing technical guidance and mentoring to your team, driving innovation and embracing new processes.\nDeliver top-tier code and detailed system design documents, setting the standard with technical walkthroughs that inspire excellence.\nBridge the gap between technical and non-technical stakeholders, turning complex requirements into elegant, actionable solutions.\nMentor junior developers, nurturing their growth and helping them build skills and careers under your leadership.\n\n\nWhat Were Looking For:Were seeking a passionate, experienced professional with:\n10-13 years of hands-on experience designing and building data-intensive solutions using distributed computing, showcasing your mastery of scalable architectures.\nProven success implementing and maintaining enterprise search solutions in large-scale environments, ensuring peak performance and reliability.\nA history of partnering with business stakeholders and users to shape research directions and craft robust, maintainable products.\nExtensive experience deploying data engineering solutions in public clouds like AWS, GCP, or Azure, leveraging cloud power to its fullest.\nAdvanced programming skills in Python, Java, .NET or Scala, backed by a portfolio of impressive projects.\nStrong knowledge of Gen AI tools (e.g., GitHub Copilot, ChatGPT, Claude, or Gemini) and their power to boost developer productivity.\nExpertise in containerization, scripting, cloud platforms, and CI/CD practices, ready to shine in a modern development ecosystem.\n5+ years working with Python, Java, .NET, Kubernetes, and data/workflow orchestration tools, proving your technical versatility.\nDeep experience with SQL, NoSQL, Apache Spark, Airflow, or similar tools, operationalizing data-driven pipelines for large-scale batch and stream processing.\nA knack for rapid prototyping and iteration, delivering high-quality solutions under tight deadlines.\nOutstanding communication and documentation skills, adept at explaining complex ideas to technical and non-technical audiences alike.\n\n\nTake the Next Step:Ready to elevate your career and make a lasting impact in data and technologyJoin us at S&P Global and help shape the future of financial information and analytics. Apply today!\n\nReturn to Work\n\nHave you taken time out for caring responsibilities and are now looking to return to workAs part of our Return-to-Work initiative (link to career site page when available), we are encouraging enthusiastic and talented returners to apply and will actively support your return to the workplace.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['airflow', 'data engineering', 'sql', 'nosql', 'spark', 'continuous integration', 'public cloud', 'kubernetes', 'python', 'github', 'scala', 'documentation', 'ci/cd', 'microsoft azure', 'cloud platforms', 'distributed computing', 'containerization', 'gen', 'java', 'gcp', '.net', 'aws', 'big data', 'cloud computing']",2025-06-11 06:01:33
AdOps Analyst,Omnicom Media Group,2 - 5 years,Not Disclosed,"['Hyderabad', 'Bangalore Rural', 'Chennai']","This exciting role of an Analyst - AdOps requires you to creatively manage digital media campaigns for our global brands. Your expertise of ad tech and knowledge of the Digital Market Cycle would make you a great fit for this position. This is a great opportunity to work closely with the Top Global brands and own large and reputed accounts\n\nAbout Annalect India\nWe are an integral part of Annalect Global and Omnicom Group, the second largest advertising agency holding company in the world in terms of revenue and is the leading global marketing communications company. Our portfolio includes: three global advertising agency networks: BBDO, DDB and TBWA; three of the worlds premium media services: OMD, PHD and Hearts & Science.\nAnnalect India plays a key role for our group companies and global agencies by providing stellar products and services in areas of Creative Services, Technology, Marketing Science (data & analytics) and Media Services. We are growing rapidly and looking for talented professionals like you to be part of this journey.\nLet us build this, together!\n\nResponsibilities\nThis is an exciting role and would entail you to\nSetup and execute digital media campaigns using ad servers like Google Campaign Manager, Sizmek, Adform and Flashtalking etc.\nQuality review of creative to ensuring technical Specs are met and ads appear properly in the desired formats\nTrafficking sheet review, upload and assign creative to appropriate ads as per the trafficking sheet\nExecute ongoing campaign optimizations and adjustments\nAnalyse and understand the campaign brief thoroughly before executing any requests within defined SOPs and procedures\nTroubleshoot campaign delivery and tracking issues across platforms/Ad techs and proactively suggesting the appropriate solutions\nEffectively communicate the technical information with wider team\nSupport team lead with day-to-day technical solutions to maintain a smooth operation and meet the respective stakeholder’s requirement.\n\nQualifications\nThis may be the right role for you if you have.\n2 to 5 years of experience in handling digital media campaigns and technical support  \nGood understanding of ad techs like Google Campaign Manager, Ad Manager and Business Manager\nGood Knowledge of digital marketing life cycle\nFamiliarity with JavaScript, Iframe & HTML tags and troubleshooting the technical issues\nUnderstanding of various third-party, fourth party and ad verification tags specifications, DoubleClick, AppNexus, DMPs and IAS etc.\nGood analytical skills and logical reasoning in managing daily deliverables\nStrong written and verbal communication.",Industry Type: Advertising & Marketing (Digital Marketing),Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['Ad Trafficking', 'Ad Operations', 'CM630', 'Campaign Management', 'DCM']",2025-06-11 06:01:35
Walkin || Cognizant is hiring For Abinitio developer,Cognizant,6 - 9 years,Not Disclosed,['Kochi'],"Greetings from Cognizant!! #MegaWalkIn\n\nWe have an exciting opportunity for the #Abinitio Developer Role with Cognizant, join us if you are an aspirant for matching the below criteria!!\n\nPrimary Skill: Abinitio Developer\nExperience: 6-9 years\nJob Location: PAN India",,,,"['Abinitio Graphs', 'Initio', 'Ab Initio']",2025-06-11 06:01:36
AI/ML Lead Engineer (Senior AI Engineer),Conversehr Business Solutions,7 - 12 years,30-45 Lacs P.A.,['Hyderabad'],"What is AI Engineer team responsible for?\nAs a Senior AI Engineer, youll be a key member of the Data & AI team. This team is responsible for designing and delivering data engineering, analytics, and generative AI solutions that drive meaningful business impact. Were looking for a pragmatic, results-driven problem solver who thrives in a fast-paced environment and is passionate about building solutions at scale.\nThe ideal candidate has a strong technical foundation, a collaborative mindset, and the ability to navigate complex challenges. You should be comfortable working in a fast-moving, startup-like environment within an established enterprise, and should bring strong skill sets to adapt new solutions fast. You will play a crucial role in integrating AI solutions in our existing digital solutions, optimizing our data infrastructure, and enabling insights through data #MID_SENIOR_LEVEL\nWhat is a Digital & AI/ML Lead Engineer (Senior AI Engineer) responsible for?\nServe as a hands-on technical lead, driving project execution and delivery in our growing AI team based in the Hyderabad office.\nCollaborate closely with the U.S.-based team and cross-functional stakeholders to understand business needs and deliver scalable, AI-powered solutions.\nDesign and build AI applications leveraging best smart solutions.\nProvide quick prototype and evaluation AI/ML solutions aligned with business objectives.\nStay current with emerging trends in AI, and machine learning and help implement best practices within the team.\nMentor and support junior engineers, fostering a culture of learning and technical excellence.\nManage unstructured data and generate embeddings that can further be leveraged into AI products.\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors or master’s degree in computer science, data science, engineering, or a related field from a premium institute.\n7+ years of experience in engineering, software engineering, data science, or machine learning, including 3+ years in a technical leadership role.\nStrong understanding with data pipelines, Snowflake ecosystem and master data management.\nProficiency in Python.\nExperience working with unstructured data, large language models (LLMs), embeddings, and building generative AI prototypes.\nSelf-starter with a passion for learning new tools and technologies.\nStrong communication skills and a collaborative, ownership-driven mindset.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Python', 'ai/ml', 'genAI', 'Generative Ai', 'Large Language Model', 'Snowflake', 'Master Data Management', 'llm']",2025-06-11 06:01:38
Regulatory Affairs Specialist,HCLTech,2 - 4 years,Not Disclosed,"['Madurai', 'Chennai']","Education: B.E Mechanical/Bio-Medical\nExperience: 2.5 to 5 yrs Knowledge on Medical device regulations Good Communication skills Ability to read through the Medical Device Documents Ability to work on Microsoft tools (Excel, Word and PowerPoint) Experience on Medical Device UDI Data management will be additional preference.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MDR', 'Regulatory Affairs', 'Medical Devices']",2025-06-11 06:01:40
Solution Architect - L1,Wipro,8 - 10 years,Not Disclosed,['Pune'],"Role Purpose\n\nThe purpose of the role is to create exceptional architectural solution design and thought leadership and enable delivery teams to provide exceptional client engagement and satisfaction.\n\n\n\n\n\nDo\n1.Develop architectural solutions for the new deals/ major change requests in existing deals\nCreates an enterprise-wide architecture that ensures systems are scalable, reliable, and manageable.\nProvide solutioning of RFPs received from clients and ensure overall design assurance\nDevelop a direction to manage the portfolio of to-be-solutions including systems, shared infrastructure services, applications in order to better match business outcome objectives\nAnalyse technology environment, enterprise specifics, client requirements to set a collaboration solution design framework/ architecture\nProvide technical leadership to the design, development and implementation of custom solutions through thoughtful use of modern technology\nDefine and understand current state solutions and identify improvements, options & tradeoffs to define target state solutions\nClearly articulate, document and sell architectural targets, recommendations and reusable patterns and accordingly propose investment roadmaps\nEvaluate and recommend solutions to integrate with overall technology ecosystem\nWorks closely with various IT groups to transition tasks, ensure performance and manage issues through to resolution\nPerform detailed documentation (App view, multiple sections & views) of the architectural design and solution mentioning all the artefacts in detail\nValidate the solution/ prototype from technology, cost structure and customer differentiation point of view\nIdentify problem areas and perform root cause analysis of architectural design and solutions and provide relevant solutions to the problem\nCollaborating with sales, program/project, consulting teams to reconcile solutions to architecture\nTracks industry and application trends and relates these to planning current and future IT needs\nProvides technical and strategic input during the project planning phase in the form of technical architectural designs and recommendation\nCollaborates with all relevant parties in order to review the objectives and constraints of solutions and determine conformance with the Enterprise Architecture\nIdentifies implementation risks and potential impacts\n2.Enable Delivery Teams by providing optimal delivery solutions/ frameworks\nBuild and maintain relationships with executives, technical leaders, product owners, peer architects and other stakeholders to become a trusted advisor\nDevelops and establishes relevant technical, business process and overall support metrics (KPI/SLA) to drive results\nManages multiple projects and accurately reports the status of all major assignments while adhering to all project management standards\nIdentify technical, process, structural risks and prepare a risk mitigation plan for all the projects\nEnsure quality assurance of all the architecture or design decisions and provides technical mitigation support to the delivery teams\nRecommend tools for reuse, automation for improved productivity and reduced cycle times\nLeads the development and maintenance of enterprise framework and related artefacts\nDevelops trust and builds effective working relationships through respectful, collaborative engagement across individual product teams\nEnsures architecture principles and standards are consistently applied to all the projects\nEnsure optimal Client Engagement\nSupport pre-sales team while presenting the entire solution design and its principles to the client\nNegotiate, manage and coordinate with the client teams to ensure all requirements are met and create an impact of solution proposed\nDemonstrate thought leadership with strong technical capability in front of the client to win the confidence and act as a trusted advisor\n\n\n\n\n\n3.Competency Building and Branding\nEnsure completion of necessary trainings and certifications\nDevelop Proof of Concepts (POCs),case studies, demos etc. for new growth areas based on market and customer research\nDevelop and present a point of view of Wipro on solution design and architect by writing white papers, blogs etc.\nAttain market referencability and recognition through highest analyst rankings, client testimonials and partner credits\nBe the voice of Wipros Thought Leadership by speaking in forums (internal and external)\nMentor developers, designers and Junior architects in the project for their further career development and enhancement\nContribute to the architecture practice by conducting selection interviews etc\n\n\n\n\n\n4.Team Management\nResourcing\nAnticipating new talent requirements as per the market/ industry trends or client requirements\nHire adequate and right resources for the team\nTalent Management\nEnsure adequate onboarding and training for the team members to enhance capability & effectiveness\nBuild an internal talent pool and ensure their career progression within the organization\nManage team attrition\nDrive diversity in leadership positions\nPerformance Management\nSet goals for the team, conduct timely performance reviews and provide constructive feedback to own direct reports\nEnsure that the Performance Nxt is followed for the entire team\nEmployee Satisfaction and Engagement\nLead and drive engagement initiatives for the team\nTrack team satisfaction scores and identify initiatives to build engagement within the team\nMandatory Skills: DataBricks - Data Engineering.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['DataBricks', 'Team Management', 'Data Engineering', 'solution design', 'Employee Engagement', 'Performance Management']",2025-06-11 06:01:41
BI Engineering Sr Manager,Amgen Inc,4 - 8 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be a Senior Manager BI & Visualization to lead and drive enterprise-wide business intelligence (BI) and data visualization initiatives. This role will be responsible for strategic planning, governance, and execution of BI and analytics solutions, ensuring that business leaders have access to actionable insights through advanced reporting and visualization platforms. The ideal candidate will have a deep understanding of BI tools, data visualization best practices, self-service BI enablement, and enterprise analytics strategy, working closely with business executives, data teams, and technology leaders to foster a data-driven culture.\nDevelop and execute a strategic BI & visualization roadmap, aligning with business goals, analytics objectives, and digital transformation strategies.\nLead and mentor BI, analytics, and visualization teams, fostering a culture of innovation, collaboration, and continuous learning.\nOwn the end-to-end BI lifecycle, including data modeling, dashboard development, analytics governance, and self-service BI adoption.\nOversee the implementation of modern BI solutions, leveraging tools like Power BI, Tableau, Looker, Qlik Sense, or similar to deliver high-impact visual insights.\nDefine and enforce data visualization best practices, ensuring dashboards are intuitive, user-friendly, and business-focused.\nDrive self-service BI enablement, empowering business users to explore, analyze, and act on data independently while maintaining data security and governance.\nCollaborate with business leaders, data scientists, and engineering teams to identify and prioritize high-value analytics use cases.\nOptimize BI infrastructure and reporting architecture, ensuring scalability, performance, and cost efficiency.\nEstablish BI governance frameworks, defining data access controls, security policies, KPI standardization, and metadata management.\nChampion the use of AI/ML-powered BI solutions, enabling predictive analytics, anomaly detection, and natural language-driven insights.\nMonitor BI performance metrics, ensuring reporting solutions meet business SLAs, operational efficiency, and data accuracy.\nStay ahead of emerging trends in BI, data visualization, and analytics automation, ensuring the company remains competitive in its data strategy.\nWhat we expect of you\nMasters degree and 8 to 10 years of experience in Computer Science, IT or related field OR\nBachelors degree and 10 to 14 years of experience in Computer Science, IT or related field OR\nDiploma and 14 to 18 years of experience in Computer Science, IT or related field.\nCertifications on PowerBI / Any other visualization tools\nBasic Qualifications:\n10-14 + years of experience in BI, analytics, and data visualization, with at least 5 years in a leadership role.\nExpertise in BI tools, including Power BI, Tableau, Looker, Qlik Sense similar enterprise BI platforms.\nStrong proficiency in data visualization principles, storytelling with data, and dashboard usability best practices.\nExperience in leading large-scale BI transformation initiatives, driving self-service analytics adoption across an enterprise.\nStrong knowledge of data modeling, dimensional modeling (star/snowflake schema), and data warehousing concepts.\nHands-on experience with SQL, DAX, Power Query (M), or other analytics scripting languages.\nStrong background in BI governance, data security, compliance, and metadata management.\nAbility to influence senior leadership, communicate insights effectively, and drive business impact through BI.\nExcellent problem-solving skills, with a track record of driving efficiency, automation, and data-driven decision-making.\nPreferred Qualifications:\nExperience in Biotechnology or pharma industry is a big plus\nExperience with Data Mesh, Data Fabric, or Federated Data Governance models.\nExperience with AI/ML-driven BI solutions, predictive analytics, and NLP-based BI capabilities.\nKnowledge infrastructure & deploayment automation for visualization platforms.\nExperience integrating BI with ERP, CRM, and operational systems (SAP, Salesforce, Oracle, Workday, etc.).\nFamiliarity with Agile methodologies and Scaled Agile Framework (SAFe) for BI project delivery.\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business intelligence', 'NLP', 'SAP', 'DAX', 'Workday', 'Oracle', 'Data Governance', 'Power Query', 'SQL', 'Salesforce']",2025-06-11 06:01:43
Machine Learning Engineer,SRS Infoway,5 - 7 years,16-17 Lacs P.A.,"['Pune', 'Bengaluru']","ML Engineer with 8+ yrs in Data & Analytics, 5+ in ML. Skilled in Python, FastAPI, Azure, APIs, and deployment. Agile team player, handles multiple projects, values diversity, inclusion, and flexibility.\nMail:kowsalya.k@srsinfoway.com",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Temporary/Contractual","['Power BI', 'Analytics modeling', 'Python', 'ML']",2025-06-11 06:01:45
Senior Artificial Intelligence Engineer,Ignitho,4 - 6 years,Not Disclosed,['Chennai( Sholinganallur )'],"Job Title: Senior AI Engineer\nLocation: Chennai\nReports To: Data Architect\n\nAbout the Company:\nIgnitho Inc. is a leading AI and data engineering company with a global presence, including US, UK, India, and Costa Rica offices.\nVisit our website to learn more about our work and culture: www.ignitho.com.\nIgnitho is a portfolio company of Nuivio Ventures Inc., a venture builder dedicated to developing Enterprise AI product companies across various domains, including AI, Data Engineering, and IoT.\nLearn more about Nuivio at: www.nuivio.com.\n\nJob Summary:\nAs a Senior AI Engineer, the candidate will lead the design, development, and deployment of cutting-edge machine learning and artificial intelligence solutions. The candidate will work closely with cross-functional teams to understand business needs and translate them into scalable AI-driven applications.\n\nKey Responsibilities:\nDesign and implement machine learning models and AI agents/LLMs.\nDevelop and optimize AI pipelines (LLM, RAG, fine-tuning)\nCollaborate with product, engineering, and data teams to define and implement AI-driven features.\nEvaluate model performance and iterate to improve accuracy and efficiency.\nMentor junior engineers and contribute to team best practices.\nStay up to date with state-of-the-art AI technologies and research.\n\nRequired Qualifications:\nBachelors or master’s in computer science, AI, or related field.\n5+ years of experience in AI/ML development.\nExperience working with LLMs, Agentic AI, RAG, and model fine-tuning\nStrong programming skills in Python and familiarity with frameworks such as TensorFlow, PyTorch, and Scikit-learn.\nExperience with cloud platforms (Azure preferred).\nSolid understanding of data preprocessing, model training, validation, and deployment.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'Pytorch', 'Azure Cloud', 'RAG', 'LLM', 'Ai Platform', 'AWS', 'Scikit-Learn', 'TensorFlow']",2025-06-11 06:01:46
Senior Machine Learning Engineer,"Technology, Information and Internet",5 - 10 years,30-35 Lacs P.A.,['Gurugram'],"We are seeking a highly skilled Senior Machine Learning Engineer to join our dynamic team. The ideal candidate will have a deep understanding of Machine Learning, AI, and cloud platforms like Azure. You will play a key role in designing, developing, and deploying ML models and Generative AI solutions to solve complex business problems.\n\nRole overview\nEngineering Masters degree or PhD in Data Science, Statistics, Mathematics, or related fields\n5 years+ experience in a Machine Learning Engineer role into large corporate organizations\nExperience of working with ML models in a cloud ecosystem.\nStatistics & amp; Machine Learning\nStatistics: Strong understanding of statistical analysis and modelling techniques (e.g., regression analysis, hypothesis testing, time series analysis)\nClassical ML: Very strong knowledge in classical ML algorithms for regression & classification, supervised and unsupervised machine learning, both theoretical and practical (e.g. using scikit-learn, xgboost)\nML niche: Expertise in at least one of the following ML specialisations: Timeseries forecasting / Natural Language Processing / Computer Vision\nDeep Learning: Good knowledge of Deep Learning fundamentals (CNN, RNN, transformer architecture, attention mechanism, ) and one of the deep learning frameworks (pytorch, tensorflow, keras)\nGenerative AI: Good understanding of Generative AI specificities and previous experience in working with Large Language Models is a plus (e.g. with openai, langchain).\nMLOps\nModel strategy: Expertise in designing, implementing, and testing machine learning strategies.\nModel integration: Very strong skills in integrating a machine learning algorithm in a data science application in production.\nModel performance: Deep understanding of model performance evaluation metrics and existing libraries (e.g., scikit-learn, evidently)\nModel deployment: Experience in deploying and managing machine learning models in production either using specific cloud platform, model serving frameworks, or containerization.\nModel monitoring: Experience with model performance monitoring tools is a plus (Grafana, Prometheus).\nSoftware Engineering\nPython: Very strong coding skills in Python including modularity, OOP, data & config manipulation frameworks (e.g., pandas, pydantic) etc.\nPython ecosystem: Strong knowledge of tooling in Python ecosystem such as dependency management tooling (venv, poetry), documentation frameworks (e.g. sphinx, mkdocs, jupyter-book), testing frameworks (unittest,pytest)\nSoftware engineering practices: Experience in putting in place good software engineering practices such as design patterns, testing (unit, integration), clean code, code formatting etc.\nDebugging: Ability to troubleshoot and debug issues within machine learning pipelines.\nData Science Experimentation and Analytics\nData Visualization: Knowledge of data visualization tools such as plotly, seaborn, matplotlib, etc. to visualise, interpret and communicate the results of machine learning models to stakeholders. Basic knowledge of PowerBI is a plus.\nData Cleaning: Experience with data cleaning and preprocessing techniques such as feature scaling, dimensionality reduction, and outlier detection (e.g. with pandas, scikit-learn).\nData Science Experiments: Understanding of experimental design and A/B testing methodologies.\nData Processing:\nDatabricks/Spark: Basic knowledge of PySpark for big data processing\nDatabases: Basic knowledge of SQL to query data in internal systems\nData Formats: Familiarity with different data storage formats such as Parquet and Delta.\nDevOps\nAzure DevOps: Experience using a DevOps platform such as Azure DevOps for using Boards, Repositories, Pipelines\nGit: Experience working with code versioning (git), branch strategies, and collaborative work with pull requests. Proficient with the most basic git commands.\nCI / CD: Experience in implementing/maintaining pipelines for continuous integration (including execution of testing strategy) and continuous deployment is preferable.\nCloud Platform:\nAzure Cloud: Previous experience with services like Azure Machine Learning Services and/or Azure Databricks on Azure is preferable.\nSoft skills:\nStrong analytical and problem-solving skills, with attention to detail\nExcellent verbal and written communication and pedagogical skills with technical and non-technical teams\nExcellent teamwork and collaboration skills\nAdaptability and reactivity to new technologies, tools, and techniques\nFluent in English.\nWhat would you do here:\nManaging the lifecycle of machine learning models\nDevelop and implement machine learning models to solve complex business problems.\nEnsure that models are accurate, efficient, reliable, and scalable.\nDeploy machine learning models to production environments, ensuring that models are integrated with software systems.\nMonitor machine learning models in production, ensuring that models are performing as expected and that any errors or performance issues are identified and resolved quickly.\nMaintain machine learning models over time. This includes updating models as new data becomes available, retraining models to improve performance, and retiring models that are no longer effective.\nDevelop and implement policies and procedures for ensuring the ethical and responsible use of machine learning models. This includes addressing issues related to bias, fairness, transparency, and accountability.\nContinuous Improvements:\nStay up to date with the latest developments in the field: read research papers, attend conferences, and participate in trainings to expand their knowledge and skills.\nIdentify and evaluate new technologies and tools that can improve the efficiency and effectiveness of machine learning projects.\nPropose and implement optimizations for current machine learning workflows and systems.\nProactively identify areas of improvement within the pipelines.\nMake sure that created code is compliant with our set of engineering standards.\nCollaboration with other data experts (Data Engineers, Platform Engineers, and Data Analysts)\nParticipate to pull requests reviews coming from other team members.\nAsk for review and comments when submitting their own work.\nActively participate to the day-to-day life of the project (Agile rituals), the data science team (DS meeting) and the rest of the Global Engineering team.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure', 'Generative AI', 'Artificial Intelligence', 'Machine Learning', 'Python']",2025-06-11 06:01:48
Lead Analytics Consultant,Course5 Intelligence,4 - 8 years,20-30 Lacs P.A.,['Bengaluru'],"Job Responsibilities\nDevelop clear, concise, actionable solutions and recommendations for Client's business needs. • Work with complex data files and develop initial understanding on data files\nUndertake exploratory data analysis to derive initial findings and create hypothesis\nWork on problem specific data models\nUndertake hands on work on data analytics, model development and testing and preparing the data files for visualization platforms\nUndertake business analysis on the data and provide insights\nCoordinate with decision makers to translate business questions into a verifiable hypothesis and data models\n• Work hands-on across various analytics problems and provide thought leadership on problems • Interact with other stakeholders (Data engineers, BI specialists, offsite team, client teams) on daily/weekly basis to gather requirements/ provide updates.\n\n\nRequirements & Qualifications:\n• 4-7 years of experience of in data science domain working across a variety of industries\n• Graduation or Post graduation in Statistics, Mathematics, Management etc.\n• Strong CPG domain knowledge\n• Have exposure to CPG marketing analytics Trade (ATL/BTL), Digital, Traditional\n• Ability to think open ended/unstructured problem solving\n• Good understanding of statistics along with analysis techniques like Correlation, Regression, Hypothesis Testing, Basic Time Series Forecasting etc.\n• Excellent Data Wrangling – using Excel, Python basics (good to have)\n• Hands on experience on SQL, Python coding\n• Strong logical, analytical, and problem-solving skills\n• Adept at writing reports, making PowerPoint presentations to present findings\n• Excellent verbal and written communication skills along with experience of client facing roles",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Wrangling', 'Cpg', 'Marketing Mix', 'Marketing Analytics', 'Python', 'Consumer Insights']",2025-06-11 06:01:49
Walkin || Cognizant is hiring For Databricks developer,Cognizant,4 - 9 years,Not Disclosed,['Kochi'],"Greetings from Cognizant!! #MegaWalkIn\n\nWe have an exciting opportunity for the #Databricks Developer Role with Cognizant, join us if you are an aspirant for matching the below criteria!!\n\nPrimary Skill: Databricks Developer\nExperience: 4-9 years\nJob Location: PAN India",,,,"['Azure Databricks', 'Data Bricks', 'ADB']",2025-06-11 06:01:51
Offshore Program Manager,"NTT DATA, Inc.",12 - 14 years,Not Disclosed,['Pune'],"Req ID: 326837\n\nWe are currently seeking a Offshore Program Manager to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesOffshore BA / PM\nGrade 10/11\nSeeking an offshore program manager for managing data projects for Australia, India, and APAC region.\n\nThe individual must be able to guide the offshore project team located in different locations across India. The role involves planning, executing, and overseeing data related projects ensuring on time and within budget delivery.\n\nKey roles and responsibilities ""“\n""¢ Requirement Analysis ""“ Act as a bridge between technical development team and onshore team to understand and document the project requirements\n""¢ Project / Program Management ""“\no Project planning, timelines, and resource allocation / management\no Managing project risks related to quality, budget, and resources\no Team Management ""“ Lead and motivate offshore teams including data engineers, QA team, and data SMEs resolving potential issues that may arise during execution\no Budget management ""“ Manage project financials and budgets\no Compliance and standards management\no Risk management ""“ Identify and mitigate potential risks like scope creep, timelines etc.\no Ability to track and monitor project KPIs like defect density, Project health index etc.\n""¢ Stakeholder communication ""“\no Excellent communication and interpersonal skills with onshore / offshore / nearshore teams\no Client communication with F2F client meetings and presentations\no Ability to communicate with senior management within NTTDATA\n\nPreferred Skills and Qualifications ""“\n""¢ Overall, 12-14 years of work experience\n""¢ 7+ years of enterprise Technical Program Management experience supporting data projects.\n""¢ Data lifecycle management ""“ Understanding of data lifecycle management principles including data acquisition, ingestion, data quality, data consumption, and data visualization\n""¢ Exposure to AI and Gen AI fundamental concepts\n""¢ 7+ years of experience with Agile and Waterfall methodologies\n""¢ Ability to travel at least 25%\n""¢ Graduate degree or equivalent combination of education and work experience.\n""¢ Undergraduate or Graduate degree preferred\n\nMinimum Skills RequiredOffshore BA / PM\nGrade 10/11\nSeeking an offshore program manager for managing data projects for Australia, India, and APAC region.\n\nThe individual must be able to guide the offshore project team located in different locations across India. The role involves planning, executing, and overseeing data related projects ensuring on time and within budget delivery.\n\nKey roles and responsibilities ""“\n""¢ Requirement Analysis ""“ Act as a bridge between technical development team and onshore team to understand and document the project requirements\n""¢ Project / Program Management ""“\no Project planning, timelines, and resource allocation / management\no Managing project risks related to quality, budget, and resources\no Team Management ""“ Lead and motivate offshore teams including data engineers, QA team, and data SMEs resolving potential issues that may arise during execution\no Budget management ""“ Manage project financials and budgets\no Compliance and standards management\no Risk management ""“ Identify and mitigate potential risks like scope creep, timelines etc.\no Ability to track and monitor project KPIs like defect density, Project health index etc.\n""¢ Stakeholder communication ""“\no Excellent communication and interpersonal skills with onshore / offshore / nearshore teams\no Client communication with F2F client meetings and presentations\no Ability to communicate with senior management within NTTDATA\n\nPreferred Skills and Qualifications ""“\n""¢ Overall, 12-14 years of work experience\n""¢ 7+ years of enterprise Technical Program Management experience supporting data projects.\n""¢ Data lifecycle management ""“ Understanding of data lifecycle management principles including data acquisition, ingestion, data quality, data consumption, and data visualization\n""¢ Exposure to AI and Gen AI fundamental concepts\n""¢ 7+ years of experience with Agile and Waterfall methodologies\n""¢ Ability to travel at least 25%\n""¢ Graduate degree or equivalent combi",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['artificial intelligence', 'data quality', 'technical program management', 'waterfall', 'agile', 'data life cycle management', 'risk management', 'consumables', 'program management', 'budgeting', 'resource allocation', 'data acquisition', 'data visualization', 'project planning']",2025-06-11 06:01:53
Walkin || Cognizant is hiring For AWS Services,Cognizant,6 - 9 years,Not Disclosed,['Kochi'],"Greetings from Cognizant!! #MegaWalkIn\n\nWe have an exciting opportunity for the #AWS Services Role with Cognizant, join us if you are an aspirant for matching the below criteria!!\n\nPrimary Skill: AWS Services\nExperience: 6-9 years\nJob Location: PAN India",,,,"['AWS Services', 'Aws Lambda', 'Aws Glue']",2025-06-11 06:01:54
Snowflake Developer,Tata Consultancy Services,5 - 10 years,Not Disclosed,['Hyderabad'],Skill- Snowflake\nLocation- Hyderabad\nExperience- 5 to 10years\n\nJD\n\nIT development experience with min 3+ years hands-on experience in Snowflake,,,,"['Snowflake', 'Python', 'SQL']",2025-06-11 06:01:56
Business Analyst,Apex Group,1 - 4 years,Not Disclosed,"['Pune', 'Greater Noida']","IT Business Analyst\n\nTheir primary responsibilities of the IT Business Analyst will be working with stakeholders to elicit requirements, analyses and propose IT solutions and deliver functional documentation, design documentation, assisting the development and quality teams during the implementation phases and ensure that the solution delivered matches with requirements.\n\nThe successful applicant will be able to quickly understand the business strategy and objectives and act as a link between the business stakeholders and developers. It will be necessary to successfully handle multiple tasks, projects and priorities in a fast-paced environment.\n\nJob Specification\n\nDefine and document business processes and requirements through collaborative stakeholder engagement. This includes working with the business to deliver solutions with a smooth transition through to the production environment.\n\nProduce technical (functional and non-functional) documentation as BRD’s, RFP’s, DRS’s and, end user product documentation.\n\nAbility to clearly communicate ideas to both technical stakeholders and business end users.\n\nInterpreting business requirements and translating these into viable solutions with tangible business benefits.\n\nSupport development and testing during the build and test phases.\n\nWrite test scripts and carry out user acceptance testing, on-going management and maintenance of business systems including end user support.\n\nIdentify new opportunities for technology.\n\n\nSkills Required\n\nExperience of 5+ years working as a IT business analyst.\n\nExperience in the financial services industry is preferable.\n\nDemonstrated ability to work in a changing and challenging environment with a record of producing high quality business analysis documentation.\n\nKnowledge of all phases of the development lifecycle.\n\nMotivated self-starter who can work in a team environment.\n\n3rd level Qualification (preferably IT related).\n\nExperience with UML, BMPN or other modelling notations.\n\nExcellent communication, listening and writing skills\n\nExcellent stakeholder management skills.\n\nBe able to plan and track activities/tasks\n\nAbility to work to complete work to schedule\n\nAbility to multi-task\n\nBackground in software development.\n\nExperience of implementation and rollout to critical systems.\n\nBe willing to travel (limited) if required to support project objectives\n\nPBA or CCBA preferable.\n\n\nWhat you will get in return:\n\nA genuinely unique opportunity to be part of an expanding large global business\n\nExposure to all aspects of the business, cross-jurisdiction and to working with senior management directly\n\n\nDisclaimerUnsolicited CVs sent to Apex (Talent Acquisition Team or Hiring Managers) by recruitment agencies will not be accepted for this position. Apex operates a direct sourcing model and where agency assistance is required, the Talent Acquisition team will engage directly with our exclusive recruitment partners.",Industry Type: Accounting / Auditing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['software development', 'business analysis', 'stakeholder management', 'uml', 'brd', 'rfps', 'design documents', 'management skills', 'business strategy', 'documentation', 'test scripts', 'financial services', 'business system', 'user acceptance testing', 'acquisition']",2025-06-11 06:01:57
AWS Cloud Tech Lead,ITC Infotech,5 - 8 years,20-30 Lacs P.A.,['Pune'],"You will participate in the design, development, and deployment of scalable and robust applications using AWS Cloudfront, S3 buckets, Node.js, TypeScript, React.js, Next.js, Stencil.js, and Aurora Postgres SQL.\nImplement microservices that integrate with Databricks as a Data Lake and consume data products.\n  Key Responsibilities\nCollaborate closely with other technical leads to align on standards, interfaces, and dependencies",,,,"['nextjs', 'confluence', 'technical', 'cloudfront', 'sql', 'microservices', 'cloud', 'react.js', 'postgresql', 'aws cloud', 'devops', 'backend', 'typescript', 'data lake', 'jira', 'architecture', 'deployment', 'rest', 'data engineering', 'javascript', 'app development', 'stenciljs', 'bucket', 'node.js', 'system architecture', 'full stack', 'scrum', 'agile', 'aws']",2025-06-11 06:01:59
Senior Backend Engineer,Welocalize,5 - 8 years,Not Disclosed,['Noida'],"Role Summary\nThe Senior Backend Engineer (Python & Node.js) is responsible for leading the design, implementation, and optimization of scalable machine learning infrastructure. This role ensures that AI/ML models are efficiently deployed, managed, and monitored in production environments while providing mentorship and technical leadership to junior engineers.\n\nYou can apply using the link below.",,,,"['Node.Js', 'Python', 'Cloud Development', 'GCP', 'Azure Cloud', 'Microservice Based Architecture', 'AWS']",2025-06-11 06:02:00
Digital Consultant - Innovation Group,"NTT DATA, Inc.",18 - 23 years,Not Disclosed,['Pune'],"Req ID: 317103\n\nWe are currently seeking a Digital Consultant - Innovation Group to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesWe are seeking a highly skilled and experienced Digital Consultant to join our Innovation Group. The ideal candidate will have a strong background in Big Data, Cloud, and AI/ML projects, with a focus on the health insurance or retail domains or manufacturing domains. This role involves engaging with clients for architecture and design, building accelerators for cloud migration, and developing innovative solutions using GenAI technologies.\nKey Responsibilities:\n""¢ Engage with clients to understand their requirements and provide architectural and design solutions.\n""¢ Develop and implement accelerators to facilitate faster cloud migration.\n""¢ Create innovative use cases or solutions to solve day to day data engineering problems using AI and GenAI tools.\n""¢ Develop reference architectures for various use cases using modern cloud data platforms.\n""¢ Understanding of Legacy toolsets, be it ETL, reporting etc is needed.\n""¢ Create migration suites for cataloging, migrating, and verifying data from legacy systems to modern platforms like Databricks and Snowflake.\n\nMinimum Skills RequiredQualifications:\n""¢ EducationB.E. in Electronics & Telecommunication or related field.\n""¢ Experience18+ years in IT, with significant experience in Big Data, Cloud, and AI/ML projects.\n""¢ Technical\n\nSkills:\nProficiency in Databricks, Snowflake, AWS, GenAI (RAG and GANs), Python, C/C++/C",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'big data', 'snowflake', 'python', 'aws', 'legacy', 'web services', 'soa', 'as400', 'data migration', 'artificial intelligence', 'retail', 'java', 'ms office outlook', 'etl', 'ml', 'mainframes', 'project management', 'erp', 'c', 'sap', 'sql server', 'data bricks', 'as', 'cobol', 'web technologies']",2025-06-11 06:02:02
Facets System Integration,Cognizant,5 - 8 years,Not Disclosed,['Chennai'],Job Summary\nWe are seeking a Sr. Product Specialist (T) with 5 to 8 years of experience to join our team. The ideal candidate will have expertise in Facets-Technical ANSI SQL and MS SQL Server. Experience in the Payer domain is a plus. This hybrid role requires working during day shifts with no travel required.,,,,"['fact', 'data management', 'analytical', 'technical', 'ansi sql', 'payments', 'documentation', 'database administration', 'sql server', 'ansi', 'sql', 'database optimization', 'database management', 'system integration', 'collaboration', 'aspect', 'product development', 'alignment', 'technical skills', 'communication skills']",2025-06-11 06:02:04
Senior AI/ML Engineer - Blackstraw - Work from Office- Chennai,Blackstraw Technologies,4 - 9 years,Not Disclosed,"['Chennai', 'Mumbai (All Areas)']","Job Summary:\nWe are looking for a passionate and skilled AI/ML Engineer to join our team, focusing on cutting edge solutions in vector search, embeddings, and semantic similarity. In this role, you will design, develop, and optimize intelligent search systems that leverage state-of-the-art machine learning and deep learning techniques. You will collaborate with cross-functional teams to build scalable and efficient solutions that transform how we retrieve and process information.",,,,"['Artificial Intelligence', 'Machine Learning', 'Object Detection', 'Algorithm Development', 'Natural Language Processing', 'Image Recognition', 'Deep Learning', 'Pytorch', 'Huggingface', 'Video Processing', 'Opencv', 'Image Processing', 'Computer Vision', 'Optimize Vector Search Systems', 'Python', 'OCR']",2025-06-11 06:02:05
Tableau Admin with AWS Experience,"NTT DATA, Inc.",2 - 6 years,Not Disclosed,['Noida'],"Req ID: 324014\n\nWe are currently seeking a Tableau Admin with AWS Experience to join our team in NOIDA, Uttar Pradesh (IN-UP), India (IN).\n\nTableau Admin with AWS Experience\n\n\n\nWe are seeking a skilled Tableau Administrator with experience in AWS to join our team. The ideal candidate will be responsible for managing and optimizing our Tableau Server environment hosted on AWS, ensuring efficient operation, data security, and seamless integration with other data sources and analytics tools.\n\n\n\nKey Responsibilities\n\n\n\n- Manage, configure, and administer Tableau Server on AWS, including setting up sites and managing user access and permissions.\n\n- Monitor server activity/performance, conduct regular system maintenance, and troubleshoot issues to ensure optimal performance and minimal downtime.\n\n- Collaborate with data engineers and analysts to optimize data sources and dashboard performance.\n\n- Implement and manage security protocols, ensuring compliance with data governance and privacy policies.\n\n- Automate monitoring and server management tasks using AWS and Tableau APIs.\n\n- Assist in the design and development of complex Tableau dashboards.\nProvide technical support and training to Tableau users.\n\n- Stay updated on the latest Tableau and AWS features and best practices, recommending and implementing improvements.\n\n\n\nQualifications -\n\n- Proven experience as a Tableau Administrator, with strong skills in Tableau Server and Tableau Desktop.\n\n- Experience with AWS, particularly with services relevant to hosting and managing Tableau Server (e.g., EC2, S3, RDS).\n\n- Familiarity with SQL and experience working with various databases.\nKnowledge of data integration, ETL processes, and data warehousing principles.\n\n- Strong problem-solving skills and the ability to work in a fast-paced environment.\n\n- Excellent communication and collaboration skills.\n\n- Relevant certifications in Tableau and AWS are a plus.\n\n\n\n\n\nA Tableau Administrator, also known as a Tableau Server Administrator, is responsible for managing and maintaining Tableau Server, a platform that enables organizations to create, share, and collaborate on data visualizations and dashboards. Here's a typical job description for a Tableau Admin\n\n1.\n\nServer AdministrationInstall, configure, and maintain Tableau Server to ensure its reliability, performance, and security.\n\n2.\n\nUser ManagementManage user accounts, roles, and permissions on Tableau Server, ensuring appropriate access control.\n\n3.\n\nSecurityImplement security measures, including authentication, encryption, and access controls, to protect sensitive data and dashboards.\n\n4.\n\nData Source ConnectionsSet up and manage connections to various data sources, databases, and data warehouses for data extraction.\n\n5. L\n\nicense Management: Monitor Tableau licensing, allocate licenses as needed, and ensure compliance with licensing agreements.\n\n6.\n\nBackup and RecoveryEstablish backup and disaster recovery plans to safeguard Tableau Server data and configurations.\n\n7.\n\nPerformance OptimizationMonitor server performance, identify bottlenecks, and optimize configurations to ensure smooth dashboard loading and efficient data processing.\n\n8.\n\nScalingScale Tableau Server resources to accommodate increasing user demand and data volume.\n\n9.\n\nTroubleshootingDiagnose and resolve issues related to Tableau Server, data sources, and dashboards.\n\n10.\n\nVersion UpgradesPlan and execute server upgrades, apply patches, and stay current with Tableau releases.\n\n11.\n\nMonitoring and LoggingSet up monitoring tools and logs to track server health, user activity, and performance metrics.\n\n12.\n\nTraining and SupportProvide training and support to Tableau users, helping them with dashboard development and troubleshooting.\n\n13.\n\nCollaborationCollaborate with data analysts, data scientists, and business users to understand their requirements and assist with dashboard development.\n\n14.\n\nDocumentationMaintain documentation for server configurations, procedures, and best practices.\n\n15.\n\nGovernanceImplement data governance policies and practices to maintain data quality and consistency across Tableau dashboards.\n\n16.\n\nIntegrationCollaborate with IT teams to integrate Tableau with other data management systems and tools.\n\n17.\n\nUsage AnalyticsGenerate reports and insights on Tableau usage and adoption to inform decision-making.\n\n18.\n\nStay CurrentKeep up-to-date with Tableau updates, new features, and best practices in server administration. A Tableau Administrator plays a vital role in ensuring that Tableau is effectively utilized within an organization, allowing users to harness the power of data visualization and analytics for informed decision-making.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data warehousing', 'sql', 'tableau', 'etl', 'data integration', 'python', 'aws iam', 'amazon redshift', 'aws administration', 'amazon rds', 'aws cloudformation', 'aws lambda', 'ansible', 'docker', 'amazon ec2', 'git', 'devops', 'server administration', 'linux', 'jenkins', 'terraform', 'aws', 'amazon cloudwatch']",2025-06-11 06:02:07
Analyst,Aim Corporate Services,1 - 4 years,3-3.5 Lacs P.A.,['Gurugram'],"Role & responsibilities\nYou like to analyze information and have a flair for writing. You have a way with words and can express your thoughts both verbally and in writing. You are a peoples person and make friends easily.\n\nThe job entails investigative research and analysis of cases involving Business Intelligence, forensics and Fraud Investigations.\n\nAnalyze financial records and datasets to identify trends, gaps, or inconsistencies.\nConduct structured research on entities, sectors, and events.\nBased on evidence gathered and research conducted, prepare clear, concise reports and presentations for clients and internal stakeholders.\n\nConduct due diligence on entities.\n\nPerform AML Compliance.\n\nPreferred candidate profile\n\nPrior working experience as analyst, consultative selling, basic knowledge of finance or operations is helpful.\n\nGraduate / MBA / Law Graduate / CS with at least 1-year work experience.\nYou have strong analytical and problem-solving skills.\nYou have excellent spoken and written English communication. Knowing an additional language is an added plus! (apart from the Mother Tongue, that is)\nYou know how computers work, especially MS Office: Excel, Word, PowerPoint.\nYou should be able to work independently and in a team. You are multi-dimensional and can think out of the box while inside the beautiful box.\nYou have the patience to work under pressure and meet stringent client deadlines. You should be willing to travel around India.\nYou have exemplary integrity, values and work ethics (and love cutting chai and cookies)",Industry Type: Management Consulting,Department: Risk Management & Compliance,"Employment Type: Full Time, Permanent","['Communication Skills', 'Research', 'Report Writing', 'Financial Statement Analysis', 'Aml Compliance', 'Due Diligence', 'Analytics']",2025-06-11 06:02:09
Business Analyst,Amgen Inc,4 - 5 years,Not Disclosed,['Hyderabad'],"The Sr Associate IS Bus Sys Analyst leverages domain and business process expertise to detail product requirements as epics and user stories, along with supporting artifacts like business process maps, use cases, and test plans for the software development teams. This role involves working closely with developers and business analysts to ensure that the technical requirements for upcoming development are thoroughly elaborated. This enables the delivery team to estimate, plan, and commit to delivery with high confidence and identify test cases and scenarios to ensure the quality and performance of IT Systems. You will collaborate with Product Managers and developers to maintain an efficient and consistent process, ensuring quality deliverables from the team.\nRoles & Responsibilities:\nCollaborate with System Architects and Product Managers to manage business analysis activities, ensuring alignment with engineering and product goals.\nCapture the voice of the customer to define business processes and product needs.\nWork with Product Managers and customers to define scope and value for new developments.\nCollaborate with Engineering and Product Management to prioritize release scopes and refine the Product backlog.\nEnsure non-functional requirements are included and prioritized in the Product and Release Backlogs.\nFacilitate the breakdown of Epics into Features and Sprint-Sized User Stories and participate in backlog reviews with the development team.\nClearly express features in User Stories/requirements so all team members and stakeholders understand how they fit into the product backlog.\nEnsure Acceptance Criteria and Definition of Done are well-defined.\nWork closely with UX to align technical requirements, scenarios, and business process maps with User Experience designs.\nStay focused on software development to ensure it meets requirements, providing proactive feedback to stakeholders.\nDevelop and execute effective product demonstrations for internal and external stakeholders.\nMaintain accurate documentation of configurations, processes, and changes.\nBasic Qualifications and Experience:\nMasters degree and 1 to 3 years of Life Science/Biotechnology/Pharmacology/Information Systems experience OR\nBachelors degree and 3 to 5 years of Life Science/Biotechnology/Pharmacology/Information Systems experience OR\nDiploma and 7 to 9 years of Life Science/Biotechnology/Pharmacology/Information Systems experience\nFunctional Skills:\nMust-Have Skills:\nExcellent problem-solving skills and a passion for tackling complex challenges in drug discovery with technology.\nExperience with Agile software development methodologies (Scrum).\nExcellent communication skills and the ability to interface with senior leadership with confidence and clarity.\nExperience in writing requirements for the development of modern web applications.\nExperience with writing user requirements and acceptance criteria in agile project management systems such as JIRA.\nGood-to-Have Skills:\nDemonstrated expertise in a scientific domain area and related technology needs.\nUnderstanding of scientific software systems strategy, governance, and infrastructure.\nExperience in managing product features for PI planning and developing product roadmaps and user journeys.\nFamiliarity with low-code, no-code test automation software.\nTechnical thought leadership.\nAble to communicate technical or complex subject matters in business terms.\nJira Align experience.\nKnowledge of network security protocols and tools (e.g., IPSEC, SSL, IDS/IPS, firewalls).\nExperience with DevOps, Continuous Integration, and Continuous Delivery methodology.\nProfessional Certifications:\nSAFe for Teams certification (preferred).\nSoft Skills:\nAble to work under minimal supervision.\nSkilled in providing oversight and mentoring team members. Demonstrated ability in effectively delegating work.\nExcellent analytical and gap/fit assessment skills.\nStrong verbal and written communication skills.\nAbility to work effectively with global, virtual teams.\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'IPSEC', 'DevOps', 'project management', 'agile', 'JIRA', 'SSL', 'IDS/IPS']",2025-06-11 06:02:11
A reputed FMCG company hiring software Engineer Bangalore location,ontimesolutions,3 - 5 years,4-9 Lacs P.A.,['Bengaluru'],"Greetings\n\nWe are currently hiring software engineers for our FMCG client\n\nPermanent position\n\nQualification: Btech Computers - from Tier 1/Tier 2 Engineering colleges\n\nKnowledge/experience with web development frameworks (e.g., React, Angular, or Vue.js) and back-end technologies (e.g., Node.js, .NET, Django) is an added advantage.\nFamiliarity with database management systems (e.g., SQL, NoSQL) is important.\nKnowledge of cloud platforms (e.g., AWS, Azure, Google Cloud) & Data Engineering technology is important\nStrong problem-solving skills and the ability to work both independently and collaboratively in a team environment.\nExcellent communication skills to effectively convey technical concepts to non-technical stakeholders.\nBachelors degree in Computer Science, Engineering, or a related field, from a leading engineering college in India.\nProficiency in one or more programming languages, such as Python. • Medium to high proficiency in AI tech stack (TensorFlow, LLMs, conversational AI etc.) as well as data processing frameworks.\nExperience with AI and ML platforms such as Google AI Platform, AWS SageMaker, or Microsoft Azure AI services.\nFamiliarity with tools and libraries specifically designed for generative AI, such as GPT/Gemini/Claude etc. WHAT WE EXPECT\n\nKindly share cv to susmitha@ontimesolutions.in",Industry Type: FMCG,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Machine Learning', 'Python', 'SQL', 'Artificial Intelligence']",2025-06-11 06:02:12
Business Analyst,Scalable Systems,1 - 6 years,Not Disclosed,['Bhubaneswar'],Design and develop interactive dashboards and reports using BI tools to visualize key performance indicators (KPIs) and metrics for various departments.,Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Power Bi Reports', 'Dax Queries', 'Power Bi Dashboards', 'Dashboards', 'Business Analysis']",2025-06-11 06:02:13
Business Analyst,Tek Inspirations LLC,1 - 5 years,2-5 Lacs P.A.,['Agra'],"We seek an IT Business Analyst to gather requirements, analyze processes, and bridge business needs with tech solutions. Must know SDLC, documentation (BRD/FRD), and tools like JIRA/Visio.",Industry Type: Software Product,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Business Analysis', 'Business Analytics', 'SDLC']",2025-06-11 06:02:15
Business Analyst,Insphere Solutions,5 - 8 years,7-8 Lacs P.A.,"['Noida', 'New Delhi( Netaji Subhash Place )', 'Delhi / NCR']","Analyze and document business requirements for system development, translating them into clear functional specifications.\nCollaborate with stakeholders to define acceptance criteria.\nAct as liaison between stakeholders and development teams.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analyst', 'Gap Analysis', 'Use Cases', 'Brd', 'Impact Analysis', 'Process Mapping', 'Requirement Gathering', 'FRD', 'Business Process Modeling', 'Business Analysis']",2025-06-11 06:02:16
Business Analyst,Dslr Technologies,2 - 6 years,Not Disclosed,['Gurugram'],"Job Role: Business Analyst - Retail\n\nAramya is a size-inclusive women's ethnic wear brand focused on delivering comfortable & beautiful daily wear at affordable prices. With $9M in funding from Z47, Accel, and marquee angel investors, were on a path of rapid expansion.\n\nDriven by in-house designing & manufacturing and deep customer insights, we have grown exponentially in the last few quarters, and we are super excited to accelerate this growth further.\n\nWere looking for a data-driven, business-oriented individual to join our team as a Business Analyst. This role is critical to unlocking growth opportunities, optimizing performance, and enabling strategic decision-making across functions. The ideal candidate will possess a strong analytical foundation, business acumen, and a passion for problem-solving.\n\nKey Responsibilities:\nDrive data-led decision-making by analyzing sales, inventory, and customer behavior to improve product, pricing, and store strategies.\nBuild and maintain dashboards and reports that track key retail KPIs, enabling real-time performance visibility.\nCollaborate cross-functionally with merchandising, marketing, and operations to translate insights into business impact.\nPresent strategic recommendations to leadership, influencing growth initiatives and operational efficiency.\n\nQualifications & Skills:\n2-6 years in business analytics, strategy, or data roles in a fast-paced D2C, e-commerce, or consumer tech environment.\nStrong command of SQL, Excel, and at least one BI tool (Power BI, Tableau, Looker, etc.).\nAbility to understand core business drivers and translate data into strategic decisions.\nStrong analytical storytelling and presentation skills to communicate insights effectively.\nBachelor's in Engineering, Economics, Statistics, Business, or a related field. MBA is a plus but not mandatory.\nSelf-starter with high ownership, detail orientation, and a bias for action",Industry Type: Textile & Apparel (Fashion),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi Tableau', 'Cross Functional Coordination', 'Retail Analytics', 'E-commerce', 'SQL', 'Business Analytics', 'Data Interpretation', 'Data Visualization', 'Data', 'Dashboarding', 'Looker']",2025-06-11 06:02:18
Analyst,VVD & Sons,3 - 5 years,6-8 Lacs P.A.,['Chennai'],"MBA in business development, business management, business analytics\nAge: 30 yrs max\nExp: Min 3 yrs working experience business consultancy, investment banker, financial advisory\nLanguage: Hindi mandatory\nMartial Status: Single",Industry Type: FMCG,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Predictive Analytics', 'Power Bi', 'Data Manipulation', 'SQL Database', 'Data Visualization', 'Python']",2025-06-11 06:02:20
Analyst-Fulfilment,Sakon,1 - 3 years,Not Disclosed,['Pune( Bavdhan )'],"Dear Candidates,\n\nSakon is hiring for Analyst-Fulfilment, interested candidate please share their update resume at namrata1.navadgi@sakon.com\n\nBelow refer the below detailed job description:\n\nPosition: Analyst - Fulfilment\n\nJob Overview\n\nThe Sakon Analyst Order Management is a key member of our ordering support team, providing that meaningful first connection with our clients. Our team has excellent client service skills and is driven to provide high quality support by addressing a broad range of order requests and accurately placing approved orders in client-approved vendor portals. Our order management analysts are trained to handle complex requests such as ordering new smartphones, plans and accessories, making service activation and change requests, assisting clients with product selection, and explaining rate plans and features as approved by their company policies.\n\nKey Responsibilities\n\nReview and interpret order requests to accurately enter order data in global telecom vendor portals or via email to buy, terminate, move and change telecom products and services.\nGather critical order related information from vendors such as shipping tracking number, device details and other while navigating between multiple tools and systems to update order request through order completion.\nCo-ordinate with Clients, Partners, and Vendors, validating pricing details, resolving discrepancies or disputes, and assuring clear communication resulting in a positive result while maintaining compliance with client business rules and established processing Service Level Agreements.\nUse problem-solving skills when working with internal global teams to resolve any ordering related questions and understanding the downstream impacts of the order process\nMaintain all requisite documentation including vendor order forms, communication logs, contract/quote documents login setup for customers and daily trackers in Excel and the work queue management.\n\nRequirements\n\nExcellent English reading, comprehension and writing skills and the ability to communicate effectively in English with a neutral or US accent\nOne to two years of order management experience and the ability to identify and resolve order issues to effectively process the order request\nHighly responsive and self-motivated, able to meet tight deadlines and maintain quality while working under pressure and with minimum supervision\nExperience with mobile device/ plans and troubleshooting methods, both client and vendor\nKnowledge of Microsoft Office products including Excel, Word, and PowerPoint\nWillingness to work in flexible night shifts\n\nConsent: We will be using your resume for job applications open with us for a full-time appointment and will save them for future reference.",Industry Type: IT Services & Consulting,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Order Management', 'Quote To Cash', 'Order To Cash', 'Ordering', 'Analyst- Fulfilment', 'Order Fulfillment', 'Order Processing']",2025-06-11 06:02:21
GenAI Engineer,Xebia It Architects,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","About Xebia\nXebia is a trusted advisor in the modern era of digital transformation, serving hundreds of leading brands worldwide with end-to-end IT solutions. The company has experts specializing in technology consulting, software engineering, AI, digital products and platforms, data, cloud, intelligent automation, agile transformation, and industry digitization. In addition to providing high-quality digital consulting and state-of-the-art software development, Xebia has a host of standardized solutions that substantially reduce the time-to-market for businesses.\nXebia also offers a diverse portfolio of training courses to help support forward-thinking organizations as they look to upskill and educate their workforce to capitalize on the latest digital capabilities. The company has a strong presence across 16 countries with development centres across the US, Latin America, Western Europe, Poland, the Nordics, the Middle East, and Asia Pacific.\n\nJob Title: Generative AI Engineer\nExp: 5 9 yrs\nLocation: Bengaluru, Chennai, Gurgaon & Pune\nJob Summary:\nWe are seeking a highly skilled Generative AI Engineer with hands-on experience in developing and deploying cutting-edge AI solutions using AWS, Amazon Bedrock, and agentic AI frameworks. The ideal candidate will have a strong background in machine learning and prompt engineering, with a passion for building intelligent, scalable, and secure GenAI applications.\nKey Responsibilities:\n\nDesign, develop, and deploy Generative AI models and pipelines for real-world use cases.\nBuild and optimize solutions using AWS AI/ML services, including Amazon Bedrock, SageMaker, and related cloud-native tools.\nDevelop and orchestrate Agentic AI systems, integrating autonomous agents with structured workflows and dynamic decision-making.\nCollaborate with cross-functional teams including data scientists, cloud engineers, and product managers to translate business needs into GenAI solutions.\nImplement prompt engineering, fine-tuning, and retrieval-augmented generation (RAG) techniques to optimize model performance.\nEnsure robustness, scalability, and compliance in GenAI workloads deployed in production environments.\nRequired Skills & Qualifications:\n\nStrong experience with Generative AI models (e.g., GPT, Claude, Mistral, etc.)\nHands-on experience with Amazon Bedrock and other AWS AI/ML services.\nProficiency in building and managing Agentic AI systems using frameworks like LangChain, AutoGen, or similar.\nSolid understanding of cloud-native architectures and ML Ops on AWS.\nProficiency in Python and relevant GenAI/ML libraries (Transformers, PyTorch, LangChain, etc.)\nFamiliarity with security, cost, and governance best practices for GenAI on cloud.\nPreferred Qualifications:\n\nAWS certifications (e.g., AWS Certified Machine Learning Specialty)\nExperience with LLMOps tools and vector databases (e.g., Pinecone, FAISS, Weaviate)\nBackground in NLP, knowledge graphs, or conversational AI.\nWhy Join Us?\nWork on cutting-edge AI technologies that are transforming industries.\nCollaborative and innovative environment.\nOpportunities for continuous learning and growth.",Industry Type: IT Services & Consulting,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['GenAI', 'Agentic Ai', 'Ml']",2025-06-11 06:02:23
VKYC Analyst,Urbane Corps,1 - 6 years,3.25-5 Lacs P.A.,['Mumbai (All Areas)( Kalina )'],"A detail-oriented Video KYC Analyst to join our Operations and Support team. you will be responsible for reviewing and processing of KYC applications, ensuring onboardings align with internal bank policies.\nJanvi HR-85340014622",Industry Type: Banking,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['videokyc', 'kyc analyst', 'Client Onboarding', 'KYC', 'Kyc Operations', 'Document Verification', 'Document Checking', 'Due Diligence', 'Kyc Verification', 'Credit Cards', 'Background Verification', 'Know Your Customer']",2025-06-11 06:02:24
Business Analyst(ARC industry),ARC Industry,8 - 13 years,14-16 Lacs P.A.,['Mumbai (All Areas)'],"Main Responsibility\n\n1. Prepare detailed documentation such as Business Requirement Documents (BRD), Software Requirement Specifications (SRS), and high-level technical/functional design documents.\n2. Techno-functional experience in the Banking/BFSI industry is mandatory.\n3. Strong understanding of Loan Management Systems (LMS) and CRM platforms.\n4. Collaborate with development, QA, and business teams to ensure the successful delivery of solutions aligned with business goals\n5. Proficiency in documentation tools and business analysis methodologies.\n6. ARC business knowledge is value added\nExperience with Loan Management Systems (LMS) and CRM platforms\nExperience with Loan Collection Systems\nCandidates with a background in ARC/Banking Industry\n\n\nQualification and Experience Requirement\nEducation : MBA or any Degree with relevant certifications\nExperience : 8-10 Yrs",Industry Type: Financial Services (Asset Management),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Loan Management System', 'CRM', 'Banking Sector', 'Asset Reconstruction']",2025-06-11 06:02:26
Webi Analyst,webid,1 - 6 years,1-4 Lacs P.A.,['Hyderabad'],"Role & responsibilities\n\nsend cvs to shilpa.srivastava@orcapod.work subject Webi Analyst\n\n27k max in hand\nImmediate joiners only\nWebi Contractor\n\nAnalyst\n2-4 years\nHyderabad\n\n2 PM to 11 PM\n\nWeb intelligence development, experience with universe/information design (tool) development, finance background, proficient in SQL,\nexperience with tableau, Power BI, reporting & Data visualization",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Excellent Verbal And Written Communication Skills', 'Finance Reporting', 'WebI', 'Web Intelligence', 'universe design tool', 'Power Bi Reports', 'Power Bi', 'Tableau', 'Dashboards', 'Information Design', 'Data Reporting', 'SQL']",2025-06-11 06:02:28
Immediate Openings For ETL Developer - Delhi,Trigyn Technologies,5 - 10 years,Not Disclosed,['New Delhi'],"We are seeking a skilled Data Engineer with at least 5 years of experience to join our data analytics team, focusing on building robust data pipelines and systems to support the creation of dynamic dashboards. The role involves designing, building, and optimizing data architecture, enabling real-time data flow for visualization and analytics. The Data Engineer will be responsible for managing ETL processes, ensuring data quality, and supporting the scalable integration of various data sources into our analytics platform.\nThe ideal candidate should have extensive experience in working with complex data architectures, managing ETL workflows, and ensuring seamless data integration across platforms. They should also have a deep understanding of cloud technologies and database management.\nKey Responsibilities:\n•Data Pipeline Development\no Design, build, and maintain scalable ETL (Extract, Transform, Load) processes for collecting, storing, and processing structured and unstructured data from multiple sources.\no Develop workflows to automate data extraction from APIs, databases, and external sources.\no Ensure data pipelines are optimized for performance and handle large data volumes with minimal latency.\n•Data Integration and Management\no Integrate data from various sources (e.g., databases, APIs, cloud storage) into the centralized daIta warehouse or data lake to support real-time dashboards.\no Ensure smooth data flow and seamless integration with analytics tools like Power BI and Tableau.\no Manage and maintain data storage solutions, including relational (SQL-based) and NoSQL databases.\nData Quality and Governance\no Implement data validation checks and quality assurance processes to ensure data accuracy, consistency, and integrity.\no Develop monitoring systems to identify and troubleshoot data inconsistencies, duplications, or errors during ingestion and processing.\no Ensure compliance with data governance policies and standards, including data protection regulations such as the Digital Personal Data Protection (DPDP) Act.\n•Database Management and Optimization\no Design and manage both relational and NoSQL databases, ensuring efficient storage, query performance, and reliability.\no Optimize database performance, ensuring fast query execution times and efficient data retrieval for dashboard visualization.\no Implement data partitioning, indexing, and replication strategies to support large-scale data operations.\n•Data Security and Compliance\no Ensure that all data processes adhere to security best practices, including encryption, authentication, and access control.\no Implement mechanisms for secure data storage and transmission, especially for sensitive government or public sector data.\no Conduct regular audits of data pipelines and storage systems to ensure compliance with relevant data protection regulations.\n• Cloud Infrastructure and Deployment\nDeploy and manage cloud-based data solutions using AWS, Azure, or GCP, including data lakes, data warehouses, and cloud-native ETL tools.\no Set up cloud infrastructure to support high availability, fault tolerance, and scalability of data systems.\no Monitor cloud usage and optimize costs for data storage, processing, and retrieval.\n•Performance Monitoring and Troubleshooting\no Continuously monitor data pipeline performance and data ingestion times to identify bottlenecks and areas for improvement\nTroubleshoot and resolve any data flow issues, ensuring high availability and reliability of data for dashboards and analytics.\no Implement logging and alerting mechanisms to detect and address any operational issues proactively.\nQualifications:\n•Education: Bachelors degree in Computer Science, Information Technology, Data Engineering, or a related field. A Master’s degree is a plus.\n•Experience: At least 5 years of hands-on experience as a Data Engineer, preferably in a data analytics or dashboarding environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Informatica', 'ETL', 'Azure Data Factory', 'SQL']",2025-06-11 06:02:29
MDM Associate Analyst,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"We are seeking an MDM Associate Analystwith 25 years of development experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment.To succeed in this role, the candidate must have strong experience on MDM (Master Data Management) on configuration (L3 Configuration, Assets creation, Data modeling etc), ETL and data mappings (CAI, CDI) , data mastering (Match/Merge and Survivorship rules), source and target integrations (RestAPI, Batch integration, Integration with Databricks tables etc)\nRoles & Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM solutions.\nPerform advanced SQL queries and data analysis to validate and ensure master data integrity.\nLeverage Python, PySpark, and Databricks for scalable data processing and automation.\nCollaborate with business and data engineering teams for continuous improvement in MDM solutions.\nImplement data stewardship processes and workflows, including approval and DCR mechanisms.\nUtilize AWS cloud services for data storage and compute processes related to MDM.\nContribute to metadata and data modeling activities.\nTrack and manage data issues using tools such as JIRA and document processes in Confluence.\nApply Life Sciences/Pharma industry context to ensure data standards and compliance.\nBasic Qualifications and Experience:\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\nStrong experience with Informatica or Reltio MDM platforms in building configurations from scratch (Like L3 configuration or Data modeling, Assets creations, Setting up API integrations, Orchestration)\nStrong experience in building data mappings, data profiling, creating and implementation business rules for data quality and data transformation\nStrong experience in implementing match and merge rules and survivorship of golden records\nExpertise in integrating master data records with downstream systems\nVery good understanding of DWH basics and good knowledge on data modeling\nExperience with IDQ, data modeling and approval workflow/DCR.\nAdvanced SQL expertise and data wrangling.\nExposure to Python and PySpark for data transformation workflows.\nKnowledge of MDM, data governance, stewardship, and profiling practices.\nGood-to-Have Skills:\nFamiliarity with Databricks and AWS architecture.\nBackground in Life Sciences/Pharma industries.\nFamiliarity with project tools like JIRA and Confluence.\nBasics of data engineering concepts.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL, Python, Databricks)\nAny cloud certification (AWS or AZURE)\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MDM', 'PySpark', 'Reltio MDM', 'Databricks', 'JIRA', 'AWS cloud', 'Python', 'SQL']",2025-06-11 06:02:31
Onix is Hiring Hadoop GCP Engineers!!!,Datametica,4 - 8 years,Not Disclosed,"['Pune', 'Bengaluru']","We are looking for skilled Hadoop and Google Cloud Platform (GCP) Engineers to join our dynamic team. If you have hands-on experience with Big Data technologies and cloud ecosystems, we want to hear from you!\nKey Skills:\nHadoop Ecosystem (HDFS, MapReduce, YARN, Hive, Spark)\nGoogle Cloud Platform (BigQuery, DataProc, Cloud Composer)\nData Ingestion & ETL pipelines\nStrong programming skills (Java, Python, Scala)\nExperience with real-time data processing (Kafka, Spark Streaming)\nWhy Join Us?\nWork on cutting-edge Big Data projects\nCollaborate with a passionate and innovative team\nOpportunities for growth and learning\nInterested candidates, please share your updated resume or connect with us directly!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['gcp', 'pyspark', 'Hive', 'Sqoop', 'mapreduce', 'Hadoop', 'Spark', 'YARN', 'pig']",2025-06-11 06:02:33
ECG Analyst,Medpace Clinical Research India,2 - 6 years,Not Disclosed,['Navi Mumbai'],"Title\nOur global activities are growing rapidly, and we are currently seeking a full-time, office-based ECG Analyst to join our Core Labs in Mumbai office. This position will work in a team to accomplish tasks and projects that are instrumental to the company’s success.\nOverview\n Medpace is a full-service clinical contract research organization (CRO). We provide Phase I-IV clinical development services to the biotechnology, pharmaceutical and medical device industries. Our mission is to accelerate the global development of safe and effective medical therapeutics through its scientific and disciplined approach. We leverage local regulatory and therapeutic expertise across all major areas including oncology, cardiology, metabolic disease, endocrinology, central nervous system, anti-viral and anti-infective. Headquartered in Cincinnati, Ohio, employing more than 5,000 people across 40+ countries.\nResponsibilities\n  Process ECGs in the system and assign them to Cardiologists;\nPerform ECG database cleanup activities;\nDevelop and revise ECG documents, and assist in ECG database design and development; and\nProvide status reports regarding ECG metrics to study team.\nQualifications\n  Required medical degree with extensive ECG experience\nRequired prior work experience in Clinical Research\nKnowledge of MS Word and Excel;\nExcellent organizational skills.\nGood command in English.\nPeople. Purpose. Passion. Make a Difference Tomorrow. Join Us Today.\n The work we’ve done over the past 30+ years has positively impacted the lives of countless patients and families who face hundreds of diseases across all key therapeutic areas. The work we do today will improve the lives of people living with illness and disease in the future.\n Medpace Perks\nFlexible work environment \nCompetitive compensation and benefits package\nCompetitive PTO packages\nStructured career paths with opportunities for professional growth\nCompany-sponsored employee appreciation events\nEmployee health and wellness initiatives\nAwards\nRecognized by Forbes as one of America's Most Successful Midsize Companies in 2021, 2022, 2023 and 2024\nContinually recognized with CRO Leadership Awards from Life Science Leader magazine based on expertise, quality, capabilities, reliability, and compatibility\n What to Expect Next\nA Medpace team member will review your qualifications and, if interested, you will be contacted with details for next steps.",Industry Type: Pharmaceutical & Life Sciences,Department: Research & Development,"Employment Type: Full Time, Permanent","['medical', 'ecg', 'yoga', 'exercise', 'nursing', 'training', 'echo', 'research', 'healthcare', 'dressing', 'fitness', 'wellness', 'english', 'icu', 'tmt', 'hospital', 'communication skills', 'x-ray', 'spa', 'health', 'patient care', 'excel', 'health club', 'clinical research', 'gym', 'word', 'cardiac']",2025-06-11 06:02:34
Snowflake Architect,Allegis Global Solutions (AGS),9 - 14 years,Not Disclosed,[],"Snowflake Architect\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\nJob Details\n\nTechnical Expertise:\nStrong proficiency in Snowflake architecture, including data sharing, partitioning, clustering, and materialized views.\nAdvanced experience with DBT for data transformations and workflow management.\nExpertise in Azure services, including Azure Data Factory, Azure Data Lake, Azure Synapse, and Azure Functions.\nData Engineering:\nProficiency in SQL, Python, or other relevant programming languages.\nStrong understanding of data modeling concepts, including star schema and normalization.\nHands-on experience with ETL/ELT pipelines and data integration tools.\nSoft Skills:\nExcellent problem-solving and analytical skills.\nStrong communication and stakeholder management abilities.\nAbility to work in agile teams and handle multiple priorities.\nPreferred Qualifications:\nCertifications in Snowflake, DBT, or Azure Data Engineering.\nFamiliar with data visualization tools like Power BI or Tableau.\nKnowledge of CI/CD pipelines and DevOps practices for data workflows.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Snowflake', 'Data Build Tool', 'SQL']",2025-06-11 06:02:36
Databrics Engineer,Fortune India 500 IT Services Firm,5 - 10 years,Not Disclosed,[],"Detailed job description - Skill Set:\nStrong Knowledge in Databricks. This includes creating scalable ETL (Extract, Transform, Load) processes, data lakes\nStrong knowledge in Python and SQL\nStrong experience with AWS cloud platforms is a must\nGood understanding of data modeling principles and data warehousing concepts\nStrong knowledge of optimizing ETL jobs, batch processing jobs to ensure high performance and efficiency\nImplementing data quality checks, monitoring data pipelines, and ensuring data consistency and security\nHands on experience with Databricks features like Unity Catalog\nMandatory Skills\nDatabricks, AWS",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Databricks Engineer', 'AWS', 'Data Pipeline', 'Data Lake', 'ETL', 'Unity Catalog', 'Python', 'SQL']",2025-06-11 06:02:38
HRMS Business Analyst,The Veca,7 - 12 years,Not Disclosed,['Mumbai (All Areas)'],"Role & responsibilities -\nPosition Name - HRMS Business Analyst \nLocation - Mumbai \nYears of experience - 7+ years\n\nJob Description - \nSystem Support & Maintenance: Provide ongoing support for HRMS platforms, troubleshoot issues, and ensure data integrity across systems.\nRequirements Gathering & Analysis: Collaborate with HR, IT, and other departments to gather and document business requirements, ensuring alignment with organizational goals.\nSystem Configuration and Implementation: Assist in configuring, testing, and deploying HRMS solutions to ensure they meet business needs and compliance standards.\nData Analysis & Reporting: Analyze HRMS data to identify trends and anomalies, and generate reports to support decision-making processes.\nTraining & Documentation: Develop user manuals, conduct training sessions, and create documentation to ensure effective use of HRMS tools.\nContinuous Improvement: Identify opportunities for process enhancements and system optimizations to improve HR operations.\nPreferred candidate profile\nNeed candidates with have done B.TECH/MCA/BE and immediate to 30 days are preferred. Good experience is needed in HRMS .Please share resume at shraddha.shukla@theveca.com.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['HRMS', 'business analyst', 'HR Implementation', 'Human Resource Management', 'Payroll Software', 'Hr Software', 'Business Analysis']",2025-06-11 06:02:40
Director - Analytics,Astar Data,14 - 22 years,Not Disclosed,['Bengaluru'],"VP - Data Analytics\nThis role will be a leadership position in the data science group at Sigmoid.\nAn ideal person will come from a services industry background with a good mix of experience in\nsolving complex business intelligence and data analytics problems, team management, delivery\nmanagement and customer handling. This position will give you an immense opportunity to work\non challenging business problems faced by fortune 500 companies across the globe. The role is\npart of the leadership team and includes accountability for a part of her/his team and customers.\nThe person is expected to be someone who can contribute in developing the practice with\nrelevant experience in the domain, nurturing the talent in the team and working with customers to\ngrow accounts.\nResponsibilities Include\n\nBuild trust with senior stakeholders through strategic insight and delivery credibility.\nAbility to translate ambiguous client business problems into BI solutions and ability to\nimplement them\nOversight of multi-client BI and analytics programs with competing priorities and\ntimelines, while collaborating with Data Engineering and other functions on a common\ngoal.\nEnsure scalable, high-quality deliverables aligned with business impact.\nHelp recruiting and onboarding team members; directly manage 15 - 20 team members\nYou would be required to own customer deliverables and ensure, along with project\nmanagers, that the project schedules are in line with the expectations set to the\ncustomers\nExperience and Qualifications\n15+ years of overall experience with a minimum of 10+ years in data analytics execution.\nStrong organizational and multitasking skills with the ability to balance multiple priorities.\nHighly analytical with the ability to collate, analyze and present data and drive clear\ninsights to lead decisions that improve KPIs.\nAbility to effectively communicate and manage relationships with senior management,\nother departments and partners.\nMastery of BI tools (Power BI, Tableau, Qlik), backend systems (SQL, ETL frameworks),\nand data modeling.\nExperience with cloud-native platforms (Snowflake, Databricks, Azure, AWS), data\nlakes\nExpertise in managing compliance, access controls, and data quality frameworks is a\nplus\nExperience working in CPG, Supply Chain, Manufacturing and Marketing domains are a\nplus\nStrong problem-solving skills and ability to prioritize conflicting requirements\nExcellent written and verbal communication skills and ability to succinctly summarize the\nkey findings",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Intelligence', 'Marketing Analytics', 'Data Analytics', 'Analytics', 'Business Analytics', 'Advanced Analytics', 'Predictive Analytics']",2025-06-11 06:02:41
Ml Engineer/,Orcapod,3 - 5 years,Not Disclosed,['Pune'],"Dear Candidate,\n\n\nWe are currently hiring for our client in Pune location.\nKey Responsibilities\n\n• •Extraction, Transformation, and Loading (ETL) data from source systems and bringing into the desired format for better decision-making.\n• •Identify, analyze, and interpret trends or patterns in complex data sets (Sensor/Machine Data)\n• •Understand Data warehouses and Enterprise Data Lakes  (EDL) and how they work.\n• •Build and refine machine learning models to solve complex problems.\n• •Develop data pipelines for optimal model training and deployment.\n• •Explore new/advanced Data Science techniques/methodologies.\n• •Monitor and optimize model performance post-deployment.\n• •Building and testing the hypothesis using data.\n• •Maintain thorough documentation and communicate technical insights effectively.\n\nRequired Skills\n\n• •Strong knowledge in SQL & PySpark\n• •Hands-on experience in productionizing the ML models.\n• •Experience in working with cloud technologies will be an added advantage (Databricks)\n• •Hands-on experience on any visualization tool (Tableau Preferred)\n• •Strong knowledge of statistical techniques for data analysis\n• •Experience with data mining, machine learning and deep learning for analytical insights\n• •Good communication skills",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'EDL', 'Machine Learning', 'SQL']",2025-06-11 06:02:42
Ml Engineer,Solix Technologies,5 - 10 years,15-25 Lacs P.A.,['Hyderabad'],"Solix Technologies Inc. is a leading provider of big data applications for enterprise archiving, data privacy, and advanced analytics. We are on a mission to help organizations manage and leverage their data for maximum value, efficiency, and compliance.\nJob Summary:\nWe are seeking a highly skilled and motivated Machine Learning Engineer with hands-on experience in Natural Language Processing (NLP) and Large Language Models (LLMs). You will play a key role in designing, developing, and deploying scalable ML/NLP solutions that drive intelligent automation and data insight across our platforms.\nKey Responsibilities:\nDesign and develop machine learning models, particularly in the domain of NLP and LLMs.\nFine-tune, evaluate, and deploy transformer-based models (e.g., BERT, GPT, T5, LLaMA, etc.).\nApply techniques such as named entity recognition (NER), text classification, semantic search, summarization, and question answering.\nWork with large-scale datasets to extract insights and build data pipelines.\nCollaborate with cross-functional teams including data engineers, product managers, and software developers.\nConduct experiments, model training, and optimization to improve accuracy and performance.\nStay up-to-date with the latest research in NLP, LLMs, and machine learning.\nRequired Skills and Experience:\nBachelor's or Masters degree in Computer Science, Data Science, AI/ML, or a related field.\nMinimum 5+ years of hands-on experience with NLP and LLMs\nProficient in Python and ML frameworks like TensorFlow, PyTorch, Hugging Face Transformers.\nStrong understanding of modern NLP techniques (tokenization, embeddings, attention mechanisms, etc.).\nExperience with ML lifecycle including model development, evaluation, and deployment (MLOps).\nFamiliarity with data handling libraries (Pandas, NumPy) and cloud platforms (AWS, GCP, or Azure).\nGood understanding of data preprocessing, feature engineering, and model validation techniques.\nExperience with open-source LLM fine-tuning and deployment.\nKnowledge of vector databases (e.g., FAISS, Pinecone) and retrieval-augmented generation (RAG).\nPrior experience with large-scale data systems or enterprise data environments.\nPublished papers or open-source contributions in the ML/NLP space.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLm', 'Large Language Model', 'Natural Language Processing', 'Ml Algorithms']",2025-06-11 06:02:44
Scrum Lead- Project Management,Metlife,8 - 12 years,Not Disclosed,['Hyderabad'],"Position Summary\n\nMetLife established a Global capability center (MGCC) in India to scale and mature Data & Analytics, technology capabilities in a cost-effective manner and make MetLife future ready. The center is integral to Global Technology and Operations with a with a focus to protect & build MetLife IP, promote reusability and drive experimentation and innovation. The Data & Analytics team in India mirrors the Global D&A team with an objective to drive business value through trusted data, scaled capabilities, and actionable insights. The operating models consists of business aligned data officers- US, Japan and LatAm & Corporate functions enabled by enterprise COEs- data engineering, data governance and data science.\n\nRole Value Proposition:\nDriven by passion and a zeal to succeed, we are looking for accomplished Program Manager to structure, plan and handle multiple projects with minimum supervision and will be responsible for successful completion of projects supporting MGCC and US D&A leadership with various strategic initiatives in development and successful implementation of governance and process excellence practices.\nThis position would be responsible for complete adherence of the projects and its objectives and support all aspects of project management. This role will support development of best practices, processes and framework to achieve standardization and streamlining across various initiatives.\n\nJob Responsibilities\nServe as analytics program manager on data, analytics projects and POCs working with data engineers, business analysts, data scientists, IT teams, vendors, executive leaders, and business stakeholders\nDrive transparency leveraging tech stack and data, own progress reporting and proactively communicate status\nDrive delivery of projects using Agile methodology for data and analytics programs\nFacilitate scrum ceremonies including Sprint planning, Daily stand ups, sprint reviews and retrospectives\nResponsible for defining relevant program metrics, status reports and continuous measurement of program portfolio best practices\nLead, coach, support and mentor junior team members\nInteract with senior leadership teams across Data and analytics, IT and business teams.\n\nKnowledge, Skills and Abilities\nBachelors degree. Technology/IT specialization is preferred.\nMBA is a preferred qualification\n8-12+ years of progressive experience in project/program management role with proven people influencing experience including with virtual and global teams\nAgile project management/delivery experience is a must preferably with Data and Analytics background\nProficient in MS Office suite: Excel, PowerPoint, Project.\nUnderstanding of analytical tool stack, Azure Devboards, Jira, SharePoint is a plus\nCSM, SAFe Agilist certifications are preferred\nAbility to identify risks to project success and recommend course of action to prevent risk from negatively impacting the project; Effectively recognize when to escalate issues and options to senior management for resolution\nSuperior solutioning techniques, organizational skills and ability to manage multiple ongoing projects.\nExcellent collaboration and communication skills, both written and verbal\nDemonstrated competency with cross-group collaboration, organizational agility, and analytical planning\nStrong leadership & negotiation skills.",Industry Type: Insurance,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Agile Methodology', 'Sprint Planning', 'scrum', 'Azure Devops', 'Agile Framework', 'Backlog Refinement', 'Scrum Development', 'Sprint Review', 'Retrospective']",2025-06-11 06:02:45
Associate-DIG,Tresvista Financial Services,3 - 7 years,Not Disclosed,"['Mumbai', 'Pune', 'Bengaluru']","Roles and Responsibilities:\nExpertise and experience with Analytics, SQL, Python and handling large datasets is mandatory. • Use technical skills to ensure the accuracy of large analytical data sets, automate processes with scripts and macros and efficiently query information from a vast database.\nHave a good understanding of Fixed Income, Equity, Derivatives and Alternatives products and how they are modeled and traded\nExhibit attention to detail when quality checking Package analytics and be accountable of timely delivery of reports to clients in accordance with Service Level Agreements.\nEngage in meetings with end-users of the Package product from all levels within the company from Portfolio and Risk Managers to Operations teams and also with our external Clients.\nSupport client requests related to the Package analytics.\nProject work: engaging with other internal teams to think creatively and deliver innovative solutions to our complex client demands.\n\n\nPrerequisites\n• Excellent problem-solving and critical-thinking skills and an ability to identify problems, design and articulate solutions and implement change.\nExpertise and experience with Analytics, SQL, Python and handling large datasets is mandatory.\nExperience with UNIX, PERL and developing an Asset Management platform is highly desirable.\nKnowledge and understanding of Fixed Income, Equity, Derivatives and Alternatives products is preferred.\nExperience with Risk analytics such as Durations, Spreads, Beta and VaR is preferable.\nMust possess strong verbal and written communication skills and be able to develop good working relationships with stakeholders.\nMust be detail orientated, possess initiative and work well under pressure.\nWork experience with BFSI will be an added advantage.\nA Degree in Engineering or Technology is required.\n\n\nExperience\n3-7 years\n\nEducation\nB.Tech/B.E/MCA\n\nCompensation\nThe compensation structure will be as per industry standard",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Unix', 'Data Engineering', 'Analytics', 'SQL', 'Python', 'Large Databases', 'VAR', 'Perl', 'Data Analyst', 'Fixed Income', 'Equity Derivatives', 'Beta', 'Spread', 'Risk Analysis', 'Data Analysis']",2025-06-11 06:02:47
Technical Lead,WebMD,9 - 14 years,Not Disclosed,['Navi Mumbai'],"Position: Tech Lead No. of Positions: 1\nAbout Company:\nHeadquartered in El Segundo, Calif., Internet Brands is a fully integrated online media and software services organization focused on four high-value vertical categories: Health, Automotive, Legal, and Home/Travel. The company's award- winning consumer websites lead their categories and serve more than 250 million monthly visitors, while a full range of web presence offerings has established deep, long-term relationships with SMB and enterprise clients. Internet Brands' powerful, proprietary operating platform provides the flexibility and scalability to fuel the company's continued growth. Internet Brands is a portfolio company of KKR and Temasek.\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health information services, serving patients, physicians, health care professionals, employers, and health plans through our public and private online portals, mobile platforms, and health-focused publications. The WebMD Health Network includes WebMD Health, Medscape, Jobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals Consumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape Education, and other owned WebMD sites. WebMD, Medscape, CME Circle®, Medpulse®, eMedicine®, MedicineNet®, ®, and RxList® are among the trademarks of WebMD Health Corp. or its subsidiaries.\n\n\n\nFor Company details, visit our website: /\nB.E. Computer Science/IT degree (or any other engineering discipline)\n2 PM to 11 PM IST\n8+ years",,,,"['Data Warehousing', 'ETL', 'SQL']",2025-06-11 06:02:48
Gen AI Consultant,Client of Edge,9 - 12 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Delhi / NCR']","The Company\nIndia's marquee global technology & consulting company. They are an international flag-bearer of technical and managerial excellence. With offices around the globe, the company has a comprehensive presence across multiple segments of the technology product and service industries as well as a blue-chip roster of clients for their Consulting engagements. They are a respected career company and a long-term wealth creator.\nThe Job\nWe are seeking a highly skilled Generative AI Consultant with 9+ Years of IT industry experience in which 2/3 years should be in AI/ML/DS domain, including Gen AI technologies to lead the design and deployment of cutting-edge AI solutions. The ideal candidate will possess deep expertise in Generative AI, strong analytical skills, and excellent communication abilities to engage with clients and internal teams. This role involves designing AI architectures, implementing scalable models, and ensuring solutions align with business objectives.\nKey Responsibilities\nCollaborate with clients to understand AI project needs and define measurable success metrics\nDesign end-to-end Generative AI solutions with a focus on performance, security, and ethical AI\nDevelop models using GANs, VAEs, NLP transformers, and other generative techniques\nImplement AI solutions utilizing cloud platforms (AWS, Azure, GCP) and ML frameworks (TensorFlow, PyTorch, LangChain, Semantic Kernels)\nEnsure compliance with Responsible AI and Data Privacy principles, mitigating bias and ensuring ethical AI practices\nWork alongside engineers and data scientists to integrate and optimize AI models into real-world applications\nConduct performance monitoring, troubleshooting, and continuous improvements for AI systems\nStay updated with AI advancements and share knowledge with internal teams\nYour Profile\nPrimary Skills\nExpertise in Generative AI techniques, including image generation, text synthesis, and function calling\nStrong understanding of AI/ML algorithms, cloud computing, NLP, and computer vision\nProficiency in data science tools (NumPy, SciPy, Pandas, Matplotlib, TensorFlow, Keras)\nSolid grasp of Responsible AI, fairness in algorithms, and bias mitigation strategies\nExcellent communication and solution design skills, capable of translating business needs into technical AI architectures.\nSecondary Skills\nKnowledge of industry-specific AI applications (healthcare, finance, manufacturing, etc.)\nBasic project management abilities to oversee timelines and deliverables\nFoundational data preprocessing and quality assurance expertise",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Genrative Ai', 'Natural Language Processing', 'Computer Vision', 'Solution Design', 'Ml', 'Python']",2025-06-11 06:02:50
Principal Machine Learning Engineer,Amgen Inc,2 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nWe are seeking a highly skilled Machine Learning Engineer with a strong MLOps background to join our team. You will play a pivotal role in building and scaling our machine learning models from development to production. Your expertise in both machine learning and operations will be essential in creating efficient and reliable ML pipelines.\nRoles & Responsibilities:\nCollaborate with data scientists to develop, train, and evaluate machine learning models.\nBuild and maintain MLOps pipelines, including data ingestion, feature engineering, model training, deployment, and monitoring.\nLeverage cloud platforms (AWS, GCP, Azure) for ML model development, training, and deployment.\nImplement DevOps/MLOps best practices to automate ML workflows and improve efficiency.\nDevelop and implement monitoring systems to track model performance and identify issues.\nConduct A/B testing and experimentation to optimize model performance.\nWork closely with data scientists, engineers, and product teams to deliver ML solutions.\nGuide and mentor junior engineers in the team\nStay updated with the latest trends and advancements\n\nBasic Qualifications:\nDoctorate degree and 2 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nMasters degree and 8 to 10 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nBachelors degree and 10 to 14 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nDiploma and 14 to 18 years of years of Computer Science, Statistics, and Data Science, Machine Learning experience\nPreferred Qualifications:\nMust-Have Skills:\nStrong foundation in machine learning algorithms and techniques\nExperience in MLOps practices and tools (e.g., MLflow, Kubeflow, Airflow); Experience in DevOps tools (e.g., Docker, Kubernetes, CI/CD)\nProficiency in Python and relevant ML libraries (e.g., TensorFlow, PyTorch, Scikit-learn)\nOutstanding analytical and problem-solving skills; Ability to learn quickly; Excellent communication and interpersonal skills\nGood-to-Have Skills:\nExperience with big data technologies (e.g., Spark), and performance tuning in query and data processing\nExperience with data engineering and pipeline development\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification\nKnowledge of NLP techniques for text analysis and sentiment analysis\nExperience in analyzing time-series data for forecasting and trend analysis\nFamiliar with AWS, Azure, or Google Cloud;\nFamiliar with Databricks platform for data analytics and MLOps\nProfessional Certifications\nCloud Computing and Databricks certificate preferred\nSoft Skills:\nExcellent analytical and fixing skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Azure', 'NLP', 'MLOps', 'Databricks', 'AWS', 'Google Cloud']",2025-06-11 06:02:52
Life sciences - Sr.Project Manager,Agilisium,10 - 17 years,Not Disclosed,['Chennai( Perungudi )'],"Job Title: IT Project Manager (Life Sciences)\nLocation: OMR, Chennai\nWork Mode: On-site (Ready to work from office)\nExperience: 12+ Years\nJob Description:\nWe are looking for an IT Project Manager with deep expertise in Life Sciences (Pharma/Biotech/MedTech) and hands-on experience managing Data Engineering projects. The ideal candidate will have 12+ years of experience leading IT initiatives, including data pipelines, cloud-based analytics, and regulatory-compliant data solutions in the Life Sciences domain.",,,,"['Life Sciences', 'Project Management', 'Profit And Loss', 'Project Monitoring', 'Project Documentation', 'Project Planning', 'Project Scheduling', 'Salesforce']",2025-06-11 06:02:53
Technology Lead,SRSInfoway,12 - 21 years,Not Disclosed,"['Hyderabad', 'Pune', 'Chennai']","12+ years of work exp with a coming with solid experience in eCommerce projects to act as lead developer for the team.\nRequirements: Full stack (.net core, angular , Dev-Ops domain exp)with good angular knowledge, Strong personality with problem solving and critical thinking skills, Ability to apply their expertise during code review\none strong & experienced senior who is full stack developer and has infrastructure & data engineering capabilities.\nTalent should be Full  stack developer (.NET + Angular) exp:10 to 12+ years\nBy Infrastructure is  referring to having strong In-depth DevOps knowledge(Azure, AWS, Terraform, YAML  - Ability to setup environments, debug  & identify issues in environments etc.)\nBy Data Engineer  capabilities  referring to person who have capabilities to deal with Data e.g. lot of time this team deals to inventory data pulling from PIM  or other sources, doing manipulation, converting data to usable information etc.\nGiven that all above three pieces are difficult to get, but person should be very strong  in one  area e.g. Full Stack developer being  In-Depth knowledge on DevOps & Data handling, Terraform But eventually expectation would be that  talent scales up on all  these areas.\nWants to make  team  self-sufficient and remove dependencies on other track.\nNeed strong all rounded  team member. A very strong Full Stack developer who has experience  in the modern tech stack.\nNeed to have a good Knowledge on Microservices as the client is migrating from Monolithic architecture to Microservices.\nThe person should be technically sound as a full stack developer but at the same time should be able to lead the team and bring them together, nudge them to open up. They need someone who can stabilize this team as a lead.\nStrong personality with problem solving and critical thinking skills, Ability to apply their  expertise during code review.\nAlso, Person should be Strong enough to handle Escalations, Along with Strong Leadership, communication, Team building & Team Leading skills.\nSupreme understanding and experience in Microservices and Event Driven Architecture & Cloud Development.\nShould possess skills and knowledge to drive and coach the team\n\nMandatory Skills\n.Net Core, Angular, Dev-Ops domain exp\nDatabase: SQL Server\nDesign and Developing Microservice RESTful services.\nExperienced in MS Azure cloud solutions and offerings. (Kubernetes, Docker Swarm, App service, function app, Service Bus, Event hub, Azure SQL, OMS Azure AD, ADF, ADLS, Databricks application, Data Security.\nSharepoint technologies: Office 365, Graph API, OnPromise\nAgile Scrum Methodologies, Devops, (CI/CD Containeraization)",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Temporary/Contractual","['.Net', 'Angular', 'Azure', 'Terraform', 'AWS']",2025-06-11 06:02:55
Senior AI/ML,Voxai,6 - 10 years,Not Disclosed,['Hyderabad'],"Voxai IT Solution Pvt. Ltd.\nhttp://www.voxai.com/\n\nJob Description\nAs a Senior AI/ML Engineer, you will play a key role in the design, development, and continuous improvement of our AI and machine learning solutions. Your primary responsibilities will include ensuring the scalability, accuracy, and reliability of our models, while collaborating with team members and providing technical expertise to drive success.\nJob Title: - Senior AI/ML Engineer Work Location: Hyderabad\n\nKey Responsibilities:\nModel Development: Design, develop, and deploy machine learning models and algorithms to address complex business problems.\nData Analysis: Analyze datasets to uncover trends, patterns, and insights, and ensure data quality and integrity for model training.\nAlgorithm Optimization: Continuously optimize machine learning models for performance, scalability, and efficiency, ensuring that they meet business and technical requirements.\nCollaboration: Work closely with cross-functional teams, including data scientists, engineers, and product managers, to integrate machine learning models into production systems.\nResearch & Innovation: Stay updated with the latest AI/ML trends and research, applying innovative approaches and solutions to improve models and processes.\nModel Evaluation & Tuning: Perform rigorous model validation, hyperparameter tuning, and evaluation to ensure model accuracy and generalization.\nAutomation: Develop and implement tools to automate and streamline ML workflows, including data preprocessing, feature engineering, and model retraining.\nMentorship: Provide guidance and technical leadership to junior team members, helping them with best practices, problem-solving, and professional growth.\nDocumentation: Document model development processes, experiments, and results for internal use and knowledge sharing.\nPerformance Monitoring: Monitor the performance of deployed models in production environments and troubleshoot issues as they arise.\nEthical AI Development: Ensuring AI solutions are ethical, fair, and unbiased while adhering to privacy and security standards.\nQualifications:\nBachelors or Masters degree in Computer Science or a related field.\n8+ years of experience in software development\nMachine Learning (Supervised, Unsupervised, and Reinforcement Learning)\nProgramming: Python, TensorFlow, PyTorch\nCloud Platforms: AWS/Azure/GCP\nModel Deployment (Docker/ Kubernetes)\n\nExperience with SQL / NoSQL databases (e.g.SQL Server, Dynamo DB, MongoDB, Cassandra).\nExperience in Natural language processing (NLP)\nExperience with various Design patterns(eg Pipeline, Ensemble, Transfer Learning) and Metrics (eg Precision, Recall, F1-Score ROC-AUC, MSE, RMSE, Confusion Matrix)\nExperience in evaluating Model Deployment metrics\nKnowledge of ML OPS is a plus\nStrong problem-solving and debugging skills.\nFamiliarity with Agile/Scrum methodologies.\nExcellent communication and interpersonal skills.\nAbility to work effectively in a collaborative team environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Python or pytorch or Tensorflow', 'Machine Learning', 'Docker or Kubernetes', 'AWS or Azure or GCP', 'SQL']",2025-06-11 06:02:56
Gen AI Developer - Lead,A Edge Client,5 - 9 years,Not Disclosed,"['Pune', 'Chennai', 'Delhi / NCR']","The Company\nIndia's marquee global technology & consulting company. They are an international flag-bearer of technical and managerial excellence. With offices around the globe, the company has a comprehensive presence across multiple segments of the technology product and service industries as well as a blue-chip roster of clients for their Consulting engagements. They are a respected career company and a long-term wealth creator.\nThe Job\nWe are seeking an 6 Years of IT industry experience in which 2/3 years should be in AI/ML/DS domain, including Gen AI technologies.\nWe are seeking an accomplished Generative AI Technical Lead to spearhead the development, implementation, and optimization of Generative AI solutions. As the Lead Developer, you will play a pivotal role in prompt engineering, pipeline creation, workflow establishment, and ensuring the quality of the technical outputs generated by the team. This role requires strong technical expertise in Generative AI, hands-on experience in development, and the ability to lead and guide a team effectively.\nPrimary Skill Set:\nGenerative AI Expertise: Good understanding of various Generative AI techniques, including GANs, VAEs, and other relevant architectures. Proven experience in applying these techniques to real-world problems for tasks such as image and text generation. Conversant with Gen AI development tools like Prompt engineering, Langchain, Semantic Kernels, Function calling. Exposure to both API based and opens source LLMs based solution design.\nTechnical Proficiency:\nMachine learning algorithms: Linear regression, logistic regression, decision trees, random forests, support vector machines, neural networks\nData science tools: NumPy, SciPy, Pandas, Matplotlib, TensorFlow, Keras\nCloud computing platforms: AWS, Azure, GCP\nNatural language processing (NLP): Transformer models, attention mechanisms, word embeddings\nComputer vision: Convolutional neural networks, recurrent neural networks, object detection\nRobotics: Reinforcement learning, motion planning, control systems\nData ethics: Bias in machine learning, fairness in algorithms\nYour Profile\nRoles & Responsibilities:\nTechnical Leadership: Lead a team of developers in the creation, implementation, and optimization of Generative AI solutions. Provide technical guidance, resolve challenges, and foster a collaborative environment.\nPrompt Engineering: Spearhead the design and development of prompt engineering strategies to influence and control the output of Generative AI models. Optimize prompts for desired results.\nPipeline Design: Design end-to-end data pipelines that encompass data preprocessing, feature engineering, model training, and deployment. Ensure pipelines are efficient, scalable, and well-documented.\nTechnical Review: Review the technical outputs generated by the team, including code, models, and pipelines. Ensure high-quality and maintainable solutions that adhere to best practices.\nTesting and Validation: Implement testing methodologies to validate the performance and accuracy of Generative AI models. Develop and execute unit tests, integration tests, and validation strategies.\nDeployment Strategy: Collaborate with DevOps and deployment teams to deploy trained models into production environments. Ensure smooth integration and monitor performance post-deployment.\nWorkflow Optimization: Identify opportunities to optimize development workflows, enhance productivity, and streamline processes. Implement tools and practices to improve efficiency.\nCollaboration: Interface with cross-functional teams, including data scientists, architects, and business stakeholders. Collaborate on solution design, implementation, and project milestones.\nDocumentation: Maintain comprehensive documentation of technical designs, code, and workflows. Ensure documentation is up-to-date, accessible, and understandable for team members.",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Gen AI', 'Gen AI Lead', 'Computer Vision', 'Open Source', 'Generative Ai', 'Artificial Intelligence', 'Machine Learning', 'Generative Artificial Intelligence', 'Robotics', 'Python']",2025-06-11 06:02:58
Tableau Developer,Viaprom Technologies,4 - 5 years,Not Disclosed,['Bengaluru'],"Job Description:\nWe at VIAPROM are seeking a skilled and experienced Tableau Developer to join our team within 15-20 days in Bengaluru. Candidates who are based out of Bengaluru will be preferred. The ideal candidate should have a strong background in Visualization and data storytelling.\n\nKey Responsibilities:\n1. Collaborate with business stakeholders to understand their data analysis and reporting requirements.\n2. Design and develop interactive and visually appealing dashboards, reports, and data visualizations using Tableau Desktop.\n3. Utilize Tableau's features, including calculated fields, parameters, data blending, and advanced analytics, to create insightful and actionable visualizations.\n4. Transform complex data sets into intuitive visualizations that effectively communicate key insights and trends.\n5. Work closely with data engineers to extract, transform, and load data from various sources into Tableau.\n6. Optimize the performance and efficiency of Tableau dashboards and reports by implementing best practices and data optimization techniques.\n7. Conduct data analysis to identify trends, patterns, and anomalies within the data.\n8. Collaborate with cross-functional teams to integrate Tableau dashboards and visualizations into business applications or portals.\n9. Stay up-to-date with the latest Tableau features, trends, and best practices to continually enhance data visualization capabilities.\n10. Provide support and training to end-users on Tableau functionality and usage.\n\nDesired Candidate Skills:\n1. Proven experience as a Tableau Developer with atleast 4-5 years of hands-on experience.\n2. In-depth knowledge of Tableau Desktop and Tableau Server, including data preparation, visualization design, and dashboard development.\n3. Strong understanding of data visualization best practices, data analysis, and data storytelling.\n4. Proficiency in SQL and experience with data modeling and database design.\n5. Ability to gather and translate business requirements into effective Tableau visualizations.\n6. Excellent communication and collaboration skills, with the ability to work effectively with cross-functional teams and stakeholders.\n\nEducational Qualifications\n1. Bachelor's degree in Computer Science, Information Systems, Data Science, or a related field.\n2. Candidates with Tableau Desktop and/or Tableau Server certification is preferable.\n3. Familiarity with statistical analysis and data mining techniques.\n4. Understanding of data governance and data security best practices.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Tableau', 'database design', 'data analysis', 'data modeling', 'data mining', 'Advanced Excel', 'statistical analysis', 'SQL']",2025-06-11 06:02:59
AWS Quicksight Developer,Wavicle Data Solutions,6 - 11 years,12-20 Lacs P.A.,"['Chennai', 'Coimbatore', 'Bengaluru']","Job Summary:\nWe are seeking an experienced Amazon QuickSight Developer to design and develop interactive dashboards, business intelligence (BI) reports, and data visualizations. The ideal candidate will have hands-on experience with Amazon QuickSight, a strong background in data analytics, and the ability to work closely with stakeholders to transform business requirements into actionable insights.\nKey Responsibilities:\nDesign, develop, and maintain BI dashboards, reports, and visualizations using Amazon QuickSight.\nIntegrate QuickSight with AWS data services like Amazon Redshift, Athena, S3, and RDS.\nOptimize dashboards and visualizations for performance, usability, and scalability.\nGather, analyze, and translate business requirements into technical specifications for BI solutions.\nImplement security settings, row-level security, and user access controls in QuickSight.\nCollaborate with cross-functional teams including data engineers, data scientists, and business analysts.\n\n\nInterested can send your resume to gowtham.veerasamy@wavicledata.com.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Quicksight', 'AWS', 'SQL']",2025-06-11 06:03:01
Snowflake Developer,TechStar Group,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","JD:\nSnowflake Implementer :\nDesigning, implementing, and managing Snowflake data warehouse solutions, ensuring data integrity, and optimizing performance for clients or internal teams.\nStrong SQL skills: Expertise in writing, optimizing, and troubleshooting SQL queries. \nExperience with data warehousing: Understanding of data warehousing concepts, principles, and best practices. \nKnowledge of ETL/ELT technologies: Experience with tools and techniques for data extraction, transformation, and loading. \nExperience with data modeling: Ability to design and implement data models that meet business requirements. \nFamiliarity with cloud platforms: Experience with cloud platforms like AWS, Azure, or GCP (depending on the specific Snowflake environment). \nProblem-solving and analytical skills: Ability to identify, diagnose, and resolve technical issues. \nCommunication and collaboration skills: Ability to work effectively with cross-functional teams. \nExperience with Snowflake (preferred): Prior experience with Snowflake is highly desirable. \nCertifications (preferred): Snowflake certifications (e.g., Snowflake Data Engineer, Snowflake Database Administrator) can be a plus. Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'SQL', 'Cloud']",2025-06-11 06:03:02
HR Recruiter cum HRBP,Anarock Property Consultants,4 - 8 years,6-7.5 Lacs P.A.,"['Bengaluru( Palace Road, HSR Layout )']","Responsibilities:\nPartner with hiring managers to understand technical hiring needs and develop sourcing strategies.\nManage end-to-end recruitment life cycle for tech roles (SDEs, QA, DevOps, Data Engineers, etc.).\nSource, screen, and interview candidates using job portals, Naukri, IIM Jobs, LinkedIn, employee referrals, and other channels.",,,,"['Technical Hiring', 'Negotiation', 'Sales Hiring', 'Business Partnering', 'Recruitment', 'Hiring', 'Employee Engagement', 'Campus Hiring', 'Talent Sourcing']",2025-06-11 06:03:04
Qa Lead,Tredence,8 - 13 years,Not Disclosed,"['Pune', 'Chennai', 'Delhi / NCR']","QA Lead/ SME:\n8-12 yrs\nLocation: Chennai/ Pune/ Gurgaon/ Kolkata\nJob description:\nBachelor's degree or higher in an engineering field (e. g. Computer Science, Computer Engineering, etc.) with 8- 12 years of relevant experience in building QA teams\nProven track record of working as a QA SME and experienced in implementing a range of QA projects in the areas of full stack solutions, Data Engineering and Data Science solutions in an agile environment.",,,,"['Performance Testing', 'API Testing', 'Automation Testing', 'Ui Automation', 'Manual Testing', 'JMeter']",2025-06-11 06:03:05
Palantir Tech Lead,Veear Projects Inc,13 - 22 years,Not Disclosed,['Hyderabad'],"Palantir Tech Lead\nSkills - Python , Pyspark, Palantir\nLocation - WFO Hyderabad\n\nRole & responsibilities\n\nLeads data engineering activities on moderate to complex data and analytics-centric problems which have broad impact and require in-depth analysis to obtain desired results; assemble, enhance, maintain, and optimize current, enable cost savings and meet individual project or enterprise maturity objectives.",,,,"['Pyspark', 'Palantir', 'Foundry Tools', 'Workshop', 'Python', 'Map', 'SQL']",2025-06-11 06:03:07
Senior Workato Developer,Foundever,5 - 9 years,9.5-19.5 Lacs P.A.,['Chennai'],"About Us: At Foundever, we are dedicated to harnessing the power of data to drive business transformation and deliver exceptional results for our clients. We are seeking a detail-oriented and analytical Data Engineer to join our team. The ideal candidate will have strong ETL skills and experience in data management within a BPO environment.\n\nResponsibilities\nSenior Workato Developer",,,,"['Workato', 'integration development', 'SQL', 'Python']",2025-06-11 06:03:09
Sr. SAP Functional Consultant,Simdaa Technologies,6 - 8 years,Not Disclosed,['Mumbai'],"We are looking for an experienced SAP Functional Consultant with expertise in the Agro-Chemical industry to support development, and support of SAP modules of FI, CO, SD, MM based reporting in SAP BW, Datalake, Visualisation tools. The ideal candidate must have a strong command over configuration, transaction codes (Tcodes), and integration with other modules. Need hands-on with SAP BW reports understanding (atleast as End User). Additionally, the candidate should be well-acquainted with both SAP ECC and S/4HANA configurations and data dictionaries, with a good-to-have experience in migrating from SAP ECC to SAP S/4HANA.\nThe role also involves managing analytics projects, collaborating with business users, preparing functional specifications, and ensuring seamless development execution following Agile methodology.\nKey Responsibilities:\nLead SAP FI, CO, MM, SD modules related reporting enhancement and support, ensuring best practices for the Mfg. domain.\nWork closely with business users and SAP Functional team to gather requirements, draft functional specifications, and ensure system alignment.\nEnsure smooth integration and data consistency across SAP ECC and SAP S/4HANA environments with SAP BW reports, datalake, dashboarding solutions.\nCollaborate with development teams of SAP BW, Visualisation Engineers and Data Engineers to drive execution and optimize performance.\nHandle analytics-driven projects, including dashboard creation and KPI tracking.\nConduct User Acceptance Testing (UAT) and manage the closure of analytics reports and dashboards.\nEnsure compliance with corporate governance and best practices within an Indian MNC setup.\nKey Requirements: years of experience as an SAP MM/FICO/SD Functional Consultant (Agro-Chemical industry preferred).\nExpertise in SAP MM, FICO and SD configuration, master data\nStrong understanding of Procure-to-Pay, Order-to-Cash, Record-to-Report processes\nWork experience on SAP BW reports\nExposure to handling analytics requirements, dashboards, and reports.\nExcellent stakeholder management skills, ensuring smooth collaboration with business users and development teams.\nHands-on experience with UAT, issue resolution, and deployment coordination.\nStrong interpersonal skills to adapt to the corporate culture of an Indian MNC.\nPreferred Skills:\nAbility to work in fast-paced environments, managing multiple projects efficiently.\nKnowledge of SAP S/4HANA migration strategies, ensuring data consistency.\nFamiliarity with third-party integrations and data analytics tools.",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['SAP FICO', 'SAP SD', 'SAP MM', 'BW', 'Sap S4 Hana Sd', 'HANA', 'Functional Consultancy', 'Sap Sales And Distribution']",2025-06-11 06:03:10
Jr.AI Engineer,Tekone It Services,1 - 3 years,1.5-6.5 Lacs P.A.,['Hyderabad'],"Position Overview\nWe are hiring five AI Engineers with 12 years of experience to join our dynamic team in Hyderabad. The ideal candidates will have a solid foundation in Large Language Models (LLMs), LangChain, and Generative AI (GenAI) frameworks. This is a great opportunity to work on innovative AI solutions, contributing to projects that integrate LLMs, prompt engineering, RAG pipelines, and cloud-based deployments.\nKey Responsibilities\nContribute to the design and development of AI-powered applications utilizing LLMs (GPT-3.5, GPT-4, Gemini).\nAssist in building LangChain-based pipelines and workflows, including LangSmith and LangGraph.\nSupport the implementation of Retrieval-Augmented Generation (RAG) frameworks using vector databases such as ChromaDB.\nApply prompt engineering techniques to optimize model responses and improve contextual accuracy.\nDevelop RESTful APIs using Flask or FastAPI to enable model consumption in production environments.\nWrite and manage data workflows using SQL, PySpark, and Spark SQL.\nDeploy and monitor models on Azure Machine Learning or AWS Bedrock platforms.\nCollaborate with cross-functional teams, including data scientists, engineers, and business stakeholders.\nRequired Skills\nProficiency in Python, SQL, PySpark, and Spark SQL\nHands-on experience with LLMs: GPT-3.5, GPT-4, Gemini\nKnowledge of LangChain, LangSmith, LangGraph\nFamiliarity with Vector Databases (e.g., ChromaDB) and embeddings\nExperience with prompt engineering and RAG-based architectures\nExposure to cloud platforms such as Azure ML or AWS Bedrock\nStrong understanding of REST APIs and version control systems (Git/GitHub)\nPreferred Qualifications\nBachelor's degree in Computer Science, Artificial Intelligence, Data Science, or a related field\nInternship or academic project experience in NLP, LLMs, or GenAI technologies\nFamiliarity with MLOps tools and practices (e.g., CI/CD, Airflow)\nStrong problem-solving abilities, attention to detail, and a collaborative mindset",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'Prompt Engineering', 'Artificial Intelligence', 'llm']",2025-06-11 06:03:12
AI Ml Engineer,Fulcrum Worldwide Software,4 - 8 years,15-30 Lacs P.A.,['Pune'],"The Role\nMachine learning and data science are interdisciplinary fields that require a combination of skills in mathematics, statistics, programming, and domain-specific knowledge to extract meaningful insights from data and develop predictive models.\nSkills Requirements\nMandatory Skillset - ML Techniques, any ML Framework, Gen AI COncepts, Azure, Python\nSecondary Skillset - AWS, or Google Cloud\n\nRequirements\nWork closely with cross-functional teams to understand business requirements and translate them into machine learning solutions.\nCollect, clean, and pre-process large datasets to prepare them for machine learning models.\nDevelop and implement machine learning algorithms and models for solving specific business problems.\nCollaborate with data engineers to ensure seamless integration of machine learning models into production systems.\nFine-tune models for optimal performance and conduct thorough testing and validation.\nStay updated on the latest advancements in machine learning and artificial intelligence and assess their potential impact on our projects.\nEffectively communicate complex technical concepts and findings to non-technical stakeholders.\nMonitor and maintain deployed models and update them as needed to adapt to changes in data or business requirements.\nAdhere to ethical standards and ensure that machine learning solutions comply with relevant regulations.\nProficiency in Python with hands-on experience in cloud platforms including GCP, AWS, and Azure; strong skills in API integration.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['NLP', 'Generative ai', 'Machine Learning', 'Deep Learning']",2025-06-11 06:03:13
Learning Counselor - Inside Sales,Analytics Vidhya,1 - 5 years,Not Disclosed,['Gurugram'],"About Analytics Vidhya\nAnalytics Vidhya is on a mission to build next-generation AI Professionals across the globe. We aim to build the best community knowledge platform to learn, teach, apply and evaluate their data skills (data science, data engineering, data analysis, machine learning, Artificial Intelligence and more).\nThe AVians (our community members) seamlessly learn through our high quality AI programs, apply these skills to solve real life industry problems and find career defining jobs through Analytics Vidhya.\n\nWhy join Analytics Vidhya?\nAnalytics Vidhya is in the growth stage and while we are at it, we also ensure growth of our employees! Join us if you want to be the sales leader of tomorrow, join us for learning, join us for ownership - and get rewarded handsomely for your performance. \n\nResponsibilities\nBuild Relationships: Engage and follow up with prospects to nurture trust and long-term connections.\nManage Complete Customer Lifecycle: Oversee the sales process from lead engagement to program enrollment.\nCounsel Professionals: Recommend courses aligned with career goals via calls and emails.\nMaintain Records: Use CRM tools to track sales activities and manage pipelines.\n\nRequirements\nProven experience as B2C sales representative \nExcellent communication and interpersonal skills\nOutstanding negotiation skills to close sales with the ability to resolve issues and address complaints\nHigh customer empathy\n\nPerks and Benefits\nAt Analytics Vidhya, we believe in rewarding excellence and fostering a vibrant, success-driven team culture. Heres what you can look forward to as part of our dynamic sales team:\nDream Destinations: Pack your bags for international trips to exciting destinations as part of our celebrations for top-performing teams!\nExciting Prizes: Join an exceptional sales team and win incredible rewards like the latest iPhones, gift vouchers, and more.\nTeam Competitions: Participate in fun sales challenges to win cash prizes and uplift team spirit.\nQuarterly Reward Challenges: Achieve greatness with exclusive individual and team challenges, earning cash rewards for outstanding performance.\n\nCriteria\nShould have worked in B2C sales role in for at-least one year\nMin Qualification: Graduate/Diploma\nYour success is our success, and we cant wait to celebrate it with you—on stage, on trips, and with incredible rewards!\n***Analytics Vidhya holds the right to change/update any perks and benefits at its sole discretion***",Industry Type: E-Learning / EdTech,Department: Sales & Business Development,"Employment Type: Full Time, Permanent","['sales', 'Edtech', 'Admission Counselling', 'B2C Sales', 'Edtech Sales', 'Inside Sales', 'Business Development', 'Career Counselling']",2025-06-11 06:03:15
Power Bi Developer,IntelERA Inc,7 - 12 years,20-35 Lacs P.A.,[],"Job Title: Power BI Engineer\nLocation: Fully Remote (Preferred candidates in Pune, but open to all)\nWork Hours: 4:30 PM to 1:30 AM IST\nExperience Required: 7+ Years\n\nWe are seeking an experienced Power BI Engineer to join our team. The ideal candidate has a strong background in data analytics, visualization, and business intelligence, with a proven track record of designing and implementing impactful dashboards and reports. This role requires advanced skills in Power BI, data modeling, DAX, and ETL processes, as well as experience with large datasets and complex data transformations.\n\n\nKey Responsibilities:\n\nDesign, develop, and deploy Power BI dashboards and reports to provide insights that drive business decisions.\nWork closely with business stakeholders to gather and translate requirements into scalable, impactful data visualizations.\nImplement and optimize data models, using DAX to create robust calculations and complex measures.\nIntegrate and manage diverse data sources, including SQL databases, cloud services (e.g., Azure), APIs, and flat files.\nConduct data analysis and quality assurance to ensure data accuracy and report integrity.\nDevelop ETL processes using Power Query, SQL, and Azure Data Factory to extract, transform, and load data.\nImplement row-level security (RLS) and ensure adherence to data governance and security policies.\nPerform performance tuning on Power BI dashboards, ensuring efficient load times and smooth user experience.\nCollaborate with data engineers, data scientists, and other stakeholders to integrate advanced analytics and predictive insights.\nStay up-to-date with the latest Power BI features, best practices, and trends to continuously enhance BI solutions.\n\nRequired Skills and Qualifications:\n\n7+ years of experience in business intelligence and data analytics, with a strong focus on Power BI.\nProficient in Power BI Desktop, Power BI Service, and Power BI Report Server.\nExpertise in data modeling, DAX (Data Analysis Expressions), and M language.\nSolid understanding of relational databases and SQL, with experience in database management (SQL Server, Oracle, etc.).\nHands-on experience with ETL tools, such as Power Query, SQL Server Integration Services (SSIS), or Azure Data Factory.\nFamiliarity with cloud platforms, particularly Azure services like Azure SQL Database, Azure Data Lake, and Azure Analysis Services.\nStrong analytical skills, with a keen eye for detail and a commitment to data quality.\nExperience in implementing row-level security (RLS) and maintaining compliance with data governance standards.\nFamiliarity with scripting languages like Python or PowerShell for automation is a plus.\nExcellent communication and interpersonal skills, with the ability to effectively collaborate with both technical and non-technical stakeholders.\n\nSoft Skills:\n\nTakes ownership, is a proactive problem-solver with a positive, can-do attitude.\nPassionate about development.\nExcellent communication and teamwork skills.\nAbility to work effectively in a remote environment.\n\nNice to Have:\n\nKnowledge of Agile methodologies and familiarity with tools like Azure DevOps or JIRA.\nCertifications in Power BI, Microsoft Azure, or data engineering are advantageous.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Data Visualization', 'Dax', 'Data Modeling', 'Data Analytics', 'Azure Data Factory', 'Power Bi Dashboards', 'SSIS', 'Power Query', 'SQL']",2025-06-11 06:03:17
Staff Quality Engineer,Fanatics,9 - 14 years,40-50 Lacs P.A.,['Hyderabad'],"As a Staff Quality Engineer at Fanatics, you will specialize in building automated testing including test suites, reporting capabilities and continuous integration to ensure the accuracy and consistency of the systems driving our inventory lifecycle, planning and product performance. Youll be part of a multi-disciplinary Quality Engineering team while also working with our data engineers to ensure we have tools and processes that warrant the correctness of the functionality and data and quickly identify any problems. The teams within our inventory organization own the source of truth for all inventory position related info, develop the systems that handle the inventory lifecycle of our products and the integrations that keep our partners updated with an accurate picture of what we can sell to our fans and what they demand the most. Our teams also work on models to drive efficiency of our inventory management. Multiple teams across Fanatics Commerce, including Fulfillment, Merchandising and Planning, Finance, Business Intelligence and Analytics rely on these datasets and several KPIs dashboards used by our Senior Leadership aid data-driven business decisions. The team takes pride in ensuring that these datasets and the processes that generate the data are accurate, reliable and available on time by building and operating near real-time & batch data pipelines, exploring emerging technologies in this area, and helping other teams consume and realize the value of the data. \nIf you are an experienced Quality Engineer with a passion for delivering quality products and are curious and eager to keep learning and discovering new tools to improve the efficiency of our quality cycle this opportunity is for you. \n\nResponsibilities:\nNurture a culture of quality through collaboration with teammates across the engineering function to make sure quality is embedded in both processes and technology. \nDevelop quality focused test strategies that help the team deliver software that provides maximum quality without sacrificing business value. \nDefine quality metrics and build quality monitoring solutions and dashboards. \nMentor/coach team members to ensure appropriate testing coverage within the team with a focus on continuous testing and a shift-left approach. \nContribute to process improvements and refinement of Quality Engineering practices across the QE function. \nRecommend best test practices and evangelize a testing early culture to empower the team to make sound engineering decisions. \nWork with multiple teams of engineers and product managers to gather requirements and data sources to design and build test plans. \nParticipate in projects developed with agile methodology. \nDesign and document test cases and test data to ensure proper coverage. \nPerform exploratory/manual tests as needed to ensure quality of the data across all data platform components. \nCollaborates with data partners to triage issues and works to ensure the validity, timeliness, consistency, completeness and accuracy of our data across all data platform components. \nWrite, execute, and monitor automated test suites for integration and regression testing. \nIntegrate tests as part of continuous delivery pipelines. \nResearch new tools that can improve our quality process and implement them. \n\nA suitable candidate would have:\nMinimum 8 years of testing experience certifying the quality of with applications developed in languages like Python, Golang, Java, C#. \nExperience in a programming language like Python, Java, Golang, Node,js using it for automation and data validation. \nMust understand databases and ORMs, experienced with at least one RDBMS and DB Query language. \nAbility to define technical standards and guidelines for test framework development. \nExperience on modern Quality Engineering principles such as Continuous Testing and Shift Left. \nClear understanding of different testing principles like data driven testing, behavior driven development, etc. \nSolid experience in writing clear, concise, and comprehensive test plans and test cases. \nExperience in building automated test suites for API's REST and gRPC with focus on data validation. \nExtensive experience with OpenAPI Specifications and tools like Swagger for designing, documenting, and validating APIs. \nProficiency in using schema validation tools for ensuring compliance with API contracts. \nStrong understanding of RESTful principles, HTTP protocols, and JSON schema validation. \nProficiency in managing test data and mocking/stubbing for complex test scenarios. \nExperience testing applications using producer/consumer models and event streaming like Kafka, RabbitMQ, Pulsar is a plus. \nProficiency in testing tools such as Postman, K6 and JMeter for API performance and functional testing. \nHands-on experience with test automation tools like Playwright, Selenium, and other modern test frameworks. \nGood understanding of service oriented and microservices architecture. \nExperience with cloud environments like AWS, GCP, source control tools like Github and continuous integration and delivery software, preferably, Gitlab. \nExperience with AWS services like S3, SQS, RDS, Elasticache, EMR \nProven track record of mentoring team members on automation best practices, framework design, and testing methodologies \nAbility to lead technical discussions and drive decisions on test automation strategies and implementation. \nStrong debugging, root cause analysis, and problem-solving skills. \nAbility to troubleshoot issues in distributed systems, APIs, and automation tools. \nGreat communication skills \nExperience on inventory management and fulfillment processes and workflow orchestration tools like Maestro, Temporal, etc. are a plus.",Industry Type: Internet (E-Commerce),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Playwright', 'Data Testing', 'Automation Testing', 'Selenium', 'Python', 'java', 'API Testing', 'Functional Testing', 'SQL']",2025-06-11 06:03:18
Azure Solution Architect,Tanisha Systems,12 - 18 years,40-45 Lacs P.A.,"['Pune', 'Gurugram', 'Bengaluru']","Role & responsibilities\nWe are looking for Azure Solution Architect for MNC company permanent position for Remote.\n\nPreferred candidate profile\nIndustry: Financial Services / Banking\nKey Responsibilities\n\nArchitect, design, and lead the migration of Sybase ASE/IQ databases to Azure SQL\nConduct in-depth assessments of existing on-prem Sybase infrastructure and define the cloud migration blueprint\nDefine and implement scalable, secure, and high-performing Azure SQL architectures tailored to financial workloads\nCollaborate with cloud engineering, security, infrastructure, and business teams to ensure alignment\nLead end-to-end migration execution, including schema conversion, data movement, validation, and cutover\nEstablish automated ETL pipelines, backup/recovery strategies, and rollback mechanisms in Azure\nEnsure compliance with financial data regulations (SOX, GDPR, PCI-DSS, etc.)\nOptimize workloads post-migration for performance, cost efficiency, and maintainability\nLead governance reviews, technical workshops, and create documentation and runbooks\n\n\nRequired Skills & Experience\n\n12+ years of overall experience in enterprise data architecture and cloud solutions\nMandatory experience in migrating Sybase ASE/IQ to Azure SQL in large-scale environments\nDeep understanding of Azure SQL, Azure Data Factory, Azure Migrate, BACPAC, and SSMA\nExperience with cloud-native data tools for transformation, validation, and monitoring\nProficiency in scripting (PowerShell, Python) for automation and orchestration\nStrong understanding of data lineage, data masking, encryption, and compliance frameworks\nDemonstrated success in leading multi-terabyte data migrations in financial or regulated environments\nExcellent communication skills with experience in dealing with onsite and offshore teams\n\n\nPreferred Qualifications\n\nMicrosoft Certified: Azure Solutions Architect Expert or Data Engineer Associate\nFamiliarity with performance tuning and resource optimization in Azure SQL\nExperience with application re-platforming or modernization post-migration",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure data migration', 'financial domain', 'Azure services', 'Sybase', 'Architect', 'Azure Migrate', 'Azure', 'Power shell', 'Azure Certified', 'Azure SQL', 'SQL', 'Azure Data Factory', 'scripting', 'SSMA', 'ETL', 'BACPAC', 'Python']",2025-06-11 06:03:20
Product manager (Solar),Renew,5 - 8 years,Not Disclosed,['Haryana'],"About Company\nJob Description\nResponsibilities\n•    Understand Business problems and identify constraints\n•    Design digital and advance analytics solutions to Business problems\n•    Implement the solution with an understanding of end-to-end architecture\n•    Identify opportunities for implementation of new use cases\n•    Keep updates of any policy changes in power markets\n•    Ensure ReD targets are met and delivered on time\n•    Ensure documentation of Use Cases\nOur Ideal Candidate\n•    Education - Engineering (Electrical/Electronics/IT) + MBA \n•    Experience Range – 4 to 7 years\n•    Experience in Renewable energy/ Storage/Hydro/RTC power/Power Trading\n•    Good program management, Project planning & coordination skills\n•    Good Experience of working in cross functional teams\n•    Good IT skills - understanding different solutions and matching solutions to problems\n•    Analytical approach to solving problems with focus on solution delivery\n•    Capable of extrapolating current situation to future scenarios\nFunctional/ Domain expertise\n•    Knowledge of Power markets is a must\n•    Should have experience in evaluating or implementing any of the new technologies (BESS/ Hybrids/ EV, Charging Infra/ Pumped Storage/ Hydrogen/Market procurement of RE - GTAM) \n•    Participated in some Digital transformation/enablement exercise in organisation\n•    Basic understanding of work of Data Scientists & Data engineering roles is a plus\n•    Experience in agile working methodology would be a plus\n•    Experience with tools like PowerBI, Tableau, JIRA would be a plus\nCommunication skills\n•    Ability to communicate with cross functional roles is a must\n•    Excellent written and verbal communication skills\n•    Good presentation skills\nTeamwork\n•    Ability to work as a self-motivated team player\n•    Has worked in large teams with an agile setup in the past\n•    Handle multiple projects across intra and inter-department teams\n",Industry Type: Power,Department: Product Management,"Employment Type: Full Time, Permanent","['power trading', 'analytical', 'program management', 'verbal communication', 'presentation skills', 'power bi', 'coordination', 'analytics', 'tableau', 'product management', 'use cases', 'writing', 'agile', 'rational team concert', 'digital transformation', 'project planning', 'communication skills', 'jira', 'architecture']",2025-06-11 06:03:21
Job opportunity For MLOPS,Sightspectrum,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Greetings from Sight Spectrum Technologies!!!\n\n\nWe would like to ensure that you are interested in this position.\n\nCompany: Sight Spectrum Technologies(https://sightspectrum.com/)\n\nExperience :  5+Years \nLocation: Chennai, Bangalore, hyderabad, Coimbatore\nDescription:\n\n\nStrong experience in ETL development, data modeling, and managing data in large-scale environments.\n- Proficient in AWS services including SageMaker, S3, Glue, Lambda, and CloudFormation/Terraform. - Hands-on expertise with MLOps best practices, including model versioning, monitoring, and CI/CD for ML pipelines.\n- Proficiency in Python and SQL; experience with Java is a plus for streaming jobs.\n- Deep understanding of cloud infrastructure automation using Terraform or similar IaC tools.\n- Excellent problem-solving skills with the ability to troubleshoot data processing and deployment issues.\n- Experience in fast-paced, agile development environments with frequent delivery cycles.- Strong communication and collaboration skills to work effectively across cross-functional teams\n\nPlease fill the below details for reference.\nTotal Experience:\nRelevant Experience:\nCurrent CTC:\nExpected CTC:\nNotice Period (LWD):\nCurrent Location:\nPreferred Location:\nPayroll Company:\nReason for change:\nClient Company:\nOffer Details:\nPF(Yes/No):\nUAN No:\nLinkdln Id:\n\n\nIf interested kindly share your resume to roopavahini@sightspectrum.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MLOPS', 'Aws Sagemaker', 'ETL', 'AWS']",2025-06-11 06:03:23
ETL Engineer,Jasper Colin Research,3 - 8 years,10-20 Lacs P.A.,['Gurugram'],"Role Overview\nWe are looking for a Senior ETL Engineer with deep expertise in Apache Airflow to design, build, and manage complex data workflows and pipelines across cloud platforms. The ideal candidate will bring strong experience in Python, SQL, and cloud-native tools (AWS/GCP) to deliver scalable and reliable data infrastructure, supporting analytics, reporting, and operational systems.\nKey Responsibilities\nDesign, implement, and optimize scalable ETL/ELT workflows using Apache Airflow DAGs.\nBuild and maintain data pipelines with Python and SQL, integrating multiple data sources.\nDevelop robust solutions for pipeline orchestration, failure recovery, retries, and notifications.\nLeverage AWS or GCP services (e.g., S3, Lambda, BigQuery, Cloud Functions, IAM).\nIntegrate with internal and external data systems via secure REST APIs and Webhooks.\nMonitor Airflow performance, manage DAG scheduling, and resolve operational issues.\nImplement observability features like logging, metrics, alerts, and pipeline health checks.\nCollaborate with analytics, data science, and engineering teams to support data needs.\nDrive automation and reusability across pipeline frameworks and templates.\nEnsure data quality, governance, compliance, and lineage across ETL processes.\nRequired Skills\n5+ years of experience in ETL/Data Engineering with hands-on Airflow expertise.\nStrong programming skills in Python, with solid experience writing production scripts.\nProficiency in SQL for data manipulation, joins, and performance tuning.\nDeep knowledge of Apache Airflowscheduling, sensors, operators, XComs, and hooks.\nExperience working on cloud platforms like AWS or GCP in data-heavy environments.\nComfort with REST APIs, authentication protocols, and data integration techniques.\nKnowledge of CI/CD tools, Git, and containerization (Docker, Kubernetes).\nNice to Have\nFamiliarity with dbt, Snowflake, Redshift, BigQuery, or other modern data platforms.\nExperience with Terraform or infrastructure-as-code for Airflow deployment.\nUnderstanding of data privacy, regulatory compliance (GDPR, HIPAA), and metadata tracking.",Industry Type: Miscellaneous,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Apache Airflow', 'Data Engineering', 'ETL', 'SQL', 'Python']",2025-06-11 06:03:25
Database Engineer,TechStar Group,9 - 12 years,25-35 Lacs P.A.,['Hyderabad'],"Experience 12yrs only\nNotice period :- immediate / 15days\nLocation :- Hyderabad\nClient :- Tech Star Group\nPlease highlight the mandatory sill in resume .\nClient Feedback :-\nIn short, the client is primarily looking for a candidate with strong expertise in data-related skills, including:\n\nSQL & Database Management: Deep knowledge of relational databases (PostgreSQL), cloud-hosted data platforms (AWS, Azure, GCP), and data warehouses like Snowflake.\nETL/ELT Tools: Experience with SnapLogic, StreamSets, or DBT for building and maintaining data pipelines. / ETL Tools Extensive Experience on data Pipelines\nData Modeling & Optimization: Strong understanding of data modeling, OLAP systems, query optimization, and performance tuning.\nCloud & Security: Familiarity with cloud platforms and SQL security techniques (e.g., data encryption, TDE).\nData Warehousing: Experience managing large datasets, data marts, and optimizing databases for performance.\nAgile & CI/CD: Knowledge of Agile methodologies and CI/CD automation tools.\n\nImp :-The candidate should have a strong data engineering background with hands-on experience in handling large volumes of data, data pipelines, and cloud-based data systems",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Warehousing', 'ETL', 'Elt', 'SQL', 'Azure', 'Data Pipeline', 'GCP', 'Postgresql', 'AWS', 'Python']",2025-06-11 06:03:27
Python Software Developer,66degrees,5 - 8 years,18-25 Lacs P.A.,"['Mysuru', 'Pune', 'Bengaluru']","Overview of 66degrees\n66degrees is a leading consulting and professional services company specializing in developing AI-focused, data-led solutions leveraging the latest advancements in cloud technology. With our unmatched engineering capabilities and vast industry experience, we help the world's leading brands transform their business challenges into opportunities and shape the future of work.\nAt 66degrees, we believe in embracing the challenge and winning together. These values not only guide us in achieving our goals as a company but also for our people. We are dedicated to creating a significant impact for our employees by fostering a culture that sparks innovation and supports professional and personal growth along the way.",,,,"['Python Development', 'Java', 'Software Development', 'GCP', 'Javascript', 'Google Cloud Platforms']",2025-06-11 06:03:29
Walk-In Drive Chiacon Consulting_21st June,Chiacon Consulting,3 - 7 years,Not Disclosed,['Gurugram'],"We're Hiring at Chiacon!\n\nJoin our growing team of innovators and changemakers! Were conducting a Walk-In Drive for multiple roles check out the key skills below\n\nWalk-In Drive Details\nDate: 21st June 2025 (Saturday)\nTime: 11:00 AM to 3:00 PM\n\nLocation: Chiacon Consulting Pvt. Ltd. 8th floor, Magnum Tower1 , Sector-58, Gurugram -122011\n\nRegister Here: https://forms.office.com/r/6auLmSZ16E\n\nTech Lead-Data & Analytics : (6 + years of experience)\nProven experience in technical leadership, solution architecture, and team guidance .\nSQL & Data Engineering\nSolution Architecture & Technical Leadership\nMicrosoft Power Platform (Power BI, Power Apps, Power Automate)\nAzure (Blob Storage, Data Lake, AI services)\nETL & Cloud Data Pipelines\nIT Business Development Manager:4-10 Years of\nexperience (mandatory 2 years of experience in IT Sales)\nInside Sales & Lead Generation\nMinimum of 2 years of proven experience in IT Sales, preferably in consulting or services\nIT Services Sales (RPA, Data Analytics, QA, App Dev)\nProposal & Pitch Deck Creation\nCRM Tools (Zoho, HubSpot, Salesforce)\nStrong Communication & Client Handling\nBusiness Analyst:(2-4 years of experience)\nBusiness Process Mapping & Documentation\nRequirement Gathering & Gap Analysis\nProject Planning & Roadmap Creation\nData-Driven Insights & Presentations (Excel, PowerPoint)\nCross-Functional Team Collaboration\nPower Apps Developer:(2-7 years of experience)\nPower Apps (Canvas & Model-Driven)\nPower Automate, Dataverse, SharePoint Integration\nCustom Connectors & Power Fx\nMicrosoft 365 (Teams, Outlook, SharePoint)\nPower Platform Certifications (PL-100/PL-400) is pluse\nIT Recruiter:(0-6 months)\nEnd-to-End Technical Recruitment\nIT Recuritment\nResume Screening & Sourcing via Naukri, LinkedIn\nInterview Coordination\nExcellent Communication & Stakeholder Management\n\nIf you or someone you know is interested, don't miss the opportunity.\n\nRegister now and walk in to explore your next big career move!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Team Handling', 'Lead Generation', 'Power Automate', 'Team Leading', 'IT Sales', 'End To End Sales', 'Powerapps', 'Recruitment']",2025-06-11 06:03:31
SRE - Site Reliability Engineer,TechBlocks,6 - 11 years,20-25 Lacs P.A.,"['Hyderabad', 'Ahmedabad']","Hi Aspirant,\n\nGreetings from TechBlocks - IT Software of Global Digital Product Development - Hyderabad !!!\n\nAbout us : TechBlocks is a global digital product engineering company with 16+ years of experience helping Fortune 500 enterprises and high-growth brands accelerate innovation, modernize technology, and drive digital transformation. From cloud solutions and data engineering to experience design and platform modernization, we help businesses solve complex challenges and unlock new growth opportunities. \n\n\nJob Title:  Senior DevOps Site Reliability Engineer (SRE)\n\nLocation: Hyderabad & Ahmedabad \n\nEmployment Type: Full-Time \n\nWork Model - 3 Days from office\n\nJob Overview \nDynamic, motivated individuals deliver exceptional solutions for the production resiliency of the systems. The role incorporates aspects of software engineering and operations, DevOps skills to come up with efficient ways of managing and operating applications. The role will require a high level of responsibility and accountability to deliver technical solutions.\n\nSummary:\nAs a Senior SRE, you will ensure platform reliability, incident management, and performance optimization. You'll define SLIs/SLOs, contribute to robust observability practices, and drive proactive reliability engineering across services.\n\nExperience Required:\n610 years of SRE or infrastructure engineering experience in cloud-native environments.\n\nMandatory:\nCloud: GCP (GKE, Load Balancing, VPN, IAM)\nObservability: Prometheus, Grafana, ELK, Datadog\nContainers & Orchestration: Kubernetes, Docker\nIncident Management: On-call, RCA, SLIs/SLOs\nIaC: Terraform, Helm\nIncident Tools: PagerDuty, OpsGenie\n\nNice to Have:\nGCP Monitoring, Skywalking\nService Mesh, API Gateway\nGCP Spanner,\n\nScope:\nDrive operational excellence and platform resilience\nReduce MTTR, increase service availability\nOwn incident and RCA processes\n\nRoles and Responsibilities:\n\nDefine and measure Service Level Indicators (SLIs), Service Level Objectives (SLOs), and manage error budgets across services.\nLead incident management for critical production issues drive Root Cause Analysis (RCA) and postmortems.\nCreate and maintain runbooks and standard operating procedures for high availability services.\nDesign and implement observability frameworks using ELK, Prometheus, and Grafana; drive telemetry adoption.\nCoordinate cross-functional war-room sessions during major incidents and maintain response logs.\nDevelop and improve automated System Recovery, Alert Suppression, and Escalation logic.\nUse GCP tools like GKE, Cloud Monitoring, and Cloud Armor to improve performance and security posture.\nCollaborate with DevOps and Infrastructure teams to build highly available and scalable systems.\nAnalyze performance metrics and conduct regular reliability reviews with engineering leads.\nParticipate in capacity planning, failover testing, and resilience architecture reviews.\n\nIf you are interested , then please share me your updated resume to kranthikt@tblocks.com\n\n\nWarm Regards,\n\nKranthi Kumar\nkranthikt@tblocks.com\nContact: 8522804902\nSenior Talent Acquisition Specialist\nToronto | Ahmedabad | Hyderabad | Pune\nwww.tblocks.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SRE', 'Gcp Cloud', 'Monitoring Tools', 'Site Reliability Engineering', 'Log Management', 'Kibana', 'Cloud Monitoring', 'Prometheus', 'Logstash', 'Datadog', 'Grafana', 'Elk Cluster', 'Docker', 'Dynatrace', 'Alerts', 'Kubernates', 'Appdynamics']",2025-06-11 06:03:32
Snowflake Implementer,TechStar Group,9 - 14 years,10-20 Lacs P.A.,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","JD:\nSnowflake Implementer :\nDesigning, implementing, and managing Snowflake data warehouse solutions, ensuring data integrity, and optimizing performance for clients or internal teams.\nStrong SQL skills: Expertise in writing, optimizing, and troubleshooting SQL queries.\nExperience with data warehousing: Understanding of data warehousing concepts, principles, and best practices.\nKnowledge of ETL /ELT technologies: Experience with tools and techniques for data extraction, transformation, and loading.\nExperience with data modeling: Ability to design and implement data models that meet business requirements.\nFamiliarity with cloud platforms: Experience with cloud platforms like AWS, Azure, or GCP (depending on the specific Snowflake environment).\nProblem-solving and analytical skills: Ability to identify, diagnose, and resolve technical issues.\nCommunication and collaboration skills: Ability to work effectively with cross-functional teams.\nExperience with Snowflake (preferred): Prior experience with Snowflake is highly desirable.\nCertifications (preferred): Snowflake certifications (e.g., Snowflake Data Engineer, Snowflake Database Administrator) can be a plus. Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'ETL', 'Snowflake certifications', 'SQL', 'Cloud Platforms', 'ELT']",2025-06-11 06:03:34
Databricks with pyspark&python Trainer,Coactive It Solutions,6 - 10 years,Not Disclosed,[],Databricks with pyspark&python Training,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Databricks with pyspark&python', 'Python', 'Spark']",2025-06-11 06:03:35
Power Bi Developer,Mindsprint,4 - 6 years,12-15 Lacs P.A.,['Chennai'],"Job Description: An experienced and skilled BI engineer with designing, developing, and deploying business intelligence solutions using Microsoft Power BI\nMandatory Skills\n4-6 years of experience in Power BI\nStrong knowledge in data transformation using Power Query.\nAbility to write complex DAX formula for data aggregation, filtering, ranking etc.\nString knowledge in schema modelling in Power BI.",,,,"['Power Bi', 'Fabric', 'Azure Devops']",2025-06-11 06:03:37
Snowflake Developer ( Snowflake + DBT/SQL Expert),Fortune India 500 IT Services Firm,6 - 11 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","We are looking for a Snowflake Developer with deep expertise in Snowflake and DBT or SQL to help us build and scale our modern data platform.\n\nKey Responsibilities:\nDesign and build scalable ELT pipelines in Snowflake using DBT/SQL.\nDevelop efficient, well-tested DBT models (staging, intermediate, and marts layers).\nImplement data quality, testing, and monitoring frameworks to ensure data reliability and accuracy.\nOptimize Snowflake queries, storage, and compute resources for performance and cost-efficiency.\nCollaborate with cross-functional teams to gather data requirements and deliver data solutions.\n\n\nRequired Qualifications:\n5+ years of experience as a Data Engineer, with at least 4 years working with Snowflake.\nProficient with DBT (Data Build Tool) including Jinja templating, macros, and model dependency management.\nStrong understanding of ELT patterns and modern data stack principles.\nAdvanced SQL skills and experience with performance tuning in Snowflake.\n\n\n\n\nInterested candidates share your CV at himani.girnar@alikethoughts.com with below details\n\nCandidate's name-\nEmail and Alternate Email ID-\nContact and Alternate Contact no-\nTotal exp-\nRelevant experience-\nCurrent Org-\nNotice period-\nCCTC-\nECTC-\nCurrent Location-\nPreferred Location-\nPancard No-",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data build Tool', 'Snowflake', 'DBT', 'SQL', 'Etl Pipelines', 'Data Engineering', 'Advance Sql']",2025-06-11 06:03:38
Scala developer (with Spark),Fortune India 500 IT Services Firm,5 - 8 years,Not Disclosed,"['Hyderabad', 'Pune']","We are looking for a highly skilled Scala Developer with solid experience in Apache Spark to join our data engineering team.\n\nExperience- 5 to 8yrs\nLocation- Pune, Hyderabad\nMandatory skills- Scala development, Spark\nKey Responsibilities:\nDesign, develop, and optimize batch and streaming data pipelines using Scala and Apache Spark.\nWrite efficient, reusable, and testable code following functional programming best practices.\nWork with large-scale datasets from a variety of sources (e.g., Kafka, Hive, S3, Parquet).\nCollaborate with data scientists, data analysts, and DevOps to ensure robust and scalable pipelines.\nTune Spark jobs for performance and resource efficiency.\nImplement data quality checks, logging, and error-handling mechanisms.\n\n\n\nInterested candidates share your CV at himani.girnar@alikethoughts.com with below details\n\nCandidate's name-\nEmail and Alternate Email ID-\nContact and Alternate Contact no-\nTotal exp-\nRelevant experience-\nCurrent Org-\nNotice period-\nCCTC-\nECTC-\nCurrent Location-\nPreferred Location-\nPancard No-",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala Programming', 'Spark', 'SCALA', 'Big Data Technologies', 'SQL']",2025-06-11 06:03:40
Scala Developer,Inumellas Consultancy Services,4 - 9 years,20-35 Lacs P.A.,"['Pune', 'Mumbai (All Areas)', 'India']",Exp - 4 to 8 Yrs\nLocation - Pune (Relocation accepted)\n\nMUST HAVE - Hands-on experience of atleast 2 years latest with AKKA HTTPS & AKKA Framework along with strong Scala programming,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala Programming', 'SCALA', 'Akka Framework', 'Akka Http', 'Akka', 'Apache Storm', 'Spark Streaming', 'Devops']",2025-06-11 06:03:42
Palantir Developer,Algoleap Technologies,5 - 10 years,Not Disclosed,['Hyderabad'],"Work Responsibilities\n\nThe Palantir Developer will be responsible for designing and implementing modern data architecture solutions that facilitate enterprise-level transformation. Key responsibilities include:\n\nData Architecture Design: Create and optimize modern data architectures that support advanced analytics and operational requirements.\n\nPipelining: Develop and maintain efficient data pipelines using Palantir Foundry to ensure seamless data flow and accessibility for analytics.\n\nAdvanced Analytics: Create and deploy advanced analytics products that provide actionable insights to stakeholders, enhancing decision-making processes.\n\nArtificial Intelligence Integration: Collaborate with data scientists to incorporate AI and machine learning models into data pipelines and analytics products, enabling predictive capabilities.\n\nAgentic AI Exposure: Leverage knowledge of Agentic AI to develop systems that can autonomously make decisions and take actions based on data insights, enhancing operational capabilities.\n\nCollaboration: Work closely with cross-functional teams, including data scientists, engineers, and business analysts, to gather requirements and deliver tailored solutions.\n\nCloud Technologies: Utilize cloud-based tools and services to enhance scalability, security, and performance of data solutions.\n\nBest Practices: Implement best practices for data governance, quality, and security to maintain data integrity and compliance with relevant regulations.\n\nContinuous Improvement: Identify opportunities for process improvements and automation to enhance operational efficiency within data ecosystems.\n\nDocumentation: Maintain comprehensive documentation of data architecture designs, pipeline configurations, and analytics processes.\n\n\n\nArtificial Intelligence & Data Engineering: In this age of disruption, organizations need to embrace data-driven decision-making to\n\nDeliver Enterprise Value. Our Team Leverages Data, Analytics, Robotics, And Cognitive Technologies To Uncover Insights And Drive Transformation In Business. Key Initiatives Include\n\nData Ecosystem Implementation: Collaborate with clients to implement large-scale data ecosystems that integrate structured and unstructured data for comprehensive insights.\n\nPredictive Analytics: Utilize machine learning and predictive modeling techniques to derive actionable insights and predict future scenarios.\n\nAI Solutions Development: Work on developing AI-driven solutions that enhance data analytics capabilities, including natural language processing (NLP), computer vision, and recommendation systems.\n\nAgentic AI Development: Engage in projects that involve the development and deployment of Agentic AI systems capable of autonomous decision-making and action-taking based on real-time data.\n\nOperational Efficiency: Drive operational efficiency by utilizing automation and cognitive techniques for data management, ensuring timely and accurate reporting.\n\nClient Engagement: Engage with clients to understand their unique challenges and tailor solutions that align with their strategic objectives.\n\nInnovative Solutions: Research and implement innovative technologies and methodologies that enhance data analytics capabilities and drive business value.\n\nTraining and Support: Provide training and support to clients on data tools and platforms to ensure they can maximize the value of their data assets.\n\n\n\nRequired:\n\nEducation: Bachelors degree in Computer Science, Data Science, Engineering, or a related field.\n\n\n\n3+ years of hands-on experience in data extraction and manipulation using various tools and programming languages.\n\n3+ years of experience in engineering and developing Palantir pipelines, with a strong understanding of data integration techniques.\n\n3+ years of experience collaborating with Palantir Foundry data scientists and engineers on complex data projects.\n\n2+ years of experience working with AI and machine learning technologies, including model development, deployment, and performance tuning.\n\nFamiliarity with Agentic AI concepts and applications, including experience developing or working with autonomous systems is a plus.\n\nTechnical Skills: Proficiency in programming languages such as Python, SQL, or R, along with experience in statistical analysis and machine learning techniques.\n\nProblem-Solving: Strong analytical and problem-solving skills, with the ability to think critically and creatively.\n\nCommunication: Excellent interpersonal and communication skills to effectively convey technical concepts to non-technical stakeholders.",,,,"['Palantir Pipelines', 'Data Extraction', 'Agentic Ai', 'R', 'Artificial Intelligence', 'Palantir Foundry', 'Performance Tuning', 'manipulation', 'Machine Learning', 'Model Development', 'Python', 'SQL']",2025-06-11 06:03:43
AI Developer,Raymoon Enterprise,1 - 6 years,6-15 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Qualifications and Experience:\nExp. : 1+ year\nSalary : Upto 15LPA\nLocation : Gurgaon\nSkills : AI, ML, Python, SQL, NOSQL, AWS, GCP, Tensorflow, Database Management, Language R",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Tensorflow', 'Pytorch', 'Data Science', 'NoSQL', 'Database Management', 'Aiml', 'Machine Learning', 'Deep Learning', 'SQL', 'LANGUAGE R', 'Ml']",2025-06-11 06:03:45
SQL Developer,Ibasis,5 - 10 years,10-18 Lacs P.A.,['Hyderabad( Begumpet )'],"About the Role:\nSelf-driven Data Engineer/Developer, who is a fast-learner and can push forward to\nget the job done is a must.\nBachelor's Degree in Computer Science, Engineering, Mathematics or related\nexperience in IT/IS, Operations and DevOps environment.\n5+ years of recent working experience in IT/IS environment directly working with\nengineering teams.\n3+ years of recent working experience architecting and designing (architecture, design\npatterns, reliability and scaling) of new and current systems.\nKnowledge of professional software engineering practices and best practices for the\nfull software development life cycle, including coding standards, code reviews, source\ncontrol management, build processes, testing, and operations.\nWorking knowledge of software development methodologies like Agile, Scrum,\nKanban, or equivalent.\nExperience partnering with product and/or program management teams and servicing\ninternal customers.\nMust be adept at analyzing large-scale systems and clearly documenting design\nSolutions.\nGood interpersonal and communication skills, both written and oral, are a must.\nAbility to be both an Individual Contributor and Team Player when necessary.\nMust be very well organized and attentive to details.\nYoull Bring:\nStrong knowledge in UNIX/Linux Operating Systems, including experience.\nworking with Linux/Unix/Shell scripts.\nVery strong knowledge and working experience with SQL (Oracle, PostgreSQL)\nVery experienced hands-on ETL developer with one of the ETL tools below:\nExperienced ODI (Oracle Data Integrator) developer\nExperienced Pentaho Data Integrator (PDI) developer\nVery experienced in SAP BI building universes and complex BI reports – experience with Tableau dashboards is a plus.\nRecent working experience in ETL and Data Warehouse development.\nStrong analytical capability in trouble-shooting and debugging skills that involve a deep understanding of software, hardware, firmware and network layers/components.",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle SQL', 'SQL Development', 'Bi Reporting Tools', 'Tableau', 'ETL', 'SQL']",2025-06-11 06:03:46
Databricks Administrator - AWS /Azure,Numeric Technologies India,8 - 13 years,20-35 Lacs P.A.,[],"Databricks Administrator Azure/AWS | Remote | 6+ Years\n\nJob Description:\nWe are seeking an experienced Databricks Administrator with 6+ years of expertise in managing and optimizing Databricks environments. The ideal candidate should have hands-on experience with Azure/AWS Databricks, cluster management, security configurations, and performance optimization. This role requires close collaboration with data engineering and analytics teams to ensure smooth operations and scalability.\nKey Responsibilities:\nDeploy, configure, and manage Databricks workspaces, clusters, and jobs.\nMonitor and optimize Databricks performance, auto-scaling, and cost management.\nImplement security best practices, including role-based access control (RBAC) and encryption.\nManage Databricks integration with cloud storage (Azure Data Lake, S3, etc.) and other data services.\nAutomate infrastructure provisioning and management using Terraform, ARM templates, or CloudFormation.\nTroubleshoot Databricks runtime issues, job failures, and performance bottlenecks.\nSupport CI/CD pipelines for Databricks workloads and notebooks.\nCollaborate with data engineering teams to enhance ETL pipelines and data processing workflows.\nEnsure compliance with data governance policies and regulatory requirements.\nMaintain and upgrade Databricks versions and libraries as needed.\nRequired Skills & Qualifications:\n6+ years of experience as a Databricks Administrator or in a similar role.\nStrong knowledge of Azure/AWS Databricks and cloud computing platforms.\nHands-on experience with Databricks clusters, notebooks, libraries, and job scheduling.\nExpertise in Spark optimization, data caching, and performance tuning.\nProficiency in Python, Scala, or SQL for data processing.\nExperience with Terraform, ARM templates, or CloudFormation for infrastructure automation.\nFamiliarity with Git, DevOps, and CI/CD pipelines.\nStrong problem-solving skills and ability to troubleshoot Databricks-related issues.\nExcellent communication and stakeholder management skills.\nPreferred Qualifications:\nDatabricks certifications (e.g., Databricks Certified Associate/Professional).\nExperience in Delta Lake, Unity Catalog, and MLflow.\nKnowledge of Kubernetes, Docker, and containerized workloads.\nExperience with big data ecosystems (Hadoop, Apache Airflow, Kafka, etc.).\n\nEmail : Hrushikesh.akkala@numerictech.com\nPhone /Whatsapp : 9700111702\n\nFor immediate response and further opportunities, connect with me on LinkedIn: https://www.linkedin.com/in/hrushikesh-a-74a32126a/",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'S3', 'Ci Cd Pipeline', 'Aws Cloud', 'Notebook', 'scala', 'libraries', 'Azure Databricks', 'Arm Templates', 'SQL', 'Terraform', 'Job Scheduling', 'ARM', 'AWS', 'Python']",2025-06-11 06:03:48
Kafka Administrator,Top B2C/B2B Corporation specializing in ...,5 - 10 years,7-14 Lacs P.A.,"['Mumbai', 'Goregaon', 'Mumbai (All Areas)']","Opening for the Insurance Company.\n\n**Looking someone with 30 days notice period**\n\nLocation : Mumbai (Lower Parel)\n\nKey Responsibilities:\n\nKafka Infrastructure Management: Design, implement, and manage Kafka clusters to ensure high availability, scalability, and security. Monitor and maintain Kafka infrastructure, including topics, partitions, brokers, Zookeeper, and related components. Perform capacity planning and scaling of Kafka clusters based on application needs and growth.\nData Pipeline Development: Develop and optimize Kafka data pipelines to support real-time data streaming and processing. Collaborate with internal application development and data engineers to integrate Kafka with various HDFC Life data sources. Implement and maintain schema registry and serialization/deserialization protocols (e.g., Avro, Protobuf).\nSecurity and Compliance: Implement security best practices for Kafka clusters, including encryption, access control, and authentication mechanisms (e.g., Kerberos, SSL).\nDocumentation and Support: Create and maintain documentation for Kafka setup, configurations, and operational procedures.\nCollaboration: Provide technical support and guidance to application development teams regarding Kafka usage and best practices. Collaborate with stakeholders to ensure alignment with business objectives\n\n\n\nInterested candidates shared resume on snehal@topgearconsultants.com",Industry Type: Insurance,Department: Other,"Employment Type: Full Time, Permanent","['Confluent', 'Kstreams', 'Kafka', 'Kafka Cluster', 'Apache', 'Kafka Brokers', 'Kafka Streams']",2025-06-11 06:03:49
ETL_GCP_Bigquery,Tekskillsindia,6 - 8 years,Not Disclosed,"['Hyderabad', 'Pune']","Develop, implement, and optimize ETL/ELT pipelines for processing large datasets efficiently.\n• Work extensively with BigQuery for data processing, querying, and optimization.\n• Utilize Cloud Storage, Cloud Logging, Dataproc, and Pub/Sub for data ingestion, storage, and event-driven processing.\n• Perform performance tuning and testing of the ELT platform to ensure high efficiency and scalability.\n• Debug technical issues, perform root cause analysis, and provide solutions for production incidents.\n• Ensure data quality, accuracy, and integrity across data pipelines.\n• Collaborate with cross-functional teams to define technical requirements and deliver solutions.\n• Work independently on assigned tasks while maintaining high levels of productivity and efficiency.\nSkills Required:\n• Proficiency in SQL and PL/SQL for querying and manipulating data.\n• Experience in Python for data processing and automation.\n• Hands-on experience with Google Cloud Platform (GCP), particularly:\no BigQuery (must-have)\no Cloud Storage\no Cloud Logging\no Dataproc\no Pub/Sub\n• Experience with GitHub and CI/CD pipelines for automation and deployment.\n• Performance tuning and performance testing of ELT processes.\n• Strong analytical and debugging skills to resolve data and pipeline issues efficiently.\n• Self-motivated and able to work independently as an individual contributor.\n• Good understanding of data modeling, database design, and data warehousing concepts.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'ETL']",2025-06-11 06:03:51
Autosys Administrator,E2techspace,5 - 10 years,Not Disclosed,[],"Immediate Joiners only. This is a 100% remote role but strict background checks will be conducted on candidates to prevent any possibility of moonlighting. The client is based out of New York so candidate will be expected to work in EST hours.\n\nAbout the Company:\n\nOur client is a leading hedge fund with a global footprint, managing multi-billion-dollar portfolios across a diverse range of asset classes. Technology is a cornerstone of the funds investment strategy, operational infrastructure, and risk management framework. The firm is seeking an experienced Autosys Specialist to manage and optimize enterprise-level batch job scheduling critical to front-, middle-, and back-office operations.\n\nPosition Overview:\n\nAs an Autosys Specialist, you will be responsible for the design, implementation, monitoring, and maintenance of the Autosys job scheduling environment. You will work closely with infrastructure, DevOps, application support, and business stakeholders to ensure the timely and reliable execution of jobs that support trading, risk, reporting, and operational workflows.\n\nKey Responsibilities:\n\nAdminister and support the Autosys Workload Automation environment.\nDevelop and maintain job schedules, calendars, dependencies, and event-driven workflows.\nMonitor job performance and proactively resolve job failures or delays with minimal disruption to critical processes.\nWork with developers, data engineers, and business analysts to onboard and optimize batch jobs.\nProvide incident management and root cause analysis (RCA) for failed or delayed jobs.\nPerform patching, version upgrades, and configuration management of the Autosys environment.\nMaintain compliance with change management and audit processes.\nDevelop operational documentation and automate repetitive tasks using scripting languages (Shell, Python, Perl, etc.).\nImplement job control and alerting integration with monitoring systems (e.g., Splunk, AppDynamics, or similar).\nSupport DR (Disaster Recovery) testing and high availability configuration of Autosys infrastructure.\n\nRequired Qualifications:\n\n5+ years of experience as an Autosys Administrator/Scheduler in an enterprise environment.\nStrong understanding of job scheduling best practices, dependencies, and SLA management.\nExperience with UNIX/Linux environments, scripting (Shell, Bash, Python), and SQL.\nSolid knowledge of event-based and time-based scheduling models.\nExperience with job onboarding automation and REST APIs related to Autosys (e.g., WCC APIs).\nStrong problem-solving and debugging skills.\n\nPreferred Qualifications:\n\nExperience with cloud environments (AWS, Azure) and hybrid scheduling setups.\nExperience in financial services, preferably within hedge funds, investment banking, or asset management.\nFamiliarity with front- and middle-office workflows (e.g., trading, risk, compliance reporting).\nKnowledge of other schedulers (Control-M, UC4, Tidal) is a plus.\nExposure to CI/CD tools like Jenkins, GitLab, or similar pipelines.\nKnowledge of monitoring/logging tools (e.g., Splunk, Grafana).\nITIL Foundation certification or similar operational frameworks.",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AutoSys', 'Autosys Scheduler', 'Autosys R5']",2025-06-11 06:03:52
