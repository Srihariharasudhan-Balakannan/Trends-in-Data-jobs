title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Data Architect,Acesoft,6 - 10 years,19-22.5 Lacs P.A.,['Bengaluru'],"Hi all,\nWe are hiring fore the Data Architecture\nExperience: 6 - 9 years\nLocation: Bangalore\nNotice Period: Immediate - 15 Days\nSkills:\nData Architecture\nAzure Data Factory\nAzure Data Bricks\nAzure Cloud\nArchitecture\n\nIf you are interested drop your resume at mojesh.p@acesoftlabs.com\nCall: 9701971793",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Azure Data Factory', 'Architecture', 'Azure Databricks', 'Data Architecture', 'Azure Synapse', 'Data Modeling']",2025-06-12 14:22:11
Data Architect,CGI,8 - 12 years,15-30 Lacs P.A.,['Hyderabad'],"Job Title: Data Architect / Data Modeler\nExperience Level: 8+ Years\nLocation: Hyderabad\nJob Summary\nWe are seeking a highly experienced Data Architect to join our growing Data & Analytics team. This role demands a strategic thinker and technical expert who can design and build robust, scalable, and efficient data solutions. You will play a critical role in architecting end-to-end data pipelines, designing optimized data models, and delivering business-centric data infrastructure using cutting-edge technologies such as Python, PySpark, SQL, Snowflake, and/or Databricks.\nThe ideal candidate will have a deep understanding of data engineering best practices and a proven track record of enabling data-driven decision-making through innovative and scalable data solutions.\nKey Responsibilities\nArchitect & Design Scalable Data Pipelines\nLead the design and implementation of high-performance, scalable, and maintainable data pipelines that support batch and real-time processing.\nData Modeling & Data Architecture\nDesign and implement optimized data models and database schemas to support analytics, reporting, and machine learning use cases.\nCloud Data Platforms\nDevelop and manage modern cloud-based data architectures using platforms like Snowflake or Databricks, ensuring performance, security, and cost-efficiency.\nData Integration & ETL Development\nBuild robust ETL/ELT workflows to ingest, transform, and provision data from a variety of internal and external sources.\nCollaboration with Stakeholders\nWork closely with data analysts, data scientists, product managers, and business leaders to translate business requirements into technical specifications and data solutions.\nData Quality & Governance\nImplement and advocate for best practices in data quality, security, compliance, lineage, and governance.\nPerformance Optimization\nOptimize data storage and query performance using advanced SQL, partitioning, indexing, caching strategies, and compute resource tuning.\nMentorship & Best Practices\nProvide mentorship to junior engineers, establish coding standards, and contribute to the growth and maturity of the data engineering practice.\nRequired Qualifications\nBachelors or Masterâ€™s degree in Computer Science, Engineering, Data Science, or a related field.\n8+ years of experience in data engineering or related roles.\nStrong expertise in Python and PySpark for data processing and transformation.\nProficient in advanced SQL with a deep understanding of query optimization and performance tuning.\nHands-on experience with Snowflake and/or Databricks in a production environment.\nExperience in designing and implementing data warehouses and data lakes.\nSolid understanding of distributed computing frameworks, big data ecosystems, and modern data architecture patterns.\nExperience with CI/CD, version control systems (e.g., Git), and workflow orchestration tools (e.g., Airflow, dbt, etc.).\nStrong communication skills with the ability to clearly articulate technical concepts to non-technical stakeholders.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'sql', 'snowflake', 'Data Architecture']",2025-06-12 14:22:13
Data Architect,Ford,14 - 17 years,Not Disclosed,['Chennai'],"We are looking for Data Solution Architect to join FC India IT Architecture team. In this role, you will define analytics solutions and guide engineering teams to implement big data solutions on the cloud. Work involves migrating data from legacy on-prem warehouses to Google cloud data platform. This role will provide architecture assistance to data engineering teams in India, with key responsibility of supporting applications globally. This role will also drive business adoption of the new platform and sunset of legacy platforms.\nGoogle Professional Solution Architect certification.\n8+ years of relevant work experience in analytics application and data architecture, with deep understanding of cloud hosting concepts and implementations.\n5+ years experience in Data and Solution Architecture in analytics space. Solid knowledge of cloud data architecture, data modelling principles, and expertise in Data Modeling tools.\nExperience in migrating legacy analytics applications to Cloud platform and business adoption of these platforms to build insights and dashboards through deep knowledge of traditional and cloud Data Lake, Warehouse and Mart concepts.\nGood understanding of domain driven design and data mesh principles.\nExperience with designing, building, and deploying ML models to solve business challenges using Python/BQML/Vertex AI on GCP.\nKnowledge of enterprise frameworks and technologies. Strong in architecture design patterns, experience with secure interoperability standards and methods, architecture tolls and process.\nDeep understanding of traditional and cloud data warehouse environment, with hands on programming experience building data pipelines on cloud in a highly distributed and fault-tolerant manner. Experience using Dataflow, pub/sub, Kafka, Cloud run, cloud functions, Bigquery, Dataform, Dataplex , etc.\nStrong understanding on DevOps principles and practices, including continuous integration and deployment (CI/CD), automated testing & deployment pipelines.\nGood understanding of cloud security best practices and be familiar with different security tools and techniques like Identity and Access Management (IAM), Encryption, Network Security, etc. Strong understanding of microservices architecture.\nNice to Have\nBachelor s degree in Computer science/engineering, Data science or related field.\nStrong leadership, communication, interpersonal, organizing, and problem-solving skills\nGood presentation skills with ability to communicate architectural proposals to diverse audiences (user groups, stakeholders, and senior management).\nExperience in Banking and Financial Regulatory Reporting space.\nAbility to work on multiple projects in a fast paced & dynamic environment.\nExposure to multiple, diverse technologies, platforms, and processing environments.\nUtilize Google Cloud Platform & Data Services to modernize legacy applications.\nUnderstand technical business requirements and define architecture solutions that align to Ford Motor & Credit Companies Patterns and Standards.\nCollaborate and work with global architecture teams to define analytics cloud platform strategy and build Cloud analytics solutions within enterprise data factory.\nProvide Architecture leadership in design & delivery of new Unified data platform on GCP.\nUnderstand complex data structures in analytics space as well as interfacing application systems. Develop and maintain conceptual, logical & physical data models. Design and guide Product teams on Subject Areas and Data Marts to deliver integrated data solutions.\nProvide architectural guidance for optimal solutions considering regional Regulatory needs.\nProvide architecture assessments on technical solutions and make recommendations that meet business needs and align with architectural governance and standard.\nGuide teams through the enterprise architecture processes and advise teams on cloud-based design, development, and data mesh architecture.\nProvide advisory and technical consulting across all initiatives including PoCs, product evaluations and recommendations, security, architecture assessments, integration considerations, etc.\nLeverage cloud AI/ML Platforms to deliver business and technical requirements.",Industry Type: Auto Components,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Data modeling', 'Access management', 'Enterprise architecture', 'Consulting', 'Network security', 'Data structures', 'Analytics', 'Python']",2025-06-12 14:22:15
Snowflake Data Architect,Kasmo Digital,10 - 16 years,Not Disclosed,['Hyderabad'],"Required Skills & Qualifications:\n10-12 years of experience in data architecture, data warehousing, and cloud technologies.\nStrong expertise in Snowflake architecture, data modeling, and optimization.\nSolid hands-on experience with cloud platforms: AWS, Azure, and GCP.\nIn-depth knowledge of SQL, Python, PySpark, and related data engineering tools.\nExpertise in data modeling (both dimensional and normalized models).\nStrong experience with data integration, ETL processes, and pipeline development.\nCertification in Snowflake, AWS, Azure, or related cloud technologies.\nExperience working with large-scale data processing frameworks and platforms.\nExperience in data visualization tools and BI platforms (e.g., Tableau, Power BI).\nExperience in Agile methodologies and project management.\nStrong problem-solving skills with the ability to address complex technical challenges.\nExcellent communication skills and ability to work collaboratively with cross-functional teams.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Visualization', 'Data Modeling', 'Data Warehousing', 'SQL', 'Data Architecture', 'Python']",2025-06-12 14:22:17
Data Architect,Ford,8 - 13 years,Not Disclosed,['Chennai'],"We are looking for Data Solution Architect to join FC India IT Architecture team. In this role, you will define analytics solutions and guide engineering teams to implement big data solutions on the cloud. Work involves migrating data from legacy on-prem warehouses to Google cloud data platform. This role will provide architecture assistance to data engineering teams in India, with key responsibility of supporting applications globally. This role will also drive business adoption of the new platform and sunset of legacy platforms.\nGoogle Professional Solution Architect certification.\n8+ years of relevant work experience in analytics application and data architecture, with deep understanding of cloud hosting concepts and implementations.\n5+ years experience in Data and Solution Architecture in analytics space. Solid knowledge of cloud data architecture, data modelling principles, and expertise in Data Modeling tools.\nExperience in migrating legacy analytics applications to Cloud platform and business adoption of these platforms to build insights and dashboards through deep knowledge of traditional and cloud Data Lake, Warehouse and Mart concepts.\nGood understanding of domain driven design and data mesh principles.\nExperience with designing, building, and deploying ML models to solve business challenges using Python/BQML/Vertex AI on GCP.\nKnowledge of enterprise frameworks and technologies. Strong in architecture design patterns, experience with secure interoperability standards and methods, architecture tolls and process.\nDeep understanding of traditional and cloud data warehouse environment, with hands on programming experience building data pipelines on cloud in a highly distributed and fault-tolerant manner. Experience using Dataflow, pub/sub, Kafka, Cloud run, cloud functions, Bigquery, Dataform, Dataplex , etc.\nStrong understanding on DevOps principles and practices, including continuous integration and deployment (CI/CD), automated testing & deployment pipelines.\nGood understanding of cloud security best practices and be familiar with different security tools and techniques like Identity and Access Management (IAM), Encryption, Network Security, etc. Strong understanding of microservices architecture.\nNice to Have\nBachelor s degree in Computer science/engineering, Data science or related field.\nStrong leadership, communication, interpersonal, organizing, and problem-solving skills\nGood presentation skills with ability to communicate architectural proposals to diverse audiences (user groups, stakeholders, and senior management).\nExperience in Banking and Financial Regulatory Reporting space.\nAbility to work on multiple projects in a fast paced & dynamic environment.\nExposure to multiple, diverse technologies, platforms, and processing environments.\nUtilize Google Cloud Platform & Data Services to modernize legacy applications.\nUnderstand technical business requirements and define architecture solutions that align to Ford Motor & Credit Companies Patterns and Standards.\nCollaborate and work with global architecture teams to define analytics cloud platform strategy and build Cloud analytics solutions within enterprise data factory.\nProvide Architecture leadership in design & delivery of new Unified data platform on GCP.\nUnderstand complex data structures in analytics space as well as interfacing application systems. Develop and maintain conceptual, logical & physical data models. Design and guide Product teams on Subject Areas and Data Marts to deliver integrated data solutions.\nProvide architectural guidance for optimal solutions considering regional Regulatory needs.\nProvide architecture assessments on technical solutions and make recommendations that meet business needs and align with architectural governance and standard.\nGuide teams through the enterprise architecture processes and advise teams on cloud-based design, development, and data mesh architecture.\nProvide advisory and technical consulting across all initiatives including PoCs, product evaluations and recommendations, security, architecture assessments, integration considerations, etc.\nLeverage cloud AI/ML Platforms to deliver business and technical requirements.",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Data modeling', 'Access management', 'Enterprise architecture', 'Consulting', 'Network security', 'Data structures', 'Analytics', 'Python']",2025-06-12 14:22:19
Data Architect,Opus Technologies,10 - 16 years,35-50 Lacs P.A.,['Pune'],"Role & responsibilities\nDefine and evolve data engineering & analytics offerings aligned with payments domain needs (e.g., transaction analytics, fraud detection, customer insights).\nLead reference architecture creation for data modernization, real-time analytics, and cloud-native data platforms (e.g., Azure Synapse, GCP BigQuery).\nBuild reusable components, PoCs, and accelerators for ingestion, transformation, data quality, and governance.\nSupport pre-sales engagements with solution design, estimation, and client workshops.\nGuide delivery teams on data platform implementation, optimization, and security.\nMentor and upskill talent pool through learning paths and certifications.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Spark', 'Python', 'Azure Data Factory']",2025-06-12 14:22:22
Data Architect,"NTT DATA, Inc.",3 - 7 years,Not Disclosed,['Bengaluru'],"Additional Career Level Description\n\n\nKnowledge and application\nApplies advanced wide-ranging experience and in-depth professional knowledge to develop and resolve complex models and procedures in creative way .\nDirects the application of existing principles and guides development of new policies and ideas.\nDetermines own methods and procedures on new assignments .\n\n\n\nProblem solving\nUnderstands and works on complex issues where analysis of situation or data requires an in-depth evaluation of variable factors, solutions may need to be devised from limited informatio n.\nExercises judgment in selecting methods, evaluating, adapting of complex techniques and evaluation criteria for obtaining results.\n\n\n\nInteraction\nFrequently advises key people outside own area of expertise on complex matters, using persuasion in delivering messages.\n\n\n\nImpact\nDevelops and manages operational initiatives to deliver tactical results and achieve medium-term goals.\n\n\n\nAccountability\nMay be accountable through team for delivery of tactical business targets .\nWork is reviewed upon completion and is consistent with departmental objectives.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Architecture', 'data modeling', 'Data Architect', 'artificial intelligence', 'sql']",2025-06-12 14:22:24
Data Architect / Engagement Lead,Ignitho,7 - 10 years,Not Disclosed,['Chennai( Sholinganallur )'],"Job Title: Data Architect / Engagement Lead\nLocation: Chennai\nReports To: CEO\n\nAbout the Company:\nIgnitho Inc. is a leading AI and data engineering company with a global presence, including US, UK, India, and Costa Rica offices.\nVisit our website to learn more about our work and culture: www.ignitho.com.\nIgnitho is a portfolio company of Nuivio Ventures Inc., a venture builder dedicated to developing Enterprise AI product companies across various domains, including AI, Data Engineering, and IoT.\nLearn more about Nuivio at: www.nuivio.com.\n\nJob Summary:\nAs the Data Architect and Engagement Lead, you will define the data architecture strategy and lead client engagements, ensuring alignment between data solutions and business goals. This dual role blends technical leadership with client-facing responsibilities.\n\nKey Responsibilities:\nDesign scalable data architectures, including storage, processing, and integration layers.\nLead technical discovery and requirements gathering sessions with clients.\nProvide architectural oversight for data and AI solutions.\nAct as a liaison between technical teams and business stakeholders.\nDefine data governance, security, and compliance standards.\n\nRequired Qualifications:\nBachelors or Masters in computer science, Information Systems, or similar.\n7+ years of experience in data architecture, with client-facing experience.\nDeep knowledge of data modelling, cloud data platforms (Snowflake / BigQuery/ Redshift / Azure), and orchestration tools.\nExcellent communication, stakeholder management, and technical leadership skills.\nFamiliarity with AI/ML systems and their data requirements is a strong plus.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Data Modeling', 'Azure Cloud', 'Bigquery', 'Redshift Aws', 'Artificial Intelligence', 'Snowflake', 'Machine Learning']",2025-06-12 14:22:26
Data Architect,HARMAN,10 - 15 years,Not Disclosed,['Bengaluru'],"Introduction: HARMAN Technology Services (HTS)\n.\nCombine the physical and digital, making technology a more dynamic force to solve challenges and serve humanity s needs\nWork at the convergence of cross channel UX, cloud, insightful data, IoT and mobility\nEmpower companies to create new digital business models, enter new markets, and improve customer experiences",,,,"['Cloud computing', 'Data analysis', 'Project management', 'Manager Technology', 'Healthcare', 'Oracle', 'microsoft', 'RFP', 'Information technology', 'Automotive']",2025-06-12 14:22:28
Data Architect,"NTT DATA, Inc.",8 - 13 years,Not Disclosed,['Chennai'],"Req ID: 324664\n\nWe are currently seeking a Data Architect to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDevelop and articulate long-term strategic goals for data architecture vision and establish data standards for enterprise systems.\n\nUtilize various cloud technologies, including Azure, AWS, GCP, and data platforms like Databricks and Snowflake.\n\nConceptualize and create an end-to-end vision outlining the seamless flow of data through successive stages.\n\nInstitute processes for governing the identification, collection, and utilization of corporate metadata, ensuring accuracy and validity.\n\nImplement methods and procedures for tracking data quality, completeness, redundancy, compliance, and continuous improvement.\n\nEvaluate and determine governance, stewardship, and frameworks for effective data management across the enterprise.\n\nDevelop comprehensive strategies and plans for data capacity planning, data security, life cycle data management, scalability, backup, disaster recovery, business continuity, and archiving.\n\nIdentify potential areas for policy and procedure enhancements, initiating changes where required for optimal data management.\n\nFormulate and maintain data models and establish policies and procedures for functional design.\n\nOffer technical recommendations to senior managers and technical staff in the development and implementation of databases and documentation.\n\nStay informed about upgrades and emerging database technologies through continuous research.\n\nCollaborate with project managers and business leaders on all projects involving enterprise data.\n\nDocument the data architecture and environment to ensure a current and accurate understanding of the overall data landscape.\n\nDesign and implement data solutions tailored to meet customer needs and specific use cases.\n\nProvide thought leadership by recommending the most suitable technologies and solutions for various use cases, spanning from the application layer to infrastructure.\n\nBasic Qualifications:\n\n8+ years of hands-on experience with various database technologies\n\n6+ years of experience with Cloud-based systems and Enterprise Data Architecture, driving end-to end technology solutions.\n\nExperience with Azure, Databricks, Snowflake\n\nKnowledgeable on concepts of GenAI\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nPossess certifications in AWS, Azure, and GCP to complement extensive hands-on experience.\n\nDemonstrated expertise with certifications in Snowflake.\n\nValuable ""Big 4"" Management Consulting experience or exposure to multiple industries.\n\nUndergraduate or graduate degree preferred.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['enterprise information architecture', 'microsoft azure', 'gcp', 'database creation', 'aws', 'snowflake', 'data life cycle management', 'metadata', 'data management', 'data security', 'data warehousing', 'data architecture', 'sql', 'data bricks', 'data quality', 'database implementation']",2025-06-12 14:22:31
Data Architect,Coforge,11 - 16 years,Not Disclosed,"['Noida', 'Greater Noida', 'Delhi / NCR']","-Data Architect Department:\nData & Analytics The Data Architect having more than 14 years of experience and should play a pivotal role in designing, developing, and governing scalable data architectures to support enterprise-wide data integration, analytics, and reporting.\nThis role will focus on creating unified data models, optimizing data pipelines, and ensuring compliance with regulatory standards (GDPR) using cloud-based platforms.\nThe ideal candidate is a strategic thinker with deep expertise in data modeling, cloud data platforms, and governance.",,,,"['Data Migration', 'Data Warehousing', 'Data Modeling', 'Informatica', 'SSIS', 'ETL Tool']",2025-06-12 14:22:33
AWS Data Architect (Standard),Infogain,12 - 14 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nDesign and implement scalable, reliable, and high-performance data architectures to support business\nneeds.\nDevelop and maintain real-time data streaming solutions using Kafka and other streaming\ntechnologies.\nUtilize AWS cloud services to build and manage data infrastructure, ensuring security, performance,\nand cost optimization.\nCreate efficient and optimized data models for structured and unstructured datasets.\nDevelop, optimize, and maintain SQL queries for data processing, analysis, and reporting.\nWork with cross-functional teams to define data requirements and implement solutions that align with\nbusiness goals.\nImplement ETL/ELT pipelines using Python and other relevant tools.\nEnsure data quality, consistency, and governance across the organization.\nTroubleshoot and resolve issues related to data pipelines and infrastructure.\nRequired Skills and Qualifications:\nExperience in Data Engineering and Architecture.\nProficiency in Python for data processing and automation.\nStrong expertise in AWS (S3, Redshift, Glue, Lambda, EMR, etc.) for cloud-based data solutions.\nHands-on experience with Kafka for real-time data streaming.\nDeep understanding of data modeling principles for transactional and analytical workloads.\nStrong knowledge of SQL for querying and performance optimization.\nExperience in building and maintaining ETL/ELT pipelines.\nFamiliarity with big data technologies like Spark, Hadoop, or Snowflake is a plus.\nStrong problem-solving skills and ability to work in a fast-paced environment.\nExcellent communication and stakeholder management skills\nEXPERIENCE\n12-14 Years\nSKILLS\nPrimary Skill: Data Engineering\nSub Skill(s): Data Engineering\nAdditional Skill(s): Kafka, Python, Data Modeling, ETL, Data Architecture, SQL, Redshift, Pyspark",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data modeling', 'Analytical', 'Data processing', 'Data quality', 'Stakeholder management', 'AWS', 'SQL', 'Python', 'Data architecture']",2025-06-12 14:22:36
Data Architect,.,7 - 12 years,20-35 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Job Description\nWe are seeking a highly skilled Azure Data Engineer with strong expertise in Data Architecture, PySpark/Python, Azure Databricks, and data streaming solutions. The ideal candidate will have hands-on experience in designing and implementing large-scale data pipelines, along with solid knowledge of data governance and data modeling.\nKey Responsibilities\nDesign, develop, and optimize PySpark/Python-based data streaming jobs on Azure Databricks.\nBuild scalable and efficient data pipelines for batch and real-time processing.\nImplement data governance policies, ensuring data quality, security, and compliance.\nDevelop and maintain data models (dimensional, relational, NoSQL) to support analytics and reporting.\nCollaborate with cross-functional teams (data scientists, analysts, and business stakeholders) to deliver data solutions.\nTroubleshoot performance bottlenecks and optimize Spark jobs for efficiency.\nEnsure best practices in CI/CD, automation, and monitoring of data workflows.\nMentor junior engineers and lead technical discussions (for senior/managerial roles).\nMandatory Skills & Experience\n5+ years of relevant experience as a Data Engineer/Analyst/Architect (8+ years for Manager/Lead positions).\nExpert-level proficiency in PySpark/Python and Azure Databricks (must have worked on real production projects).\nStrong experience in building and optimizing streaming data pipelines (Kafka, Event Hubs, Delta Lake, etc.).\n4+ years of hands-on experience in data governance & data modeling (ER, star schema, data vault, etc.).\nIn-depth knowledge of Azure Data Factory, Synapse, ADLS, and SQL/NoSQL databases.\nExperience with Delta Lake, Databricks Workflows, and performance tuning.\nFamiliarity with data security, metadata management, and lineage tracking.\nExcellent communication skills (must be able to articulate technical concepts clearly).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure Databricks', 'Data Modeling', 'Data Governance', 'Python', 'ETL']",2025-06-12 14:22:38
GCP Data Architect (Standard),Infogain,12 - 14 years,Not Disclosed,['Bengaluru'],"Position Summary Experienced Senior Data Engineer utilizing Big Data & Gogle Cloud technologies to develop large scale, on-cloud data processing pipelines and data warehouses. What you ll do Consult customers across the world on their data engineering needs around Adobes Customer Data Platform. Support pre-sales discsusions around complex and large scale cloud, data engineering solutions. Design custom solutions on cloud integrating Adobes solutions in scalable and performant manner. Deliver complex, large scale, enterprise grade on-clould data engineer and integration solutions in hand-on manner. Good to have Experience of consulting India customers. Multi-cloud expertise preferable AWS and GCP\nEXPERIENCE\n12-14 Years\nSKILLS\nPrimary Skill: Data Engineering\nSub Skill(s): Data Engineering\nAdditional Skill(s): Python, BigQuery",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SUB', 'GCP', 'Data Architect', 'Consulting', 'Cloud', 'Presales', 'Data processing', 'Adobe', 'big data', 'Python']",2025-06-12 14:22:40
Data Architect - AWS,Happiest Minds Technologies,10 - 15 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Roles and responsibilities\nWork closely with the Product Owners and stake holders to design the Technical Architecture for data platform to meet the requirements of the proposed solution.\nWork with the leadership to set the standards for software engineering practices within the machine learning engineering team and support across other disciplines\nPlay an active role in leading team meetings and workshops with clients.\nChoose and use the right analytical libraries, programming languages, and frameworks for each task.",,,,"['SQL', 'data architect', 'Python', 'Pyspark', 'Apache Airflow', 'GLUE', 'Kinesis', 'Amazon Redshift', 'Data Architecture Principles', 'Data Modeling', 'Data Warehousing', 'Athena', 'Lambda', 'AWS']",2025-06-12 14:22:43
Data Architect,Calibo,12 - 16 years,Not Disclosed,[],"About the Role:\n\nWe are looking for a highly skilled Data Engineering Architect with strong Data Engineering pipeline implementation experience to serve as the lead Solution/Technical Architect and Subject Matter Expert for customer experience data solutions across multiple data sources. The ideal candidate will collaborate with the Enterprise Architect and the client IT team to establish and implement strategic initiatives.\n\nResponsibilities and Technical Skills:\n12+ years of relevant experience in designing and Architecting ETL, ELT, Reverse ETL, Data Management or Data Integration, Data Warehouse, Data Lake, and Data Migration.\nMust have expertise in building complex ETL pipelines and large Data Processing, Data Quality and Data security\nExperience in delivering quality work on time with multiple, competing priorities.\nExcellent troubleshooting and problem-solving skills must be able to consistently identify critical elements, variables and alternatives to develop solutions.\nExperience in identifying, analyzing and translating business requirements into conceptual, logical and physical data models in complex, multi-application environments.\nExperience with Agile and Scaled Agile Frameworks.\nExperience in identifying and documenting data integration issues, and challenges such as duplicate data, non-conformed data, and unclean data. Multiple platform development experience.\nStrong experience in performance tuning of ETL processes using Data Platforms\nMust have experience in handling Data formats like Delta Tables, Parquet files, Iceberg etc.\nExperience in Cloud technologies such as AWS/Azure or Google Cloud.\nApache Spark design and development experience using Scala, Java, Python or Data Frames with Resilient Distributed Datasets (RDDs).\nDevelopment experience in databases like Oracle, AWS Redshift, AWS RDS, Postgres Databricks and/or Snowflake.\nHands-on professional work experience with Python is highly desired.\nExperience in Hadoop ecosystem tools for real-time or batch data ingestion.\nStrong communication and teamwork skills to interface with development team members, business analysts, and project management. Excellent analytical skills.\nIdentification of data sources, internal and external, and defining a plan for data management as per business data strategy.\nCollaborating with cross-functional teams for the smooth functioning of the enterprise data system.\nManaging end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nPlanning and execution of big data solutions using Databricks, Big Data, Hadoop, Big Query, Snowflake, MongoDB, DynamoDB, PostgreSQL and SQL Server\nHands-on experience in defining and implementing various Machine Learning models for different business needs.\nIntegrating technical functionality, ensuring data accessibility, accuracy, and security.\nProgramming / Scripting Languages like Python / Java / Go, Microservices\nMachine Learning / AI tools like Scikit-learn / TensorFlow / PyTorch",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud', 'ETL', 'AWS', 'Data Handling', 'Spark']",2025-06-12 14:22:45
Data Architect (Data Bricks),Diacto Technologies Pvt Ltd,5 - 9 years,Not Disclosed,['Pune( Baner )'],"Job Overview:\nDiacto is seeking an experienced and highly skilled Data Architect to lead the design and development of scalable and efficient data solutions. The ideal candidate will have strong expertise in Azure Databricks, Snowflake (with DBT, GitHub, Airflow), and Google BigQuery. This is a full-time, on-site role based out of our Baner, Pune office.\n\nQualifications:\nB.E./B.Tech in Computer Science, IT, or related discipline\nMCS/MCA or equivalent preferred\n\nKey Responsibilities:\nDesign, build, and optimize robust data architecture frameworks for large-scale enterprise solutions\nArchitect and manage cloud-based data platforms using Azure Databricks, Snowflake, and BigQuery\nDefine and implement best practices for data modeling, integration, governance, and security\nCollaborate with engineering and analytics teams to ensure data solutions meet business needs\nLead development using tools such as DBT, Airflow, and GitHub for orchestration and version control\nTroubleshoot data issues and ensure system performance, reliability, and scalability\nGuide and mentor junior data engineers and developers\n\nExperience and Skills Required:\n5 to12 years of experience in data architecture, engineering, or analytics roles\nHands-on expertise in Databricks, especially Azure Databricks\nProficient in Snowflake, with working knowledge of DBT, Airflow, and GitHub\nExperience with Google BigQuery and cloud-native data processing workflows\nStrong knowledge of modern data architecture, data lakes, warehousing, and ETL pipelines\nExcellent problem-solving, communication, and analytical skills\n\nNice to Have:\nCertifications in Azure, Snowflake, or GCP\nExperience with containerization (Docker/Kubernetes)\nExposure to real-time data streaming and event-driven architecture\n\nWhy Join Diacto Technologies?\nCollaborate with experienced data professionals and work on high-impact projects\nExposure to a variety of industries and enterprise data ecosystems\nCompetitive compensation, learning opportunities, and an innovation-driven culture\nWork from our collaborative office space in Baner, Pune\nHow to Apply:\nOption 1 (Preferred)\n\nCopy and paste the following link on your browser and submit your application for the automated interview process: -\n\nhttps://app.candidhr.ai/app/candidate/gAAAAABoRrTQoMsfqaoNwTxsE_qwWYcpcRyYJk7NzSUmO3LKb6rM-8FcU58CUPYQKc65n66feHor-TGdCEfyouj0NmKdgYcNbA==/\n\nOption 2\n\n1. Please visit our website's career section at https://www.diacto.com/careers/\n2. Scroll down to the ""Who are we looking for?"" section\n3. Find the listing for "" Data Architect (Data Bricks)"" and\n4. Proceed with the virtual interview by clicking on ""Apply Now.""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Azure Databricks', 'Airflow', 'Etl Pipelines', 'Github', 'google BigQuery', 'DBT', 'Data Security', 'Data Modeling', 'Elt', 'Data Governance']",2025-06-12 14:22:48
Data Architect,Salesforce,10 - 15 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","The Data Excellence Data Architect is a demonstrated expert in technical and/or functional aspects of customer and partner engagements that lead to the successful delivery of data management projects. The Data Architect plays a critical role for setting customers up for success by prescriptively helping to shape and then execute in the Salesforce data space. This role also provides subject matter expertise related to the data management solutions and ensures successful project delivery. This includes helping identify and proactively manage risk areas, and ensuring issues are seen through to complete resolution as it relates to implementations. Will have the ability to configure and drive solutions to meet the customer s business and technical requirements. Additionally, this role will include helping align on the development of client-specific implementation proposals, SOWs, and staffing plans, engaging with SMEs across the organization to gain consensus on an acceptable proposal, developing best practices within the data excellence community, developing of shared assets.\n  Responsibilities\nServe as the Subject Matter Expert for Salesforce data excellence practice\nRecognized as a valuable and trusted advisor by our customers and other members of Salesforce community and continue to build a reputation for excellence in professional services\nLead development of multi-year Data platform capabilities roadmaps for internal business units like Marketing, Sales, Services, and Finance.\nFacilitate enterprise information & data strategy development, opportunity identification, business cases, technology adoption opportunities, operating model development, and innovation opportunities.\nMaximize value derived from data & analytics by leveraging data assets through data exploitation, envisioning data-enabled strategies as we'll as enabling business outcomes through analytics, data & analytics governance, and enterprise information policy.\nTranslating business requirements into technical specifications, including data streams, integrations, transformations, databases, and data warehouses\nDefining the data architecture framework, standards and principles, including modeling, metadata, security, reference data such as product codes and client categories, and master data such as clients, vendors, materials, and employees\nDefining data flows, ie, which parts of the organization generate data, which require data to function, how data flows are managed, and how data changes in transition\nDesign and implement effective data solutions and models to store and retrieve data from different data sources\nPrepare accurate dataset, architecture, and identity mapping design for execution and management purposes.\nExamine and identify data structural necessities by evaluating client operations, applications, and programming.\nResearch and properly evaluate new sources of information and new technologies to determine possible solutions and limitations in reliability or usability\nAssess data implementation procedures to ensure they comply with internal and external regulations.\nLead or participate in the architecture governance, compliance, and security activities (architectural reviews, technology sourcing) to ensure technology solutions are consistent with the target state architecture.\nPartner with stakeholders early in the project lifecycle to identify business, information, technical, and security architecture issues and act as a strategic consultant throughout the technology lifecycle.\nOversee the migration of data from legacy systems to new solutions.\nPreferred Qualifications and Skills:\nBA/BS degree or foreign equivalent\nOverall 10+ years of experience in Marketing data & Data management space.\nMinimum 1 year of hands-on full lifecycle CDP implementation experience on platforms like Salesforce CDP(formerly 360 Audiences), Tealium AudienceStream, Adobe AEP, Segment, Arm Treasure Data, BlueShift, SessionM, RedPoint, etc\n5+ years of experience with data management, data transformation, ETL, preferably using cloud-based tools/infrastructure\nExperience with Data architecture (ideally with marketing data) using batch and/or real-time ingestion\nRelevant Salesforce experience in Sales & Service Cloud as we'll as Marketing Cloud, related certifications is a plus (Marketing Cloud Consultant, Administrator, Advanced Administrator, Service Cloud Consultant, Sales Cloud Consultant, etc)\nExperience with Technologies and Processes for Marketing, Personalization, and Data Orchestration.\nExperience with master data management (MDM), data governance, data security, data quality and related tools desired.\nDemonstrate deep data integration and/or migration experience with Salesforce.com and other cloud-enabled tools\nDemonstrate expertise in complex SQL statements and RDBMS systems such as Oracle, Microsoft SQL Server, PostGres\nDemonstrate experience with complex coding through ETL tools such as Informatica, SSIS, Pentaho, and Talend\nKnowledge of Data Governance and Data Privacy concepts and regulations a plus\nRequired Skills\nAbility to work independently and be a self-starter\nComfort and ability to learn new technologies quickly & thoroughly\nSpecializes in gathering and analyzing information related to data integration, subscriber management, and identify resolution\nExcellent analytical & problem-solving skills\nDemonstrated ability to influence a group audience, facilitate solutions and lead discussions such as implementation methodology, Road-mapping, Enterprise Transformation strategy, and executive-level requirement gathering sessions\nTravel to client site (up to 50%)",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['metadata', 'RDBMS', 'Coding', 'Data quality', 'Informatica', 'Oracle', 'SSIS', 'Adobe', 'Pentaho', 'Salesforce']",2025-06-12 14:22:51
Data Architect Telecom Domain databrick BSS OSS,fast growing Data Driven IT solutions an...,10 - 20 years,45-55 Lacs P.A.,"['Noida', 'Hyderabad', 'Gurugram']","Data Architect Telecom Domain\nTo design comprehensive data architecture and technical solutions specifically for telecommunications industry challenges, leveraging TMforum frameworks and modern data platforms. To work closely with customers, and technology partners to deliver data solutions that address complex telecommunications business requirements including customer experience management, network optimization, revenue assurance, and digital transformation initiatives.\nResponsibilities:\nDesign and articulate enterprise-scale telecom data architectures incorporating TMforum standards and frameworks, including SID (Shared Information/Data Model), TAM (Telecom Application Map), and eTOM (enhanced Telecom Operations Map)\nDevelop comprehensive data models aligned with TMforum guidelines for telecommunications domains such as Customer, Product, Service, Resource, and Partner management\nCreate data architectures that support telecom-specific use cases including customer journey analytics, network performance optimization, fraud detection, and revenue assurance\nDesign solutions leveraging Microsoft Azure and Databricks for telecom data processing and analytics\nConduct technical discovery sessions with telecom clients to understand their OSS/BSS architecture, network analytics needs, customer experience requirements, and digital transformation objectives\nDesign and deliver proof of concepts (POCs) and technical demonstrations showcasing modern data platforms solving real-world telecommunications challenges\nCreate comprehensive architectural diagrams and implementation roadmaps for telecom data ecosystems spanning cloud, on-premises, and hybrid environments\nEvaluate and recommend appropriate big data technologies, cloud platforms, and processing frameworks based on telecom-specific requirements and regulatory compliance needs.\nDesign data governance frameworks compliant with telecom industry standards and regulatory requirements (GDPR, data localization, etc.)\nStay current with the latest advancements in data technologies including cloud services, data processing frameworks, and AI/ML capabilities\nContribute to the development of best practices, reference architectures, and reusable solution components for accelerating proposal development\nQualifications:\nBachelor's or Master's degree in Computer Science, Telecommunications Engineering, Data Science, or a related technical field\n10+ years of experience in data architecture, data engineering, or solution architecture roles with at least 5 years in telecommunications industry\nDeep knowledge of TMforum frameworks including SID (Shared Information/Data Model), eTOM, TAM, and their practical implementation in telecom data architectures\nDemonstrated ability to estimate project efforts, resource requirements, and implementation timelines for complex telecom data initiatives\nHands-on experience building data models and platforms aligned with TMforum standards and telecommunications business processes\nStrong understanding of telecom OSS/BSS systems, network management, customer experience management, and revenue management domains\nHands-on experience with data platforms including Databricks, and Microsoft Azure in telecommunications contexts\nExperience with modern data processing frameworks such as Apache Kafka, Spark and Airflow for real-time telecom data streaming\nProficiency in Azure cloud platform and its respective data services with an understanding of telecom-specific deployment requirements\nKnowledge of system monitoring and observability tools for telecommunications data infrastructure\nExperience implementing automated testing frameworks for telecom data platforms and pipelines\nFamiliarity with telecom data integration patterns, ETL/ELT processes, and data governance practices specific to telecommunications\nExperience designing and implementing data lakes, data warehouses, and machine learning pipelines for telecom use cases\nProficiency in programming languages commonly used in data processing (Python, Scala, SQL) with telecom domain applications\nUnderstanding of telecommunications regulatory requirements and data privacy compliance (GDPR, local data protection laws)\nExcellent communication and presentation skills with ability to explain complex technical concepts to telecom stakeholders\nStrong problem-solving skills and ability to think creatively to address telecommunications industry challenges\nGood to have TMforum certifications or telecommunications industry certifications\nRelevant data platform certifications such as Databricks, Azure Data Engineer are a plus\nWillingness to travel as required\nif you will all or most of the criteria contact bdm@intellisearchonline.net M 9341626895",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Telecom Bss', 'Data Architect', 'Telecom OSS', 'ETOM', 'Data Bricks']",2025-06-12 14:22:53
Data Architect - Supply Chain,Exxon Mobil Corporation,3 - 8 years,Not Disclosed,['Bengaluru'],"About us\nWe invite you to bring your ideas to ExxonMobil to help create sustainable solutions that improve quality of life and meet society s evolving needs. Learn more about our What and our Why and how we can work together .\nExxonMobil s affiliates in India\nExxonMobil s affiliates have offices in India in Bengaluru, Mumbai and the National Capital Region.\nExxonMobil s affiliates in India supporting the Product Solutions business engage in the marketing, sales and distribution of performance as well as specialty products across chemicals and lubricants businesses. The India planning teams are also embedded with global business units for business planning and analytics.",,,,"['ERP', 'SAP', 'Networking', 'Data management', 'Data modeling', 'XML', 'Agile', 'JSON', 'SQL', 'Python']",2025-06-12 14:22:55
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Bengaluru'],"Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 14:22:57
"Data Analyst, Staff",Qualcomm,4 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Miscellaneous Group, Miscellaneous Group > Data Analyst\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nAbout the Team\n\nQualcomm's People Analytics team plays a crucial role in transforming data into strategic workforce insights that drive HR and business decisions. As part of this lean but high-impact team, you will have the opportunity to analyze workforce trends, ensure data accuracy, and collaborate with key stakeholders to enhance our data ecosystem. This role is ideal for a generalist who thrives in a fast-paced, evolving environment""â€someone who can independently conduct data analyses, communicate insights effectively, and work cross-functionally to enhance our People Analytics infrastructure.\n\nWhy Join Us\n\n\nEnd-to-End ImpactWork on the full analytics cycle""â€from data extraction to insight generation""â€driving meaningful HR and business decisions.\n\n\nCollaboration at ScalePartner with HR leaders, IT, and other analysts to ensure seamless data integration and analytics excellence.\n\n\nData-Driven CultureBe a key player in refining our data lake, ensuring data integrity, and influencing data governance efforts.\n\n\nProfessional GrowthGain exposure to multiple areas of people analytics, including analytics, storytelling, and stakeholder engagement.\n\n\nKey Responsibilities\n\n\nPeople Analytics & Insights\nAnalyze HR and workforce data to identify trends, generate insights, and provide recommendations to business and HR leaders.\nDevelop thoughtful insights to support ongoing HR and business decision-making.\nPresent findings in a clear and compelling way to stakeholders at various levels, including senior leadership.\n\n\nData Quality & Governance\nEnsure accuracy, consistency, and completeness of data when pulling from the data lake and other sources.\nIdentify and troubleshoot data inconsistencies, collaborating with IT and other teams to resolve issues.\nDocument and maintain data definitions, sources, and reporting standards to drive consistency across analytics initiatives.\n\n\nCollaboration & Stakeholder Management\nWork closely with other analysts on the team to align methodologies, share best practices, and enhance analytical capabilities.\nAct as a bridge between People Analytics, HR, and IT teams to define and communicate data requirements.\nPartner with IT and data engineering teams to improve data infrastructure and expand available datasets.\n\n\nQualifications\n\nRequired4-7 years experience in a People Analytics focused role\n\n\nAnalytical & Technical Skills\nStrong ability to analyze, interpret, and visualize HR and workforce data to drive insights.\nExperience working with large datasets and ensuring data integrity.\nProficiency in Excel and at least one data visualization tool (e.g., Tableau, Power BI).\n\n\nCommunication & Stakeholder Management\nAbility to communicate data insights effectively to both technical and non-technical audiences.\nStrong documentation skills to define and communicate data requirements clearly.\nExperience collaborating with cross-functional teams, including HR, IT, and business stakeholders.\n\n\nPreferred:\n\n\nTechnical Proficiency\nExperience with SQL, Python, or R for data manipulation and analysis.\nFamiliarity with HR systems (e.g., Workday) and cloud-based data platforms.\n\n\nPeople Analytics Expertise\nPrior experience in HR analytics, workforce planning, or related fields.\nUnderstanding of key HR metrics and workforce trends (e.g., turnover, engagement, diversity analytics).\n\n\nAdditional Information\nThis is an office-based position (4 days a week onsite) with possible locations that may include India and Mexico",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'people analytics', 'documentation', 'tableau', 'data integration tools', 'hiring', 'data warehousing', 'data architecture', 'sourcing', 'jquery', 'staffing', 'plsql', 'oracle 10g', 'java', 'etl tool', 'html', 'etl', 'mongodb', 'python', 'oracle', 'power bi', 'hrsd', 'r', 'node.js', 'hr analytics', 'angularjs']",2025-06-12 14:23:00
Data & Analytics Specialist,Hoffmann La Roche,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\n.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\nA healthier future drives us to innovate. Together, more than 100 000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. Our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our Diagnostics products. We empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-12 14:23:02
Data & Gen AI Specialist,Altimetrik,1 - 4 years,Not Disclosed,['Bengaluru'],"Job Title: Data & GenAI AWS Specialist\nExperience: 1-4 Years\nLocation: Bangalore\nMandatory Qualification: B.E./ B.Tech/ M.Tech/ MS from IIT or IISc ONLY\nJob Overview:\nWe are seeking a seasoned Data & GenAI Specialist with deep expertise in AWS Managed Services (PaaS) to join our innovative team. The ideal candidate will have extensive experience in designing sophisticated, scalable architectures for data pipelines and Generative AI (GenAI) solutions leveraging cloud services.",,,,"['Generative Ai', 'Cloud', 'Data Science', 'Open Source', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Snowflake', 'Machine Learning', 'AWS']",2025-06-12 14:23:04
Data & Analytics Specialist,Roche Diagnostics,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\nAt Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we ve become one of the world s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\n.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-12 14:23:07
Lead Data Management Analyst,Wells Fargo,5 - 10 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Lead Data Management Analyst. We believe in the power of working together because great ideas can come from anyone. Through collaboration, any employee can have an impact and make a difference for the entire company. Explore opportunities with us for a career in a supportive environment where you can learn and grow. This role requires a blend of technical expertise, analytical thinking, and strategic decision making to drive impactful insights.\nAt Wells Fargo, we are looking for talented people who will put our customers at the center of everything we do. We are seeking candidates who embrace diversity, equity and inclusion in a workplace where everyone feels valued and inspired. Help us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.",,,,"['Data Management', 'Hive', 'Power BI', 'DB2', 'SQL Server', 'Tableau', 'Oracle', 'Teradata', 'Analytics', 'Python', 'Business Analysis']",2025-06-12 14:23:09
Data migration Manager,Robert Bosch Engineering and Business Solutions Private Limited,5 - 9 years,Not Disclosed,['Hyderabad'],"10+ years of experience in SAP Data Migration with Minimum Migration Management experience of 2 migration S/4 HANA rollout projects\nGood experience in migration management including planning, tracking, reporting, technical migration consulting, migration cutover planning etc.\nExperience of migration with SAP BODS (past experience of at least 3 projects using SAP BODS)\nGood SAP process knowledge required for data migration to work with cross-functional teams in gathering migration requirements and migration rules\nDeep knowledge in understanding the business objects requirements, dependencies, and technical knowledge (focus on migration cockpit and BODS) to coordinate and support the technical migration team during the technical design and release\nShould be proficient in Business Analysis, Business Knowledge Technical Solution Design\nPrior customer-facing roles to ensure client management is mandatory\nExhibit effective communication, presentation, and people skills along with demonstrated experience working with cross-functional teams, including teams working on interfaces.\nShould be a good collaborator, leader to drive the migration team (servant leadership mindset)\nAdditional migration tool knowledge is an added advantage\nRoles and Responsibilities\nComplete coordination of migration project starting from preparation till go-live and stabilization\nDrive the Migration concept and understand the high-level process of data migration, objects and dependencies and owning Migration cutover planning\nProcess overview desirable\nProvide technical migration solutions and guidance to technical team where ever necessary\nClose engagement with IT experts / Functional consultants, Process experts in gathering migration requirement\nClose collaboration, support and value addition to Central Migration Manager as applicable\nWorking closely between the technical migration team and cross functional team during development phase, technical unit testing\nPreparation of migration planning and cutover during each phase\nPlanning, tracking reporting of status to project management periodically.\nComplete responsibility and ownership of technical deliverables\nSingle point of contact for technical data migration within the project",Industry Type: Automobile,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution design', 'Business objects', 'Data migration', 'SAP', 'Business analysis', 'Technical design', 'Project management', 'Consulting', 'Unit testing', 'Client management']",2025-06-12 14:23:12
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nAs part of the cybersecurity organization, In this vital role you will be responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The role sits at the intersection of data infrastructure and business insight delivery, requiring the Data Engineer to design and build robust data pipelines while also translating data into meaningful visualizations for stakeholders across the organization. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nBuild data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nDevelop and maintain interactive dashboards and reports using tools like Tableau, ensuring data accuracy and usability\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\n\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, GitLab, LucidChart, etc.\nHands-on experience with data visualization and dashboarding toolsTableau, Power BI, or similar is a plus\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\n\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\n\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\n\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data engineering', 'data analysis', 'data modeling', 'analysis tools', 'data warehousing', 'troubleshooting', 'data architecture', 'data integration', 'etl process']",2025-06-12 14:23:14
IN_Manager_Azure Data Engineer_Data Analytics_Advisory,PwC Service Delivery Center,5 - 10 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nMust have\nCandidates with minimum 5 years of relevant experience for 1012 years of total experience (Architect / Managerial level).\nDeep expertise with technologies such as Data factory, Data Bricks (Advanced), SQLDB (writing complex Stored Procedures), Synapse, Python scripting (mandatory), Pyspark scripting, Azure Analysis Services.\nMust be certified with DP 203 (Azure Data Engineer Associate), Databricks Certified Data Engineer Professional (Architect / Managerial level)\nStrong troubleshooting and debugging skills. Proven experience in working source control technologies (such as GITHUB, Azure DevOps), build and release pipelines.\nExperience in writing complex PySpark queries to perform data analysis.\nMandatory skill sets\nAzure Databricks, Pyspark, Datafactory\nPreferred skill sets\nAzure Databricks, Pyspark, Datafactory, Python, Azure Devops\nYears of experience required\n712yrs\nEducation qualification\nB.Tech / M.Tech / MBA / MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nMicrosoft Azure\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling {+ 32 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Data modeling', 'Debugging', 'Database administration', 'Agile', 'Stored procedures', 'Apache', 'Business intelligence', 'Troubleshooting', 'Python']",2025-06-12 14:23:17
Data Engineer III,Expedia Group,5 - 10 years,Not Disclosed,['Bengaluru'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nData Engineer III\nIntroduction to the Team\nExpedia Technology teams partner with our Product teams to create innovative products, services, and tools to deliver high-quality experiences for travelers, partners, and our employees. A singular technology platform powered by data and machine learning provides secure, differentiated, and personalized experiences that drive loyalty and traveler satisfaction.\nExpedia Group is seeking a skilled and motivated Data Engineer III to join our Finance Business Intelligence team supporting the Product & Technology Finance organization. In this role, you will help drive data infrastructure and analytics solutions that support strategic financial planning, reporting, and operational decision-making across the Global Finance community. You ll work closely with Finance and Technology partners to ensure data accuracy, accessibility, and usability in support of Expedia s business objectives.\nAs a Data Engineer III, you have strong experience working with a variety of datasets, data environments, tools, and analytical techniques. You enjoy a fun, collaborative and stimulating team environment. Successful candidates should be able to own projects end-to-end, including identifying problems and solutions, building and maintain data pipelines and dashboards, distilling key insights and communicate to stakeholders.\nIn this role, you will:\nDevelop new and improve existing end to end Business Intelligence products (data pipelines, Tableau dashboards, and Machine Learning predictive forecasting models).\nDrive internal efficiencies through streamline code/documentation/Tableau development to maintain high data integrity.\nTroubleshoot and resolve production issues with the team products (automation opportunities, optimizations, back-end data issues, data reconciliations).\nProactively reach out to subject matter experts /stakeholders and collaborate to solve problems.\nRespond to ad hoc data requests and conduct analysis to provide valuable insights to stakeholders.\nCollaborate and coordinate with team members/stakeholders to translate complex data into meaningful insights, that improve the analytical capabilities of the business.\nApply knowledge of database design to support migration of data pipelines from on prem to cloud environment (including data extraction, ingestion, processing of large data sets)\nSupport dashboard development on cloud environment to enable self-service reporting.\nCommunicate clearly on current work status and design considerations\nThink broadly and comprehend the how, why, and what behind data architecture designs\nExperience & Qualifications:\nBachelor s in Computer Science, Mathematics, Statistics, Information Systems, or related field\n5+ years experience in a Data Analyst, Data Engineer or Business Analyst role\nProven expertise in SQL, with practical experience utilizing query engines including SQL Server, Starburst, Trino, Querybook and data science tools such as Python/R, SparkSQL.\nProficient visualization skills (Tableau, Looker, or similar) and excel modeling/report automation.\nExceptional understanding of relational and dimensional datasets, data warehouse and data mining and applies database design principles to solve data requirements\nExperience building robust data extract, load and transform (ELT) processes, that source data from multiple databases.\nDemonstrated record of defining and executing key analysis and solving problems with minimal supervision.\nDynamic individual contributor who consistently enhances operational playbooks to address business problems.\n3+ year working in a hybrid environment that uses both on-premise and cloud technologies is preferred.\nExperience working in an environment that manipulates large datasets on the cloud platform preferred.\nBackground in analytics, finance or a comparable reporting and analytics role preferred.\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Database design', 'Machine learning', 'Business intelligence', 'Data mining', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-12 14:23:19
Data Engineer - Databricks,KPI Partners,3 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About KPI Partners.\nKPI Partners is a leading provider of data analytics solutions, dedicated to helping organizations transform data into actionable insights. Our innovative approach combines advanced technology with expert consulting, allowing businesses to leverage their data for improved performance and decision-making.\n\nJob Description.\nWe are seeking a skilled and motivated Data Engineer with experience in Databricks to join our dynamic team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and data processing solutions that support our analytics initiatives. You will collaborate closely with data scientists, analysts, and other engineers to ensure the consistent flow of high-quality data across our platforms.",,,,"['python', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'data pipeline', 'data architecture', 'data engineering', 'sql', 'data bricks', 'cloud', 'analytics', 'data quality', 'data modeling', 'gcp', 'teamwork', 'integration', 'aws', 'etl', 'programming', 'communication skills', 'etl scripts']",2025-06-12 14:23:22
Big Data Engineer - Hadoop,Info Origin Technologies Pvt Ltd,3 - 7 years,Not Disclosed,"['Hyderabad', 'Gurugram']","Role: Hadoop Data Engineer\nLocation: Gurgaon / Hyderabad\nWork Mode: Hybrid\nEmployment Type: Full-Time\nInterview Mode: First Video then In Person\nJob Description\nJob Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'Hadoop', 'Pyspark', 'Big Data', 'Python', 'SQL']",2025-06-12 14:23:24
Hadoop Data Engineer,Info Origin,0 - 2 years,Not Disclosed,['Gurugram'],"Job Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'Scalability', 'data security', 'spark', 'Hadoop', 'Data processing', 'big data', 'SQL', 'Python']",2025-06-12 14:23:26
Data Engineer,Centrilogic,15 - 20 years,Not Disclosed,['Hyderabad'],"Data Engineer\n\nPurpose:\n\nOver 15 years, we have become a premier global provider of multi-cloud management, cloud-native application development solutions, and strategic end-to-end digital transformation services.\nHeadquartered in Canada and with regional headquarters in the U.S. and the United Kingdom, Centrilogic delivers smart, streamlined solutions to clients worldwide.\n\nWe are looking for a passionate and experienced Data Engineer to work with our other 70 Software, Data and DevOps engineers to guide and assist our clients data modernization journey.\n\nOur team works with companies with ambitious missions - clients who are creating new, innovative products, often in uncharted markets. We work as embedded members and leaders of our clients development and data teams. We bring experienced senior engineers, leading-edge technologies and mindsets, and creative thinking. We show our clients how to move to the modern frameworks of data infrastructures and processing, and we help them reach their full potential with the power of data.\n\nIn this role, youll be the day-to-day primary point of contact with our clients to modernize their data infrastructures, architecture, and pipelines.\n\nPrincipal Responsibilities:\n\nConsulting clients on cloud-first strategies for core bet-the-company data initiatives\nProviding thought leadership on both process and technical matters\nBecoming a real champion and trusted advisor to our clients on all facets of Data Engineering\nDesigning, developing, deploying, and supporting the modernization and transformation of our client s end-to-end data strategy, including infrastructure, collection, transmission, processing, and analytics\nMentoring and educating clients teams to keep them up to speed with the latest approaches, tools and skills, and setting them up for continued success post-delivery\n\nRequired Experience and Skills:\n\nMust have either Microsoft Certified Azure Data Engineer Associate or Fabric Data Engineer Associate certification.\nMust have experience working in a consulting or contracting capacity on large data management and modernization programs.\nExperience with SQL Servers, data engineering, on platforms such as Azure Data Factory, Databricks, Data Lake, and Synapse.\nStrong knowledge and demonstrated experience with Delta Lake and Lakehouse Architecture.\nStrong knowledge of securing Azure environment, such as RBAC, Key Vault, and Azure Security Center.\nStrong knowledge of Kafka and Spark and extensive experience using them in a production environment.\nStrong and demonstrable experience as DBA in large-scale MS SQL environments deployed in Azure.\nStrong problem-solving skills, with the ability to get to the route of an issue quickly.\nStrong knowledge of Scala or Python.\nStrong knowledge of Linux administration and networking.\nScripting skills and Infrastructure as Code (IaC) experience using PowerShell, Bash, and ARM templates.\nUnderstanding of security and corporate governance issues related with cloud-first data architecture, as well as accepted industry solutions.\nExperience in enabling continuous delivery for development teams using scripted cloud provisioning and automated tooling.\nExperience working with Agile development methodology that is fit for purpose.\nSound business judgment and demonstrated leadership",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['MS SQL', 'Networking', 'Data management', 'Powershell', 'Consulting', 'Application development', 'microsoft', 'Analytics', 'Python', 'Data architecture']",2025-06-12 14:23:28
Azure Data Engineer,HTC Global Services,4 - 8 years,Not Disclosed,['Bengaluru( Murugeshpalya )'],"Job Summary:\nWe are looking for a highly skilled Azure Data Engineer with experience in building and managing scalable data pipelines using Azure Data Factory, Synapse, and Databricks. The ideal candidate should be proficient in big data tools and Azure services, with strong programming knowledge and a solid understanding of data architecture and cloud platforms.\n\nKey Responsibilities:",,,,"['Power Bi', 'Azure Databricks', 'Azure Data Factory', 'Synapse', 'Python', 'Java', 'Scala', 'Kafka', 'big data tools', 'SQL', 'EventHub', 'Azure cloud services', 'Spark']",2025-06-12 14:23:31
Microsoft Fabrics Data Engineer,Swits Digital,5 - 10 years,Not Disclosed,['Bengaluru'],"Job TItle: Microsoft Fabric Data Engineer\nLocation: Bangalore\nJob Type: Conract (24 Months)\nJob Description:\nWe are seeking a highly skilled and experienced Microsoft Fabric Data Engineer/Architect to design, develop, and maintain robust, scalable, and secure data solutions within the Microsoft Fabric ecosystem. This role will leverage the full suite of Microsoft Azure data services, including Azure Data Bricks, Azure Data Factory, and Azure Data Lake, to build end-to-end data pipelines, data warehouses, and data lakehouses that enable advanced analytics and business intelligence.\nRequired Skills & Qualifications:\nBachelors degree in Computer Science, Engineering, or a related field.\n5+ years of experience in data architecture and engineering, with a strong focus on Microsoft Azure data platforms.\nProven hands-on expertise with Microsoft Fabric and its components, including:\nOneLake\nData Factory (Pipelines, Dataflows Gen2)\nSynapse Analytics (Data Warehousing, SQL analytics endpoint)\nLakehouses and Warehouses\nNotebooks (PySpark)\nExtensive experience with Azure Data Bricks, including Spark development (PySpark, Scala, SQL).\nStrong proficiency in Azure Data Factory for building and orchestrating ETL/ELT pipelines.\nDeep understanding and experience with Azure Data Lake Storage Gen2.\nProficiency in SQL (T-SQL, Spark SQL), Python, and/or other relevant scripting languages.\nSolid understanding of data warehousing concepts, dimensional modeling, and data lakehouse architectures.\nExperience with data governance principles and tools (e.g., Microsoft Purview).\nFamiliarity with CI/CD practices, version control (Git), and DevOps for data pipelines.\nExcellent problem-solving, analytical, and communication skills.\nAbility to work independently and collaboratively in a fast-paced, agile environment.\nPreferred Qualifications:\nMicrosoft certifications in Azure Data Engineering (e.g., DP-203, DP-600: Microsoft Fabric Analytics Engineer Associate).\nExperience with Power BI for data visualization and reporting.\nFamiliarity with real-time analytics and streaming data processing.\nExposure to machine learning workflows and integrating ML models with data solutions",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Analytical', 'microsoft azure', 'data visualization', 'microsoft', 'Business intelligence', 'Data warehousing', 'Analytics', 'Data architecture', 'Python']",2025-06-12 14:23:33
Big Data Engineer,Apexon,11 - 16 years,Not Disclosed,['Bengaluru'],"We enable #HumanFirstDigital\n\nJob Summary:\nWe are looking for a highly experienced and strategic Data Engineer to drive the design, development, and optimization of our enterprise data platform. This role requires deep technical expertise in AWS, StreamSets, and Snowflake, along with solid experience in Kubernetes, Apache Airflow, and unit testing. The ideal candidate will lead a team of data engineers and play a key role in delivering scalable, secure, and high-performance data solutions for both historical and incremental data loads.\nKey Responsibilities:\nLead the architecture, design, and implementation of end-to-end data pipelines using StreamSets and Snowflake.\nOversee the development of scalable ETL/ELT processes for historical data migration and incremental data ingestion.\nGuide the team in leveraging AWS services (S3, Lambda, Glue, IAM, etc.) to build cloud-native data solutions.\nProvide technical leadership in deploying and managing containerized applications using Kubernetes.\nDefine and implement workflow orchestration strategies using Apache Airflow.\nEstablish best practices for unit testing, code quality, and data validation.\nCollaborate with data architects, analysts, and business stakeholders to align data solutions with business goals.\nMentor junior engineers and foster a culture of continuous improvement and innovation.\nMonitor and optimize data workflows for performance, scalability, and cost-efficiency.\nRequired Skills & Qualifications:\nHigh proficiency in AWS, including hands-on experience with core services (S3, Lambda, Glue, IAM, CloudWatch).\nExpert-level experience with StreamSets, including Data Collector, Transformer, and Control Hub.\nStrong Snowflake expertise, including data modeling, SnowSQL, and performance tuning.\nMedium-level experience with Kubernetes, including container orchestration and deployment.\nWorking knowledge of Apache Airflow for workflow scheduling and monitoring.\nExperience with unit testing frameworks and practices in data engineering.\nProven experience in building and managing ETL pipelines for both batch and real-time data.\nStrong command of SQL and scripting languages such as Python or Shell.\nExperience with CI/CD pipelines and version control tools (e.g., Git, Jenkins).\nPreferred Qualifications:\nAWS certification (e.g., AWS Certified Data Analytics, Solutions Architect).\nExperience with data governance, security, and compliance frameworks.\nFamiliarity with Agile methodologies and tools like Jira and Confluence.\nPrior experience in a leadership or mentoring role within a data engineering team.\nOur Commitment to Diversity & Inclusion:\nOur Perks and Benefits:\nOur benefits and rewards program has been thoughtfully designed to recognize your skills and contributions, elevate your learning/upskilling experience and provide care and support for you and your loved ones. As an Apexon Associate, you get continuous skill-based development, opportunities for career advancement, and access to comprehensive health and well-being benefits and assistance.\nWe also offer:\no Group Health Insurance covering family of 4\no Term Insurance and Accident Insurance\no Paid Holidays & Earned Leaves\no Paid Parental LeaveoLearning & Career Development\no Employee Wellness\nJob Location : Bengaluru, India",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Agile', 'Wellness', 'Workflow', 'Healthcare', 'Unit testing', 'Apache', 'SQL', 'Python']",2025-06-12 14:23:35
Senior Data Engineer,Qualcomm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Data Engineer\n\nGeneral Summary:\n\nDeveloper will play an integral role in the PTEIT Machine Learning Data Engineering team. Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics. Design, develop and support CI/CD of data pipelines and services. - 5+ years of experience with Python or equivalent programming using OOPS, Data Structures and Algorithms - Develop new services in AWS using server-less and container-based services. - 3+ years of hands-on experience with AWS Suite of services (EC2, IAM, S3, CDK, Glue, Athena, Lambda, RedShift, Snowflake, RDS) - 3+ years of expertise in scheduling data flows using Apache Airflow - 3+ years of strong data modelling (Functional, Logical and Physical) and data architecture experience in Data Lake and/or Data Warehouse - 3+ years of experience with SQL databases - 3+ years of experience with CI/CD and DevOps using Jenkins - 3+ years of experience with Event driven architecture specially on Change Data Capture - 3+ years of Experience in Apache Spark, SQL, Redshift (or) Big Query (or) Snowflake, Databricks - Deep understanding building the efficient data pipelines with data observability, data quality, schema drift, alerting and monitoring. - Good understanding of the Data Catalogs, Data Governance, Compliance, Security, Data sharing - Experience in building the reusable services across the data processing systems. - Should have the ability to work and contribute beyond defined responsibilities - Excellent communication and inter-personal skills with deep problem-solving skills.\n\nMinimum Qualifications:\n3+ years of IT-related work experience with a Bachelor's degree in Computer Engineering, Computer Science, Information Systems or a related field.\nOR\n5+ years of IT-related work experience without a Bachelors degree.\n\n2+ years of any combination of academic or work experience with programming (e.g., Java, Python).\n1+ year of any combination of academic or work experience with SQL or NoSQL Databases.\n1+ year of any combination of academic or work experience with Data Structures and algorithms.\n5 years of Industry experience and minimum 3 years experience in Data Engineering development with highly reputed organizations- Proficiency in Python and AWS- Excellent problem-solving skills- Deep understanding of data structures and algorithms- Proven experience in building cloud native software preferably with AWS suit of services- Proven experience in design and develop data models using RDBMS (Oracle, MySQL, etc.)\n\nDesirable - Exposure or experience in other cloud platforms (Azure and GCP) - Experience working on internals of large-scale distributed systems and databases such as Hadoop, Spark - Working experience on Data Lakehouse platforms (One House, Databricks Lakehouse) - Working experience on Data Lakehouse File Formats (Delta Lake, Iceberg, Hudi)\n\nBachelor's or Master's degree in Computer Science, Software Engineering, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data quality', 'data structures', 'aws', 'schema', 'continuous integration', 'glue', 'amazon redshift', 'event driven architecture', 'ci/cd', 'data engineering', 'sql', 'alerts', 'java', 'data modeling', 'spark', 'devops', 'data flow', 'nosql databases', 'sql database']",2025-06-12 14:23:37
Data Migration Consultant,Excellerate Global Solutions,6 - 10 years,8-18 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Data migration SAP ABAP , S4 HANA , FSCM ,FICO",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Temporary/Contractual","['Sap Data Migration', 'SAP FICO', 'SAP ABAP', 'FSCM', 'Sap Hana', 'S4 Hana Finance']",2025-06-12 14:23:40
Data Engineer- MS Fabric,InfoCepts,5 - 9 years,Not Disclosed,['India'],"Position: Data Engineer â€“ MS Fabric\n  Purpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\n",,,,"['components', 'data', 'scala', 'delta', 'pyspark', 'data warehousing', 'rules', 'azure data factory', 'sql', 'parquet', 'analytics', 'sql azure', 'spark', 'oracle adf', 'data pipeline architecture', 'etl', 'python', 'azure synapse', 'microsoft azure', 'power bi', 'data bricks', 'data quality', 'system', 't', 'fabric', 'data integration', 'etl process']",2025-06-12 14:23:43
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Pune'],"ZSs Insights & Analytics group partners with clients to design and deliver solutions to help them tackle a broad range of business challenges. Our teams work on multiple projects simultaneously, leveraging advanced data analytics and problem-solving techniques. Our recommendations and solutions are based on rigorous research and analysis underpinned by deep expertise and thought leadership.\nWhat you'll Do\nDevelop advanced and efficient statistically effective algorithms that solve problems of high",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 14:23:45
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,"['Noida', 'Gurugram']","Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 14:23:47
Data Modelling,Tech Mahindra,7 - 12 years,Not Disclosed,['Chennai'],"Role: Data Modelling\nFulltime (Work from office Monday to Friday)\nLocation: Chennai\n\n\nMinimum Qualifications:\nBachelors Degree or experience in Engineering,\n5+ years of experience in data architecture or a related field, with a strong understanding of cloud-based data solutions\nProficiency in designing and implementing Medallion Architecture with bronze, silver, and gold layers\nExperienced curating and Erwin modeling data into Star Schemas\nStrong background in semantic modeling and creating meaningful, user-friendly data sets\nExperience with AWS, Synapse, and Power BI.\nWork visa sponsorship is not available for this position",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Warehouse', 'Data Modeling', 'ERwin', 'Azure Cloud', 'GCP', 'AWS']",2025-06-12 14:23:49
IT Manager - Data Engineering & Analytics,ZS,12 - 15 years,Not Disclosed,['Pune'],"IT MANAGER, DATA ENGINEERING AND ANALYTICS will lead a team of data engineers and analysts responsible for designing, developing, and maintaining robust data systems and integrations. This role is critical for ensuring the smooth collection, transformation, integration and visualization of data, making it easily accessible for analytics and decision-making across the organization. The Manager will collaborate closely with analysts, developers, business leaders and other stakeholders to ensure that the data infrastructure meets business needs and is scalable, reliable, and efficient.\n",,,,"['Data modeling', 'Project management', 'Analytical', 'Financial planning', 'Management consulting', 'Data quality', 'Troubleshooting', 'Stakeholder management', 'Analytics', 'SQL']",2025-06-12 14:23:51
Cloud Data Engineer - GCP,Synechron,2 - 3 years,Not Disclosed,"['Hyderabad', 'Gachibowli']","Job Summary\nSynechron is seeking a highly motivated and skilled Senior Cloud Data Engineer GCP to join our cloud solutions team. In this role, you will collaborate closely with clients and internal stakeholders to design, implement, and manage scalable, secure, and high-performance cloud-based data solutions on Google Cloud Platform (GCP). You will leverage your technical expertise to ensure the integrity, security, and efficiency of cloud data architectures, enabling the organization to derive maximum value from cloud data assets. This role contributes directly to our mission of delivering innovative digital transformation solutions and supports the organizations strategic objectives of scalable and sustainable cloud infrastructure.\nSoftware Requirements\nRequired Skills:\nProficiency with Google Cloud Platform (GCP) services (Compute Engine, Cloud Storage, BigQuery, Cloud Pub/Sub, Dataflow, etc.)\nBasic scripting skills with Python, Bash, or similar languages\nFamiliarity with virtualization and cloud networking concepts\nUnderstanding of cloud security best practices and compliance standards\nExperience with infrastructure as code tools (e.g., Terraform, Deployment Manager)\nStrong knowledge of data management, data pipelines, and ETL processes\nPreferred Skills:\nExperience with other cloud platforms (AWS, Azure)\nKnowledge of SQL and NoSQL databases\nFamiliarity with containerization (Docker, GKE)\nExperience with data visualization tools\nOverall Responsibilities\nDesign, implement, and operate cloud data solutions that are secure, scalable, and optimized for performance\nCollaborate with clients and internal teams to identify infrastructure and data architecture requirements\nManage and monitor cloud infrastructure and ensure operational reliability\nResolve technical issues related to cloud data workflows and storage solutions\nParticipate in project planning, timelines, and technical documentation\nContribute to best practices and continuous improvement initiatives within the organization\nEducate and support clients in adopting cloud data services and best practices\nTechnical Skills (By Category)\nProgramming Languages:\nEssential: Python, Bash scripts\nPreferred: SQL, Java, or other data processing languages\nDatabases & Data Management:\nEssential: BigQuery, Cloud SQL, Cloud Spanner, Cloud Storage\nPreferred: NoSQL databases like Firestore, MongoDB\nCloud Technologies:\nEssential: Google Cloud Platform core services (Compute, Storage, BigQuery, Dataflow, Pub/Sub)\nPreferred: Cloud monitoring, logging, and security tools\nFrameworks & Libraries:\nEssential: Data pipeline frameworks, Cloud SDKs, APIs\nPreferred: Apache Beam, Data Studio\nDevelopment Tools & Methodologies:\nEssential: Infrastructure as Code (Terraform, Deployment Manager)\nPreferred: CI/CD tools (Jenkins, Cloud Build)\nSecurity Protocols:\nEssential: IAM policies, data encryption, network security best practices\nPreferred: Compliance frameworks such as GDPR, HIPAA\nExperience Requirements\n2-3 years of experience in cloud data engineering, cloud infrastructure, or related roles\nHands-on experience with GCP is preferred; experience with AWS or Azure is a plus\nBackground in designing and managing cloud data pipelines, storage, and security solutions\nProven ability to deliver scalable data solutions in cloud environments\nExperience working with cross-functional teams on cloud deployments\nAlternative experience pathways: academic projects, certifications, or relevant internships demonstrating cloud data skills\nDay-to-Day Activities\nDevelop and deploy cloud data pipelines, databases, and analytics solutions\nCollaborate with clients and team members to plan and implement infrastructure architecture\nPerform routine monitoring, maintenance, and performance tuning of cloud data systems\nTroubleshoot technical issues affecting data workflows and resolve performance bottlenecks\nDocument system configurations, processes, and best practices\nEngage in continuous learning on new cloud features and data management tools\nParticipate in project meetings, code reviews, and knowledge sharing sessions\nQualifications\nBachelors or Masters degree in computer science, engineering, information technology, or a related field\nRelevant certifications (e.g., Google Cloud Professional Data Engineer, Cloud Architect) are preferred\nTraining in cloud security, data management, or infrastructure design is advantageous\nCommitment to professional development and staying updated with emerging cloud technologies\nProfessional Competencies\nCritical thinking and problem-solving skills to resolve complex cloud architecture challenges\nAbility to work collaboratively with multidisciplinary teams and clients\nStrong communication skills for technical documentation and stakeholder engagement\nAdaptability to evolving cloud technologies and project priorities\nOrganized with a focus on quality and detail-oriented delivery\nProactive learner with a passion for innovation in cloud data solutions\nAbility to manage multiple tasks effectively and prioritize in a fast-paced environment",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Jenkins', 'Java', 'NoSQL', 'Bash scripts', 'Data Studio', 'Data Management', 'CI/CD', 'Apache Beam', 'MongoDB', 'Cloud Build']",2025-06-12 14:23:53
"Sr. Solution Architect, Startups",Amazon,8 - 13 years,Not Disclosed,['Mumbai'],"Sales, Marketing and Global Services (SMGS)\nAWS Sales, Marketing, and Global Services (SMGS) is responsible for driving revenue, adoption, and growth from the largest and fastest growing smalland mid-market accounts to enterprise-level customers including public sector.\n\nDo you like startups? Are you interested in Cloud Computing & Generative AI? Yes? We have a role you might find interesting.\n\nStartups are the large enterprises of the future. These young companies are founded by ambitious people who have a desire to build something meaningful and to challenge the status quo. To address underserved customers, or to challenge incumbents. They usually operate in an environment of scarcity: whether that s capital, engineering resource, or experience. This is where you come in.\n\nThe Startup Solutions Architecture team is dedicated to working with these early stage startup companies as they build their businesses. We re here to make sure that they can deploy the best, most scalable, and most secure architectures possible and that they spend as little time and money as possible doing so.\n\nWe are looking for technical builders who love the idea of working with early stage startups to help them as they grow. In this role, you ll work directly with a variety of interesting customers and help them make the best (and sometimes the most pragmatic) technical decisions along the way. You ll have a chance to build enduring relationships with these companies and establish yourself as a trusted advisor.\n\nAs well as spending time working directly with customers, you ll also get plenty of time to sharpen the saw and keep your skills fresh. We have more than 175 services across a range of different categories and it s important that we can help startups take advantages of the right ones. You ll also play an important role as an advocate with our product teams to make sure we are building the right products for the startups you work with. And for the customers you don t get to work with on a 1:1 basis you ll get the chance to share your knowledge more broadly by working on technical content and presenting at events.\n\nA day in the life\nYou re surrounded by innovation. You re empowered with a lot of ownership. Your growth is accelerated. The work is challenging. You have a voice here and are encouraged to use it. Your experience and career development is in your hands. We live our leadership principles every day. At Amazon, its always ""Day 1"".\n\nDiverse Experiences\nAmazon values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn t followed a traditional path, or includes alternative experiences, don t let it stop you from applying.\n\nWhy AWS\nAmazon Web Services (AWS) is the world s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating that s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses.\n\nWork/Life Balance\nWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there s nothing we can t achieve in the cloud.\n\nInclusive Team Culture\nHere at AWS, it s in our nature to learn and be curious. Our employee-led affinity groups foster a culture of inclusion that empower us to be proud of our differences. Ongoing events and learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences, inspire us to never stop embracing our uniqueness.\n\nMentorship and Career Growth\nWe re continuously raising our performance bar as we strive to become Earth s Best Employer. That s why you ll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional.\n8+ years of specific technology domain areas (e.g. software development, cloud computing, systems engineering, infrastructure, security, networking, data & analytics) experience\n3+ years of design, implementation, or consulting in applications and infrastructures experience\n10+ years of IT development or implementation/consulting in the software or Internet industries experience\nExperience in a technical role within a sales organization 5+ years of infrastructure architecture, database architecture and networking experience\nKnowledge of cloud architecture\nExperience working with end user or developer communities",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database architecture', 'Cloud computing', 'Career development', 'Web services', 'Networking', 'Consulting', 'Manager Technology', 'Data analytics', 'infrastructure security', 'Senior Solution Architect']",2025-06-12 14:23:55
Data Architecture,Top B2B MNC in Management Consulting Dom...,5 - 8 years,Not Disclosed,['Bengaluru'],"About the Company\nGreetings from Teamware Solutions a division of Quantum Leap Consulting Pvt. Ltd\n\nAbout the Role\nWe are hiring a Data Architecture\n\nLocation: Bangalore\nWork Model: Hybrid\nExperience: 5-9 Years\nNotice Period: Immediate to 15 Days\n\nJob Description:\nData Architecture, Data Governance, Data Modeling\n\nAdditional Information:\nMandatory Skills: Data Architecture, Data Governance, Data Modeling\nNice to have skills Certification in Data Engineering\nInterview Mode Virtual Interview\nminimum 5 yrs relevant experience and maximum 9 yrs for this requirement. Someone with more experience in building PySpark data streaming jobs on Azure Databricks\nwho have done real projects, have expertise, and hands-on experience also\nAlso, Data governance and data modeling experience with a minimum of 4 years is mandatory\nCommunication should be excellent\n\n\nPlease let me know if you are interested in this position and send me your resumes to netra.s@twsol.com",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Architecture', 'Data Modeling', 'Data Governance', 'Data Engineering']",2025-06-12 14:23:57
"Senior Manager- Middle and Back Office Data Analyst- ISS,",Fidelity International,10 - 15 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Title: Middle and Back Office Data Analyst - ISS Data (Senior Manager)\nDepartment: Technology\nLocation: Bangalore & Gurgaon (hybrid / flexible working permitted)\nReports To: Middle and Back Office Data Product Owner\nLevel: Senior Manager\nWe re proud to have been helping our clients build better financial futures for over 50 years. How have we achieved this? By working together - and supporting each other - all over the world. So, join our [insert name of team/ business area] team and feel like you re part of something bigger.\nAbout your team\nThe Technology function provides IT services that are integral to running an efficient run-the business operating model and providing change-driven solutions to meet outcomes that deliver on our business strategy. These include the development and support of business applications that underpin our revenue, operational, compliance, finance, legal, marketing and customer service functions. The broader organisation incorporates Infrastructure services that the firm relies on to operate on a day-to-day basis including data centre, networks, proximity services, security, voice, incident management and remediation.\nThe ISS Technology group is responsible for providing Technology solutions to the Investment Solutions & Services (ISS) business (which covers Investment Management, Asset Management Operations & Distribution business units globally)\n\nThe ISS Technology team supports and enhances existing applications as well as designs, builds and procures new solutions to meet requirements and enable the evolving business strategy.\nAs part of this group, a dedicated ISS Data Programme team has been mobilised as a key foundational programme to support the execution of the overarching ISS strategy.\nAbout your role\nThe Middle and Back Office Data Analyst role is instrumental in the creation and execution of a future state design for Fund Servicing & Oversight data across Fidelity s key business areas. The successful candidate will have an in- depth knowledge of data domains that represent Middle and Back-office operations and technology.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned to deliver Fidelity s cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our clients.\nData Product - Requirements Definition and Delivery of Data Outcomes\nAnalysis of data product requirements to enable business outcomes, contributing to the data product roadmap\nCapture both functional and non-functional data requirements considering the data product and consumers perspectives.\nConduct workshops with both the business and tech stakeholders for requirements gathering, elicitation and walk throughs.\nResponsible for the definition of data requirements, epics and stories within the product backlog and providing analysis support throughout the SDLC.\nResponsible for supporting the UAT cycles, attaining business sign off on outcomes being delivered\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering principles.\nCoordination and Communication:\nExcellent communication skills to influence technology and business stakeholders globally, attaining alignment and sign off on the requirements.\nCoordinate with internal and external stakeholders to communicate data product deliveries and the change impact to the operating model.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\nAbout you\nAt least 10 years of proven experience as a business/technical/data analyst within technology and/or business changes within the financial services /asset management industry.\nMinimum 5 years as a senior business/technical/data analyst adhering to agile methodology, delivering data solutions using industry leading data platforms such as Snowflake, State Street Alpha Data, Refinitiv Eikon, SimCorp Dimension, BlackRock Aladdin, FactSet etc.\nProven experience. of delivering data driven business outcomes using industry leading data platforms such as Snowflake.\nExcellent knowledge of data life cycle that drives Middle and Back Office capabilities such as trade execution, matching, confirmation, trade settlement, record keeping, accounting, fund & cash positions, custody, collaterals/margin movements, corporate actions , derivations and calculations such as holiday handling, portfolio turnover rates, funds of funds look through .\nIn Depth expertise in data and calculations across the investment industry covering the below.\nAsset-specific data: This includes data related to financial instruments reference data like asset specifications, maintenance records, usage history, and depreciation schedules.\nMarket data: This includes data like security prices, exchange rates, index constituents and licensing restrictions on them.\nABOR & IBOR data: This includes calculation engines covering input data sets, calculations and treatment of various instruments for ABOR and IBOR data leveraging platforms such as Simcorp, Neoxam, Invest1, Charles River, Aladdin etc. Knowledge of TPAs, how data can be structured in a unified way from heterogenous structures.\nShould possess Problem Solving, Attention to detail, Critical thinking.\nTechnical Skills: Excellent hands-on SQL, Advanced Excel, Python, ML (optional) and proven experience and knowledge of data solutions.\nKnowledge of data management, data governance, and data engineering practices\nHands on experience on data modelling techniques such as dimensional, data vault etc.\nWillingness to own and drive things, collaboration across business and tech stakeholders.\nFeel rewarded",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['IT services', 'Data analysis', 'Data management', 'Incident management', 'Scrum', 'Customer service', 'Asset management', 'SDLC', 'SQL', 'Python']",2025-06-12 14:24:00
Lead AWS Glue Data Engineer,Allegis Group,8 - 13 years,Not Disclosed,[],"Lead AWS Glue Data Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\n\nWe are seeking a skilled Lead AWS Data Engineer with 8+ years of strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena.\nImplement data migration and transformation processes using AWS DMS and Glue.\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\n\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDS', 'Glue', 'DMS', 'Lambda', 'Redshift', 'Athena']",2025-06-12 14:24:02
Sr Data Engineer,Lowes Services India Private limited,5 - 10 years,Not Disclosed,['Bengaluru'],"We are seeking a seasoned Senior Data Engineer to join our Marketing Data Platform team. This role is pivotal in designing, building, and optimizing scalable data pipelines and infrastructure that support our marketing analytics and customer engagement strategies. The ideal candidate will have extensive experience with big data technologies, cloud platforms, and a strong understanding of marketing data dynamics.\n\nData Pipeline Development & Optimization\nDesign, develop, and maintain robust ETL/ELT pipelines using Apache PySpark on GCP services like Dataproc and Cloud Composer.\nEnsure data pipelines are scalable, efficient, and reliable to handle large volumes of marketing data.\nData Warehousing & Modeling\nImplement and manage data warehousing solutions using BigQuery, ensuring optimal performance and cost-efficiency.\nDevelop and maintain data models that support marketing analytics and reporting needs.\nCollaboration & Stakeholder Engagement\nWork closely with marketing analysts, data scientists, and cross-functional teams to understand data requirements and deliver solutions that drive business insights.\nTranslate complex business requirements into technical specifications and data architecture.\nData Quality & Governance\nImplement data quality checks and monitoring to ensure the accuracy and integrity of marketing data.\nAdhere to data governance policies and ensure compliance with data privacy regulations.\nContinuous Improvement & Innovation\nStay abreast of emerging technologies and industry trends in data engineering and marketing analytics.\nPropose and implement improvements to existing data processes and infrastructure\n  Years of Experience\n5 Years in Data Engineer space\n  Education Qualification & Certifications\nB.Tech or MCA\n  Experience\nProven experience with Apache PySpark, GCP (including Dataproc, BigQuery, Cloud Composer), and data pipeline orchestration.\nTechnical Skills\nProficiency in SQL and Python.\nExperience with data modeling, ETL/ELT processes, and data warehousing concepts.",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['orchestration', 'Data modeling', 'data governance', 'Data quality', 'Apache', 'Continuous improvement', 'Monitoring', 'SQL', 'Python', 'Data architecture']",2025-06-12 14:24:05
Data Engineer,Dun & Bradstreet,5 - 9 years,Not Disclosed,['Hyderabad'],"Key Responsibilities:\n1. Design, build, and deploy new data pipelines within our Big Data Eco-Systems using Streamsets/Talend/Informatica BDM etc. Document new/existing pipelines, Datasets.\n2. Design ETL/ELT data pipelines using StreamSets, Informatica or any other ETL processing engine. Familiarity with Data Pipelines, Data Lakes and modern Data Warehousing practices (virtual data warehouse, push down analytics etc.)\n3. Expert level programming skills on Python\n4. Expert level programming skills on Spark\n5. Cloud Based Infrastructure: GCP\n6. Experience with one of the ETL Informatica, StreamSets in creation of complex parallel loads, Cluster Batch Execution and dependency creation using Jobs/Topologies/Workflows etc.,\n7. Experience in SQL and conversion of SQL stored procedures into Informatica/StreamSets, Strong exposure working with web service origins/targets/processors/executors, XML/JSON Sources and Restful APIs.\n8. Strong exposure working with relation databases DB2, Oracle & SQL Server including complex SQL constructs and DDL generation.\n9. Exposure to Apache Airflow for scheduling jobs\n10. Strong knowledge of Big data Architecture (HDFS), Cluster installation, configuration, monitoring, cluster security, cluster resources management, maintenance, and performance tuning\n11. Create POCs to enable new workloads and technical capabilities on the Platform.\n12. Work with the platform and infrastructure engineers to implement these capabilities in production.\n13. Manage workloads and enable workload optimization including managing resource allocation and scheduling across multiple tenants to fulfill SLAs.\n14. Participate in planning activities, Data Science and perform activities to increase platform skills\n\nKey Requirements:\n1. Minimum 6 years of experience in ETL/ELT Technologies, preferably StreamSets/Informatica/Talend etc.,\n2. Minimum of 6 years hands-on experience with Big Data technologies e.g. Hadoop, Spark, Hive.\n3. Minimum 3+ years of experience on Spark\n4. Minimum 3 years of experience in Cloud environments, preferably GCP\n5. Minimum of 2 years working in a Big Data service delivery (or equivalent) roles focusing on the following disciplines:\n6. Any experience with NoSQL and Graph databases\n7. Informatica or StreamSets Data integration (ETL/ELT)\n8. Exposure to role and attribute based access controls\n9. Hands on experience with managing solutions deployed in the Cloud, preferably on GCP\n10. Experience working in a Global company, working in a DevOps model is a plus",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'GCP', 'Data engineer', 'Spark', 'ETL']",2025-06-12 14:24:07
Data Engineer - SAS Migration,Crisil,2 - 4 years,Not Disclosed,['Mumbai'],"The SAS to Databricks Migration Developer will be responsible for migrating existing SAS code, data processes, and workflows to the Databricks platform\n\nThis role requires expertise in both SAS and Databricks, with a focus on converting SAS logic into scalable PySpark and Python code\n\nThe developer will design, implement, and optimize data pipelines, ensuring seamless integration and functionality within the Databricks environment\n\nCollaboration with various teams is essential to understand data requirements and deliver solutions that meet business needs",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'scala', 'pyspark', 'data warehousing', 'data migration', 'azure data factory', 'sql', 'sql azure', 'java', 'spark', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'sas', 'microsoft azure', 'power bi', 'machine learning', 'sql server', 'data bricks', 'migration', 'sqoop', 'aws', 'ssis']",2025-06-12 14:24:11
Senior Data Engineer,The Main Stage Productions,4 - 6 years,Not Disclosed,['Bengaluru'],"Design and implement cloud-native data architectures on AWS, including data lakes, data warehouses, and streaming pipelines using services like S3, Glue, Redshift, Athena, EMR, Lake Formation, and Kinesis.\nDevelop and orchestrate ETL/ELT pipelines\n\nRequired Candidate profile\nParticipate in pre-sales and consulting activities such as:\nEngaging with clients to gather requirements and propose AWS-based data engineering solutions.\nSupporting RFPs/RFIs, technical proposals",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS Glue', 'GitHub Actions', 'PySpark', 'Scala', 'CodePipeline', 'Step Functions', 'data engineering']",2025-06-12 14:24:13
"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon",One of the largest insurance providers.,5 - 10 years,Not Disclosed,['Gurugram'],"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon\n\nSummary: An excellent opportunity for someone having a minimum of five years of experience with expertise in building data pipelines. A person must have experience in Python, Pyspark and AWS.\n\nLocation- Gurgaon (Hybrid)\n\nYour Future Employer- One of the largest insurance providers.\n\nResponsibilities-\nTo design, develop, and maintain large-scale data pipelines that can handle large datasets from multiple sources.\nReal-time data replication and batch processing of data using distributed computing platforms like Spark, Kafka, etc.\nTo optimize the performance of data processing jobs and ensure system scalability and reliability.\nTo collaborate with DevOps teams to manage infrastructure, including cloud environments like AWS.\nTo collaborate with data scientists, analysts, and business stakeholders to develop tools and platforms that enable advanced analytics and reporting.\n\nRequirements-\nHands-on experience with AWS services such as S3, DMS, Lambda, EMR, Glue, Redshift, RDS (Postgres) Athena, Kinesics, etc.\nExpertise in data modeling and knowledge of modern file and table formats.\nProficiency in programming languages such as Python, PySpark, and SQL/PLSQL for implementing data pipelines and ETL processes.\nExperience data architecting or deploying Cloud/Virtualization solutions (Like Data Lake, EDW, Mart ) in the enterprise.\nCloud/hybrid cloud (preferably AWS) solution for data strategy for Data lake, BI and Analytics.\nWhat is in for you-\nA stimulating working environment with equal employment opportunities.\nGrowing of skills while working with industry leaders and top brands.\nA meritocratic culture with great career progression.\n\nReach us- If you feel that you are the right fit for the role please share your updated CV at randhawa.harmeen@crescendogroup.in\n\nDisclaimer- Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Pipeline', 'AWS', 'Data Ingestion', 'Data Engineering', 'Data Processing']",2025-06-12 14:24:15
CBS & Data Migration - M/Sr.M - Navi Mumbai,Wikilabs India,7 - 12 years,Not Disclosed,['Navi Mumbai'],"Job Title: Manager / Senior Manager CBS & Data Migration\n\nLocation: Navi Mumbai\n\nExperience: 7-12 years\n\nIndustry: Banking\n\nJob Responsibilities:\nCore Banking System (CBS) Implementation & Management:\nLead end-to-end implementation and maintenance of Core Banking Systems (CBS).\nWork closely with vendors, IT teams, and business units to ensure smooth CBS operations.\nProvide technical support, troubleshooting, and system optimization.\nEnsure compliance with banking regulatory standards and security protocols.\n\nData Migration & Management:\nPlan, execute, and oversee large-scale data migration projects for banking platforms.\nEnsure data accuracy, integrity, and security during migration.\nCollaborate with cross-functional teams to define data mapping, transformation, and validation strategies.\nConduct post-migration audits and performance tuning.\n\nTechnical & Project Coordination:\nWork on SQL, Oracle, and other database management systems to support migration.\nHandle data extraction, transformation, and loading (ETL) processes.\nCollaborate with software vendors for CBS and data migration improvements.\nMonitor project timelines, risks, and deliverables.\n\nKey Skills Required:\nCBS Implementation & Support (Finacle, TCS BaNCS, Flexcube, Temenos, etc.)\nBanking Data Migration & Transformation\nETL Processes & Data Mapping\nSQL / Oracle Database Management\nRegulatory Compliance & Risk Management\nProject Management & Vendor Coordination\n\nPreferred Qualifications:\nB.Tech / B.E. / MCA / M.Sc. (IT) or equivalent\nExperience in Banking, Fintech, or IT Services\n\n\nImmediate joiners or candidates with a notice period of 30 days preferred.",Industry Type: Banking,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['CBS Implementation', 'Data Migration', 'Flexcube', 'Finacle', 'Data Mapping', 'CBS Migration', 'Banking IT', 'Core Banking System Migration']",2025-06-12 14:24:17
Data Engineer,Grid Dynamics,4 - 9 years,Not Disclosed,['Bengaluru'],"Required Qualifications:\n4+ years of professional experience in data engineering and data analysis roles.\nStrong proficiency in SQL and experience with database management systems such as MySQL, PostgreSQL, Oracle, and MongoDB.\nHands-on experience with big data tools like Hadoop and Apache Spark.\nProficient in Python programming.\nExperience with data visualization tools such as Tableau, Power BI, and Jupyter Notebooks.\nProven ability to design, build, and maintain scalable ETL pipelines using tools like Apache Airflow, DBT, Composer (GCP), Control-M, Cron, and Luigi.\nFamiliarity with data engineering tools including Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nExperience working with cloud data warehouses and services (Snowflake, Redshift, BigQuery, AWS Glue, GCP Dataflow, Azure Data Factory).\nUnderstanding of data modeling concepts and data lake/data warehouse architectures.\nExperience supporting CI/CD practices with Git, Docker, Terraform, and DevOps workflows.\nKnowledge of both relational and NoSQL databases, including PostgreSQL, BigQuery, MongoDB, and DynamoDB.\nExposure to Agile and DevOps methodologies.\nExperience with at least one cloud platform:\nGoogle Cloud Platform (BigQuery, Dataflow, Composer, Cloud Storage, Pub/Sub)\nAmazon Web Services (S3, Glue, Redshift, Lambda, Athena)\nMicrosoft Azure (Data Factory, Synapse Analytics, Blob Storage)\nEssential functions\nKey Responsibilities:\nDesign, develop, and maintain robust, scalable ETL pipelines using Apache Airflow, DBT, Composer (GCP), Control-M, Cron, Luigi, and similar tools.\nBuild and optimize data architectures including data lakes and data warehouses.\nIntegrate data from multiple sources ensuring data quality and consistency.\nCollaborate with data scientists, analysts, and stakeholders to translate business requirements into technical solutions.\nAnalyze complex datasets to identify trends, generate actionable insights, and support decision-making.\nDevelop and maintain dashboards and reports using Tableau, Power BI, and Jupyter Notebooks for visualization and pipeline validation.\nManage and optimize relational and NoSQL databases such as MySQL, PostgreSQL, Oracle, MongoDB, and DynamoDB.\nWork with big data tools and frameworks including Hadoop, Spark, Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nUtilize cloud data services and warehouses like AWS Glue, GCP Dataflow, Azure Data Factory, Snowflake, Redshift, and BigQuery.\nSupport CI/CD pipelines and DevOps workflows using Git, Docker, Terraform, and related tools.\nEnsure data governance, security, and compliance standards are met.\nParticipate in Agile and DevOps processes to enhance data engineering workflows.\nQualifications\nData Engineer with experience in MySQL or SQL or PL/SQL and any cloud experience like GCP or AWS or Azure\nWould be a plus\nPreferred Skills:\nStrong problem-solving and communication skills.\nAbility to work independently and collaboratively in a team environment.\nExperience with service development, REST APIs, and automation testing is a plus.\nFamiliarity with version control systems and workflow automation.\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Automation', 'Data modeling', 'MySQL', 'Workflow', 'Informatica', 'Oracle', 'Apache', 'SSIS', 'Analytics']",2025-06-12 14:24:20
Senior Data Engineer,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nOR\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Git', 'PySpark', 'CI/CD', 'Databricks', 'ETL', 'NOSQL', 'AWS', 'data integration', 'SQL', 'Apache Spark', 'Python']",2025-06-12 14:24:22
Oracle BRM Data Migration Engineer,Techstar Group,5 - 10 years,Not Disclosed,"['Noida', 'Hyderabad', 'Bengaluru']","Work Location : Hyderabad, Bangalore, Noida, Pune\nQualifications and Skills :\n- Proven expertise in Oracle BRM (Mandatory skill) with a strong understanding of its architecture and modules to effectively manage data migration processes.\n\n- Hands-on experience in data migration activities, particularly with Oracle BRM, ensuring high efficiency and accuracy throughout migration projects.\n\n- Knowledge in SQL for querying and managing databases, crucial for data migration and integration tasks.\n\n- Strong knowledge of ETL tools and processes for efficient data extraction, transformation, and loading from various sources.\n\n- Ability to perform detailed data mapping, ensuring logical transformation and compatibility between source and target system data structures.\n\n- Experience in data cleansing techniques to ensure data integrity and consistency throughout the migration process.\n\n- Understanding of data quality principles and practices, essential to maintain high standards of data accuracy and dependability.\n\n- Proficiency in scripting for automation of data migration tasks, enhancing efficiency and reducing potential for errors.\n\n- Excellent analytical and problem-solving skills to identify and address data-related challenges and opportunities.\n\n- Handling the execution of the data migration and validations.\n\n- Handle the develop Migration strategy documents and techniques. Execute data integrity testing post migration.\n\n- Understanding BRM : Having a working knowledge of BRM data migration components, the BRM 12 schema, and the data model\n\n- Data migration strategy : Developing a migration strategy and implementation plan\n\n- Data loading : Being able to load data and integrate it with systems\n\n- Post-migration analysis : Performing post-migration analysis on events, invoices, open items, bills, and dunning\n\n- Data reconciliation : Developing scripts to reconcile migrated data\n\n- Working Knowledge of all the BRM Data migration components.\n\n- Must have hands-on in BRM to verify the sanity of the Data migration.\n\n- Advantage - Programming skills on Java technologies. Exp. in C/C++, Oracle 12c/19c, PL/SQL, PCM Java, BRM Webservice, Scripting language (perl/python)\n\nRoles and Responsibilities :\n\n- Analyze client data and formulating effective data migration plans tailored to Oracle BRM specifications.\n\n- Collaborate with cross-functional teams to gather and interpret data migration requirements accurately.\n\n- Develop and implement efficient data migration scripts and processes, ensuring minimal disruption to business operations.\n\n- Conduct thorough testing and validation of data migration outputs to guarantee data accuracy and conformity.\n\n- Monitor and troubleshoot migration activities to ensure seamless execution and rectify any issues promptly.\n\n- Document data migration processes, maps, and transformations for knowledge sharing and continuous improvement.\n\n- Liaise with stakeholders to present progress updates and discuss ongoing improvements to data migration practices.\n\n- Contribute to the development of data migration best practices and reusable frameworks within the organization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle BRM', 'Java', 'Data Quality', 'Oracle Apps', 'C++', 'PL/SQL', 'Data Migration', 'Data reconciliation', 'ETL', 'SQL']",2025-06-12 14:24:24
Senior Data Engineer,Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data engineering', 'performance tuning', 'data security', 'data processing', 'Hadoop', 'Apache Spark', 'SQL', 'CI/CD', 'troubleshooting', 'big data', 'aws', 'ETL', 'Python']",2025-06-12 14:24:27
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nRole Description:\nWe are seeking a Senior Data Engineer with expertise in Graph Data technologies to join our data engineering team and contribute to the development of scalable, high-performance data pipelines and advanced data models that power next-generation applications and analytics. This role combines core data engineering skills with specialized knowledge in graph data structures, graph databases, and relationship-centric data modeling, enabling the organization to leverage connected data for deep insights, pattern detection, and advanced analytics use cases. The ideal candidate will have a strong background in data architecture, big data processing, and Graph technologies and will work closely with data scientists, analysts, architects, and business stakeholders to design and deliver graph-based data engineering solutions.\nRoles & Responsibilities:\nDesign, build, and maintain robust data pipelines using Databricks (Spark, Delta Lake, PySpark) for complex graph data processing workflows.\nOwn the implementation of graph-based data models, capturing complex relationships and hierarchies across domains.\nBuild and optimize Graph Databases such as Stardog, Neo4j, Marklogic or similar to support query performance, scalability, and reliability.\nImplement graph query logic using SPARQL, Cypher, Gremlin, or GSQL, depending on platform requirements.\nCollaborate with data architects to integrate graph data with existing data lakes, warehouses, and lakehouse architectures.\nWork closely with data scientists and analysts to enable graph analytics, link analysis, recommendation systems, and fraud detection use cases.\nDevelop metadata-driven pipelines and lineage tracking for graph and relational data processing.\nEnsure data quality, governance, and security standards are met across all graph data initiatives.\nMentor junior engineers and contribute to data engineering best practices, especially around graph-centric patterns and technologies.\nStay up to date with the latest developments in graph technology, graph ML, and network analytics.\nWhat we expect of you\nMust-Have Skills:\nHands-on experience in Databricks, including PySpark, Delta Lake, and notebook-based development.\nHands-on experience with graph database platforms such as Stardog, Neo4j, Marklogic etc.\nStrong understanding of graph theory, graph modeling, and traversal algorithms\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies with strong problem-solving and analytical skills\nExcellent collaboration and communication skills, with experience working with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SPARQL', 'Maven', 'PySpark', 'GSQL', 'Subversion', 'AWS services', 'Stardog', 'Cypher', 'SAFe', 'Jenkins', 'DevOps', 'Git', 'Neo4j', 'Delta Lake', 'Graph Databases', 'Spark', 'Marklogic', 'Gremlin']",2025-06-12 14:24:29
"Data & Analytics Analyst, VP",NatWest Markets,15 - 20 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Join us as a Data & Analytics Analyst\nTake on a new challenge in Data & Analytics and help us shape the future of our business\nYou ll take accountability for the analysis of complex data to identify business issues and opportunities, and supporting the delivery of high quality business solutions\nWere committed to mapping a career path that works for you, with a focus on helping you build new skills and engage with the latest ideas and technologies in data analytics\nWere offering this role at vice president level\nWhat youll do\nAs a Data & Analytics Analyst, you ll be driving the production of high quality analytical input to support the development and implementation of innovative processes and problem resolution. You ll be capturing, validating and documenting business and data requirements, making sure they are in line with key strategic principles.\nWe ll look to you to interrogate, interpret and visualise large volumes of data to identify, support and challenge business opportunities and identify solutions.\nYou ll also be:\nPerforming data extraction, storage, manipulation, processing and analysis\nConducting and supporting options analysis, identifying the most appropriate solution\nAccountable for the full traceability and linkage of business requirements of analytics outputs\nSeeking opportunities to challenge and improve current business processes, ensuring the best result for the customer\nCreating and executing quality assurance at various stages of the project in order to validate the analysis and to ensure data quality, identify data inconsistencies, and resolve as needed\nStrong sense of ownership with a focus on delivering high-quality outcomes\nExceptional attention to detail\nEmphasis on measurable outcomes and impact of work\nExpertise in data analytics and reporting\nThe skills youll need\nYou ll need a background in business analysis tools and techniques, along with the ability to influence through communications tailored to a specific audience. Additionally, you ll need the ability to use core technical skills.\nYou ll also demonstrate:\nClear and effective communication\nProficiency in SQL, and tools such as Excel and Power BI\nExperience in Informatica, Snowflake or others\nResponsible for performance metrics and data solutions across the entire data architecture team\nSkilled in data visualization, report generation and presentation to both technical and business audiences\nOver 15 years of professional experience\nHours\n45\nJob Posting Closing Date:\n23/06/2025",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'Business analysis', 'Analytical', 'Data quality', 'Data analytics', 'Informatica', 'Business solutions', 'SQL', 'Data extraction', 'Data architecture']",2025-06-12 14:24:32
Technical Architect,PwC India,10 - 15 years,Not Disclosed,"['Mumbai', 'Navi Mumbai', 'Gurugram']","Role Description\nWe are looking for a suitable candidate for the opening of Data/Technical Architect role for Data Management, preferably for one who has worked in Insurance or Banking and Financial Services domain and holds relevant experience of 10+ years. The candidate should be willing to take up the role of Senior Manager/Associate Director in an organization based on overall experience.\nLocation : Mumbai and Gurugram\nRelevant experience : 10+ years",,,,"['Data Architecture', 'Technical Architecture', 'Java', 'Bigquery', 'SCALA', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'Data Bricks', 'Python']",2025-06-12 14:24:34
Immediate Joiner- Data Engineer,Healthedge,1 - 4 years,Not Disclosed,['Bengaluru'],"Data Engineer\nYou will be working with agile cross functional software development teams developing cutting age software to solve a significant problem in the Provider Data Management space. This hire will have experience building large scale complex data systems involving multiple cross functional data sets and teams. The ideal candidate will be excited about working on new product development, is comfortable pushing the envelope and challenging the status quo, sets high standards for him/herself and the team, and works well with ambiguity.\nWhat you will do:\nBuild data pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements.\nWork closely with data architect, SMEs and other technology partners to develop & execute data architecture and product roadmap.\nBuild analytical tools to utilize the data pipeline, providing actionable insight into key business performance including operational efficiency and business metrics.\nWork with stakeholders including the leadership, product, customer teams to support their data infrastructure needs while assisting with data-related technical issues.\nAct as a subject matter expert to other team members for technical guidance, solution design and best practices within the customer organization.\nKeep current on big data and data visualization technology trends, evaluate, work on proof-of-concept and make recommendations on cloud technologies.\nWhat you bring:\n2+ years of data engineering experience working in partnership with large data sets (preferably terabyte scale)\nExperience in building data pipelines using any of the ETL tools such as Glue, ADF, Notebooks, Stored Procedures, SQL/Python constructs or similar.\nDeep experience working with industry standard RDBMS such Postgres, SQL Server, Oracle, MySQL etc. and any of the analytical cloud databases such as Big Query, Redshift, Snowflake or similar\nAdvanced SQL expertise and solid programming experience with Python and/or Spark\nExperience working with orchestration tools such as Airflow and building complex dependency workflows.\nExperience, developing and implementing Data Warehouse or Data Lake Architectures, OLAP technologies, data modeling with star/snowflake-schemas to enable analytics & reporting.\nGreat problem-solving capabilities, troubleshooting data issues and experience in stabilizing big data systems.\nExcellent communication and presentation skills as youll be regularly interacting with stakeholders and engineering leadership.\nBachelors or master's in quantitative disciplines such as Computer Science, Computer Engineering, Analytics, Mathematics, Statistics, Information Systems, or other scientific fields.\nBonus points:\nHands-on deep experience with cloud data migration, and experience working with analytic platforms like Fabric, Databricks on the cloud.\nCertification in one of the cloud platforms (AWS/GCP/Azure)\nExperience or demonstrated understanding with real-time data streaming tools like Kafka, Kinesis or any similar tools.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Pyspark', 'Cloud', 'Python']",2025-06-12 14:24:37
Senior Data Manager/ Lead,Codeforce 360,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description:\nWe are looking for a highly experienced and dynamic Senior Data Manager / Lead to oversee a team of Data Engineers and Data Scientists. This role demands a strong background in data platforms such as Snowflake and proficiency in Python, combined with excellent people management and project leadership skills. While hands-on experience in the technologies is beneficial, the primary focus of this role is on team leadership, strategic planning, and project delivery .\n\nJob Title : Senior Data Manager / Lead\nLocation: Hyderabad (Work From Office)\nShift Timing: 10AM-7PM\nKey Responsibilities:\nLead, mentor, and manage a team of Data Engineers and Data Scientists.\nOversee the design and implementation of data pipelines and analytics solutions using Snowflake and Python.\nCollaborate with cross-functional teams (product, business, engineering) to align data solutions with business goals.\nEnsure timely delivery of projects, with high quality and performance.\nConduct performance reviews, training plans, and support career development for the team.\nSet priorities, allocate resources, and manage workloads within the data team.\nDrive adoption of best practices in data management, governance, and documentation.\nEvaluate new tools and technologies relevant to data engineering and data science.\n\nRequired Skills & Qualifications:\n6+ years of experience in data-related roles, with at least 23 years in a leadership or management position.\nStrong understanding of Snowflake architecture, performance tuning, data sharing, security, etc.\nSolid knowledge of Python for data engineering or data science tasks.\nExperience in leading data migration, ETL/ELT, and analytics projects.\nAbility to translate business requirements into technical solutions.\nExcellent leadership, communication, and stakeholder management skills.\nExposure to tools like Databricks, Dataiku, Airflow, or similar platforms is a plus.\nBachelors or Masterâ€™s degree in Computer Science, Engineering, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Bricks', 'Python', 'Airflow', 'Data Migration', 'Dataiku', 'Data Warehousing', 'ETL', 'ELT', 'SQL']",2025-06-12 14:24:39
Data Engineer,Axis Finance (AFL),7 - 11 years,Not Disclosed,"['Mumbai', 'Mumbai (All Areas)']","Key Responsibilities:\nShould have experience in below\nDesign, develop, and implement a Data Lake House architecture on AWS, ensuring scalability, flexibility, and performance.\nBuild ETL/ELT pipelines for ingesting, transforming, and processing structured and unstructured data.\nCollaborate with cross-functional teams to gather data requirements and deliver data solutions aligned with business needs.\nDevelop and manage data models, schemas, and data lakes for analytics, reporting, and BI purposes.\nImplement data governance practices, ensuring data quality, security, and compliance.\nPerform data integration between on-premise and cloud systems using AWS services.\nMonitor and troubleshoot data pipelines and infrastructure for reliability and scalability.\nSkills and Qualifications:\n7 + years of experience in data engineering, with a focus on cloud data platforms.\nStrong experience with AWS services: S3, Glue, Redshift, Athena, Lambda, IAM, RDS, and EC2.\nHands-on experience in building data lakes, data warehouses, and lake house architectures.\nShould have experience in ETL/ELT pipelines using tools like AWS Glue, Apache Spark, or similar.\nExpertise in SQL and Python or Java for data processing and transformations.\nFamiliarity with data modeling and schema design in cloud environments.\nUnderstanding of data security and governance practices, including IAM policies and data encryption.\nExperience with big data technologies (e.g., Hadoop, Spark) and data streaming services (e.g., Kinesis, Kafka).\nHave lending domain knowledge will be added advantage\nPreferred Skills:\nExperience with Databricks or similar platforms for data engineering.\nFamiliarity with DevOps practices for deploying data solutions on AWS (CI/CD pipelines).\nKnowledge of API integration and cloud data migration strategies.",Industry Type: NBFC,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['pipeline tools', 'lending domain', 'AWS', 'data models', 'spark', 'devops', 'databricks', 'date engineering platforms', 'hadoop', 'data lake house', 'API integration']",2025-06-12 14:24:41
Data Engineer,Aqilea Softech,5 - 9 years,13-20 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Job Title: Data Engineer\nCompany : Aqilea India(Client : H&M India)\nEmployment Type: Full Time\nLocation: Bangalore(Hybrid)\nExperience: 4.5 to 9 years\nClient : H&M India\n\nAt H&M, we welcome you to be yourself and feel like you truly belong. Help us reimagine the future of an entire industry by making everyone look, feel, and do good. We take pride in our history of making fashion accessible to everyone and led by our values we strive to build a more welcoming, inclusive, and sustainable industry. We are privileged to have more than 120,000 colleagues, in over 75 countries across the world. Thats 120 000 individuals with unique experiences, skills, and passions. At H&M, we believe everyone can make an impact, we believe in giving people responsibility and a strong sense of ownership. Our business is your business, and when you grow, we grow.\nWebsite : https://career.hm.com/\n\nWe are seeking a skilled and forward-thinking Data Engineer to join our Emerging Tech team. This role is designed for someone passionate about working with cutting-edge technologies such as AI, machine learning, IoT, and big data to turn complex data sets into actionable insights.\nAs the Data Engineer in Emerging Tech, you will be responsible for designing, implementing, and optimizing data architectures and processes that support the integration of next-generation technologies. Your role will involve working with large-scale datasets, building predictive models, and utilizing emerging tools to enable data-driven decision-making across the business. You ll collaborate with technical and business teams to uncover insights, streamline data pipelines, and ensure the best use of advanced analytics technologies.\n\nKey Responsibilities:\nDesign and build scalable data architectures and pipelines that support machine learning, analytics, and IoT initiatives.\nDevelop and optimize data models and algorithms to process and analyse large-scale, complex data sets.\nImplement data governance, security, and compliance measures to ensure high-quality\nCollaborate with cross-functional teams (engineering, product, and business) to translate business requirements into data-driven solutions.\nEvaluate, integrate, and optimize new data technologies to enhance analytics capabilities and drive business outcomes.\nApply statistical methods, machine learning models, and data visualization techniques to deliver actionable insights.\nEstablish best practices for data management, including data quality, consistency, and scalability.\nConduct analysis to identify trends, patterns, and correlations within data to support strategic business initiatives.\nStay updated on the latest trends and innovations in data technologies and emerging data management practices.\n\nSkills Required :\nBachelors or masters degree in data science, Computer Science, Engineering, Statistics, or a related field.\n4.5-9 years of experience in data engineering, data science, or a similar analytical role, with a focus on emerging technologies.\nProficiency with big data frameworks (e.g., Hadoop, Spark, Kafka) and experience with modern cloud platforms (AWS, Azure, or GCP).\nSolid skills in Python, SQL, and optionally R, along with experience using machine learning libraries such as Scikit-learn, TensorFlow, or PyTorch.\nExperience with data visualization tools (e.g., Tableau or Power BI or D3.js) to communicate insights effectively.\nFamiliarity with IoT and edge computing data architectures is a plus.\nUnderstanding of data governance, compliance, and privacy standards.\nAbility to work with both structured and unstructured data.\nExcellent problem-solving, communication, and collaboration skills, with the ability to work in a fast-paced, cross-functional team environment.\nA passion for emerging technologies and a continuous desire to learn and innovate.\nInterested Candidates can share your Resumes to mail id karthik.prakadish@aqilea.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Powerbi', 'Hadoop', 'Kafka', 'Tableau', 'Azure', 'GCP', 'Data Engineer', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-12 14:24:43
"Walk-In Senior Data Engineer - DataStage, Azure & Power BI",Net Connect,6 - 10 years,5-11 Lacs P.A.,['Hyderabad( Madhapur )'],"Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\nBelow JD for your reference\n\nJob Description:\n\nWe are hiring an experienced Senior Data Engineer with strong expertise in IBM DataStage, , and . The ideal candidate will be responsible for end-to-end data integration, transformation, and reporting solutions that drive business decisions.",,,,"['Azure Data Factory', 'Datastage', 'Etl Datastage']",2025-06-12 14:24:45
Data & Analytics Subject Matter Expert,Trianz,10 - 15 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role Overview\nWe are looking for a Data & Analytics Subject Matter Expert with deep expertise in Data Engineering, Business Intelligence (BI), and AWS cloud ecosystems . This role demands strategic thinking, hands-on execution, and collaboration across technical and business teams to deliver impactful data-driven solutions.\nKey Responsibilities\n1. Data Architecture & Engineering\nDesign and implement scalable, high-performance data solutions on AWS.\nBuild robust data pipelines, ETL/ELT workflows, and data lake architectures.\nEnforce data quality, security, and governance practices.\n2. Business Intelligence & Insights\nDevelop interactive dashboards and visualizations using Power BI, Tableau, or QuickSight.\nDefine data models and KPIs to support data-driven decision-making.\nCollaborate with business teams to extract insights that drive action.\n3. Cloud & Advanced Analytics\nDeploy data warehousing solutions using Redshift, Glue, S3, Athena, and other AWS services.\nOptimize storage and processing strategies for performance and cost-efficiency.\nExplore AI/ML integrations for predictive and advanced analytics (preferred).\n4. Collaboration & Best Practices\nPartner with cross-functional teams (engineering, data science, business) to align on data needs.\nChampion best practices in data governance, compliance, and architecture.\nTranslate business requirements into scalable technical solutions.\nRequired Qualifications\nEducation\nBachelor s or Master s in Computer Science, Information Technology, Data Science, or related discipline.\nExperience\n10+ years of experience in data engineering, BI, and analytics domains.\nProven experience with AWS data tools and modern data architectures.\nTechnical Skills\nStrong command of AWS services: Redshift, Glue, S3, Athena, Lambda, Kinesis.\nProficient in SQL, Python, or Scala.\nExperience building and maintaining ETL/ELT workflows and data models.\nExpertise in BI tools like Power BI, Tableau, QuickSight, or Looker.\nFamiliarity with AI/ML models and frameworks is a plus.\nCertifications\nPreferred: AWS Certified Data Analytics - Specialty.\nAdditional certifications in AWS, data engineering, or analytics are a plus.\nWhy Join Trianz\nJoin a high-growth, innovation-led firm delivering transformation at scale.\nCollaborate with global teams on cutting-edge cloud and analytics projects.\nEnjoy a competitive compensation structure and clear career progression pathways.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['power bi', 'Data analytics', 'Subject Matter Expert', 'Business intelligence', 'AWS', 'Information technology', 'Analytics', 'SQL', 'Data architecture', 'Python']",2025-06-12 14:24:48
"Sr. Data Engineer, R&D Data Catalyst Team",Amgen Inc,7 - 9 years,Not Disclosed,['Hyderabad'],"The R&D Data Catalyst Team is responsible for buildingData Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nThe Data Engineer will be responsible for the end-to-end development of an enterprise analytics and data mastering solution leveraging Databricks and Power BI. This role requiresexpertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that research cohort-building and advanced research pipeline.The ideal candidate will have experience creating and surfacing large unifiedrepositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\nYou will collaborate closely with stakeholders, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a strong background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nRoles & Responsibilities:\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with stakeholders to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\nBasic Qualifications and Experience:\nMasters degree with 1 to 3years of experience in Data Engineering OR\nBachelors degree with 4 to 5 years of experience in Data Engineering\nDiploma and 7 to 9 years of experience in Data Engineering.\nFunctional Skills:\nMust-Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nStrong communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood-to-Have Skills:\nExperience in developing differentiated and deliverable solutions\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'ETL processes', 'DAX', 'Business Objects', 'data warehouse design', 'ETL', 'PowerBI Models', 'AWS', 'Power Query']",2025-06-12 14:24:50
Sr Data Engineering Manager,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a Senior Data Engineering Manager with a strong background in Regulatory or Integrated Product Teams within the Biotech or Pharmaceutical domain. This role will lead the end-to-end data strategy and execution for regulatory product submissions, lifecycle management, and compliance reporting, ensuring timely and accurate delivery of regulatory data assets across global markets.You will be embedded in a cross-functional Regulatory Integrated Product Team (IPT) and serve as the data and technology lead, driving integration between scientific, regulatory, and engineering functions to support submission-ready data and regulatory intelligence solutions.\nRoles & Responsibilities:\nFunctional Skills:\nLead the engineering strategy and implementation for end-to-end regulatory operations, including data ingestion, transformation, integration, and delivery across regulatory systems.\nServe as the data engineering SME in the Integrated Product Team (IPT) to support regulatory submissions, agency interactions, and lifecycle updates.\nCollaborate with global regulatory affairs, clinical, CMC, quality, safety, and IT teams to gather submission data requirements and translate them into data engineering solutions.\nManage and oversee the development of data pipelines, data models, and metadata frameworks that support submission data standards (e.g., eCTD, IDMP, SPL, xEVMPD).\nEnable integration and reporting across regulatory information management systems (RIMS), EDMS, clinical trial systems, and lab data platforms.\nImplement data governance, lineage, validation, and audit trails for regulatory data workflows, ensuring GxP and regulatory compliance.\nGuide the development of automation solutions, dashboards, and analytics that improve visibility into submission timelines, data quality, and regulatory KPIs.\nEnsure interoperability between regulatory data platforms and enterprise data lakes or lakehouses for cross-functional reporting and insights.\nCollaborate with IT, data governance, and enterprise architecture teams to ensure alignment with overall data strategy and compliance frameworks.\nDrive innovation by evaluating emerging technologies in data engineering, graph data, knowledge management, and AI for regulatory intelligence.\nLead, mentor, and coach a small team of data engineers and analysts, fostering a culture of excellence, innovation, and delivery.\nDrive Agile and Scaled Agile (SAFe) methodologies, managing sprint backlogs, prioritization, and iterative improvements to enhance team velocity and project delivery.\nStay up-to-date with emerging data technologies, industry trends, and best practices, ensuring the organization leverages the latest innovations in data engineering and architecture.\nMust-Have Skills:\n812 years of experience in data engineering or data architecture, with 3+ years in a senior or managerial capacity, preferably within the biotech or pharmaceutical industry.\nProven experience supporting regulatory functions, including submissions, tracking, and reporting for FDA, EMA, and other global authorities.\nExperience with ETL/ELT tools, data pipelines, and cloud-based data platforms (e.g., Databricks, AWS, Azure, or GCP).\nFamiliarity with regulatory standards and data models such as eCTD, IDMP, HL7, CDISC, and xEVMPD.\nDeep understanding of GxP data compliance, audit requirements, and regulatory submission processes.\nExperience with tools like Power BI, Tableau, or Qlik for regulatory dashboarding and visualization is a plus.\nStrong project management, stakeholder communication, and leadership skills, especially in matrixed, cross-functional environments.\nAbility to translate technical capabilities into regulatory and business outcomes.Prepare team members for stakeholder discussions by helping assess data costs, access requirements, dependencies, and availability for business scenarios.\nGood-to-Have Skills:\nPrior experience working on integrated product teams or regulatory transformation programs.\nKnowledge of Regulatory Information Management Systems (RIMS), Veeva Vault RIM, or Master Data Management (MDM) in regulated environments.\nFamiliarity with Agile/SAFe methodologies and DevOps/DataOps best practices.\nEducation and Professional Certifications\n12 to 15 years of experience in Computer Science, IT or related field\nScaled Agile SAFe certification preferred\nProject Management certifications preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'engineering strategy', 'DevOps', 'Project Management', 'DataOps', 'Agile', 'data strategy']",2025-06-12 14:24:52
Data Engineer (AWS),Neoware Technology Solutions,4 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Engineer (AWS) - Neoware Technology Solutions Private Limited Data Engineer (AWS)\nRequirements\n4 - 10 years of hands-on experience in designing, developing and implementing data engineering solutions.\nStrong SQL development skills, including performance tuning and query optimization.\nGood understanding of data concepts.\nProficiency in Python and a solid understanding of programming concepts.\nHands-on experience with PySpark or Spark Scala for building data pipelines.\nUnderstanding of streaming data pipelines for near real-time analytics.\nExperience with Azure services including Data Factory, Functions, Databricks, Synapse Analytics, Event Hub, Stream Analytics and Data Lake Storage.\nFamiliarity with at least one NoSQL database.\nKnowledge of modern data architecture patterns and industry trends in data engineering.\nUnderstanding of data governance concepts for data platforms and analytical solutions.\nExperience with Git for managing version control for source code.\nExperience with DevOps processes, including experience implementing CI/CD pipelines for data engineering solutions.\nStrong analytical and problem-solving skills.\nExcellent communication and teamwork skills.\nResponsibilities\nAzure Certifications related to Data Engineering are highly preferred.\nExperience with Amazon AppFlow, EKS, API Gateway, NoSQL database services.\nStrong understanding and experience with BI/visualization tools like Power BI.\nChennai, Bangalore\nFull time\n4+ years\nOther positions Principal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Version control', 'GIT', 'NoSQL', 'query optimization', 'Analytical', 'data governance', 'Analytics', 'Python', 'Data architecture']",2025-06-12 14:24:54
Data Engineer (Azure),Neoware Technology Solutions,4 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Engineer (Azure) - Neoware Technology Solutions Private Limited Data Engineer (Azure)\nRequirements\n4 - 10 years of hands-on experience in designing, developing and implementing data engineering solutions.\nStrong SQL development skills, including performance tuning and query optimization.\nGood understanding of data concepts.\nProficiency in Python and a solid understanding of programming concepts.\nHands-on experience with PySpark or Spark Scala for building data pipelines.\nEnsure data consistency and address ambiguities or inconsistencies across datasets.\nUnderstanding of streaming data pipelines for near real-time analytics.\nExperience with Azure services including Data Factory, Functions, Databricks, Synapse Analytics, Event Hub, Stream Analytics and Data Lake Storage.\nFamiliarity with at least one NoSQL database.\nKnowledge of modern data architecture patterns and industry trends in data engineering.\nUnderstanding of data governance concepts for data platforms and analytical solutions.\nExperience with Git for managing version control for source code.\nExperience with DevOps processes, including experience implementing CI/CD pipelines for data engineering solutions.\nStrong analytical and problem-solving skills.\nExcellent communication and teamwork skills.\nResponsibilities\nAzure Certifications related to Data Engineering are highly preferred.\nExperience with Azure Kubernetes Service (AKS), Container Apps and API Management.\nStrong understanding and experience with BI/visualization tools like Power BI.\nChennai, Bangalore\nFull time\n4+ years\nOther positions\nChennai / Bangalore / Mumbai\n3+ years\nPrincipal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Version control', 'GIT', 'query optimization', 'NoSQL', 'Analytical', 'SCALA', 'Analytics', 'Python', 'Data architecture']",2025-06-12 14:24:56
"Data Engineering : Sr Software Engineer, Tech Lead & Sr Tech Lead",Reflion Tech,7 - 12 years,22.5-37.5 Lacs P.A.,"['Mumbai( Ghansoli )', 'Navi Mumbai', 'Mumbai (All Areas)']","Hiring: Data Engineering Senior Software Engineer / Tech Lead / Senior Tech Lead\n\n- Hybrid (3 Days from office) | Shift: 2 PM 11 PM IST\n- Experience: 5 to 12+ years (based on role & grade)\n\nOpen Grades/Roles:\nSenior Software Engineer: 58 Years\nTech Lead: 7â€“10 Years\nSenior Tech Lead: 10â€“12+ Years\n\nJob Description â€“ Data Engineering Team\n\nCore Responsibilities (Common to All Levels):\n\nDesign, build and optimize ETL/ELT pipelines using tools like Pentaho, Talend, or similar\nWork on traditional databases (PostgreSQL, MSSQL, Oracle) and MPP/modern systems (Vertica, Redshift, BigQuery, MongoDB)\nCollaborate cross-functionally with BI, Finance, Sales, and Marketing teams to define data needs\nParticipate in data modeling (ER/DW/Star schema), data quality checks, and data integration\nImplement solutions involving messaging systems (Kafka), REST APIs, and scheduler tools (Airflow, Autosys, Control-M)\nEnsure code versioning and documentation standards are followed (Git/Bitbucket)\n\nAdditional Responsibilities by Grade\n\nSenior Software Engineer (5â€“8 Yrs):\nFocus on hands-on development of ETL pipelines, data models, and data inventory\nAssist in architecture discussions and POCs\nGood to have: Tableau/Cognos, Python/Perl scripting, GCP exposure\n\nTech Lead (7â€“10 Yrs):\nLead mid-sized data projects and small teams\nDecide on ETL strategy (Push Down/Push Up) and performance tuning\nStrong working knowledge of orchestration tools, resource management, and agile delivery\n\nSenior Tech Lead (10â€“12+ Yrs):\nDrive data architecture, infrastructure decisions, and internal framework enhancements\nOversee large-scale data ingestion, profiling, and reconciliation across systems\nMentoring junior leads and owning stakeholder delivery end-to-end\nAdvantageous: Experience with AdTech/Marketing data, Hadoop ecosystem (Hive, Spark, Sqoop)\n\n- Must-Have Skills (All Levels):\n\nETL Tools: Pentaho / Talend / SSIS / Informatica\nDatabases: PostgreSQL, Oracle, MSSQL, Vertica / Redshift / BigQuery\nOrchestration: Airflow / Autosys / Control-M / JAMS\nModeling: Dimensional Modeling, ER Diagrams\nScripting: Python or Perl (Preferred)\nAgile Environment, Git-based Version Control\nStrong Communication and Documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SQL', 'ETL', 'Orchestration', 'Postgresql', 'Peri', 'Informatica', 'ETL Tool', 'SSIS', 'Elt', 'Modeling', 'MongoDB', 'Data Architecture', 'Talend', 'Pentaho', 'Python']",2025-06-12 14:24:59
Senior Data Engineer,Jeavio,5 - 10 years,Not Disclosed,[],"We are seeking an experienced Senior Data Engineer to join our team. The ideal candidate will have a strong background in data engineering and AWS infrastructure, with hands-on experience in building and maintaining data pipelines and the necessary infrastructure components. The role will involve using a mix of data engineering tools and AWS services to design, build, and optimize data architecture.\n\nKey Responsibilities:\nDesign, develop, and maintain data pipelines using Airflow and AWS services.\nImplement and manage data warehousing solutions with Databricks and PostgreSQL.\nAutomate tasks using GIT / Jenkins.\nDevelop and optimize ETL processes, leveraging AWS services like S3, Lambda, AppFlow, and DMS.\nCreate and maintain visual dashboards and reports using Looker.\nCollaborate with cross-functional teams to ensure smooth integration of infrastructure components.\nEnsure the scalability, reliability, and performance of data platforms.\nWork with Jenkins for infrastructure automation.\n\nTechnical and functional areas of expertise:\nWorking as a senior individual contributor on a data intensive project\nStrong experience in building high performance, resilient & secure data processing pipelines preferably using Python based stack.\nExtensive experience in building data intensive applications with a deep understanding of querying and modeling with relational databases preferably on time-series data.\nIntermediate proficiency in AWS services (S3, Airflow)\nProficiency in Python and PySpark\nProficiency with ThoughtSpot or Databricks.\nIntermediate proficiency in database scripting (SQL)\nBasic experience with Jenkins for task automation\n\nNice to Have :\nIntermediate proficiency in data analytics tools (Power BI / Tableau / Looker / ThoughSpot)\nExperience working with AWS Lambda, Glue, AppFlow, and other AWS transfer services.\nExposure to PySpark and data automation tools like Jenkins or CircleCI.\nFamiliarity with Terraform for infrastructure-as-code.\nExperience in data quality testing to ensure the accuracy and reliability of data pipelines.\nProven experience working directly with U.S. client stakeholders.\nAbility to work independently and take the lead on tasks.\n\nEducation and experience:\nBachelors or masters in computer science or related fields.\n5+ years of experience\n\nStack/Skills needed:\nDatabricks\nPostgreSQL\nPython & Pyspark\nAWS Stack\nPower BI / Tableau / Looker / ThoughSpot\nFamiliarity with GIT and/or CI/CD tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'AWS', 'Data Bricks', 'Python', 'Etl Pipelines', 'Airflow', 'Database Scripting', 'Postgresql', 'Looker', 'SQL']",2025-06-12 14:25:01
Data Engineer,Nemetschek,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\n5+ years in software development, with a focus on data-intensive applications, cloud solutions, and scalable data architectures.\nDevelopment experience in GoLang for building scalable and efficient data applications.\nExperience with Snowflake, Redshift, or similar data platforms including architecture, data modeling, performance optimization, and integrations.\nExperience designing and building data lakes and data warehouses, ensuring data integrity, scalability, and performance.\nProficient in developing and managing ETL pipelines, using modern tools and techniques to transform, load, and integrate data efficiently.\nExperience with high-volume event streams (such as Kafka, Kinesis) and near real-time data processing solutions for fast and accurate reporting.\nHands-on experience with Terraform for automating infrastructure deployment and configuration management in cloud environments.\nExperience with containerization technologies (Docker, Kubernetes) and orchestration.\nSolid grasp of database fundamentals (SQL, NoSQL, data modeling, performance tuning)\nExperience with CI/CD pipelines and automation tools for testing, deployment, and continuous improvement.\nExperience working in AWS cloud environments, specifically with big data solutions and serverless architectures\nAbility to mentor and guide junior engineers, fostering a culture of learning and innovation\nStrong communication skills to articulate technical concepts clearly to non-technical stakeholders.\nWHAT WE OFFER\nA young, dynamic, and innovation-oriented environment\nA wide variety of projects within different industries\nA very open and informal culture where knowledge sharing, and employee development are key.\nRoom for personal initiative, development, and growth\nRealistic career opportunities\nCompetitive package and fringe benefits.\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Golang', 'Snowflake', 'Javascript', 'ETL', 'AWS']",2025-06-12 14:25:03
Data Modelling Expert (with Avaloq experience),Luxoft,5 - 10 years,Not Disclosed,['Bengaluru'],"Design, implement, and maintain conceptual, logical, and physical data models within the Avaloq Core Banking system.\nCollaborate with business analysts, product owners, and Avaloq parameterisation teams to translate business requirements into robust data models.\nEnsure alignment of data models with Avaloqs object model and industry best practices.\nPerform data profiling, quality checks, and lineage tracing to support regulatory and compliance requirements (e.g., Basel III, MiFID II, ESG).\nSupport integration of Avaloq data with downstream systems (e.g., CRM, data warehouses, reporting platforms).\nProvide expert input on data governance, metadata management, and model documentation.\nContribute to change requests, upgrades, and data migration projects involving Avaloq.\nCollaborate with cross-functional teams to drive data consistency, reusability, and scalability.\nReview and validate existing data models, identify gaps or optimisation opportunities.\nEnsure data models meet performance, security, and privacy requirements.\nSkills\nMust have\nProven experience (5+ years) in data modelling or data architecture, preferably within financial services.\n3+ years of hands-on experience with Avaloq Core Banking Platform, especially its data structures and object model.\nStrong understanding of relational databases and data modelling tools (e.g., ER/Studio, ERwin, or similar).\nProficient in SQL and data manipulation in Avaloq environments.\nKnowledge of banking products, client lifecycle data, and regulatory data requirements.\nFamiliarity with data governance, data quality, and master data management concepts.\nExperience working in Agile or hybrid project delivery environments.\nNice to have\nExposure to Avaloq Scripting or parameterisation is a strong plus.\nExperience integrating Avaloq with data lakes, BI/reporting tools, or regulatory platforms.\nUnderstanding of data privacy regulations (GDPR, FINMA, etc.).\nCertification in Avaloq or relevant financial data management domains is advantageous.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nAvaloq Software Engineer (Data)\nAvaloq\nIndia\nRemote India\nAvaloq Technical Lead\nAvaloq\nAustralia\nSydney\nSenior Avaloq Engineer\nAvaloq\nAustralia\nSydney\nBengaluru, India\nReq. VR-114445\nAvaloq\nBCM Industry\n21/05/2025\nReq. VR-114445\nApply for Data Modelling Expert (with Avaloq experience) in Bengaluru\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data migration', 'metadata', 'Data management', 'Agile', 'Data structures', 'Data quality', 'Financial services', 'SQL', 'CRM', 'Core banking']",2025-06-12 14:25:05
Data Modelling Expert (with Avaloq experience),Luxoft,5 - 10 years,Not Disclosed,[],"Design, implement, and maintain conceptual, logical, and physical data models within the Avaloq Core Banking system.\nCollaborate with business analysts, product owners, and Avaloq parameterisation teams to translate business requirements into robust data models.\nEnsure alignment of data models with Avaloqs object model and industry best practices.\nPerform data profiling, quality checks, and lineage tracing to support regulatory and compliance requirements (e.g., Basel III, MiFID II, ESG).\nSupport integration of Avaloq data with downstream systems (e.g., CRM, data warehouses, reporting platforms).\nProvide expert input on data governance, metadata management, and model documentation.\nContribute to change requests, upgrades, and data migration projects involving Avaloq.\nCollaborate with cross-functional teams to drive data consistency, reusability, and scalability.\nReview and validate existing data models, identify gaps or optimisation opportunities.\nEnsure data models meet performance, security, and privacy requirements.\nSkills\nMust have\nProven experience (5+ years) in data modelling or data architecture, preferably within financial services.\n3+ years of hands-on experience with Avaloq Core Banking Platform, especially its data structures and object model.\nStrong understanding of relational databases and data modelling tools (e.g., ER/Studio, ERwin, or similar).\nProficient in SQL and data manipulation in Avaloq environments.\nKnowledge of banking products, client lifecycle data, and regulatory data requirements.\nFamiliarity with data governance, data quality, and master data management concepts.\nExperience working in Agile or hybrid project delivery environments.\nNice to have\nExposure to Avaloq Scripting or parameterisation is a strong plus.\nExperience integrating Avaloq with data lakes, BI/reporting tools, or regulatory platforms.\nUnderstanding of data privacy regulations (GDPR, FINMA, etc.).\nCertification in Avaloq or relevant financial data management domains is advantageous.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nAvaloq Technical Lead\nAvaloq\nAustralia\nSydney\nSenior Avaloq Engineer\nAvaloq\nAustralia\nSydney\nAvaloq Engineer\nAvaloq\nIndia\nBengaluru\nRemote India, India\nReq. VR-114445\nAvaloq\nBCM Industry\n21/05/2025\nReq. VR-114445\nApply for Data Modelling Expert (with Avaloq experience) in Remote India\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data migration', 'metadata', 'Data management', 'Agile', 'Data structures', 'Data quality', 'Financial services', 'SQL', 'CRM', 'Core banking']",2025-06-12 14:25:08
Azure Data Bricks (4-15 Yrs) - Bangalore,Happiest Minds Technologies,4 - 9 years,Not Disclosed,['Bengaluru'],"Hi,\n\nGreetings from Happiest Minds Technologies\n\nCurrently we are hiring for below positions and looking for immediate joiners.\n1. Azure Databricks Bangalore 5 to 10 Yrs - Bangalore\nAs a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools. Proficiency in Python PySpark, Pandas, NumPy, SciPy, Spark SQL, DataFrames, RDDs, Delta Lake, Databricks Notebooks, and MLflow is required, along with hands-on experience in Azure Data Lake, Blob Storage, and Synapse Analytics.",,,,"['Pyspark', 'Azure', 'Data Bricks', 'sql', 'ETL']",2025-06-12 14:25:10
Senior Data Engineer : 7+ Years,Jayam Solutions Pvt Ltd - CMMI Level III Company,5 - 9 years,Not Disclosed,['Hyderabad( Madhapur )'],"Job Description:\nPosition: Sr.Data Engineer\nExperience: Minimum 7 years\nLocation: Hyderabad\nJob Summary:\n\nWhat Youll Do\n\nDesign and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.\nParticipate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.\nIndependently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.\nMonitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.\nMaintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.\nAcquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.\nEngage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.\nDrive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.\n\nWhat Youll Bring\n\nBachelors degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.\n7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.\nExtensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.\nCloud - AWS\nApache Flink/Spark , Redis\nDatabase platform- Databricks.\nProficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.\nExpertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.\nExperience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).\nAbility to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.\nStrong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.\nExceptional attention to detail and professionalism in report and dashboard creation.\nExcellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.\nOutstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.\n\nPlease feel free to contact us: 9440806850\nEmail ID : careers@jayamsolutions.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Flink', 'Redis', 'Spark', 'Python', 'SCALA', 'Ci/Cd', 'Devops', 'AWS']",2025-06-12 14:25:12
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Maven', 'SparkSQL Apache Spark', 'PySpark', 'Subversion', 'OLAP', 'Scaled Agile methodologies', 'SQL', 'Scaled Agile Framework', 'Jenkins', 'NOSQL database', 'Git', 'Databricks', 'Data Fabric', 'Data Mesh', 'AWS', 'Python']",2025-06-12 14:25:14
Data Engineer,XL India Business Services Pvt. Ltd,1 - 7 years,Not Disclosed,['Gurugram'],"Senior Engineer, Data Modeling Gurgaon/Bangalore, India AXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained industrious advantage\n\nOur Chief Data Office also known as our Innovation, Data Intelligence & Analytics team (IDA) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking a Data Engineer\n\nThe role will support the team s efforts towards creating, enhancing, and stabilizing the Enterprise data lake through the development of the data pipelines\n\nThis role requires a person who is a team player and can work well with team members from other disciplines to deliver data in an efficient and strategic manner\n\nWhat you ll be doing What will your essential responsibilities include? Act as a data engineering expert and partner to Global Technology and data consumers in controlling complexity and cost of the data platform, whilst enabling performance, governance, and maintainability of the estate\n\nUnderstand current and future data consumption patterns, architecture (granular level), partner with Architects to make sure optimal design of data layers\n\nApply best practices in Data architecture\n\nFor example, balance between materialization and virtualization, optimal level of de-normalization, caching and partitioning strategies, choice of storage and querying technology, performance tuning\n\nLeading and hands-on execution of research into new technologies\n\nFormulating frameworks for assessment of new technology vs business benefit, implications for data consumers\n\nAct as a best practice expert, blueprint creator of ways of working such as testing, logging, CI/CD, observability, release, enabling rapid growth in data inventory and utilization of Data Science Platform\n\nDesign prototypes and work in a fast-paced iterative solution delivery model\n\nDesign, Develop and maintain ETL pipelines using Py spark in Azure Databricks using delta tables\n\nUse Harness for deployment pipeline\n\nMonitor Performance of ETL Jobs, resolve any issue that arose and improve the performance metrics as needed\n\nDiagnose system performance issue related to data processing and implement solution to address them\n\nCollaborate with other teams to make sure successful integration of data pipelines into larger system architecture requirement\n\nMaintain integrity and quality across all pipelines and environments\n\nUnderstand and follow secure coding practice to make sure code is not vulnerable\n\nYou will report to the Application Manager\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Effective Communication skills\n\nBachelor s degree in computer science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience\n\nRelevant years of extensive work experience in various data engineering & modeling techniques (relational, data warehouse, semi-structured, etc), application development, advanced data querying skills\n\nRelevant years of programming experience using Databricks\n\nRelevant years of experience using Microsoft Azure suite of products (ADF, synapse and ADLS)\n\nSolid knowledge on network and firewall concepts\n\nSolid experience writing, optimizing and analyzing SQL\n\nRelevant years of experience with Python\n\nAbility to break complex data requirements and architect solutions into achievable targets\n\nRobust familiarity with Software Development Life Cycle (SDLC) processes and workflow, especially Agile\n\nExperience using Harness\n\nTechnical lead responsible for both individual and team deliveries\n\nDesired Skills and Abilities: Worked in big data migration projects\n\nWorked on performance tuning both at database and big data platforms\n\nAbility to interpret complex data requirements and architect solutions\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExcellent basics on parquet files and delta files\n\nEffective Knowledge of Azure cloud computing platform\n\nFamiliarity with Reporting software - Power BI is a plus\n\nFamiliarity with DBT is a plus\n\nPassion for data and experience working within a data-driven organization\n\nYou care about what you do, and what we do\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides dynamic compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Coding', 'Agile', 'Workflow', 'Application development', 'SDLC', 'SQL', 'Python', 'Firewall']",2025-06-12 14:25:17
"Data Engineer, AVP",NatWest Markets,16 - 18 years,Not Disclosed,['Gurugram'],"Join us as a Data Engineer\nWe re looking for someone to build effortless, digital first customer experiences to help simplify our organisation and keep our data safe and secure\nDay-to-day, you ll develop innovative, data-driven solutions through data pipelines, modelling and ETL design while inspiring to be commercially successful through insights\nIf you re ready for a new challenge, and want to bring a competitive edge to your career profile by delivering streaming data ingestions, this could be the role for you\nWere offering this role at assistant vice president level\nWhat you ll do\nYour daily responsibilities will include you developing a comprehensive knowledge of our data structures and metrics, advocating for change when needed for product development. You ll also provide transformation solutions and carry out complex data extractions.\nWe ll expect you to develop a clear understanding of data platform cost levels to build cost-effective and strategic solutions. You ll also source new data by using the most appropriate tooling before integrating it into the overall solution to deliver it to our customers.\nYou ll also be responsible for:\nDriving customer value by understanding complex business problems and requirements to correctly apply the most appropriate and reusable tools to build data solutions\nParticipating in the data engineering community to deliver opportunities to support our strategic direction\nCarrying out complex data engineering tasks to build a scalable data architecture and the transformation of data to make it usable to analysts and data scientists\nBuilding advanced automation of data engineering pipelines through the removal of manual stages\nLeading on the planning and design of complex products and providing guidance to colleagues and the wider team when required\nThe skills you ll need\nTo be successful in this role, you ll have an understanding of data usage and dependencies with wider teams and the end customer. You ll also have experience of extracting value and features from large scale data.\nWe ll expect you to have experience of ETL technical design, data quality testing, cleansing and monitoring, data sourcing, exploration and analysis, and data warehousing and data modelling capabilities.\nYou ll also need:\nExperience of using programming languages alongside knowledge of data and software engineering fundamentals\nGood knowledge of modern code development practices\nGreat communication skills with the ability to proactively engage with a range of stakeholders\nHours\n45\nJob Posting Closing Date:\n16/06/2025",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Usage', 'Technical design', 'Programming', 'Data structures', 'Data quality', 'Assistant Vice President', 'Data warehousing', 'Monitoring', 'Data architecture']",2025-06-12 14:25:19
"Director, Enterprise Data Architecture",Horizon Therapeutics,10 - 12 years,Not Disclosed,['Hyderabad'],"Career Category Engineering Job Description\nABOUT AMGEN\nAmgen harnesses the best of biology and technology to fight the world s toughest diseases, and make people s lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what s known today.\nABOUT THE ROLE\nRole Description:\nThe Director for Data Architecture and Solutions will lead Amgen s enterprise data architecture and solutions strategy, overseeing the design, integration, and deployment of scalable, secure, and future-ready data systems. This leader will define the architectural vision and guide a high-performing team of architects and technical experts to implement data and analytics solutions that drive business value and innovation.\nThis role demands a strong blend of business acumen, deep technical expertise, and strategic thinking to align data capabilities with the companys mission and growth. The Director will also serve as a key liaison with executive leadership, influencing technology investment and enterprise data direction\n.\nRoles & Responsibilities:\nDevelop and own the enterprise data architecture and solutions roadmap, aligned with Amgen s business strategy and digital transformation goals.\nProvide executive leadership and oversight of data architecture initiatives across business domains (R&D, Commercial, Manufacturing, etc.).\nLead and grow a high-impact team of data and solution architects. Coach, mentor, and foster innovation and continuous improvement in the team.\nDesign and promote modern data architectures (data mesh, data fabric, lakehouse etc.) across hybrid cloud environments and enable for AI readiness.\nCollaborate with stakeholders to define solution blueprints, integrating business requirements with technical strategy to drive value.\nDrive enterprise-wide adoption of data modeling, metadata management, and data lineage standards.\nEnsure solutions meet enterprise-grade requirements for security, performance, scalability, compliance, and data governance.\nPartner closely with Data Engineering, Analytics, AI/ML, and IT Security teams to operationalize data solutions that enable advanced analytics and decision-making.\nChampion innovation and continuous evolution of Amgen s data and analytics landscape through new technologies and industry best practices.\nCommunicate architectural strategy and project outcomes to executive leadership and other non-technical stakeholders.\nFunctional Skills:\nMust-Have Skills:\n10+ years of experience in data architecture or solution architecture leadership roles, including experience at the enterprise level.\nProven experience leading architecture strategy and delivery in the life sciences or pharmaceutical industry.\nExpertise in cloud platforms (AWS, Azure, or GCP) and modern data technologies (data lakes, APIs, ETL/ELT frameworks).\nStrong understanding of data governance, compliance (e.g., HIPAA, GxP), and data privacy best practices.\nDemonstrated success managing cross-functional, global teams and large-scale data programs.\nExperience with enterprise architecture frameworks (TOGAF, Zachman, etc.).\nProven leadership skills with a track record of managing and mentoring high-performing data architecture teams.\nGood-to-Have Skills:\nMaster s or doctorate in Computer Science, Engineering, or related field.\nCertifications in cloud architecture (AWS, GCP, Azure).\nExperience integrating AI/ML solutions into enterprise Data Achitecture.\nFamiliarity with DevOps, CI/CD pipelines, and Infrastructure as Code (Terraform, CloudFormation).\nScaled Agile or similar methodology experience.\nLeadership and Communication Skills:\nStrategic thinker with the ability to influence at the executive level.\nStrong executive presence with excellent communication and storytelling skills.\nAbility to lead in a matrixed, global environment with multiple stakeholders.\nHighly collaborative, proactive, and business-oriented mindset.\nStrong organizational and prioritization skills to manage complex initiatives.\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nBasic Qualifications:\nDoctorate degree and 2 years of Information Systems experience, or\nMaster s degree and 6 years of Information Systems experience, or\nBachelor s degree and 8 years of Information Systems experience, or\nAssociates degree and 10 years of Information Systems experience, or\n4 years of managerial experience directly managing people and leadership experience leading teams, projects, or programs.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'metadata', 'Data modeling', 'Enterprise architecture', 'TOGAF', 'HIPAA', 'Agile', 'Analytics', 'Data architecture']",2025-06-12 14:25:22
Data Engineering Lead,Yotta Techports,10 - 15 years,30-35 Lacs P.A.,['Hyderabad'],"Responsibilities:\nLead and manage an offshore team of data engineers, providing strategic guidance, mentorship, and support to ensure the successful delivery of projects and the development of team members.\nCollaborate closely with onshore stakeholders to understand project requirements, allocate resources efficiently, and ensure alignment with client expectations and project timelines.\nDrive the technical design, implementation, and optimization of data pipelines, ETL processes, and data warehouses, ensuring scalability, performance, and reliability.\nDefine and enforce engineering best practices, coding standards, and data quality standards to maintain high-quality deliverables and mitigate project risks.\nStay abreast of emerging technologies and industry trends in data engineering, and provide recommendations for tooling, process improvements, and skill development.\nAssume a data architect role as needed, leading the design and implementation of data architecture solutions, data modeling, and optimization strategies.\nDemonstrate proficiency in AWS services such as:\nExpertise in cloud data services, including AWS services like Amazon Redshift, Amazon EMR, and AWS Glue, to design and implement scalable data solutions.\nExperience with cloud infrastructure services such as AWS EC2, AWS S3, to optimize data processing and storage.\nKnowledge of cloud security best practices, IAM roles, and encryption mechanisms to ensure data privacy and compliance.\nProficiency in managing or implementing cloud data warehouse solutions, including data modeling, schema design, performance tuning, and optimization techniques.\nDemonstrate proficiency in modern data platforms such as Snowflake and Databricks, including:\nDeep understanding of Snowflake's architecture, capabilities, and best practices for designing and implementing data warehouse solutions.\nHands-on experience with Databricks for data engineering, data processing, and machine learning tasks, leveraging Spark clusters for scalable data processing.\nAbility to optimize Snowflake and Databricks configurations for performance, scalability, and cost-effectiveness.\nManage the offshore team's performance, including resource allocation, performance evaluations, and professional development, to maximize team productivity and morale.\n\nQualifications:\nBachelor's degree in Computer Science, Engineering, or a related field; advanced degree preferred.\n10+ years of experience in data engineering, with a proven track record of leadership and technical expertise in managing complex data projects.\nProficiency in programming languages such as Python, Java, or Scala, as well as expertise in SQL and relational databases (e.g., PostgreSQL, MySQL).\nStrong understanding of distributed computing, cloud technologies (e.g., AWS), and big data frameworks (e.g., Hadoop, Spark).\nExperience with data architecture design, data modeling, and optimization techniques.\nExcellent communication, collaboration, and leadership skills, with the ability to effectively manage remote teams and engage with onshore stakeholders.\nProven ability to adapt to evolving project requirements and effectively prioritize tasks in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team Handling', 'Snowflake', 'Data Services', 'Cloud Infrastructure', 'Data Bricks']",2025-06-12 14:25:24
Full Stack Data Scientist,Vimo Getinsured,2 - 7 years,Not Disclosed,['Gurugram( Sector 61 Gurgaon )'],"About the Role\nAs a Data Science Engineer, you will need strong technical skills in data modeling, machine learning, data engineering, and software development. You will have the ability to conduct literature reviews and critically evaluate research papers to identify applicable techniques. Additionally, you should be able to design and implement efficient and scalable data processing pipelines, perform exploratory data analysis, and collaborate with other teams to integrate data science models into production systems. Passion for conversational AI and a desire to solve some of the most complex problems in the Natural Language Processing space are essential. You will work on highly scalable, stable, and automated deployments, aiming for high performance. Taking on the challenge of building and scaling a truly remarkable AI platform to impact the lives of millions of customers will be part of your responsibilities. Working in a challenging yet enjoyable environment, where learning new things is the norm, you should think of solutions beyond boundaries. You should also drive outcomes with full ownership, deeply believe in customer obsession, and thrive in a fast-paced environment of learning and innovation.\nYou will work in a challenging, consumer-facing problem space, where you can make an immediate impact. You will get to work with the latest technologies, learn to use new tools and get the opportunity to have your say in the final product. Youll work alongside a great team in an open, collaborative environment. We are part of Vimo, a well-funded, stable mid-size company with excellent salaries, medical/dental/vision coverage, and perks. Vimo is an Equal Opportunity Employer.",,,,"['python', 'Langchain', 'Neural Networks', 'LLM', 'Linux', 'Data Structures', 'Natural Language Processing', 'Jupyter Notebook', 'Machine Learning', 'Deep Learning', 'Numpy', 'Data Science', 'pandas', 'Nltk', 'Langgraph', 'Transformers', 'BERT', 'langsmith']",2025-06-12 14:25:27
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nAs part of the cybersecurity organization, the Data Engineer is responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nCreate data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, Gitlab, LucidChart,etc.\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data engineering', 'data security', 'Agile', 'cloud data platforms', 'Databricks', 'data governance frameworks', 'ETL', 'AWS', 'SQL', 'Python']",2025-06-12 14:25:29
Data Engineer,Konrad Group,3 - 7 years,15-30 Lacs P.A.,['Gurugram( Sector 42 Gurgaon )'],"Who We Are\n\nKonrad is a next generation digital consultancy. We are dedicated to solving complex business problems for our global clients with creative and forward-thinking solutions. Our employees enjoy a culture built on innovation and a commitment to creating best-in-class digital products in use by hundreds of millions of consumers around the world. We hire exceptionally smart, analytical, and hard working people who are lifelong learners.\nAbout The Role\nAs a Data Engineer youll be tasked with designing, building, and maintaining scalable data platforms and pipelines. Your deep knowledge of data platforms such as Azure Fabric, Databricks, and Snowflake will be essential as you collaborate closely with data analysts, scientists, and other engineers to ensure reliable, secure, and efficient data solutions.\n\nWhat Youll Do\n\nDesign, build, and manage robust data pipelines and data architectures.\nImplement solutions leveraging platforms such as Azure Fabric, Databricks, and Snowflake.\nOptimize data workflows, ensuring reliability, scalability, and performance.\nCollaborate with internal stakeholders to understand data needs and deliver tailored solutions.\nEnsure data security and compliance with industry standards and best practices.\nPerform data modelling, data extraction, transformation, and loading (ETL/ELT).\nIdentify and recommend innovative solutions to enhance data quality and analytics capabilities.\n\nQualifications\n\nBachelors degree or higher in Computer Science, Data Engineering, Information Technology, or a related field.\nAt least 3 years of professional experience as a Data Engineer or similar role.\nProficiency in data platforms such as Azure Fabric, Databricks, and Snowflake.\nHands-on experience with data pipeline tools, cloud services, and storage solutions.\nStrong programming skills in SQL, Python, or related languages.\nExperience with big data technologies and concepts (Spark, Hadoop, Kafka).\nExcellent analytical, troubleshooting, and problem-solving skills.\nAbility to effectively communicate technical concepts clearly to non-technical stakeholders.\nAdvanced English\n\nNice to have\n\nCertifications related to Azure Data Engineering, Databricks, or Snowflake.\nFamiliarity with DevOps practices and CI/CD pipelines.\n\nPerks and Benefits\n\nComprehensive Health & Wellness Benefits Package \nSocials, Outings & Retreats\nCulture of Learning & Development\nFlexible Working Hours\nWork from Home Flexibility\nService Recognition Programs\n\nKonrad is committed to maintaining a diverse work environment and is proud to be an equal opportunity employer. All qualified applicants, regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status will receive consideration for employment. If you have any accessibility requirements or concerns regarding the hiring process or employment with us, please notify us so we can provide suitable accommodation.\nWhile we sincerely appreciate all applications, only those candidates selected for an interview will be contacted.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Azure Data Factory', 'Azure Databricks', 'Spark', 'Fabric', 'Python']",2025-06-12 14:25:32
Lead Data Engineer (Immediate joiner),Decision Point,4 - 9 years,15-30 Lacs P.A.,"['Gurugram', 'Chennai']","Role & responsibilities\nâ€¢ Assume ownership of Data Engineering projects from inception to completion.\nImplement fully operational Unified Data Platform solutions in production environments using technologies like Databricks, Snowflake, Azure Synapse etc.\nShowcase proficiency in Data Modelling and Data Architecture\nUtilize modern data transformation tools such as DBT (Data Build Tool) to streamline and automate data pipelines (nice to have).",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Synapse', 'Python', 'Etl Pipelines', 'Airflow', 'Bigquery', 'Advance Sql', 'Azure Cloud', 'GCP', 'Data Modeling', 'Data Architecture', 'AWS']",2025-06-12 14:25:34
Data Engineer,Xenonstack,2 - 5 years,Not Disclosed,['Mohali( Phase 8B Mohali )'],"At XenonStack, We committed to become the Most Value Driven Cloud Native, Platform Engineering and Decision Driven Analytics Company. Our Consulting Services and Solutions towards the Neural Company and its Key Drivers.\nXenonStacks DataOps team is looking for a Data Engineer who will be responsible for employing techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field.\nYou should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. The ideal candidate should be highly skilled in all aspects of Python, Java/Scala, SQL and analytical skills.\nJob Responsibilities:\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with co-workers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analysed quickly by BI & AI Teams.\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant co-workers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nRequirements:\nTechnical Requirements\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch, Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores.\nProfessional Attributes:\nExcellent communication skills & Attention to detail.\nAnalytical mind and problem-solving Aptitude with Strong Organizational skills & Visual Thinking.\nBenefits:\nDiscover the benefits of joining our team:\nDynamic and purposeful work culture in a people-oriented organization contributing to multi-million-dollar projects with guaranteed job security.\nOpen, authentic, and transparent communication fostering a warm work environment.\nRegular constructive feedback and exposure to diverse technologies.\nRecognition and rewards for exceptional performance achievements.\nAccess to certification courses & Skill Sessions to develop continually and refine your skills.\nAdditional allowances for team members assigned to specific projects.\nSpecial skill allowances to acknowledge and compensate for unique expertise.\nComprehensive medical insurance policy for your health and well-being.\nTo Learn more about the company -\nWebsite - http://www.xenonstack.com/",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Spark', 'ETL', 'Python', 'SQL', 'Java', 'Data Processing', 'Machine Learning']",2025-06-12 14:25:36
Technical Consultant(ETL + SQL + Data Migration),Insightsoftware,4 - 7 years,Not Disclosed,['Hyderabad'],"Insightsoftware (ISW) is a growing, dynamic computer software company that helps businesses achieve greater levels of financial intelligence across their organization with our world-class financial reporting solutions. At insightsoftware, you will learn and grow in a fast-paced, supportive environment that will take your career to the next level. The Data Conversion Specialist is a member of the insightsoftware Project Management Office (PMO) who demonstrates teamwork, results orientation, a growth mindset, disciplined execution, and a winning attitude.\nLocation: Hyderabad (Work from Office)\nWorking Hours: 5:00 PM - 2:00AM IST or 6:00 PM to 3:00 AM IS T, should be ok to work in night shift as per requirement.\nPosition Summary\nThe Consultant will integrate and map customer data from client source system(s) to our industry-leading platform. The role will include, but is not limited to:\nUsing strong technical data migration, scripting, and organizational skills to ensure the client data is converted efficiently and accurately to the insightsoftware (ISW) platform.\nPerforming extract, transform, load (ETL) activities to ensure accurate and timely data conversions.\nProviding in-depth research and analysis of complex scenarios to develop innovative solutions to meet customer needs whilst remaining within project governance.\nMapping and maintaining business requirements to the solution design using tools such as requirements traceability matrices (RTM).\nPresenting findings, requirements, and problem statements for ratification by stakeholders and working groups.\nIdentifying and documenting data gaps to allow change impact and downstream impact analysis to be conducted.\nExperience assessing data and analytic requirements to establish mapping rules from source to target systems to meet business objectives.\nExperience with real-time, batch, and ETL for complex data conversions.\nWorking knowledge of extract, transform, load (ETL) methodologies and tools such as Talend, Dell Boomi, etc.\nUtilize data mapping tools to prepare data for data loads based on target system specifications.\nWorking experience using various data applications/systems such as Oracle SQL, Excel, .csv files, etc.\nStrong SQL scripting experience.\nCommunicate with clients and/or ISW Project Manager to scope, develop, test, and implement conversion/integration\nEffectively communicate with ISW Project Managers and customers to keep project on target\nContinually drive improvements in the data migration process.\nCollaborate via phone and email with clients and/or ISW Project Manager throughout the conversion/integration process.\nDemonstrated collaboration and problem-solving skills.\nWorking knowledge of software development lifecycle (SDLC) methodologies including, but not limited to: Agile, Waterfall, and others.\nClear understanding of cloud and application integrations.\nAbility to work independently, prioritize tasks, and manage multiple tasks simultaneously.\nEnsure client s data is converted/integrated accurately and within deadlines established by ISW Project Manager.\nExperience in customer SIT, UAT, migration and go live support.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data migration', 'Data conversion', 'Financial reporting', 'Project management', 'project governance', 'Agile', 'Software development life cycle', 'data mapping', 'SDLC', 'Downstream']",2025-06-12 14:25:38
Project Manager - Data Migration,TALWORX,9 - 14 years,22.5-35 Lacs P.A.,['Mumbai (All Areas)'],"Role & responsibilities\nDefine project scope, objectives and deliverables in collaboration with IT leads and business sponsors\nBuild & manage detailed migration plan. Coordinate with the internal client PMO function for project onboarding, configuration and tracking on approved PM tools\nCoordinate with cross functional teams & vendors\nOversee data mapping, transformation, validation and testing activities\nManage vendor coordination for tools & services supporting the migration\nEnsure compliance with data governance, security policies and regulatory requirements\nIdentify & manage migration related risks including data loss, downtime & performance issues\nEnsure adherence to QA/UAT protocols and change management process\nSupport audit readiness and documentation\nDefine, Setup and run governance forums\nCommunicate progress, risks, decisions to executive leadership and stakeholders\nDrive validation, optimization and improvement opportunities post migration to enhance data performance and usability\nMentor and guide technical teams throughout the migration journey\nLead and drive change management & knowledge transfer\n\nPreferred candidate profile\n8+ years of experience\nProven expertise delivering atleast one enterprise level end to end data warehouse management program\nSolid understanding of relational database systems, data structures and SQL\nStrong understanding of data modelling, ETL tools/processes, performance tuning and migration planning best practices\nExperience with cloud based database platforms is a plus\nStrong project management skills\nStrong knowledge of SDLC and project management methodologies (Agile, waterfall, Hybrid)\n\nPreferred Skills:\nBFSI or Life Sciences industry exposure\nExperience with data reconciliation and validation frameworks\nStrong interpersonal and communication skills\nAbility to lead cross functional, geographically distributed teams\nFamiliarity with Oracle DMS or Sybase",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['data warehouse', 'Data Migration', 'Oracle', 'SyBase', 'SQL', 'Waterfall', 'Database Management', 'Agile', 'oracle DMS', 'SDLC']",2025-06-12 14:25:41
Assoc. Data Engineer - R&D Precision Medicine Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nThe R&D Precision Medicine team is responsible for Data Standardization, Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with access to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These data include clinical data, omics, and images. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\n\nThe Data Engineer will be responsible for full stack development of enterprise analytics and data mastering solutions leveraging Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that support research cohort-building and advanced AI pipelines. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\n\nYou will collaborate closely with partners, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a solid background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\n\nRoles & Responsibilities\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data management tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with partners to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is someone with these qualifications.\n\nBasic Qualifications:\nMasters degree with 1 to 3 years of experience in Data Engineering OR\nBachelors degree with 1 to 3 years of experience in Data Engineering\nMust-Have\n\nSkills:\nMinimum of 1 year of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 1 year of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nExperience using cloud platforms (AWS), data lakes, and data warehouses.\nWorking knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling and data anlysis\nGood-to-Have\n\nSkills:\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft CertifiedData Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data lakes', 'data pipelines', 'ETL processes', 'AWS', 'data warehouses', 'BI solutions']",2025-06-12 14:25:43
OAC ODI Architect (Senior Oracle Analytics Consultant),Mastek,10 - 15 years,15-30 Lacs P.A.,"['Ahmedabad', 'Chennai']","We are looking for OAC ODI Architect to be based in Ahemdabad or Chennai\nMinimum Architect exp 4 Yrs\nOracle Analytics Consultant (OAC, ODI, FDI) Tech Architect\nLocation: [Specify Location or Remote] Chennai Ahmedabad\nExpected DOJ June\nEmployment Type: Full-time\nExperience Level: 10 - 15+ Years\nJob Summary:\nWe are seeking an experienced and results-driven Senior Oracle Analytics Consultant with over 10 years of hands-on experience in Oracle Analytics Cloud (OAC), Oracle Data Integrator (ODI), and Fusion Data Intelligence (FDI). The ideal candidate will have a deep understanding of enterprise data architecture, data integration best practices, and cloud-based analytics solutions. This role involves working closely with cross-functional teams to design, implement, and support advanced analytics and data integration solutions that drive business value.\nKey Responsibilities:\nLead the design, development, and deployment of analytics solutions using Oracle Analytics Cloud (OAC).\nArchitect and implement data integration pipelines using Oracle Data Integrator (ODI) for on-prem and cloud data sources.\nCollaborate with business and IT stakeholders to design and deploy Fusion Data Intelligence (FDI) based dashboards and KPIs.\nOptimize performance of OAC dashboards and reports, including data modeling and visualization best practices.\nDevelop and manage data models, RPDs, and semantic layers within OAC.\nBuild and maintain ETL mappings, packages, and workflows in ODI.\nIntegrate Oracle Fusion Applications with OAC and FDI for near-real-time reporting.\nDrive data governance and quality initiatives across analytics platforms.\nTroubleshoot technical issues and provide solutions in a timely manner.\nMentor junior developers and provide technical leadership on complex projects.\nQualifications:\nBachelors or Masters degree in Computer Science, Information Systems, or related field.\n10+ years of relevant experience with strong focus on:\nOracle Analytics Cloud (OAC) - Must\nOracle Data Integrator (ODI) - Must\nFusion Data Intelligence (FDI) Good to Have\nExpertise in Oracle Fusion ERP/HCM data models and subject areas.\nExperience integrating multiple data sources, including on-premise and cloud systems.\nStrong understanding of SQL, PL/SQL, and performance tuning.\nFamiliarity with data lake architecture, data warehousing, and ELT/ETL design patterns.\nProven experience working in Agile/DevOps environments.\nExcellent communication, analytical thinking, and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oac', 'ODI', 'Odi Architecture', 'FDI']",2025-06-12 14:25:45
Software Data Operations Engineer (BS+2),MAQ Software,2 - 5 years,Not Disclosed,['Noida'],"MAQ LLC d.b.a MAQ Software hasmultiple openings at Redmond, WA for:\nSoftware Data Operations Engineer (BS+2)\n\nResponsible for gathering & analyzing business requirements from customers. Implement,test and integrate software applications for use by customers. Develop &review cost effective data architecture to ensure appropriateness with currentindustry advances in data management, cloud & user experience. Automateuser test scenarios, debug & fix errors in cloud-based data infrastructure,reporting applications to meet customer needs. Must be able to traveltemporarily to client sites and or relocate throughout the United States.\n\nRequirements:Bachelors Degree or foreign equivalent in Computer Science, ComputerApplications, Computer Information Systems, Information Technology or relatedfield with two years of work experience in job offered, software engineer, systemsanalyst or related job.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Data Operations', 'software applications', 'data management', 'Data Operations', 'data architecture', 'data infrastructure']",2025-06-12 14:25:47
Data Engineer,Trantor,5 - 10 years,Not Disclosed,[],"We are looking for a skilled and motivated Data Engineer with deep expertise in GCP,\nBigQuery, Apache Airflow to join our data platform team. The ideal candidate should have hands-on experience building scalable data pipelines, automating workflows, migrating large-scale datasets, and optimizing distributed systems. The candidate should have experience with building Web APIs using Python. This role will play a key part in designing and maintaining robust data engineering solutions across cloud and on-prem environments.\nKey Responsibilities\nBigQuery & Cloud Data Pipelines:\nDesign and implement scalable ETL pipelines for ingesting large-scale datasets.\nBuild solutions for efficient querying of tables in BigQuery.\nAutomated scheduled data ingestion using Google Cloud services and scheduled\nApache Airflow DAGs",,,,"['Airflow', 'Etl Pipelines', 'GCP', 'Bigquery', 'Python', 'SFTP', 'ETL', 'SQL']",2025-06-12 14:25:50
Manager Data Engineer â€“ Research Data and Analytics,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will create and develop data lake solutions for scientific data that drive business decisions for Research. You will build scalable and high-performance data engineering solutions for large scientific datasets and collaborate with Research collaborators. You will also provide technical leadership to junior team members. The ideal candidate possesses experience in the pharmaceutical or biotech industry, demonstrates deep technical skills, is proficient with big data technologies, and has a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nLead, manage, and mentor a high-performing team of data engineers\nDesign, develop, and implement data pipelines, ETL processes, and data integration solutions\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks\nDevelop and maintain data models for biopharma scientific data, data dictionaries, and other documentation to ensure data accuracy and consistency\nOptimize large datasets for query performance\nCollaborate with global multi-functional teams including research scientists to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nCollaborate with Data Architects, Business SMEs, Software Engineers and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve data-related challenges\nAdhere to best practices for coding, testing, and designing reusable code/component\nExplore new tools and technologies that will help to improve ETL platform performance\nParticipate in sprint planning meetings and provide estimations on technical implementation\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nDoctorate Degree OR\nMasters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications:\n3+ years of experience in implementing and supporting biopharma scientific research data analytics (software platforms)\n\n\nFunctional Skills:\nMust-Have Skills:\nProficiency in SQL and Python for data engineering, test automation frameworks (pytest), and scripting tasks\nHands on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing\nExcellent problem-solving skills and the ability to work with large, complex datasets\nAble to engage with business collaborators and mentor team to develop data pipelines and data models\n\n\nGood-to-Have Skills:\nA passion for tackling complex challenges in drug discovery with technology and data\nGood understanding of data modeling, data warehousing, and data integration concepts\nGood experience using RDBMS (e.g. Oracle, MySQL, SQL server, PostgreSQL)\nKnowledge of cloud data platforms (AWS preferred)\nExperience with data visualization tools (e.g. Dash, Plotly, Spotfire)\nExperience with diagramming and collaboration tools such as Miro, Lucidchart or similar tools for process mapping and brainstorming\nExperience writing and maintaining technical documentation in Confluence\nUnderstanding of data governance frameworks, tools, and best practices\n\n\nProfessional Certifications:\nDatabricks Certified Data Engineer Professional preferred\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Spotfire', 'PySpark', 'PostgreSQL', 'Plotly', 'SparkSQL', 'SQL server', 'SQL', 'process mapping', 'Dash', 'MySQL', 'ETL', 'Oracle', 'data governance frameworks', 'Python']",2025-06-12 14:25:52
"Manager, CFO Master Data Management Specialist",Fidelity International,13 - 16 years,Not Disclosed,['Gurugram'],"Job Title : Manager, CFO Master Data Management Specialist\nDept : CFO Data Management\nLocation : Gurgaon\nLevel : 5\nAbout your team\nThe CFO Data Management team is passionate about improving the data culture and maturity of the CFO function, and the FIL enterprise.\nWe work closely with a wide range of Technology and Change teams across the enterprise to embed the right data governance operating models, tools and data architecture into the CFO business that will help us to advance on our data journey.\nWe partner with data role holders across CFO, and the wider enterprise, to understand and catalogue our metadata, and improve data management, quality and accessibility, as key foundations for improving visualisation and AI capabilities.\nWe work with the CFO business to understand data challenges, as well as opportunities, and support them to develop lasting solutions.\nWe own the Finance Ledger master data including the Chart of Accounts, hierarchies and mappings, and partner with the CFO business to manage and maintain these in line with organisational change.\nAbout your role\nFIL has implemented Core Finance Platform (CFP) based upon Oracle Financials and Procurement Cloud ERP (Fusion / R13) - replacing legacy JD Edwards general ledger and Oracle Web Centre based procurement systems.\nAs part of the CFP implementation, the finance ledger Chart of Accounts was reviewed and fundamentally redesigned. The current Chart of Accounts segment values, descriptions and hierarchies are mastered and maintained using Oracle DRM, supported by workflow approval processes using K2. In addition, some of the legacy chart of account values are also required to be maintained, along with supporting mappings.\nThe ongoing support and maintenance of this dataset is critical, not only to Finance processes and reporting - both internal and external - but across the entire FIL enterprise, with many other business areas using these values and hierarchies to drive reporting.\nYou will be working closely with CFO business to understand, validate and action change requests in a structured way, working to clear month end timetables, and considering the impact on related datasets and processes such as cross validations rules, and allocations rules.\nAs part of this role, you will also be required to manage and administer the ARCS Account Attestation system which is a key control and reconciliation tool for the Financial Accounting team.\nYou will be required to build a good understanding of the CFP solution and associated systems to provide support and contribute to ongoing continuous improvement and change initiatives.\nYou will also contribute to other CFO Data Management activities and projects as required.\nKey Responsibilities\nMDM Operations - Maintenance of Chart of Accounts, Allocations Rules, CVRs and Mappings. MDM change activity - Chart of Accounts and mappings project work, process improvements and remediation activity (DRM) Data steward for Master Data (DRM) ARCS month end support & product ownership FIL Life reconciliation support (monthly) Regression testing of Oracle Quarterly Patches. Data Management Operations under CFO data program.\nSkills, Experience and Qualifications Required\nExperience of working in any Oracle ERP preferably Finance module (Oracle Cloud experience preferred, but experience with Oracle eBusiness Suite eBS -R12 acceptable)., knowledge of chart of accounts (CoAs). EPM systems and multi-dimensional reporting. Business Analysis and requirements writing for business process owner to be used by tech partners Finance background required, familiar with financial and management accounting (MA) concepts. Proven analytical skills, demonstrating accuracy and attention to detail. Good Microsoft office skills - PowerPoint, Project, Excel, Word Experience of data presentational tools such as PowerBI Strong presentation skills, both written & verbal Accounting or Management qualification highly desirable\nFeel rewarded\nFor starters, we ll offer you a comprehensive benefits package. We ll value your wellbeing and support your development. And we ll be as flexible as we can about where and when you work - finding a balance that works for all of us. It s all part of our commitment to making you feel motivated by the work you do and happy to be part of our team. For more about our work, our approach to dynamic working and how you could build your future here, .\nFor more about our work, our approach to dynamic working and how you could build your future here,",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Management accounting', 'Procurement', 'Business process', 'Data management', 'Business analysis', 'Reconciliation', 'JD Edwards', 'Workflow', 'Oracle financials', 'Continuous improvement']",2025-06-12 14:25:54
Snowflake Developer with Azure Data Factory,Net Connect,6 - 10 years,6-11 Lacs P.A.,['Hyderabad'],Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\n\nBelow JD for your reference\n\nJob Description:,,,,"['Azure Data Factory', 'Snowflake', 'SQL']",2025-06-12 14:25:56
Data Engineer 4,Comcast,5 - 11 years,Not Disclosed,['Chennai'],".\nResponsible for designing, building and overseeing the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs. Work with data modelers/analysts to understand the business problems they are trying to solve then create or augment data assets to feed their analysis. Integrates knowledge of business and functional priorities. Acts as a key contributor in a complex and crucial environment. May lead teams or projects and shares expertise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBachelors Degree\nWhile possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.\n7-10 Years\nComcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.",,,,"['Engineering services', 'Assurance', 'Process optimization', 'MySQL', 'Machine learning', 'Data structures', 'Data quality', 'Troubleshooting', 'Downstream', 'Python']",2025-06-12 14:25:58
Data Migration Expert - Talend,Maddisoft Solutions,10 - 20 years,Not Disclosed,['Hyderabad'],"Job Title: Data Migration Expert - Talend\nLocation: Hyderabad, India\n\nJob Description:\nMinimum of 9+ years of experience in data migration in Talend projects\nHandsons experience 5+ mandatory knowledge of Talend/SQL tools(Basic knowledge of SAP Routing and SAP Production order tables Individually is an add on)\nProficiency in data migration tools and methodologies, SAP ECC AND S/4HANA Migration Cockpit.\nHands-on experience Data Replication, Data Quality, Data Workbench, Talend or similar ETL tools.\nFamiliarity with Talend (ETL) is plus.\nStrong understanding of data modeling concepts, data mapping techniques, and data transformation rules.\nExcellent SQL skills for data extraction, manipulation, and analysis.\nExperience with SAP all Cross functional modules such as Finance (FI), Controlling (CO), Material Management (MM), Sales and Distribution (SD), or Production Planning (PP).\nStrong analytical, problem-solving, and troubleshooting skills.\nExcellent communication, presentation, and interpersonal skills.\nAbility to work independently and as part of a team in a fast-paced, dynamic environment.\nSAP certification in Data Migration or related field is a plus.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['SAP ECC', 'Data Migration', 'Talend']",2025-06-12 14:26:00
Technical Architect,XL India Business Services Pvt. Ltd,8 - 10 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","The Technical Architect role is the lead technologist for a product or platform\n\nThird party products with high out of box implementation most likely will have their own technical architecture roles, so company Technical Architects mainly manage internal assets\n\nTechnical Architects perform deliverable reviews and manage measurement of deliverable quality within the Delivery Team\n\nWhat you ll be DOING What will your essential responsibilities include? Specification of technologies, application architectures and data structures as a basis for application change for internal assets\n\nProducing quality, secure, scalable, high-performing, and resilient designs for new or improved services\n\nLead the systems analysts, developers, and testers in sympathetic change to the applications\n\nFor internal assets, support Application Managers to develop and maintain the Product Roadmap\n\nDefine and maintain development standards such as system and data design, coding, etc Maintain a capacity plan with historical performance metrics, a future forecast, and a capacity model to ensure services and infrastructure deliver performance and growth targets in a cost effective and proactive manner\n\nManage architecture exceptions for the application, including identifying, documenting, taking through exception approval process, and remediation where and when possible\n\nMonitor application services to ensure performance consistently meets non-functional requirements (response time, security, etc)\n\nWork with the Application Manager & Delivery Lead(s) in defining, analyzing, planning, measuring and improving product availability and continuity\n\nLeads the DevOps team and developers in targeted use of DevOps for their application platform assets\n\nYou will report to the Release Train Engineer\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: A moderate amount of experience with Guidewire Cloud and DevOps practices including CI/CD pipelines, infrastructure as code and containerization\n\nProven experience in designing complex, scalable architectures for large enterprise environments\n\nAbility to break down complex problems, evaluate multiple solutions & choose the most suitable outcome based on tradeoffs\n\nUphold technical integrity of internal assets at the logical and physical level, ensuring designs and changes done based on an outstanding architectural foundation\n\nDeep understanding of enterprise integration patterns (API, middleware and/or data migration)\n\nAbility to work closely with cross-functional teams, including developers, product owners, and operations to ensure alignment on technical goals and priorities\n\nDevelop, own and publish the product technical architecture and design documentation\n\nIdentify technical resources required to deliver the service and maintain plans for the short-, mid-, and long-term to support business cases for technical investments (upgrades, etc)\n\nTranslates the high-level designs into the technical specifications to facilitate efficient and effective development and unit testing\n\nDesired Skills and Abilities: Proficiency with multiple application delivery models including Agile, iterative and waterfall\n\nAdaptable to new/different strategies, programs, technologies, practices, cultures, etc Comfortable with change, able to easily make transitions\n\nAdvanced skills in developing tools, frameworks and processes intended to maximize software quality and minimize time-to-delivery\n\nBachelor s degree in the field of computer science, information systems, business management, or a related field preferred",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data migration', 'Coding', 'Agile', 'Data structures', 'Unit testing', 'software quality', 'Business strategy', 'Middleware', 'Analytics']",2025-06-12 14:26:03
IN Senior Associate AWS DataOps Engineer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary We are looking for a seasoned AWS DataOps Engineer\nResponsibilities\nDesign, implement, and manage scalable data pipelines and ETL processes on AWS. Collaborate with data teams to understand requirements and translate them into robust data solutions. Proven experience with AWS data services such as S3, Redshift, RDS, Glue, and Lambda. Strong understanding of data architecture, data modeling, and data warehousing concepts. Strong programming and scripting skills in languages like Python, SQL, or Shell scripting. Experience with data pipeline and ETL tools, such as Apache Airflow or AWS Data Pipeline. Ensure data quality, integrity, and security through automated testing, monitoring, and alerting systems. Optimize data storage and retrieval using AWS services such as S3, Redshift, RDS, and DynamoDB. Implement data governance and compliance standards to ensure data privacy and security. Automate data integration and deployment processes using tools like AWS Data Pipeline, Glue, and Step Functions. Monitor and troubleshoot data workflows to ensure reliability and performance. Provide technical support and guidance to data teams on best practices for data management and operations.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired\n48 Years\nEducation qualification BE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Master of Engineering, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nDevOps\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data management', 'Data modeling', 'Shell scripting', 'Database administration', 'Agile', 'Apache', 'Technical support', 'SQL', 'Python']",2025-06-12 14:26:05
IN Senior Associate AWS DataOps Engineer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary We are looking for a seasoned AWS DataOps Engineer\nResponsibilities\nDesign, implement, and manage scalable data pipelines and ETL processes on AWS. Collaborate with data teams to understand requirements and translate them into robust data solutions. Proven experience with AWS data services such as S3, Redshift, RDS, Glue, and Lambda. Strong understanding of data architecture, data modeling, and data warehousing concepts. Strong programming and scripting skills in languages like Python, SQL, or Shell scripting. Experience with data pipeline and ETL tools, such as Apache Airflow or AWS Data Pipeline. Ensure data quality, integrity, and security through automated testing, monitoring, and alerting systems. Optimize data storage and retrieval using AWS services such as S3, Redshift, RDS, and DynamoDB. Implement data governance and compliance standards to ensure data privacy and security. Automate data integration and deployment processes using tools like AWS Data Pipeline, Glue, and Step Functions. Monitor and troubleshoot data workflows to ensure reliability and performance. Provide technical support and guidance to data teams on best practices for data management and operations.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired\n48 Years\nEducation qualification BE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Master of Engineering, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nDevOps\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data management', 'Data modeling', 'Shell scripting', 'Database administration', 'Agile', 'Apache', 'Technical support', 'SQL', 'Python']",2025-06-12 14:26:08
Technical Architect,XL India Business Services Pvt. Ltd,5 - 10 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Technical Architect - Salesforce Platform Bangalore/Gurugram, India We are looking for a seasoned Technical Salesforce Platform Architect with expertise in design thinking, DevOps, CI/CD, Azure, API Integration, Data Streaming, and an effective background in commercial insurance\n\nAs a Technical Salesforce Platform Architect, you will lead the design and implementation of scalable solutions on the Salesforce platform while overseeing multiple solutions within a Salesforce org\n\nYou will collaborate closely with cross-functional teams in a scaled agile delivery setup, ensuring alignment with enterprise architecture principles and best practices\n\nWhat you ll be DOING What will your essential responsibilities include? Solution Design and Architecture: Lead the design and architecture of complex solutions on the Salesforce platform, leveraging design thinking methodologies to address business requirements effectively\n\nDefine architectural standards, patterns, and best practices for Salesforce development in alignment with enterprise architecture principles\n\nEnsure solutions are scalable, maintainable, and comply with industry regulations in the commercial insurance domain\n\nTechnical Leadership: Provide technical leadership and guidance to development teams, promoting DevOps practices and CI/CD pipelines for efficient delivery\n\nCollaborate with stakeholders to prioritize features and enhancements, ensuring alignment with business goals and objectives\n\nMentored and coached team members on Salesforce development, DevOps, and enterprise architecture concepts\n\nIntegration and Data Management: Design and implement API integrations with external systems, leveraging Azure services and other technologies as needed\n\nArchitects data streaming solutions for real-time analytics and insights, ensuring data accuracy, integrity, and security\n\nCollaborate with data architects to define data models and ensure proper data governance practices within the Salesforce Org\n\nScaled Agile Delivery: Work within a scaled agile framework, participating in program increment planning, backlog grooming, and sprint ceremonies\n\nFacilitate collaboration between agile teams, resolving dependencies and ensuring alignment with the overall program roadmap\n\nDrive continuous improvement initiatives to enhance delivery processes and optimize team performance\n\nSalesforce Org Oversight: Oversee multiple solutions within the Salesforce org, ensuring cohesiveness, scalability, and maintainability across different business units\n\nConduct regular reviews and audits of Salesforce configurations, identifying opportunities for optimization and enhancement\n\nPartner with Salesforce administrators to establish governance policies, security controls, and release management processes\n\nYou will report to the Platform Lead\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Bachelor s degree in computer science, Information Technology, or a related field\n\nProven experience as a Salesforce Technical Architect in enterprise environments, preferably within the commercial insurance industry\n\nExtensive experience in delivering solutions on Salesforce including Sales Cloud, Service Cloud and Financial Services Cloud\n\nEffective knowledge of Salesforce platform capabilities, including design thinking methodologies, DevOps practices, and CI/CD pipelines\n\nDesired Skills and Abilities: Hands-on experience with Azure services, API integration, and data streaming technologies\n\nFamiliarity with enterprise architecture frameworks such as TOGAF or Zachman\n\nExcellent communication, leadership, and collaboration skills\n\nSalesforce certifications such as Certified Technical Architect (CTA) and Certified Application Architect are a plus\n\nExperience working in scaled agile delivery setups, preferably SAFe\n\nExcellent problem-solving and troubleshooting skills\n\nProven tech leader and mentor, able and willing to share knowledge and experience in order to develop colleagues\n\nEffective communication and collaboration skills",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Enterprise architecture', 'Agile', 'Troubleshooting', 'Information technology', 'Release management', 'Analytics', 'Financial services', 'Salesforce']",2025-06-12 14:26:10
Data Engineer - R&D Data Catalyst Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role, you will be responsible for the end-to-end development of an enterprise analytics and data mastering solution using Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and impactful enterprise solutions that research cohort-building and advanced research pipeline. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be extraordinarily skilled with data analysis and profiling.\nYou will collaborate closely with key customers, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a good background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with key customers to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The R&D Data Catalyst Team is responsible for building Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nBasic Qualifications:\nMasters degree and 1 to 3 years of Data Engineering experience OR\nBachelors degree and 3 to 5 years of Data Engineering experience OR\nDiploma and 7 to 9 years of Data Engineering experience\nMust Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3 years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood to Have Skills:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nThe highest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, remote teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data management', 'Power BI', 'data governance', 'data warehousing', 'Databricks', 'ETL', 'AWS']",2025-06-12 14:26:13
"Technical Architect, AVP",XL India Business Services Pvt. Ltd,13 - 20 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Technical Architect Bangalore, Karnataka, India We invent the new to help the world move forward\n\nCombining powerful analytics and deeper insights with bigger ideas and innovative solutions, we free up our clients potential, thereby fulfilling our own\n\nTake it seriously\n\nMake it fun\n\nKnow it matters\n\nThe Technical Architect role is the lead technologist for a product or platform\n\nThird party products with high out of box implementation most likely will have their own technical architecture roles, so company Technical Architects mainly manage internal assets\n\nTechnical Architects perform deliverable reviews and manage measurement of deliverable quality within the Delivery Team\n\nWhat you ll be DOING What will your essential responsibilities include? Specification of technologies, application architectures and data structures as a basis for application change for internal assets\n\nProducing quality, secure, scalable, high-performing, and resilient designs for new or improved services on both OnPrem and Cloud platform\n\nLead the systems analysts, developers, and testers in sympathetic change to the applications\n\nFor internal assets, support Application Managers to develop and maintain the Product Roadmap\n\nDefine and maintain development standards such as system and data design, coding, etc Maintain a capacity plan with historical performance metrics, a future forecast, and a capacity model to ensure services and infrastructure deliver performance and growth targets in a cost effective and proactive manner\n\nManage architecture exceptions for the application, including identifying, documenting, taking through exception approval process, and remediation where and when possible\n\nMonitor application services to ensure performance consistently meets non-functional requirements (response time, security, etc)\n\nManage AXA XL security standards for the applications, including clean code, vulnerability identification and remediation, penetration test etc Work with the Application Manager & Delivery Lead(s) in defining, analyzing, planning, measuring and improving product availability and continuity\n\nLeads the DevOps team and developers in targeted use of DevOps for their application platform assets\n\nYou will report to the Delivery Lead - Claims\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Proven experience in Microsoft technical suit like Dot Net (Core, Standard), SQL Server, C#, VBDot Net, ASPDot Net\n\nDeep understanding of enterprise integration patterns (API, middlewear and/or data migration, secured data flow)\n\nA moderate amount of Cloud based experience with Azure\n\nDevOps practices including CI/CD pipelines, infrastructure as code and containerization\n\nProven experience in designing complex, scalable architectures for large enterprise environments\n\nAbility to break down complex problems, evaluate multiple solutions & choose the most suitable outcome based on tradeoffs\n\nUphold technical integrity of internal assets at the logical and physical level, ensuring designs and changes done based on an outstanding architectural foundation\n\nAbility to work closely with cross-functional teams, including developers, product owners, and operations to ensure alignment on technicals goals and priorities\n\nDevelop, own and publish the product technical architecture and design documentation\n\nIdentify technical resources required to deliver the service and maintain plans for the short-, mid-, and long-term to support business cases for technical investments (upgrades, etc)\n\nTranslates the high-level designs into the technical specifications to facilitate efficient and effective development and unit testing\n\nDesired Skills and Abilities: Proficiency with multiple application delivery models including Agile, iterative and waterfall\n\nAdaptable to new/different strategies, programs, technologies, practices, cultures, etc Comfortable with change, able to easily make transitions\n\nAdvanced skills in developing tools, frameworks and processes intended to maximize software quality and minimize time-to-delivery\n\nBachelor s degree in the field of computer science, information systems, business management, or a related field preferred\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides competitive compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data migration', 'Coding', 'Agile', 'Data structures', 'Unit testing', 'software quality', 'microsoft', 'Analytics', 'SQL']",2025-06-12 14:26:15
IN Senior Associate AWS AI/ML/GenAI Developer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\n& Summary We are looking for a seasoned AWS\nAI/ML/GenAI Developer\nResponsibilities\nDesign and implement AI/ML/GenAI models using AWS services such as AWS Bedrock, SageMaker, Comprehend, Rekognition, and others.\nStrong programming skills in Python, R etc\nExperience with machine learning frameworks such as TensorFlow, PyTorch, or Scikitlearn.\nKnowledge of data preprocessing, feature engineering, and model evaluation techniques.\nDevelop and deploy generative AI solutions to solve complex business problems and improve operational efficiency.\nCollaborate with data scientists, engineers, and product teams to understand requirements and translate them into technical solutions.\nOptimize and finetune machine learning models for performance and scalability. Ensure the security, reliability, and scalability of AI/ML solutions by adhering to best practices.\nMaintain and update existing AI/ML models to ensure they meet evolving business needs.\nStay uptodate with the latest advancements in AI/ML and GenAI technologies and integrate relevant innovations into our solutions.\nProvide technical guidance and mentorship to junior developers and team members.\nExcellent problemsolving skills and ability to work in a fastpaced, collaborative environment.\nGood to have AWS Certified Machine Learning Specialty or other relevant AWS certifications.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired 48 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Bachelor of Engineering, Bachelor of Technology\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'GCP', 'Data modeling', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Corporate advisory', 'Operations', 'AWS']",2025-06-12 14:26:18
Architect,Trianz,12 - 17 years,Not Disclosed,['Bengaluru'],"Role Snowflake Architect\nJob Summary -\nAs a Data Architect, you are core to the D&AI (Data & AI) Practices success. Data is foundational to everything we do, and you are accountable for defining and delivering best-in-class Snowflake data management solutions across all major cloud platforms. This is a senior role with high visibility and reporting to the D&AI Practice Tower Lead.\nJob Responsibilities\nArchitectural Design: Architect secure, scalable, highly performant data engineering and management solutions, including data warehouses, data lake, ELT / ETL and real-time data engineering / pipeline solutions. Support Principal Data Architect in defining and maintaining Practice reference data engineering and data management architectures.\nSnowflake Implementation: Design and manage scalable end-to-end data solutions leveraging native Snowflake workloads including : Data Engineering; Data Lake; Data Warehouse; Applications; Unistore; AI/ML; Governed Collaboration, Marketplace, Streamlit.\nHyperscaler Design: Competently leverage data-related cloud platform (AWS or Azure) capabilities to architect and develop end-to-end data engineering and data management solutions.\nClient Engagement: Regular collaboration and partnership with clients to understand their challenges and needs then translate requirements into data solutions that drive customer value. Support proposal development.\nData Modeling: Create and maintain conceptual, logical, and physical data models that support both transactional and analytical needs. Ensure data models are optimized for performance and scalability.\nCreativity: Be an out-of-the-box thinker and passionate about applying your skills to new and existing solutions alike while always demonstrating a customer-first mentality.\nMandatory Skills\n12+ years hands-on data solution architecture and implementation experience on modern cloud platforms (AWS preferred) including microservice and event-driven architectures.\nSnowflake SnowPro Advanced Architect certification. An architectural certification on either AWS, Azure or GCP.\nHands-on experience with Snowflake capabilities including Snowpipe, Snowpark, Cortex, Polaris Catalog, native applications, Notebooks, Horizon, Marketplace, Streamlit.\nPractical experience with end-to-end data engineering and data management supporting functions including data modeling (conceptual, logical & physical), BI & analytics, data governance, data quality, data security / privacy / compliance, IAM, performance optimization. Advanced SQL and data profiling.\nPython, Scala or Java. Strong communication skills with the ability to convey technical concepts to non-technical users.\nStrong, self-management skills demonstrating ability to multitask and self-manage goals and activities.\nAdditional / Nice-to-have Qualifications-\nSnowflake SnowPro Advanced Data Engineer certification Snowflake SnowPro Advanced Data Scientist certification Snowflake SnowPro Advanced Administrator certification Snowflake SnowPro Advanced Data Analyst certification\nRequired Education Master or Bachelor (CS, IT, Applied Mathematics or demonstrated experience)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'Architect', 'Data management', 'Data modeling', 'data security', 'Analytical', 'Data quality', 'Analytics', 'SQL', 'Python']",2025-06-12 14:26:20
Lead/Solution Architect: Java Full Stack,Omega Healthcare,10 - 15 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role : Tech Lead\nEducational Qualification : ME / BE / MCA / MSc\nExperience Required : 10+ Years\nShifts : Day shift\nResponsibilities:\n10+ years of expertise working in Core Java,J2EE, Design Patterns, Spring, Spring Boot, Micro service architecture\nExpertise in Web services; SOAP and REST\nExperience with at least one UI technology like Angular, React JS, OJET\nGood knowledge in Postgres/ MySQL, Advanced PL SQL with JSON data management\nBuild tools like Ant, Maven and Gradle\nExperience in Azure Repos/SVN/ Git source control tool is must\nSolid understanding of Software development life cycle and OOPS concept\nWorking with APIs/Integrations is must\nExperience working with JDK 17 + is preferred\nExcellent understanding of architectural principles involved in SaaS and multi-tenant platforms\nStrong in development tools like Eclipse with SDS, IntelliJ, Git, Cradle, Sonar, Jenkins, Jira/ADO/ Artifactory\nORM technologies like Hibernate\nExperience with Kubernetes and Dockers is preferred\nExperience of messaging systems and data pipelines such as RabbitMQ and Kafka is preferred\nExperience using other API Management solutions like Apigee\nWell versed with scalability, automation, resiliency, high availability and user experience\nExperience in cloud computing application implementations on AWS is preferred\nStrong background in creating secure cloud architectures for customer facing applications that are enterprise grade and highly scalable is strongly preferred\nExperience in Agile, DevOps culture is a plus\nShould have managed a team of SSE/SE and lead at least 2 projects\nGood communication skills",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'cloud integration', 'solution architecture', 'Java Architecture', 'data architecture', 'Angular', 'application development']",2025-06-12 14:26:23
Teamcenter Architect,We are hiring for well knowned MNC,8 - 13 years,20-35 Lacs P.A.,['Bengaluru'],"10+ years of experience in Teamcenter PLM implementations, with at least 3+ years as a Technical Architect.\nLead end-to-end Teamcenter PLM technical architecture and deployment strategy.\nDefine and implement Teamcenter solutions including data modeling, workflow customization, and BMIDE configurations.\nCollaborate with business stakeholders to gather requirements and propose scalable Teamcenter solutions.\nDrive architectural decisions, performance tuning, and capacity planning.\nOversee Teamcenter installations, upgrades, environment setup, clustering, and disaster recovery planning.\nSupport Teamcenter integrations with CAD (NX, SolidWorks, CATIA), ERP, and other enterprise systems.\nConduct code reviews and provide governance for customizations, extensions, and data migration strategies.\nDeep expertise in:\nTeamcenter architecture, ITK, RAC, SOA, and Active Workspace.\nBMIDE, Workflows, Custom Handlers, ACLs, and Stylesheets.\nDeployment on multi-tier architecture and cloud platforms (optional but preferred).",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Temporary/Contractual","['PLM', 'ITK', 'Architect', 'Bmide', 'SOA', 'RAC', 'Awc 1', 'Architecting']",2025-06-12 14:26:25
Solution Architect,Hitachi Vantara,10 - 20 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Job Title: Senior Technical Consultant / Solution Architect Storage & Content Technologies\n\nJob Summary:\n\nWe are looking for a client-facing Technical Consultant or Solution Architect with strong leadership and problem-solving skills. You will act as a key advisor to both internal and customer technical teams, providing expert guidance on system architecture, implementation techniques, troubleshooting, and solution development. This role requires active engagement in architectural planning, change management, and team coordination, while also contributing to client growth opportunities through technical insight and solution proposals.",,,,"['Storage', 'Infrastructure', 'Presales', 'Solutioning']",2025-06-12 14:26:28
Job opening For Data Warehouse + ADF + ETL,bct,3 - 6 years,Not Disclosed,['Pune'],"Greetings of the Day !!!\n\nWe have job opening for Data Warehouse + ADF + ETL with one of our Client .If you are interested for this role , kindly share update resume along with below details in this email id : shaswati.m@bct-consulting.com\n\nJob Description:\nSenior Data Engineer\nAs a Senior Data Engineer, you will support the European World Area using the Windows & Azure suite of Analytics & Data platforms. The focus of the role is on the technical aspects and implementation of data gathering, integration and database design.\nWe look forward to seeing your application!\nIn This Role, Your Responsibilities Will Be:\nData Ingestion and Integration: Collaborate with Product Owners and analysts to understand data requirements & design, develop, and maintain data pipelines for ingesting, transforming, and integrating data from various sources into Azure Data Services.\nMigration of existing ETL packages: Migrate existing SSIS packages to Synapse pipelines\nData Modelling: Assist in designing and implementing data models, data warehouses, and databases in Azure Synapse Analytics, Azure Data Lake Storage, and other Azure services.\nData Transformation: Develop ETL (Extract, Transform, Load) processes using SQL Server Integration Services (SSIS), Azure Synapse Pipelines, or other relevant tools to prepare data for analysis and reporting.\nData Quality and Governance: Implement data quality checks and data governance practices to ensure the accuracy, consistency, and security of data assets.\nMonitoring and Optimization: Monitor and optimize data pipelines and workflows for performance, scalability, and cost efficiency.\nDocumentation: Maintain comprehensive documentation of processes, including data lineage, data dictionaries, and pipeline schedules.\nCollaboration: Work closely with cross-functional teams, including data analysts, data scientists, and business stakeholders, to understand their data needs and deliver solutions accordingly.\nAzure Services: Stay updated on Azure data services and best practices to recommend and implement improvements in our data architecture and processes\nFor This Role, You Will Need:\n3-5 years of experience in Data Warehousing with On-Premises or Cloud technologies\nStrong practical experience of Synapse pipelines / ADF.\nStrong practical experience of developing ETL packages using SSIS.\nStrong practical experience with T-SQL or any variant from other RDBMS.\nGraduate degree educated in computer science or a relevant subject.\nStrong analytical and problem-solving skills.\nStrong communication skills in dealing with internal customers from a range of functional areas.\nWillingness to work flexible working hours according to project requirements.\nTechnical documentation skills.\nFluent in English.\nPreferred Qualifications that Set You Apart:\nOracle PL/SQL.\nExperience in working on Azure Services like Azure Synapse Analytics, Azure Data Lake.\nWorking experience with Azure DevOps paired with knowledge of Agile and/or Scrum methods of delivery.\nLanguages: French, Italian, or Spanish would be an advantage.\nAgile certification.\nThanks,\nShaswati",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ADF', 'ETL', 'SSIS', 'Data ware house']",2025-06-12 14:26:30
Data Engineer,Databeat,3 - 7 years,Not Disclosed,['Hyderabad( Rai Durg )'],"Experience Required: 3+ years\n\nTechnical knowledge: AWS, Python, SQL, S3, EC2, Glue, Athena, Lambda, DynamoDB, RedShift, Step Functions, Cloud Formation, CI/CD Pipelines, Github, EMR, RDS,AWS Lake Formation, GitLab, Jenkins and AWS CodePipeline.\n\n\n\nRole Summary: As a Senior Data Engineer,with over 3 years of expertise in Python, PySpark, SQL to design, develop and optimize complex data pipelines, support data modeling, and contribute to the architecture that supports big data processing and analytics to cutting-edge cloud solutions that drive business growth. You will lead the design and implementation of scalable, high-performance data solutions on AWS and mentor junior team members.This role demands a deep understanding of AWS services, big data tools, and complex architectures to support large-scale data processing and advanced analytics.\nKey Responsibilities:\nDesign and develop robust, scalable data pipelines using AWS services, Python, PySpark, and SQL that integrate seamlessly with the broader data and product ecosystem.\nLead the migration of legacy data warehouses and data marts to AWS cloud-based data lake and data warehouse solutions.\nOptimize data processing and storage for performance and cost.\nImplement data security and compliance best practices, in collaboration with the IT security team.\nBuild flexible and scalable systems to handle the growing demands of real-time analytics and big data processing.\nWork closely with data scientists and analysts to support their data needs and assist in building complex queries and data analysis pipelines.\nCollaborate with cross-functional teams to understand their data needs and translate them into technical requirements.\nContinuously evaluate new technologies and AWS services to enhance data capabilities and performance.\nCreate and maintain comprehensive documentation of data pipelines, architectures, and workflows.\nParticipate in code reviews and ensure that all solutions are aligned to pre-defined architectural specifications.\nPresent findings to executive leadership and recommend data-driven strategies for business growth.\nCommunicate effectively with different levels of management to gather use cases/requirements and provide designs that cater to those stakeholders.\nHandle clients in multiple industries at the same time, balancing their unique needs.\nProvide mentoring and guidance to junior data engineers and team members.\n\n\n\nRequirements:\n3+ years of experience in a data engineering role with a strong focus on AWS, Python, PySpark, Hive, and SQL.\nProven experience in designing and delivering large-scale data warehousing and data processing solutions.\nLead the design and implementation of complex, scalable data pipelines using AWS services such as S3, EC2, EMR, RDS, Redshift, Glue, Lambda, Athena, and AWS Lake Formation.\nBachelor's or Masters degree in Computer Science, Engineering, or a related technical field.\nDeep knowledge of big data technologies and ETL tools, such as Apache Spark, PySpark, Hadoop, Kafka, and Spark Streaming.\nImplement data architecture patterns, including event-driven pipelines, Lambda architectures, and data lakes.\nIncorporate modern tools like Databricks, Airflow, and Terraform for orchestration and infrastructure as code.\nImplement CI/CD using GitLab, Jenkins, and AWS CodePipeline.\nEnsure data security, governance, and compliance by leveraging tools such as IAM, KMS, and AWS CloudTrail.\nMentor junior engineers, fostering a culture of continuous learning and improvement.\nExcellent problem-solving and analytical skills, with a strategic mindset.\nStrong communication and leadership skills, with the ability to influence stakeholders at all levels.\nAbility to work independently as well as part of a team in a fast-paced environment.\nAdvanced data visualization skills and the ability to present complex data in a clear and concise manner.\nExcellent communication skills, both written and verbal, to collaborate effectively across teams and levels.\n\nPreferred Skills:\nExperience with Databricks, Snowflake, and machine learning pipelines.\nExposure to real-time data streaming technologies and architectures.\nFamiliarity with containerization and serverless computing (Docker, Kubernetes, AWS Lambda).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'SQL', 'Data Pipeline', 'Python', 'Amazon Ec2', 'Data Engineering', 'Data Bricks', 'Aws Lambda', 'Amazon Redshift', 'Azure Cloud', 'Data Lake', 'Data Modeling', 'Athena']",2025-06-12 14:26:32
Solution Architect - Java,Epam Systems,10 - 13 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","We are looking for a Solution Architect - Java to join our team.\nYour expertise will shape the design of scalable software solutions. You will leverage your extensive experience in solution architecture and microservice architecture styles to drive innovation. If you are ready to take your career to the next level, we encourage you to apply.\n\nResponsibilities\nDesign secure, reliable, high availability, scalable solutions for the program\nDefine, plan, and support execution of the technology strategy for one or more products\nCollaborate closely with the global solution architecture and engineering team to define principles and best practices\nEngage with wider architecture and technology teams to ensure alignment with technical strategies and policies\nSupport development teams and work with stakeholders, promoting agile development\nCreate a culture of technical excellence and continuous improvement\nResearch, create, and evaluate technical solution alternatives for business needs using current and upcoming technologies\nDrive overall software implementation using expertise in microservices-based architectures for the fintech industry\nPartner with senior technical and product leaders to deliver on designs\nCollaborate with development teams, operations, and product owners\nProvide technical leadership and mentorship to development teams\nRepresent as the primary architect and technical advocate in program discussions\nRequirements\nExperience in product engineering with over 15 years in designing scalable software solutions\nBackground in computer science fundamentals, web applications, and microservices-based software architecture\nExperience with high transaction volume financial systems operating at global scale\nKnowledge of web technologies including HTML5, CSS, and JavaScript, along with front-end frameworks like AngularJS and ReactJS\nProficiency in designing and building back-end microservices using Java and Spring frameworks\nUnderstanding of storage technologies such as PostgreSQL and SQL Server for large-scale applications\nFamiliarity with cloud-native technologies and best practices, including Amazon Web Services and Microsoft Azure\nCapability to work effectively in an Agile environment focused on continuous improvement\nDesire to collaborate and provide mentorship to technology teams\nHands-on experience in building prototypes to solve complex business problems\nEnglish proficiency at a professional level",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['css', 'technical leadership', 'web services', 'web application', 'microservices', 'sql', 'spring', 'react.js', 'java', 'postgresql', 'computer science', 'gcp', 'design', 'software solutions', 'end', 'html', 'architecture', 'microsoft azure', 'javascript', 'sql server', 'high availability', 'framework', 'web technologies', 'agile', 'aws', 'angularjs']",2025-06-12 14:26:34
Lead Data Engineer,Prolegion,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Lead Data Engineer/Associate Architect to lead the design, implementation, and optimization of scalable data architectures. The ideal candidate will have a deep understanding of data modeling, ETL processes, cloud data solutions, and big data technologies. You will work closely with cross-functional teams to build robust, high-performance data pipelines and infrastructure to enable data-driven decision-making.\n\nExperience: 8 - 12+ years\nWork Location: Hyderabad (Hybrid)\nMandatory skills: Python, SQL, Snowflake\n\nResponsibilities:\nDesign and Develop scalable and resilient data architectures that support business needs, analytics, and AI/ML workloads.\nData Pipeline Development: Design and implement robust ETL/ELT processes to ensure efficient data ingestion, transformation, and storage.\nBig Data & Cloud Solutions: Architect data solutions using cloud platforms like AWS, Azure, or GCP, leveraging services such as Snowflake, Redshift, BigQuery, and Databricks.\nDatabase Optimization: Ensure performance tuning, indexing strategies, and query optimization for relational and NoSQL databases.\nData Governance & Security: Implement best practices for data quality, metadata management, compliance (GDPR, CCPA), and security.\nCollaboration & Leadership: Work closely with data engineers, analysts, and business stakeholders to translate business requirements into scalable solutions.\nTechnology Evaluation: Stay updated with emerging trends, assess new tools and frameworks, and drive innovation in data engineering.\n\nRequired Skills:\nEducation: Bachelors or Masters degree in Computer Science, Data Engineering, or a related field.\nExperience: 8 - 12+ years of experience in data engineering\nCloud Platforms: Strong expertise in AWS data services.\nBig Data Technologies: Experience with Hadoop, Spark, Kafka, and related frameworks.\nDatabases: Hands-on experience with SQL, NoSQL, and columnar databases such as PostgreSQL, MongoDB, Cassandra, and Snowflake.\nProgramming: Proficiency in Python, Scala, or Java for data processing and automation.\nETL Tools: Experience with tools like Apache Airflow, Talend, DBT, or Informatica.\nMachine Learning & AI Integration (Preferred): Understanding of how to architect data solutions for AI/ML applications\n\n,",Industry Type: Defence & Aerospace,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'Data modeling', 'Postgresql', 'Informatica', 'Apache', 'Analytics', 'SQL', 'Python']",2025-06-12 14:26:37
SAP DATA Steward,Maddisoft Solutions,10 - 20 years,Not Disclosed,['Hyderabad'],"Job Title: SAP DATA STEWARD (CONTRACT)\nLocation: HYDERABAD, INDIA\n\nAs the SAP Data Steward is responsible for creating, maintaining, and deactivating master data and data attributes in SAP with focus on data migration. The Data Stewards has an essential role in establishing in monitoring existing and new data against and ensuring timely and high-quality creation of new data in the system.\nBachelors Degree or Associates degree with additional 9+ years of work experience required or an equivalent combination of education and experience.\nRequires SAP functional knowledge on SAP Routings with Migration Perspective.\nShould have complete knowledge on SAP Routings tables. Need to have basic knowledge on linking between the tables and joins needed for getting the extract template ready as per Client's Format.\nExcellent attention to detail, exceptional interest in creating order and consistency required.\n10+ years of experience in data management and/or data governance activities and responsibilities.\nExperience working with SAP ECC required.\nDemonstrated expert-level experience and capability with MS Excel required.\nHigh degree of initiative and ownership, as well as a proven history of delivering results while working with several different departments in a fast-paced environment required.\nExperience creating and running business reports and data queries is preferred.\nConfident user of Microsoft Office (Word, Excel, Outlook, PowerPoint, Teams).\nExperience working with teams across multiple functions.\nAbility to multi-task and work under tight timelines required.\nExcellent communication skills both verbal and written.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['SAP ECC', 'Data Stewardship', 'Data Management', 'Data Governance', 'SAP Routing']",2025-06-12 14:26:39
SAP MDG Architect,Akana Services,10 - 15 years,Not Disclosed,[],"Key Responsibilities:\n*Good understanding of SAP MDG technical framework includes BADI, BAPI/RFC/FM, Workflows, BRF+, Enterprise Services, IDoc, Floorplan Manager, WebDynPro, Fiori, and MDG API framework, Workflows, AMDP, CDS, BRF+, BOPF.\n*Should have knowledge working with REST API, SOAP API, ODATA. SAP AIF Application Interface Framework, Inapp extensibility.\n*Knowledge of SAP data dictionary tables, views, relationships, and corresponding data architecture for ECC and S/4 HANA for various SAP master.\n*Implementation experience of SAP MDG in key domains such as Customer, Supplier, Material, Business Partners.\nRole Requirements and Qualifications:\n*10+ years of SAP experience, with at least 5 years focused on SAP MDG architecture and implementation.\n*Proven experience with multiple end-to-end SAP MDG implementations.\n*Deep understanding of SAP MDG Central Governance, Consolidation, and Mass Processing.\n*Expertise in MDG data modeling, BRF+ rules, workflows, FPM, and UI configuration.\n*Strong integration knowledge using SOA services, IDocs, and MDG Data Replication Framework.\n*Experience with SAP S/4HANA, SAP Fiori, and MDG on S/4HANA.\n*Solid understanding of data governance frameworks and master data management principles.\n*Ability to assess current master data landscapes and propose future-state architecture.\n*Excellent communication, leadership, and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['SAP Fiori', 'Sap Mdg', 'SAP S/4HANA']",2025-06-12 14:26:41
Sap Hybris Architect,Pepplo Consulting,14 - 20 years,40-45 Lacs P.A.,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Minimum 12 years of experience in IT, with proven expertise in at least 2 large-scale projects (More than 10 lakh users for a B2C application) involving system design & deployment of enterprise architecture, data architecture & cloud implementations.\n\nRequired Candidate profile\nKnowledge on Hybris ORM/WCMS/Backoffice/Cockpits/SOLR search engine\nExpert in Hybris B2B/B2C Accelerators, Hybris Workflow/Task Mgmt\nExpert in the catalog/order management/promotions/vouchers/coupons",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spring Mvc', 'Sap Hybris Commerce', 'Javascript', 'Html/Css', 'GIT', 'JSP', 'J2Ee', 'SVN', 'JQuery']",2025-06-12 14:26:43
Mobile B2C BSS OSS Solution Architect Service fulfilment,Prodapt Solutions,5 - 10 years,Not Disclosed,['Bengaluru'],"Overview\n\n*Need Telecom Solution Architect/Digital Solution Designer with telecom domain experience.\n*Should have handson in Lead to Cash journey - Processes and Systems for Cellular Mobile service for Retail customers\n*Should have solid understanding on different Service Fulfilment journey metrics.\n*Should have solid understanding of different BSS/OSS applications that support Lead to Cash journey.\n*Should have strong understanding of different Channels, Product Catalog, Order Orchestration, Provisioning & Activation\n*Should have conducted workshops to gather requirements from stakeholders, presenting different solution options to architects.\n*Should have expertise in:\n**Designing multi-channel Enterprise solutions among BSS domains CRM, Order Management, BPM, OmniChannel implementations, TMForum Open APIs , Catalog, Intelligence platforms etc\n**Knowledge on Event/Data driven architecture, micro service framework and API integration\n**Experience in designing enterprise systems in BSS\n\nResponsibilities\n\nMandatory to have excellent understanding of Telecom Domain, and experience of Consulting / Solution Design / Solution Architecture for Telecom Solutions (Products and Services development).\nAbility to analyze Telco processes, simplify them and develop solutions in alignment with architecture principles.\nExperience in working with business and application teams to collect requirements and deliver solutions.\nExperience in BSS/OSS is must, at least 5+ Years.\nUnderstands Open API or TMF API specification or API based integration with partners\nDefine L2/L3 data model using reference data architecture like SID\nDeveloping micro services-based architecture\nTMF - open digital architecture or similar framework to develop channels architecture\nExperience of using modelling tools for architecture development\nAs a Telecom Domain expert, you will be required to seek out the best ways to improve business processes and effectiveness through technology, strategy, analytic solutions.\nBrings in strong knowledge in the telecom domain, and having working experience across Business and Technology Roles, across B2B and B2C businesses.\nExperience with Business Analysis, Business Process Re-engineering, E2E Solution Design / Component or System Design / Solution Architecture, in Telecom OSS / BSS domain is required.\n\nGood to have working experience on Digital Transformation, OSS/BSS Cloud Implementations, Network Transformation programs.\nUnderstanding of Technology Landscape such as E2E OSS & BSS, Microservices based Architecture, RPA, Service Assurance, Billing, and Invoicing for B2C business.\nShould have a business focus, and an equal understanding of IT/Technology enablement, so as to be able to align the business goals/objectives to the IT delivery.\nGood experience in data modelling and integration patterns.\nHas good understanding of frameworks relevant for Telecommunications domain (for e.g. eTOM, SID, TAM, ITIL, ODA)\nAble to understand and translate business requirements, and working with product / technical and operations teams to help design and build new Products and Services\nAble to conduct workshops and design thinking sessions, to understand IT Process / Technology / Architecture and existing OSS/BSS implementations.\nAble to deliver intelligent business insights through translation of reports and analytics.\nUnderstanding of technologies such as 4G/5G/Metro Ethernet/ATM/MPLS/SONET/SDH etc. and their mapping to OSS systems is an added advantage.\nConduct proof-of-concept (POCs) for requirements.\nGood communication and Presentation skills.\n\nGood to have\nUnderstanding of autonomous networks and closed loop automation\nKnowledge of Architecture design methodologies and Industry reference frameworks â€“ TOGAF etc. â€“ with a clear demonstration of the deliverables created as a result of applying the framework\nKnowledge of 5G & IoT\nExperience in any of the COTS products\nExperience in Network optimization techniques\nUnderstanding of intelligent applying across OSS land scape\nExperience of working on network technologies\nKnowledge of DevOps and open-source tools\nExperience in any one of the programming languages - Java or Python\nExperience in any one of the DB technologies - Oracle, MySQL, Postgres, Cassandra\nWorking with development teams and product managers to build software solutions/web applications\n\n\nBachelor's degree (in any field) is mandatory.\nMSc/BE /Masters with specialization in IT/Computer Science is desired.\nAtleast 5 years of work experience.\n\nAdaptability and experience working in multi-channel delivery projects is preferred.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['product catalog', 'bss', 'open source', 'telecom', 'provisioning', 'python', 'etom', 'oracle', 'togaf', 'business analysis', 'solution delivery', 'data architecture', 'tmf', 'microservices', 'java', 'postgresql', 'solution design', 'cassandra', 'devops', 'open api', 'e2e', 'mysql', 'api', 'digital transformation']",2025-06-12 14:26:46
Mobile B2C BSS OSS Solution Architect Service Assurance,Prodapt Solutions,5 - 10 years,Not Disclosed,['Bengaluru'],"Overview\n\n*Need Telecom Solution Architect/Digital Solution Designer with telecom domain experience.\n*Should have hands-on in Service Assurance journey - Processes and Systems for Cellular Mobile service for Retail customers\n*Should have solid understanding on different Service Assurance journey metrics.\n*Should have solid understanding of different BSS OSS applications that support Service Assurance Journey\n*Should have solid understanding of Billing & Rating\n*Should have conducted workshops to gather requirements from stakeholders, presenting different solution options to architects.\n*Should have expertise in:\n**Designing multi-channel Enterprise solutions among BSS/OSS domains with in-depth knowledge in Network monitoring system, Fault Orchestration/Incident Management system, Test & Diagnostics system, Billing & Rating system, Provisioning & Activation system, Service Inventory, CRM, Field Engineering Management system, OmniChannel implementations, Network performance management system, TMForum Open APIs, Intelligence platforms etc\n**Knowledge on Event/Data driven architecture, micro service framework and API integration\n**Experience in designing enterprise systems in BSS/OSS\n\nResponsibilities\n\nMandatory to have excellent understanding of Telecom Domain, and experience of Consulting / Solution Design / Solution Architecture for Telecom Solutions (Products and Services development).\nAbility to analyze Telco processes, simplify them and develop solutions in alignment with architecture principles.\nExperience in working with business and application teams to collect requirements and deliver solutions.\nExperience in BSS/OSS is must, at least 5+ Years.\nUnderstands Open API or TMF API specification or API based integration with partners\nDefine L2/L3 data model using reference data architecture like SID\nDeveloping micro services-based architecture\nTMF - open digital architecture or similar framework to develop channels architecture\nExperience of using modelling tools for architecture development\nAs a Telecom Domain expert, you will be required to seek out the best ways to improve business processes and effectiveness through technology, strategy, analytic solutions.\nBrings in strong knowledge in the telecom domain, and having working experience across Business and Technology Roles, across B2B and B2C businesses.\nExperience with Business Analysis, Business Process Re-engineering, E2E Solution Design / Component or System Design / Solution Architecture, in Telecom OSS / BSS domain is required.\n\nGood to have working experience on Digital Transformation, OSS/BSS Cloud Implementations, Network Transformation programs.\nUnderstanding of Technology Landscape such as E2E OSS & BSS, Microservices based Architecture, RPA, Service Assurance, Billing, and Invoicing for B2C business.\nShould have a business focus, and an equal understanding of IT/Technology enablement, so as to be able to align the business goals/objectives to the IT delivery.\nGood experience in data modelling and integration patterns.\nHas good understanding of frameworks relevant for Telecommunications domain (for e.g. eTOM, SID, TAM, ITIL, ODA)\nAble to understand and translate business requirements, and working with product / technical and operations teams to help design and build new Products and Services\nAble to conduct workshops and design thinking sessions, to understand IT Process / Technology / Architecture and existing OSS/BSS implementations.\nAble to deliver intelligent business insights through translation of reports and analytics.\nUnderstanding of technologies such as 4G/5G/Metro Ethernet/ATM/MPLS/SONET/SDH etc. and their mapping to OSS systems is an added advantage.\nConduct proof-of-concept (POCs) for requirements.\nGood communication and Presentation skills.\n\nGood to have\nUnderstanding of autonomous networks and closed loop automation\nKnowledge of Architecture design methodologies and Industry reference frameworks â€“ TOGAF etc. â€“ with a clear demonstration of the deliverables created as a result of applying the framework\nKnowledge of 5G & IoT\nExperience in any of the COTS products\nExperience in Network optimization techniques\nUnderstanding of intelligent applying across OSS land scape\nExperience of working on network technologies\nKnowledge of DevOps and open-source tools\nExperience in any one of the programming languages - Java or Python\nExperience in any one of the DB technologies - Oracle, MySQL, Postgres, Cassandra\nWorking with development teams and product managers to build software solutions/web applications\n\n\nBachelor's degree (in any field) is mandatory.\nMSc/BE /Masters with specialization in IT/Computer Science is desired.\nAtleast 5 years of work experience.\n\nAdaptability and experience working in multi-channel delivery projects is preferred.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['bss', 'service assurance', 'software testing', 'billing', 'telecom', 'python', 'engineering management', 'field engineering', 'performance management system', 'open source', 'network monitoring', 'network performance', 'java', 'postgresql', 'incident management', 'provisioning', 'mysql', 'crm']",2025-06-12 14:26:48
Senior Business Analyst,ANZ,5 - 10 years,Not Disclosed,['Bengaluru'],"The Senior Business Analyst role will provide business analysis expertise to help deliver complex initiatives. The purpose of this role is to:\nWork in an agile and collaborative team, working with a range of business and technical stakeholders to deliver outcomes and value for customers.\nApply business analysis skills, techniques, and activities to understand, validate, synthesise, and articulate problems, opportunities, requirements, options, and solutions.\nIdentify opportunities to deliver continuous improvement to ensure the continuous delivery of customer centric benefits.\nShare knowledge and support capability growth of peers and Business Analysts.\nProvide passion about agile product development and will help drive continuous improvement in our scrum teams.\nBanking is changing and we re changing with it, giving our people great opportunities to try new things, learn and grow. Whatever your role at ANZ, you ll be building your future, while helping to build ours.\nRole Type: Permanent, 4.4\nRole Location: Bengaluru\nWork Hours: 7 AM to 4 PM\nWhat will your day look like?\n\nAs a Senior Business Analyst, you will:\nProviding technical business analysis expertise to help deliver complex initiatives.\nDeveloping requirements that are fit for purpose and accurate, display an understanding of the implications to the project or program, and manage the requirements throughout the life of the project.\nDeveloping presentations and documentation to communicate insights and recommendations in a compelling manner, and present regular project updates to the wider team.\nNurturing a growth mindset and curiosity within our teams.\nActively researching industry trends, standards, and methods, offering technical insights and direction.\nEnsuring quality control and data governance processes are followed.\nAdhering to established ANZ policy and procedures and display an understanding of project & program practices and governance.\nEnsuring that assigned business analysis activities are conducted efficiently and within agreed timeframes and ensure that the outputs and benefits are clear.\nWhat will you bring?\nThe must have knowledge, skill, and experience (KSE) the role requires are:\n5+ years experience as a Business Analyst and currently operating at senior level.\nUnderstanding and experience with Data warehousing, data projects or data migration is required.\nAbility to use SQL or SAS is highly recommended.\nAnalytical skills while being highly organised and adaptable.\nPositive can-do attitude and an excellent communicator both written, verbal and visual.\nProblem solver and critical thinker.\nNatural ability to build relationships with stakeholders within the business and technology.\nGrowth mindset.\nThe good to have knowledge, skill and experience (KSE) the role requires are:\nExperience delivering work and leading others in an Agile context (Scrum Master experience is beneficial).\nExperience in the creation of user stories into minimum viable increments, acceptance criteria and specification by example.\nExperience delivering data outcomes that drive decision making in a financial services or banking organisation.\nData warehousing experience.\nExperience with data modelling and databases will be advantageous.\nExpert in story mapping and building backlogs to support iterative development.",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAS', 'Business analysis', 'Agile', 'Senior Business Analyst', 'Scrum', 'Manager Quality Control', 'Continuous improvement', 'Data warehousing', 'Financial services', 'SQL']",2025-06-12 14:26:50
Software Dev Engineer,Amazon,1 - 3 years,Not Disclosed,['Bengaluru'],"AWS Infrastructure Services owns the design, planning, delivery, and operation of all AWS global infrastructure. In other words, we re the people who keep the cloud running. We support all AWS data centers and all of the servers, storage, networking, power, and cooling equipment that ensure our customers have continual access to the innovation they rely on. We work on the most challenging problems, with thousands of variables impacting the supply chain and we re looking for talented people who want to help.\n\nYou ll join a diverse team of software, hardware, and network engineers, supply chain specialists, security experts, operations managers, and other vital roles. You ll collaborate with people across AWS to help us deliver the highest standards for safety and security while providing seemingly infinite capacity at the lowest possible cost for our customers. And you ll experience an inclusive culture that welcomes bold ideas and empowers you to own them to completion.\n\nThese are all greenfield projects offering a rare opportunity to build systems from the ground up. You will be part of a new organization within AWS building large scale distributed applications. We will be designing and developing these systems from scratch utilizing technologies and frameworks like reactive micro services, serverless computing and distributed NoSQL data stores.\n\nIf you have a solid understanding of fundamental algorithms and system design and are able to produce bulletproof code, we are looking for you. To succeed in this role, you must be passionate about delivering high-quality designs and components. You must be creative in solving hard problems and unafraid to think out-of-the-box.\n\n\nThe candidate must be able to:\n1. Solve complex architecture and business problems to come up with extensible solutions.\n2. Write high quality code that are modular, functional and testable.\n3. Formally mentor junior engineers on design, coding and troubleshooting.\n4. Communicate, collaborate and work effectively in a global environment.\n\nAbout the team\nDiverse Experiences\nAmazon values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn t followed a traditional path, or includes alternative experiences, don t let it stop you from applying.\n\nWhy AWS\nAmazon Web Services (AWS) is the world s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating that s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses.\n\nWork/Life Balance\nWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there s nothing we can t achieve.\n\nInclusive Team Culture\nAWS values curiosity and connection. Our employee-led and company-sponsored affinity groups promote inclusion and empower our people to take pride in what makes us unique. Our inclusion events foster stronger, more collaborative teams. Our continual innovation is fueled by the bold ideas, fresh perspectives, and passionate voices our teams bring to everything we do.\n\nMentorship and Career Growth\nWe re continuously raising our performance bar as we strive to become Earth s Best Employer. That s why you ll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional. Several years of non-internship professional software development experience\nSeveral years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nKnowledge of engineering practices and patterns for the full software/hardware/networks development life cycle, including coding standards, code reviews, source control management, build processes, testing, certification, and livesite operations\nExperience programming with at least one software programming language, preferably Java among them.\nBachelors degree or equivalent Several years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nExperience building complex software systems that have been successfully delivered to customers\nExperience designing or architecting (design patterns, reliability and scaling) of new and existing systems\nExperience with full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations",,,,"['Supply chain', 'Cloud computing', 'Automation', 'NoSQL', 'Coding', 'Software development life cycle', 'System design', 'Troubleshooting', 'Internship', 'infrastructure services']",2025-06-12 14:26:53
Architect (ETL),Insightsoftware,10 - 15 years,Not Disclosed,['Hyderabad'],"Insightsoftware (ISW) is a growing, dynamic computer software company that helps businesses achieve greater levels of financial intelligence across their organization with our world-class financial reporting solutions. At insightsoftware, you will learn and grow in a fast-paced, supportive environment that will take your career to the next level. The Data Conversion Specialist is a member of the insightsoftware Project Management Office (PMO) who demonstrates teamwork, results orientation, a growth mindset, disciplined execution, and a winning attitude.\nLocation: Hyderabad (Work from Office)\nWorking Hours: 5:00PM - 2:00AM IST\nPosition Summary\nThe Team Lead, Data Conversion will build positive relationships with customers and serve as a primary point of contact for large scale complex product implementations and business-led change initiatives. The role will include, but is not limited to:\n10+ years of total experience in IT and at least 9+ yrs working experience in any ETL Tool.\nLeading the data conversion team to implement conversion implementation project deliverables.\nCollaborating with client operations and executive teams, confirming business requirements, and aligning to technical requirements.\nUnderstanding project status and facilitate achievement of project objectives and priorities.\nIdentifying process gaps and facilitating continuous refinement of conversion methodology.\nOverseeing data-related initiatives and identifying innovation and improvement opportunities.\nAdapting to and incorporating best practices across the conversion team while documenting and communicating to impacted stakeholders.\nActing as an escalation point for active client engagements.\nFacilitating the accurate and timely integration mapping of client data from source system to our industry-leading platform.\nApplying and staying within the requirements of project governance, standards, and processes.\nWorking in collaboration with project managers in the successful delivery of key projects and objectives.\nDeveloping a deep understanding of conversion products sufficient to facilitate decision-making at various levels.\nCommunicating appropriately to internal and external stakeholders about the project deliverables and key updates.\nTraining and mentoring other data conversion team members.\nExperience performing rapid assessments and being able to distill key themes as early as possible in an assessment process.\nExperience with assessing data and analytic requirements to establish mapping rules from Working experience using various data applications/systems such as Oracle SQL, Excel, .csv files, etc.\nDemonstrated experience automating manual data conversion, migration, or integration processes.\nExperience with real-time, batch, and ETL for complex data conversions.\nStrong knowledge of extract, transform, load (ETL) methodologies for large-scale data conversions, integrations, and/or migrations.\nUtilize data mapping tools to prepare data for data loads based on target system specifications.\nStrong SQL scripting experience.\nA track record of building and maintaining relationships with leaders, both internally and externally.\nDemonstrated ability to scope, develop, test, and implement data conversions.\nProven success implementing data conversion processes.\nAbility to determine priorities daily to achieve business objectives.\nExperience in customer SIT, UAT, migration and go live support.\nExperience in creating cloud migration strategies, defining delivery architecture and creating data migration plans.\nExperience performing rapid assessments and being able to distill key themes as early as possible in an assessment process.\nExperience with assessing data and analytic requirements to establish mapping rules from Working experience using various data applications/systems such as Oracle SQL, Excel, .csv files, etc.\nDemonstrated experience automating manual data conversion, migration, or integration processes.\nExperience with real-time, batch, and ETL for complex data conversions.\nStrong knowledge of extract, transform, load (ETL) methodologies for large-scale data conversions, integrations, and/or migrations.\nUtilize data mapping tools to prepare data for data loads based on target system specifications.\nStrong SQL scripting experience.\nA track record of building and maintaining relationships with leaders, both internally and externally.\nDemonstrated ability to scope, develop, test, and implement data conversions.\nProven success implementing data conversion processes.\nAbility to determine priorities daily to achieve business objectives.\nExperience in customer SIT, UAT, migration and go live support.\nExperience in creating cloud migration strategies, defining delivery architecture and creating data migration plans.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data migration', 'Data conversion', 'Financial reporting', 'Oracle SQL', 'Architecture', 'Project management', 'project governance', 'Team Leader', 'data mapping', 'SQL scripting']",2025-06-12 14:26:55
Senior Software Engineer - Adobe Experience Platform ( AEP ),Wells Fargo,4 - 7 years,Not Disclosed,['Hyderabad'],"About this role:\nWells Fargo is seeking a senior Software Engineer - Adobe Experience Platform (AEP)\nIn this role, you will:\nLead moderately complex initiatives and deliverables within technical domain environments\nContribute to large scale planning of strategies\nDesign, code, test, debug, and document for projects and programs associated with technology domain, including upgrades and deployments",,,,"['Software Engineering', 'Data Science', 'data model', 'data analysis', 'data modeling', 'configuration', 'APIs', 'AEP', 'CDP']",2025-06-12 14:26:57
IN Senior Associate AWS AI/ML/GenAI Developer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\n& Summary We are looking for a seasoned AWS\nAI/ML/GenAI Developer\nResponsibilities\nDesign and implement AI/ML/GenAI models using AWS services such as AWS Bedrock, SageMaker, Comprehend, Rekognition, and others.\nStrong programming skills in Python, R etc\nExperience with machine learning frameworks such as TensorFlow, PyTorch, or Scikitlearn.\nKnowledge of data preprocessing, feature engineering, and model evaluation techniques.\nDevelop and deploy generative AI solutions to solve complex business problems and improve operational efficiency.\nCollaborate with data scientists, engineers, and product teams to understand requirements and translate them into technical solutions.\nOptimize and finetune machine learning models for performance and scalability. Ensure the security, reliability, and scalability of AI/ML solutions by adhering to best practices.\nMaintain and update existing AI/ML models to ensure they meet evolving business needs.\nStay uptodate with the latest advancements in AI/ML and GenAI technologies and integrate relevant innovations into our solutions.\nProvide technical guidance and mentorship to junior developers and team members.\nExcellent problemsolving skills and ability to work in a fastpaced, collaborative environment.\nGood to have AWS Certified Machine Learning Specialty or other relevant AWS certifications.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired 48 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'GCP', 'Data modeling', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Corporate advisory', 'Operations', 'AWS']",2025-06-12 14:27:00
SQL DB Architect,Sutherland Global Services Inc,6 - 12 years,Not Disclosed,['Hyderabad'],"Job Title: SQL Database Architect (SQL Server, Azure SQL Server, SSIS, SSRS, Data Migration, Azure Data Factory, Power BI)\nJob Summary:\nWe are seeking a highly skilled SQL Database Architect with expertise in SQL Server, Azure SQL Server, SSIS, SSRS, Data Migration, and Power BI to design, develop, and maintain scalable database solutions. The ideal candidate will have experience in database architecture, data integration, ETL processes, cloud-based solutions, and business intelligence reporting. Excellent communication and documentation skills are essential for collaborating with cross-functional teams and maintaining structured database records.\nKey Responsibilities:\nDatabase Design & Architecture: Develop highly available, scalable, and secure database solutions using Azure SQL Server.\nETL & Data Integration: Design and implement SSIS packages for data movement, transformations, and automation.\nData Migration: Oversee database migration projects, including on-premises to cloud transitions and cloud to cloud transitions, data conversion, and validation processes.\nAzure Data Factory (ADF): Build and manage data pipelines, integrating various data sources and orchestrating ETL workflows in cloud environments\nReporting & Business Intelligence: Develop SSRS reports and leverage Power BI for creating interactive dashboards and data visualizations.\nPerformance Optimization: Analyze and optimize query performance, indexing strategies, and database configurations.\nCloud Integration: Architect Azure-based database solutions, including Azure SQL Database, Managed Instances, and Synapse Analytics.\nSecurity & Compliance: Ensure data security, encryption, and compliance with industry standards.\nBackup & Disaster Recovery: Design and implement backup strategies, high availability, and disaster recovery solutions.\nAutomation & Monitoring: Utilize Azure Monitor, SQL Profiler, and other tools to automate and monitor database performance.\nCollaboration & Communication: Work closely with developers, BI teams, DevOps, and business stakeholders, explaining complex database concepts in a clear and concise manner.\nDocumentation & Best Practices: Maintain comprehensive database documentation, including design specifications, technical workflows, and troubleshooting guides.\nRequired Skills & Qualifications:\nExpertise in SQL Server & Azure SQL Database.\nExperience with SSIS for ETL processes, data transformations, and automation.\nProficiency in SSRS for creating, deploying, and managing reports.\nStrong expertise in Data Migration, including cloud and on-premises database transitions.\nPower BI skills for developing dashboards, reports, and data visualizations.\nDatabase modeling, indexing, and query optimization expertise.\nKnowledge of cloud-based architecture, including Azure SQL Managed Instance.\nProficiency in T-SQL, stored procedures, Triggers, and database scripting.\nUnderstanding of security best practices, including role-based access control (RBAC).\nExcellent communication skills to explain database solutions to technical and non-technical stakeholders.\nStrong documentation skills to create and maintain database design specs, process documents, and reports.\nPreferred Qualifications:\nKnowledge of CI/CD pipelines for database deployments.\nFamiliarity with Power BI and other data visualization tools.\nExperience with Azure Data Factory and Synapse Analytics for advanced data engineering workflows\n\n\nBachelors or Masters degree in Business, Computer Science, Engineering, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data migration', 'Database design', 'SSRS', 'Stored procedures', 'SSIS', 'Business intelligence', 'Troubleshooting', 'Analytics', 'Monitoring']",2025-06-12 14:27:02
Solution Architect,TechVantage,10 - 20 years,Not Disclosed,['Thiruvananthapuram'],"Solution Architect\nOur company:\nTechvantage.ai is a next-generation technology and product engineering company at the forefront of innovation in Generative AI, Agentic AI , and autonomous intelligent systems . We design intelligent platforms that solve complex business problems and deliver measurable impact through cutting-edge AI technologies.\nRole Overview:\nWe are seeking an experienced Solution Architect with a strong foundation in software architecture and a working knowledge of AI-based products or platforms . In this role, you will be responsible for designing robust, scalable, and secure architectures that support AI-driven applications and enterprise systems.\nYou will work closely with cross-functional teams including data scientists, product managers, and engineering leads to bridge the gap between business needs, technical feasibility, and AI capability.\nWhat we are looking from an ideal candidate?\nArchitect end-to-end solutions for enterprise and product-driven platforms, including components such as data pipelines, APIs, AI model integration, cloud infrastructure, and user interfaces.\nGuide teams in selecting the right technologies, tools, and design patterns to build scalable systems.\nCollaborate with AI/ML teams to understand model requirements and ensure smooth deployment and integration into production.\nDefine system architecture diagrams, data flow, service orchestration, and infrastructure provisioning using modern tools.\nWork closely with stakeholders to translate business needs into technical solutions, with a focus on scalability, performance, and security.\nProvide leadership on best practices for software development, DevOps, and cloud-native architecture.\nConduct architecture reviews and ensure alignment with security, compliance, and performance standards.\nPreferred Skills: What skills do you need?\nRequirements:\n10+ years of experience in software architecture or solution design roles.\nProven experience designing systems using microservices , RESTful APIs , event-driven architecture , and cloud-native technologies.\nHands-on experience with at least one major cloud provider: AWS , GCP , or Azure .\nFamiliarity with AI/ML platforms or components , such as integrating AI models, MLOps pipelines, or inference services.\nUnderstanding of data architectures, including data lakes , streaming , and ETL pipelines.\nStrong experience with containerization ( Docker , Kubernetes ) and DevOps principles.\nAbility to lead technical discussions, make design trade-offs, and communicate with both technical and non-technical stakeholders.\nPreferred Qualifications:\nExposure to AI model lifecycle management , prompt engineering , or real-time inference workflows.\nExperience with infrastructure-as-code (Terraform, Pulumi).\nKnowledge of GraphQL, gRPC, or serverless architectures.\nPrevious experience working in AI-driven product companies or digital transformation programs.\nWhat We Offer:\nHigh-impact role in designing intelligent systems that shape the future of AI adoption\nWork with forward-thinking engineers, researchers, and innovators\nStrong focus on career growth, learning, and technical leadership\nCompensation is not a constraint for the right candidate",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System architecture', 'software architecture', 'Architect', 'Product engineering', 'orchestration', 'Architecture', 'GCP', 'Cloud', 'Infrastructure', 'Solution Architect']",2025-06-12 14:27:04
AWS Technical Architect,Puma Energy,10 - 14 years,35-40 Lacs P.A.,"['Mumbai', 'Pune']","We are seeking a highly experienced AWS Technical Architect to lead the design, implementation, and optimization of cloud infrastructure solutions on the Amazon Web Services (AWS) platform. The ideal candidate will have extensive hands-on experience with AWS services, a deep understanding of cloud architecture best practices, and a proven track record of delivering complex, enterprise-scale cloud solutions. Preferably from Energy, Retail sector.\nProvide technical leadership and strategic direction for the design and implementation of AWS-based solutions.\nCollaborate with cross-functional teams, including business stakeholders, to understand requirements and translate them into robust, scalable, and cost-effective technical solutions.\nDevelop and maintain reference architectures, design patterns, and implementation guidelines for AWS services.\nPerform capacity planning, cost optimization, and performance tuning of AWS environments.\nImplement and automate infrastructure as code (IaC) using tools like AWS CloudFormation, Terraform, or AWS CDK\nEnsure compliance with security, governance, and regulatory requirements.\nMentor and provide technical guidance to development teams on AWS best practices.\nStay up to date with the latest AWS services and features and drive their adoption within the organization.\nParticipate in the evaluation and selection of third-party tools and services for integration with AWS.\nImplement security best practices, including IAM, encryption, and network security measures.\nEnsure solutions comply with relevant regulatory standards and corporate policies.\nConduct regular audits and assessments to maintain cloud environment integrity.\nExperience\nShould have around 8-10 years of experience within IT.\nQualifications\n8-10 years of experience in designing and implementing cloud solutions, with a strong focus on AWS\nExtensive hands-on experience with a wide range of AWS services, including EC2, VPC, ELB, RDS, S3, CloudFront, Lambda, and more.\nDeep expertise in infrastructure as code (IaC) tools like AWS CloudFormation, Terraform, or AWS CDK\nStrong understanding of cloud architecture principles, including high availability, scalability, security, and cost optimization\nExperience with containerization technologies like Docker and container orchestration platforms (e.g., ECS, EKS)\nProficiency in at least one programming language (e.g., Python, Java, Node.js)\nExperience with CI/CD pipelines and automation tools (e.g., AWS CodePipeline, Jenkins)\nStrong problem-solving, analytical, and communication skills\nAWS Certified Solutions Architect (Professional) or equivalent certification is required.\nExperience in leading and mentoring technical teams.\nInternal and External",Industry Type: Oil & Gas,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS', 'Aws Migration', 'Solution Architecting', 'Aws Infrastructure', 'Cloud Architecture', 'Designing And Developing']",2025-06-12 14:27:06
SOFTWARE DEVELOPMENT MANAGER,Amazon,8 - 13 years,Not Disclosed,['Bengaluru'],"you'll join a diverse team of software, hardware, and network engineers, supply chain specialists, security experts, operations managers, and other vital roles. you'll collaborate with people across AWS to help us deliver the highest standards for safety and security while providing seemingly infinite capacity at the lowest possible cost for our customers. And you'll experience an inclusive culture that welcomes bold ideas and empowers you to own them to completion.\n\nCome and be part of the AWS Supply Chain team and build systems that enable AWS to rapidly grow and continue to be the pioneer and widely recognized leader in Cloud Computing, and have direct and immediate impact on the hundreds of thousands developers and businesses around the world who use AWS every day.\n\nSupply Chain Management (AWS_SCM) systems are at heart of ensuring that we can provide our customers with the right computing services, in the right region, at the right time! The SC.os team is responsible for building the end-to-end supply chain automation that enable control of our global footprint for our hyper-growth cloud computing.\n\nAs a technical leader on the team, you will be responsible guiding your team through the design, development, testing, and deployment of a range of products.\nIn addition, you will help build the roadmaps for software teams, developing innovative customer experiences.\nWe are looking for an experienced technical manager who is excited about building industry leading distributed systems in the Supply Chain domain.\nYou will be part of the team that architects, designs, and implements highly scalable distributed systems that provide availability, scalability and latency guarantees.\nThis is a hands on technology position. You need to not only be a top software manager with a good track record of delivering, but also excel in communication, leadership and customer focus.\nThis is a unique and rare opportunity to get in on the ground floor and help shape the supply chain software, product and business.\nA successful candidate will bring deep technical and software expertise and ability to work within a fast moving, startup environment in a large company to deliver solid code that has a broad business impact.\n\nAbout the team\nDiverse Experiences\nAmazon values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn t followe'd a traditional path, or includes alternative experiences, don t let it stop you from applying.\n\nWhy AWS\nAmazon Web Services (AWS) is the world s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating that s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses.\n\nWork/Life Balance\nWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there s nothing we can t achieve.\n\nMentorship and Career Growth\nwe're continuously raising our performance bar as we strive to become Earth s Best Employer. That s why you'll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional.",,,,"['Cloud computing', 'Automation', 'Supply chain management', 'Team management', 'development testing', 'Coding', 'Design development', 'SCM', 'Distribution system', 'infrastructure services']",2025-06-12 14:27:08
Solution Architect,Azine Technologies,5 - 8 years,Not Disclosed,['Ahmedabad'],"We are seeking an experienced Solution Architect to design scalable, high-performance technical solutions aligned with business goals. The role involves collaborating with cross-functional teams and leading architecture strategy and implementation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution Architecting', 'Data Architecture', 'Solution Design', 'System Integration', 'Enterprise Architecture', 'Software Development Life Cycle', 'Cloud Platforms', 'TOGAF', 'Cloud Architecture', 'IT Architecture', 'Enterprise Integration', 'IT Strategy']",2025-06-12 14:27:11
App Dev & Support Engineer III,Conduent,2 - 6 years,Not Disclosed,['Bengaluru'],"Job Track Description:\nCandidate needs to be Oracle Fusion HCM CloudSubject Mater expertin Integration (API/Extract), Fast Formula & Reporting tools.\nAnalyze client requirements and provide design solutions using Oracle Fusion HCM modules.\nEnsure Team customize and configure Fusion HCM applications to align with client business processes.\nEnsure Team develops and maintain reports, interfaces, conversions, and extensions as per project needs.\nLead data migration activities, ensuring accurate and timely transfer of data from legacy systems to Fusion HCM.\nCollaborate with cross-functional teams to integrate Fusion HCM with other enterprise systems.\nProvide technical guidance and support to junior team members.\nStay updated with Oracle Fusion HCM updates and best practices.\nEnsure Team adherence to SOPs - Change Management, Coding Standards, Knowledge Management, Oracle SR, Batch Job Monitoring\nRequires formal education and relevant expertise in professional & technical area.\nPerforms technical-based activities.\nContributes to and manages projects.\nUses deductive reasoning to solve problems and make recommendations.\nInterfaces with and influences key stakeholders.\nLeverages previous knowledge and expertise to achieve results.\nProficiency in independently managing and completing tasks.",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['oracle', 'engineering support', 'oracle fusion hcm', 'oracle fusion', 'change management', 'bi publisher', 'oracle core hr', 'oracle apps technical', 'core hr', 'sql', 'plsql', 'hdl', 'hcm', 'application support', 'linux', 'hcm extracts', 'attendance management', 'payroll', 'oracle hrms']",2025-06-12 14:27:13
AWS DevOps Senior Software Engineer,Care Allianz,3 - 7 years,Not Disclosed,['Thiruvananthapuram'],"Care Allianz is looking for AWS DevOps Senior Software Engineer to join our dynamic team and embark on a rewarding career journey.\n\nyou will be responsible for designing, implementing, and managing end - to - end DevOps processes and infrastructure on the Amazon Web Services (AWS) platform. This role involves leading a team, collaborating with development and operations teams, and ensuring the seamless integration of development and deployment pipelines. Key Responsibilities : DevOps Strategy : Develop and implement a comprehensive DevOps strategy for the organization, focusing on automation, continuous integration, and continuous delivery. Lead the adoption of best practices in DevOps processes. AWS Infrastructure : Design, configure, and manage AWS infrastructure, including EC2 instances, S3 buckets, VPCs, and other services. Implement infrastructure as code (IaC) using tools such as AWS CloudFormation or Terraform. Continuous Integration/Continuous Deployment (CI/CD) : Design, implement, and maintain CI/CD pipelines for multiple applications and environments. Ensure efficient and automated deployment processes, minimizing downtime and errors. Automation : Implement automation scripts and tools to streamline repetitive tasks and enhance operational efficiency. Utilize scripting languages such as Python, Shell, or PowerShell for automation. Monitoring and Logging : Implement monitoring and logging solutions to proactively identify and address issues. Set up and manage logging tools and monitoring dashboards for AWS services. Security and Compliance : Implement security best practices for infrastructure and applications on AWS. Ensure compliance with security policies, industry standards, and regulatory requirements. Team Leadership : Lead and mentor a team of DevOps engineers. Foster a culture of collaboration, innovation, and continuous improvement within the DevOps team. Collaboration : Collaborate with development, operations, and other cross - functional teams to align DevOps processes with overall business goals. Participate in Agile/Scrum processes and contribute to sprint planning and retrospectives.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Career development', 'Networking', 'devops', 'IT infrastructure', 'Manager Technology', 'Healthcare', 'Management', 'Financial services', 'Spectrum', 'Teaching']",2025-06-12 14:27:15
Solution Architect,Amgen Inc,9 - 14 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are seeking a seasoned Solution Architect to drive the architecture, development and implementation of data solutions to Amgen functional groups. The ideal candidate able to work in large scale Data Analytic initiatives, engage and work along with Business, Program Management, Data Engineering and Analytic Engineering teams. Be champions of enterprise data analytic strategy, data architecture blueprints and architectural guidelines. As a Solution Architect, you will play a crucial role in designing, building, and optimizing data solutions to Amgen functional groups such as R&D, Operations and GCO.\nRoles & Responsibilities:\nImplement and manage large scale data analytic solutions to Amgen functional groups that align with the Amgen Data strategy\nCollaborate with Business, Program Management, Data Engineering and Analytic Engineering teams to deliver data solutions\nResponsible for design, develop, optimize, delivery and support of Data solutions on AWS and Databricks architecture\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nProvide expert guidance and mentorship to the team members, fostering a culture of innovation and best practices.\nBe passionate and hands-on to quickly experiment with new data related technologies\nDefine guidelines, standards, strategies, security policies and change management policies to support the Enterprise Data platform.\nCollaborate and align with EARB, Cloud Infrastructure, Security and other technology leaders on Enterprise Data Architecture changes\nWork with different project and application groups to drive growth of the Enterprise Data Platform using effective written/verbal communication skills, and lead demos at different roadmap sessions\nOverall management of the Enterprise Data Platform on AWS environment to ensure that the service delivery is cost effective and business SLAs around uptime, performance and capacity are met\nEnsure scalability, reliability, and performance of data platforms by implementing best practices for architecture, cloud resource optimization, and system tuning.\nCollaboration with RunOps engineers to continuously increase our ability to push changes into production with as little manual overhead and as much speed as possible.\nMaintain knowledge of market trends and developments in data integration, data management and analytics software/tools\nWork as part of team in a SAFe Agile/Scrum model\nBasic Qualifications and Experience:\nMasters degree with 6 - 8 years of experience in Computer Science, IT or related field OR\nBachelors degree with 9 - 12 years of experience in Computer Science, IT or related field OR\nFunctional Skills:\nMust-Have Skills:\n7+ years of hands-on experience in Data integrations, Data Management and BI technology stack.\nStrong experience with one or more Data Management tools such as AWS data lake, Snowflake or Azure Data Fabric\nExpert-level proficiency with Databricks and experience in optimizing data pipelines and workflows in Databricks environments.\nStrong experience with Python, PySpark, and SQL for building scalable data workflows and pipelines.\nExperience with Apache Spark, Delta Lake, and other relevant technologies for large-scale data processing.\nFamiliarity with BI tools including Tableau and PowerBI\nDemonstrated ability to enhance cost-efficiency, scalability, and performance for data solutions\nStrong analytical and problem-solving skills to address complex data solutions\nGood-to-Have Skills:\nPreferred to have experience in life science or tech or consultative solution architecture roles\nExperience working with agile development methodologies such as Scaled Agile.\nProfessional Certifications\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'Data Engineering', 'PySpark', 'Tableau', 'SQL', 'Apache Spark', 'Enterprise Data platform', 'AWS data lake', 'Program Management', 'Databricks architecture', 'Azure Data Fabric', 'PowerBI', 'Snowflake', 'Delta Lake', 'Scaled Agile', 'AWS', 'Python']",2025-06-12 14:27:18
Senior MSBI Developer (SQL & SSRS/SSIS Expert),Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027513\n\nJob Summary\nSynechron is seeking an experienced and detail-oriented Senior MSBI Developerexpertise in MSBI (Microsoft Business Intelligence) to join our data and analytics team. In this role, you will contribute to designing, developing, and maintaining robust reporting and data integration solutions that support our business objectives. Your expertise will help deliver actionable insights, improve decision-making processes, and enhance overall data management efficiency within the organization.\n\nSoftware\n\nRequired\n\nSkills:\nMSBI Suite (including SSIS, SSRS, SSAS)\nSQL Server (including SQL Server Management Studio and Query Performance Tuning)\nVersionsRecent versions of SQL Server (2016 or later preferred)\nProven experience in creating complex reports, data transformation, and integration workflows\nPreferred\n\nSkills:\nPower BI or other visualization tools\nExperience with cloud-based data solutions (e.g., Azure SQL, Synapse Analytics)\nOverall Responsibilities\nDevelop, implement, and maintain MSBI solutions such as SSIS packages, SSRS reports, and data models to meet business requirements\nCollaborate with business stakeholders and data teams to gather reporting needs and translate them into scalable solutions\nOptimize and troubleshoot existing reports and data pipelines to improve performance and reliability\nEnsure data accuracy, security, and compliance within reporting processes\nDocument solution architectures, workflows, and processes for ongoing support and knowledge sharing\nParticipate in team initiatives to enhance data governance and best practices\nContribute to strategic planning for data platform evolution and modernization\nTechnical Skills (By Category)\n\nProgramming Languages:\nRequiredSQL (Advanced proficiency in query writing, stored procedures, and performance tuning)\nPreferredT-SQL scripting for data transformations and automation\nDatabases / Data Management:\nRequiredDeep knowledge of relational database concepts with extensive experience in SQL Server databases\nPreferredFamiliarity with data warehouse concepts, OLAP cubes, and data mart design\nCloud Technologies:\nDesiredBasic understanding of cloud-based data platforms like Azure Data Factory, Azure Synapse\nFrameworks and Libraries:\nNot directly applicable, focus on MSBI tools\nDevelopment Tools and Methodologies:\nExperience working within Agile development environments\nData pipeline development and testing best practices\nSecurity Protocols:\nImplement data security measures, role-based access controls, and ensure compliance with data privacy policies\nExperience\n8 to 10 years of professional experience in software development with substantial hands-on MSBI expertise\nDemonstrated experience in designing and deploying enterprise-level BI solutions\nDomain experience in finance, healthcare, retail, or similar industries is preferred\nAlternative candidacyExtensive prior experience with BI tools and proven success in similar roles may be considered in lieu of exact industry background\nDay-to-Day Activities\nDesign and develop SSIS data integration workflows to automate data loading processes\nCreate and optimize SSRS reports and dashboards for various organizational units\nEngage in troubleshooting and resolving technical issues in existing BI solutions\nCollaborate with data architects, developers, and business analysts to align data solutions with business needs\nConduct code reviews, testing, and validation of reports and data pipelines\nParticipate in scrum meetings, planning sessions, and stakeholder discussions\nEnsure documentation of solutions, processes, and workflows for ease of maintenance and scalability\nQualifications\nBachelors degree or equivalent in Computer Science, Information Technology, or related field\nRelevant certifications in Microsoft BI or SQL Server (e.g., Microsoft Certified Data Engineer Associate) preferred\nOngoing engagement in professional development related to BI, data management, and analytics tools\nProfessional Competencies\nAnalytical mindset with strong problem-solving abilities in data solution development\nCapable of working collaboratively across diverse teams and communicating technical concepts effectively\nStakeholder management skills to interpret and prioritize reporting needs\nAdaptability to evolving technologies and continuous learning mindset\nFocus on delivering high-quality, sustainable data solutions with attention to detail\nEffective time management, prioritizing tasks to meet project deadlines",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['performance tuning', 'stored procedures', 'sql scripting', 'query writing', 'msbi', 'sql server database', 'software development', 'ssas', 'power bi', 'microsoft azure', 'data mart design', 'olap cubes', 'sql server', 'sql azure', 'ssrs', 'data warehousing concepts', 'data transformation', 'ssis']",2025-06-12 14:27:20
Solution Architect,Kanerika Software,10 - 20 years,Not Disclosed,"['Indore', 'Hyderabad', 'Ahmedabad']","Job Summary:\n\nAs a Solution Architect, you will collaborate with our sales, presales and COE teams to provide technical expertise and support throughout the new business acquisition process. You will play a crucial role in understanding customer requirements, presenting our solutions, and demonstrating the value of our products.\nYou thrive in high-pressure environments, maintaining a positive outlook and understanding that career growth is a journey that requires making strategic choices. You possess good communication skills, both written and verbal, enabling you to convey complex technical concepts clearly and effectively. You are a team player, customer-focused, self-motivated, responsible individual who can work under pressure with a positive attitude. You must have experience in managing and handling RFPs/ RFIs, client demos and presentations, and converting opportunities into winning bids. You possess a strong work ethic, positive attitude, and enthusiasm to embrace new challenges. You can multi-task and prioritize (good time management skills), willing to display and learn. You should be able to work independently with less or no supervision. You should be process-oriented, have a methodical approach and demonstrate a quality-first approach.",,,,"['Azure Data Factory', 'Microsoft Fabric', 'Data Analytics', 'Azure Cloud']",2025-06-12 14:27:23
Immediate Joiner / Java Developer,Service based Top B2B MNC in IT Services...,5 - 10 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nBuild high performant, highly scalable, complex and\ndistributed systems end to end\nDevelop simple solutions to address complex\nproblems.\nGive to a future-ready, high quality, and performant\ncode base.\nBring technical solutions to the leadership team,\nfeedback on solutions recommended, new product\nideas with the team through design review, pair\nprogramming, code review and tech talk.\nAbility to share technical solutions and product\nideas with the broader team through design review,\ncode review, proof-of-concepts and show and tell\nParticipate in brainstorming sessions and give ideas\nto our technology, algorithms and products\nImplement new features in a highly collaborative\nenvironment with product managers, UI/UX guides,\nand software and hardware engineers.\nMinimum Qualifications\nBachelor's degree (or above) in\nengineering/computer science with an overall work\nexperience of 5-8 years in in Java/Kotlin\nDevelopment\nPossess advanced knowledge of object-oriented\ndesign and development and data architectures.\nConfirmed ability to communicate with different\nlevels in the organization, influence others and\nbuilding consensus, developing teamwork across\nteams, and seek problems/remove obstacles in a\ntimely manner\n\nBroad Information Technology experience,\nincluding understanding of tools, processes and\nstandard methodologies of project execution\nAdept in coordinating and participating in all\nactivities (analysis, scoping, and design,\ncoding/code reviews, test case reviews, defect\nmanagement, implementation planning /execution\nand support, leading resources and timelines)\nExperience working with and applying Design\npatterns to tackle problems.\nStrong CS fundamentals in algorithms and data\nstructures\nExtensive technical experience and development\nexpertise in Core Java, Koltin, Kotlin Co-routines,\nKafka, Vertx, Nodejs , Java Script, JQuery and\nAJAX, Asynchronous programming .\nExperience doing Object Oriented Analysis and\nDesign, using Domain Driven Design, and Design\nPatterns\nExperience with tools like Maven, Jenkins, Git\nExtensive technical experience and development\nexpertise in data-driven applications utilizing\nsignificant relational database engines/NO SQL\nDatabases as\npart of the overall application architecture\n(experience with any or all of the following helpful:\nMysql Oracle, SQL Server, MongoDB, Cassandra)\nExcellent debugging and testing skills\nAbility to perform performance and scalability\nanalysis as needed.\nGood knowledge of technology and product trends,\nincluding knowledge of what is happening in Open\n\nSource and in other parts of the software\ndevelopment industry\nAbility to work with large-scale distributed systems\nExcellent in problem-solving and multitasking skills\nAbility to work in fast paced environment with good\npartnership capabilities.\nImplement solutions focusing on reuse and industry\nstandards at a program, enterprise or operational\nscope.\nStrong understanding of system performance and\nscaling\nAbility to work in an agile and collaborative setup\nwithin an engineering team.\nPossess excellent communication, sharp analytical\nabilities with validated design skills, able to think\ncritically of the current system in terms of growth\nand stability\nProven ability to mentor other software developers\nto maintain architectural vision and software quality\nStrong desire to build, sense of ownership, urgency,\nand drive\nVerify stability, interoperability, portability, security\nand scalability of java system architecture.\n\nDesired Qualifications\nValidated experience of strong desire to work in\nproduct development\nBe highly flexible and adaptable and demonstrate\npassion for platform development\nExcellent partnership, written and verbal\ncommunication skills\n\nExpertise in delivering high-quality, innovative\napplication\nFamiliar with AWS or other Cloud environment\nConsistent track record of innovation and thought\nleadership.\nObsession with quality and customer experience -\nAttention to detail coupled with ability to think\nabstractly\nFamiliarity with running large scale web services;\nunderstanding of systems internals and networking\nare a plus\nConsistent track record of developing large\nenterprise grade software and delivering high\nquality products/releases on time\nAnalyze performance characteristics to identify\nbottlenecks, failure points, and security holes in\nlarge scale systems. Passionate about performance\nand scalability\nSelf-Starter with the ability to remain flexible and\nlearn new technologies quickly\nCan do attitude and flexibility in pursuing various\nassignments to make the company successful\nConsistent track record of innovation and thought\nleadership.\nObsession with quality and customer experience -\nAttention to detail coupled with ability to think\nabstractly\nThe ability to take convert raw requirements into\ngood design while exploring technical feasibility\ntradeoffs\nProven ability to achieve stretch goals in a highly\ninnovative and fast paced environment\n\nEvaluate current or emerging technologies to\nconsider monetary factors of java program.\n\n\nMandatory skills - Core Java, AWS, , Kafka, OOPS, Maven/Jenkins and No-SQL",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Core Java', 'Kafka', 'AWS', 'Jenkins', 'Maven', 'NoSQL', 'Manves', 'OOPS']",2025-06-12 14:27:25
"Delivery Lead (.NET, C#, AWS, Agile & Cloud Solutions)",Synechron,12 - 15 years,Not Disclosed,"['Pune', 'Bengaluru', 'Hinjewadi']","job requisition idJR1027350\n\nJob Summary\nSynechron is seeking a experienced and strategic Delivery Lead to oversee complex technology projects utilizing .NET, C#, and AWS. This role is instrumental in managing end-to-end project delivery, guiding cross-functional teams, and ensuring alignment with business objectives. The Delivery Lead will drive improvements in delivery efficiency, quality, and stakeholder satisfaction, contributing significantly to the organizations technological growth and operational excellence.\n\nSoftware\n\nRequired\n\nSkills:\nDevelopment and delivery experience with .NET (preferably version 4.7 or later)\nC# programming proficiency\nHands-on experience with Amazon Web Services (AWS) (EC2, S3, Lambda, etc.)\nProject management toolsJira, SharePoint, MS Excel, PowerPoint, Power BI\nAgile and Waterfall project management methodologies\nPreferred\n\nSkills:\nExperience with DevOps/CI-CD pipelines\nKnowledge of Azure cloud platform\nFamiliarity with software release management\nOverall Responsibilities\nEnd-to-End Project Delivery: Manage multiple projects from initiation to closure, ensuring delivery is on time, within scope, and within budget.\nTeam Leadership: Lead and mentor diverse project teams, fostering collaboration and high performance.\nStakeholder Management: Act as the primary point of contact for clients and internal stakeholders, translating business needs into technical solutions.\nGovernance & Compliance: Ensure adherence to organizational policies, standards, and industry best practices.\nTechnical Oversight: Provide guidance on architecture, technology choices, and solution design aligned with best practices.\nProcess Optimization: Continuously identify opportunities to improve delivery processes, increase efficiency, and reduce risk.\nFinancial Oversight: Monitor project budgets, optimize resource utilization, and report on financial performance.\nRisk & Issue Management: Identify, assess, and mitigate risks impacting project delivery.\nPerformance Measurement: Establish metrics and KPIs to measure project success and customer satisfaction.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: C#, .NET Framework/Core\nPreferred: Java, Python (for integration or automation)\nDatabases/Data Management:\nSQL Server, AWS RDS\nCloud Technologies:\nAWS cloud services (EC2, S3, Lambda, CloudWatch)\nFrameworks & Libraries:\n.NET Core / .NET Framework\nRESTful APIs, Microservices architecture\nDevelopment Tools & Methodologies:\nAgile, Scrum, Kanban\nDevOps tools (Jenkins, Azure DevOps, Git)\nSecurity Protocols:\nAWS security best practices\nData privacy and compliance standards\nExperience\n12 to 15 years of professional experience in managing IT projects and delivery teams.\nDemonstrable experience leading large-scale software development and implementation projects.\nStrong background with .NET/C# development, AWS cloud solutions, and cross-functional team management.\nExperience managing global or distributed teams.\nProven stakeholder management experience with senior management and clients.\nPrior exposure to Agile and Waterfall project methodologies.\nAlternative Experience: Candidates with extensive experience in software delivery, cloud migration, or enterprise application implementation may be considered.\n\nDay-to-Day Activities\nConduct project planning, resource allocation, and status reporting.\nHold regular stand-ups, progress reviews, and stakeholder meetings.\nReview development progress, remove blockers, and ensure adherence to quality standards.\nCollaborate with technical teams on architecture design and problem resolution.\nManage change requests, scope adjustments, and project adjustments.\nTrack project KPIs, update dashboards, and communicate progress to leadership.\nOversee risk registers and implement mitigation strategies.\nFacilitate retrospectives and process improvement initiatives.\nQualifications\nBachelors degree in Computer Science, Engineering, or related field; Masters preferred.\nProject Management certifications such as PMP, PMI-ACP, or ScrumMaster are advantageous.\nTraining or certification in AWS or cloud architecture is preferred.\nCommitment to continuous learning and professional development.\nProfessional Competencies\nStrong analytical and problem-solving skills\nEffective leadership and team management capabilities\nExcellent stakeholder communication and negotiation skills\nAbility to adapt to evolving project requirements and technologies\nStrategic thinking and organizational agility\nData-driven decision-making\nPrioritization and time management skills\nChange management and process improvement orientation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud migration', 'c#', 'software development', 'team management', '.net', 'continuous integration', 'azure devops', 'sql', 'java', 'git', 'it projects', 'aws cloud', 'devops', 'waterfall', 'kanban', 'jenkins', 'jira', 'rest', 'python', 'software delivery', 'amazon ec2', 'lambda expressions', 'scrum', 'agile', 'aws']",2025-06-12 14:27:27
Senior ODI Developer (OCI PaaS/IaaS Expertise),Oracle,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","Role Overview:\nWe are seeking a highly skilled Senior ODI Developer with strong hands-on experience in SQL, PL/SQL, and Oracle Data Integrator (ODI) projects, particularly on OCI (Oracle Cloud Infrastructure) PaaS or IaaS platforms. The ideal candidate will design, implement, and optimize ETL processes, leveraging cloud-based solutions to meet evolving business needs. Prior experience in banking or insurance projects is a significant advantage.\nKey Responsibilities:\nDesign, develop, and deploy ETL processes using Oracle Data Integrator (ODI) on OCI PaaS/IaaS.\nConfigure and manage ODI instances on OCI, ensuring optimal performance and scalability.\nDevelop and optimize complex SQL and PL/SQL scripts for data extraction, transformation, and loading.\nImplement data integration solutions, connecting diverse data sources like cloud databases, on-premise systems, APIs, and flat files.\nMonitor and troubleshoot ODI jobs running on OCI to ensure seamless data flow and resolve any issues promptly.\nCollaborate with data architects and business analysts to understand integration requirements and deliver robust solutions.\nConduct performance tuning of ETL processes, SQL queries, and PL/SQL procedures.\nPrepare and maintain detailed technical documentation for developed solutions.\nAdhere to data security and compliance standards, particularly in cloud-based environments.\nProvide guidance and best practices for ODI and OCI-based data integration projects.\nSkills and Qualifications:\nMandatory Skills:\nStrong hands-on experience with Oracle Data Integrator (ODI) development and administration.\nProficiency in SQL and PL/SQL for complex data manipulation and query optimization.\nExperience deploying and managing ODI solutions on OCI PaaS/IaaS environments.\nDeep understanding of ETL processes, data warehousing concepts, and cloud data integration.\nPreferred Experience:\nHands-on experience in banking or insurance domain projects, with knowledge of domain-specific data structures.\nFamiliarity with OCI services like Autonomous Database, Object Storage, Compute, and Networking.\nExperience in integrating on-premise and cloud-based data sources.\nOther Skills:\nStrong problem-solving and debugging skills.\nExcellent communication and teamwork abilities.\nKnowledge of Agile methodologies and cloud-based DevOps practices.\nEducation and Experience:\nBachelors degree in computer science, Information Technology, or a related field.\n5 to 8 years of experience in ODI development, with at least 2 years of experience in OCI-based projects.\nDomain experience in banking or insurance is an added advantage.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Oracle Data Integrator', 'OCI', 'Data Integrator', 'Data Warehousing', 'ETL', 'SQL']",2025-06-12 14:27:29
Professional Services Architect,Splunk,5 - 10 years,Not Disclosed,['Mumbai'],"Description\nJoin us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and seek to deliver the best experience for our customers. At Splunk, we re committed to our work, customers, having fun and most importantly to each other s success. Learn more about Splunk careers and how you can become a part of our journey!\n\nRole\nAs a Professional Services Architect, you will join a world-class team that provides technical and thought leadership through focused solutions within the Customer Success & Experience organisation. This role involves working closely with Splunk s portfolio of products, aligning closely with stakeholders, and acting as the subject-matter expert and Trusted Advisor across\nSplunk s solutions, including Splunk s core, Security, enterprise cloud solutions and Splunk ITSI.",,,,"['Product management', 'Unix', 'Business services', 'Automation', 'Analytical', 'Consulting', 'PHP', 'Application development', 'Windows', 'cisco']",2025-06-12 14:27:32
Java Developer(Immediate Joiner's within 10 Days),Service based Top B2B MNC in IT Services...,6 - 9 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nBuild high performant, highly scalable, complex and\ndistributed systems end to end\nDevelop simple solutions to address complex\nproblems.\nGive to a future-ready, high quality, and performant\ncode base.\nBring technical solutions to the leadership team,\nfeedback on solutions recommended, new product\nideas with the team through design review, pair\nprogramming, code review and tech talk.\nAbility to share technical solutions and product\nideas with the broader team through design review,\ncode review, proof-of-concepts and show and tell\nParticipate in brainstorming sessions and give ideas\nto our technology, algorithms and products\nImplement new features in a highly collaborative\nenvironment with product managers, UI/UX guides,\nand software and hardware engineers.\nMinimum Qualifications\nBachelor's degree (or above) in\nengineering/computer science with an overall work\nexperience of 5-8 years in in Java/Kotlin\nDevelopment\nPossess advanced knowledge of object-oriented\ndesign and development and data architectures.\nConfirmed ability to communicate with different\nlevels in the organization, influence others and\nbuilding consensus, developing teamwork across\nteams, and seek problems/remove obstacles in a\ntimely manner\n\nBroad Information Technology experience,\nincluding understanding of tools, processes and\nstandard methodologies of project execution\nAdept in coordinating and participating in all\nactivities (analysis, scoping, and design,\ncoding/code reviews, test case reviews, defect\nmanagement, implementation planning /execution\nand support, leading resources and timelines)\nExperience working with and applying Design\npatterns to tackle problems.\nStrong CS fundamentals in algorithms and data\nstructures\nExtensive technical experience and development\nexpertise in Core Java, Koltin, Kotlin Co-routines,\nKafka, Vertx, Nodejs , Java Script, JQuery and\nAJAX, Asynchronous programming .\nExperience doing Object Oriented Analysis and\nDesign, using Domain Driven Design, and Design\nPatterns\nExperience with tools like Maven, Jenkins, Git\nExtensive technical experience and development\nexpertise in data-driven applications utilizing\nsignificant relational database engines/NO SQL\nDatabases as\npart of the overall application architecture\n(experience with any or all of the following helpful:\nMysql Oracle, SQL Server, MongoDB, Cassandra)\nExcellent debugging and testing skills\nAbility to perform performance and scalability\nanalysis as needed.\nGood knowledge of technology and product trends,\nincluding knowledge of what is happening in Open\n\nSource and in other parts of the software\ndevelopment industry\nAbility to work with large-scale distributed systems\nExcellent in problem-solving and multitasking skills\nAbility to work in fast paced environment with good\npartnership capabilities.\nImplement solutions focusing on reuse and industry\nstandards at a program, enterprise or operational\nscope.\nStrong understanding of system performance and\nscaling\nAbility to work in an agile and collaborative setup\nwithin an engineering team.\nPossess excellent communication, sharp analytical\nabilities with validated design skills, able to think\ncritically of the current system in terms of growth\nand stability\nProven ability to mentor other software developers\nto maintain architectural vision and software quality\nStrong desire to build, sense of ownership, urgency,\nand drive\nVerify stability, interoperability, portability, security\nand scalability of java system architecture.\n\nDesired Qualifications\nValidated experience of strong desire to work in\nproduct development\nBe highly flexible and adaptable and demonstrate\npassion for platform development\nExcellent partnership, written and verbal\ncommunication skills\n\nExpertise in delivering high-quality, innovative\napplication\nFamiliar with AWS or other Cloud environment\nConsistent track record of innovation and thought\nleadership.\nObsession with quality and customer experience -\nAttention to detail coupled with ability to think\nabstractly\nFamiliarity with running large scale web services;\nunderstanding of systems internals and networking\nare a plus\nConsistent track record of developing large\nenterprise grade software and delivering high\nquality products/releases on time\nAnalyze performance characteristics to identify\nbottlenecks, failure points, and security holes in\nlarge scale systems. Passionate about performance\nand scalability\nSelf-Starter with the ability to remain flexible and\nlearn new technologies quickly\nCan do attitude and flexibility in pursuing various\nassignments to make the company successful\nConsistent track record of innovation and thought\nleadership.\nObsession with quality and customer experience -\nAttention to detail coupled with ability to think\nabstractly\nThe ability to take convert raw requirements into\ngood design while exploring technical feasibility\ntradeoffs\nProven ability to achieve stretch goals in a highly\ninnovative and fast paced environment\n\nEvaluate current or emerging technologies to\nconsider monetary factors of java program.\n\n\nMandatory skills - Core Java, AWS, , Kafka, OOPS, Maven/Jenkins and No-SQL",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Core Java', 'Kafka', 'AWS', 'Jenkins', 'Maven', 'NoSQL', 'Manves', 'OOPS']",2025-06-12 14:27:34
IT Auditor,KPMG Assurance and Consulting Services LLP,2 - 7 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nPerform testing of IT Application Controls, IPE, and Interface Controls through code reviews, IT General Controls review covering areas such as Change Management, Access Management, Backup Management, Incident and Problem Management, SDLC, Data Migration, Batch Job scheduling/monitoring and Business Continuity and Disaster Recovery\nRisk Based IT Internal Audit for Financial Services Entities\nIT SOX 404 Controls Testing, Quality Assurance\nInternal Financial Controls related to IT General Controls as part of Financial Statements Audits\nBusiness Systems Controls / IT Application Controls\nIT Risk & Control Self-Assessment\nAuditing Emerging Technologies such as Cloud Security, Intelligent Automation, RPA, IoT etc.\nWorking knowledge of programming languages(C/C++/Java/SQL)\n\nPreferred candidate profile\nA Bachelor's degree in engineering and approximately 2-7 years of related work experience; or a masters or MBA degree in business, computer science, information systems, engineering\nExpertise in code review skills (e.g., Java, C++, C, SQL, Oracle)\nExperience in performing IT audits of banking/financial sector applications\nGood to have knowledge of other IT regulations, standards and benchmarks used by the IT industry (e.g., NIST, PCI-DSS, ITIL, OWASP, SOX, COBIT, SSAE18/ISAE 3402 etc.)",Industry Type: Management Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['IT Audit', 'ITGC', 'Code Review', 'ITAC']",2025-06-12 14:27:36
ETL Developer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\nResponsibilities:\nDesign and implement the data modeling, data ingestion and data processing for various datasets\nDesign, develop and maintain ETL Framework for various new data source\nDevelop data ingestion using AWS Glue/ EMR, data pipeline using PySpark, Python and Databricks.\nBuild orchestration workflow using Airflow & databricks Job workflow\nDevelop and execute adhoc data ingestion to support business analytics.\nProactively interact with vendors for any questions and report the status accordingly\nExplore and evaluate the tools/service to support business requirement\nAbility to learn to create a data-driven culture and impactful data strategies.\nAptitude towards learning new technologies and solving complex problem.\nQualifications:\nMinimum of bachelors degree. Preferably in Computer Science, Information system, Information technology.\nMinimum 5 years of experience on cloud platforms such as AWS, Azure, GCP.\nMinimum 5 year of experience in Amazon Web Services like VPC, S3, EC2, Redshift, RDS, EMR, Athena, IAM, Glue, DMS, Data pipeline & API, Lambda, etc.\nMinimum of 5 years of experience in ETL and data engineering using Python, AWS Glue, AWS EMR /PySpark and Airflow for orchestration.\nMinimum 2 years of experience in Databricks including unity catalog, data engineering Job workflow orchestration and dashboard generation based on business requirements\nMinimum 5 years of experience in SQL, Python, and source control such as Bitbucket, CICD for code deployment.\nExperience in PostgreSQL, SQL Server, MySQL & Oracle databases.\nExperience in MPP such as AWS Redshift, AWS EMR, Databricks SQL warehouse & compute cluster.\nExperience in distributed programming with Python, Unix Scripting, MPP, RDBMS databases for data integration\nExperience building distributed high-performance systems using Spark/PySpark, AWS Glue and developing applications for loading/streaming data into Databricks SQL warehouse & Redshift.\nExperience in Agile methodology\nProven skills to write technical specifications for data extraction and good quality code.\nExperience with big data processing techniques using Sqoop, Spark, hive is additional plus\nExperience in data visualization tools including PowerBI, Tableau.\nNice to have experience in UI using Python Flask framework anglular\n\n\nMandatory Skills: Python for Insights. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'data bricks', 'aws glue', 'amazon ec2', 'python', 'spark', 'glue', 'amazon redshift', 'cloud platforms', 'aws', 'data engineering', 'sql']",2025-06-12 14:27:39
Sr SQL Developer,HTC Global Services,8 - 13 years,Not Disclosed,['Hyderabad'],"We are looking for a skilled SSIS/T-SQL Developer with 8 years of good expertise. The incumbent will be responsible for developing, deploying and maintaining SSIS packages, as well as writing and optimizing T-SQL queries, stored procedures and functions. The role involves working closely with stakeholders to understand data requirements and deliver high-quality data solutions.\nRequirements:\nStrong understanding of relational database concepts and data modelling.\nExperience with SQL query optimization and performance tuning.",,,,"['T-SQL', 'Performance tuning', 'SQL queries', 'Data migration', 'query optimization', 'Data modeling', 'Debugging', 'Stored procedures', 'SSIS', 'Data warehousing']",2025-06-12 14:27:41
Snowflake Developer,internal,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","We are seeking a skilled Snowflake Developer to design, develop, and manage scalable data solutions using the Snowflake cloud data platform. The ideal candidate will have deep experience in data warehousing, SQL development, ETL processes, and cloud data architecture. Understanding of Control M and Tableau will be an added advantage. 1. Snowflake (Cloud Data Warehouse):1. Good understanding of Snowflake ECO system2. Good experience on Data modeling and Dimensional Modeling and techniques and will be able to drive the Technical discussions with IT & Business and Architects / Data Modelers3. Need to guide the team and provide the technical solutions4. Need to prepare the technical solution and architectures as part of project requirements5. Virtual Warehouse (Compute) - Good Understanding of Warehouse creation & manage6. Data Modeling & Storage - Strong knowledge on LDM/PDM design7. Data Loading/Unloading and Data Sharing- Should have good knowledge8. SnowSQL (CLI)- Expertise and excellent understanding of Snowflake Internals and Integration9. Strong hands on experience on SNOWSQL queries and Stored procedures and performance tuning techniques10. Good knowledge on SNOWSQL Scripts preparation the data validation and Audits11. SnowPipe Good knowledge of Snow pipe implementation12. Expertise and excellent understanding of S3 - Internal data copy/movement13. Good knowledge on Security & Readers and Consumers accounts14. Good knowledge and hands on experience on Query performance tuning implementation techniques2. SQL Knowledge:1. Advance SQL knowledge and hands on experience on complex queries writing using with Analytical functions2. Strong knowledge on stored procedures3. Troubleshooting, problem solving and performance tuning of SQL queries accessing data warehouse",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Part Time, Temporary/Contractual","['Snowflake', 'S3 integration', 'SQL']",2025-06-12 14:27:43
Senior Database Engineer,Cvent,3 - 6 years,Not Disclosed,['Gurugram'],"In This Role, You Will: Design, develop, and manage databases on the AWS cloud platform Develop and maintain automation scripts or jobs to perform routine database tasks such as provisioning, backups, restores, and data migrations\nBuild and maintain automated testing frameworks for database changes and upgrades to minimize the risk of introducing errors\nImplement self-healing mechanisms to automatically recover from database failures or performance degradation\nIntegrate database automation tools with CI/CD pipelines to enable continuous delivery and deployment of database changes",,,,"['Computer science', 'Hospitality', 'Database design', 'database security', 'MySQL', 'Disaster recovery', 'Troubleshooting', 'Event marketing', 'Information technology', 'Python']",2025-06-12 14:27:45
Workday Consultant,Tata Consultancy Services,4 - 9 years,4-8.5 Lacs P.A.,['Bengaluru'],"Dear Candidate,\nGreetings from TATA Consultancy Services!!\nThank you for expressing your interest in exploring a career possibility with the TCS Family.\nHiring For:- Workday\nLocation: Brigade Bhuwalka, Bangalore\nExperience: 4 to 12 years\nExperience in the role include Workday HCM cloud solutions experience including implementation, integrations, data migrations, development in Workday Extend and support (AMS/AD)",,,,"['Workday', 'Workday Functional', 'Workday Hcm']",2025-06-12 14:27:47
"Software Development Engineer II, Global Payments Tech",Amazon,3 - 8 years,Not Disclosed,['Gurugram'],"Want to participate in building the next generation of online payment system that supports numerous countries and payment methods? We are seeking talented software engineers to join one of the fastest growing areas in Amazon s e-commerce services platform. We offer competitive salary and benefits, career and growth opportunities and an exciting and team-oriented atmosphere.\nWe want to move all the money in the world! We deliver game-changing financial processing power for the some of the world s largest technology platforms including Amazon.com, Amazon Kindle, and Amazon Web Services.\nWe build systems that process payments at an unprecedented scale, with accuracy, speed, and mission-critical availability. We innovate to improve customer experience across the globe, with support for currency choice, in-store payments, pay on delivery, credit and debit payments, seller disbursements, gift cards, and many new exciting and challenging ideas are in the works.\nThe Amazon Payments Platform processes millions of transactions every day across numerous countries and payment methods. Over 100 million customers and merchants send tens of billions of dollars moving at light-speed through our systems annually. Come challenge yourself in our team-oriented atmosphere, and watch yourself grow with one of the fastest growing areas of the Amazon e-commerce services platform.\n\n\nDefine, design, and implement multi-tier distributed software applications.\nEstimate engineering effort, plan implementation, and rollout system changes that meet requirements for functionality, performance, scalability, reliability, and adherence to development goals and principles.\nMust be able to independently design code and test major features, as well as work jointly with other team members to deliver complex changes.\nMust be able to effectively collaborate in a fast paced environment with multiple teams in a large organization (software development, QA, Project/Release Management, Build and Release, etc.,).\nProvide on-call production support for payment platform applications 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Production support', 'Software Development Engineer II', 'Coding', 'Architectural design', 'Software development life cycle', 'Customer experience', 'Application software', 'Internship', 'Release management']",2025-06-12 14:27:49
Senior Security Operations Analyst,ZS,4 - 9 years,Not Disclosed,['Pune'],"We are seeking an experienced professional to join our Pune, India office as a Senior Security Operations Analyst with a strong background in Security Information and Event Management (SIEM) platforms, specifically in Microsoft Sentinel and Wiz. The ideal candidate will be responsible for leading advanced threat detection, response, and monitoring activities. This role will be critical in enhancing our cybersecurity posture and ensuring the ZS environment remains secure against emerging threats.\n  What you'll do:",,,,"['Event management', 'Analytical', 'Cloud', 'Management consulting', 'Financial planning', 'Vulnerability management', 'AWS', 'Information technology', 'Operations', 'Analytics']",2025-06-12 14:27:52
"Advisor, Technical Professional Services",Fiserv,10 - 12 years,Not Disclosed,['Noida'],"The Mainframe Advisor is responsible to Design large and complex Data Migration, Data Warehousing, and Business Intelligence Solutions, working in close collaboration with the Project Manager and the Development team. This is a full-time position with career growth opportunities and a competitive benefits package. If you want to in financial institutions and businesses worldwide solve complex business challenges every day, this is the right opportunity for you.\nWhat you will do\nUnder general supervision, analyse conversion requirements and write Conversion Programs.\nInterpret client s existing systems, workflows, and processing parameters.\nMust take complete ownership of the technical delivery the assigned conversion/implementation.\nManages multiple clients and adhere to project timelines.\nMonitors project progress by tracking activity, resolving problems, publishing progress reports, recommending actions in accordance with stated procedure.\nAssists management with the planning and design of improvements to business processes.\nUses sound judgment and experience to solve moderately complex problems based on precedent, example and experience that is commiserate with that of Business Analyst.\nUtilizes system and data to resolve business issues in the most effective manner.\nAnalyses and identifies root cause; providing input to solutions that lead to success of the project.\nCommunicate progress and any potential problems to Project Manager for awareness and/or resolution.\nMaintain the tools used to ensure the efficiency and effectiveness of the conversion process (system studies, timelines, and questionnaires).\nWork in late night shift to provide overlap with US working hours as and when required.\nProvide post implementation support for 1 week. (US shift timing will depend on time zone of client) during Conversion Go Live. 4 times in a year.\nWhat you will need to have\nB. Tech/MCA\n10 to 12 years of experience in IT Industry.\nExcellent Programming skills on IBM Mainframe Programming, COBOL, TSO, JCL.\nGood understanding of mainframe files\nShould have good understanding of activities performed in conversion/implementation of core Banking application.\nGood knowledge in identifying valid business scenarios, business workflows and business process.\nKnowledge of Banking domain.\nExperience of estimating data migration projects.\nExperienced problem solving and debugging skills.\nGood verbal and written communication and interpersonal skills\nWhat would be great to have\nExperience supporting Banking Core Conversions.\nExperience on Account Processing core is a plus.\nExposure to Banking and Financial Services industry with a good understanding of Banking Products, Services & Procedures.\nStrong analytical skills, good verbal and written communication skills and the ability to interact professionally with a diverse group.\nLeadership and mentoring skills.\nProficiency with Excel.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business process', 'TSO', 'JCL', 'Data migration', 'Cobol', 'Debugging', 'Business intelligence', 'Financial services', 'Mainframes', 'Core banking']",2025-06-12 14:27:54
App Dev & Support Engineer II,Conduent,2 - 5 years,Not Disclosed,['Bengaluru'],"Job Track Description:\n\n\nCandidate needs to be Oracle Fusion HCM Cloud\\u202FSubject Mater expert\\u202Fin Integration ( API/Extract ), Fast Formula & Reporting tools. ( OTBI , BI Publisher )\n\n\nAnalyze client requirements and provide design solutions using Oracle Fusion HCM modules.\n\n\nEnsure Team customize and configure Fusion HCM applications to align with client business processes.\n\n\nEnsure Team develops and maintain reports, interfaces, conversions, and extensions as per project needs.\\u202F\\u202F\n\n\nLead data migration activities, ensuring accurate and timely transfer of data from legacy systems to Fusion HCM.\n\n\nCollaborate with cross-functional teams to integrate Fusion HCM with other enterprise systems.\n\n\nProvide technical guidance and support to junior team members.\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\n\n\nStay updated with Oracle Fusion HCM updates and best practices.\n\n\nEnsure Team adherence to SOPs - Change Management, Coding Standards, Knowledge Management, Oracle SR, Batch Job Monitoring\n\n\nRequires formal education and relevant expertise in professional & technical area.\n\n\nPerforms technical-based activities.\n\n\nContributes to and manages projects.\n\n\nUses deductive reasoning to solve problems and make recommendations.\n\n\nInterfaces with and influences key stakeholders.\n\n\nLeverages previous knowledge and expertise to achieve results.\\u202F\n\n\nProficiency in independently managing and completing tasks.",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['oracle', 'engineering support', 'oracle fusion hcm', 'change management', 'sql', 'bi publisher', 'networking', 'oracle fusion', 'plsql', 'production support', 'technical support', 'unix shell scripting', 'application support', 'linux', 'desktop support', 'hcm extracts', 'troubleshooting', 'unix']",2025-06-12 14:27:56
"Senior Engineer, Application Development",S&P Global Market Intelligence,5 - 8 years,Not Disclosed,['Hyderabad'],"Grade Level (for internal use):\n10\nMarket Intelligence\nThe Role: Senior Full Stack Developer\nGrade level :10\nThe Team: You will work with a team of intelligent, ambitious, and hard-working software professionals. The team is responsible for the architecture, design, development, quality, and maintenance of the next-generation financial data web platform. Other responsibilities include transforming product requirements into technical design and implementation. You will be expected to participate in the design review process, write high-quality code, and work with a dedicated team of QA Analysts, and Infrastructure Teams\nThe Impact: Market Intelligence is seeking a Software Developer to create software design, development, and maintenance for data processing applications. This person would be part of a development team that manages and supports the internal & external applications that is supporting the business portfolio. This role expects a candidate to handle any data processing, big data application development. We have teams made up of people that learn how to work effectively together while working with the larger group of developers on our\nplatform.\nWhats in it for you:\nOpportunity to contribute to the development of a world-class Platform Engineering team .\nEngage in a highly technical, hands-on role designed to elevate team capabilities and foster continuous skill enhancement.\nBe part of a fast-paced, agile environment that processes massive volumes of dataideal for advancing your software development and data engineering expertise while working with a modern tech stack.\nContribute to the development and support of Tier-1, business-critical applications that are central to operations.\nGain exposure to and work with cutting-edge technologies including AWS Cloud , EMR and Apache NiFi .\nGrow your career within a globally distributed team , with clear opportunities for advancement and skill development.\nResponsibilities:\nDesign and develop applications, components, and common services based on development models, languages and tools, including unit testing, performance testing and monitoring and implementation\nSupport business and technology teams as necessary during design, development and delivery to ensure scalable and robust solutions\nBuild data-intensive applications and services to support and enhance fundamental financials in appropriate technologies.( C#, .Net Core, Databricsk, Spark ,Python, Scala, NIFI , SQL)\nBuild data modeling, achieve performance tuning and apply data architecture concepts\nDevelop applications adhering to secure coding practices and industry-standard coding guidelines, ensuring compliance with security best practices (e.g., OWASP) and internal governance policies.\nImplement and maintain CI/CD pipelines to streamline build, test, and deployment processes; develop comprehensive unit test cases and ensure code quality\nProvide operations support to resolve issues proactively and with utmost urgency\nEffectively manage time and multiple tasks\nCommunicate effectively, especially written with the business and other technical groups\nWhat Were Looking For:\nBasic Qualifications:\nBachelorsMasters Degree in Computer Science, Information Systems or equivalent.\nMinimum 5 to 8 years of strong hand-development experience in C#, .Net Core, Cloud Native, MS SQL Server backend development. Proficiency with Object Oriented Programming.\nAdvance SQL programming skills\nPreferred experience or familiarity with tools and technologies such as Odata, Grafana, Kibana, Big Data platforms, Apache Kafka, GitHub, AWS EMR, Terraform, and emerging areas like AI/ML and GitHub Copilot.\nHighly recommended skillset in Databricks, SPARK, Scalatechnologies.\nUnderstanding of database performance tuning in large datasets\nAbility to manage multiple priorities efficiently and effectively within specific timeframes\nExcellent logical, analytical and communication skills are essential, with strong verbal and writing proficiencies\nKnowledge of Fundamentals, or financial industry highly preferred.\nExperience in conducting application design and code reviews\nProficiency with following technologies:\nObject-oriented programming\nPrograming Languages (C#, .Net Core)\nCloud Computing\nDatabase systems (SQL, MS SQL)\nNice to have: No-SQL (Databricks, Spark, Scala, python), Scripting (Bash, Scala, Perl, Powershell)\nPreferred Qualifications:\nHands-on experience with cloud computing platforms including AWS , Azure , or Google Cloud Platform (GCP) .\nProficient in working with Snowflake and Databricks for cloud-based data analytics and processing.\nBenefits:\nHealth & Wellness: Health care coverage designed for the mind and body.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GitHub Copilot', 'AI/ML', 'Kibana', 'python', 'GitHub', 'Scala', 'AWS EMR', 'Grafana', 'Odata', 'Big Data platforms', 'Terraform', 'Apache Kafka', 'Databricks', 'Spark']",2025-06-12 14:27:58
"Startup Account Manager, North",Amazon,10 - 15 years,Not Disclosed,['Gurugram'],"Sales, Marketing and Global Services (SMGS)\n\nAWS Sales, Marketing, and Global Services (SMGS) is responsible for driving revenue, adoption, and growth from the largest and fastest growing smalland mid-market accounts to enterprise-level customers including public sector.\n\nAmazon Web Services (AWS) offers a set of cloud services that enable all companies, from startups to enterprises, to run virtually everything in the cloud, including mobile applications, big data analytics, AI/ML platforms, and microservices/serverless infrastructures. AWS India Pvt. Ltd. , the reseller for cloud services in India, is looking for a Senior Startup Account Manager to help drive the growth of high-potential startups in India.\nYou need to possess passion about Startups, be a self-starter with a strong entrepreneurial spirit who is prepared to work in a fast-paced, often ambiguous environment, execute against ambitious goals, and consistently embrace the Amazon Culture. Your responsibilities will include driving growth and user adoption, migrations and ensuring startups select AWS as their preferred cloud provider in India. You will work closely with counterparts in business development, marketing, solution architecture and partner teams to lead execution of BD plays.\n\nThe candidate should have technical background that enables him/her to drive engagement at the CXO level as well as with software developers and IT architects. The candidate should be an exceptional analytical thinker who thrives in fast-paced dynamic environments and has excellent communication and presentation skills. The candidate should be visioning and executing via collaboration with an extended team to address all startup s needs.\n\n\nEnsure customer success with early and growth stage startups in India\nDrive growth and market share in a defined territory\nAccelerate customer adoption through well-developed BD engagements\nDevelop and execute against a comprehensive account/territory plan.\nCreate & articulate compelling value propositions around AWS services.\nAccelerate customer adoption by engaging Founders, CXO, Board of Directors and VC influencers\nWork with AWS partners to manage joint selling opportunities\nAssist customers in identifying use cases for priority adoption of AWS as well as best practices implementations\nDevelop long-term strategic relationships with key accounts.\n\nA day in the life\nMeet startup CXOs and help them Build on AWS\nLeverage AWS startup programs to support early stage startups to bring idea to market\nTrack investments, technology trends; build coverage plans and oversee execution\nCollaborate with cross functional teams such as Sales, VC BD, Solutions Architect, Partners, Marketing\nEnsure high standards and maintain sales pipeline hygiene\n\nAbout the team\nThe AWS Startups team partners with startups around the world to build, launch, grow, and help scale their business. We don t just support startups with cloud infrastructure, but also partner with our startup customers throughout their journey by providing resources to tackle challenges from early stage fundraising to building technical teams and developing startup culture.\n\nAbout AWS\n\nDiverse Experiences\nAWS values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn t followed a traditional path, or includes alternative experiences, don t let it stop you from applying.\n\nWhy AWS?\nAmazon Web Services (AWS) is the world s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating that s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses.\n\nInclusive Team Culture\nHere at AWS, it s in our nature to learn and be curious. Our employee-led affinity groups foster a culture of inclusion that empower us to be proud of our differences. Ongoing events and learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences, inspire us to never stop embracing our uniqueness.\n\nMentorship & Career Growth\nWe re continuously raising our performance bar as we strive to become Earth s Best Employer. That s why you ll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional.\n\nWork/Life Balance\nWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there s nothing we can t achieve in the cloud. 10+ years of technology experience with a focus on field BD (quota-carrying) Experience in working with Startups in identifying, developing, negotiating, and closing large-scale technology deals.\nExperience in positioning and selling technology to new customers and in new market segments. Experience in proactively growing customer relationships within an account while expanding their understanding of the customer s business.\nExcellent verbal and written communications skills Functioned in an environment where they managed an account list in technology which included large growth in net new opportunities.\nProven track record of consistent territory growth and quota attainment. BA/BS/B.Tech degree required. Masters or MBA is a plus.\nUnderstanding of AWS and/or technology as a service (Iaas,SaaS,PaaS) is preferred.",,,,"['Solution architecture', 'Territory growth', 'Cloud computing', 'big data analytics', 'Cloud Services', 'Analytical', 'PAAS', 'cxo', 'Mobile applications', 'Solution Architect']",2025-06-12 14:28:00
Senior Cloud Engineering Lead,ZS,6 - 11 years,Not Disclosed,['Pune'],"As a Senior Cloud Engineering Lead , you will be an individual contributor and subject matter expert who maintains and participates in the design and implementation of technology solutions. The engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients needs. This position will be working with the latest Amazon Web Services technologies around cloud architecture, infrastructure automation, and network security.\n  What you'll Do:",,,,"['Product management', 'Automation', 'Financial planning', 'Management consulting', 'Cloud', 'Network security', 'Agile', 'Application development', 'Python']",2025-06-12 14:28:02
Oracle/ Informatica/ PLSQL/ ETL/ Snaplogic,Photon,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Job Description:\n\nRole: Orcale, Informatica, PLSQL, ETL\nLocation: Chennai/ Bangalore\nExperience: 5+ Years\nMust have: Orcale, Informatica, PLSQL, ETL\n\nLooking for a candidate with expertise on Oracle Database,  Snaplogic and Oracle PL/SQL with knowledge on AWS cloud.",,,,"['Snaplogic', 'PLSQL', 'Informatica', 'ETL', 'ORACE']",2025-06-12 14:28:05
MSD365 Finance Technical,Hexaware Technologies,6 - 11 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']",MS Dynamics365 Finance Technical\nHybrid\nPAN India\nRequired:\nIntegration and Data Management:\nDesign and implement integrations with other systems.\nData consistency and accuracy during data migration and transformation.,,,,"['D365', 'MSD365', 'MICROSOFT DYNAMICS', 'X++', 'F&O']",2025-06-12 14:28:07
MS Dynamics365 Finance Functional,Hexaware Technologies,6 - 11 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","MS Dynamics365 Finance Functional\nHybrid\nPAN India\n\nDescription:\nWe are looking for a highly skilled Microsoft Dynamics Functional Consultant to join our team.\nThe ideal candidate will have in-depth knowledge of Microsoft Dynamics 365 (Finance and Operations, Customer Engagement, or other modules) and a strong understanding of business processes.",,,,"['MSD365', 'MS dynamics 365', 'MSDynamics365', 'F&O']",2025-06-12 14:28:09
Lead .Net Developer,Conduent,5 - 9 years,Not Disclosed,['Bengaluru'],"Must have skills: \nHands on experience in design and development of Windows based applications using C#, .Net Core, WPF and WCF, Java, Python and React.js /Angular.\nUnderstanding of Apigee /API Gateway concepts for managing and securing APIs.\nExperience with SignalR for real-time communication in.NET applications.\nWorking knowledge of Kubernetes for container orchestration and managing microservices deployments.\n\n :",,,,"['design patterns', 'data structures', 'presentation skills', 'product design', 'ooad', 'kubernetes', 'c++', 'api gateway', 'socket programming', 'unit testing', 'apigee', 'react.js', 'java', 'wcf', 'sdlc process', 'c#', 'python', 'microsoft azure', 'stl', 'mfc', 'wpf', 'angular', '.net core', 'nunit', 'aws', 'sdlc']",2025-06-12 14:28:11
Senior Sap Fico Consultant,IT service and consulting,6 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","We are actively looking out for the candidate having 6- years of experience into SAP FICO\nJob Title - Senior SAP FICO Consultant\nLocation - Bangalore / Pune/ Hyderabad\nWork mode - Hybrid\nNotice period - 15-30 days / Serving notice period\n\nRequired Skills :-\nSAP FI (Finance):\nGeneral Ledger (GL), Accounts Payable (AP), Accounts Receivable (AR), Asset Accounting (AA)\nBank Accounting, Electronic Bank Statement (EBS)\nIntegration with MM and SD\nTax configuration (GST/VAT, Withholding Tax, etc.)\nAutomatic payment program (APP)\nDocument Splitting, Parallel Ledger, Special Purpose Ledger\nSAP CO (Controlling):\nCost Center Accounting (CCA)\nInternal Orders\nProfit Center Accounting (PCA)\nProduct Costing, Profitability Analysis (COPA)\nActivity-Based Costing (ABC)\nIntegration with FI, PP, and SD\nTechnical/Functional Skills:\nStrong SAP FICO configuration and process design experience\nPreparation of Functional Specification Documents (FSDs)\nUnderstanding of ABAP debugging and basic technical design\nExperience with SAP S/4HANA is highly preferred (especially Universal Journal, New Asset Accounting, etc.)\nKnowledge of SAP Fiori apps relevant to finance\nExperience in LSMW, BAPIs, BDCs, and other data migration tools\nWorking knowledge of IDocs, interfaces, and third-party integrations\nGood understanding of authorization concepts and roles in FICO",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP FICO Implementation', 'Sap Configuration', 'SAP Implementation', 'IDOCS', 'mm', 'Sap S Hana', 'gst', 'Profit Center Accounting', 'Bdcs', 'ap', 'ar', 'sd', 'LSMW', 'FSD', 'General Ledger', 'Internal Orders', 'BAPIs', 'Copa', 'Cost Center Accounting', 'Activity Based Costing']",2025-06-12 14:28:14
Senior SAP MM Functional Consultant,Sopra Steria,14 - 18 years,Not Disclosed,['Chennai'],"Role & responsibilities\nLead the workshop and identify the GAP\nShould be autonomous, able to challenge the Business, to orient the business on core solutions without customizations\nConfigure and customize the SAP MM module in alignment with client-specific needs\nLead full-cycle SAP MM implementation projects, including requirements gathering, design, testing, training, and post-implementation support.",,,,"['SAP MM', 'implementation', 'Hana Implementation', 'Sap Mm Hana', 'SAP MM Implementation']",2025-06-12 14:28:16
SAP MM Senior Consultant,Sopra Steria,14 - 18 years,Not Disclosed,['Noida'],"We are looking for a highly experienced Senior SAP MM Consultant with deep expertise in S/4HANA Cloud to join our team. The ideal candidate will bring 14+ years of SAP MM implementation experience, including hands-on involvement in Greenfield S/4HANA projects (preferably in European contexts), and possess the ability to lead end-to-end project life cycles, collaborate with global stakeholders, and ensure solution excellence across business processes.\nLocation: Chennai / Noida\nExperience: 14 - 18 years",,,,"['Computer science', 'variant configuration', 'Data migration', 'SAP MM', 'VAT', 'Stakeholder management', 'Information technology', 'ABAP', 'Analytics', 'Logistics']",2025-06-12 14:28:18
Womens walkin- Automation testing/ETL/UX on 13th June @Pune,Infosys BPM,2 - 4 years,Not Disclosed,['Pune'],"Greeting from Infosys BPM Ltd,\n\nExclusive Women's Walkin drive\n\nWe are hiring for UX with JavaScript, ETL Testing + Python Programming, Automation Testing with Java, Selenium, BDD, Cucumber, ETL DB Testing, ETL Testing Automation skills. Please walk-in for interview on 13th June 2025 at Pune location\n\n\nNote: Please carry copy of this email to the venue and make sure you register your application before attending the walk-in. Please use below link to apply and register your application. Please mention Candidate ID on top of the Resume ***\n\n\nhttps://career.infosys.com/jobdesc?jobReferenceCode=PROGEN-HRODIRECT-215163\n\n\nInterview details\nInterview Date: 13th June 2025\nInterview Time: 10 AM till 1 PM\n\n\nInterview Venue:\nPune:: Hinjewadi Phase 1\nInfosys BPM Limited, Plot No. 1, Building B1, Ground floor, Hinjewadi Rajiv Gandhi Infotech Park, Hinjewadi Phase 1, Pune, Maharashtra-411057\n\n\nPlease find below Job Description for your reference:\n\n\nWork from Office***\nMin 2 years of experience on project is mandate***\nJob Description: UX with JavaScript\nTechnical Design tools (e.g., Photoshop, XD, Figma), strong knowledge of HTML, CSS, JavaScript, and experience with SharePoint customization.\nExperience with Wireframe, Prototype, intuitive, responsive design, Documentation,\nAble to Lead the Team\nNice to have Understanding of SharePoint Framework (SPFx) and modern SharePoint development.\n\nJob Description: ETL Testing + Python Programming\nExperience in Data Migration Testing (ETL Testing), Manual & Automation with Python Programming. Strong on writing complex SQLs for data migration validations.\nWork experience with Agile Scrum Methodology\nFunctional Testing- UI Test Automation using Selenium, Java\nFinancial domain experience\nGood to have AWS knowledge\n\nJob Description: Automation Testing with Java, Selenium, BDD, Cucumber\nHands on exp in Automation.\nJava, Selenium, BDD , Cucumber expertise is mandatory.\nBanking Domian Experience is good.\nFinancial domain experience\nAutomation Talent with TOSCA skills, Payment domain skills is preferable.\n\nJob Description: ETL DB Testing\nStrong experience in ETL testing, data warehousing, and business intelligence.\nStrong proficiency in SQL.\nExperience with ETL tools (e.g., Informatica, Talend, AWS Glue, Azure Data Factory).\nSolid understanding of Data Warehousing concepts, Database Systems and Quality Assurance.\nExperience with test planning, test case development, and test execution.\nExperience writing complex SQL Queries and using SQL tools is a must, exposure to various data analytical functions.\nFamiliarity with defect tracking tools (e.g., Jira).\nExperience with cloud platforms like AWS, Azure, or GCP is a plus.\nExperience with Python or other scripting languages for test automation is a plus.\nExperience with data quality tools is a plus.\nExperience in testing of large datasets.\nExperience in agile development is must\nUnderstanding of Oracle Database and UNIX/VMC systems is a must\n\nJob Description: ETL Testing Automation\nStrong experience in ETL testing and automation.\nStrong proficiency in SQL and experience with relational databases (e.g., Oracle, MySQL, PostgreSQL, SQL Server).\nExperience with ETL tools and technologies (e.g., Informatica, Talend, DataStage, Apache Spark).\nHands-on experience in developing and maintaining test automation frameworks.\nProficiency in at least one programming language (e.g., Python, Java).\nExperience with test automation tools (e.g., Selenium, PyTest, JUnit).\nStrong understanding of data warehousing concepts and methodologies.\nExperience with CI/CD pipelines and version control systems (e.g., Git).\nExperience with cloud-based data warehouses like Snowflake, Redshift, BigQuery is a plus.\nExperience with data quality tools is a plus.\n\n\n\nREGISTRATION PROCESS:\nThe Candidate ID & SHL Test(AMCAT ID) is mandatory to attend the interview. Please follow the below instructions to successfully complete the registration. (Talents without registration & assessment will not be allowed for the Interview).\n\nCandidate ID Registration process:\nSTEP 1: Visit: https://career.infosys.com/joblist\nSTEP 2: Click on ""Register"" and provide the required details and submit.\nSTEP 3: Once submitted, Your Candidate ID(100XXXXXXXX) will be generated.\nSTEP 4: The candidate ID will be shared to the registered Email ID.\n\nSHL Test(AMCAT ID) Registration process:\nThis assessment is proctored, and talent gets evaluated on Basic analytics, English Comprehension and writex (email writing).\n\nSTEP 1: Visit: https://apc01.safelinks.protection.outlook.com/?url=https://autologin-talentcentral.shl.com/?link=https://amcatglobal.aspiringminds.com/?data=JTdCJTIybG9naW4lMjIlM0ElN0IlMjJsYW5ndWFnZSUyMiUzQSUyMmVuLVVTJTIyJTJDJTIyaXNBdXRvbG9naW4lMjIlM0ExJTJDJTIycGFydG5lcklkJTIyJTNBJTIyNDE4MjQlMjIlMkMlMjJhdXRoa2V5JTIyJTNBJTIyWm1abFpUazFPV1JsTnpJeU1HVTFObU5qWWpRNU5HWTFOVEU1Wm1JeE16TSUzRCUyMiUyQyUyMnVzZXJuYW1lJTIyJTNBJTIydXNlcm5hbWVfc3E5QmgxSWI5NEVmQkkzN2UlMjIlMkMlMjJwYXNzd29yZCUyMiUzQSUyMnBhc3N3b3JkJTIyJTJDJTIycmV0dXJuVXJsJTIyJTNBJTIyJTIyJTdEJTJDJTIycmVnaW9uJTIyJTNBJTIyVVMlMjIlN0Q=&apn=com.shl.talentcentral&ibi=com.shl.talentcentral&isi=1551117793&efr=1&data=05|02|omar.muqtar@infosys.com|a7ffe71a4fe4404f3dac08dca01c0bb3|63ce7d592f3e42cda8ccbe764cff5eb6|0|0|638561289526257677|Unknown|TWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0=|0|||&sdata=s28G3ArC9nR5S7J4j/V1ZujEnmYCbysbYke41r5svPw=&reserved=0\n\nSTEP 2: Click on ""Start new test"" and follow the instructions to complete the assessment.\nSTEP 3: Once completed, please make a note of the AMCAT ID( Access you Amcat id by clicking 3 dots on top right corner of screen).\n\nNOTE:\nDuring registration, you'll be asked to provide the following information:\nPersonal Details: Name, Email Address, Mobile Number, PAN number.\nAvailability: Acknowledgement of work schedule preferences (Shifts, Work from Office, Rotational Weekends, 24/7 availability, Transport Boundary) and reason for career change.\nEmployment Details: Current notice period and total annual compensation (CTC) in the format 390000 - 4 LPA (example).\nCandidate Information: 10-digit candidate ID starting with 100XXXXXXX, Gender, Source (e.g., Vendor name, Naukri/LinkedIn/Found it, or Direct), and Location\nInterview Mode: Walk-in\nAttempt all questions in the SHL Assessment app.\nThe assessment is proctored, so choose a quiet environment.\nUse a headset or Bluetooth headphones for clear communication.\nA passing score is required for further interview rounds.\n5 or above toggles, multi face detected, face not detected, or any malpractice will be considered rejected\nOnce you've finished, submit the assessment and make a note of the AMCAT ID (15 Digit) used for the assessment.\n\nDocuments to Carry:\nPlease have a note of Candidate ID & AMCAT ID along with registered Email ID.\nPlease do not carry laptops/cameras to the venue as these will not be allowed due to security restrictions.\nPlease carry 2 set of updated Resume/CV (Hard Copy).\nPlease carry original ID proof for security clearance.\nPlease carry individual headphone/Bluetooth for the interview.\n\nPointers to note:\nPlease do not carry laptops/cameras to the venue as these will not be allowed due to security restrictions.\nOriginal Government ID card is must for Security Clearance.\n\nRegards,\nInfosys BPM Recruitment team.",Industry Type: BPM / BPO,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['ETL DB', 'UX', 'Automation Testing', 'ETL', 'ETL Testing', 'Javascript', 'Python']",2025-06-12 14:28:20
Transition Analyst,Capco,5 - 7 years,8-10 Lacs P.A.,['Pune'],"About Us\nCapco, a Wipro company, is a global technology and management consulting firm. Awarded with Consultancy of the year in the British Bank Award and has been ranked Top 100 Best Companies for Women in India 2022 by Avtar & Seramount. With our presence across 32 cities across globe, we support 100+ clients across banking, financial and Energy sectors. We are recognized for our deep transformation execution and delivery.\nWHY JOIN CAPCO?\nYou will work on engaging projects with the largest international and local banks, insurance companies, payment service providers and other key players in the industry. The projects that will transform the financial services industry.\nMAKE AN IMPACT\nInnovative thinking, delivery excellence and thought leadership to help our clients transform their business. Together with our clients and industry partners, we deliver disruptive work that is changing energy and financial services.\n#BEYOURSELFATWORK\nCapco has a tolerant, open culture that values diversity, inclusivity, and creativity.\nCAREER ADVANCEMENT\nWith no forced hierarchy at Capco, everyone has the opportunity to grow as we grow, taking their career into their own hands.\nDIVERSITY & INCLUSION\nWe believe that diversity of people and perspective gives us a competitive advantage.\nMAKE AN IMPACT\nJob Title: Transition Analyst\nLocation: Pune\nExperience Required:\nBachelorâ€™s degree is required.\nMinimum 6 to 8 years of total work experience.\nMinimum 1 to 2 years of relevant experience in Project/Program Management or Support roles.\nTechnical & Functional Expertise:\nTechnical:\nProficiency in MS Office products including Office 365, Project Online, SharePoint, Power BI, and other analytics tools.\nStrong understanding of process workflow design, data architecture, and related tools.\nFunctional:\nStrong business acumen and functional understanding.\nExperience in planning and monitoring for program workstreams, project deliverables, and reporting.\nAbility to handle transition-related documentation, administrative tasks, risk management, due diligence, and stakeholder coordination.\nKey Responsibilities:\nSupport planning and execution of program and transition projects.\nTrack deliverables, manage risks, and ensure timely reporting.\nEnsure compliance with GBS methodologies and toolkits.\nManage travel and logistics for transition-related requirements.\nCoordinate with operational teams and business functions for successful transitions.\nLead the documentation of SOPs and manage sign-off processes.\nCollaborate with various business units including Procurement, Finance, and IT.\nSupport project reporting, dashboard preparation, and Power BI-based analytics.\nHandle highly confidential material with discretion and professionalism.\nParticipate in customer-facing meetings and internal stakeholder communications.\nFacilitate workshops, team meetings, and process improvement initiatives.\nKey Challenges:\nNavigating fragmented systems and tools.\nEngaging a wide range of stakeholders across global functions.\nManaging services at a large scale with geographical and cultural diversity.\nAdapting to evolving digital technologies and technical tools.\nEnsuring alignment with global process design standards.\nSkills & Competencies:\nCore Skills:\nProject planning and reporting skills\nWorkflow and process documentation\nRisk identification and mitigation\nData visualization and reporting tools (especially Power BI)\nSoft Skills:\nExcellent multitasking and prioritization skills\nStrong interpersonal, presentation, and written communication skills\nFluency in English (spoken and written)\nKnowledge of local regulations and compliance standards\nFamiliarity with Puneâ€™s local business environment\nAbility to work effectively in a regional service center ecosystem",Industry Type: BPM / BPO,Department: Consulting,"Employment Type: Full Time, Permanent","['Project Management', 'Transition Management', 'Process Transition', 'Process Migration', 'Transition Planning', 'Business Transition', 'Project Governance', 'Project Transition']",2025-06-12 14:28:23
Transistion Analyst,Capco,1 - 8 years,Not Disclosed,['Pune'],"About Us\nCapco, a Wipro company, is a global technology and management consulting firm. Awarded with Consultancy of the year in the British Bank Award and has been ranked Top 100 Best Companies for Women in India 2022 by\nAvtar & Seramount\n. With our presence across 32 cities across globe, we support 100+ clients across banking, financial and Energy sectors. We are recognized for our deep transformation execution and delivery.\nWHY JOIN CAPCO?\nYou will work on engaging projects with the largest international and local banks, insurance companies, payment service providers and other key players in the industry. The projects that will transform the financial services industry.\nMAKE AN IMPACT\nInnovative thinking, delivery excellence and thought leadership to help our clients transform their business. Together with our clients and industry partners, we deliver disruptive work that is changing energy and financial services.\n#BEYOURSELFATWORK\nCapco has a tolerant, open culture that values diversity, inclusivity, and creativity.\nCAREER ADVANCEMENT\nWith no forced hierarchy at Capco, everyone has the opportunity to grow as we grow, taking their career into their own hands.\nDIVERSITY & INCLUSION\nWe believe that diversity of people and perspective gives us a competitive advantage.\nMAKE AN IMPACT\nJob Title: Transition Analyst\nLocation: Pune\nExperience Required:\nBachelor s degree is required.\nMinimum 6 to 8 years of total work experience.\nMinimum 1 to 2 years of relevant experience in Project/Program Management or Support roles.\nTechnical & Functional Expertise:\nTechnical:\nProficiency in MS Office products including Office 365, Project Online, SharePoint, Power BI, and other analytics tools.\nStrong understanding of process workflow design, data architecture, and related tools.\nFunctional:\nStrong business acumen and functional understanding.\nExperience in planning and monitoring for program workstreams, project deliverables, and reporting.\nAbility to handle transition-related documentation, administrative tasks, risk management, due diligence, and stakeholder coordination.\nExperience in knowledge transfer, SOP documentation, and hyper care support.\nKey Responsibilities:\nSupport planning and execution of program and transition projects.\nTrack deliverables, manage risks, and ensure timely reporting.\nEnsure compliance with GBS methodologies and toolkits.\nManage travel and logistics for transition-related requirements.\nCoordinate with operational teams and business functions for successful transitions.\nLead the documentation of SOPs and manage sign-off processes.\nCollaborate with various business units including Procurement, Finance, and IT.\nSupport project reporting, dashboard preparation, and Power BI-based analytics.\nHandle highly confidential material with discretion and professionalism.\nParticipate in customer-facing meetings and internal stakeholder communications.\nFacilitate workshops, team meetings, and process improvement initiatives.\nKey Challenges:\nNavigating fragmented systems and tools.\nEngaging a wide range of stakeholders across global functions.\nManaging services at a large scale with geographical and cultural diversity.\nAdapting to evolving digital technologies and technical tools.\nEnsuring alignment with global process design standards.\nSkills & Competencies:\nCore Skills:\nProject planning and reporting skills\nWorkflow and process documentation\nRisk identification and mitigation\nData visualization and reporting tools (especially Power BI)\nSoft Skills:\nExcellent multitasking and prioritization skills\nStrong interpersonal, presentation, and written communication skills\nFluency in English (spoken and written)\nKnowledge of local regulations and compliance standards\nFamiliarity with Pune s local business environment\nAbility to work effectively in a regional service center ecosystem",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Procurement', 'Administration', 'Management consulting', 'Project planning', 'MS Office', 'Risk management', 'Financial services', 'Analytics', 'Monitoring', 'Logistics']",2025-06-12 14:28:25
Associate Director-Oracle Techno-Functional,Acuity Knowledge Partners,10 - 17 years,Not Disclosed,['Gurugram'],"Acuity Knowledge Partners\nAcuity Knowledge Partners (Acuity) is a leading provider of bespoke research, analytics and technology solutions to the financial services sector, including asset managers, corporate and investment banks, private equity and venture capital firms, hedge funds and consulting firms. Its global network of over 6,000 analysts and industry experts, combined with proprietary technology, supports more than 600 financial institutions and consulting companies to operate more efficiently and unlock their human capital, driving revenue higher and transforming operations. Acuity is headquartered in London and operates from 10 locations worldwide.",,,,"['Oracle Cloud', 'Oracle Fusion', 'Oracle Finance', 'Oracle Fusion Technical', 'Oracle Cloud Applications', 'Erp Cloud', 'Oic', 'Fusion Financials', 'Oracle Fusion Financials', 'Oracle ERP', 'Oracle Peoplesoft Financials']",2025-06-12 14:28:27
Senior Database Engineer,Cvent,3 - 6 years,Not Disclosed,['Gurugram'],"Design, develop, and manage databases on the AWS cloud platform Develop and maintain automation scripts or jobs to perform routine database tasks such as provisioning, backups, restores, and data migrations\nBuild and maintain automated testing frameworks for database changes and upgrades to minimize the risk of introducing errors\nImplement self-healing mechanisms to automatically recover from database failures or performance degradation\nIntegrate database automation tools with CI/CD pipelines to enable continuous delivery and deployment of database changes",,,,"['Hospitality', 'Database design', 'database security', 'MySQL', 'Disaster recovery', 'Troubleshooting', 'Event marketing', 'Information technology', 'Python']",2025-06-12 14:28:29
Oracle EBS /Cloud/ Fusion SCM Consultant,Infosys,2 - 5 years,Not Disclosed,['Pune'],"Job Title\nOracle EBS /Cloud/ Fusion SCM Consultant\n\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys consulting team, your primary role would be to actively aid the consulting team in different phases of the project including problem definition, effort estimation, diagnosis, solution generation and design and deployment\nYou will explore the alternatives to the recommended solutions based on research that includes literature surveys, information available in public domains, vendor evaluation information, etc. and build POCs\nYou will create requirement specifications from the business needs, define the to-be-processes and detailed functional designs based on requirements.\nYou will support configuring solution requirements on the products; understand if any issues, diagnose the root-cause of such issues, seek clarifications, and then identify and shortlist solution alternatives\nYou will also contribute to unit-level and organizational initiatives with an objective of providing high quality value adding solutions to customers. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! Technical and Professional :\nMinimum 2 years of implementation experience with Oracle EBS/Cloud/Fusion in Order Management, Procurement, Inventory, Bill of Material, Manufacturing modules\nHave at least 1 full life cycle implementations experience, with hands-on configuration, implementation, and support of Oracle EBS/Cloud/Fusion SCM\nResponsible for leading the requirements elicitation, fit-gap analysis, development, configuration, functional testing, and post-production support\nHave experience with data migration using FBDI\nStrong experience in gathering requirements, designing solutions for Very High transaction volumes and should have good experience of Performance Testing of solutions\nShould have experience of designing and delivering complex custom solutions in highly integrated applications landscape\nExperience in handling integration with external partners/ applications like E-Commerce Portals, Part Catalogs, trading partners - Suppliers & Customers, EDI Preferred Skills:\nTechnology-Oracle eBS Functional-Oracle Order Management Technology-Oracle Cloud-Oracle Planning Cloud Additional Responsibilities:\nAbility to work with clients to identify business challenges and contribute to client deliverables by refining, analyzing, and structuring relevant data\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate\nAbility to assess the current processes, identify improvement areas and suggest the technology solutions\nOne or two industry domain knowledge Educational Master Of Commerce,Master Of Engineering,Master Of Science,Master Of Technology,Master of Business Administration,Bachelor Of Commerce,Bachelor Of Science,Bachelor of Engineering,Bachelor Of Technology Service LineEnterprise Package Application Services* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle EBS', 'solution design', 'order management', 'procurement', 'Cloud', 'oracle e-business suite', 'Fusion SCM Consultant']",2025-06-12 14:28:31
Sr. Application Developer,XL India Business Services Pvt. Ltd,3 - 7 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Senior Application Developer - Salesforce Bangalore, Karnataka India AXA XL are creating a new delivery model based on agile and a new vendor model to enable more efficient delivery of technology with the business fully embedded in what we deliver\n\nThis Digital Factory will first deliver new capabilities for Fusion Transformation and then will be rolled out to other capabilities within GT (Global Technology) and IDA (Innovation, Data & Analytics)\n\nWe are seeking a Senior Salesforce Developer with extensive expertise in automated software testing and a passion for coaching others to join our dynamic team\n\nAs a Senior Salesforce Developer, you will play a key role in architecting, designing, and implementing our insurance workbench application on the Salesforce platform, with a focus on test-driven development, automated testing, and continuous integration\n\nWhat you ll be DOING What will your essential responsibilities include? Responsibilities: Drive the development and maintenance of our insurance workbench application on the Salesforce platform, leveraging your expertise to design and integrate relevant modules, customizations, and configurations that optimize workflows and data management\n\nManage and resolve support requests, issues, and escalations from end-users\n\nDiagnose and solve technical issues related to Salesforce applications\n\nPerform administrative tasks such as user account maintenance, reports, dashboards, and workflows\n\nConfigure Salesforce settings for users, roles, security, profiles, and workflow rules\n\nAssist in training new users and enhancing the Salesforce skill set across the organization\n\nCommunicate regularly with the Salesforce user base regarding new features, enhancements, and changes\n\nUpdate, maintain, and create customized reports for different departments and management\n\nEnsure data integrity and perform data imports/exports as needed\n\nWork with management to identify new opportunities to leverage Salesforce for additional business processes\n\nDocument system configurations, processes, and procedures\n\nTest Salesforce updates and new releases to ensure smooth implementation\n\nProvide feedback and suggestions for system improvements\n\nLead and mentor a team of developers, providing technical guidance, conducting code reviews, and promoting best practices in Salesforce development, automated testing, and software quality assurance\n\nBuild and lead the automated software testing chapter for the insurance workbench suite\n\nLead the development of APIs to facilitate data exchange and connectivity with external systems and partners\n\nChampion a product-led Agile approach to development, applying design thinking principles to create intuitive user experiences (UX) and customer experiences (CX) within the workbench application\n\nDrive continuous improvement initiatives, staying ahead of emerging technologies and industry trends, and adapting to changing product requirements\n\nSolution Design and Architecture: Lead the design and architecture of complex solutions on the Salesforce platform, leveraging design thinking methodologies to address business requirements effectively\n\nDefine architectural standards, patterns, and best practices for Salesforce development in alignment with enterprise architecture principles\n\nEnsure solutions are scalable, maintainable, and comply with industry regulations in the commercial insurance domain\n\nTechnical Leadership: Provide technical leadership and guidance to development teams, promoting DevOps practices and CI/CD pipelines for efficient delivery\n\nCollaborate with stakeholders to prioritize features and enhancements, ensuring alignment with business goals and objectives\n\nMentor and coach team members on Salesforce development, DevOps, and enterprise architecture concepts\n\nIntegration and Data Management: Design and implement API integrations with external systems, leveraging Azure services and other technologies as needed\n\nArchitect data streaming solutions for real-time analytics and insights, ensuring data accuracy, integrity, and security\n\nCollaborate with data architects to define data models and ensure proper data governance practices within the Salesforce Org\n\nScaled Agile Delivery: Work within a scaled agile framework, participating in program increment planning, backlog grooming, and sprint ceremonies\n\nFacilitate collaboration between agile teams, resolving dependencies and ensuring alignment with the overall program roadmap\n\nDrive continuous improvement initiatives to enhance delivery processes and optimize team performance\n\nSalesforce Org Oversight: Oversee multiple solutions within the Salesforce org, ensuring cohesiveness, scalability, and maintainability across different business units\n\nConduct regular reviews and audits of Salesforce configurations, identifying opportunities for optimization and enhancement\n\nPartner with Salesforce administrators to establish governance policies, security controls, and release management processes\n\nYou will be reporting to Platform Delivery Lead\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Proven experience as a Senior Salesforce Developer, with a track record of delivering complex solutions in enterprise environments, preferably within the insurance domain\n\nStrong knowledge of Salesforce administration and declarative capabilities\n\nExperience with Salesforce configuration, customization, and data management\n\nExtensive experience with APEX, LWC , Flows and familiarity with cloud-based integration, design thinking, UX/CX\n\nExperience in Salesforce administration, including user account maintenance, security settings, and workflow rules\n\nExperience in application support, including triaging incidents and assisting support teams in resolving issues\n\nProven experience in optimizing the Salesforce platform for performance, security, and audit\n\nExperience in understanding and implementing Salesforce AI capabilities will be an added advantage\n\nHands on experience in Implementing Integrations with various systems\n\nSalesforce devloper 1 certificate mandatory and developer 2 certificate preferable\n\nPrefereable experience in Omni Studio\n\nSignificant leadership and communication skills, with the ability to effectively guide and collaborate with development squads, stakeholders, and business users\n\nA deep understanding of Agile principles, with a passion for driving continuous improvement and innovation\n\nDesired Skills and Abilities: Outstanding ability to coach and mentor team members, helping them to develop expertise in automated testing and software quality assurance\n\nLeading a automation testing chapter\n\nExcellent analytical abilities to identify issues and devise effective solutions in a dynamic environment\n\nA Bachelors or Masters degree in Computer Science, Software Engineering, or equivalent work experience",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Application support', 'Data management', 'Enterprise architecture', 'Agile', 'Workflow', 'Release management', 'Software quality assurance', 'Salesforce', 'Auditing']",2025-06-12 14:28:34
Senior Consultant Technical - ETL + SQL Expertise,Insightsoftware,7 - 11 years,Not Disclosed,['Hyderabad'],"Insightsoftware (ISW) is a growing, dynamic computer software company that helps businesses achieve greater levels of financial intelligence across their organization with our world-class financial reporting solutions. At insightsoftware, you will learn and grow in a fast-paced, supportive environment that will take your career to the next level. The Data Conversion Specialist is a member of the insightsoftware Project Management Office (PMO) who demonstrates teamwork, results orientation, a growth mindset, disciplined execution, and a winning attitude.\nLocation: Hyderabad (Work from Office- Hybrid)\nWorking Hours: 2:30 PM to 11:30 PM for 3 Days 5:00 PM - 2:00AM IST or 6:00 PM to 3:00 AM IST for 2 Days, should be ok to work in night shift as per requirement.\nPosition Summary\nThe Senior Consultant will integrate and map customer data from client source system(s) to our industry-leading platform. The role will include, but is not limited to:\nUsing strong technical data migration, scripting, and organizational skills to ensure the client data is converted efficiently and accurately to the insightsoftware (ISW) platform.\nPerforming extract, transform, load (ETL) activities to ensure accurate and timely data conversions.\nProviding in-depth research and analysis of complex scenarios to develop innovative solutions to meet customer needs whilst remaining within project governance.\nMapping and maintaining business requirements to the solution design using tools such as requirements traceability matrices (RTM).\nPresenting findings, requirements, and problem statements for ratification by stakeholders and working groups.\nIdentifying and documenting data gaps to allow change impact and downstream impact analysis to be conducted.\nExperience assessing data and analytic requirements to establish mapping rules from source to target systems to meet business objectives.\nExperience with real-time, batch, and ETL for complex data conversions.\nWorking knowledge of extract, transform, load (ETL) methodologies and tools such as Talend, Dell Boomi, etc.\nUtilize data mapping tools to prepare data for data loads based on target system specifications.\nWorking experience using various data applications/systems such as Oracle SQL, Excel, .csv files, etc.\nStrong SQL scripting experience.\nCommunicate with clients and/or ISW Project Manager to scope, develop, test, and implement conversion/integration\nEffectively communicate with ISW Project Managers and customers to keep project on target\nContinually drive improvements in the data migration process.\nCollaborate via phone and email with clients and/or ISW Project Manager throughout the conversion/integration process.\nDemonstrated collaboration and problem-solving skills.\nWorking knowledge of software development lifecycle (SDLC) methodologies including, but not limited to: Agile, Waterfall, and others.\nClear understanding of cloud and application integrations.\nAbility to work independently, prioritize tasks, and manage multiple tasks simultaneously.\nEnsure client s data is converted/integrated accurately and within deadlines established by ISW Project Manager.\nExperience in customer SIT, UAT, migration and go live support.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data migration', 'Data conversion', 'Financial reporting', 'Project management', 'project governance', 'Agile', 'Software development life cycle', 'data mapping', 'SDLC', 'Downstream']",2025-06-12 14:28:36
Senior Workday Integration Developer,Luxoft,5 - 10 years,Not Disclosed,['Gurugram'],"Act as the Tech Business Analyst in the end-to-end implementation of Workday integrations.\nGather integration requirements from downstream systems and document detailed design specifications.\nSupport all project phases, including design, build, testing (SIT), PRR, and deployment.\nDocument field mappings, transformation logic, user stories, and cost trackers.\nCreate and update process models using enterprise modeling tools.\nFacilitate alignment between business and technical teams to ensure smooth delivery.\nConduct business analysis such as scenario, data, gap, and change impact analysis.\nDevelop process flows, data models, business rules, and reporting structures as part of solution design.\nMaintain internal documentation repositories (e.g., Confluence) and manage stakeholder updates.\nCollaborate with cross-functional teams, including integration, development, QA, and business stakeholders.\nSupport vendor coordination, testing, cutover, and go-live planning.\nApply agile practices to ensure high-quality, user-centric outcomes.\nSkills\nMust have\n5+ years of experience as a Senior Workday Integration Developer/ Consultant with proven expertise in Workday integrations, configuration, and system optimization.\nStrong experience in Workday Integrations, including design and delivery.\nProven integration experience with: oCustomer Master ADA [MEID] oBenevity, Clarity, ODI oAspect & ControliQ (Bidirectional Time & Attendance interfaces) oODB and GPR\nUK\nProficient in data mapping, transformation logic, and integration architecture.\nExperience with Workday security frameworks, authentication protocols (OAuth, SAML), and data governance.\nAbility to translate business requirements into scalable technical solutions.\nExcellent analytical, documentation, and process modeling skills.\nStrong workshop facilitation and stakeholder engagement capabilities.\nProficiency working within agile delivery environments.\nNice to have\nWorkday Certified Integration Developer with exposure to large-scale implementations.\nExperience with reporting and data migration in cloud-based HR platforms.\nWorking knowledge of enterprise modeling tools and data lake environments.\nAdvanced Excel and experience with data analytics and validation.\nFamiliarity with compliance, regulatory reporting, and change management practices.\nStrong communication and stakeholder management across technical and non-technical teams.\nDemonstrated ability to work independently, manage risks, and adapt in fast-paced environments.\nExperience collaborating with external vendors and integration partners.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Change management', 'Data migration', 'Business analysis', 'Analytical', 'Agile', 'business rules', 'data mapping', 'SAML', 'Stakeholder management', 'Downstream']",2025-06-12 14:28:39
Senior Software Engineer-7916,WebMD,5 - 10 years,Not Disclosed,['Navi Mumbai'],"Position: Senior Software Engineer (Data Engineer)\nNo. of Positions: 1\nAbout WebMD:\nHeadquartered in El Segundo, Calif., Internet Brands is a fully integrated online media and software services\norganization focused on four high-value vertical categories: Health, Automotive, Legal, and Home/Travel. The\ncompanys award-winning consumer websites lead their categories and serve more than 250 million monthly\nvisitors, while a full range of web presence offerings has established deep, long-term relationships with SMB and\nenterprise clients. Internet Brands,powerful, proprietary operating platform provides the flexibility and\nscalability to fuel the companys continued growth. Internet Brands is a portfolio company of KKR and Temasek.\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health information services, serving\npatients, physicians, health care professionals, employers, and health plans through our public and private online\nportals, mobile platforms, and health-focused publications. The WebMD Health Network includes WebMD\nHealth, Medscape, Jobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals\nConsumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape Education, and other owned\nWebMD sites. WebMD, Medscape, CME Circle, Medpulse, eMedicineÂ®, MedicineNetÂ®, theheart.orgÂ®, and\nRxListÂ® are among the trademarks of WebMD Health Corp. or its subsidiaries.\nFor Company details, visit our website: www.webmd.com / www.internetbrands.com\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex,\nsexual orientation, gender identity, national origin, disability, or veteran status\nEducation: B.E. Computer Science/IT degree (or any other engineering discipline)\nExperience: 5+ years\nWork timings: 2:00 PM to 11:00 PM IST.\nDescription:\nWe are seeking an experienced and passionate Senior Software Developer to join our team. In this role,\nyou will work closely with cross-functional teams, developers, stakeholders, and business units to\ngather and analyze business requirements, design, build and implement ETL Solutions, and maintain\nthe infrastructure. The ideal candidate will have a strong background in business analysis, SQL, Unix,\nPython, ETL Tools to ensure the successful execution of projects.\nResponsibilities:\nLead requirements gathering sessions with key stakeholders to understand business needs and\nobjectives.\nCollaborate with and across Agile teams to design, develop, test, implement and support ETL\nprocesses for data transformation and preparation.\nManage data pipelines for analytics and operational use.\nEnsure data quality, data accuracy and integrity across multiple sources and systems.\nPerform unit tests and conduct reviews with other team members to make sure your code is\nrigorously designed, elegantly coded, and effectively tuned for performance.\nShould be able to come up with multiple approaches to any ETL\nproblem statement/solution/technical challenge and take well informed decision to pick the\nbest solution.\nAutomate ETL Processes using Cron and/or using Job Scheduler tools like AirFlow.\nAdhere to company standards and Serve as a key contributor to the design and development of\nexception handling, code/data standardization procedures, resolution steps and Quality Assurance\ncontrols.\nMaintain a version repository and ensure version control.\nCreate visual aids such as diagrams, charts, and screenshots to enhance documentation.\nWork with Infrastructure/systems team and developers to ensure all modules are up-to- date and\nare compatible with the code.",,,,"['ETL', 'SQL', 'UNIX', 'Python']",2025-06-12 14:28:41
"R&D Member of Technical Staff II, Product Development",Aveva,2 - 6 years,Not Disclosed,['Hyderabad'],"job requisition idR010044\n\nAVEVA is creating software trusted by over 90% of leading industrial companies.\n\nJob TitleR&D Member of Technical Staff II, Product Development\n\nLocationHyderabad\n\nEmployment Type Full-time\n\nBenefits Gratuity, Medical and accidental insurance, very attractive leave entitlement, emergency leave days, childcare support, maternity, paternity and adoption leaves, education assistance program, home office set up support (for hybrid roles), well-being support\n\nThe job\n\nAs a part of this function, the Cloud Unified Engineering team of R&D Technology and Execution Development, the development engineer will be responsible for ensuring the high quality of AVEVA Software Product deliveries to customers on the CONNECT cloud platform.\n\nKey responsibilities\n\nWe are looking for a Developer with skills to design and develop required functionality in the Cloud Unified Engineering solution / platform. This will include implement new user stories, write unit tests and fixing the reported defects by system test, DevOps teams.\n\nAlso, proactively identify improvements and enhancements to existing unit test cases test suites. This role reports into Development manager located in Hyderabad, India.\n\nEssential requirements\n\nDevelopment experience with exposure to programming skills, e.g. C#, .NET.\nProven experience on Amazon Web Services (AWS)Desired skills and competenciesKnowledge of PowerShell or Node JS scripting.Knowledge of AWS CloudFormation and Infrastructure as Code (IaC).\nKnowledge of API, REST, microservices and serverless architecture\nKnowledge and experience of operational support, software development and deployment methodologies and principles.\nHands-on in AWS administration, AWS APIs and tools or equivalent Azure experience.\nStrong written, verbal and presentation skills, able to convey information clearly and concisely to technical and non-technical audiences.R&D at AVEVA Our global team of 2000+ developers work on an incredibly diverse portfolio of over 75 industrial automation and engineering products, which cover everything from data management to 3D design. AI and cloud are at the centre of our strategy, and we have over 150 patents to our name.Our track record of innovation is no fluke its the result of a structured and deliberate focus on learning, collaboration and inclusivity. If you want to build applications that solve big problems, join us.Find out moreaveva.com/en/about/careers/r-and-d-careers/ India Benefits include:Gratuity, Medical and accidental insurance, very attractive leave entitlement, emergency leave days, childcare support, maternity, paternity and adoption leaves, education assistance program, home office set up support (for hybrid roles), well-being support Its possible were hiring for this position in multiple countries, in which case the above benefits apply to the primary location. Specific benefits vary by country, but our packages are similarly comprehensive.Find out moreaveva.com/en/about/careers/benefits/ Hybrid workingBy default, employees are expected to be in their local AVEVA office three days a week, but some positions are fully office-based. Roles supporting particular customers or markets are sometimes remote.Hiring processInterestedGreat! Get started by submitting your cover letter and CV through our application portal. AVEVA is committed to recruiting and retaining people with disabilities. Please let us know in advance if you need reasonable support during your application process.Find out moreaveva.com/en/about/careers/hiring-process",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['presentation skills', 'aws administration', 'microsoft azure', 'aws cloudformation', 'aws', 'c#', 'rest', 'software development', 'web services', 'artificial intelligence', 'microservices', 'node.js', 'devops', 'powershell', 'product development', '.net', 'api', 'product engineering']",2025-06-12 14:28:43
Senior Etl Informatica Developer,VBeyond,6 - 8 years,19-25 Lacs P.A.,"['Noida', 'Chennai', 'Bengaluru']","We are seeking a highly skilled and experienced Senior ETL & Reporting QA Analyst to join our dynamic team. The ideal candidate will bring strong expertise in ETL and Report Testing, with a solid command of SQL, and hands-on experience in Informatica, as well as BI Reporting tools. A strong understanding of the Insurance domain is crucial to this role. This position will be instrumental in ensuring the accuracy, reliability, and performance of our data pipelines and reporting solutions.\n\nKey Responsibilities:\nDesign, develop, and execute detailed test plans and test cases for ETL processes, data migration, and data warehousing solutions.\nPerform data validation and data reconciliation using complex SQL queries across various source and target systems.\nValidate Informatica ETL workflows and mappings to ensure accurate data transformation and loading.\nConduct end-to-end report testing and dashboard validations using Cognos (preferred), or comparable BI tools such as Tableau or Power BI.\nCollaborate with cross-functional teams including Business Analysts, Developers, and Data Engineers to understand business requirements and transform them into comprehensive test strategies.\nIdentify, log, and track defects to closure using test management tools and actively participate in defect triage meetings.\nMaintain and enhance test automation scripts and frameworks where applicable.\nEnsure data integrity, consistency, and compliance across reporting environments, particularly in the insurance domain context.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Informatica', 'ETL', 'Power Bi', 'Insurance Domain', 'Tableau', 'SQL']",2025-06-12 14:28:46
Java + Springboot Lead,Infosys,5 - 9 years,Not Disclosed,['Chennai'],"Job Title\nJava + Springboot Lead\n\nResponsibilities\nLead and mentor a team of Java & Springboot Developers in the design, development, and maintenance of applications.Work with business stakeholders and technical teams to gather and analyze requirements for Java & Springboot applications.Design, develop, and enhance software solutions using Java & Springboot, including Microservices, MVC, Spring Data, and Spring Security.Write efficient and well-structured code to implement business logic and functionality on the Java platform.Perform unit testing and debugging to ensure the quality and reliability of developed applications.Maintain and enhance existing Java & Springboot applications by troubleshooting issues, implementing bug fixes, and optimizing performance.Collaborate with other developers, database administrators, and system administrators to integrate Java & Springboot applications with other systems and databases.Develop and maintain technical documentation, including system design, coding standards, and user manuals.Stay updated with the latest Java & Springboot technologies and industry trends, and recommend improvements or alternative solutions to enhance system performance and efficiency.Collaborate with cross-functional teams to support system integration, data migration, and software deployment activities.Participate in code reviews and provide constructive feedback to ensure adherence to coding standards and best practices.Proactively identify and address potential risks or issues related to Java & Springboot applications and propose appropriate solutions.Provide leadership and guidance to the team and create a positive and productive work environment.Manage the team's workload and ensure that projects are completed on time and within budget.Delegate tasks and responsibilities to team members and provide regular feedback.Identify and develop the team's strengths and weaknesses and provide opportunities for professional growth.\n\nTechnical and Professional :\nPrimary skills:Technology-Java-Springboot\nPreferred Skills:\nTechnology-Java-Springboot Additional Responsibilities:Bachelor's degree in Computer Science, Information Technology, or a related field.Minimum of 5 years of experience as a Java & Springboot Developer, with at least 3 years of team handling experienceStrong understanding of Java programming concepts, including object-oriented programming, data structures, and algorithms.Proficiency in Springboot framework, including Microservices, MVC, Spring Data, and Spring Security.Extensive experience with Java development tools, such as Eclipse and IntelliJ IDEA.Deep familiarity with relational databases, particularly MySQL and PostgreSQL.Expert knowledge of Java performance tuning and optimization techniques.Excellent problem-solving and analytical skills.Strong written and verbal communication skills, with the ability to effectively communicate technical concepts to both technical and non-technical stakeholders.Detail-oriented with a commitment to delivering high-quality software solutions.Proven ability to lead and mentor a team of developers.Leadership and management skills Educational Master Of Comp. Applications,Bachelor Of Comp. Applications,Bachelor of Engineering,Bachelor Of Technology Service LineApplication Development and Maintenance* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'postgresql', 'performance tuning', 'java development', 'mysql', 'spring boot framework', 'relational databases']",2025-06-12 14:28:48
Solution Design Lead & implimantation,Excellerate Global Solutions,13 - 23 years,10-20 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Solution Design & Implementation:\nLead and participate in the full project lifecycle of SAP S/4HANA Public Cloud implementations, focusing on Procurement (Sourcing & Procurement/MM), Finance (FI/CO), and Sales & Distribution (SD) modules.\nConduct in-depth business process analysis, gather requirements, and translate them into robust and scalable SAP S/4HANA Public Cloud solutions aligned with SAP best practices.\nDesign, configure, and customize SAP S/4HANA Public Cloud functionalities for Order-to-Cash (O2C), Procure-to-Pay (P2P), Record-to-Report (R2R), and other relevant cross-functional processes.\nEnsure seamless integration between Procurement, Finance, and SD modules, as well as with other SAP Cloud modules (e.g., EWM, PP) and third-party applications where applicable.\nLeverage SAP Activate methodology for project delivery, guiding clients through fit-to-standard workshops and solution design.\nFunctional Expertise:\nProcurement (MM): Expertise in Material Master, Vendor Master, Purchase Requisitions, Purchase Orders, Contracts, Sourcing, Inventory Management, Invoice Verification, and supplier collaboration.\nFinance (FI/CO): Strong knowledge of General Ledger, Accounts Payable, Accounts Receivable, Asset Accounting, Bank Accounting, Cost Center Accounting, Profit Center Accounting, Internal Orders, Product Costing, Profitability Analysis (CO-PA), and treasury functions. Understanding of the Universal Journal (ACDOCA) and its impact.\nSales & Distribution (SD): Proficiency in Sales Order Management, Pricing, Delivery Processing, Billing, Credit Management, Returns Management, and ATP (Available-to-Promise).\nTechnical Acumen (Public Cloud Specific):\nUnderstanding of SAP S/4HANA Public Cloud architecture, standard scope, extensibility options (e.g., in-app extensibility, side-by-side extensions using SAP BTP).\nFamiliarity with SAP Fiori applications and user interfaces for relevant modules.\nKnowledge of data migration strategies and tools within the Public Cloud environment (e.g., Migration Cockpit).\nExperience with SAP Cloud ALM for implementation, operations, and monitoring.\nClient Engagement & Leadership:\nAct as a trusted advisor to clients, effectively communicating complex technical and functional concepts to both business and IT stakeholders.\nLead workshops, facilitate discussions, and drive decisions throughout the project lifecycle.\nProvide expert guidance on cloud transformation strategies, change management, and user adoption.\nMentor and guide junior consultants, fostering a culture of knowledge sharing and continuous improvement.\nTesting, Training & Support:\nDevelop and execute comprehensive test plans (unit, integration, UAT) to ensure the solution meets business requirements and is defect-free.\nPrepare detailed training materials and conduct engaging training sessions for end-users.\nProvide post-implementation support, troubleshoot issues, and drive resolution in collaboration with technical teams.\nContinuous Improvement & Innovation:\nStay updated with the latest SAP S/4HANA Public Cloud releases, functionalities, and industry best practices.\nIdentify opportunities for process optimization and leverage new SAP innovations (e.g., AI, Machine Learning capabilities within S/4HANA) to enhance client value.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Sap Hana', 'Solution Design', 'SAP FICO', 'SAP SD', 'SAP MM', 'SAP Finance']",2025-06-12 14:28:50
"Spark, Java, Kafka- Hyderabad",Cognizant,12 - 15 years,Not Disclosed,['Hyderabad'],"Skill: Java, Spark, Kafka\nExperience: 10 to 16 years\nLocation: Hyderabad\n As Data Engineer, you will :\n       Support in designing and rolling out the data architecture and infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources\n       Identify data source, design  and implement data schema/models and integrate data that meet the requirements of the business stakeholders",,,,"['hive', 'cloudera', 'modeling', 'scala', 'data warehousing', 'apache pig', 'data pipeline', 'data architecture', 'scalability', 'sql', 'java', 'data modeling', 'spark', 'mysql', 'hadoop', 'etl', 'big data', 'hbase', 'python', 'oozie', 'data processing', 'airflow', 'elt', 'data engineering', 'nosql', 'mapreduce', 'kafka', 'feasibility analysis', 'hdfs', 'sqoop', 'aws']",2025-06-12 14:28:52
MD365 Finance Functional Consultant,Protiviti India,3 - 8 years,6-16 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nWe are seeking an experienced Microsoft Dynamics 365 Finance Functional Consultant to join our team in Bangalore. The ideal candidate will have hands-on experience in implementing, configuring, and supporting D365 Finance solutions for diverse business requirements. This role involves working closely with clients to understand their financial processes and translate them into effective D365 Finance configurations.\nKey Responsibilities\nImplementation & Configuration",,,,"['Finance', 'D365 Functional', 'Microsoft dynamics']",2025-06-12 14:28:54
Sr. Associate Full Stack Software Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will be responsible for designing, developing, and maintaining software applications and solutions that meet business needs and ensuring the availability and performance of critical systems and applications. This role involves working closely with product managers, designers, data engineers, and other engineers to create high-quality, scalable software solutions and automating operations, monitoring system health, and responding to incidents to minimize downtime.\n\nYou will play a key role in a regulatory submission content automation initiative which will modernize and digitize the regulatory submission process, positioning Amgen as a leader in regulatory innovation. The initiative demonstrates innovative technologies, including Generative AI, Structured Content Management, and integrated data to automate the creation, and management of regulatory content.\n\n\n\nRoles & Responsibilities:\nPossesses strong rapid prototyping skills and can quickly translate concepts into working code\nContribute to both front-end and back-end development using cloud technology\nDevelop innovative solution using generative AI technologies\nEnsure code quality and consistency to standard methodologies\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations\nIdentify and resolve technical challenges effectively\nStay updated with the latest trends and advancements\nWork closely with product team, business team, and other collaborators\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDevelop and implement unit tests, integration tests, and other testing strategies to ensure the quality of the software\nIdentify and resolve software bugs and performance issues\nWork closely with multi-functional teams, including product management, design, and QA, to deliver high-quality software on time\nCustomize modules to meet specific business requirements\nWork on integrating with other systems and platforms to ensure seamless data flow and functionality\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nMasters degree and 1 to 3 years of experience in Computer Science, IT or related field OR\nBachelors degree and 3 to 5 years of experience in Computer Science, IT or related field OR\nDiploma and 7 to 9 years of experience in Computer Science, IT or related field\nPreferred Qualifications:\n\n\n\nFunctional\n\nSkills:\nMust-Have Skills:\nProficiency in Python/PySpark development, Fast API, PostgreSQL, Databricks, DevOps Tools, CI/CD, Data Ingestion.\nCandidates should be able to write clean, efficient, and maintainable code.\nKnowledge of HTML, CSS, and JavaScript, along with popular front-end frameworks like React or Angular, is required to build interactive and responsive web applications\nIn-depth knowledge of data engineering concepts, ETL processes, and data architecture principles. Solid understanding of cloud computing principles, particularly within the AWS ecosystem\nSolid understanding of software development methodologies, including Agile and Scrum\nExperience with version control systems like Git\nHands on experience with various cloud services, understand pros and cons of various cloud service in well architected cloud design principles\nStrong problem solving, analytical skills; Ability to learn quickly; Good communication and interpersonal skills\nExperienced with API integration, serverless, microservices architecture.\nExperience in SQL/NOSQL database, vector database for large language models\n\n\n\nGood-to-Have\n\nSkills:\nSolid understanding of cloud platforms (e.g., AWS, GCP, Azure) and containerization technologies (e.g., Docker, Kubernetes)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience with data processing tools like Hadoop, Spark, or similar\n\n\n\nSoft\n\nSkills:\nExcellent analytical and solving skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'PySpark development', 'Data Ingestion', 'PostgreSQL', 'Fast API', 'CI/CD', 'DevOps Tools', 'Databricks']",2025-06-12 14:28:57
Sr. Databricks Developer,Newscape Consulting,7 - 9 years,Not Disclosed,['Pune( Baner )'],"We are looking for a highly skilled Senior Databricks Developer to join our data engineering team. You will be responsible for building scalable and efficient data pipelines using Databricks, Apache Spark, Delta Lake, and cloud-native services (Azure/AWS/GCP). You will work closely with data architects, data scientists, and business stakeholders to deliver high-performance, production-grade solutions.\nKey Responsibilities :\n- Design, build, and maintain scalable and efficient data pipelines on Databricks using PySpark, Spark SQL, and optionally Scala.\n- Work with Databricks components including Workspace, Jobs, DLT (Delta Live Tables), Repos, and Unity Catalog.\n- Implement and optimize Delta Lake solutions aligned with Lakehouse and Medallion architecture best practices.\n- Collaborate with data architects, engineers, and business teams to understand requirements and deliver production-grade solutions.\n- Integrate CI/CD pipelines using tools such as Azure DevOps, GitHub Actions, or similar for Databricks deployments.\n- Ensure data quality, consistency, governance, and security by using tools like Unity Catalog or Azure Purview.\n- Use orchestration tools such as Apache Airflow, Azure Data Factory, or Databricks Workflows to schedule and monitor pipelines.\n- Apply strong SQL skills and data warehousing concepts in data modeling and transformation logic.\n- Communicate effectively with technical and non-technical stakeholders to translate business requirements into technical solutions.\nRequired Skills and Qualifications :\n- Hands-on experience in data engineering, with specifically in Databricks.\n- Deep expertise in Databricks Workspace, Jobs, DLT, Repos, and Unity Catalog.\n- Strong programming skills in PySpark, Spark SQL; Scala experience is a plus.\n- Proficient in working with one or more cloud platforms : Azure, AWS, or GCP.\n- Experience with Delta Lake, Lakehouse architecture, and medallion architecture patterns.\n- Proficient in building CI/CD pipelines for Databricks using DevOps tools.\n- Familiarity with orchestration and ETL/ELT tools such as Airflow, ADF, or Databricks Workflows.\n- Strong understanding of data governance, metadata management, and lineage tracking.\n- Excellent analytical, communication, and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'ETL', 'Delta Lake', 'Azure Data Lake', 'Apache', 'Data Bricks']",2025-06-12 14:28:59
"Lead FullStack Developer - Java, React.JS",S&P Global Market Intelligence,10 - 15 years,Not Disclosed,['Hyderabad'],"About the Role:\nGrade Level (for internal use):\n11\nThe Team:\nWe are looking for a highly motivated, enthusiastic, and skilled engineering lead for Commodity Insights. We strive to deliver solutions that are sector-specific, data-rich, and hyper-targeted for evolving business needs. Our Software development Leaders are involved in the full product life cycle, from design through release.\nThe resource would be joining a strong innovative team working on the content management platforms which support a large revenue stream for S&P Commodity Insights.Working very closely with the Product owner and Development Manager, teams are responsible for the development of user enhancements and maintaining good technical hygiene.\nThe successful candidate will assist in the design, development, release and support of content platforms. Skills required include ReactJS, Spring Boot, RESTful microservices, AWS services (S3, ECS, Fargate, Lambda, etc.), CSS HTML, AJAX JSON, XML and SQL (PostgreSQL/Oracle), .\nThe candidate should be aware of GEN AI or LLM models like Open AI and Claude etc.\nThe candidate should be enthusiast in working on prompt building related to GenAI and business-related prompts.\nThe candidate should be able to develop and optimize prompts for AI models to improve accuracy and relevance.\nThe candidate must be able to work well with a distributed team, demonstrate an ability to articulate technical solutions for business requirements, have experience with content management/packaging solutions, and embrace a collaborative approach for the implementation of solutions.\nResponsibilities:\nLead and mentor a team through all phases of the software development lifecycle, adhering to agile methodologies (Analyze, design, develop, test, debug, and deploy). Ensure high-quality deliverables and foster a collaborative environment.\nBe proficient with the use of developer tools supporting the CI/CD process including configuring and executing automated pipelines to build and deploy software components\nActively contribute to team planning and ceremonies and commit to team agreement and goals\nEnsure code quality and security by understanding vulnerability patterns, running code scans, and be able to remediate issues.\nMentoringthe junior developers.\nMake sure that code review tasks on all user storiesare added and timely completed.\nPerform reviews and integration testing to assure quality of project development eorts\nDesign database schemas, conceptual data models, UI workows and application architectures that t into the enterprise architecture\nSupport the user base, assisting with tracking down issues and analyzing feedback to identify product improvements\nUnderstand and commit to the culture of S&P Global: the vision, purpose and values of the organization\nBasic Qualifications:\n10+ years experience in an agile team development role, delivering software solutions using Scrum\nJava, J2EE, Javascript, CSS/HTML, AJAX\nReactJS, Spring Boot, Microservices, RESTful services, OAuth\nXML, JSON, data transformation\nSQL and NoSQL Databases (Oracle, PostgreSQL)\nWorking knowledge of Amazon Web Services (Lambda, Fargate, ECS, S3, etc.)\nExperience on GEN AI or LLM models like Open AI and Claude is preferred.\nExperience with agile workflow tools (e.g. VSTS, JIRA)\nExperience with source code management tools (e.g. git), build management tools (e.g. Maven) and continuous integration/delivery processes and tools (e.g. Jenkins, Ansible)\nSelf-starter able to work to achieve objectives with minimum direction\nComfortable working independently as well as in a team\nExcellent verbal and written communication skills\nPreferred Qualifications:\nAnalysis of business information patterns, data analysis and data modeling\nWorking with user experience designers to deliver end-user focused benefits realization\nFamiliar with containerization (Docker, Kubernetes)\nMessaging/queuing solutions (Kafka, etc.)\nFamiliar with application security development/operations best practices (including static/dynamic code analysis tools)\nAbout S&P Global Commodity Insights\nAt S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\nWere a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the worlds foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the worlds leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit .\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\n\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nWe take care of you, so you cantake care of business. We care about our people. Thats why we provide everything youand your careerneed to thrive at S&P Global.\n\nHealth & Wellness: Health care coverage designed for the mind and body.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nGlobal Hiring and Opportunity at S&P Global:\nAt S&P Global, we are committed to fostering a connected andengaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n-----------------------------------------------------------\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\nIf you need an accommodation during the application process due to a disability, please send an email to:and your request will be forwarded to the appropriate person.\n\nUS Candidates Only:The EEO is the Law Poster describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision -\n-----------------------------------------------------------\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.2 - Middle Professional Tier II (EEO Job Group), SWP Priority Ratings - (Strategic Workforce Planning)",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'S3', 'PostgreSQL', 'Spring Boot', 'J2EE', 'Microservices', 'OAuth', 'Fargate', 'ECS', 'ReactJS', 'Javascript', 'CSS/HTML', 'RESTful services', 'Oracle', 'Lambda', 'AJAX']",2025-06-12 14:29:02
Application Developer - Dot Net IFRS 17 // Mumbai // 4-8 Yrs,2coms,4 - 9 years,Not Disclosed,['Mumbai'],"SUMMARY\nAbout the client:\n\nOur client is an IT Technology & Services Management MNC , supporting millions of internal and external customers with state of-the-art IT solutions to everyday problems & dedicated to bringing digital innovations to every aspect of the landscape of insurance. Our Client is a part of one of the major insurance groups based out of Germany and Europe. The Group is represented in around 26 countries worldwide, with Over 37,000 people worldwide, focusing mainly on Europe and Asia & offers a comprehensive range of insurances, pensions, investments and services by focusing on all cutting edge technologies majorly on Could, Digital, Robotics Automation, IoT, Voice Recognition, Big Data science, advanced mobile solutions and much more to accommodate the customers future needs around the globe.\n\n\n\n\n\nRequirements\nCompentences for .NET\nNET Framework (incl. .net core), SQL Server, Entity Framework\nPrioritize business impact and urgency\nAbility to learn new technology and methodology quickly.\nKnowledge User Task API, GUI (Graphical User Interfaces) Design Compentences for NTS (New Tech Stack)\nNTS (New Tech Stack), Container runtime environment with Docker containers and Kubernetes, Cloud with AWS (Amazon Web Services), CI/CD - Continuous Integration / Continuous Deployment with Jenkins, Knowledge Source Management Github and Nexus\nOpenshift, Kerberos Authentication, Competences for Cluster Workflow\nDesign + implementation of process model, Design + implementation of input interfaces in REST format\nDevelopment of flow services below process model\n\nEducational Qualifications:\nBachelorâ€™s or Masterâ€™s  degree in Computer Science /Engineering/Information Technology\nCandidate with non-computer science degree must have minimum 1 year of relevant experience\nMBA in IT / Insurance/Finance   can also apply for Requirements Engineer and Test Engineer role.\n\n\n\nBenefits",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'kubernetes', 'nexus', 'web services', 'openshift', 'ci/cd', 'kerberos', 'ccm', 'business objects administration', 'docker', 'profibus', 'jenkins', 'cd', 'rest', 'e-discovery', 'github', 'entity framework', 'intools', 'sql server', 'application development', 'net application', '.net core', '.net', 'abb dcs', 'aws']",2025-06-12 14:29:04
Project Manager,Nomura,5 - 9 years,Not Disclosed,['Mumbai'],"Job Title: Project Manager\nJob Code: 9922\nCountry: IN\nCity: Mumbai\nSkill Category: India CMT\nDescription:\nRole description\nThe candidate will be a member of the international change team responsible for delivery and support of core transformation initiatives. Strong skills in business requirements & product coverage analysis, data mapping across platforms, support solution design and technical implementation deliverables. The ability to help prototype, visualise, reengineer and automate processes in cross functional teams would be key.\nThe candidate must have strong analytical and problem solving skills and be able to work in a dynamic environment to rapidly produce robust solutions. The candidate will have to liaise with key stakeholders across Finance, MiddleOffice, Risk and Technology, have the ability to work independently to see initiatives through from inception to delivery and have a strong attention to detail.\nKey objectives critical to success\nStrong analytical and communication skills with the initiative to identify and solve problems\nAbility to define requirements, solutions, scope and work across endtoend implementation lifecycle\nAbility to proactively reengineer processes and deliver digital solutions to improve department efficiency\nDemonstrate confidence in engaging and presenting complex solutions to senior individuals\nManage relationships with stakeholders in Finance, Risk and Technology functions across all regions\nSkills, experience, qualifications and knowledge required\nFinancial services experience, ideally in a transformational, middle office, collateral, product control role\nGood understanding of data preparation, modelling and visual data delivery through dashboards\nExperience with implementing data management capability centralisation of data stores, data sourcing, data quality and controls definition and simplification\nAn understanding of profiling current state, define future state, consolidate data insights and present these succinctly to senior stakeholders\nAbility to be proactive and use initiative to improve and reengineer processes and systems and help support legacy solutions e.g. a centralised adjustment mechanism and op model\nStrong communication skills to accurately elicit requirements from stakeholders, present project updates to senior management and provide training to project end users\nAbility to take ownership of end to end project management from inception to delivery.\nProficient in effectively documenting complex business and technical requirements and workflows using a variety of tools and ability to present these to business and technology stakeholders\nExperience of Agile process and tools, e.g. JIRA, Confluence etc\nCoordinate UAT testing for new process implementation with an ability to identify risks involved and communicate with respective teams for resolution\nWillingness to help others across the department in improving their digital skills through training sessions or workshops\nDesirable skills and experience\nUnderstanding of collateral management, P&L reporting, balance sheet substantiation, and trade lifecycle controls\nSome experience of using digital tools would be beneficial that would support key platform data migrations e.g. Python, Alteryx, VBA\nBasic understanding of equities, fixed income and derivatives products\nWe are committed to providing equal opportunities throughout employment including in the recruitment, training and development of employees. We prohibit discrimination in the workplace whether on grounds of gender, marital or domestic partnership status, pregnancy, carer s responsibilities, sexual orientation, gender identity, gender expression, race, color, national or ethnic origins, religious belief, disability or age.\n*Applying for this role does not amount to a job offer or create an obligation on Nomura to provide a job offer. The expression ""Nomura"" refers to Nomura Services India Private Limited together with its affiliates.",Industry Type: Financial Services,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Prototype', 'Data management', 'Project management', 'Fixed income', 'Agile', 'Data quality', 'JIRA', 'Financial services', 'Python', 'Recruitment']",2025-06-12 14:29:07
"Business Systems Analyst, Collaboration Tools",Zendesk,2 - 7 years,Not Disclosed,['Bengaluru'],"Business Analyst, Collaboration Tools\nZendesk is re-envisioning how we use our collaboration tools to provide an outstanding employee experience! We want to be innovators with how employees are served and provide a showcase example that our industry peers can follow. Successful candidates should be experienced with managing Google Workspace (formerly G Suite) as well as other collaboration tools, successful at working with internal partners, and proficient in implementing leading SaaS applications. The successful candidate will collaborate with various internal partners, including but not limited to, Product Development, People & Places, Go To Market, and Internal Communications.\nThis role resides within the Enterprise-Wide Applications team and will have a variety of responsibilities, including assisting with strategic projects from start to finish, implementing governance within our collaboration applications, collaborating with leadership to develop and maintain our long-term roadmap to ensure scalable, secure, and innovative solutions to facilitate Zendesk s growth. Partner with our Global Service Desk to train the team on standard methodologies and create and maintain both internal and employee-facing documentation for supporting Google Workspace and other collaboration applications to ensure best-in-class service for all of Zendesk!\nThis position is in-office, but candidates only have to work 2-3 days onsite per week. We require fluency in written and spoken English.\nWhat youll be doing\nPartner closely with various Zendesk organizations to turn short and long-term business needs into high quality, scalable, secure systems to enable Zendesk s critical initiatives.\nDevelop positive relationships with business partners. Understand their goals, workflows, and processes. Use your deep system knowledge to drive system strategy/vision, design, and implementation to mutually beneficial ends.\nWork in partnership with management and business partners to prioritize and shape our team s roadmap and long-range planning.\nAssist with system improvement projects from design through implementation and support. The individual in this role will be working directly with users to collect requirements, implement, test and deploy new features on a periodic basis.\nIdentify manual processes and problems for both the business and the employees who use Enterprise-Wide Applications and work to determine solutions.\nSupport our Global Service Desk team in technical troubleshooting employee issues.\nWhat you bring to the role\nBasic Qualifications:\n2+ years of experience administering Google Workspace\nProven track record of having implemented, improved and supported enterprise-class SaaS systems. This includes planning, analysis and design, configuration, development, data migrations, system testing, cutover plan, and production support.\nDemonstrable ability to work closely with a diverse and distributed team\nSolid grasp of IT fundamentals, including SDLC, agile methodologies, and change management.\nBachelors degree or equivalent work experience\nPreferred Qualifications:\n2+ years of consulting experience, preferably focused on process optimization, system implementations, and application integrations.\nExperience handling multiple SaaS collaboration applications\nCertified Google Workspace Admin or Developer\nFamiliar with Google Workspace APIs, using GAM, and experienced using SDLC to build and maintain integrations with Integration Platform as a Service (iPaaS) technology.\nZendesk endeavors to make reasonable accommodations for applicants with disabilities and disabled veterans pursuant to applicable federal and state law.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System testing', 'Change management', 'Process optimization', 'Business Analyst', 'Production support', 'Consulting', 'Agile', 'Customer service', 'Troubleshooting', 'SDLC']",2025-06-12 14:29:41
Lead Support Analyst,Nomura,10 - 11 years,Not Disclosed,['Mumbai'],"Job Title: Lead Support Analyst\nJob Code: 10182\nCountry: IN\nCity: Mumbai\nSkill Category: IT\\Technology\nDescription:\nRole & Responsibilities:\nInstinet Cloud Operations team manages daily operations for Instinet s cloud portfolio primarily based on the Amazon Web Services (AWS) platform. The team collaborates with Applications and Infrastructure teams to operate the cloud environment.\nThis is a handson technical role . The selected person would be part of Mumbai Instinet Cloud Operations and would report to Instinet IT Cloud Ops Head. It s a very high ownership as the selected person would be part of a team that s responsible for a significant part of overall global Cloud Ops coverage. In addition, the selected person would independently work with global stakeholders and various Application groups towards high quality delivery out of Mumbai Cloud Ops team.\nDuties & responsibilities (includes but is not limited to):\nManage incoming service requests via JIRA Service Desk. Perform triage, scheduling and execution of incoming requests. Coordinate with internal teams to successfully resolve requests.\nRealtime monitoring and alerting, proactive notifications, problem isolation and remediation.\nCloud environment change management, coordination and provisioning/execution\nAutomation for BAU activities such as account creation, landing zone management, application inventory, trusted advisor recommendations, cost oversight, operational guard rails etc.\nSoftware Lifecycle Maintenance (development and bug fixes) of established automation frameworks such as Backup management, cost management, landing zone etc.\nMonthly reporting of KPIs; apply continuous improvement methodology to support required KPIs.\nWilling to own deliveries and work handson across all aspects of Cloud Ops delivery out of Mumbai\nWilling to learn new technologies/tool as required, in order to effectively deliver output.\nShifts:\nMon to Friday 7.30 am IST to 4.30 pm IST\nMon to Friday 4 pm IST to 01:00 am IST\nKey Skills:\nMandatory\nDesirable\nDomain\nAWS Certified SysOps Administrator certification or a higher AWS Professional certification.\nPrior experience of working in IBs is a plus but not mandatory\nTechnical\nExperience utilizing service desk tools such as JIRA, Service Now etc.\nStrong Operations and/or NOC (Network Operations Centre) experience including monitoring, alert response, problem isolation and remediation.\nStrong experience with operating applications within Amazon Web Services (AWS). Lambda, Config, IAM ,Ec2,RDS , Landing zone Others\nAbility to utilize and perform administration via a command line interface in Linux or Unix.\nKnowledge and experience utilizing Git via command line\nStrong Unix/Linux skills well versed in tasks like file editing, system resource monitoring, running and scheduling processes, and troubleshooting system issues.\nScripting shell scripting or Python scripting\nKnowledge and experience with creating, maintaining and updating YAML and JSON configuration files.\nPrior release management experience\nStrong verbal and written communication skills (as this is a senior global stakeholder facing role)\nQuick learner\nHighly proactive and takes initiative to identify problem areas to evolve solutions.\nClient focused and attentive to businesscritical issues\nWe are committed to providing equal opportunities throughout employment including in the recruitment, training and development of employees. We prohibit discrimination in the workplace whether on grounds of gender, marital or domestic partnership status, pregnancy, carer s responsibilities, sexual orientation, gender identity, gender expression, race, color, national or ethnic origins, religious belief, disability or age.\n*Applying for this role does not amount to a job offer or create an obligation on Nomura to provide a job offer. The expression ""Nomura"" refers to Nomura Services India Private Limited together with its affiliates.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Change management', 'Automation', 'Linux', 'Scheduling', 'Troubleshooting', 'Continuous improvement', 'Operations', 'Monitoring', 'Recruitment']",2025-06-12 14:29:44
Senior Cloud Engineer,Infrassist Technologies Pvt. Ltd.,2 - 7 years,1-6 Lacs P.A.,['Ahmedabad'],"Number of open positions 2\n\nTitle: Cloud Engineer or Sr. Cloud Engineer\n\nExperience: 3-5 years relevant work experience\n\nQualification, Experience & Skills Required\n\nGood to have Professional Certification in Domain (1. Microsoft 365 Certified: Administrator Expert / Endpoint Administrator 2. Azure Administrator)\nExcellent oral & written communication skills Ready to work in morning & afternoon shifts along with occasional night shifts (if required).\nThe purpose of the role is to be the main customer facing point of contact for all Microsoft Office 365 & Azure Cloud projects implementation & consulting work.\nAbility to work intuitively, along or as part of a team environment, Self-starter with ability to plan and execute work.\nProven hands-on experience in Implementing, Configuring, Testing, Migrating & Securing Microsoft 365 & Azure Cloud Solutions.\nMicrosoft Windows Active Directory, Azure AD Connect Azure AD Premium Features deployment mainly User & Device based Conditional Access, Multi-Factor Authentication Policies.\nEmail & data Migrations from different source environments to Office 365 (Exchange & SharePoint Online, Microsoft Teams & One Drive for Business respectively) using Microsoft methods, tools and/or 3rd party migration solutions bit titan, SkyKick, AvePoint, Sharegate etc.\nMicrosoft Mobile Device Management (Intune MDM & MAM) Solutions, Windows Autopilot Technologies",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Azure Certified', 'Cloud Administration', 'microsoft 365', 'Endpoint', 'Azure Virtual Desktop', 'Entra Id', 'Microsoft Certified', 'Sharegate', 'Networking', 'Migration', 'avepoint', 'Intune', 'Autopilot', 'microsoft mobile device management', 'Security', 'Active Directory', 'MDM', 'Mam', 'skykick', 'Bittitan']",2025-06-12 14:29:47
Senior Devops Engineer,Epam Systems,5 - 8 years,18-22.5 Lacs P.A.,"['Chennai', 'Coimbatore']","Proven experience as a DevOps Engineer, with a focus on designing and implementing cloud-based infrastructure.\nStrong expertise in cloud platforms such as Amazon Web Services (AWS)\nExperience with infrastructure-as-code tools such as Terraform.\nProficiency in scripting languages (e.g., Python/Shell) for automation and infrastructure management.\nDeep understanding of CI/CD concepts and experience with CI/CD tools such as Jenkins, GitLab CI/CD\nFamiliarity with containerization and orchestration technologies like Docker and Kubernetes.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Jenkins', 'Gitlab', 'Terraform', 'Docker', 'Bash Scripting', 'Shell Scripting', 'Ansible', 'Groovy', 'Kubernetes', 'Python']",2025-06-12 14:29:49
Oracle Database Developer,Mobile Programming,6 - 10 years,Not Disclosed,['Bengaluru'],"We are hiring an experienced Oracle Database Developer for a prestigious client, Siemens. The ideal candidate should have deep expertise in Oracle databases with a solid foundation in SQL and PL/SQL development.\nThis role demands experience in database migrations, trigger management, and a basic understanding of PostgreSQL.\nKey Responsibilities:Design, develop, and maintain Oracle database solutions using SQL and PL/SQLHandle database migrations, develop and manage triggers, stored procedures, and packages Optimize and troubleshoot performance issues Collaborate with development and QA teams to ensure seamless data integrationWork with PostgreSQL for basic database interactions and data migration tasks Technical Skills\n(Mandatory):Oracle DB (11g, 12c, 19c)SQL, PL/SQLDatabase Migrations Triggers\nAdditional:Basic knowledge of PostgreSQL",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle Database', 'QA', 'PostgreSQL', 'PL/SQL Database Migrations', 'SQL']",2025-06-12 14:29:51
Technical Lead - PLM,Tata Technologies,8 - 13 years,Not Disclosed,['Pune'],"TC Migration Lead:Minimum 8 + years of experience in developing Teamcenter\nHands-on experience in Teamcenter implementations in various phases of project like business blueprinting, configuration, cut-over and post go-live support\nStrong expertise of Teamce nter product architecture and its integration frameworks like T4EA/T4S/T4O/CAD Integrations etc. Technical capabilities in Part Management, BOM Management, Change Management, Classification, Workflow, Organization\nProficiency in Teamcenter ITK, RAC, BMIDE, and SOA customization\nExperience with Teamcenter workflows, access control, and BMIDE configurations\nExperience with SQL/Oracle and d atabase management systems\nExperience in C/C++ programming language and TC server side (ITK & SOA), BMIDE, Java, HTML, XML and Web services and Active Workspace Configuration and declarative programming\nExperience with data migration, mapping, transformation, data validation to Teamcenter from legacy applications\nShould have exposure to CAD, NX and Teamcenter",Industry Type: Building Material (Cement),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Teamcenter', 'c++', 'caa', 'oracle', 'soa', 'web services', 'product life cycle management', 'engineering change management', 'bmide', 'cad', 'data migration', 'change management', 'catia', 'sql', 'plm', 'java', 'rac', 'xml', 'pdm', 'teamcenter itk', 'html', 'sap plm', 'teamcenter unified']",2025-06-12 14:29:53
MDM Developer,NetApp,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nParticipates in reviewing, analyzing, and modifying client/server applications and systems.\nJob Requirements\nDevelop and maintain integrations between CDM and other systems, such as CRM, order management, and other boundary systems.\nCustomize and extend MDM functionality using Oracle tools, such as Oracle Integration Cloud, Oracle Application Composer, Oracle Visual Builder.",,,,"['data management', 'web services', 'unit testing', 'groovy scripting', 'data migration', 'tools', 'oracle fusion', 'master data management', 'sql', 'plsql', 'cloud', 'operations', 'java', 'spark', 'oracle erp', 'visual', 'etl', 'pubsub', 'rest', 'python', 'oracle', 'software testing', 'application', 'mdm', 'integration', 'oracle sql', 'data integration']",2025-06-12 14:29:56
Marketing Analytics - TL / Assistant Manager,Leading Client,3 - 8 years,Not Disclosed,['Bengaluru'],Candidate Expectations\n3--12 years of work experience\nFlexible to work in shift as per client requirement\nCertification(s) Preferred:\nAdobe Experience Platform\nAdobe Real-Time Customer Data Platform\nAdobe Customer Journey Analytics\nJob Responsibilities\nUtilizing previous experience in CDPs (Customer data platforms) such as Adobe Experience Platform (RTCDP) or similar products;\nHaving an enhanced understanding of customer profile segmentation and experience in 360 degree view of customers in CDP for further analytical processing and decision making;\nShowcasing proven track record of managing successful CDP implementation/management and delivering capabilities that drive business growth;\nDemonstrating work experience in data architecture and data modeling; preferably with experience working in CDP CRM paid media social and offline data;\nEnsuring an enhanced understanding of data-driven marketing analytics and relevance / usage of real-time event data and associated attributes;\nSetting strategic direction and driving execution through collaboration with a cross-functional team;\nDemonstrating familiarity with CRM and Marketing Automation platforms (i.e. Salesforce Sales Cloud Salesforce Marketing Cloud etc.);\nHaving extensive hands on expertise in implementing/administering Adobe Experience Cloud Products and marketing automation platforms and technologies;\nEnsuring an enhanced understanding of identity resolution components and the enabling technology i.e. profile merge rules identity graph identity service provider etc.;\nWorking with audience-based digital marketing strategy either at an agency or brand;\nDriving project management through a full lifecycle including the ability to prioritize sequence execute and deliver projects on time and on budget;\nTranslating business requirements and objectives into segmentation strategies and audience building logic;\nOwning Adobe AEP and driving proactive platform ownership that is directly aligned to the day-to-day delivery of marketing tactics and closely connected to the other digital marketing teams to develop capabilities and manage technology roadmap;\nHaving oversight of the CDP technology solution (e.g. oversee steady state support teams interact with business owner plan for break / fix and other enhancements / maintenance);\nDriving how customer and prospect information is unified across sales marketing and service channels;\nCollaborating with Insights team to refine and optimize measurement process;\nImplementing and refining Adobe Products and marketing automation system;\nCollaborating with Marketing leadership to evolve the use of data within the marketing function allowing us to stay ahead responding to clients in real-time and limiting redundancy.\nContact Person : - Subhikshaa\nContact Number : - 9840114687,Industry Type: BPM / BPO,Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['Adobe Experience Platform', 'Adobe Customer Journey Analytics', 'Salesforce Sales Cloud', 'Sales Accreditation', 'Paid Media', 'Adobe Real-Time Customer Data Platform', 'Salesforce Marketing Cloud', 'Adobe Experience Cloud', 'Marketing Automation platforms', 'CRM']",2025-06-12 14:29:58
Team center developer,tekskill,4 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Responsibilities\nDevelop and customize solutions in Siemens Teamcenter based on business requirements.\nImplement BMIDE configurations including data model changes, business objects, and properties.\nDesign and develop ITK and SOA-based server-side customizations.\nWork on Active Workspace Client (AWC) customizations including stylesheets, XRTs, and client extensions.\nTroubleshoot and resolve issues related to Teamcenter performance, customization, and user support.\nParticipate in data migration activities using tools like Import/Export (IPEC), BMIDE XML, or custom scripts.\nSupport Teamcenter integrations with CAD tools (NX, CATIA, etc.) and other enterprise applications (SAP, CRM).\nDevelop and maintain workflows, handlers, and process templates.\nCollaborate with functional teams to understand requirements and deliver scalable PLM solutions.\nProvide technical documentation, code reviews, and mentoring for junior team members if needed.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ITK', 'team center', 'SOA', 'Bmide']",2025-06-12 14:30:00
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Bengaluru'],"Responsibilities :\n\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:30:02
ZuroCPQ Developer,Mobile Programming,5 - 10 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:Implement and configure ZuroCPQ to meet business requirements.Customize and extend the functionality of the ZuroCPQ application based on business needs.\nCollaborate with cross-functional teams to understand the requirements and ensure that the CPQ solutions align with sales processes.\nDevelop workflows and pricing rules for complex quotes and configurations.Integrate ZuroCPQ with other enterprise applications, including CRM and ERP systems.Provide technical support and troubleshooting for ZuroCPQ users.\nCreate and maintain technical documentation for the solutions developed.Participate in the design and review process for system enhancements and new features.Monitor and ensure system performance and data integrity.\nRequired Skills and Qualifications:\n5 to 10 years of experience in implementing, configuring, and customizing CPQ solutions.Hands-on experience with ZuroCPQ is essential.\nProficient in pricing and quoting tools, and experience integrating CPQ solutions with CRM (Salesforce) and ERP systems.\nStrong understanding of Salesforce and Salesforce CPQ (if applicable).Ability to work in a collaborative environment with sales, product, and engineering teams.Strong analytical skills and the ability to solve complex business challenges.\nFamiliarity with API integration and data migration.\nUnderstanding of workflow automation and business process optimization.\nAbility to write clean and maintainable code and deliver effective solutions.Strong problem-solving skills and attention to detail.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ZuroCPQ', 'Salesforce CPQ', 'ERP systems', 'CPQ', 'data migration', 'API integration', 'CRM', 'Salesforce']",2025-06-12 14:30:05
Informatica/ETL PowerCenter Developer,Saama Technologies,7 - 12 years,Not Disclosed,"['Pune', 'Chennai', 'Coimbatore']","Role Description: The Informatica/ETL PowerCenter Developer would need to have at least 7+ years of experience.\n\nResponsibilities and Qualifications:  \nParticipates in ETL Design of new or changing mappings and workflows with the team and prepares technical specifications.\nCreates ETL Mappings, Mapplets, Workflows, Worklets using Informatica PowerCenter 10.x and prepare corresponding documentation.",,,,"['Etl Informatica', 'PLSQL', 'Informatica Powercenter', 'SQL']",2025-06-12 14:30:07
Informatica MDM-Manager,Mclaren Strategic Ventures India,9 - 14 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Bengaluru']","Role & responsibilities\nD for IDMC Manager: 8+ years of experience in MDM development, with at least 2 years on the Informatica IDMC platform. Key Responsibilities: â€¢ Lead the development and implementation of MDM solutions on the Informatica IDMC platform. â€¢ Design and configure Business 360 (B360), Customer 360 (C360), Product 360 (P360), and Reference 360 (R360) solutions. â€¢ Implement match/merge logic, survivorship rules, business validations, and workflow orchestration using Cloud Application Integration (CAI). â€¢ Configure Data Quality (DQ) services and perform data profiling to support cleansing and validation processes. â€¢ Integrate MDM with enterprise systems such as ERP, CRM, and data warehouses using IDMCs standard connectors, APIs, and real-time integration patterns. â€¢ Collaborate with data architects, business stakeholders, and project teams to gather requirements and translate them into scalable MDM solutions. â€¢ Utilize data modeling best practices to design and maintain golden records for domains like Customer, Product. â€¢ Work with real-time REST APIs exposed by Informatica for operations such as search, create, update, and delete. â€¢ Provide leadership in solution design reviews, troubleshooting, performance tuning, and production support. JD for IDMC Developer: 4+ years of experience in MDM development, with at least 1 year on the Informatica IDMC platform. Key Responsibilities: â€¢ Develop and implement Master Data Management (MDM) solutions on the Informatica IDMC platform. â€¢ Configure and maintain IDMC components such as Customer 360 (C360), Product 360 (P360), Business 360 (B360), and Reference 360 (R360). â€¢ Design and implement match rules, business rules, survivorship rules, and data validations. â€¢ Build and orchestrate workflows using Cloud Application Integration (CAI). â€¢ Perform data profiling and implement data quality (DQ) rules and transformations. â€¢ Develop data pipelines and workflows using IDMC Data Integration service. â€¢ Integrate IDMC MDM with external systems like ERP, CRM, and data lakes using connectors and APIs. â€¢ Use real-time REST APIs provided by IDMC for operations such as search, create, update, and delete. â€¢ Collaborate with data architects, analysts, and business users to gather requirements and deliver scalable solutions.""\nAdditional Information: SPOC - Rajalaxmi (rajalaxmi.k.kar@pwc.com)\nMandatory Skills IDMC MDM\nNice to have skills CDQ, IDQ\nInterview Mode Virtual Interview\nWork Model Remote\nCleanroom: No\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Informatica Mdm', 'Informatica Master Data Management', 'Master Data Management']",2025-06-12 14:30:09
Backend Developer - AWS / Java,Sadup Soft,7 - 11 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Designation : AWS Java back end developer\n\nExperience : 7-11yrs\n\nLocation : Remote, Delhi NCR, Bangalore, Chennai, Pune, Kolkata, Ahmedabad, Mumbai, Hyderabad\n\nMust have skills :\n\n- 8-10 years of experience\n\n- 6+ years of experience in JAVA\n\n- Exposure to AWS\n\n- Willingness to work in Kotlin\n\n- Preference is to have the person from product-based company.\n\nRoles &responsibilities :\n\nAs an senior software engineer in our team, you will contribute not only to all aspects of backend, but also get involved in all aspects of software engineering: infra, data, application, deployment, monitoring, architecture, code reviews, pair and mob programming, etc.\n\nYou will also be leading a few initiatives with your design and architectural know-how and working incollaboration with other Architects and Managers.\n\nHere, we own everything we do: we own our successes as well as our mistakes and setbacks. In this spirit, we are looking for someone reliable, curious, and >\nYour day-to-day :\n\n- You will focus on delivering high quality solutions using your Java and Kotlin skills\n\n- You will work as part of an agile team, planning, refining, delivering and inspecting sprint deliveries\n\n- You work closely with product and design to understand context and vision\n\n- You will work with your peers across teams to make platform improvements and increase learning opportunities for all in a supportive environment\n\nWhat do you need to bring :\n\nToday we work with technologies like :\n\n- Java and Kotlin\n\n- Drop wizard, Spring Boot\n\n- Docker, Terraform\n\n- PostgreSQL\n\n- AWS\n\n- Everything is hosted on Amazon Web Services - we use managed cloud services as much as we can.\n\nWe believe that you have :\n\n- 6 to 8 years of experience in software development\n- 5+ years of experience building distributed systems\n\n- 5 + years of experience with SQL and data modelling\n\n- Experience working with continuously delivered systems\n\n- A result and delivery-oriented mindset\n\n- A genuine learning and knowledge sharing spirit\n\n- Unix/Linux/Docker experience\n\n- Interest in or experience of FinTech industry",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Unix', 'Linux', 'PostgreSQL', 'Spring Boot', 'AWS', 'Backend Architecture', 'Kotlin']",2025-06-12 14:30:11
SAP MDM Specialist,Sadup Soft,7 - 10 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Job Title : SAP MDM Consultant.\n\nLocation : Remote,Delhi NCR, Bangalore, Chennai, Pune, Kolkata, Ahmedabad, Mumbai, Hyderabad\n\nExperience : 7-10 years.\n\nKey Responsibilities :\n\n- Own and manage all Master Data Management (MDM) activities for SAP projects, ensuring alignment with business objectives.\n\n- Develop and implement comprehensive MDM strategies and roadmaps.\n\n- Lead the design and implementation of data governance frameworks and processes.\n\n- Lead data migration and cutover activities in SAP S/4HANA projects, including Greenfield implementations, system migrations, and rollouts.\n\n- Develop and execute data migration plans, ensuring data accuracy and consistency.\n\n- Manage data cleansing, transformation, and validation processes.\n\n- Establish and implement MDM best practices and data management capabilities.\n\n- Define and enforce data management principles, policies, and lifecycle strategies.\n\n- Ensure data compliance with regulatory requirements and internal policies.\n\n- Work closely with MDM Leads and stakeholders to drive data governance initiatives.\n\n- Develop and implement data quality metrics and reporting mechanisms.\n\n- Monitor data quality and identify areas for improvement.\n\n- Implement data quality controls and validation rules.\n\n- Track and manage MDM objects, ensuring timely delivery and adherence to project timelines.\n\n- Participate in daily stand-ups, issue tracking, and dashboard updates.\n\n- Collaborate with cross-functional teams, including functional consultants, developers, and business stakeholders.\n\n- Identify risks and process improvements for MDM.\n\n- Conduct training sessions for teams on S/4HANA MDM best practices and processes.\n\n- Develop and maintain training materials and documentation.\n\nRequired Skills & Qualifications :\n\n- 7-10 years of experience in SAP Master Data Management (MDM).\n\n- Strong knowledge of SAP S/4HANA, Data Migration, and Rollouts.\n\n- Expertise in data governance, lifecycle management, and compliance.\n\n- Experience in defining data management principles, policies, and lifecycle strategies.\n\n- Ability to monitor data quality with consistent metrics and reporting.\n\n- Familiarity with KANBAN boards, ticketing tools, and dashboards.\n\n- Strong problem-solving and communication skills.\n\n- Ability to track and manage MDM objects, ensuring timely delivery.\n\nPreferred Skills :\n\n- Experience in training teams on MDM best practices.\n\nAutomation & Productivity Tools :\n\n- Knowledge of automation and productivity improvement tools.\n\n- Familiarity with ABAP and SQL.\n\n- Experience with SAP Data Services or other data migration tools.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP MDM', 'SAP ERP', 'S/4 HANA', 'SAP Implementation', 'SAP projects']",2025-06-12 14:30:13
SAP DMS Specialist,Sadup Soft,5 - 10 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Locations : Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote\n\nAbout the Role :\nWe are seeking a highly skilled SAP DMS Specialist to manage, configure, and optimize the Document Management System (DMS) within the SAP environment. The successful candidate will be responsible for overseeing document lifecycle management, ensuring seamless integration of DMS with other SAP modules, and maintaining security and compliance standards. This role requires collaboration with cross-functional teams to implement solutions that enhance document management processes, improve data accessibility, and support business operations.\n\nResponsibilities :\n\n- Provide hands-on expertise in SAP DMS configuration and management, with 5-10 years of relevant experience.\n\n- Manage the complete document lifecycle within SAP DMS, including creation, storage, retrieval, version control, and archiving.\n\n- Configure and maintain metadata within SAP DMS to ensure accurate document categorization and efficient searching.\n\n- Implement and manage version control strategies within SAP DMS to track document changes and maintain audit trails.\n\n- Possess strong experience with SAP ECC and SAP S/4HANA, including a deep understanding of how DMS integrates with other SAP modules such as Materials Management (MM), Plant Maintenance (PM), and Quality Management (QM).\n\n- Design, implement, and manage SAP DMS workflows to automate document routing, approval processes, and other document-related tasks.\n\n- Define and implement document categorization strategies within SAP DMS to organize and classify documents effectively.\n\n- Configure and manage various storage solutions integrated with SAP DMS, ensuring optimal performance and accessibility.\n\n- Implement and maintain SAP DMS security protocols, including user roles, authorizations, and access controls, to protect sensitive information.\n\n- Apply knowledge of compliance standards and document retention policies within SAP systems to ensure adherence to regulatory requirements.\n\n- Collaborate with business users and IT teams to gather requirements and translate them into effective SAP DMS solutions.\n\n- Develop and maintain comprehensive documentation for SAP DMS configurations, processes, and user guides.\n\n- Provide end-user training and support for SAP DMS functionalities.\n\n- Troubleshoot and resolve issues related to SAP DMS functionality and integrations.\n\n- Participate in upgrades and enhancements of the SAP DMS system.\n\n- Stay up-to-date with the latest SAP DMS features and best practices.\n\nQualifications : Bachelor's degree in Computer Science, Information Technology, or a related field.\n\nRequired Skills :\n\n- 5-10 years of hands-on experience in SAP DMS configuration and management.\n\n- Deep expertise in document lifecycle management, metadata configuration, and version control in SAP DMS.\n\n- Proven strong experience with SAP ECC and SAP S/4HANA.\n\n- Extensive experience with the integration of SAP DMS with other SAP modules (e.g., MM, PM, QM).\n\n- Proficient in designing, implementing, and managing SAP DMS workflows.\n\n- Strong understanding of document categorization principles and experience in implementing them within SAP DMS.\n\n- Experience in configuring and managing various storage solutions integrated with SAP DMS.\n\n- Comprehensive understanding and experience with SAP DMS security protocols and access controls.\n\n- Solid knowledge of compliance standards and document retention policies within SAP systems.\n\n- Excellent analytical and problem-solving skills.\n\n- Strong communication and interpersonal skills, with the ability to collaborate effectively with cross functional teams.\n\n- Ability to work independently and manage tasks effectively in a remote environment.\n\nPreferred Skills :\n\n- Experience with SAP Engineering Control Center (ECTR).\n\n- Knowledge of SAP Content Server.\n\n- Experience with OpenText Extended ECM for SAP Solutions.\n\n- Familiarity with data migration tools and techniques related to SAP DMS.\n\n- Experience in developing custom solutions or enhancements within SAP DMS.\n\n- SAP DMS certification.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP DMS', 'SAP ECC', 'SAP S/4 HANA', 'SAP PM', 'SAP QM', 'Document Management System', 'SAP MM', 'SAP Integration', 'SAP System Configuration', 'SAP Implementation', 'SAP Support']",2025-06-12 14:30:16
SAP ABAP Developer,Mobile Programming,6 - 9 years,Not Disclosed,['Bengaluru'],"We are looking for an experienced SAP ABAP Developer to join our team in Bengaluru. As an SAP ABAP Developer, you will be responsible for designing, developing, and maintaining custom SAP applications and solutions using the ABAP programming language. You will work on enhancements, custom reports, interfaces, and data migration tasks.\nYour role will involve collaborating with functional teams to ensure the seamless integration of SAP modules and a smooth business workflow.\nKey Responsibilities:\nDevelop and maintain custom ABAP programs and SAP modules to support business needs.\nDesign and implement SAP enhancements, reports, and interfaces.Work on data migration and system integration between SAP and other enterprise systems.Collaborate with functional teams to understand business requirements and translate them into technical solutions.\nPerform unit testing, integration testing, and ensure the quality of deliverables.\nProvide support for SAP troubleshooting and issue resolution.Ensure that all developed solutions comply with SAP standards and best practices.Maintain up-to-date documentation for all technical solutions and processes.Participate in code reviews and contribute to improving the development process.\nRequired Skills and Qualifications:6-9 years of hands-on experience as an SAP ABAP Developer.\nProficiency in ABAP programming language with a deep understanding of SAP architecture.\nExperience with SAP modules (e.g., MM, SD, FICO, etc.) and their integration.\nExpertise in ABAP Workbench, Smart Forms, ALV Reports, and BAPI development.\nExperience in Data Migration using LSMW, IDOC, and BAPI.\nHands-on experience with SAP NetWeaver, SAP Fiori, and HANA integration.\nStrong problem-solving and debugging skills.\nKnowledge of performance optimization and security best practices for ABAP programs.Ability to work with functional teams to define requirements and design solutions.\nStrong communication skills to collaborate with business stakeholders and technical teams.\nTechnical Skills:\nSAP ABAP | ABAP Workbench | Smart Forms | ALV Reports | BAPI | LSMW | IDOC | SAP NetWeaver | SAP Fiori | SAP HANA | SAP Modules (MM, SD, FICO) | Data Migration | SAP OData | ABAP Objects | UI5 | Object-Oriented Programming | SAP PI/PO | Performance Optimization | Debugging | SAP Workflow | Integration",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'SAP Fiori', 'ABAP Workbench', 'integration testing', 'SAP HANA', 'SAP ABAP', 'unit testing', 'BAPI development']",2025-06-12 14:30:18
SAP PP Consultant - S/4 HANA Module,Zettamine Labs,6 - 11 years,Not Disclosed,"['Mumbai', 'Pune', 'Bengaluru']","Location : Pune, Mumbai, Bangalore\n\nNotice Period : Immediate\n\nJob Overview :\n\nWe are looking for an experienced SAP PP (Production Planning) Consultant to join our team. The ideal candidate will have strong expertise in SAP PP module, including configuration, implementation, and integration with other SAP modules.\n\nKey Responsibilities :\n\n- Implement and configure SAP PP to optimize production planning and control.\n\n- Collaborate with stakeholders to gather business requirements and translate them into system solutions.\n\n- Perform end-to-end process mapping for demand planning, capacity planning, and shop floor control.\n\n- Ensure seamless integration with SAP MM, SD, and QM modules.\n\n- Troubleshoot and resolve technical issues in SAP PP environments.\n\n- Provide end-user training and documentation support.\n\n- Work closely with SAP ERP teams to enable master data integration and production planning\nrequirements.\n\n- Support SAP S/4HANA migration and implementation projects.\n\n- Optimize Bill of Materials (BOM), Routing, and Work Centers for efficient production\nprocesses.\n\n- Lead data migration from legacy systems to SAP PP.\n\nRequired Skills and Experience :\n\n- 6-12 years of experience in SAP PP implementation.\n\n- Strong knowledge of production planning, demand management, and shop floor control.\n\n- Hands-on experience in SAP PP configuration, development (ABAP), and integration.\n\n- Expertise in SAP S/4HANA integration for production planning.\n\n- Experience in capacity planning, MRP, and scheduling.\n\n- Strong understanding of warehouse business processes related to production planning.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['S/4 HANA Module', 'SAP QM', 'S/4 HANA', 'SAP SD', 'SAP PP', 'SAP ABAP', 'SAP MM', 'SAP Integration', 'SAP Implementation', 'SAP WM']",2025-06-12 14:30:21
"Engineer, ETL",XL India Business Services Pvt. Ltd,2 - 5 years,Not Disclosed,['Gurugram'],"Engineer, ETL Gurgaon/Bangalore, India AXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained industrious advantage\n\nOur Chief Data Office also known as our Innovation, Data Intelligence & Analytics team (IDA) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking an Data Engineer\n\nThe role will support the team s efforts towards creating, enhancing, and stabilizing the Enterprise data lake through the development of the data pipelines\n\nThis role requires a person who is a team player and can work well with team members from other disciplines to deliver data in an efficient and strategic manner\n\nWhat you ll be DOING What will your essential responsibilities include? Act as a data engineering expert and partner to Global Technology and data consumers in controlling complexity and cost of the data platform, whilst enabling performance, governance, and maintainability of the estate\n\nUnderstand current and future data consumption patterns, architecture (granular level), partner with Architects to ensure optimal design of data layers\n\nApply best practices in Data architecture\n\nFor example, balance between materialization and virtualization, optimal level of de-normalization, caching and partitioning strategies, choice of storage and querying technology, performance tuning\n\nLeading and hands-on execution of research into new technologies\n\nFormulating frameworks for assessment of new technology vs business benefit, implications for data consumers\n\nAct as a best practice expert, blueprint creator of ways of working such as testing, logging, CI/CD, observability, release, enabling rapid growth in data inventory and utilization of Data Science Platform\n\nDesign prototypes and work in a fast-paced iterative solution delivery model\n\nDesign, Develop and maintain ETL pipelines using Pyspark in Azure Databricks using delta tables\n\nUse Harness for deployment pipeline\n\nMonitor Performance of ETL Jobs, resolve any issue that arose and improve the performance metrics as needed\n\nDiagnose system performance issue related to data processing and implement solution to address them\n\nCollaborate with other teams to ensure successful integration of data pipelines into larger system architecture requirement\n\nMaintain integrity and quality across all pipelines and environments\n\nUnderstand and follow secure coding practice to make sure code is not vulnerable\n\nYou will report to Technical Lead\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Effective Communication skills\n\nBachelor s degree in computer science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience\n\nRelevant years of extensive work experience in various data engineering & modeling techniques (relational, data warehouse, semi-structured, etc), application development, advanced data querying skills\n\nRelevant years of programming experience using Databricks\n\nRelevant years of experience using Microsoft Azure suite of products (ADF, synapse and ADLS)\n\nSolid knowledge on network and firewall concepts\n\nSolid experience writing, optimizing and analyzing SQL\n\nRelevant years of experience with Python\n\nAbility to break complex data requirements and architect solutions into achievable targets\n\nRobust familiarity with Software Development Life Cycle (SDLC) processes and workflow, especially Agile\n\nExperience using Harness\n\nTechnical lead responsible for both individual and team deliveries\n\nDesired Skills and Abilities: Worked in big data migration projects\n\nWorked on performance tuning both at database and big data platforms\n\nAbility to interpret complex data requirements and architect solutions\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExcellent basics on parquet files and delta files\n\nEffective Knowledge of Azure cloud computing platform\n\nFamiliarity with Reporting software - Power BI is a plus\n\nFamiliarity with DBT is a plus\n\nPassion for data and experience working within a data-driven organization\n\nYou care about what you do, and what we do",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'System architecture', 'Coding', 'Agile', 'Workflow', 'Application development', 'SDLC', 'SQL', 'Python', 'Firewall']",2025-06-12 14:30:23
Technial Lead - backend,Cheq Digital,7 - 9 years,40-55 Lacs P.A.,['Bengaluru'],"About CheQ\nHey Future Teammate! Ready to dive into the fintech revolution with us at CheQ? We're not your typical 9-to-5 crew; we're the dynamic force turning credit management into a fun and rewarding journey.\nImagine instant repayments that are not just manageable but downright enjoyable. Founded by ex-Flipkart executive, Aditya Soni, CheQ has processed over $2 Billion in credit repayments in a short span. Yes, that's buying all IPL teams together! We've built a user base of over a million and raised a whopping $16 M, backed by 3one4 Capital, Venture Highway and marquee angels like Ram Shriram & Dr. LloydMarquee fintech angels like Naveen Kukreja of PaisaBazar and Shailaz Nag of Dotpe trust our venture. We're on a mission to make credit easy, enjoyable, and filled with awesome rewards. Picture being part of a team that turns the credit maze into an adventure. If you're ready to make credit management a journey worth taking, where work feels like play, hit us up. Let's transform the game together! #JoinCheQ #FintechRevolution\n\nEngineering @CheQ\nEngineers at CheQ are true builders. They bring in a myriad of ideas, and create the infrastructure, systems and products that drive our services. Our engineers build Zero to One products, microservices, DBs and everything else (from farm to table).\n\nWith their magic they take a product, feature or experience from ideation to production, always keeping the customers and their interests in mind. This includes understanding consumer & business needs, critiquing & challenging product and design counterparts and owning the execution & delivery of the scope.\n\nWhat youll be doing\nWe are much more than our job descriptions,but here is where you will begin:\nCollaborate with stakeholders, including product owners, project managers, and scrum masters, to define and clarify project requirements.\nTranslate business requirements into technical specifications and ensure all stakeholders have a clear understanding of the project scope and objectives.\nFacilitate effective communication and coordination among cross-functional teams to ensure alignment and successful project delivery.\nDesign, develop, and maintain scalable and efficient software solutions that meet business needs.\nWrite clean, maintainable, and well-documented code while adhering to best practices and coding standards.\nPerform code reviews and provide constructive feedback to team members to ensure code quality and consistency.\nWork closely with the DevOps team to establish and maintain CI/CD pipelines for seamless product building, deployment, and testing across all release cycles from development to production.\nEnhance and apply a strong understanding of modern security principles and practices to the development and deployment of applications.\nImplement security measures such as authentication, authorization, data encryption, and vulnerability assessments to protect applications from security threats.\nRequirements\nLike us, youâ€™ll be deeply committed to delivering impactful outcomes for customers.\n7+ years of demonstrated ability to develop resilient, high-performance, and scalable code tailored to application usage demands.\nAbility to lead by example with hands-on development while managing project timelines and deliverables.\nExperience in agile methodologies and practices, including sprint planning and execution, to drive team performance and project success.\nExperience building RESTful web services using NodeJS.\nExperience writing batch/cron jobs using Python and Shell scripting.\nExperience in web application development using JavaScript and JavaScript libraries.\nHave a basic understanding of Typescript, JavaScript, HTML, CSS, JSON and REST based applications.\nExperience/Familiarity with RDBMS and NoSQL Database technologies like MySQL, MongoDB, Redis, ElasticSearch and other similar databases.\nUnderstanding of code versioning tools such as Git.\nUnderstanding of building applications deployed on the cloud using Google cloud platform or Amazon Web Services (AWS), Docker and kubernetes.\nExperienced in JS-based build /Package tools like Grunt, Gulp, Bower, Webpack and NPM.\nBenefits\nYou define your work\nWe acknowledge that your work does not define you. Itâ€™s you who will define your work here. We do not encourage trade-offs between work and life.\nPropelled by courage & care\nWe dare each other with the art of possible and then watch each otherâ€™s back delivering the solution with speed, agility, heart and rigor.\nLearn with the best\nWith a strong leadership team from diverse backgrounds, you can expect to get the best of many worlds\n\nAnd much more\n\nIndustry competitive compensation\nESOPs of a boat that you must onboard before it becomes a ship\nWork on real problems of India that will create Impact at scale\nWork with all the jazz and fancy that new and innovative technologies bring\n\nWhat you will not get\nWe come from a place of honesty. So letâ€™s set our expectations right!\nPredictability of work\nYou will be a spider in the web; we will throw everything at you!\nClimbing the slow ladder of Career Growth\nWe all love to hop, skip & grow!\nBureaucracy and slow decision making\nWhat was that again??\nMeetings, meetings & only meetings\nWe believe in agility, empowerment and get the work done!",Industry Type: FinTech / Payments,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MySQL', 'Node.Js', 'MongoDB']",2025-06-12 14:30:26
Application Manager,XL India Business Services Pvt. Ltd,14 - 18 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Application Manager Bangalore, Karnataka, India We invent the new to help the world move forward\n\nCombining powerful analytics and deeper insights with bigger ideas and innovative solutions, we free up our clients potential, thereby fulfilling our own\n\nTake it seriously\n\nMake it fun\n\nKnow it matters\n\nApplication Managers oversee technical teams within a Delivery Team and help to manage day-to-day tasks to ensure high levels of productivity, accuracy, and work priority\n\nApplication manager is responsible for the technical solution delivery and maintenance of the same in production\n\nWhat you ll be DOING What will your essential responsibilities include? Technically lead and manage Business Analysts and Developers including assignment of work\n\nAssists Delivery Lead to manage SI Partners by helping to provide partner day to day direction on prioritization and decisions\n\nPerforms deliverable reviews and manage measurement of deliverable quality\n\nAssists to maintain application standards like app certification, vendor management, release coordination, apply security standard etc Act as liaison between SI partner team and stakeholders\n\nEnsure technical team alignment with business expectations and delivery roadmap\n\nWill liaise and consult with the Architecture team to ensure design alignment with AXA XL s architecture strategy\n\nProvide technical SME assistance for the insurance billing and payment solutions (Ex\n\nGuidewire, Majesco, SAP)\n\nEstimate work requests at various levels\n\nPartners with Release Management to Coordinate Release Activities\n\nWorks with Operational Change Management team to ensure training materials and release notes are being delivered\n\nMonitor and execute release and deployment activities\n\nEnsure full compliance to AXA standards of the products for the business (incl\n\nSecurity & Data Privacy)\n\nAct as liaison between SI Partner team and stakeholders\n\nSolid experience working in an Agile environment\n\nAssist in Coordinating and Participate in Agile Ceremonies as required\n\nMonitor Agile ceremonies and activities to ensure compliance with Digital Factory standards\n\nYou will report to the Delivery Lead - Claims\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Relevant years of hands-on work experience with complex applications\n\nRelevant years of experience working in an Agile environment\n\nProven experience in Microsoft technical suite like Dot Net (Core, Standard), SQL Server, C#, VBDot Net, ASPDot Net\n\nDeep understanding of enterprise integration patterns (API, middleware and/or data migration, secured data flow) Cloud based experience with Azure\n\nDevOps practices including CI/CD pipelines, infrastructure as code and containerization Proficient in use of JIRA, Confluence, Bitbucket, team city and Data dog\n\nTimely and accurate completion of deliverables in a manner that is auditable, testable, and maintainable\n\nImplementation consistent with solution design and business specifications\n\nEnsure for technical integrity of changes made to systems\n\nAdherence to development governance & SDLC standards\n\nTeam leadership abilities required, including experience leading and mentoring development professionals\n\nMust be able to set priorities and multi-task\n\nPrior work experience with Commercial Lines of Insurance\n\nDesired Skills and Abilities: Proficiency with multiple application delivery models including Agile, iterative and waterfall\n\nBroad understanding of application development and support technologies\n\nPrior work experience in an insurance or technology field preferred\n\nPrior experience working with multiple vendor partners\n\nAdaptable to new/different strategies, programs, technologies, practices, cultures, etc Comfortable with change, able to easily make transitions\n\nBachelor s degree in the field of computer science, information systems or a related field preferred\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides competitive compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Change management', 'SAP', 'Billing', 'Agile', 'Application development', 'microsoft', 'Middleware', 'SDLC', 'Analytics', 'SQL']",2025-06-12 14:30:28
PostgreSQL DBA / Developer,Srs Business Solutions India,3 - 8 years,6-11 Lacs P.A.,['Jaipur'],"Hello,\n\nWe are hiring for ""PostgreSQL DBA / Developer"" for Jaipur Location.\n\nJob Title PostgreSQL DBA / Developer\nWork Mode: Work from Office\nExp: 3+Years\nLoc: Jaipur\nNotice Period: Immediate joiners(notice period served candidate)\n\nNOTE: We are looking for Immediate joiners(notice period served candidate)\n\nApply only If you are Immediate joiners(notice period served candidate)\n\nApply only If you are from Jaipur or Rajasthan.\n\n\nRequired experience:\nStrong expertise in PostgreSQL database development or administration\nShould have experience in writing queries, replication, Optimization and data migration\n\nNOTE: Let your co-workers or Circle know about this opportunity if you lack these skills set.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Postgresql', 'Postgresql Database Administration', 'Postgresql development']",2025-06-12 14:30:30
Test Lead,Infosys,5 - 10 years,10-20 Lacs P.A.,[],"Role & responsibilities\n\nThe Test Lead oversees the testing strategy and execution for the Microsoft Fabric migration and Power BI reporting solutions. This offshore role ensures quality, reliability, and client satisfaction through rigorous validation.\nThe successful candidate will have a strong testing background and coordination skills.\nResponsibilities\nDevelop and execute the testing strategy for Microsoft Fabric and Power BI deliverables.\nValidate data migration, pipeline functionality, and report accuracy against requirements.\nCoordinate with the Offshore Project Manager to align testing with development milestones.\nCollaborate with onsite technical leads to validate results and resolve defects. â€¢ Oversee offshore testers, ensuring comprehensive coverage and quality standards.\nProactively identify risks and articulate solutions to minimize delivery issues.\nSkills\nBachelors degree in IT, computer science, or a related field.\n5+ years of experience in test leadership for data platforms and BI solutions.\nKnowledge of Microsoft Fabric, Power BI, and data migration testing.\nProficiency with testing tools (e.g., Azure DevOps, Selenium) and SQL.\nStrong communication and stakeholder management skills.\nDetail-oriented with a focus on quality and continuous improvement\n1. JD for Data Modeler\nThe Data Modeler designs and implements data models for Microsoft Fabric and Power BI, supporting the migration from Oracle/Informatica. This offshore role ensures optimized data structures for performance and reporting needs. The successful candidate will bring expertise in data modeling and a collaborative approach.\nResponsibilities\nDevelop conceptual, logical, and physical data models for Microsoft Fabric and Power BI solutions.\nImplement data models for relational, dimensional, and data lake environments on target platforms.\nCollaborate with the Offshore Data Engineer and Onsite Data Modernization Architect to ensure model alignment.\nDefine and govern data modeling standards, tools, and best practices.\nOptimize data structures for query performance and scalability.\nProvide updates on modeling progress and dependencies to the Offshore Project Manager.\nSkills\nBachelorâ€™s or masterâ€™s degree in computer science, data science, or a related field.\n5+ years of data modeling experience with relational and NoSQL platforms.\nProficiency with modeling tools (e.g., Erwin, ER/Studio) and SQL.\nExperience with Microsoft Fabric, data lakes, and BI data structures.\nStrong analytical and communication skills for team collaboration.\nAttention to detail with a focus on performance and consistency.\nmanagement, communication, and presentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Test lead', 'Migration', 'power bi', 'microsoft fabric']",2025-06-12 14:30:32
Oracle Recruitment Cloud Expert,UST,7 - 12 years,Not Disclosed,"['Kochi', 'Bengaluru', 'Thiruvananthapuram']","Role: Oracle Recruitment Cloud Expert\nLocation: PAN India\n\nKey Responsibilities\n1. Development & Implementation\nInterpret application/component designs and develop accordingly\nCode, debug, test, document, and communicate development stages",,,,"['Oracle Hcm Cloud', 'Oracle Hcm', 'Oracle Recruiting Cloud']",2025-06-12 14:30:34
MDM Testing - Associate Analyst,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for a skilled MDM Testing Associate Analyst who will responsible for ensuring the quality and integrity of Master Data Management (MDM) applications through rigorous testing processes. This role involves collaborating with cross-functional teams to define testing objectives, scope, and deliverables, and to ensure that master data is accurate, consistent, and reliable. and comply with Amgens standard operating procedures, policies, and guidelines. Your expertise will be instrumental in ensuring quality and adherence to required standards so that the engineering teams can build and deploy products that are compliant.\nRoles & Responsibilities:\nTest Planning: Develop and implement comprehensive testing strategies for MDM applications, including defining test objectives, scope, and deliverables. This includes creating detailed test plans, test cases, and test scripts.\nTest Execution: Execute test cases, report defects, and ensure that all issues are resolved before deployment. This involves performing functional, integration, regression, and performance testing.\nData Analysis: Analyze data to identify trends, patterns, and insights that can be used to improve business processes and decision-making. This includes validating data accuracy, completeness, and consistency.\nCollaboration: Work closely with the MDM, RefData and DQDG team and other departments to ensure that the organizations data needs are met. This includes coordinating with data stewards, data architects, and business analysts.\nDocumentation: Maintain detailed documentation of test cases, test results, and any issues encountered during testing. This includes creating test summary reports and defect logs.\nQuality Assurance: Develop and implement data quality metrics to ensure the accuracy and consistency of master data. This includes conducting regular data audits and implementing data cleansing processes.\nCompliance: Ensure that all master data is compliant with data privacy and protection regulations. This includes adhering to industry standards and best practices for data management.\nTraining and Support: Provide training and support to end-users to ensure proper use of MDM systems. This includes creating user manuals and conducting training sessions\nStay current on new technologies, validation trends, and industry best practices to improve validation efficiencies.\nCollaborate and communicate effectively with the product teams.\nBasic Qualifications and Experience:\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\n2+ years of experience in MDM implementations, primarily with testing (pharmaceutical, biotech, medical devices, etc.)\nExtensive experience on ETL/ELT and MDM testing (Creating test plan, test scripts and execution of test scripts and bugs tracking/reporting in JIRA)\nInformatica MDM: Proficiency in Informatica MDM Hub console, configuration, IDD (Informatica Data Director), IDQ, and data modeling\nor\nReltio MDM: Experience with Reltio components, including data modeling, integration, validation, cleansing, and unification.\nAdvanced SQL: Ability to write and optimize complex SQL queries, including subqueries, joins, and window functions.\nData Manipulation: Skills in data transformation techniques like pivoting and unpivoting.\nStored Procedures and Triggers: Proficiency in creating and managing stored procedures and triggers for automation.\nPython: Strong skills in using Python for data analysis, including libraries like Pandas and NumPy etc.\nAutomation: Experience in automating tasks using Python scripts.\nMachine Learning: Basic understanding of machine learning concepts and libraries like scikit-learn.\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nGood-to-Have Skills:\nETL Processes: Knowledge of ETL processes for extracting, transforming, and loading data from various sources.\nData Quality Management: Skills in data profiling and cleansing using tools like Informatica.\nData Governance: Understanding of data governance frameworks and implementation.\nData Stewardship: Ability to work with data stewards to enforce data policies and standards.\nSelenium: Experience with Selenium for automated testing of web applications.\nJIRA: Familiarity with JIRA for issue tracking and test case management.\nPostman: Skills in using Postman for API testing.\nUnderstanding of compliance and regulatory considerations in master data.\nIn depth knowledge of GDPR and HIPPA guidelines.\nProfessional Certifications:\nMDM certification (Informatica or Reltio)\nSQL Certified\nAgile or SAFe certified\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MDM Testing', 'ETL Processes', 'Data Stewardship', 'MDM', 'Agile', 'Data Quality Management', 'Data Governance', 'SQL']",2025-06-12 14:30:37
CRM Technical professional,big 4,4 - 9 years,Not Disclosed,"['Gurugram', 'Bengaluru', 'Mumbai (All Areas)']","Job Summary\n1.1 CRM Technical\n1.1.1 Must Have\nDesign, develop, and implement Microsoft Dynamics 365 solutions to meet business requirements.\nKnowledge of minimum two D365 CRM modules like Sales, Service, Field Service and Customer Insights\nCustomize and configure D365 applications, including workflows, plugins, and integrations.\nStrong technical skills in Power Pages, PowerApps, Power Automate, and related technologies\nExperience with CanvasApp development\nKnowledge of form script using JS, Business Process Flow, Business Rules\nKnowledge of solution deployment, solution management\nKnowledge of Integrations WebAPI\nExperience with development tools like Visual Studio, XRMToolBox, Postman etc\nDevelop and maintain technical documentation for D365 solutions.\nMicrosoft Certification\n1.1.2 Good To have:\n(Must meet 50% for Con level, above 70% match for Sr. Con and above)\nProficiency in programming languages such as C#, .NET, JavaScript, and SQL.\nExperience with D365 integrations using APIs and web services.\nKnowledge of CI/CD pipeline configuration for deployment\nDevelopment knowledge on PCF Controls for both canvas and model driven apps\nAbility to create custom connectors for Power Automate\nExperience with SharePoint integration scenarios with Power Apps and D365\nExperience with Azure platform including functions, LogicApps, ServiceBus, KeyVault\nPowerBI report development Connecting data sources, importing data, and transforming data for Business intelligence.\nExperience with Large enterprise level data migration using Azure Data Factory, Kingsway soft\nKnowledge of CoPilot configuration\n1.1.3 Power Pages:\nDesign, develop, and implement web applications using Microsoft Power Pages.\nCustomize and configure Power Pages to meet business requirements.\nKnowledge of Table Permission, Forms, Lists configuration\nKnowledge of HTML, JS and Liquid for Web Template configuration\nExperience with Power Pages Design Studio and its features",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power apps', 'Microsoft Dynamics 365', 'Power Pages', 'CanvasApp development', 'Microsoft Certification', 'XRMToolBox', 'Power Automate', 'Visual Studio', 'Postman', 'Integrations WebAPI']",2025-06-12 14:30:39
.net fullstack developer - Azure Expert,Kyndryl,10 - 15 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward â€“ always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.\n\nThe Role\nAre you ready to join the team of software engineering experts at Kyndryl? We are seeking a talented Software Engineering Technical Specialist to contribute to our software engineering space and provide critical skills required for the development of cutting-edge products.  \n\nAs a Software Engineering Technical Specialist, you will develop solutions in specific domains such as Security, Systems, Databases, Networking Solutions, and more. You will be a leader â€“ contributing knowledge, guidance, technical expertise, and team leadership skills. Your leadership will be demonstrated in your work, to your customers, and within your teams.\n\nAt Kyndryl, we value effective communication and collaboration skills. When you recognise opportunities for business change, you will have the ability to clearly and persuasively communicate complex technical and business concepts to both customers and team members. Youâ€™ll be the go-to person for problem-solving of customersâ€™ business and technical issues. You have a knack for effectively identifying and framing problems, leading the collection of elements of information, and integrating this information to produce timely and thoughtful decisions. Your aim throughout, is to improve the effectiveness, efficiency and delivery of services through the use of technology and technical methods and methodologies.\n\nDriving the design, development, integration, delivery, and evolution of highly scalable distributed software you will integrate with other layers and offerings. You will provide deeper functionality and solutions to address customer needs. You will work closely with software engineers, architects, product managers, and partner teams to get high-quality products and features through the agile software development lifecycle.\n\nYour continuous grooming of features/user stories to estimate, identify technical risks/dependencies and clearly communicate them to project stakeholders will ensure the features are delivered with the right quality and within timeline. You will maintain and drive the clearing of technical debt, vulnerabilities, and currency of the 3rd party components within the product.\n\nAs a Software Engineering Technical Specialist, you will also coach and mentor engineers to design and implement highly available, secure, distributed software in a scalable architecture. This is an opportunity to make a real impact and contribute to the success of Kyndryl's innovative software products. Join us and become a key player in our team of software engineering experts!",,,,"['kubernetes', 'server', 'css', 'methods', 'openshift', 'data migration', 'azure migration', 'sql', 'iis', 'apache', 'devops', 'html', 'software engineering', 'api', 'agile principles', 'communication skills', 'process', 'new relic', 'microsoft azure', 'sql server', 'nosql', 'framework', 'devops automation', 'collaboration', 'splunk', '.net', 'aws']",2025-06-12 14:30:41
NEO4j / Neptune Developer,Mobile Programming,3 - 5 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced NEO4j / Neptune Developer to join our team in Hyderabad\nIn this role, you will be responsible for designing, implementing, and optimizing graph-based solutions using NEO4j or Amazon Neptune databases\nYou will collaborate with cross-functional teams to integrate and deploy graph technologies that solve complex business problems\nThis is a fantastic opportunity for someone who thrives in a dynamic environment and is excited about leveraging graph databases to create innovative solutions\nKey Responsibilities:\nDesign, develop, and maintain graph database models using NEO4j or Amazon Neptune\nDevelop and implement graph query languages like Cypher (for NEO4j) or SPARQL for efficient data retrieval\nOptimize graph database performance for large-scale data and high-volume queries\nCollaborate with teams to identify business requirements and design graph-based data models\nIntegrate graph database solutions with existing systems and applications\nTroubleshoot and resolve performance issues or bugs within the graph database solutions\nContribute to the continuous improvement of the development process, tools, and techniques\nProvide support for data migration and integration of graph technologies with other enterprise systems\nWrite high-quality, clean, and maintainable code, ensuring best practices are followed\nRequired Skills and Qualifications:\n3-5 years of experience in developing with NEO4j or Amazon Neptune\nStrong knowledge of graph database modeling, relationships, and graph theory\nProficiency in Cypher query language (for NEO4j) and SPARQL (for Amazon Neptune)\nHands-on experience with graph analytics and performance tuning\nExperience integrating graph databases with other systems and services\nFamiliarity with NoSQL databases and distributed data architectures\nUnderstanding of cloud-based graph database solutions (eg, AWS Neptune)\nAbility to work in an Agile development environment\nStrong troubleshooting and problem-solving skills\nExcellent written and verbal communication skills\nTechnical Skills:\nNEO4j | Amazon Neptune | Cypher | SPARQL | Graph Database Modeling | NoSQL | Graph Analytics | Python | Java | AWS | Data Migration | ETL | Cloud Solutions | Agile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['NEO4j', 'Java', 'NoSQL', 'Data Migration', 'Agile', 'ETL', 'AWS', 'Graph Analytics', 'Python', 'Cloud Solutions']",2025-06-12 14:30:44
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Agra'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n- Configure and refine related workflows within NetSuite\n- Provide post-implementation support and troubleshooting\n- Understand and manage system configuration, data flows, and integration points\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\nQualifications :\n- Strong functional and technical knowledge of NetSuite\n- Experience with ERP data migration, especially from QuickBooks\n- Familiarity with third-party integration tools and NetSuites workflow engine\n- Ability to support post-go-live activities and optimize ERP performance\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:30:46
Fullstack Developer,Grid Dynamics,5 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai']","We are seeking an experienced Full Stack Java Developer skilled in designing, developing, and deploying scalable applications using Java/J2EE, Spring Boot, Microservices, and Angular. The ideal candidate will have hands-on experience with Gradle, Kafka, MySQL, Amazon S3, Docker, and AWS cloud services. You will play a key role in building robust backend systems and intuitive front-end interfaces, leveraging modern DevOps and cloud-native practices.\n\nKey Responsibilities:\nDesign and implement scalable microservices using Java/J2EE and Spring Boot frameworks.\nDevelop RESTful APIs to facilitate communication between distributed services.\nBuild and maintain front-end components using Angular (or similar frameworks).\nIntegrate and manage message-driven architectures using Kafka for event streaming and processing.\nUse Gradle for dependency management and build automation.\nImplement and optimize relational database solutions using MySQL.\nManage file storage and retrieval operations with Amazon S3.\nContainerize applications using Docker and orchestrate deployments on AWS.\nCollaborate with DevOps teams to automate CI/CD pipelines and infrastructure provisioning.\nEnsure application reliability, scalability, and security in a cloud-native (AWS) environment.\nParticipate in code reviews, unit testing, and integration testing to ensure code quality.\nDocument technical designs and implementation processes for future reference.\nEngage in Agile/Scrum ceremonies and collaborate with cross-functional teams.\n\nRequired Qualifications:\nBachelors degree in Computer Science, Information Technology, or a related field.\nProven experience (typically 5+ years) in Java/J2EE application development.\nProficiency in Spring Framework, including Spring Boot and Spring Data JPA.\nStrong understanding of microservices architecture and RESTful API design.\nHands-on experience with Gradle for build automation.\nProficient in integrating and configuring Kafka for messaging solutions.\nExperience with MySQL or other relational databases.\nFamiliarity with Amazon S3 for cloud storage operations.\nFront-end development skills with Angular (TypeScript, HTML, CSS).\nExperience with Docker for containerization and AWS for cloud deployments.\nKnowledge of CI/CD tools and DevOps practices.\nStrong problem-solving, communication, and teamwork skills.\n\nPreferred/Good to Have:\nExperience with additional AWS services (EC2, Lambda, EKS, etc.).\nFamiliarity with security best practices (OAuth2, JWT, etc.).\nExposure to observability tools (Prometheus, Grafana) and infrastructure-as-code (Terraform).\nExperience with Agile methodologies and mentoring junior developers.",Industry Type: IT Services & Consulting,Department: Research & Development,"Employment Type: Full Time, Permanent","['Java', 'Amazon Web Services', 'Kafka', 'Spring Boot', 'Spring', 'AWS', 'Microservices', 'Angular', 'SQL']",2025-06-12 14:30:48
CX Sales and Service Consultant,Gnostic Solutions,10 - 18 years,20-35 Lacs P.A.,[],"Overall, 8+ years of experience relevant to this position.\nDemonstrable experience as a techno-functional lead on at least two large-scale full-life cycle implementations of Oracle CX Applications, with strong implementation expertise in at least two of the following products is a must. \nCX Sales\nB2B Service Cloud\nField Service Cloud\nCPQ\nAsset Based Service\nSubscription Management\nIncentive Compensation\nFamiliarity and exposure to business processes such as Target to Lead, Opportunity to Cash, Request to Resolution.\nStrong techno-functional skills in proposing, designing optimal solutions, including ownership of the overall solution for customization/extension/integrations on Oracle CX Cloud Projects.\nExperience developing process flow diagrams, gathering requirements, conducting workshops, design and prototyping, testing, training, defining support procedures, and implementing practical business solutions.\nOracle CPQ - Hands on experience in configuring Oracle CPQ Cloud module Pricing, Quotation Configuration, Product Management-Items, BOMs, and System Configuration, Document Engine and Big Machines Language (BML), Utilities Libraries, Validation/Hiding/Constraint rules, Layout editor, Commerce layout, Custom CSS, Designing extensions and interfaces in Oracle CPQ Cloud module.\nOSC/B2B Cloud - Prior hands-on experience in functional configurations and customization in the following areas - custom objects, groovy scripting, assignment manager, workflows, triggers email alerts, Data Migration, OTBI Analytics etc.\nOracle Field Service - Prior hands-on experience in configurations and customization in the following areas Core Application, Profiles, Permissions, Business Rules, Routing Plans, Work Schedules, Smart Collaboration, Action Management, Mobility and Manage Displays, Reports, Data migration, Inbound/Outbound messages. Experience developing Oracle Field Service forms and plug-ins.\nExperience developing extensions using Redwood UI, VBCS, VBS, JET\nGood knowledge in web based front end development using â€“ HTML, JavaScript, and CSS\nExperience with React and/or other front-end JavaScript frameworks, would be a plus.\nStrong hands-on experience with design and development of integrations (point to point and Integration Cloud) using various",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['oracle cx applications cloud', 'service cloud', 'Field Service', 'vbcs', 'oracle cs', 'oracle cloud', 'Functional', 'B2B', 'Cpq Cloud', 'ofsc', 'Techno Functional', 'integration', 'fusion sales', 'B2B service', 'fusion service']",2025-06-12 14:30:51
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Kanpur'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'HubSpot', 'Data Migration', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:30:53
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Lucknow'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:30:55
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Agra'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:30:57
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Noida'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:30:59
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Faridabad'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:01
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Ludhiana'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:03
SAP RAR Consultant - Support & Integration,Sadup Soft,8 - 10 years,Not Disclosed,['Gurugram'],"- We are seeking a skilled SAP RAR Implementation Consultant with relevant 5+ years of experience to lead the implementation and integration of the SAP Revenue Accounting and Reporting (RAR) module.\n\n- The ideal candidate will have a strong understanding of revenue recognition standards (IFRS 15, ASC 606) and hands-on experience in configuring and deploying SAP RAR solutions.\n\n- SAP FI overall experience 8+ years\n\n- Analyze and document business requirements related to revenue recognition.\n\n- Collaborate with stakeholders to understand the intricacies of revenue recognition processes and translate them into system requirements.\n\n- Design and configure the SAP RAR module to meet business needs, ensuring seamless integration with SAP SD, SAP FI, and other relevant modules.\n\n- Customize the system to comply with IFRS 15 and ASC 606 standards.\n\n- Support data migration efforts to ensure accurate transfer of revenue data into the SAP RAR system.\n\n- Oversee data mapping, cleansing, and validation to maintain data integrity.\n\n- Develop and execute comprehensive test plans to validate the implementation of the SAP RAR module.\n\n- Identify and resolve issues during the testing phase to ensure a smooth deployment.\n\n- Provide training and support to end-users to facilitate the adoption of the SAP RAR system.\n\n- Address user queries and issues post-implementation to ensure ongoing system effectiveness.\n\n- Manage project timelines, deliverables, and documentation.\n\n- Ensure projects are completed on time, within scope, and in accordance with predefined quality standards.\n\n- Analyze and document business requirements for revenue recognition.\n\n- Design and configure the SAP RAR module, ensuring seamless integration with SAP SD, FI, and other modules.\n\n- Support data migration and ensure accurate revenue data transfer.\n\n- Develop and execute test plans to validate the implementation.\n\n- Provide training and post-implementation support to end-users.\n\n- Manage project timelines, deliverables, and documentation.\n\n- Proficiency in SAP RAR configuration and integration.\n\n- Experience with SAP S/4HANA is mandatory\n\n- Excellent communication and project management skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP RAR', 'SAP S/4 HANA', 'SAP SD', 'SAP Integration', 'SAP FI', 'SAP Implementation', 'SAP Support']",2025-06-12 14:31:06
Manager,XL India Business Services Pvt. Ltd,10 - 15 years,Not Disclosed,['Gurugram'],"Manager, Business Intelligence & Reporting Gurugram/Bangalore, India AXA XL recognizes digital, data and information assets are critical for the business, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained advantage\n\nOur Data, Intelligence & Analytics function is focused on driving innovation through optimizing how we leverage digital, data to drive strategy and differentiate ourselves from the competition\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and strengthen our digital reporting capabilities we are seeking a Manager, BI and Reporting\n\nIn this role you will support/manage BI & reporting\n\nWhat you ll be DOING What will your essential responsibilities include? Be an expert and manage BI & Reporting BAU and effective stakeholder management\n\nBe and expert and manage BI & Reporting products and Impart training to users for self-service reporting\n\nManage IDA BI & Reporting on various strategic initiatives as they arise and enable the development of the function and related capabilities\n\nOversee the design/production/change for BI reporting\n\nContribute to best practices and standards to ensure a consistent approach to BI reporting\n\nRaising data gaps to the IDA Data Quality team so that accurate information flows in our Data Architecture\n\nPartner with key areas within IDA, GT, and across business stakeholders for any BI requirement\n\nInstill a customer-first culture, prioritizing service for our business stakeholders above all\n\nAn understanding of AI will be an added advantage\n\nYou will report to the Senior Delivery Lead, Business Intelligence & Reporting\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Ability to communicate well within team, peers, teams across the globe, manage stakeholders effectively\n\nBrings in a collaborative spirit, can-do attitude and a Customer First mindset\n\nA minimum of a bachelor s or master s degree in a relevant discipline\n\nRelevant years of experience in a data role (analytics or engineer) supporting multiple specialty areas of Data and Analytics\n\nPassion for digital, data and experience working within a digital and data driven organization\n\nExperience on BI tool like Power BI etc Desired Skills and Abilities: Intermediate proficiency in SQL, Advance Excel and Power BI\n\nAble to help/guide his team members on any technical issues and develop them so that the team can self-directedly manage the same\n\nAbility to lead a project/team\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and enables business growth and is critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most inclusive workforce possible, and create a culture where everyone can bring their full selves to work and reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe\n\nRobust support for Flexible Working Arrangements Enhanced family-friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides competitive compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far-reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business intelligence reporting', 'Agile', 'digital strategy', 'Data quality', 'Business strategy', 'Stakeholder management', 'Manager Business Intelligence', 'Analytics', 'SQL', 'Data architecture']",2025-06-12 14:31:08
Cloud Engineer,S2 Tech,6 - 11 years,Not Disclosed,[],"Role & responsibilities\nMaintain the AWS resource library using the AWS Cloud Development Kit (CDK) written in typescript\nWrite tests and assertions to harden the resiliency of the code\nAdminister cloud network resources including subnets, routes, DNS, load balancers and firewalls\nTroubleshoot Microsoft .NET application performance and functionality\nIdentify and implement opportunities for quality improvement using test automation\nAdminister and interpret results from performance tools such as X-Ray, CloudWatch Logs, and Dynatrace APM to promote continuous improvement of products to our customers\nOptimize cloud configuration from both a technical and budgetary perspective\nReview and enhance runbooks, knowledge base documentation, and log defects\nEnforce security tool actions for both short term remediation and prevent recurrence\nIdentify and remediate application performance and database performance on AWS EC2, Lambda, and RDS (Relational Database Service) instances\nAssist with moving workloads from on premise data center to cloud\nConfigure and maintain Microsoft .NET workloads running on both EC2 instances and AWS Lambda\nLearn and stay up to date with current AWS best practices and service offerings\nRequired Skills\nA Bachelor's Degree from an accredited college or university with a major in Computer Science, Information Systems, Engineering, or other related scientific or technical discipline or three (3) years of equivalent experience in a related field\nAWS Certification\n\nPreferred candidate profile\nAt least five (5) years of experience administering Amazon Web Services\nAt least two (2) years of experience administering serverless workloads\nAWS Certification in one or more of the following\nAdvanced Networking\nDevOps Engineer Professional\nSolutions Architect - Professional\nAt least one (2) year of writing and supporting AWS CDK code\nAn advanced knowledge of typescript development\nAn advanced knowledge of .NET application development\nAt least two (2) years of experience building CI/CD automation processes\nExperience with software development life cycle (SDLC) and agile/iterative methodologies\nExperience in large-scale migrations such Data Center to Cloud\nStrong technical communication skills, both verbal and written\nExcel at being a team player who excels at knowledge sharing and understanding the interpersonal aspects of working in a team.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Migration', 'AWS', 'Devops', 'Subnetting', 'Cloud Development', 'Aws Certified', 'Javascript', 'DNS', 'Routing', 'Ci/Cd']",2025-06-12 14:31:10
SQL Developer,Sutherland Global Services Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"Job Title: SQL Developer with SSIS\nExperience Level: Mid-Level\nJob Summary:\nWe are seeking a skilled SQL Developer with expertise in SSIS (SQL Server Integration Services) to join our data engineering team. The ideal candidate will be responsible for developing, maintaining, and optimizing SQL queries, stored procedures, and SSIS packages to support data integration, transformation, and reporting needs.\nKey Responsibilities:",,,,"['Computer science', 'Performance tuning', 'Version control', 'GIT', 'query optimization', 'Analytical', 'data integrity', 'Stored procedures', 'SSIS', 'Reporting tools']",2025-06-12 14:31:13
Software Engineer,Maxwell Geosystems,2 - 3 years,Not Disclosed,"['Kochi( Marine Drive )', 'Ernakulam']","1. Design, develop, test, and maintain the web/mobile application using various technologies\n2. Write efficient, reusable, and scalable code to ensure high performance, security and reliability.\n3. Identify and resolve bugs and performance bottlenecks to ensure smooth functionality.\n4. Maintain clear and concise documentation for code, design, and technical processes.\n5. Work closely with other developers, product managers, and designers to understand project requirements and deliver high quality solutions.\n6. Write unit tests, conduct code reviews, and participate in deployment activities to ensure high quality code.\n7. Develop and maintain databases, including schema design, data migration, and query optimization.\n8. Mentoring of junior software engineers.",Industry Type: Engineering & Construction,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PHP', 'Python', 'MYSQL']",2025-06-12 14:31:15
LMS Manager,Rachna Sagar,3 - 5 years,Not Disclosed,['New Delhi'],"Location : Delhi\nSkill Set : Proficiency in LMS Platforms\nJob Summary:\nThe LMS Manager will manage the day-to-day operations of the Learning Management System (LMS), ensuring optimal performance, integration with existing systems, and user support. The role also involves content management, data analytics, project management, and coding for system improvements. This position requires a strong background in educational technology, server management, and technical skills related to LMS platforms and web development.\nKey Responsibilities:\nLMS Management: Oversee daily operations, troubleshoot issues, and ensure system efficiency.\nImplementation & Integration: Collaborate to add new features, integrate systems, and manage data migration.\nUser Support: Train and assist teachers, staff, students, and parents in using the LMS.\nContent Management: Maintain course materials, ensuring compliance with standards.\nAnalytics & Reporting: Generate reports for instructional insights and system effectiveness.\nProject & Vendor Management: Handle LMS-related projects and coordinate with vendors for upgrades and support.\nQuality Assurance: Ensure compliance with educational and regulatory requirements.\nDatabase & Server Handling: Manage cloud/shared hosting, data relationships, and server infrastructure.\nSales Support & Play Store Management: Address sales queries and oversee app publishing.\nCoding & Development: Develop LMS features (30% coding) using PHP, JavaScript, React, Node.js, etc.\nKey Skills & Requirements:\nEducation: Bachelors in Educational Technology, Computer Science, or related field.\nExperience: 3-5 years in LMS management, preferably in K-12 education.\nTechnical Skills: LMS platforms (Canvas, Moodle, Blackboard), database & server management, coding (PHP, React, Node.js).\nSoft Skills: Strong project management, communication, and problem-solving abilities.",Industry Type: Education / Training,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['LMS Management', 'User Support', 'Content Management', 'Coding', 'Sales Support', 'PHP', 'Node.js', 'Quality Assurance', 'React', 'Analytics']",2025-06-12 14:31:17
DBA PL/SQL Developer,Softeon,3 - 6 years,Not Disclosed,['Chennai'],"Job Summary:\n\nWe are seeking a skilled and experienced DBA PL/SQL Developer to join our IT team. The ideal candidate will have a strong understanding of Oracle Database Architecture and be proficient in PL/SQL programming. This role involves the design, development, implementation, and maintenance of database solutions to support enterprise applications, ensuring high performance, data integrity, and reliability.\n\nRole & responsibilities:\nDesign and implement database structures including tables, indexes, views, and constraints.\nDevelop and maintain PL/SQL code including procedures, functions, triggers, and packages.\nWrite, debug, and optimize complex SQL and PL/SQL queries for data manipulation and reporting.\nManage Oracle database architecture, including schema design, storage management, and space utilization.\nImplement and manage database backup, recovery, and disaster recovery procedures.\nMonitor and tune database performance for optimal efficiency.\nCollaborate with application developers and project managers to support application requirements.\nEnsure database security, data integrity, and access control compliance.\nExecute data migration and transformation jobs using scripts and tools.\nApply database best practices, standards, and version control processes.\nParticipate in database upgrades, patching, and maintenance activities.\nRequired Skills and Qualifications:\n\nBachelor's degree in Computer Science, Information Technology, or related field.\n3 6 years of hands-on experience with Oracle Database and PL/SQL development.\nIn-depth understanding of Oracle database architecture and administration.\nStrong experience in database design, normalization, and data modeling.\nProven expertise in SQL query optimization and PL/SQL performance tuning.\nKnowledge of backup and recovery strategies using RMAN or equivalent.\nExperience in database security and access control implementation.\nFamiliarity with version control tools (e.g., Git) and agile development methodologies.\nAbility to manage multiple priorities and communicate effectively with stakeholders.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Pl / Sql Development', 'Sql Database Development', 'Oracle SQL', 'Architecture', 'Oracle Database']",2025-06-12 14:31:19
JDE S&D - Freelancers /Business Analyst,Clarisity,10 - 15 years,Not Disclosed,[],"Oracle JDEdwards Sr. Consultant / Freelancers - Sales & Distribution , Business Analyst\n\nJob description:\n\nOracle JD Edwards Sales and Distribution Management- Inventory, Procurement, Sales & Advance Pricing\nApplications Modules - Primary\nJD Edwards EnterpriseOne Distribution Suite\n- Inventory Mgt.\n- Procurement Management\n- Sales Order Management\n- Advanced Pricing\nApplications/technology domain- secondary\n- Transport / Warehouse Management\n- Business Analysis\nSkills Required :\n- 14+ years of Domain /IT experience (at least 10 years of grounds up experience in JDE Distribution functional and as Business analyst)\n- Graduate / Post graduate with Exposure/knowledge in Manufacturing CPG/ Retail, Materials, Procurement & Sales industry domains\n- Should have experience in Oracle JDE EnterpriseOne Projects (Implementation / Roll Outs /Support / Upgrade) as a functional consultant.\n- Experience in JDE Inventory, Sales, Procurement Management & Advanced Pricing is a must\n- Experiences in Transport management will be an added advantage\n- Excellent business communication skills, negotiation skills, and presentation skills\n- Ability to work within large teams and learn new functionality and modules during the project\n- Should have prior experience of working in Onsite/Offshore model\n- Should have knowledge of ERP implementation activities such as Business requirements & analysis, configuration, Conference Room pilot, Gap fit analysis, Application customization functional specifications, testing, data migration and or conversion\n- Should have Industry recognized certifications.\n\nJob Location - Remote/ WFH\nMode - Freelancer / Contractor\nShift timings - 7 pm IST - 3 am IST (Mon-Fri)\nEarly joiners are preferred.\n\n\nWe look forward to receiving your application! All your information will be kept strictly confidential.\n\nPlease furnish below details while applying with your updated resume:\nTotal Experience (yrs) -\nRelevant Experience in JDE SD -\nCurrent CTC -\nExpected CTC -\nNotice period -\nCurrent Location -\n\nThanks & Regards,\nKavita Tikone | Manager - HR & Recruitment\nM: +91 9819386379\neMail: Kavita.tikone@clarisity.com\nWeb: www.clarisity.com",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Freelance/Homebased","['Procurement', 'Inventory Management', 'Advanced Pricing', 'Taxation', 'JDE Sales & distribution', 'Business Analysis']",2025-06-12 14:31:21
Laravel Backend Developer,Ai Mobile Idea Box,3 - 5 years,4-7 Lacs P.A.,[],"URGENT HIRING (REMOTE)\n\nPayrollNinja is a saas provider and software development house. We're looking for an experienced Backend Laravel developer to join our IT team. You will be responsible for the backend logic of our web applications.\n\nIf you have an excellent and strong coding skills and passion for developing challenging web applications or improving existing ones, let's talk. As a backend Laravel developer, you'll work closely with our team to ensure system consistency and improve user experience.\n\nUltimately, you should be able to develop and maintain functional and stable web applications to meet our company's needs.\n\nOur team are actively working on plantation based ERP which consists of many modules from operational to accounting. Potential to work on-site (Malaysia) if you've proven skillful.\n\n\nResponsibilities\nParticipate in the entire application lifecycle, focusing on coding and debugging.\nWrite clean code to develop functional web applications.\nTroubleshoot and debug applications.\nPerform tests to optimize performance and security.\nManage cutting-edge technologies to improve legacy applications.\nCollaborate with Front-end developers to integrate user-facing elements with server side logic.\nGather and address technical and design requirements.\nBuild reusable code and libraries for future use.\nContinuously propose system enhancement and implement security patches.\nFollow emerging technologies.\nCommunicate with client related to system development requirements and progress update.\n\nRequirements\nBachelors degree in computer programming, computer science, or a related field.\nProven work experience as a laravel backend developer (minimum 3 years)\nIn-depth understanding of the entire web development process (design, development and deployment)\nFluent in scripting languages like PHP, Java, Phyton or Node.js. (PHP is compulsory)\nExperience with SQL, MySQL and Oracle database systems (MySQL is compulsory)\nExpert in PHP Laravel framework and API integration\nFamiliar with Amazon Web Services (AWS)\nVersion control, such as Git, CVS or SVN\nFamiliarity with front-end languages (e.g. HTML, JavaScript, Jquery and CSS)\nExcellent analytical and time management skills\nTeamwork skills with a problem-solving attitude\nHigh attention to detail\nCritical thinker and can analyse client documents thoroughly.\nExcellent communication skills (English is compulsory)\nGood internet speed and fine working laptop condition is compulsory for remote job.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Php Laravel', 'Laravel', 'Api Integration', 'JQuery', 'Git Version Control', 'Software Development', 'GIT', 'Git Repository', 'MySQL', 'Web Application', 'Bootstrap', 'Html And Css', 'Ajax']",2025-06-12 14:31:23
Oracle Fusion Technical Consultant ( OIC Specialist ),Strawberry Infotech Pvt. Ltd.,4 - 9 years,Not Disclosed,['Gurugram'],"Role & responsibilities\nKey Responsibilities:\nDevelop and implement integrations using Oracle Integration Cloud (OIC) including REST, SOAP, File, and FTP adapters.\nWork on BIP Reports, OTBI Reports, Fast Formulas, and Personalizations in Oracle Fusion.\nCollaborate with functional consultants and business stakeholders to gather requirements and translate them into technical solutions.\nDesign and develop data migrations using FBDI/ADFDi, and support data conversion and transformation activities.\nTroubleshoot and resolve integration issues, provide performance tuning, and ensure system stability.\nImplement security policies, error handling, and logging for integrations.\nParticipate in unit testing, integration testing, and deployment activities.\nSupport post-implementation, production support, and ongoing enhancements.\nRequired Skills:\n4+ years of experience as an Oracle Fusion Technical Consultant.\nStrong expertise in Oracle Integration Cloud (OIC) REST APIs, SOAP Web Services, File/FTP, and Process Automation.\nExperience with Oracle Fusion SaaS modules like HCM, Finance, or SCM.\nGood knowledge of PaaS components (Visual Builder, PCS, etc.).\nStrong SQL/PLSQL skills and knowledge of Web Services and XML/XSDs.\nExperience with Reports (BIP, OTBI) and Data Extracts.\nHands-on experience in FBDI, HDL, and Data Migration processes.\nGood understanding of Oracle Fusion Cloud data models and structures.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oic', 'Oracle Fusion', 'Fusion Cloud']",2025-06-12 14:31:25
SAP Group Reporting,SRSInfoway,4 - 9 years,Not Disclosed,"['Noida', 'Hyderabad', 'Pune']","Job Title: SAP Group Reporting\nExperience: 4 to 7 Years\nLocation: Noida / Hyderabad / Pune\nEmployment Type: Full-time\nNotice Period: Immediate to 20 Days\n\nJob Summary:\nWe are seeking a highly skilled SAP Group Reporting Consultant with hands-on experience in implementing Group Reporting solutions, data migration, and strong knowledge of SAP BPC or SAP FICO. The ideal candidate should have deep expertise in financial consolidation concepts, configuration, and technical migration at the table level.\n\nKey Responsibilities:\nLead and implement SAP Group Reporting (GR) solutions, including configuration and testing.\nManage end-to-end data migration from legacy systems such as SAP BPC or SAP FICO into Group Reporting tables.\nUnderstand and configure GR master data, dimensions, consolidation units, and hierarchies.\nEnsure data integrity and accuracy throughout the migration and consolidation processes.\nWork closely with business stakeholders to understand financial consolidation requirements and design scalable GR solutions.\nConduct technical validations and reconciliation during migration.\nProvide support during UAT and post-go-live phases.\nParticipate in client interviews and demonstrate strong communication and consulting capabilities.\n\nMust-Have Skills:\nProven experience in SAP Group Reporting implementation projects.\nStrong knowledge in data migration from BPC or FICO to Group Reporting, including table-level understanding.\nSolid background in SAP BPC (Business Planning and Consolidation) or SAP FICO (Financial Accounting and Controlling).\nDeep understanding of FICO consolidation concepts and financial closing procedures.\nStrong analytical and troubleshooting skills.\nExcellent verbal and written communication skills must be able to represent the project during client interactions.\n\nPreferred Qualifications:\nSAP certification in Group Reporting, BPC, or FICO.\nExperience in S/4HANA Financials.\nExposure to SAP Analytics Cloud (SAC) for reporting (optional)\n\n\nMail: krishna.jothi@srsinfoway.com",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Sap Group Reporting', 'FICO', 'FI', 'Co']",2025-06-12 14:31:28
Onix is Hiring MDM Informatica Developer,Onix,3 - 8 years,Not Disclosed,"['Hyderabad', 'Pune']","Job Summary:\nWe are seeking a highly skilled Informatica MDM Developer to join our data integration and management team. The ideal candidate will have extensive experience in Informatica Master Data Management (MDM) solutions and a deep understanding of data quality, data governance, and master data modeling.\nKey Responsibilities:\nDesign, develop, and deploy Informatica MDM solutions (including Hub, IDD, SIF, and MDM Hub configurations).\nWork closely with data architects, business analysts, and stakeholders to understand master data requirements.\nConfigure and manage Trust, Merge, Survivorship rules, and Match/Merge logic.\nImplement data quality (DQ) checks and profiling using Informatica DQ tools.\nDevelop batch and real-time integration using Informatica MDM SIF APIs and ETL tools (e.g., Informatica PowerCenter).\nMonitor and optimize MDM performance and data processing.\nDocument MDM architecture, data flows, and integration touchpoints.\nTroubleshoot and resolve MDM issues across environments (Dev, Test, UAT, Prod).\nSupport data governance and metadata management initiatives.\nRequired Skills:\nStrong hands-on experience with Informatica MDM (10.x or later).\nProficient in match/merge rules, data stewardship, hierarchy management, and SIF APIs.\nExperience with Informatica Data Quality (IDQ) is a plus.\nSolid understanding of data modeling, relational databases, and SQL.\nFamiliarity with REST/SOAP APIs, web services, and real-time data integration.\nExperience in Agile/Scrum environments.\nExcellent problem-solving and communication skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['informatica', 'Informatica Mdm', 'Informatica Master Data Management', 'Mdm Informatica', 'Etl Informatica', 'MDM']",2025-06-12 14:31:30
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Aurangabad'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:32
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Ludhiana'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:31:34
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Madurai'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:36
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Vadodara'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:39
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Jaipur'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:41
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Chennai'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:43
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Bhubaneswar'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:45
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Allahabad'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:47
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Jamshedpur'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:49
Partner Alliance - Solutions,Prodapt Solutions,4 - 7 years,Not Disclosed,['Chennai'],"Overview\n\n2 roles - Presales solution engineers\n\nResponsibilities\n\nknown as a "" Presales Consultant,"" ""Sales Consultant,"" or â€œSales Engineerâ€ the Solution Consultant is responsible for leading the solution evaluation throughout the sales cycle and delivering thought leadership to companies to transform their customerâ€™s experience. You will be responsible for developing innovative demonstrations and proof of concepts to prove the value of Salesforce solutions within the context of our customers needs.\nYou will have a proven track record of building innovative multi-product solutions that co-exist with our customers existing environments.\nBeing able to translate our customers business goals into a working demonstrable vision will be a key skill.\n\nYour Responsibilities\nTo develop and present innovative customer solutions to key decision makers to address their business issues and needs whilst showing business value\nCoordinate and own the entire solution cycle through close collaboration with other Salesforce product teams.\nIndustry experience, incorporating and developing a point of view based on Salesforceâ€™s (or similar) solutions.\nTo fully understand and clearly articulate the benefits of Salesforce to customers at all levels, examples include; Administration/IT Staff, managers and ""C"" level executives.\nDevelop new go to market propositions that reflect changing market demands. Such as how AI can benefit our industries.\nDisplay initiative, and self-motivation while delivering high-quality results along with meeting all expectations for both internal and external customers.\nHave a strong interest in growing your career and participating in our internal training programs and mentorship initiatives.\n\nPersonality Attributes/Experience\nExperience will be evaluated based on alignment to the core competencies for the role (e.g. extracurricular leadership roles, military experience, volunteer work, etc.).\nThis role is within our Automotive sector to help to lead UK OEM and Retailer organisations to implement Digital Transformation strategies and deliver more success to their customers. An understanding of these industries is essential.\nAwareness of industry trends, challenges, and common business solutions in use for both Auto OEMs and retailers.\nTrack record of solution engineering, consultancy, or delivery for an enterprise software solution organisation that works within the Auto industry. We are open to a variety of backgrounds for the role.\nSolid oral, written, presentation and interpersonal communication and relationship skills.\nProven time management skills in a dynamic team environment.\nAbility to work as part of a team to solve problems in multifaceted, energizing environments.\nInquisitive, practical and passionate about technology and sharing knowledge.\nLikes to be the first to know something and to understand why and how things happen.\nGood at searching out information and experimenting likes to concentrate on a particular topic and solve puzzles.\nGood at explaining ideas and finding ways to keep peopleâ€™s attention.\nWilling and able to travel as needed.\n\n\nhiring a ServiceNow Presales Solution Consultant to join their team in London. The successful candidate will drive enterprise-wide transformations, manage relationships with C-level executives and create innovative solutions that elevate the digital experience.\n\nResponsibilities\nAs the ServiceNow Presales Solution Consultant, you will lead sal es and business development initiatives, uncovering new opportunities for ServiceNow solutions.\nLead the pre-sales journey, from crafting proposals to negotiating contracts and securing sign-offs.\nPartner with sales teams to define and present ServiceNow solutions to potential clients.\nServe as a trusted advisor to senior stakeholders, including C-suite executives and IT leaders.\nBuild and maintain strong relationships with ServiceNow leadership and strategic partners for seamless collaboration.\nCollaborate with key business functions (e.g. HR, IT, customer service) to customize ServiceNow solutions for their specific needs.\nStay up to date on market trends and ensure ServiceNow offerings are aligned with evolving business needs.\n\nSkillset\nMasterâ€™s or Bachelorâ€™s degree in Information Technology or similar.\nExtensive knowledge of ITSM, ITOM, CSM, HRSD, Employee Service Center, Case & Knowledge Management and Performance Analytics.\nProficient in system integration, data migration, automation, scripting and ServiceNow customization.\nUnderstanding of HR processes, employee engagement strategies and workflow design.\nSkilled in mapping customer journeys, improving service delivery and boosting customer satisfaction.\nExperienced in demonstrating the value of ServiceNow and advising clients on strategic implementation.\nStrong problem-solving abilities, with a knack for assessing business needs and proposing effective solutions.\nProven experience in driving adoption of new technology and training end-users for smooth transitions.\nCapable of engaging with stakeholders up to the CXO level, simplifying technical concepts for non-technical audiences, and fostering collaboration across teams.\nPreferred CertificationsServiceNow Certified System Administrator (CSA), ServiceNow Certified Implementation Specialist (CIS), ServiceNow Certified Application Developer (CAD), Customer Service Management (CSM) / CRM, Human Resources Service Delivery (HRSD).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['presentation skills', 'servicenow', 'itsm', 'itom', 'csm', 'performance analytics', 'cmdb', 'hrsd', 'business development', 'data migration', 'knowledge management', 'problem management', 'javascript', 'system integration', 'incident management', 'html', 'servicenow development', 'itil']",2025-06-12 14:31:51
Informatica SAP Expert,Randomtrees,3 - 6 years,Not Disclosed,['Hyderabad'],"The Data Steward will play a critical role in ensuring data integrity, quality, and governance within SAP systems.\nThe responsibilities include:\nData Governance:\no Define ownership and accountability for critical data assets to ensure they are effectively managed and maintain integrity throughout systems.\no Collaborate with business and IT teams to enforce data governance policies, ensuring alignment with enterprise data standards.\nData Quality Management:\no Promote data accuracy and adherence to defined data management and governance practices.\no Identify and resolve data discrepancies to enhance operational efficiency.\nData Integration and Maintenance:\no Manage and maintain master data quality for Finance and Material domains within the SAP system.\no Support SAP data migrations, validations, and audits to ensure seamless data integration.\nCompliance and Reporting:\no Ensure compliance with regulatory and company data standards.\no Develop and distribute recommendations and supporting documentation for new or proposed data standards, business rules, and policies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'SAP data migrations', 'Data Maintenance', 'Data Quality Management', 'Data Integration']",2025-06-12 14:31:53
SQL Server & Oracle Developer,Mobile Programming,3 - 8 years,Not Disclosed,['Mumbai'],"We are seeking a skilled SQL Server Oracle Developer to join our team.\nThe ideal candidate will have strong expertise in working with SQL Server and Oracle databases, capable of managing and optimizing complex database systems.\nYou will be responsible for writing efficient queries, designing and maintaining database structures, and collaborating with cross-functional teams to ensure seamless integration of database solutions.\nKey Responsibilities:\nDevelop, manage, and optimize SQL queries and stored procedures for SQL Server and Oracle databases.\nDesign, implement, and maintain database systems ensuring high performance, security, and scalability.\nCollaborate with developers to integrate backend services with databases.\nTroubleshoot and resolve performance bottlenecks and database-related issues.\nPerform database backup and recovery, ensuring data integrity and availability.Assist in database migrations and version upgrades.\nProvide support for database-related queries and performance tuning.Ensure compliance with data security and privacy policies.\nRequired Skills:3+ years of experience in SQL Server and Oracle database management.\nStrong proficiency in T-SQL and PL/SQL for writing queries, stored procedures, and functions.In-depth understanding of database design, normalization, and optimization techniques.Experience with performance tuning, query optimization, and database indexing.\nFamiliarity with data migration and database backup/recovery processes.Ability to troubleshoot and resolve complex database issues.\nKnowledge of database security practices and ensuring compliance.\nStrong communication and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'T-SQL', 'PL/SQL', 'Oracle database management', 'Oracle databases']",2025-06-12 14:31:55
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Nashik'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:31:58
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Surat'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:00
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Jaipur'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n- Configure and refine related workflows within NetSuite\n- Provide post-implementation support and troubleshooting\n- Understand and manage system configuration, data flows, and integration points\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\nQualifications :\n- Strong functional and technical knowledge of NetSuite\n- Experience with ERP data migration, especially from QuickBooks\n- Familiarity with third-party integration tools and NetSuites workflow engine\n- Ability to support post-go-live activities and optimize ERP performance\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:02
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Visakhapatnam'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:04
Specialist Salesforce Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role the Salesforce Specialist IS Engineer for the MyAmgen Product Team will help in driving high-quality, efficient delivery for the companys intranet. This role ensures the development team adheres to best practices and definitions of done while proactively identifying technical debt and collaborating with the Architect and Product Owner to prioritize its resolution. The engineer has expertise in the Salesforce development stack and data model and leverages AI and automation to enhance quality and speed. They take a hands-on approach to innovation, working on proofs of concept (PoCs) to validate the technical feasibility of backlog items. Additionally, the role involves designing and breaking down technical work to minimize dependencies and improve team flow during sprints. The Salesforce Development Lead also supports testing and validation to ensure reliable and impactful deliverables.\nRoles & Responsibilities:\nDeliver high-quality Salesforce solutions using LWC, Apex, Flows and other Salesforce technologies.\nEnsure alignment to established best practices and definitions of done, maintaining high-quality standards in deliverables.\nTake architectural design and translate to code deliverables\nCreate user stories that effectively describe business and technical needs\nProactively identify technical debt and collaborate with the Architect and Product Owner to prioritize and address it effectively.\nInnovate and improve development workflows by leveraging AI and automation tools to increase efficiency, speed, and quality.\nDesign and decompose technical tasks to minimize interdependencies and optimize the team's workflow during sprints.\nSupport the testing and validation of deliverables to ensure reliability, performance, and alignment with business goals.\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The Salesforce professional we seek is a type of person with these qualifications.\nBasic Qualifications:\nMasters degree and 4 to 6 years of experience in Computer Science, IT or related field OR\nBachelors degree and 6 to 8 years of experience in Computer Science, IT or related field OR\nDiploma and 10 to 12 years of Computer Science, IT or related field experience\nFunctional Skills:\nMust-Have Skills:\nExperience with Apex, JavaScript, and Lightning Web Components (LWC)to create scalable applications\nExperienced in using CI/CD automation to deploy Salesforce code and configurations\nStrong understanding of declarative tools like Flows and Process Builder\nProficiency in using Salesforce tools such as SOQL, SOSL, and Data Loader to query, manipulate and export data\nGood ability to lead development teams and collaborate with Product Owners and Architects for task prioritization and execution\nAbility to train and guide junior developers in best practices\nHands-on experience in testing, debugging, and validating deliverables for reliability and performance\nFamiliarity with Agile practices such as User Story Creation and, sprint planning\n\nGood-to-Have Skills:\nExperience with using Copado for CI/CD automations\nExperience using AI, automation, or cutting-edge Salesforce tools.\nProficiency in data migration, modeling, and security configurations.\nExperience in fine-tuning Salesforce org for scalability and speed.\nFamiliarity with industry-specific Salesforce tools like Health Cloud or Financial Services Cloud.\nAbility to identify and resolve technical debt while designing scalable, maintainable solutions\nExperience creating proofs of concept (PoCs) to validate new ideas or backlog items.\n\nProfessional Certifications (preferred):\nSalesforce Advanced Administrator\nSalesforce Developer I and II\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong written and verbal communications skills (English) in translating technology content into business-language at various levels\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong time and task management skills to estimate and successfully meet project timeline with ability to bring consistency and quality assurance across various projects.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Salesforce', 'SOQL', 'Lightning Web Components', 'SOSL', 'JavaScript', 'CI/CD', 'debugging', 'Apex']",2025-06-12 14:32:07
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Delhi / NCR'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:09
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Indore'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:11
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Ahmedabad'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:13
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Kolkata'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:15
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Pune'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'System Implementation', 'QuickBooks', 'NetSuite Implementation']",2025-06-12 14:32:17
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Hyderabad'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n- Configure and refine related workflows within NetSuite\n- Provide post-implementation support and troubleshooting\n- Understand and manage system configuration, data flows, and integration points\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\nQualifications :\n- Strong functional and technical knowledge of NetSuite\n- Experience with ERP data migration, especially from QuickBooks\n- Familiarity with third-party integration tools and NetSuites workflow engine\n- Ability to support post-go-live activities and optimize ERP performance\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:20
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Chennai'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n- Configure and refine related workflows within NetSuite\n- Provide post-implementation support and troubleshooting\n- Understand and manage system configuration, data flows, and integration points\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\nQualifications :\n- Strong functional and technical knowledge of NetSuite\n- Experience with ERP data migration, especially from QuickBooks\n- Familiarity with third-party integration tools and NetSuites workflow engine\n- Ability to support post-go-live activities and optimize ERP performance\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:22
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Thane'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:24
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Bhopal'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:26
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Mumbai'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:28
Transportation Master Team Lead,Amgen Inc,7 - 8 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will take ownership of Transportation Master data processes, ensuring the accuracy, consistency, and governance of critical data across the organization. This role will lead data validation, cleansing, and enrichment efforts, and collaborate with cross-functional teams to resolve complex data issues and drive process improvements. The Transportation Master Team Lead will also oversee key performance metrics, ensure compliance with data governance standards, and lead data migration and integration initiatives.\nRoles & Responsibilities:\nLead and manage day-to-day MDM operations, including data validation, cleansing, and enrichment processes\nOversee data governance practices, ensuring compliance with internal standards and regulatory requirements\nCollaborate with cross-functional teams, including IT and business units, to resolve complex data issues and improve data workflows.\nImplement and drive continuous improvements in MDM processes to enhance data accuracy, quality, and operational efficiency.\nLead data migration, integration projects, and system upgrades to ensure seamless data consistency across platforms.\nMonitor and report on key performance indicators related to master data quality and operational success.\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nBachelors in a STEM discipline and 7-8 years of experience in enterprise applications like SAP, and Oracle, proven experience in Transportation Master data management and data governance\nIndustry experience preferably in healthcare or biotech supply chain\nProficient in MS Office, visualization tools like Spotfire, Tableau, Power BI\nStrong analytical skills with the ability to collaborate cross-functionally and resolve data issues.\nExperience in leading and managing successful teams\nPreferred Qualifications:\nMust-Have Skills:\nExpertise in master data management processes and data governance\nSolid understanding of SAP ECC and proven experience in data implementation/integration projects\nMaster data knowledge in the Transportation domain, other domains such as Material, Customer, and Production Master are a plus.\nAbility to lead and collaborate with cross-functional teams to set data strategy for a master domain(s) and drive prioritization across different projects and day-to-day operations\nStrong understanding of data governance frameworks and regulatory compliance standards and regulations (e.g., GDPR, HIPAA, GxP).\nExcellent problem-solving and analytical skills, with a focus on driving continuous improvement in data accuracy and quality\nGood-to-Have Skills:\nSAP S/4, SAP MDG, SAP TM\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills.\nAbility to work effectively with global, virtual teams.\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data management processes', 'SAP ECC', 'Transportation Master data management', 'SAP MDG', 'HIPAA', 'data governance', 'SAP TM', 'SAP S/4', 'GDPR', 'GxP', 'STEM']",2025-06-12 14:32:30
SAP WM Consultant - E2E Implementation,Zettamine Labs,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Summary :\nWe are looking for a highly skilled and experienced SAP WM (Warehouse Management) Consultant to join our growing team.\nThe ideal candidate will have a strong background in SAP WM module implementations, support, and integrations, along with excellent problem-solving and communication skills.\nExperience with EWM is a plus.\nKey Responsibilities :\n- Gather business requirements and translate them into SAP WM solutions.\n- Configure and customize SAP WM modules to meet business needs.\n- Lead and support SAP WM module implementation, rollout, and support projects.\n- Collaborate with cross-functional teams including MM, SD, PP, and EWM.\n- Develop functional specifications for custom developments.\n- Perform system testing, integration testing, and support UAT.\n- Provide training and documentation for end-users and support teams.\n- Resolve incidents and provide ongoing maintenance and support.\n\nRequired Skills :\n- Strong expertise in SAP WM configuration and end-to-end implementation.\n- Hands-on experience with Inventory Management (IM), Inbound/Outbound Logistics, Putaway, Picking, and Stock Transfer Processes.\n- Integration experience with SAP MM, SD, and EWM.\n- Experience with RF devices, barcode scanning, and warehouse automation is an advantage.\n- Good understanding of IDoc handling, interfaces, and data migration.\n- Ability to analyze business processes and map them into SAP WM functionality.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP', 'Technical Consultant', 'SAP SD', 'SAP MM', 'SAP PP', 'SAP Integration', 'SAP EWM', 'SAP Implementation', 'IDoc', 'SAP Support', 'SAP WM']",2025-06-12 14:32:33
Power BI Developer,Luxoft,3 - 5 years,Not Disclosed,['Pune'],"Design, develop and maintain/support Power BI workflows to take data from multiple sources to make it ready for analytics and reporting.\nOptimize existing workflows to ensure performance, scalability and reliability.\nSupport the automation of manual processes to improve operational efficiency.\nDocument workflows, processes, and best practices for knowledge sharing.\nProvide training and mentorship to other team members on Alteryx development.\nCollaborate with other members of the team to deliver data solutions for the program.\nSkills\nMust have\nProficiency in Power BI Desktop, Power BI Service (5+ yrs of experience)\nExperience with creating interactive dashboards, custom visuals, and reports.\nData Modeling:\nStrong understanding of data modeling concepts, including relationships, calculated columns, measures, and hierarchies.\nExpertise in using DAX (Data Analysis Expressions) for complex calculations.\nSQL and Database Management:\nProficiency in SQL to extract, manipulate, and analyze data from databases.\nKnowledge of database design and querying.\nETL (Extract, Transform, Load) Tools:\nExperience with data transformation and cleaning using tools like Power Query, SSIS, or other ETL tools.\nNice to have\nData Architecture & Engineering: Design and implement efficient and scalable data warehousing solutions using Azure Databricks and Microsoft Fabric.\nBusiness Intelligence & Data Visualization: Create insightful Power BI dashboards to help drive business decisions.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nAlpharetta\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nRemote United States\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nPune, India\nReq. VR-114885\nData Science\nBCM Industry\n05/06/2025\nReq. VR-114885\nApply for Power BI Developer in Pune\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data analysis', 'Data modeling', 'Database design', 'SSIS', 'Business intelligence', 'microsoft', 'Analytics', 'SQL', 'Data architecture']",2025-06-12 14:32:35
Actuarial Consultant,"Top B2B Corporate, Management Consulting...",4 - 9 years,22.5-30 Lacs P.A.,['Pune'],"Summary:\nResponsible for providing actuarial support across Life, Health, and Retirement domains, including product development, pricing, valuation, financial reporting, and risk management.\n\nKey Responsibilities:\nSupport actuarial functions: pricing, projection, valuation, illustrations, and reinsurance.\nConduct data migration, validation, testing, and documentation.\nAssist in model conversion, regulatory modeling, and system improvements.\nEnhance valuation efficiency and reporting processes.\nAutomate tools and reports; analyze data for business insights.\nSimplify complex actuarial data for cross-functional teams.\nManage multiple projects with a focus on quality and timeliness.\n\nQualifications & Skills:\nBachelor's/Masters in Mathematics, Statistics, Actuarial Science, or related.\nPursuing actuarial exams (minimum 4 passes, CM1 required; CM2 preferred).\nYears of actuarial experience in Life, Health, or Retirement.\nProficient in valuation, modeling, reporting, and pricing.\nSkilled in actuarial software (Prophet, AXIS, MG ALFA, etc.).\nAdvanced Excel/VBA; exposure to R, SAS, or Python.\nStrong analytical and communication skills.",Industry Type: IT Services & Consulting,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Actuarial Reserving Methodologies', 'Health', 'Life', 'Retirement', 'Retirement Planning', 'Valuation']",2025-06-12 14:32:37
Database Administrator(6 months Contract),T2 Innovations,10 - 15 years,Not Disclosed,['Chennai'],"Job Summary:\nWe are looking for an experienced Database Administrator (DBA) with strong expertise in migrating databases from Oracle to MySQL. The role involves hands-on work in schema conversion, data migration, performance tuning, scripting, and post-migration support.\n\nKey Responsibilities:\nLead end-to-end Oracle to MySQL migration\nPerform schema conversion, data export/import, and validation\nUse tools like MySQL Workbench or Oracle SQL Developer\nAutomate tasks using Python, Perl, or Bash\nOptimize queries, indexing, and ensure performance\nMaintain data integrity, security, and compliance\nProvide post-migration monitoring and support\nKey Skills:\nOracle to MySQL Migration\nMySQL Administration\nSchema & Data Conversion\nQuery Optimization & Performance Tuning\nScripting (Python/Perl/Bash)\nPreferred:\nExperience with large datasets and automation tools\nFamiliarity with cloud databases (AWS RDS, GCP SQL)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Oracle to MySQL Migration', 'MySQL Administration', 'Schema & Data Conversion', 'perl', 'Bash', 'Performance Tuning', 'Scripting', 'Python']",2025-06-12 14:32:39
Python Technical Lead Developer - Only Male candidates,AM Infoweb,2 - 7 years,Not Disclosed,['Pune( Kalyani Nagar )'],"Job Title: Technical Lead Python Developer\nLocation: Kalyani Nagar, Pune\nShift Timing: 3:00 PM to 12:00 AM IST\nAbout the Role:\nWe are looking for a highly skilled and passionate Python Technical Lead to join our growing team at AM Infoweb. This role is purely backend and hands-on coding focused, and ideal for someone who is not only technically strong but also has the capability to design, architect, and lead backend projects.\n\nAs a Technical Lead, you'll take ownership of the backend architecture, manage complex data-driven systems, and guide the team through technical challenges using best-in-class tools and practices.\n\nKey Responsibilities:\nLead backend development efforts using Python and associated frameworks.\nDesign, develop, and maintain scalable backend architecture for web and AI-based applications.\nArchitect solutions around given datasets and business logic.\nGuide junior developers, conduct code reviews, and ensure best coding practices.\nCollaborate with cross-functional teams including AI engineers, frontend developers, DevOps, and product owners.\nWork with CI/CD pipelines to ensure rapid and stable deployment cycles.\nIntegrate and manage databases such as MySQL, PostgreSQL, and MongoDB.\nDeploy, monitor, and maintain services on AWS and Azure cloud platforms.\n\nTechnical Requirements:\nStrong experience with Python backend frameworks Django, Flask, FastAPI (at least two).\nExperience with relational and non-relational databases MySQL, PostgreSQL, MongoDB.\nSolid understanding of API development, RESTful services, and system architecture.\nExperience working with cloud platforms AWS, Azure.\nFamiliarity with CI/CD tools and DevOps practices.\nStrong problem-solving and communication skills.\n\nPreferred Qualifications:\n6+ years of experience in backend development.\n12 years in a technical leadership role.\nExposure to AI and machine learning integrations is a plus.\nExperience with microservices and containerization (Docker, Kubernetes) is an advantage.\n\nWhat You Get:\nInternational exposure to global projects and clients.\nWork with trending AI tools and intelligent bots.\nOnsite cafeteria and break facilities.\nFun and engaging team events and celebrations.\n\n* Know your organization - https://www.aminfoweb.in/\n* Know your workspace! - https://www.youtube.com/watch?v=T1UKFelepCk\n* Our Annual RNR'2022 - https://www.youtube.com/watch?v=T1UKFelepCk\n* Exploring the myths surrounding outsourced healthcare management - https://www.youtube.com/watch?v=fwf3jFa2T-A",Industry Type: Analytics / KPO / Research,Department: Product Management,"Employment Type: Full Time, Permanent","['Python', 'Data Structures And Algorithms', 'Lead Architect', 'Django', 'Postgresql', 'MySQL', 'FastAPI', 'MongoDB', 'Data Architecture', 'AWS', 'Flask', 'Backend Development']",2025-06-12 14:32:42
"Manager, Software Development",SAS Institute of Management,12 - 17 years,Not Disclosed,['Pune'],"The role is based in the Pune R&D Center, at SAS R&D Pune facility. We are looking for a\nSoftware Manager\nto development product within the Customer Intelligence line of business. You will be leading the development activities, optimizing allocation for team member, lead the team technically, involve all the stakeholders appropriately, and maintain project standards and best practices and deliver outcomes as planned.\nEssential Experience:\nAt least 12 years of working experience in software development\nAt least 2 years of management/leading experience of software development teams\nExperience of architecting technically sophisticated systems.\nStrong experience in Agile software development cycle\nExperience in enforcing best technical practices in software development\nExcellent written and oral communication skills.\nUnderstanding of manual and automation testing processes and architecture\nUnderstanding of full stack of technology\nExcellent knowledge of enterprise design patterns\nGood understanding of data modelling best practices\nExperience in leading/managing teams with energy and optimism\nAbility to motivate team members to function collaboratively\nGood in multi-tasking\nExperience of these items would be useful:\nCustomer Intelligence domain preferred\nExperience with Docker, Kubernetes\nManagement in development on any public cloud providers like Amazon Web Services, Microsoft Azure, and Google Cloud, etc.\nManaging Continuous Integration and Continuous Delivery (CI, CD) model\nWe are a friendly team, and we ll be offering you plenty of opportunities to develop your career. Interested? Then please get in touch to find out more!\nPrimary Responsibilities\nOrganizes, develops, prioritizes, and assigns resources to deliver high quality, testable and scalable software solutions within established timelines, while adhering to R&D best practices and processes.\nLeads development project designs and enforces technical standards to ensure solutions execute correctly across various supported environments (i.e. browsers, devices, operating systems).\nProactively leads and solicits the involvement of other project stakeholders (e.g. managers, developers, user interface and visual designers, product managers) to ensure implementation satisfies functional requirements and is consistent with established R&D standards.\nManages and leads project scoping and scheduling; tracks progress of individual tasks and alerts executive management and stakeholders of concerns meeting schedules, while following established R&D standards.\nManages product quality standards by ensuring functional, unit and performance testing is comprehensive and thorough; works closely with development and testing teams to verify test plans. Organizes, prioritizes, and assigns resources to implement and resolve code changes related to enhancements, redesigns and/or bug fixes.\nMaintains accountability for the entire life cycle of the code including support for both internal and external consumers.\nEnsures the veracity of design and technical documentation to satisfy both internal and external consumers.\nContributes to product or project direction through collaboration with Product Management, Marketing, Sales, Customers, and others within SAS.\nProvides technical leadership as appropriate for projects and to the team through mentoring, training, and managing the activities of the team.\nManages all aspects of the department including teamwork, performance management, feedback, professional growth through collaboration with SAS human resources, SAS education and executive leadership.",Industry Type: Aviation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Product quality', 'SAS', 'Performance management', 'Enterprise applications', 'Performance testing', 'Agile', 'Scheduling', 'Management', 'Technical documentation']",2025-06-12 14:32:45
AWS Admin,Hakkda,10 - 15 years,Not Disclosed,['Jaipur'],"ABOUT HAKKODA\n\nHakkoda, an IBM Company, is a modern data consultancy that empowers data driven organizations to realize the full value of the Snowflake Data Cloud. We provide consulting and managed services in data architecture, data engineering, analytics and data science. We are renowned for bringing our clients deep expertise, being easy to work with, and being an amazing place to work! We are looking for curious and creative individuals who want to be part of a fast-paced, dynamic environment, where everyone s input and efforts are valued. We hire outstanding individuals and give them the opportunity to thrive in a collaborative atmosphere that values learning, growth, and hard work. Our team is distributed across North America, Latin America, India and Europe. If you have the desire to be a part of an exciting, challenging, and rapidly-growing Snowflake consulting services company, and if you are passionate about making a difference in this world, we would love to talk to you!.\n\nWe are seeking a highly skilled and experienced AWS Administrator to join a long-term project (12+ months), fully allocated and 100% hands-on. This role will backfill a senior AWS Admin with 10-15 years of experience and requires deep technical capability across AWS infrastructure service. This is not a team leadership role the ideal candidate will operate independently, take full ownership of AWS administration tasks, and contribute directly to maintaining and optimizing cloud operations.\nRole & Responsibilities\nAWS Infrastructure Management: Provision, configure, and maintain AWS services such as EC2, S3, IAM, VPC, Lambda, RDS, CloudWatch, CloudTrail, and more.\nMonitoring & Incident Response: Set up monitoring, logging, and alerting solutions. Respond to and resolve infrastructure issues proactively.\nSecurity & IAM: Manage IAM roles, policies, and user access with a strong focus on security best practices and compliance requirements.\nAutomation & Scripting: Automate routine tasks using scripting (Bash, Python) and AWS CLI/SDK.\nInfrastructure as Code (IaC): Use tools like Terraform or CloudFormation to manage and automate infrastructure deployments and changes.\nCost Optimization: Monitor resource usage and implement cost-control strategies to optimize AWS spending.\nBackup & Disaster Recovery: Manage backup strategies and ensure systems are resilient and recoverable.\nDocumentation: Maintain detailed and up-to-date documentation of AWS environments, standard operating procedures, and runbooks.\nSkils & Qualifications\n10+ years of hands-on AWS administration experience.\nStrong understanding of AWS core services (EC2, S3, IAM, VPC, Lambda, RDS, etc.).\nExperience with scripting (Python, Bash, or PowerShell) and automation tooling.\nProven expertise in using Terraform or CloudFormation .\nDeep knowledge of IAM policy creation and security best practices.\nExperience with monitoring tools such as CloudWatch, Prometheus, or third-party APM tools.\nFamiliarity with CI/CD pipelines and DevOps principles.\nStrong troubleshooting skills with the ability to resolve complex infrastructure issues independently.\nExcellent communication skills with the ability to work effectively with remote teams.\nComfortable working during US Eastern Time zone hours.\nPreferred Qualifications:\nAWS Certifications (e.g., SysOps Administrator Associate , Solutions Architect Associate/Professional ).\nExperience in hybrid environments or with other cloud platforms (Azure, GCP).\nFamiliarity with Snowflake, GitLab, or similar DevOps tooling.\nBenefits:\n\n- Health Insurance\n- Paid leave\n- Technical training and certifications\n- Robust learning and development opportunities\n- Incentive\n- Toastmasters\n- Food Program\n- Fitness Program\n- Referral Bonus Program\n\nHakkoda is committed to fostering diversity, equity, and inclusion within our teams. A diverse workforce enhances our ability to serve clients and enriches our culture. We encourage candidates of all races, genders, sexual orientations, abilities, and experiences to apply, creating a workplace where everyone can succeed and thrive.\n\nReady to take your career to the next level? Apply today and join a team that s shaping the future!!\n\nHakkoda is an IBM subsidiary which has been acquired by IBM and will be integrated in the IBM organization. Hakkoda will be the hiring entity. By Proceeding with this application, you understand that Hakkoda will share your personal information with other IBM subsidiaries involved in your recruitment process, wherever these are located. More information on how IBM protects your personal information, including the safeguards in case of cross-border data transfer, are available here.",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Managed services', 'Infrastructure management', 'Consulting', 'Disaster recovery', 'SDK', 'Troubleshooting', 'Analytics', 'Python', 'Data architecture']",2025-06-12 14:32:47
Databricks Developer,Luxoft,5 - 10 years,Not Disclosed,['Pune'],"Design, build, and manage data pipelines using Azure Data Integration Services (Azure DataBricks, ADF, Azure Functions.)\nCollaborate closely with the security team to develop robust data solutions that support our security initiatives.\nImplement, monitor, and optimize data processes, ensuring adherence to security and data governance best practices.\nTroubleshoot and resolve data-related issues, ensuring data quality and accessibility.\nDevelop strategies for data acquisitions and integration of the new data into our existing architecture.\nDocument procedures and workflows associated with data pipelines, contributing to best practices.\nShare knowledge about latest Azure Data Integration Services trends and techniques.\nImplement and manage CI/CD pipelines to automate data and UI testcases and integrate testing with development pipelines.\nImplement and manage CI/CD pipelines to automate development and integrate test pipelines.\nConduct regular reviews of the system, identify possible security risks, and implement preventive measures.\nSkills\nMust have\nExcellent command of English\nBachelors or Masters degree in Computer Science, Information Technology, or related field.\n5+ years of experience in data integration and pipeline development using Azure Data Integration Services including Azure Data Factory and Azure Databricks.\nHands-on with Python and Spark\nStrong understanding of security principles in the context of data integration.\nProven experience with SQL and other data query languages.\nAbility to write, debug, and optimize data transformations and datasets.\nExtensive experience in designing and implementing ETL solutions using Azure Databricks, Azure Data Factory or similar technologies.\nFamiliar with automated testing frameworks using Squash\nNice to have\nData Architecture & Engineering: Design and implement efficient and scalable data warehousing solutions using Azure Databricks and Microsoft Fabric.\nBusiness Intelligence & Data Visualization: Create insightful Power BI dashboards to help drive business decisions.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nAlpharetta\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nRemote United States\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nPune, India\nReq. VR-114884\nData Science\nBCM Industry\n05/06/2025\nReq. VR-114884\nApply for Databricks Developer in Pune\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'data governance', 'Data quality', 'Engineering Design', 'Business intelligence', 'microsoft', 'Information technology', 'SQL', 'Python', 'Data architecture']",2025-06-12 14:32:49
"Manager, Software Development",SAS Research and Developement (India) Pvt Ltd,12 - 17 years,Not Disclosed,['Pune'],"Nice to meet you!\nWe re a leader in data and AI. Through our software and services, we inspire customers around the world to transform data into intelligence - and questions into answers.\nWe re also a debt-free multi-billion-dollar organization on our path to IPO-readiness. If youre looking for a dynamic, fulfilling career coupled with flexibility and world-class employee experience, youll find it here.\nAbout the role:\nThe role is based in the Pune R&D Center, at SAS R&D Pune facility. We are looking for a\nSoftware Manager\nto development product within the Customer Intelligence line of business. You will be leading the development activities, optimizing allocation for team member, lead the team technically, involve all the stakeholders appropriately, and maintain project standards and best practices and deliver outcomes as planned.\nEssential Experience:\nAt least 12 years of working experience in software development\nAt least 2 years of management/leading experience of software development teams\nExperience of architecting technically sophisticated systems.\nStrong experience in Agile software development cycle\nExperience in enforcing best technical practices in software development\nExcellent written and oral communication skills.\nUnderstanding of manual and automation testing processes and architecture\nUnderstanding of full stack of technology\nExcellent knowledge of enterprise design patterns\nGood understanding of data modelling best practices\nExperience in leading/managing teams with energy and optimism\nAbility to motivate team members to function collaboratively\nGood in multi-tasking\nExperience of these items would be useful:\nCustomer Intelligence domain preferred\nExperience with Docker, Kubernetes\nManagement in development on any public cloud providers like Amazon Web Services, Microsoft Azure, and Google Cloud, etc.\nManaging Continuous Integration and Continuous Delivery (CI, CD) model\nWe are a friendly team, and we ll be offering you plenty of opportunities to develop your career. Interested? Then please get in touch to find out more!\nPrimary Responsibilities\nOrganizes, develops, prioritizes, and assigns resources to deliver high quality, testable and scalable software solutions within established timelines, while adhering to R&D best practices and processes.\nLeads development project designs and enforces technical standards to ensure solutions execute correctly across various supported environments (i.e. browsers, devices, operating systems).\nProactively leads and solicits the involvement of other project stakeholders (e.g. managers, developers, user interface and visual designers, product managers) to ensure implementation satisfies functional requirements and is consistent with established R&D standards.\nManages and leads project scoping and scheduling; tracks progress of individual tasks and alerts executive management and stakeholders of concerns meeting schedules, while following established R&D standards.\nManages product quality standards by ensuring functional, unit and performance testing is comprehensive and thorough; works closely with development and testing teams to verify test plans. Organizes, prioritizes, and assigns resources to implement and resolve code changes related to enhancements, redesigns and/or bug fixes.\nMaintains accountability for the entire life cycle of the code including support for both internal and external consumers.\nEnsures the veracity of design and technical documentation to satisfy both internal and external consumers.\nContributes to product or project direction through collaboration with Product Management, Marketing, Sales, Customers, and others within SAS.\nProvides technical leadership as appropriate for projects and to the team through mentoring, training, and managing the activities of the team.\nManages all aspects of the department including teamwork, performance management, feedback, professional growth through collaboration with SAS human resources, SAS education and executive leadership.\nCANDIDATE PROFILE\nBackground and Experience\nSAS is seeking a Software Manager with overall experience of 12+ years, with minimum 2 years management/leading experience. The candidate should have development and management experience in Saas enterprise applications. The candidate should have excellent organizing and prioritizing skills, and sharp technical skills to choose the right direction technically. Candidate should have proven track record of delivery management and people management. Candidate should also have excellent verbal and written communication skills and possess the confidence to pull the work in the right direction, keeping all stake holders involved and engaged.\n#highlightedjob",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Product quality', 'SAS', 'Performance management', 'Enterprise applications', 'Performance testing', 'Agile', 'Scheduling', 'Management', 'Technical documentation']",2025-06-12 14:32:52
Cyber Intel Fusion Analyst,Outreach,2 - 4 years,Not Disclosed,['Hyderabad'],"Outreach is the first and only AI Sales Execution Platform built for intelligent revenue workflows. Built on the world s largest foundation of customer interactions and go-to-market team data, Outreach s leading revenue AI technology helps go-to-market professionals and their companies win by intelligently accelerating decision making and elevating sellers to do their best work. Our powerful platform gives revenue teams the tools they need to design, measure, and improve a revenue strategy for every stage of the customer journey, improving efficiency and effectiveness across the entire revenue cycle. Over 6, 000 customers, including Zoom, McKesson, Snowflake, SAP, and Okta use Outreach to power workflows, put customers at the center of their business, improve revenue results, and win in the market.\nOutreach is a privately held company based in Seattle, Washington, with offices worldwide. To learn more, please visit www. outreach. io .\n\nThe Role\n\nBridging Intelligence and Action\nThe Cyber Intel Fusion Analyst is a pivotal role within our security program.\n\nThis position serves as a critical bridge, linking strategic threat intelligence with tactical security operations.\nThe analyst will be instrumental in evolving our security practices beyond traditional, siloed functions while ensuring that intelligence capabilities are not merely insightful but are directly integrated and operationalized within our security framework.\nThis proactive operationalization of intelligence is key to anticipating emerging threats and developing innovative countermeasures to counter sophisticated cyber threats before they can impact our services or compromise sensitive information.\nThe ability to quickly fuse intelligence into operational defense mechanisms provides a distinct security advantage, crucial for maintaining service reliability and customer trust.\nYour Daily Adventures Will Include\nCore Responsibilities: Shaping Our Defenses\nThe responsibilities of the Cyber Intel Fusion Analyst are multifaceted, demanding a blend of analytical acumen, technical expertise, and collaborative skill.\nIntelligence Cycle Management Requirements Definition: The analyst will manage the intelligence analysis cycle as it pertains to team operations. This includes working closely with team operators and other stakeholders to identify and refine intelligence requirements that drive threat emulation assessments and inform defensive strategies. A key function involves identifying intelligence requirements for diverse areas such as security operations, cloud security, enterprise security, and application security, including those related to artificial intelligence. This broad scope necessitates an understanding of the unique intelligence needs of various teams, positioning the analyst as a strategic partner who can tailor and deliver relevant intelligence to enhance the effectiveness of multiple security functions.\nTactical Intelligence Analysis Adversary Understanding: A core function is providing tactical cyber intelligence analysis, meticulously identifying specific adversary tactics, techniques, and procedures (TTPs). This analysis will be consistently tied back to established frameworks like the MITRE ATTCK Framework, leveraging intelligence provided by relevant organizations. The role involves recognizing and researching attacks and attack patterns based onpublished open-source intelligence (OSINT) and other intelligence sources. The analyst will be adept at handling and organizing disparate data concerning detections, attacks, and attackers to accurately identify adversary groups and their modus operandi, thereby driving assessments pertinent to the company. This process transforms general threat data into a refined understanding of adversaries specifically targeting our environment, such as those focusing on SaaS platforms if applicable.\nDeveloping Actionable Intelligence Driving Threat Emulation: The analyst is tasked with developing, producing, and managing Adversary Response Playbooks. These playbooks are crucial for supporting and driving threat emulation assessments, ensuring our defenses are tested against realistic adversary behaviors. 1 This involves translating analyzed intelligence on adversary TTPs and campaign indicators into actionable detection strategies, such as developing custom SIEM correlation rules or contributing to Security Orchestration, Automation, and Response (SOAR) playbooks. This operationalization of intelligence is fundamental, turning analytical findings into tangible, proactive defensive measures that strengthen our security posture.\nCollaboration, Liaison Stakeholder Management: Effective relationship management is paramount. The analyst will manage relationships with organizations, both internal and external, that provide requested intelligence to the team or receive information from it. A significant part of the role includes representing the team in cyber threat intelligence-related meetings and matters, acting as a crucial liaison. This collaboration extends across multiple organizational functions, potentially including cloud engineering teams, DevSecOps personnel, SOC analysts, incident responders, and even executive leadership. By effectively sharing tailored intelligence, the analyst acts as a force multiplier, enhancing the capabilities and preparedness of various teams across the organization.\nOur Vision of You\nCore Competencies: Mastery of the Intelligence Cycle: Expertise in managing the intelligence analysis cycle, encompassing planning, collection (including OSINT and multi-source intelligence), processing, in-depth analysis of adversary TTPs, and the production and dissemination of timely, accurate, and actionable intelligence products tailored to diverse internal audiences.\nStrategic Requirements Identification: Proven ability to identify and refine intelligence requirements for a wide array of security functions, includingsecurity operations, cloud security, enterprise security, and application security (potentially including AI), ensuring intelligence efforts align with business and operational needs.\nTactical Intelligence TTP Expertise: Strong skills in tactical cyber intelligence analysis, identifying specific adversary TTPs and mapping them to frameworks like MITRE ATTCK . This includes researching current attacks, attack patterns, and understanding threats specific to modern environments (e. g. , SaaS-specific attack patterns).\nActionable Output Development: Demonstrable experience in developing, producing, and managing resources like Adversary Response Playbooks to support and drive threat emulation assessments, effectively translating intelligence into practical defensive measures.\nData Synthesis Adversary Profiling: Capability in handling and organizing disparate data about detections, attacks, and attackers to properly identify adversary groups and develop comprehensive threat actor profiles, particularly those relevant to the company s operational landscape.\nExceptional Collaboration Liaison Skills: Excellent relationship management abilities with internal and external intelligence providers and consumers, and proven experience acting as an effective liaison and team representative in intelligence matters.\nEducation and Experience: A minimum of 5 years of progressive, hands-on experience in the cybersecurity domain, with a demonstrable track record in roles that combine cyber threat intelligence analysis with security operations or incident response functions. Experience in environments with a significant cloud and SaaS focus is highly advantageous. This emphasis on combined experience highlights the need for individuals who have practically applied the ""fusion""concept.\nTechnical Prowess: The analyst must possess a robust set of technical skills to effectively investigate security incidents, analyze threat data, and implement defensive measures, especially within cloud environments.\n\nEssential technical competencies are outlined below:\n\nAn in-depth understanding of core networking protocols (TCP/IP, UDP, HTTP/S, DNS, SMTP, etc. ), network traffic analysis methodologies, and the function of common networking ports and protocols.\nProficiency with cloud security architectures (IaaS, PaaS, SaaS) and hands-onexperience with security tools native to major cloud platforms (e. g. , AWS, Azure, GCP).\nExpertise with Security Information and Event Management (SIEM) platforms for log correlation, advanced analysis, and the development of custom detection rules.\nHands-on experience with Endpoint Detection and Response (EDR/XDR) solutions for endpoint threat detection, investigation, and response.\nStrong skills in comprehensive log analysis from diverse cloud and on-premises sources, including operating systems (Windows, Linux, macOS), applications, network devices, and cloud service logs (e. g. , CloudTrail, Azure Monitor).\nA solid understanding of Windows and Linux operating systems (including distributions such as RHEL, Ubuntu, CentOS) and macOS, encompassing system administration fundamentals, security configurations, logging mechanisms, and common attack vectors.\nScripting skills for automation of analytical tasks, data manipulation, tool integration, or the development of custom detection scripts using languages such as Python, PowerShell, or Bash.\nDeep understanding and practical application of threat intelligence frameworks such as the MITRE ATTCK Framework, the Cyber Kill Chain , and the Diamond Model of Intrusion Analysis.\nThe following outlines core technical competencies and representative toolsets relevant to this role:\nCategory Examples/Specific Tools (Tailored for SaaS)\nCloud Platform Security: AWS (GuardDuty, Security Hub, Macie, Inspector), Azure (Sentinel, Defender for Cloud), GCP (Security Command Center)\nSIEM: Google SecOps, CrowdStrike NG SIEM, Sumologic CloudSiem\nEDR/XDR: CrowdStrike Falcon, JAMF Protect\nNetwork Analysis: Wireshark, Zeek (formerly Bro), Suricata, Cloud-native traffic mirroring/analysis tools\nVulnerability Management: CrowdStrike Exposure Management, Wiz, Cloud-native vulnerability scanners\nScripting Languages: Python, PowerShell, Bash\nOperating Systems: Windows (Client/Server), Linux (various distributions such as RHEL, Ubuntu, CentOS), macOS\nThreat Intelligence Platforms: (TIPs) MISP, ThreatConnect, Anomali ThreatStream, Recorded Future.\nAnalytical and Communication Skills: Exceptional analytical and problem-solving skills, with a demonstrated ability to correlate disparate datasets, identify subtle patterns of malicious activity, and make sound, evidence-based judgments, often under pressure.\nExcellent written and verbal communication skills, with the proven ability to articulate complex technical information, security concepts, and intelligence findings clearly and concisely to diverse audiences, including technical peers and management.\nWork Requirements:\nThis position requires participation in an on-call rotation to provide expert support during critical security incidents. This role does not involve regular shift work.\nBonus Points: Preferred Qualifications\nWhile not mandatory, the following qualifications will significantly differentiate strong candidates and indicate a deeper specialization.\n\nAdvanced industry-recognized cybersecurity certifications. Examples include:\n\nGIAC Cyber Threat, SANS/GIAC Cyber Threat Intelligence , Intelligence (GCTI), GIAC Certified Intrusion, SANS/GIAC Network Security Monitoring, Analyst (GCIA), Intrusion Detection, GIAC Certified Incident, SANS/GIAC Incident Response, Handler (GCIH), CISSP (ISC) Broad Cybersecurity, Management Operations, AWS Certified Security - Amazon Web Services AWS Cloud Security Specialty, Azure Security Engineer, Microsoft Azure Cloud Security, Associate (AZ-500), CompTIA Cybersecurity, CompTIA Cybersecurity Analysis, Analyst (CySA+), Intrusion Detection, Offensive Security Certified, Offensive Security Penetration Testing, Professional (OSCP), (Understanding Attacker Methods)\nPractical experience utilizing Threat Intelligence Platforms (TIPs) such as MISP, ThreatConnect, Anomali ThreatStream, or Recorded Future.\nExperience with Security Orchestration, Automation, and Response (SOAR) platforms and playbook development.\nKnowledge of malware analysis (static and dynamic) and reverse engineering techniques, and familiarity with associated tools.\nFamiliarity with DevSecOps principles and experience securing CI/CD pipelines.\nUnderstanding of compliance frameworks relevant to SaaS environments (e. g. , SOC 2, ISO 27001/27701/42001, GDPR, HIPAA).\nWhy You ll Love It Here\n\nHighly competitive salary\n25 days annual vacation time + sick time and casual leave\nGroup medical policy coverage available to employees and up to 5 eligible family members\nOPD benefit covered up to INR 10, 000\nLife insurance and personal accident insurance at 3x annual CTC\n26 weeks of maternity leave pay, and 15 days of paternity leave pay\nOpportunity to be part of company success via the RSU program\nDiversity and inclusion programs that promote employee resource groups like OWN+ (Outreach Womens Network), Adelante (Latinx community), OBX (Outreach Black Connection), Mosaic (AAPI community), Pride (LGBTQIA+), Gender+, Disability Community, and Veterans/Military\nEmployee referral bonuses to encourage the addition of great new people to the team\nFun company and team outings because we play just as hard as we work\n\nOur success is reliant on building teams that include people from different backgrounds and experiences who can elevate assumptions and ideas with fresh perspectives. Were dedicated to hiring the whole human, not just a resume. To that end, we look for a diverse pool of applicants-including those from historically marginalized groups. We would like to invite you to apply even if you dont think you meet all of the requirements listed below. We dont want a few lines in a job description to get between us and the opportunity to meet you.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Automation', 'SAP', 'Linux', 'Information security', 'DNS', 'Network security', 'HTTP', 'Windows', 'Open source', 'Python']",2025-06-12 14:32:54
Starburst Engineer,Luxoft,1 - 13 years,Not Disclosed,['Pune'],"Design, develop, and maintain scalable data solutions using Starburst.\nCollaborate with cross-functional teams to integrate Starburst with existing data sources and tools.\nOptimize query performance and ensure data security and compliance.\nImplement monitoring and alerting systems for data platform health.\nStay updated with the latest developments in data engineering and analytics.\nSkills\nMust have\nBachelors degree or Masters in a related technical field; or equivalent related professional experience.\nPrior experience as a Software Engineer applying new engineering principles to improve existing systems including leading complex, well defined projects.\nStrong knowledge of Big-Data Languages including:\nSQL\nHive\nSpark/Pyspark\nPresto\nPython\nStrong knowledge of Big-Data Platforms, such as:o The Apache Hadoop ecosystemo AWS EMRo Qubole or Trino/Starburst\nGood knowledge and experience in cloud platforms such as AWS, GCP, or Azure.\nContinuous learner with the ability to apply previous experience and knowledge to quickly master new technologies.\nDemonstrates the ability to select among technology available to implement and solve for need.\nAble to understand and design moderately complex systems.\nUnderstanding of testing and monitoring tools.\nAbility to test, debug, fix issues within established SLAs.\nExperience with data visualization tools (e.g., Tableau, Power BI).\nUnderstanding of data governance and compliance standards.\nNice to have\nData Architecture & Engineering: Design and implement efficient and scalable data warehousing solutions using Azure Databricks and Microsoft Fabric.\nBusiness Intelligence & Data Visualization: Create insightful Power BI dashboards to help drive business decisions.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nAlpharetta\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nRemote United States\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nPune, India\nReq. VR-114886\nData Science\nBCM Industry\n05/06/2025\nReq. VR-114886\nApply for Starburst Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data security', 'data governance', 'Engineering Design', 'Apache', 'Business intelligence', 'microsoft', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-12 14:32:57
Principal Software Engineer (Full Stack),Insightsoftware,10 - 14 years,Not Disclosed,['Hyderabad'],"Insightsoftware is a growing, dynamic software company that helps businesses achieve greater levels of financial intelligence across their organization with our world-class financial reporting solutions. At Insightsoftware, you will learn and grow in a fast-paced, supportive environment that will take your career to the next level. We are looking for future Insighters who can demonstrate teamwork, results orientation, a growth mindset, disciplined execution, and a winning attitude to join our growing team.\nRole : Principal Software Engineer\ninsightsoftware is seeking a Principal Software Engineer to join our team. The ideal candidate will have a strong full-stack development background, with expertise in Java coding, particularly using Spring Boot for backend development. Additionally, proficiency in front-end technologies, especially ReactJS/Vue.js, is essential.\nThis critical role would require working with the development team to ensure accessibility for all users by developing a front end that functions across browsers, platforms, and devices while meeting accessibility and security requirements.\nWe provide powerful tools to our clients which empower enterprises and organizations in business planning, consolidation, and performance monitoring.\nAre you ready to lead in this new era of technology and solve Enterprise Performance Management s most challenging problems?\nOur Engineering team\nOur engineers participate in all phases of the application development lifecycle, focusing on design, coding, and keeping the production platform up and running.\nOur engineering culture also values curiosity, humility, trust and team spirit.\nOur technical stack is Java with Spring Boot and Hibernate frameworks, SQL, ORM, JPA, Vue.js, JavaScript, SCSS, Amazon Web Services, Maven\nThis is What You Will do in This Role:\nDesign, develop, and maintain complex web applications using Vue.js and associated frameworks.\nLead the architecture and design of scalable frontend applications.\nCollaborate with backend developers to integrate APIs and ensure seamless functionality.\nMentor junior developers, conduct code reviews, and ensure adherence to best practices.\nOptimize applications for maximum speed, scalability, and performance.\nStay updated with the latest trends and advancements in Vue.js and frontend development.\nDebug and troubleshoot application issues to maintain high quality and performance.\nWork with Quality Assurance (QA) team to get the product tested, address any issues\nRequirements -\n10-14 years of web application development experience in a fast-paced agile environment experience required\nProficiency in Springboot and ReactJS/ Vue.js ecosystem (Vue Router, Vuex/Pinia, Composition API).\nStrong experience with JavaScript, TypeScript, HTML, and CSS.\nFamiliarity with modern build tools (e.g., Webpack, Vite).\nKnowledge of RESTful APIs and integration techniques.\nFamiliarity with version control systems like Git.\nStrong problem-solving skills and attention to detail.\nExcellent communication and leadership skills.\nCollaborate with team members to define project requirements, priorities, and timelines.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Maven', 'Hibernate', 'Manager Quality Assurance', 'Front end', 'Performance management', 'Coding', 'Agile', 'HTML', 'JPA', 'SQL']",2025-06-12 14:32:59
Staff Software Engineer,Fourkites,10 - 15 years,Not Disclosed,['Chennai'],"At FourKites we have the opportunity to tackle complex challenges with real-world impacts. Whether it s medical supplies from Cardinal Health or groceries for Walmart, the FourKites platform helps customers operate global supply chains that are efficient, agile and sustainable.\nJoin a team of curious problem solvers that celebrates differences, leads with empathy and values inclusivity\n.\nAs a Staff Software Engineer, you will get an opportunity to work on features end to end (backend & frontend) using the latest technologies such as RoR, Java, GoLang, Angular, React, Redis, PostgreSQL. You will develop products that can change the logistics landscape and will be used by some of the biggest corporations in the world. You will develop integrations with our strategic partners to help expand our ecosystem. You will work closely with our US team and customers to develop features that help shape the logistics and supply chain industry.\nWho you are:\nBachelor s degree in Computer Science & Engineering or related field from a reputed institution.\nMinimum of 8 years of experience in Software Engineering and Web application development.\nGood understanding of software design, Microservices architecture, object-oriented principles, and design patterns.\nExperience with Design and development of highest quality software/services using RoR/Golang/Java.\nGood knowledge of RESTful APIs and microservices architecture\nStrong understanding of Java, Spring Framework, and object-oriented programming principles\nExperience in one of Azure, Amazon Web Services or other cloud services.\nExperience with databases such as MySQL, PostgreSQL, or MongoDB\nFamiliarity with front-end technologies such as HTML, CSS, and JavaScript is a plus\nStrong knowledge of Git (branches, submodules, rebasing) and other Agile tools such as JIRA & Confluence.\nAgile SDLC experience\nExcellent oral and written communication skills\nWhat you ll be doing:\nDesign, architect, implement, test, profile, release, and optimize highest quality software/services using RoR/Golang/Java.\nPartner with product manages to analyse product requirements and plan engineering execution\nDocument HLD/LLD for easy knowledge sharing and future scaling\nPerform design and code reviews\nImplement code with very high coverage of unit tests and component tests\nCross-training peers and mentoring teammates\nPossess expert knowledge in performance, security, scalability, architecture, and best practices\nFunctionally decompose complex problems into simple, straight-forward solutions\nCollaborate with UX designers to develop responsive user interface components\nWorking knowledge of SQL based (any RDBMS) and NOSQL data stores (any one) with the ability to write intermediate level SQL\nExperience in building Web application backends using Java Spring Boot or similar\nExperience with frontend libraries/frameworks such as React/Angular is a plus.\nEducation Qualification: Graduate from B.E/ B.Tech / MCA / M.Tech Background.\nWho we are:\nFourKites , the leader in AI-driven supply chain transformation for global enterprises and pioneer of real-time visibility, turns supply chain data into automated action. FourKites Intelligent Control Tower breaks down enterprise silos by creating a real-time digital twin of orders, shipments, inventory and assets. This comprehensive view, combined with AI-powered digital workers, enables companies to prevent disruptions, automate routine tasks, and optimize performance across As the leader in AI-driven supply chain transformation, FourKites pioneered the Intelligent Control Tower powered by the world s largest real-time visibility network. Our platform creates comprehensive digital twins of your supply chain with AI-powered digital workers to automate resolution, improve collaboration and drive outcomes across all stakeholders. Unlike traditional control towers, we enable true real-time execution and intelligent fulfillment, transforming both your supply and customer\nBenefits\nMedical benefits start on first day of employment\n36 PTO days( Sick, Casual and Earned) , 5 recharge days, 2 volunteer days\nHome Office setups and Technology reimbursement\nLifestyle & Family benefits\nAnnual Swags/ Festive Swags\nOngoing learning & development opportunities ( Professional development program, Toast Master club etc.)",Industry Type: Courier / Logistics,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Software design', 'RDBMS', 'Postgresql', 'MySQL', 'Javascript', 'Agile', 'HTML', 'SDLC', 'SQL']",2025-06-12 14:33:01
PLM PROCESS LEAD - TECHNICAL,Mondelez,8 - 13 years,Not Disclosed,['Mumbai'],"P rovide s software and application knowledge to support implementation of the given solutions specifically in SAP S/4 PLM domain collaborating with stakeholders in R D .\n  How will this person contribute\nPerson will ensure that delivered services are optimized to meet business demands and the service operations strategy, plan, measure, report and communicate service improvement initiatives, and serve as a consultant on issues and resolutions. W ill also recommend actions that can be taken to optimize investments and benefits and to mitigate risks. This role will require to identify suppliers, evaluate them, on-board new vendors, establish and run vendor governance; collaborate with management and follow-up on requisitions, purchase orders, invoices, and payments; work with project resources to provide design collateral and to configure software components so they are aligned with security policy and governance; and ensure adherence to development and configuration standards and processes.\n  What Person will bring to the table\nThis role is a Solution Delivery Expert mainly in SAP S/4 PLM area\nSDE will be the process system expert in Product Lifecycle Management domains and responsible for the delivery of Projects, and also ensure all projects are delivered with quality and benefits.\nWorking collaboratively with multiple vendors\nLeading complex projects - project management\nStakeholder management and influencing skills\nManaging infrastructure services delivery, support and excellence\nWorking in global IT function with regional or global responsibilities in an environment like Mondel z International\nWorking with IT outsourcing providers using frameworks such as the IT Infrastructure Library\nWorking with internal and external teams and leading when necessary\nWhat extra ingredients person will bring:\nDeep Knowledge SAP PLM practices And associated general knowledge of SAP ECC /MDG ), Work flows , Data Migration and Integration with SAP and Non-SAP Applications\nEducation / Certifications:\nUniversity Degree in Engineering, Computer Science or related fields.\nAt least 8 years of experience in similar roles\nAny data certification is a plus\nFluent in English",Industry Type: FMCG,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP ECC', 'Data migration', 'SAP', 'SAP PLM', 'Project management', 'Product life cycle management', 'IT infrastructure', 'Stakeholder management', 'Application software', 'it outsourcing']",2025-06-12 14:33:03
PHP Backend (Platform) Developer,Arbor,6 - 11 years,9-14 Lacs P.A.,['Thiruvananthapuram'],"Role & responsibilities\nAbout you\nExperience of PHP at scale through frameworks such as Symfony /Laravel\nExperience of distributed cloud systems, and specifically Amazon Web Services\nEnterprise Software design patterns and their implementation in real-world enterprise systems\nExperience of message queuing and/or streaming systems such as SQS, ActiveMQ, Apache Kafka, AWS Kinesis, AWS Firehose\nUnderstanding of relational database technologies and their cloud versions (e.g. AWS MySQL Aurora)\nExperience with DataDog, Prometheus or similar observability tools\nA positive and proactive attitude to problem solving\nA team player, willing to muck in and help others when needed, driven personality who asks questions and actively participates in discussions\nCore responsibilities\nDevelop core platform components to aid reusability and stability of the system\nWork with Head of Platform Engineering/SRE to identify and progress platform improvements related to stability, scalability, and performance\nWork with the QA automation framework to ensure functionality is delivered to a high quality\nWork with DevOps Engineers to understand application impacts and system performance and stability, and work with engineering teams to rectify\nAssist in incident response and resolution, and subsequent post-mortems and retrospectives\nContribute to the platform code base and framework which is used by Product Engineers across Engineering\n\nPreferred candidate profile\n\nBonus skills\nPast experience with enterprise solutions running at scale\nFamiliarity with Scrum methodology or other agile development processes\nExperience with Docker and containerization\nExperience with AWS or other Cloud Infrastructure\nFamiliarity with software best practices such as Refactoring, Clean Code, Domain-Driven Design, Test-Driven Development, etc.",Industry Type: Education / Training,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['PHP', 'Symfony Framework', 'Symfony', 'OOPS', 'MVC Framework', 'Oops Design Patterns', 'Yii', 'Zend Framework', 'Oops Programming', 'Laravel', 'Core PHP']",2025-06-12 14:33:06
NetApp Storage Administrator,SVN System Technologies,3 - 6 years,Not Disclosed,['Thane'],"SVN System Technologies is looking for NetApp Storage Administrator to join our dynamic team and embark on a rewarding career journey We are seeking a skilled NetApp Storage Administrator to join our IT team\n\nThe NetApp Storage Administrator will be responsible for the design, implementation, and maintenance of NetApp storage solutions to ensure optimal performance, availability, and reliability\n\nThe ideal candidate will have a strong background in storage administration, NetApp technologies, and a proactive approach to managing storage infrastructure\n\nResponsibilities:Storage Design and Implementation:Design and implement NetApp storage solutions based on organizational requirements\n\nConfigure and optimize storage arrays for performance and efficiency\n\nStorage Administration:Administer and manage NetApp storage systems, including filers, aggregates, and volumes\n\nPerform routine monitoring and maintenance tasks to ensure system health\n\nData Migration and Storage Expansion:Plan and execute data migration activities between storage systems\n\nManage storage expansion projects to accommodate growing data needs\n\nBackup and Disaster Recovery:Implement and manage backup and disaster recovery solutions for NetApp storage\n\nPerform regular data backups and test recovery processes\n\nPerformance Optimization:Monitor and analyze storage performance metrics\n\nIdentify and implement optimization strategies for improved performance\n\nSecurity and Access Control:Configure and manage security settings for NetApp storage, including access controls\n\nEnsure compliance with data privacy and security policies\n\nTroubleshooting:Investigate and resolve storage-related issues and incidents\n\nCollaborate with other IT teams to address cross-functional infrastructure challenges\n\nDocumentation:Create and maintain comprehensive documentation for NetApp storage configurations, procedures, and best practices\n\nDevelop and update standard operating procedures (SOPs) for storage administration",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PDF', 'JPEG', 'netapp storage administrator']",2025-06-12 14:33:08
Principal Site Reliability Engineer,Swimlane,10 - 15 years,Not Disclosed,['Hyderabad'],"Do you have an interest and desire to work with cutting edge technologies to solve challenging problems? Do you love cyber-security? Would you like to help build a platform that helps security teams process millions of security alerts every day? Are you interested in a role where you can use the latest JavaScript technologies and frameworks, and contribute to open source?\n\nAs the most senior technical individual contributor within an entire division of Engineering at Swimlane, you will be deeply involved with and guide the reliability, availability, security, quality, and extensibility of our offerings. You must have an extreme ownership mentality. You will work very closely with other senior Engineering, Product, and Support resources to quickly advance the number and state of our offerings. You will create, test, and operate new services, as well as enhance existing ones.\n\nJob Requirements:\n8+ years experience developing in at least one common, general languages (i.e., C, C#, Java, Python, etc)\n8+ years experience as a senior or principal engineer\n15+ years experience as an engineer managing mission-critical services at scale\n6+ years experience with Amazon Web Services (AWS), Microsoft Azure, and/or Google Compute Platform (GCP)\nDeep, demonstrable experience with various Cloud-native monitoring, logging, and dashboarding platforms (e.g., New Relic, Datadog, Prometheus, etc)\nExcellent understanding of and ability to work with Hashicorp Terraform\nStrong understanding of modern continuous integration/continuous deployment (CI/CD) platforms (e.g., GitHub Actions, GitLab pipelines, AWS CodeBuild / Codedeploy / Codepipeline , etc)\nSubstantial experience with managing Kubernetes resources\nExcellent written and verbal English communication skills\nStrong ability to work in a fast-paced environment that does also have security and compliance requirements\nMust be available for on-call escalations\nMust be able to mentor resources of different levels\nAbility to manage up, down, and across the organization\nAutomate, automate, automate!\n\nDont meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Swimlane, we are dedicated to building a diverse, inclusive and authentic workplace, so if you are excited about this role but your past experience doesnot align perfectly with every qualification in the job description, we encourage you to apply anyway! You may just be the right candidate for this or other roles!\n\nWho we are, and what we offer:\nSwimlane is a rapidly growing, innovative startup that provides cloud-scale, low-code security automation for organizations of all industries and sizes. Our technology is relied upon by major security-forward companies around the globe and we are consistently rated as the #1 trusted low-code security automation platform. Our mission is to prevent breaches and enable continuous compliance via a low-code security automation platform that serves as the system of record for the entire security organization.\n\nWhat s the best thing about working at Swimlane? If you ask the team, they will tell you its the people. Swimlaners are innovative, collaborative and driven by the purpose of revolutionizing the way security teams automate and respond to alerts. Headquartered in beautiful Louisville,Colorado, directly between Denver and Boulder, Swimlanes staff spans 28 states and 16 countries!\n\nThe Perks of being a Swimlaner\nCompetitive Benefits & Compensation\nTraining & Professional Development Opportunities\nMacbook Pro\nGreat Company Culture\nWe value collaboration and innovation\nGive-back Volunteering Opportunities\n\nHere at Swimlane, our core focus is to Automate the World of Security and we strive to represent our five core values in everything we do:\nPunch above your weight class - We make the most of our circumstances and constantly surprise and impress with our ability to deliver.\nBe a happy innovator - The hard problems are the fun problems to solve, we re excited to take on difficult challenges and find creative solutions.\nAlways be leveling up - We are continuously improving, embracing change, and consuming information to better ourselves and each other.\nMove at the speed of WOW - We work with an extreme sense of urgency, but we never compromise quality.\nHave honesty and integrity in all the things - We make decisions with the best of intentions, doing what is right for as many stakeholders as possible.",Industry Type: Hardware & Networking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'github', 'cyber security', 'Compliance', 'GCP', 'Cloud', 'Javascript', 'Open source', 'Monitoring', 'Python']",2025-06-12 14:33:10
PHP Developer,Apto Innovations Private Limited,0 - 2 years,Not Disclosed,['Kozhikode'],"We are looking for an experienced developer to join our team.\nCandidate with in depth knowledge of PHP.\nVersioning tools such as Git, Bit Bucket\nKnowledge of operating systems, hosting and deployment on cloud\nplatforms like Amazon Web Services. will be added advantage\nWhat we expect from you\nWriting clean, well-designed code\nDebugging code and fixing bugs\nManaging code repositories and deploying builds\nTo know about PHP, Laravel, MySQL/ MariaDB, HTML5, CSS3, JavaScript, AJAX/ Fetch, jQuery, etc.\nRoleFull Stack Developer\nIndustry TypeBanking\nFunctional AreaEngineering - Software QA\nEmployment TypeFull Time, Permanent",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['css', 'web services', 'hosting', 'bitbucket', 'ajax', 'jquery', 'docker', 'iot', 'git', 'devops', 'linux', 'jenkins', 'debugging', 'json', 'html', 'mysql', 'shell scripting', 'architecture', 'laravel', 'python', 'operating systems', 'microsoft azure', 'cloud platforms', 'php development', 'javascript', 'mariadb', 'php', 'aws']",2025-06-12 14:33:12
OIC Developer,Sutherland Global Services Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"Job Description:\nWork Hours : 2:30pm IST to 11:30pm IST\nDuration : From 1st June till 31st December 2026\nResource Type : Contract Resource (Later can be converted FTE)\nRequired Skills :\nProven expertise in Oracle Integration Cloud (OIC), including integration patterns, connectivity agents, and process automation.",,,,"['Computer science', 'Process automation', 'Training', 'Performance tuning', 'Data migration', 'Analytical', 'Integration testing', 'Cloud', 'Oracle', 'SSIS']",2025-06-12 14:33:15
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Nagpur'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:33:18
SBU Support Planner-2,Solenis,5 - 7 years,Not Disclosed,['Hyderabad'],"Solenis is a leading global provider of water and hygiene solutions. The company s product portfolio includes a broad array of water treatment chemistries, process aids, functional additives, cleaners, disinfectants, and state-of-the-art monitoring, control and delivery systems. These technologies are used by customers to improve operational efficiencies, enhance product quality, protect plant assets, minimize environmental impact, and create cleaner and safer environments. Headquartered in Wilmington, Delaware, the company has 70 manufacturing facilities strategically located around the globe and employs a team of over 16, 500 professionals in 130 countries across six continents. Solenis is a 2025 Best Managed Company Gold Standard honoree. For more information about Solenis, please visit www. solenis. com .",,,,"['Procurement', 'Supply chain', 'Data analysis', 'Demand planning', 'Raw material', 'Forecasting', 'Monitoring', 'Analytics', 'Logistics', 'Capacity planning']",2025-06-12 14:33:20
DevOps Engineer,Easemytrip,1 - 6 years,Not Disclosed,"['Noida', 'Gurugram']","About the Role:\nAs a DevOps Engineer at EaseMyTrip.com, you will be pivotal in optimizing and maintaining our IT infrastructure and deployment processes. Your role involves managing cloud environments, implementing automation, and ensuring seamless deployment of applications across various platforms. You will collaborate closely with development teams to enhance system reliability, security, and efficiency, supporting our mission to provide exceptional travel experiences through robust technological solutions. This position is critical for maintaining high operational standards and driving continuous innovation.\n\nRole & responsibilities:\nCloud Computing Mastery: Expert in managing Amazon Web services (AWS) environments, with skills in GCP and Azure for comprehensive cloud solutions and automation.\nWindows Server Expertise: Profound knowledge of configuring and maintaining Windows Server systems and Internet Information Services (IIS).\nDeployment of .NET Applications: Experienced in deploying diverse .NET applications such as ASP.Net, MVC, Web API, and WCF using Jenkins.\nProficiency in Version Control: Skilled in utilizing GitLab or GitHub for effective version control and collaboration.\nLinux Server Management: Capable of administering Linux servers with a focus on security and performance optimizations.\nScripting and Automation: Ability to write and maintain scripts for automation of routine tasks to improve efficiency and reliability.\nMonitoring and Optimization: Implement monitoring tools to ensure high availability and performance of applications and infrastructure.\nSecurity Best Practices: Knowledge of security protocols and best practices to safeguard systems and data.\nContinuous Integration/Continuous Deployment (CI/CD): Develop and maintain CI/CD pipelines to streamline software updates and deployments.\nCollaboration and Support: Work closely with development teams to troubleshoot deployment issues and enhance the overall operational efficiency.\n\nPreferred candidate profile:\nMigration Project Leadership: Experienced in leading significant migration projects from planning through to execution.\nDatabase Expertise: Strong foundation in both SQL and NoSQL database technologies.\nExperience with Diverse Tech Stacks: Managed projects involving various technologies, including 2-tier, 3-tier, and microservices architectures.\nProficiency in Automation Tools: Hands-on experience with automation and deployment tools such as Jenkins, Bamboo, and Code Deploy.\nAdvanced Code Management: Highly skilled in managing code revisions and maintaining code integrity across multiple platforms.\nStrategic DevOps Experience: Proven track record in developing and implementing DevOps strategies at an enterprise level.\nConfiguration Management Skills: Proficient in using tools like Ansible, Chef, or Puppet for configuration management.\nTechnology Versatility: Experience working with a range of programming languages and frameworks, including .NET, MVC, LAMP, Python, and NodeJS.\nProblem Solving and Innovation: Ability to solve complex technical issues and innovate new solutions to enhance system reliability and performance.\nEffective Communication: Strong communication skills to collaborate with cross-functional teams and articulate technical challenges and solutions clearly.",Industry Type: Travel & Tourism,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Code Deploy', 'Aws Cloud', 'Ci/Cd', 'Devops', 'Devops Automation', 'Apm Tools', 'Cloud Automation Devops', 'Devops Jenkins', 'Aws Codedeploy', 'Aws Api Gateway', 'Vpc', 'Load Balancing', 'Route3', 'Code Pipeline', 'Auto Scaling', 'Pipeline', 'Elastic Cache', 'Aws Devops', 'Aws Lambda', 'Build', 'New Relic', 'Azure Devops', 'Cloudfront']",2025-06-12 14:33:23
Specialist Application Management- Kochi,StradaGlobal,8 - 13 years,Not Disclosed,['Kochi'],"Our story\nStrada is a technology-enabled, people powered company committed to delivering world-class payroll, human capital management, and financial management solutions to organizations globally. With a team of more than 8,000 experts and over 30 years of expertise, Strada blends leading-edge technology with human ingenuity to help businesses across the globe design and deliver at scale. Supporting over 1,400 customers in 33 countries, Strada partners with customers at every stage of their journey, to help drive their vision forward. Its why were so driven to connect passion with purpose. Our teams experience in human insights and cloud technology gives companies and employees around the world the ability to power confident decisions, for life.",,,,"['Application Support', 'Application Support Management', 'Application Management', 'Power Bi', 'Application Implementation', 'Cpq', 'Servicenow', 'Workday Financials', 'JIRA', 'Salesforce']",2025-06-12 14:33:25
Software Engineer Staff,Fourkites,9 - 14 years,30-45 Lacs P.A.,[],"At FourKites we have the opportunity to tackle complex challenges with real-world impacts. Whether its medical supplies from Cardinal Health or groceries for Walmart, the FourKites platform helps customers operate global supply chains that are efficient, agile and sustainable.\nJoin a team of curious problem solvers that celebrates differences, leads with empathy and values inclusivity.\nAs a Staff Software Engineer, you will get an opportunity to work on features end to end (backend & frontend) using the latest technologies such as RoR, Java, GoLang, Angular, React, Redis, PostgreSQL. You will develop products that can change the logistics landscape and will be used by some of the biggest corporations in the world. You will develop integrations with our strategic partners to help expand our ecosystem. You will work closely with our US team and customers to develop features that help shape the logistics and supply chain industry.\nWho you are:\nBachelorâ€™s degree in Computer Science & Engineering or related field from a reputed institution.\nMinimum of 8 years of experience in Software Engineering and Web application development.\nGood understanding of software design, Microservices architecture, object-oriented principles, and design patterns.\nExperience with Design and development of highest quality software/services using RoR/Golang/Java.\nGood knowledge of RESTful APIs and microservices architecture\nStrong understanding of Java, Spring Framework, and object-oriented programming principles\nExperience in one of Azure, Amazon Web Services or other cloud services.\nExperience with databases such as MySQL, PostgreSQL, or MongoDB\nFamiliarity with front-end technologies such as HTML, CSS, and JavaScript is a plus\nStrong knowledge of Git (branches, submodules, rebasing) and other Agile tools such as JIRA & Confluence.\nAgile SDLC experience\nExcellent oral and written communication skills\nWhat youâ€™ll be doing:\nDesign, architect, implement, test, profile, release, and optimize highest quality software/services using RoR/Golang/Java.\nPartner with product manages to analyse product requirements and plan engineering execution\nDocument HLD/LLD for easy knowledge sharing and future scaling\nPerform design and code reviews\nImplement code with very high coverage of unit tests and component tests\nCross-training peers and mentoring teammates\nPossess expert knowledge in performance, security, scalability, architecture, and best practices\nFunctionally decompose complex problems into simple, straight-forward solutions\nCollaborate with UX designers to develop responsive user interface components\nWorking knowledge of SQL based (any RDBMS) and NOSQL data stores (any one) with the ability to write intermediate level SQL\nExperience in building Web application backends using Java Spring Boot or similar\nExperience with frontend libraries/frameworks such as React/Angular is a plus.\nEducation Qualification: Graduate from B.E/ B.Tech / MCA / M.Tech Background.\nWho we are:\nFourKitesÂ®, the leader in AI-driven supply chain transformation for global enterprises and pioneer of real-time visibility, turns supply chain data into automated action. FourKitesâ€™ Intelligent Control Towerâ„¢ breaks down enterprise silos by creating a real-time digital twin of orders, shipments, inventory and assets. This comprehensive view, combined with AI-powered digital workers, enables companies to prevent disruptions, automate routine tasks, and optimize performance across As the leader in AI-driven supply chain transformation, FourKites pioneered the Intelligent Control Towerâ„¢ powered by the worldâ€™s largest real-time visibility network. Our platform creates comprehensive digital twins of your supply chain with AI-powered digital workers to automate resolution, improve collaboration and drive outcomes across all stakeholders. Unlike traditional control towers, we enable true real-time execution and intelligent fulfillment, transforming both your supply and customer\nBenefits\nMedical benefits start on first day of employment\n36 PTO days( Sick, Casual and Earned) , 5 recharge days, 2 volunteer days\nHome Office setups and Technology reimbursement\nLifestyle & Family benefits\nAnnual Swags/ Festive Swags\nOngoing learning & development opportunities ( Professional development program, Toast Master club etc.)",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Golang', 'Casandra', 'SQL Database', 'Ci/Cd', 'Elastic Search', 'Python']",2025-06-12 14:33:27
Snowflake Expert,Tekskills India,6 - 11 years,Not Disclosed,['Hyderabad'],"Location: Hyderabad (Preferred) / Bangalore (If strong candidates are available)\nType: Contractual\nKey Responsibilities:\nUse data mappings and models provided by the data modeling team to build robust Snowflake data pipelines.\nDesign and implement pipelines adhering to 2NF/3NF normalization standards.\nDevelop and maintain ETL processes for integrating data from multiple ERP and source systems.\nBuild scalable and secure Snowflake data architecture supporting Data Quality (DQ) needs.\nRaise CAB requests via Carriers change process and manage production deployments.\nProvide UAT support and ensure smooth transition of finalized pipelines to support teams.\nCreate and maintain comprehensive technical documentation for traceability and handover.\nCollaborate with data modelers, business stakeholders, and governance teams to enable DQ integration.\nOptimize complex SQL queries, perform performance tuning, and ensure data ops best practices.\nRequirements:\nStrong hands-on experience with Snowflake\nExpert-level SQL skills and deep understanding of data transformation\nSolid grasp of data architecture and 2NF/3NF normalization techniques\nExperience with cloud-based data platforms and modern data pipeline design\nExposure to AWS data services like S3, Glue, Lambda, Step Functions (preferred)\nProficiency with ETL tools and working in Agile environments\nFamiliarity with Carrier CAB process or similar structured deployment frameworks\nProven ability to debug complex pipeline issues and enhance pipeline scalability\nStrong communication and collaboration skills",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'ETL', 'aws services', 'SQL']",2025-06-12 14:33:29
Hiring SCM Functional Lead - US Shift - Ahmedabad/Pune/Remote,Synoptek,7 - 12 years,Not Disclosed,[],"Lead Functional Consultant, SCM & Manufacturing\nThis is an amazing opportunity to work within one of the fastest growing Managed Services Providers. We are a company with a heart and soul dedicated to the ongoing success and growth of our employees and continued business success of the customers we support. We foster a fun and connected environment with employee benefits extending beyond general compensation and into company sponsored events and an invested culture of learning.\nThe Lead Functional Consultant, SCM & Manufacturing, is responsible for leading the end-to-end project lifecycle, from project conception to successful implementation, specifically focusing on Supply Chain Management (SCM) and Manufacturing solutions. This role requires a deep understanding of Microsoft Dynamics AX/ F&SCM, strong leadership skills, and the ability to coordinate and manage both functional teams and project scope effectively.",,,,"['Microsoft Dynamics', 'SCM', 'dynamics', 'Supply Chain Management']",2025-06-12 14:33:32
Sap MM Consultant,Trident Group,4 - 9 years,18-25 Lacs P.A.,"['Barnala', 'Budhni']","Job Title: SAP MM Consultant\nJob Location: Punjab, Madhya Pradesh\nJob Type: Full-Time\nJob Summary: We are looking for an experienced SAP MM Consultant to join our team. The ideal candidate will have a deep understanding of the SAP Material Management module, along with strong analytical skills and the ability to translate business requirements into effective ERP solutions. Your role will involve configuring and implementing the SAP MM module, providing user support, and ensuring seamless integration with other SAP modules.",,,,"['SAP MM Configuration', 'SAP MM Module', 'Problem Solving Skills', 'SAP MM Implementation']",2025-06-12 14:33:34
Salesforce Developer,Varun Info Tech Consulting,2 - 4 years,3-6 Lacs P.A.,['Hyderabad( Madhapur )'],"Roles and Responsibilities\nDesign, develop, test, deploy, and maintain Salesforce solutions to meet business requirements.\nCollaborate with cross-functional teams to gather requirements and deliver high-quality solutions.\nTroubleshoot issues related to Salesforce platform, including data migration, integration, and performance optimization.\nDevelop custom components using Apex programming language and LWC (Lightning Web Components).\nEnsure adherence to coding standards, best practices, and company policies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Salesforce Sales Cloud', 'Salesforce Service Cloud', 'Salesforce Certification', 'Sales Force Development', 'Salesforce Integration']",2025-06-12 14:33:37
SAP Implementation Engineer,Emkay Placement Consultants,10 - 12 years,10-13 Lacs P.A.,['Kolkata'],"Lead SAP team for implementing SAP modules FICO, MM, SD, PM, PP, QM, PS .Understand business & technology ,digital platforms & drive new initiatives such as GRC, SAP Rise ,Minimize SAP run cost ,ROI analyses for SAP spending and initiatives,\n\nRequired Candidate profile\nimplementing SAP modules FICO, MM, SD, PM, PP, QM, PS .Understand modern business & technology framework, drive new initiatives such as GRC, ROI analyses for SAP spending and initiatives,",Industry Type: Building Material (Cement),Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Sap Data Migration', 'Sap Hana', 'SAP Support', 'Roi Analysis', 'SAP Implementation', 'SAP ECC', 'PP', 'PS', 'Time Management', 'SAP PP', 'Sap S Hana', 'FICO', 'SD', 'Execution', 'MM', 'ECC', 'SAP Security', 'Scheduling', 'ROI', 'Schedule', 'SAP Basis', 'GRC', 'Digitization', 'SAP Production Planning', 'Planning', 'Qm', 'SAP Quality Management']",2025-06-12 14:33:39
Salesforce Developer,Leading Client,5 - 10 years,Not Disclosed,['Hyderabad'],"Name of Projects in which the skills were used (add rows if necessary)\nNo: of months worked in each Project of work done using the skill (Mandatory/ optional)\nApex Customization - M\nSalesforce Configuration - M\nLWC - M\nJavascript/Jquery - M\nSlack - O\nMulesoft - O\nDesign, develop, and deploy customized Salesforce solutions using Apex, Visualforce, and Lightning components Collaborate with stakeholders to gather requirements and translate them into technical designs Create and maintain workflows, process builders, triggers, and validation rules Perform data migration using Data Loader or third-party tools Integrate Salesforce with external systems via REST/SOAP APIs Ensure unit testing, deployment through CI/CD tools, and post-deployment support Maintain platform security and ensure adherence to Salesforce best practices",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Salesforce', 'Lightning Web Components', 'Slack', 'JavaScript', 'Jquery', 'Salesforce Lightning', 'Mulesoft']",2025-06-12 14:33:41
D365 F&O Finance Functional Consultant,Claranet Group,5 - 8 years,Not Disclosed,[],"We at Claranet are looking for F&O Finance Functional Professionals who will be involved in all aspects of implementing supporting Dynamics solutions through the project life cycle to go live and ongoing support. You can contribute to Solution Design sessions, help with configuration, help with data migration deliverables, create required interface design and functional design documents, troubleshoot customization, etc.\n\nRole- D365 F&O Functional Consultant (Full Time)\nOffice Location Hitech, Hyderabad - India\nWorking mode : Remote\n\nInterested Candidates can your CV on Rakesh.koyya@claranet.com\n\nKey Responsibilities:\nAnalyze business processes (Finance, PMA) to identify opportunities for improvement.\nIdentify creative workarounds to meet requirements without the development of custom code.\nUnderstand the functional capabilities and limitations for out-of-the-box functionality as well as custom code.\nYou will be the one to identify our customers' requirements and match them with technological capabilities and with Microsoft continuous release plans.\n\nKey Competencies:\nDeep functional knowledge of Microsoft Dynamics 365 Finance and PMA.\nExperience of customized solutions to complex business problems.\nDemonstrable consultancy experience.\nStrong working knowledge of business processes and ER framework.\nRelevant Microsoft certification.\nExcellent documentation and communication skills.\nA logical approach to problem-solving and the structured introduction of change into operational systems.\nAbility to multitask and prioritize.\nGood interpersonal skills.\nAttention to detail.\n\nSkills Required:\nHold 6-8 years of experience within D365 F&O implementation, support\nSpecialized in Finances, PMA, Integration, ER reports\nQualified Chartered Accountant / MBA (Finance) desirable\nAre fluent in English.\nStrong communication and consulting skills\nGlobal Exposure must (Interacting with Global clients/customers)\n\nCompany Benefits:\nGroup Medical Insurance (including parents coverage), Group Term Life Policy and Personal Accidental Policy\nTax Saving flexible benefits\nFlexible working hours\nStatutory Benefits (PF, Gratuity)",Industry Type: Miscellaneous,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['PMA', 'Microsoft Dynamics', 'F&O', 'finance', 'Integration', 'configuration', 'Migration', 'Customization', 'implementation', 'Troubleshooting', 'ER', 'Data Migration', 'functional consultant']",2025-06-12 14:33:43
SAP SD Consultant,Trident Group,4 - 9 years,18-25 Lacs P.A.,"['Barnala', 'Budhni']","Job Title: SAP SD Consultant\nJob Location: Punjab, Madhya Pradesh\nJob Type: Full-Time\nJob Summary: We are seeking a dedicated SAP SD (Sales and Distribution) Consultant to implement and support our SAP software solutions. The ideal candidate will have a strong understanding of the SAP SD module, along with excellent analytical skills and the ability to translate business requirements into effective ERP solutions. Your role will involve configuring and implementing the SAP SD module, providing user support, and ensuring seamless integration with other SAP modules.",,,,"['SAP MM Configuration', 'SAP MM Module', 'SAP MM Implementation', 'Material Management', 'Problem Solving Skills']",2025-06-12 14:33:45
Netsuite Developer,Advy Chemical,2 - 4 years,Not Disclosed,['Thane'],"Resolve day-to-day NetSuite issues and user queries efficiently.\nDevelop and maintain NetSuite customizations using SuiteScript (1.0 / 2.0 / 2.1) including Client Scripts, User Events, Suitelets, RESTlets, Scheduled Scripts, and Map/Reduce.\nDesign and configure SuiteFlow workflows, Saved Searches, dashboards, and reports using SuiteAnalytics.\nCustomize forms, fields, roles, permissions, and advanced PDF templates through SuiteBuilder.\nIntegrate NetSuite with external systems using APIs (REST/SOAP) or middleware tools.\nPerform data migrations, sandbox-to-production deployments, and manage post-Go-Live support and improvements.\nMaintain technical documentation, apply version control, and ensure structured deployment processes.\nCollaborate with business stakeholders to translate functional requirements into scalable NetSuite solutions.",Industry Type: Pharmaceutical & Life Sciences,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['ERP System', 'Netsuite Erp', 'Suitescript', 'Suite Builder', 'Javascript', 'Accounting Software', 'Suite Bundler', 'Suiteflow']",2025-06-12 14:33:47
Dot Net Fullstack Developer,BriskWin IT (BWIT),6 - 11 years,Not Disclosed,"['Pune', 'Chennai', 'Mumbai (All Areas)']","Req 1. Microsoft Cloud .Net + Angular + React (Basics)\nExp: 6 to 12 Yrs\nNP : Immediate to 15 Days (Serving Notice Max 20 Days)\nLocation Chennai / Mumbai / Pune / Bangalore\n\nReq 2. API Technical Lead Azure Cloud Integration\nExp: 9+ yrs\nNP : Immediate to 30 Days\nLocation: Chennai, Mumbai, Pune, Noida, Ahmadabad, Bangalore\n\nKey Skills & Experience:\n\nProven experience in a technical leadership role focused on backed/API development, with strong proficiency in Java.\nIn-depth knowledge of Microsoft Azure services, including:\nAzure Table Storage\nAzure Data Lake Storage Gen2\nAzure Functions\nAzCopy for efficient data migration to Azure( Good to have )\nStrong grasp of Restful API design principles, cloud-native architectures, and server less computing models",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['.Net Core', 'Cloud', 'React.Js', 'Angular']",2025-06-12 14:33:50
Sap Abap Technical Consultant,Varun Info Tech Consulting,2 - 3 years,6-9 Lacs P.A.,['Hyderabad( Madhapur )'],"Roles and Responsibilities\nIntegrate SAP with Salesforce using OData Rest APIs.\nDesign, develop, test, and deploy SAP ABAP applications using OOPS concepts.\nExpert knowledge in LSMW and LTMC for data migration.\n\nImplement & Support Adobe forms.\n\nFamiliarity with Fiori App developments and enhancements.\nDesired Candidate Profile\n2 to 3 years of experience in SAP ABAP development, Preferred on HANA platform.\nBachelor's degree in Computers or Information Technology (B. Tech/B.E.).\nStrong understanding of Object-Oriented Programming (OOP) principles and design patterns.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Odata', 'LSMW', 'Sap Abap Hana', 'Sap Fiori', 'OO ABAP', 'Adobe Forms', 'Cds Views', 'LTMC']",2025-06-12 14:33:52
