job_title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Data Architect,Acesoft,6 - 10 years,19-22.5 Lacs P.A.,['Bengaluru'],"Hi all,\nWe are hiring fore the Data Architecture\nExperience: 6 - 9 years\nLocation: Bangalore\nNotice Period: Immediate - 15 Days\nSkills:\nData Architecture\nAzure Data Factory\nAzure Data Bricks\nAzure Cloud\nArchitecture\n\nIf you are interested drop your resume at mojesh.p@acesoftlabs.com\nCall: 9701971793",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Azure Data Factory', 'Architecture', 'Azure Databricks', 'Data Architecture', 'Azure Synapse', 'Data Modeling']",2025-06-12 00:50:53
Data Architect,CGI,8 - 12 years,15-30 Lacs P.A.,['Hyderabad'],"Job Title: Data Architect / Data Modeler\nExperience Level: 8+ Years\nLocation: Hyderabad\nJob Summary\nWe are seeking a highly experienced Data Architect to join our growing Data & Analytics team. This role demands a strategic thinker and technical expert who can design and build robust, scalable, and efficient data solutions. You will play a critical role in architecting end-to-end data pipelines, designing optimized data models, and delivering business-centric data infrastructure using cutting-edge technologies such as Python, PySpark, SQL, Snowflake, and/or Databricks.\nThe ideal candidate will have a deep understanding of data engineering best practices and a proven track record of enabling data-driven decision-making through innovative and scalable data solutions.\nKey Responsibilities\nArchitect & Design Scalable Data Pipelines\nLead the design and implementation of high-performance, scalable, and maintainable data pipelines that support batch and real-time processing.\nData Modeling & Data Architecture\nDesign and implement optimized data models and database schemas to support analytics, reporting, and machine learning use cases.\nCloud Data Platforms\nDevelop and manage modern cloud-based data architectures using platforms like Snowflake or Databricks, ensuring performance, security, and cost-efficiency.\nData Integration & ETL Development\nBuild robust ETL/ELT workflows to ingest, transform, and provision data from a variety of internal and external sources.\nCollaboration with Stakeholders\nWork closely with data analysts, data scientists, product managers, and business leaders to translate business requirements into technical specifications and data solutions.\nData Quality & Governance\nImplement and advocate for best practices in data quality, security, compliance, lineage, and governance.\nPerformance Optimization\nOptimize data storage and query performance using advanced SQL, partitioning, indexing, caching strategies, and compute resource tuning.\nMentorship & Best Practices\nProvide mentorship to junior engineers, establish coding standards, and contribute to the growth and maturity of the data engineering practice.\nRequired Qualifications\nBachelors or Master’s degree in Computer Science, Engineering, Data Science, or a related field.\n8+ years of experience in data engineering or related roles.\nStrong expertise in Python and PySpark for data processing and transformation.\nProficient in advanced SQL with a deep understanding of query optimization and performance tuning.\nHands-on experience with Snowflake and/or Databricks in a production environment.\nExperience in designing and implementing data warehouses and data lakes.\nSolid understanding of distributed computing frameworks, big data ecosystems, and modern data architecture patterns.\nExperience with CI/CD, version control systems (e.g., Git), and workflow orchestration tools (e.g., Airflow, dbt, etc.).\nStrong communication skills with the ability to clearly articulate technical concepts to non-technical stakeholders.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'sql', 'snowflake', 'Data Architecture']",2025-06-12 00:50:57
Data Architect,Ford,14 - 17 years,Not Disclosed,['Chennai'],"We are looking for Data Solution Architect to join FC India IT Architecture team. In this role, you will define analytics solutions and guide engineering teams to implement big data solutions on the cloud. Work involves migrating data from legacy on-prem warehouses to Google cloud data platform. This role will provide architecture assistance to data engineering teams in India, with key responsibility of supporting applications globally. This role will also drive business adoption of the new platform and sunset of legacy platforms.\nGoogle Professional Solution Architect certification.\n8+ years of relevant work experience in analytics application and data architecture, with deep understanding of cloud hosting concepts and implementations.\n5+ years experience in Data and Solution Architecture in analytics space. Solid knowledge of cloud data architecture, data modelling principles, and expertise in Data Modeling tools.\nExperience in migrating legacy analytics applications to Cloud platform and business adoption of these platforms to build insights and dashboards through deep knowledge of traditional and cloud Data Lake, Warehouse and Mart concepts.\nGood understanding of domain driven design and data mesh principles.\nExperience with designing, building, and deploying ML models to solve business challenges using Python/BQML/Vertex AI on GCP.\nKnowledge of enterprise frameworks and technologies. Strong in architecture design patterns, experience with secure interoperability standards and methods, architecture tolls and process.\nDeep understanding of traditional and cloud data warehouse environment, with hands on programming experience building data pipelines on cloud in a highly distributed and fault-tolerant manner. Experience using Dataflow, pub/sub, Kafka, Cloud run, cloud functions, Bigquery, Dataform, Dataplex , etc.\nStrong understanding on DevOps principles and practices, including continuous integration and deployment (CI/CD), automated testing & deployment pipelines.\nGood understanding of cloud security best practices and be familiar with different security tools and techniques like Identity and Access Management (IAM), Encryption, Network Security, etc. Strong understanding of microservices architecture.\nNice to Have\nBachelor s degree in Computer science/engineering, Data science or related field.\nStrong leadership, communication, interpersonal, organizing, and problem-solving skills\nGood presentation skills with ability to communicate architectural proposals to diverse audiences (user groups, stakeholders, and senior management).\nExperience in Banking and Financial Regulatory Reporting space.\nAbility to work on multiple projects in a fast paced & dynamic environment.\nExposure to multiple, diverse technologies, platforms, and processing environments.\nUtilize Google Cloud Platform & Data Services to modernize legacy applications.\nUnderstand technical business requirements and define architecture solutions that align to Ford Motor & Credit Companies Patterns and Standards.\nCollaborate and work with global architecture teams to define analytics cloud platform strategy and build Cloud analytics solutions within enterprise data factory.\nProvide Architecture leadership in design & delivery of new Unified data platform on GCP.\nUnderstand complex data structures in analytics space as well as interfacing application systems. Develop and maintain conceptual, logical & physical data models. Design and guide Product teams on Subject Areas and Data Marts to deliver integrated data solutions.\nProvide architectural guidance for optimal solutions considering regional Regulatory needs.\nProvide architecture assessments on technical solutions and make recommendations that meet business needs and align with architectural governance and standard.\nGuide teams through the enterprise architecture processes and advise teams on cloud-based design, development, and data mesh architecture.\nProvide advisory and technical consulting across all initiatives including PoCs, product evaluations and recommendations, security, architecture assessments, integration considerations, etc.\nLeverage cloud AI/ML Platforms to deliver business and technical requirements.",Industry Type: Auto Components,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Data modeling', 'Access management', 'Enterprise architecture', 'Consulting', 'Network security', 'Data structures', 'Analytics', 'Python']",2025-06-12 00:51:00
Data Architect,Wissen Infotech,7 - 10 years,12-22 Lacs P.A.,['Bengaluru'],"Be the digital functional owner of their scope: understand the technical solution, describe it to business stakeholders, and suggest evolutions to meet new business needs - Work closely with stakeholders to understand business objectives, gather requirements, and translate them into comprehensive functional specifications. - Collaborate with software development teams to communicate business requirements, validate technical feasibility, and ensure alignment with project goals - Conduct thorough analysis of end-to-end processes and data flows, identifying areas for improvement and proposing innovative solutions - Document stories, use cases, and process flows to ensure clear communication between business and technical teams - Facilitate workshops, meetings, and training sessions to promote a shared understanding of business analysis methodologies and foster a culture of continuous improvement - Partner with software development leaders to ensure that business requirements are clearly communicated, validated, and integrated into development initiatives - Champion a customer-centric approach to business analysis, seeking to understand and address the needs of end-users and stakeholders - Foster a collaborative and growth-oriented team environment, promoting knowledge sharing and skill development - Troubleshoot and resolve integration issues, ensuring minimal disruption to business operations - Participate in testing and validation activities to ensure solutions meet business requirements and quality standards Internal - Participate in architecture review board meetings and make strategic recommendations for cloud architecture Qualifications - 4+ years experience working in IT product, analyst, functional or architecture roles - Bachelor’s degree in business administration, Information Technology, or related field (technical background required) - Comprehensive knowledge and hands-on experience with MDM environments - Strong analytical and problem-solving skills - Excellent communication and collaboration skills - Ability to manage multiple projects simultaneously and prioritize tasks effectively - Strong understanding of business analysis methodologies, tools, and best practices within a software development context - A desire for continuous learning and stay updated with emerging technologies - Experience in Agile methodologies and leading business analysis efforts in Agile environments - Understanding of service business processes is a plus - Experience with Informatica Cloud and AWS environments is a plus - Certification in Business Analysis (e.g., CBAP) is a plus Skills - Due to the nature of this position sitting on a global team, fluent English communication skills (written & spoken) are required - Technical awareness to understand and validate development proposals against functional requirements - Experience with BI tools such as Power BI, Tableau, Quicksight - Ability to see the consumer perspective and act as their advocate - Strong interpersonal skills, with ability to communicate and convince at various levels of the organization, and in a multicultural environment - Ability to effectively multi-task and manage priorities - Strong analytical and synthesis skills - Initiative to uncover and solve problems proactively - Ability to understand complex software development environments Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power BI', 'Tableau', 'Quicksight']",2025-06-12 00:51:05
Data Architect-Data Modelling,Tiger Analytics,10 - 15 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\nAs an Architect, you will work to solve some of the most complex and captivating data management problems that would enable them as a data-driven organization; Seamlessly switch between roles of an Individual Contributor, team member, and Data Modeling Architect as demanded by each project to define, design, and deliver actionable insights.\nOn a typical day, you might\nEngage the clients & understand the business requirements to translate those into data models.",,,,"['Data Modeling', 'Data Architecture', 'SQL', 'Cloud Platforms', 'OLAP', 'Data Warehousing', 'OLTP']",2025-06-12 00:51:10
Data Architect,Ford,8 - 13 years,Not Disclosed,['Chennai'],"We are looking for Data Solution Architect to join FC India IT Architecture team. In this role, you will define analytics solutions and guide engineering teams to implement big data solutions on the cloud. Work involves migrating data from legacy on-prem warehouses to Google cloud data platform. This role will provide architecture assistance to data engineering teams in India, with key responsibility of supporting applications globally. This role will also drive business adoption of the new platform and sunset of legacy platforms.\nGoogle Professional Solution Architect certification.\n8+ years of relevant work experience in analytics application and data architecture, with deep understanding of cloud hosting concepts and implementations.\n5+ years experience in Data and Solution Architecture in analytics space. Solid knowledge of cloud data architecture, data modelling principles, and expertise in Data Modeling tools.\nExperience in migrating legacy analytics applications to Cloud platform and business adoption of these platforms to build insights and dashboards through deep knowledge of traditional and cloud Data Lake, Warehouse and Mart concepts.\nGood understanding of domain driven design and data mesh principles.\nExperience with designing, building, and deploying ML models to solve business challenges using Python/BQML/Vertex AI on GCP.\nKnowledge of enterprise frameworks and technologies. Strong in architecture design patterns, experience with secure interoperability standards and methods, architecture tolls and process.\nDeep understanding of traditional and cloud data warehouse environment, with hands on programming experience building data pipelines on cloud in a highly distributed and fault-tolerant manner. Experience using Dataflow, pub/sub, Kafka, Cloud run, cloud functions, Bigquery, Dataform, Dataplex , etc.\nStrong understanding on DevOps principles and practices, including continuous integration and deployment (CI/CD), automated testing & deployment pipelines.\nGood understanding of cloud security best practices and be familiar with different security tools and techniques like Identity and Access Management (IAM), Encryption, Network Security, etc. Strong understanding of microservices architecture.\nNice to Have\nBachelor s degree in Computer science/engineering, Data science or related field.\nStrong leadership, communication, interpersonal, organizing, and problem-solving skills\nGood presentation skills with ability to communicate architectural proposals to diverse audiences (user groups, stakeholders, and senior management).\nExperience in Banking and Financial Regulatory Reporting space.\nAbility to work on multiple projects in a fast paced & dynamic environment.\nExposure to multiple, diverse technologies, platforms, and processing environments.\nUtilize Google Cloud Platform & Data Services to modernize legacy applications.\nUnderstand technical business requirements and define architecture solutions that align to Ford Motor & Credit Companies Patterns and Standards.\nCollaborate and work with global architecture teams to define analytics cloud platform strategy and build Cloud analytics solutions within enterprise data factory.\nProvide Architecture leadership in design & delivery of new Unified data platform on GCP.\nUnderstand complex data structures in analytics space as well as interfacing application systems. Develop and maintain conceptual, logical & physical data models. Design and guide Product teams on Subject Areas and Data Marts to deliver integrated data solutions.\nProvide architectural guidance for optimal solutions considering regional Regulatory needs.\nProvide architecture assessments on technical solutions and make recommendations that meet business needs and align with architectural governance and standard.\nGuide teams through the enterprise architecture processes and advise teams on cloud-based design, development, and data mesh architecture.\nProvide advisory and technical consulting across all initiatives including PoCs, product evaluations and recommendations, security, architecture assessments, integration considerations, etc.\nLeverage cloud AI/ML Platforms to deliver business and technical requirements.",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Data modeling', 'Access management', 'Enterprise architecture', 'Consulting', 'Network security', 'Data structures', 'Analytics', 'Python']",2025-06-12 00:51:14
Data Architect,Accenture,15 - 19 years,Not Disclosed,['Bengaluru'],"Project Role :Data Architect\n\n\nProject Role Description :Define the data requirements and structure for the application. Model and design the application data structure, storage and integration.\nMust have skills :Google Cloud Platform Architecture\n\nGood to have skills :NA\nMinimum 15 year(s) of experience is required\n\nEducational Qualification :Total 15 years of full time education\nSummary:As an AI/ML Architect, you will be responsible for developing applications and systems that utilize AI tools and Cloud AI services. Your typical day will involve applying GenAI models as part of the solution, utilizing deep learning, neural networks, chatbots, and image processing.\nRoles & Responsibilities:\nDesign and develop CCAI applications and systems utilizing Google Cloud Machine Learning Services, dialogue flow CX, agent assist.\nDesign, develop and implement chatbot solutions that integrate seamlessly with CCAI and other Cloud services\nApply GenAI-Vertex AI models as part of the solution, utilizing deep learning, neural networks, chatbots, and image processing.\nEnsure proper cloud or on-prem application pipeline with production-ready quality.\nCollaborate with cross-functional teams to ensure successful implementation of AI/ML solutions.Professional & Technical Skills:\nMust To Have Skills:Proficiency in Google Cloud Machine Learning Services.\nGood To Have Skills:Cloud Data Architecture, Cloud ML/PCA/PDE Certification.\nStrong understanding of AI/ML algorithms and techniques.\nExperience with deep learning, neural networks, chatbots, and image processing.\nExperience with cloud data architecture solutions.\nExperience with cloud or on-prem application pipeline with production-ready quality.\nExperience with integrating multiple services such as NLP, voice recognition, and third party APIs.\nStrong communication skills, with the ability to explain technical concepts to non-technical stakeholders. Additional Information:\nThe candidate should have a minimum of 17 years of experience in Google Cloud Machine Learning Services.\nThe ideal candidate will possess a strong educational background in computer science, mathematics, or a related field, along with a proven track record of delivering impactful data-driven solutions.\nThis position is based at our Bengaluru office.QualificationTotal 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['google', 'cloud platform', 'artificial intelligence', 'ml algorithms', 'platform architecture', 'chatbot', 'python', 'natural language processing', 'neural networks', 'microsoft azure', 'data architecture', 'machine learning', 'pca', 'sql', 'deep learning', 'tableau', 'data science', 'gcp', 'aws', 'ml']",2025-06-12 00:51:18
Data Architect,"NTT DATA, Inc.",3 - 7 years,Not Disclosed,['Bengaluru'],"Additional Career Level Description\n\n\nKnowledge and application\nApplies advanced wide-ranging experience and in-depth professional knowledge to develop and resolve complex models and procedures in creative way .\nDirects the application of existing principles and guides development of new policies and ideas.\nDetermines own methods and procedures on new assignments .\n\n\n\nProblem solving\nUnderstands and works on complex issues where analysis of situation or data requires an in-depth evaluation of variable factors, solutions may need to be devised from limited informatio n.\nExercises judgment in selecting methods, evaluating, adapting of complex techniques and evaluation criteria for obtaining results.\n\n\n\nInteraction\nFrequently advises key people outside own area of expertise on complex matters, using persuasion in delivering messages.\n\n\n\nImpact\nDevelops and manages operational initiatives to deliver tactical results and achieve medium-term goals.\n\n\n\nAccountability\nMay be accountable through team for delivery of tactical business targets .\nWork is reviewed upon completion and is consistent with departmental objectives.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Architecture', 'data modeling', 'Data Architect', 'artificial intelligence', 'sql']",2025-06-12 00:51:23
Data Architect,"NTT DATA, Inc.",8 - 13 years,Not Disclosed,['Chennai'],"Req ID: 324664\n\nWe are currently seeking a Data Architect to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDevelop and articulate long-term strategic goals for data architecture vision and establish data standards for enterprise systems.\n\nUtilize various cloud technologies, including Azure, AWS, GCP, and data platforms like Databricks and Snowflake.\n\nConceptualize and create an end-to-end vision outlining the seamless flow of data through successive stages.\n\nInstitute processes for governing the identification, collection, and utilization of corporate metadata, ensuring accuracy and validity.\n\nImplement methods and procedures for tracking data quality, completeness, redundancy, compliance, and continuous improvement.\n\nEvaluate and determine governance, stewardship, and frameworks for effective data management across the enterprise.\n\nDevelop comprehensive strategies and plans for data capacity planning, data security, life cycle data management, scalability, backup, disaster recovery, business continuity, and archiving.\n\nIdentify potential areas for policy and procedure enhancements, initiating changes where required for optimal data management.\n\nFormulate and maintain data models and establish policies and procedures for functional design.\n\nOffer technical recommendations to senior managers and technical staff in the development and implementation of databases and documentation.\n\nStay informed about upgrades and emerging database technologies through continuous research.\n\nCollaborate with project managers and business leaders on all projects involving enterprise data.\n\nDocument the data architecture and environment to ensure a current and accurate understanding of the overall data landscape.\n\nDesign and implement data solutions tailored to meet customer needs and specific use cases.\n\nProvide thought leadership by recommending the most suitable technologies and solutions for various use cases, spanning from the application layer to infrastructure.\n\nBasic Qualifications:\n\n8+ years of hands-on experience with various database technologies\n\n6+ years of experience with Cloud-based systems and Enterprise Data Architecture, driving end-to end technology solutions.\n\nExperience with Azure, Databricks, Snowflake\n\nKnowledgeable on concepts of GenAI\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nPossess certifications in AWS, Azure, and GCP to complement extensive hands-on experience.\n\nDemonstrated expertise with certifications in Snowflake.\n\nValuable ""Big 4"" Management Consulting experience or exposure to multiple industries.\n\nUndergraduate or graduate degree preferred.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['enterprise information architecture', 'microsoft azure', 'gcp', 'database creation', 'aws', 'snowflake', 'data life cycle management', 'metadata', 'data management', 'data security', 'data warehousing', 'data architecture', 'sql', 'data bricks', 'data quality', 'database implementation']",2025-06-12 00:51:29
Data Architect,Coforge,11 - 16 years,Not Disclosed,"['Noida', 'Greater Noida', 'Delhi / NCR']","-Data Architect Department:\nData & Analytics The Data Architect having more than 14 years of experience and should play a pivotal role in designing, developing, and governing scalable data architectures to support enterprise-wide data integration, analytics, and reporting.\nThis role will focus on creating unified data models, optimizing data pipelines, and ensuring compliance with regulatory standards (GDPR) using cloud-based platforms.\nThe ideal candidate is a strategic thinker with deep expertise in data modeling, cloud data platforms, and governance.",,,,"['Data Migration', 'Data Warehousing', 'Data Modeling', 'Informatica', 'SSIS', 'ETL Tool']",2025-06-12 00:51:35
AWS Data Architect (Standard),Infogain,12 - 14 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nDesign and implement scalable, reliable, and high-performance data architectures to support business\nneeds.\nDevelop and maintain real-time data streaming solutions using Kafka and other streaming\ntechnologies.\nUtilize AWS cloud services to build and manage data infrastructure, ensuring security, performance,\nand cost optimization.\nCreate efficient and optimized data models for structured and unstructured datasets.\nDevelop, optimize, and maintain SQL queries for data processing, analysis, and reporting.\nWork with cross-functional teams to define data requirements and implement solutions that align with\nbusiness goals.\nImplement ETL/ELT pipelines using Python and other relevant tools.\nEnsure data quality, consistency, and governance across the organization.\nTroubleshoot and resolve issues related to data pipelines and infrastructure.\nRequired Skills and Qualifications:\nExperience in Data Engineering and Architecture.\nProficiency in Python for data processing and automation.\nStrong expertise in AWS (S3, Redshift, Glue, Lambda, EMR, etc.) for cloud-based data solutions.\nHands-on experience with Kafka for real-time data streaming.\nDeep understanding of data modeling principles for transactional and analytical workloads.\nStrong knowledge of SQL for querying and performance optimization.\nExperience in building and maintaining ETL/ELT pipelines.\nFamiliarity with big data technologies like Spark, Hadoop, or Snowflake is a plus.\nStrong problem-solving skills and ability to work in a fast-paced environment.\nExcellent communication and stakeholder management skills\nEXPERIENCE\n12-14 Years\nSKILLS\nPrimary Skill: Data Engineering\nSub Skill(s): Data Engineering\nAdditional Skill(s): Kafka, Python, Data Modeling, ETL, Data Architecture, SQL, Redshift, Pyspark",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data modeling', 'Analytical', 'Data processing', 'Data quality', 'Stakeholder management', 'AWS', 'SQL', 'Python', 'Data architecture']",2025-06-12 00:51:42
Data Architect,.,7 - 12 years,20-35 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Job Description\nWe are seeking a highly skilled Azure Data Engineer with strong expertise in Data Architecture, PySpark/Python, Azure Databricks, and data streaming solutions. The ideal candidate will have hands-on experience in designing and implementing large-scale data pipelines, along with solid knowledge of data governance and data modeling.\nKey Responsibilities\nDesign, develop, and optimize PySpark/Python-based data streaming jobs on Azure Databricks.\nBuild scalable and efficient data pipelines for batch and real-time processing.\nImplement data governance policies, ensuring data quality, security, and compliance.\nDevelop and maintain data models (dimensional, relational, NoSQL) to support analytics and reporting.\nCollaborate with cross-functional teams (data scientists, analysts, and business stakeholders) to deliver data solutions.\nTroubleshoot performance bottlenecks and optimize Spark jobs for efficiency.\nEnsure best practices in CI/CD, automation, and monitoring of data workflows.\nMentor junior engineers and lead technical discussions (for senior/managerial roles).\nMandatory Skills & Experience\n5+ years of relevant experience as a Data Engineer/Analyst/Architect (8+ years for Manager/Lead positions).\nExpert-level proficiency in PySpark/Python and Azure Databricks (must have worked on real production projects).\nStrong experience in building and optimizing streaming data pipelines (Kafka, Event Hubs, Delta Lake, etc.).\n4+ years of hands-on experience in data governance & data modeling (ER, star schema, data vault, etc.).\nIn-depth knowledge of Azure Data Factory, Synapse, ADLS, and SQL/NoSQL databases.\nExperience with Delta Lake, Databricks Workflows, and performance tuning.\nFamiliarity with data security, metadata management, and lineage tracking.\nExcellent communication skills (must be able to articulate technical concepts clearly).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure Databricks', 'Data Modeling', 'Data Governance', 'Python', 'ETL']",2025-06-12 00:51:48
Data Architect- Snowflake & DBT,InfoCepts,3 - 5 years,Not Disclosed,['Chennai'],"InfoCepts is looking for Data Architect- Snowflake & DBT to join our dynamic team and embark on a rewarding career journey\nDesign and Development: Create and implement data warehouse solutions using Snowflake, including data modeling, schema design, and ETL (Extract, Transform, Load) processes\nPerformance Optimization: Optimize queries, performance-tune databases, and ensure efficient use of Snowflake resources for faster data retrieval and processing\nData Integration: Integrate data from various sources, ensuring compatibility, consistency, and accuracy\nSecurity and Compliance: Implement security measures and ensure compliance with data governance and regulatory requirements, including access control and data encryption\nMonitoring and Maintenance: Monitor system performance, troubleshoot issues, and perform routine maintenance tasks to ensure system health and reliability\nCollaboration: Collaborate with other teams, such as data engineers, analysts, and business stakeholders, to understand requirements and deliver effective data solutions\nSkills and Qualifications:Snowflake Expertise: In-depth knowledge and hands-on experience working with Snowflake's architecture, features, and functionalities\nSQL and Database Skills: Proficiency in SQL querying and database management, with a strong understanding of relational databases and data warehousing concepts\nData Modeling: Experience in designing and implementing effective data models for optimal performance and scalability\nETL Tools and Processes: Familiarity with ETL tools and processes to extract, transform, and load data into Snowflake\nPerformance Tuning: Ability to identify and resolve performance bottlenecks, optimize queries, and improve overall system performance\nData Security and Compliance: Understanding of data security best practices, encryption methods, and compliance standards (such as GDPR, HIPAA, etc)\nProblem-Solving and Troubleshooting: Strong analytical and problem-solving skills to diagnose and resolve issues within the Snowflake environment\nCommunication and Collaboration: Good communication skills to interact with cross-functional teams and effectively translate business requirements into technical solutions\nScripting and Automation: Knowledge of scripting languages (like Python) and experience in automating processes within Snowflake",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'python', 'hipaa', 'data security', 'data warehousing', 'data architecture', 'sql querying', 'relational databases', 'sql', 'gdpr', 'database design', 'database management', 'data modeling', 'etl tool', 'data governance', 'data warehousing concepts', 'etl', 'communication skills']",2025-06-12 00:51:54
Starburst Data Engineer/ Architect,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Starburst Data Engineer/Architect \nExpertise in Starburst and policy management like Ranger or equivalent.\nIn-depth knowledge of data modelling principles and techniques, including relational and dimensional.\nExcellent problem solving skills and the ability to troubleshoot and debug complex data related issues.\nStrong awareness of data tools and platforms like: Starburst, Snowflakes, Databricks and programming languages like SQL.\nIn-depth knowledge of data management principles, methodologies, and best practices with excellent analytical, problem-solving and decision making skills.\nDevelop, implement and maintain database systems using SQL.\nWrite complex SQL queries for integration with applications.\nDevelop and maintain data models (Conceptual, physical and logical) to meet organisational needs.\n\n\n ? \n\nDo\n\n1. Managing the technical scope of the project in line with the requirements at all stages\n\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\n\nb. Develop record management process and policies\n\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\n\nd. Providing sales data, proposals, data insights and account reviews to the client base\n\ne. Identify areas to increase efficiency and automation of processes\n\nf. Set up and maintain automated data processes\n\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\n\nh. Produce and track key performance indicators\n\n\n\n2. Analyze the data sets and provide adequate information\n\na. Liaise with internal and external clients to fully understand data content\n\nb. Design and carry out surveys and analyze survey data as per the customer requirement\n\nc. Analyze and interpret complex data sets relating to customer??s business and prepare reports for internal and external audiences using business analytics reporting tools\n\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\n\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\n\nf. Develop predictive models and share insights with the clients as per their requirement\n\n ? \n\nDeliver\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nStartburst.\n\nExperience5-8 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'sql', 'data bricks', 'data modeling', 'policy management', 'hive', 'snowflake', 'python', 'data mining', 'data warehousing', 'power bi', 'dbms', 'data architecture', 'sql server', 'plsql', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'etl', 'ssis', 'data integration', 'informatica']",2025-06-12 00:51:59
GCP Data Architect (Standard),Infogain,12 - 14 years,Not Disclosed,['Bengaluru'],"Position Summary Experienced Senior Data Engineer utilizing Big Data & Gogle Cloud technologies to develop large scale, on-cloud data processing pipelines and data warehouses. What you ll do Consult customers across the world on their data engineering needs around Adobes Customer Data Platform. Support pre-sales discsusions around complex and large scale cloud, data engineering solutions. Design custom solutions on cloud integrating Adobes solutions in scalable and performant manner. Deliver complex, large scale, enterprise grade on-clould data engineer and integration solutions in hand-on manner. Good to have Experience of consulting India customers. Multi-cloud expertise preferable AWS and GCP\nEXPERIENCE\n12-14 Years\nSKILLS\nPrimary Skill: Data Engineering\nSub Skill(s): Data Engineering\nAdditional Skill(s): Python, BigQuery",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SUB', 'GCP', 'Data Architect', 'Consulting', 'Cloud', 'Presales', 'Data processing', 'Adobe', 'big data', 'Python']",2025-06-12 00:52:02
Data Architect - AWS,Happiest Minds Technologies,10 - 15 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Roles and responsibilities\nWork closely with the Product Owners and stake holders to design the Technical Architecture for data platform to meet the requirements of the proposed solution.\nWork with the leadership to set the standards for software engineering practices within the machine learning engineering team and support across other disciplines\nPlay an active role in leading team meetings and workshops with clients.\nChoose and use the right analytical libraries, programming languages, and frameworks for each task.",,,,"['SQL', 'data architect', 'Python', 'Pyspark', 'Apache Airflow', 'GLUE', 'Kinesis', 'Amazon Redshift', 'Data Architecture Principles', 'Data Modeling', 'Data Warehousing', 'Athena', 'Lambda', 'AWS']",2025-06-12 00:52:06
Data Architect (Data Bricks),Diacto Technologies Pvt Ltd,5 - 9 years,Not Disclosed,['Pune( Baner )'],"Job Overview:\nDiacto is seeking an experienced and highly skilled Data Architect to lead the design and development of scalable and efficient data solutions. The ideal candidate will have strong expertise in Azure Databricks, Snowflake (with DBT, GitHub, Airflow), and Google BigQuery. This is a full-time, on-site role based out of our Baner, Pune office.\n\nQualifications:\nB.E./B.Tech in Computer Science, IT, or related discipline\nMCS/MCA or equivalent preferred\n\nKey Responsibilities:\nDesign, build, and optimize robust data architecture frameworks for large-scale enterprise solutions\nArchitect and manage cloud-based data platforms using Azure Databricks, Snowflake, and BigQuery\nDefine and implement best practices for data modeling, integration, governance, and security\nCollaborate with engineering and analytics teams to ensure data solutions meet business needs\nLead development using tools such as DBT, Airflow, and GitHub for orchestration and version control\nTroubleshoot data issues and ensure system performance, reliability, and scalability\nGuide and mentor junior data engineers and developers\n\nExperience and Skills Required:\n5 to12 years of experience in data architecture, engineering, or analytics roles\nHands-on expertise in Databricks, especially Azure Databricks\nProficient in Snowflake, with working knowledge of DBT, Airflow, and GitHub\nExperience with Google BigQuery and cloud-native data processing workflows\nStrong knowledge of modern data architecture, data lakes, warehousing, and ETL pipelines\nExcellent problem-solving, communication, and analytical skills\n\nNice to Have:\nCertifications in Azure, Snowflake, or GCP\nExperience with containerization (Docker/Kubernetes)\nExposure to real-time data streaming and event-driven architecture\n\nWhy Join Diacto Technologies?\nCollaborate with experienced data professionals and work on high-impact projects\nExposure to a variety of industries and enterprise data ecosystems\nCompetitive compensation, learning opportunities, and an innovation-driven culture\nWork from our collaborative office space in Baner, Pune\nHow to Apply:\nOption 1 (Preferred)\n\nCopy and paste the following link on your browser and submit your application for the automated interview process: -\n\nhttps://app.candidhr.ai/app/candidate/gAAAAABoRrTQoMsfqaoNwTxsE_qwWYcpcRyYJk7NzSUmO3LKb6rM-8FcU58CUPYQKc65n66feHor-TGdCEfyouj0NmKdgYcNbA==/\n\nOption 2\n\n1. Please visit our website's career section at https://www.diacto.com/careers/\n2. Scroll down to the ""Who are we looking for?"" section\n3. Find the listing for "" Data Architect (Data Bricks)"" and\n4. Proceed with the virtual interview by clicking on ""Apply Now.""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Azure Databricks', 'Airflow', 'Etl Pipelines', 'Github', 'google BigQuery', 'DBT', 'Data Security', 'Data Modeling', 'Elt', 'Data Governance']",2025-06-12 00:52:09
Data Architect,Quincy Compressor,6 - 12 years,Not Disclosed,['Pune'],"As a Data Architect, you'll design and optimize data architecture to ensure data is accurate, secure, and accessible. you'll collaborate across teams to shape the data strategy, implement governance, and promote best practices enabling the business to gain insights, innovate, and make data-driven decisions at scale.\n  Your responsibilites\nResponsible for defining the enterprise data architecture which streamlines, standardises, and enhances accessibility of organisational data.\nElicits data requirements from senior Business stakeholders and the broader IS function, translating their needs into conceptual, logical, and physical data models.\nOversees the effective integration of data from various sources, ensuring data quality and consistency.\nMonitors and optimises data performance, collaborating with Data Integration and Product teams to deliver changes that improve data performance.\nSupports the Business, Data Integration Platforms team and wider IS management to define a data governance framework that sets out how data will be governed, accessed, and secured across the organisation; supports the operation of the data governance model as a subject matter advisor.\nProvides advisory to Data Platform teams in defining the Data Platform architecture, providing advisory on metadata, data integration, business intelligence, and data storage needs.\nSupports the Data Integration Platforms team, and other senior IS stakeholders to define a data vision and strategy, setting out how the organisation will exploit its data for maximum Business value.\nBuilds and maintains a repository of data architecture artefacts (eg, data dictionary).\nWhat we're Looking For\nProven track record in defining enterprise data architectures, data models, and database/data warehouse solutions.\nEvidenced ability to advise on the use of key data platform architectural components (eg, Azure Lakehouse, Data Bricks, etc) to deliver and optimise the enterprise data architecture.\nExperience in data integration technologies, real-time data ingestion, and API-based integrations.\nExperience in SQL and other database management systems.\nStrong problem-solving skills for interpreting complex data requirements and translating them into feasible data architecture solutions and models.\nExperience in supporting the definition of an enterprise data vision and strategy, advising on implications and/or uplifts required to the enterprise data architecture.\nExperience designing and establishing data governance models and data management practices, ensuring data is correct and secure whilst still being accessible, in line with regulations and wider organisational policies.\nAble to present complex data-related initiatives and issues to senior non-data conversant audiences.\nProven experience working with AI and Machine Learning models preferred, but not essential.\nWhat We Can Offer You\nWe support your growth within the role, department, and across the company through internal opportunities.\nWe offer a hybrid working model, allowing you to combine remote work with the opportunity to connect with your team in modern, welcoming office spaces.\nWe encourage continuous learning with access to online platforms (eg, LinkedIn Learning), language courses, soft skills training, and various we'llbeing initiatives, including workshops and webinars.\nJoin a diverse and inclusive work environment where your ideas are valued and your contributions make a difference.",Industry Type: Industrial Equipment / Machinery,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['metadata', 'Architecture', 'Data management', 'Data Architect', 'data governance', 'Data quality', 'Business intelligence', 'Information technology', 'SQL', 'Data architecture']",2025-06-12 00:52:13
Data Architect - Supply Chain,Exxon Mobil Corporation,3 - 8 years,Not Disclosed,['Bengaluru'],"About us\nWe invite you to bring your ideas to ExxonMobil to help create sustainable solutions that improve quality of life and meet society s evolving needs. Learn more about our What and our Why and how we can work together .\nExxonMobil s affiliates in India\nExxonMobil s affiliates have offices in India in Bengaluru, Mumbai and the National Capital Region.\nExxonMobil s affiliates in India supporting the Product Solutions business engage in the marketing, sales and distribution of performance as well as specialty products across chemicals and lubricants businesses. The India planning teams are also embedded with global business units for business planning and analytics.",,,,"['ERP', 'SAP', 'Networking', 'Data management', 'Data modeling', 'XML', 'Agile', 'JSON', 'SQL', 'Python']",2025-06-12 00:52:17
Data Architect Data Architect,Mufg Pension & Market Services,7 - 10 years,Not Disclosed,['Mumbai'],"Overview\nThe Data Architect will play a key role in defining and overseeing the enterprise data model, ensuring alignment with MUFG s strategic objectives. The Data Architect will be responsible for setting standards and frameworks for data classification and ensuring data flows across applications and data provenance is fully understood. These frameworks will empower the organisation to harness its data effectively for reporting, machine learning (ML), artificial intelligence (AI), auditing, and other business-critical initiatives. Working within the Enterprise Architecture practice, you will collaborate across teams globally.\n\nKey Accountabilities and main responsibilities\nStrategic Focus\nDefine and maintain the enterprise data model at an abstract level, ensuring alignment with MUFG s strategic goals and business outcomes.\nCollaborate with the Enterprise Architecture practice to shape data frameworks that align with the organisations overarching IT strategy.\nDevelop standards for data classification and data provenance to support global initiatives such as reporting, ML, AI, and auditing.\nProvide thought leadership on emerging trends, technologies, and best practices for enterprise data architecture.\nInfluence and build consensus among stakeholders across EUROPE/UK & ANZ regions to drive the adoption of strategic data practices.\nOperational Management\nEstablish a common data model, data dictionary and data business rules that represents the CM business domain\nMap and document data flows across applications to ensure consistency, efficiency, and security in data management.\nOversee adherence to data standards, facilitating governance and ensuring compliance with regional regulations (e.g., GDPR).\nMonitor and ensure traceability of data sources and transformations through robust data provenance frameworks.\nPrepare and maintain comprehensive documentation for the enterprise data model, data flows, classifications, and governance standards.\nCollaborate with teams globally to address specific operational challenges in data architecture and maintain alignment across regions.\nGovernance & Risk\nPlay an active role in technology governance as part of the Architecture Governance Board (AGB).\nThe above list of key accountabilities is not an exhaustive list and may change from time-to-time based on business needs.\n\nExperience & Personal Attributes\nProven experience (7-10 years) as a Data Architect or in a similar strategic role, ideally within financial services or a global organisation.\nDeep understanding of abstract data modelling, data flows across complex application landscapes, and enterprise data standards.\nExpertise in data governance, security, and compliance frameworks, with a strong knowledge of GDPR and regional standards.\nFamiliarity with tools and methodologies that enable reporting, ML, AI, and auditing across an enterprise.\nExceptional communication skills to engage with technical and non-technical stakeholders across EMEA and APAC regions.\nA strategic, collaborative mindset with the ability to influence and build consensus across diverse teams.\nBachelor s degree in Computer Science, Information Systems, or a related field (preferred).\nEnthusiastic attitude, discipline and approach.\nCan demonstrate independent working. Ability to work under pressure with a view to attaining monthly targets.\n\nOverview\nThe Data Architect will play a key role in defining and overseeing the enterprise data model, ensuring alignment with MUFG s strategic objectives. The Data Architect will be responsible for setting standards and frameworks for data classification and ensuring data flows across applications and data provenance is fully understood. These frameworks will empower the organisation to harness its data effectively for reporting, machine learning (ML), artificial intelligence (AI), auditing, and other business-critical initiatives. Working within the Enterprise Architecture practice, you will collaborate across teams globally.\n\nKey Accountabilities and main responsibilities\nStrategic Focus\nDefine and maintain the enterprise data model at an abstract level, ensuring alignment with MUFG s strategic goals and business outcomes.\nCollaborate with the Enterprise Architecture practice to shape data frameworks that align with the organisations overarching IT strategy.\nDevelop standards for data classification and data provenance to support global initiatives such as reporting, ML, AI, and auditing.\nProvide thought leadership on emerging trends, technologies, and best practices for enterprise data architecture.\nInfluence and build consensus among stakeholders across EUROPE/UK & ANZ regions to drive the adoption of strategic data practices.\nOperational Management\nEstablish a common data model, data dictionary and data business rules that represents the CM business domain\nMap and document data flows across applications to ensure consistency, efficiency, and security in data management.\nOversee adherence to data standards, facilitating governance and ensuring compliance with regional regulations (e.g., GDPR).\nMonitor and ensure traceability of data sources and transformations through robust data provenance frameworks.\nPrepare and maintain comprehensive documentation for the enterprise data model, data flows, classifications, and governance standards.\nCollaborate with teams globally to address specific operational challenges in data architecture and maintain alignment across regions.\nGovernance & Risk\nPlay an active role in technology governance as part of the Architecture Governance Board (AGB).\nThe above list of key accountabilities is not an exhaustive list and may change from time-to-time based on business needs.\n\nExperience & Personal Attributes\nProven experience (7-10 years) as a Data Architect or in a similar strategic role, ideally within financial services or a global organisation.\nDeep understanding of abstract data modelling, data flows across complex application landscapes, and enterprise data standards.\nExpertise in data governance, security, and compliance frameworks, with a strong knowledge of GDPR and regional standards.\nFamiliarity with tools and methodologies that enable reporting, ML, AI, and auditing across an enterprise.\nExceptional communication skills to engage with technical and non-technical stakeholders across EMEA and APAC regions.\nA strategic, collaborative mindset with the ability to influence and build consensus across diverse teams.\nBachelor s degree in Computer Science, Information Systems, or a related field (preferred).\nEnthusiastic attitude, discipline and approach.\nCan demonstrate independent working. Ability to work under pressure with a view to attaining monthly targets.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data dictionary', 'Data management', 'Enterprise architecture', 'IT strategy', 'Artificial Intelligence', 'Machine learning', 'business rules', 'Financial services', 'Data architecture']",2025-06-12 00:52:21
Data and Analytics Architect - L1,Wipro,8 - 10 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of the role is to define and develop Enterprise Data Structure along with Data Warehouse, Master Data, Integration and transaction processing with maintaining and strengthening the modelling standards and business information.\n\n\n\nDo\n1. Define and Develop Data Architecture that aids organization and clients in new/ existing deals\na. Partnering with business leadership (adopting the rationalization of the data value chain) to provide strategic, information-based recommendations to maximize the value of data and information assets, and protect the organization from disruptions while also embracing innovation\nb. Assess the benefits and risks of data by using tools such as business capability models to create an data-centric view to quickly visualize what data matters most to the organization, based on the defined business strategy\nc. Create data strategy and road maps for the Reference Data Architecture as required by the clients\nd. Engage all the stakeholders to implement data governance models and ensure that the implementation is done based on every change request\ne. Ensure that the data storage and database technologies are supported by the data management and infrastructure of the enterprise\nf. Develop, communicate, support and monitor compliance with Data Modelling standards\ng. Oversee and monitor all frameworks to manage data across organization\nh. Provide insights for database storage and platform for ease of use and least manual work\ni. Collaborate with vendors to ensure integrity, objectives and system configuration\nj. Collaborate with functional & technical teams and clients to understand the implications of data architecture and maximize the value of information across the organization\nk. Presenting data repository, objects, source systems along with data scenarios for the front end and back end usage\nl. Define high-level data migration plans to transition the data from source to target system/ application addressing the gaps between the current and future state, typically in sync with the IT budgeting or other capital planning processes\nm. Knowledge of all the Data service provider platforms and ensure end to end view.\nn. Oversight all the data standards/ reference/ papers for proper governance\no. Promote, guard and guide the organization towards common semantics and the proper use of metadata\n\n\n\np. Collecting, aggregating, matching, consolidating, quality-assuring, persisting and distributing such data throughout an organization to ensure a common understanding, consistency, accuracy and control\nq. Provide solution of RFPs received from clients and ensure overall implementation assurance\ni. Develop a direction to manage the portfolio of all the databases including systems, shared infrastructure services in order to better match business outcome objectives\nii. Analyse technology environment, enterprise specifics, client requirements to set a collaboration solution for the big/small data\niii. Provide technical leadership to the implementation of custom solutions through thoughtful use of modern technology\niv. Define and understand current issues and problems and identify improvements\nv. Evaluate and recommend solutions to integrate with overall technology ecosystem keeping consistency throughout\nvi. Understand the root cause problem in integrating business and product units\nvii. Validate the solution/ prototype from technology, cost structure and customer differentiation point of view\nviii. Collaborating with sales and delivery leadership teams to identify future needs and requirements\nix. Tracks industry and application trends and relates these to planning current and future IT needs\n\n\n\n2. Building enterprise technology environment for data architecture management\na. Develop, maintain and implement standard patterns for data layers, data stores, data hub & lake and data management processes\nb. Evaluate all the implemented systems to determine their viability in terms of cost effectiveness\nc. Collect all the structural and non-structural data from different places integrate all the data in one database form\nd. Work through every stage of data processing: analysing, creating, physical data model designs, solutions and reports\ne. Build the enterprise conceptual and logical data models for analytics, operational and data mart structures in accordance with industry best practices\nf. Implement the best security practices across all the data bases based on the accessibility and technology\ng. Strong understanding of activities within primary discipline such as Master Data Management (MDM), Metadata Management and Data Governance (DG)\nh. Demonstrate strong experience in Conceptual, Logical and physical database architectures, design patterns, best practices and programming techniques around relational data modelling and data integration\n\n\n\n3. Enable Delivery Teams by providing optimal delivery solutions/ frameworks\na. Build and maintain relationships with delivery and practice leadership teams and other key stakeholders to become a trusted advisor\nb. Define database physical structure, functional capabilities, security, back-up and recovery specifications\nc. Develops and establishes relevant technical, business process and overall support metrics (KPI/SLA) to drive results\nd. Monitor system capabilities and performance by performing tests and configurations\ne. Integrate new solutions and troubleshoot previously occurred errors\nf. Manages multiple projects and accurately reports the status of all major assignments while adhering to all project management standards\ng. Identify technical, process, structural risks and prepare a risk mitigation plan for all the projects\nh. Ensure quality assurance of all the architecture or design decisions and provides technical mitigation support to the delivery teams\ni. Recommend tools for reuse, automation for improved productivity and reduced cycle times\nj. Help the support and integration team for better efficiency and client experience for ease of use by using AI methods.\nk. Develops trust and builds effective working relationships through respectful, collaborative engagement across individual product teams\nl. Ensures architecture principles and standards are consistently applied to all the projects\nm. Ensure optimal Client Engagement\ni. Support pre-sales team while presenting the entire solution design and its principles to the client\nii. Negotiate, manage and coordinate with the client teams to ensure all requirements are met\niii. Demonstrate thought leadership with strong technical capability in front of the client to win the confidence and act as a trusted advisor\nMandatory Skills: Prophecy.AI.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Prophecy.AI', 'metadata management', 'design patterns', 'data governance', 'data warehousing', 'AI methods', 'master data management']",2025-06-12 00:52:25
Principal Architect-Data & Cloud,Tothr,12 - 18 years,40-45 Lacs P.A.,"['Chennai', 'Bengaluru']","Experience in Hadoop, GCP/AWS/Azure Cloud\nETL technologies on Cloud like Spark, Pyspark/Scala, Dataflow\nETL tools like Informatica/DataStage/OWB/Talend\nExperience inS3, Cloud Storage, Athena, Glue, Sqoop, Flume, Hive, Kafka, Pub-Sub\n\nRequired Candidate profile\nMore than 10 years of experience in Technical, Solutioning, and Analytical roles.\n5+ years of experience in building and managing Data Lakes, Data Warehouse, Data Integration,",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Cloud', 'Technical', 'Analytical', 'MongoDB', 'data lake', 'Pyspark', 'Data Migration', 'Data Warehousing', 'Data Integration']",2025-06-12 00:52:29
Big Data Architect,Meritus Management Service,8 - 10 years,20-32.5 Lacs P.A.,"['Nagpur', 'Pune']","Design and implement scalable Big Data architecture and pipelines using tools like Hadoop, Spark, Kafka, and Hive.\nCollaborate with cross-functional teams to build real-time/batch systems, ensure data quality and governance.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Hadoop', 'SQL', 'architect level exposure', 'Pyspark']",2025-06-12 00:52:32
Senior Manager - Data Architect,Protean eGov Technologies,7 - 12 years,15-25 Lacs P.A.,"['Mumbai', 'Pune', 'Mumbai (All Areas)']","*Seeking a Data Architect to design scalable data models, build pipelines, ensure governance & security, and optimize performance across cloud/data platforms. Collaborate with teams, drive innovation, lead data strategy & mentor others.",Industry Type: FinTech / Payments,"Department: UX, Design & Architecture","Employment Type: Full Time, Permanent","['RDBMS', 'Data Warehousing', 'Bigquery', 'Postgresql', 'Redshift Aws', 'Snowflake', 'Hadoop', 'google cloud', 'Spark', 'Oracle', 'Data Lake Storage']",2025-06-12 00:52:35
Data Solution Architect,Maveric,13 - 20 years,Not Disclosed,"['Chennai', 'Bengaluru']","Position Overview\nWe are looking for a highly experienced and versatile Solution Architect Data to lead the solution design and delivery of next-generation data solutions for our BFS clients. The ideal candidate will have a strong background in data architecture and engineering, deep domain expertise in financial services, and hands-on experience with cloud-native data platforms and modern data analytics tools. The role will require architecting solutions across Retail, Corporate, Wealth, and Capital Markets, as well as Payments, Lending, and Onboarding journeys. Possession of Data Analytics and Exposure to Data regulatory domain will be of distinct advantage. Hands on experience of AI & Gen AI enabling data related solution will be a distinct advantage for the position.",,,,"['Data Quality', 'Data Engineering', 'Data Governance', 'GenAI']",2025-06-12 00:52:38
Senior Associate Data Scientist,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Data Scientist is a seasoned subject matter expert, tasked with participating in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nAccountable for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources.\nAccountable for providing meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nAccountable for performing analysis using programming languages or statistical packages such as Python, pandas etc.\nDesigns scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualize the output of the models.\nCreates documentation around processes and procedures and manages code reviews.\nAccountable for ensuring that industry standards best practices are applied to development activities.\nKnowledge and Attributes:\nSeasoned in data modelling, statistical methods and machine learning techniques.\nAbility to thrive in a dynamic, fast-paced environment.\nQuantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nGood understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nAbility to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nAbility to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\nAble to apply data science principles through a business lens.\nDesire to create strategies and solutions that challenge and expand the thinking of peers and business stakeholders.\nAcademic Qualifications and Certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming (Python) certification preferred.\nAgile certification preferred.\nRequired Experience:\nSeasoned experience in a data science position in a corporate environment and/or related industry.\nSeasoned experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nSeasoned experience in programming languages (Python, etc.).\nSeasoned experience working in databases (MySQL, Microsoft SQL Server, Azure Synapse, MongoDB)\nSeasoned experience working with and creating data architectures.\nSeasoned experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nSeasoned experience visualizing and/or presenting data for stakeholder use and reuse across the business.\nSeasoned experience on working with API (creating and using APIs)\nAutomation experience using Python scripting, UIPath, Selenium, PowerAutomate.\nSeasoned experience working on Linux operating system (Ubuntu)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure Synapse', 'Microsoft SQL Server', 'MySQL', 'Python scripting', 'PowerAutomate', 'Linux operating system', 'MongoDB', 'Selenium', 'Python', 'UIPath']",2025-06-12 00:52:42
Azure Data Architect,Syren Technologies,10 - 18 years,Not Disclosed,[],"About Syren Cloud\n\nSyren Cloud Technologies is a cutting-edge company specializing in supply chain solutions and data engineering. Their intelligent insights, powered by technologies like AI and NLP, empower organizations with real-time visibility and proactive decision-making. From control towers to agile inventory management, Syren unlocks unparalleled success in supply chain management.\n\nRole Summary",,,,"['Pyspark', 'Azure', 'Architecture', 'Data Bricks']",2025-06-12 00:52:46
Principal - Data Architect,Affine Analytics,8 - 12 years,Not Disclosed,['Chennai'],"We are seeking a highly skilled Data Architect to design and implement robust, scalable, and secure data solutions on AWS Cloud. The ideal candidate should have expertise in AWS services, data modeling, ETL processes, and big data technologies, with hands-on experience in Glue, DMS, Python, PySpark, and MPP databases like Snowflake, Redshift, or Databricks.\nKey Responsibilities:\nArchitect and implement data solutions leveraging AWS services such as EC2, S3, IAM, Glue (Mandatory), and DMS for efficient data processing and storage.",,,,"['Python', 'S3', 'AWS Glue', 'DMS', 'SQL Server', 'Redshift', 'Glue', 'IAM', 'EC2', 'Snowflake', 'Databricks', 'Oracle', 'Lambda']",2025-06-12 00:52:51
Data & AI Technical Solution Architects,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Bengaluru'],"Req ID: 323775\n\nWe are currently seeking a Data & AI Technical Solution Architects to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works""",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'solution design', 'gcp', 'gcp cloud', 'ml']",2025-06-12 00:52:55
Data & AI Technical Solution Architects,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Pune'],"Req ID: 323754\n\nWe are currently seeking a Data & AI Technical Solution Architects to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'solution design', 'gcp', 'gcp cloud', 'ml']",2025-06-12 00:52:59
Data & AI Technical Solution Architects,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Hyderabad'],"Req ID: 323774\n\nWe are currently seeking a Data & AI Technical Solution Architects to join our team in Hyderabad, Telangana (IN-TG), India (IN).\n\n""Job DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works""",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'solution design', 'gcp', 'gcp cloud', 'ml']",2025-06-12 00:53:03
Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location),Allegis Group,5 - 10 years,Not Disclosed,[],"Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location)\nJob Location : Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\n\n5 -20 years of relevant hands on development experience. And 4+ years as Azure Data Engineering role\nProficient in Azure technologies like ADB, ADF, SQL(capability of writing complex SQL queries), ADB, PySpark, Python, Synapse, Delta Tables, Unity Catalog\nHands on in Python, PySpark or Spark SQL\nHands on in Azure Analytics and DevOps\nTaking part in Proof of Concepts (POCs) and pilot solutions preparation\nAbility to conduct data profiling, cataloguing, and mapping for technical design and construction of technical data flows\nExperience in business processing mapping of data and analytics solutions",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Analytics', 'Azure Data Engineering', 'Azure Databricks', 'Devops', 'Python', 'Azure Data Factory', 'Pyspark', 'Azure', 'Adb']",2025-06-12 00:53:06
Senior Data Scientist,"NTT DATA, Inc.",2 - 6 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Senior Data Scientist is an advanced subject matter expert, tasked with taking accountability in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nKey responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nResponsible for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources\nProvides meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nDirects scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualizes the output of the models.\nResponsible for ensuring that industry standards best practices are applied to development activities.\n\nTo thrive in this role, you need to have:\nAdvanced understanding of data modelling, statistical methods and machine learning techniques.\nStrong ability to thrive in a dynamic, fast-paced environment.\nStrong quantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nAdvanced understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nStrong ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nStrong ability to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\n\nAcademic qualifications and certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming certification preferred.\nAgile certification preferred.\n\nRequired experience:\nAdvanced demonstrated experience in a data science position in a corporate environment and/or related industry.\nAdvanced demonstrated experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nAdvanced demonstrated experience in programming languages (R, Python, etc.).\nAdvanced demonstrated experience working with and creating data architectures.\nAdvanced demonstrated experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nAdvanced demonstrated experience visualizing and/or presenting data for stakeholder use and reuse across the business.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'data analytics', 'natural language processing', 'data modeling', 'data mining', 'statistical modeling', 'data architecture', 'machine learning']",2025-06-12 00:53:09
Senior GenAI Data Engineer,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,"['New Delhi', 'Chennai', 'Bengaluru']","Your day at NTT DATA\nSenior GenAI Data Engineer\nWe are seeking an experienced Senior Data Engineer to join our team in delivering cutting-edge Generative AI (GenAI) solutions to clients. The successful candidate will be responsible for designing, developing, and deploying data pipelines and architectures that support the training, fine-tuning, and deployment of LLMs for various industries. This role requires strong technical expertise in data engineering, problem-solving skills, and the ability to work effectively with clients and internal teams.\nWhat you'll be doing\nKey Responsibilities:\nDesign, develop, and manage data pipelines and architectures to support GenAI model training, fine-tuning, and deployment\nData Ingestion and Integration: Develop data ingestion frameworks to collect data from various sources, transform, and integrate it into a unified data platform for GenAI model training and deployment.\nGenAI Model Integration: Collaborate with data scientists to integrate GenAI models into production-ready applications, ensuring seamless model deployment, monitoring, and maintenance.\nCloud Infrastructure Management: Design, implement, and manage cloud-based data infrastructure (e.g., AWS, GCP, Azure) to support large-scale GenAI workloads, ensuring cost-effectiveness, security, and compliance.\nWrite scalable, readable, and maintainable code using object-oriented programming concepts in languages like Python, and utilize libraries like Hugging Face Transformers, PyTorch, or TensorFlow\nPerformance Optimization: Optimize data pipelines, GenAI model performance, and infrastructure for scalability, efficiency, and cost-effectiveness.\nData Security and Compliance: Ensure data security, privacy, and compliance with regulatory requirements (e.g., GDPR, HIPAA) across data pipelines and GenAI applications.\nClient Collaboration: Collaborate with clients to understand their GenAI needs, design solutions, and deliver high-quality data engineering services.\nInnovation and R&D: Stay up to date with the latest GenAI trends, technologies, and innovations, applying research and development skills to improve data engineering services.\nKnowledge Sharing: Share knowledge, best practices, and expertise with team members, contributing to the growth and development of the team.\nRequirements:\nBachelors degree in computer science, Engineering, or related fields (Master's recommended)\nExperience with vector databases (e.g., Pinecone, Weaviate, Faiss, Annoy) for efficient similarity search and storage of dense vectors in GenAI applications\n5+ years of experience in data engineering, with a strong emphasis on cloud environments (AWS, GCP, Azure, or Cloud Native platforms)\nProficiency in programming languages like SQL, Python, and PySpark\nStrong data architecture, data modeling, and data governance skills\nExperience with Big Data Platforms (Hadoop, Databricks, Hive, Kafka, Apache Iceberg), Data Warehouses (Teradata, Snowflake, BigQuery), and lakehouses (Delta Lake, Apache Hudi)\nKnowledge of DevOps practices, including Git workflows and CI/CD pipelines (Azure DevOps, Jenkins, GitHub Actions)\nExperience with GenAI frameworks and tools (e.g., TensorFlow, PyTorch, Keras)\nNice to have:\nExperience with containerization and orchestration tools like Docker and Kubernetes\nIntegrate vector databases and implement similarity search techniques, with a focus on GraphRAG is a plus\nFamiliarity with API gateway and service mesh architectures\nExperience with low latency/streaming, batch, and micro-batch processing\nFamiliarity with Linux-based operating systems and REST APIs\nLocation: Delhi or Bangalore\nWorkplace type:\nHybrid Working",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GenAI', 'hive', 'continuous integration', 'kubernetes', 'ci/cd', 'pyspark', 'data architecture', 'sql', 'docker', 'tensorflow', 'git', 'data modeling', 'gcp', 'devops', 'linux', 'jenkins', 'pytorch', 'keras', 'hadoop', 'bigquery', 'python', 'microsoft azure', 'data engineering', 'data bricks', 'data governance', 'aws']",2025-06-12 00:53:13
Master Data Management Architect,K-logix Partnering Solutions,8 - 13 years,Not Disclosed,[],"Bachelor'sMaster's\nOverview:\n\nWe are seeking a highly skilled and experienced Celonis MDM Data Architect to lead the design, implementation, and optimization of our Master Data Management (MDM) solutions in alignment with Celonis Process Mining and Execution Management System (EMS) capabilities.\nThe ideal candidate will play a key role in bridging data architecture and business process insights, ensuring data quality, consistency, and governance across the enterprise.\n\nKey Responsibilities:\nDesign and implement MDM architecture and data models aligned with enterprise standards and best practices.\n• Lead the integration of Celonis with MDM platforms to drive intelligent process automation, data governance, and operational efficiencies.\n• Collaborate with business stakeholders and data stewards to define MDM policies, rules, and processes.\n• Support data profiling, data cleansing, and data harmonization efforts to improve master data quality.\n• Work closely with Celonis analysts, data engineers, and process owners to deliver actionable insights based on MDM-aligned process data.\n• Develop and maintain scalable, secure, and high-performance data pipelines and integration architectures.\n• Translate business requirements into technical solutions, ensuring alignment with both MDM and Celonis data models.\n• Create and maintain data architecture documentation, data dictionaries, and metadata repositories.\n• Monitor and optimize the performance of MDM systems and Celonis EMS integrations.\nQualifications:\nBachelors or Masters degree in Computer Science, Information Systems, Data Engineering, or a related field.\n• 7+ years of experience in data architecture, MDM, or enterprise data management.\n• 2+ years of hands-on experience with Celonis and process mining tools.\n• Proficient in MDM platforms (e.g., Informatica MDM, SAP MDG, Oracle MDM, etc.).\n• Strong knowledge of data modeling, data governance, and metadata management.\n• Proficiency in SQL, data integration tools (e.g., ETL/ELT platforms), and APIs.\n• Deep understanding of business process management and data-driven transformation initiatives.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Celonis', 'MDM', 'Master Data Management', 'ETL', 'Elt']",2025-06-12 00:53:18
Network Architect- Data Center,Konverge Technologies,5 - 10 years,Not Disclosed,['Gurugram'],"POSITION SUMMARY:\nWe are seeking an experienced Network Architect with expertise in Cisco Application Centric Infrastructure (ACI), Software-Defined Access (SDA), and Data Center networking technologies. The ideal candidate will design, implement, and optimise scalable, secure, and automated network solutions for enterprise and data centre environments.\n\nKey Roles & Responsibilities:\nIn this position, you will be required to:\nArchitect and design Cisco ACI-based data center network solutions, ensuring scalability and automation.\nImplement and optimize Cisco SDA for campus networking with DNA Center and ISE.\nDefine high-level network strategies, roadmaps, and architectures aligned with business objectives.\nLead data center network transformations, ensuring high availability and security.\nDeploy and configure Cisco Nexus switches, UCS, and Hyper converged infrastructure for data center environments.\nIntegrate and manage Cisco DNA Center for automation, policy enforcement, and assurance.\nImplement Zero Trust principles and micro-segmentation strategies across ACI and SDA networks.\nCollaborate with security, cloud, and infrastructure teams to develop integrated solutions.\nEvaluate emerging network technologies and provide recommendations for continuous improvement.\nDevelop and maintain technical documentation, including HLDs, LLDs, and operational guidelines.\nTroubleshoot complex network issues and provide expert-level support to operational teams.\nProvide mentorship and training to junior engineers and network teams.\n\nEducation/Experience:\nBachelors or Master’s in IT, or a related field.\n10+ years of experience in network engineering and architecture, with expertise in Cisco ACI, SDA, and Data Center technologies.\nDeep knowledge of Cisco ACI, APIC controllers, and multi-site fabric architectures.",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['SDA', 'Vxlan', 'Apic', 'Cisco Aci', 'Network Architecture']",2025-06-12 00:53:21
Sr. Network Architect - Data Centre,Konverge Technologies,5 - 10 years,Not Disclosed,['Gurugram( Civil Lines )'],"Department: IT\nTitle: Network Architect (Minimum 10 Yrs. EXP)\nLocation: Gurgaon\nStatus: Regular, Full-Time:\nTravel: Across the Globe for Project and Business meetings\n\nPOSITION SUMMARY:\nWe are seeking an experienced Network Architect with expertise in Cisco Application Centric Infrastructure (ACI), Software-Defined Access (SDA), and Data Center networking technologies. The ideal candidate will design, implement, and optimise scalable, secure, and automated network solutions for enterprise and data centre environments.\nKey Roles & Responsibilities:\nIn this position, you will be required to:\nArchitect and design Cisco ACI-based data center network solutions, ensuring scalability and automation.\nImplement and optimize Cisco SDA for campus networking with DNA Center and ISE.\nDefine high-level network strategies, roadmaps, and architectures aligned with business objectives.\nLead data center network transformations, ensuring high availability and security.\nDeploy and configure Cisco Nexus switches, UCS, and Hyper converged infrastructure for data center environments.\nIntegrate and manage Cisco DNA Center for automation, policy enforcement, and assurance.\nImplement Zero Trust principles and micro-segmentation strategies across ACI and SDA networks.\nCollaborate with security, cloud, and infrastructure teams to develop integrated solutions.\nEvaluate emerging network technologies and provide recommendations for continuous improvement.\nDevelop and maintain technical documentation, including HLDs, LLDs, and operational guidelines.\nTroubleshoot complex network issues and provide expert-level support to operational teams.\nProvide mentorship and training to junior engineers and network teams.\nEducation/Experience:\nBachelors or Masters in IT, or a related field.\n10+ years of experience in network engineering and architecture, with expertise in Cisco ACI, SDA, and Data Center technologies.\nDeep knowledge of Cisco ACI, APIC controllers, and multi-site fabric architectures.",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['SDA', 'Aci', 'Apic', 'Data Center Design', 'High Level Design', 'Low Level Design', 'Network Design', 'Ise', 'Architectural Design']",2025-06-12 00:53:24
Salesforce Data Cloud Architect,"NTT DATA, Inc.",4 - 8 years,Not Disclosed,"['Chennai', 'Gurugram', 'Bengaluru']","We are currently seeking a Salesforce Data Cloud Architect to join our team in ""‹""‹""‹""‹""‹""‹""‹Hyderabad, Telangana, India.\n\n\nSalesforce Data Cloud Expertise: Extensive knowledge of Salesforce Data Cloud features, capabilities, and best practices.\n\n\nData Modeling: Strong experience in designing and implementing data models.\n\n\nData Integration: Experience with data integration tools and techniques.\n\n\nData Quality: Understanding of data quality concepts and practices.\n\n\nData Governance: Knowledge of data governance principles and practices.\n\n\nSQL: Proficiency in SQL for data querying and manipulation.\n\n\nProblem-Solving: Strong analytical and problem-solving skills.\n\n\nCommunication: Excellent communication and collaboration skills.\n\n\nLocation - Bengaluru,Chennai,Gurugram,Hyderabad,Noida,Pune",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'salesforce', 'data modeling', 'data governance', 'data integration', 'c#', 'python', 'business analysis', 'sharepoint', 'user stories', 'news writing', 'javascript', 'editing', 'react.js', 'java', 'product management', 'content writing', 'scrum', 'html', 'agile', 'etl', 'jira']",2025-06-12 00:53:28
Master Data Management Data Architect,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible for designing, building, maintaining, analyzing, and interpreting data deliver actionable insights that drive business decisions. This role involves working with large datasets, developing reports, supporting and driving data governance initiatives, and visualizing data to ensure data is accessible, reliable, and efficiently managed. The ideal candidate has deep technical skills and provides administration support for Master Data Management (MDM) and Data Quality platform, including solution architecture, inbound/outbound data integration (ETL), Data Quality (DQ), and maintenance/tuning of match rules.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing\nCollaborate and communicate with MDM Developers, Data Architects, Product teams, Business SMEs, and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve complex data-related challenges\nAdhere to standard processes for coding, testing, and designing reusable code/component\nParticipate in sprint planning meetings and provide estimations on technical implementation\nAs a SME, work with the team on MDM related product installation, configuration, customization and optimization\nResponsible for the understanding, documentation, maintenance, and additional creation of master data related data-models (conceptual, logical, and physical) and database structures\nReview technical model specifications and participate in data quality testing\nCollaborate with Data Quality & Governance Analyst and Data Governance Organization to monitor and preserve the master data quality\nCreate and maintain system specific master data data-dictionaries for domains in scope\nArchitect MDM Solutions, including data modeling and data source integrations from proof-of-concept through development and delivery\nDevelop the architectural design for Master Data Management domain development, base object integration to other systems and general solutions as related to Master Data Management\nDevelop and deliver solutions individually or as part of a development team\nApproves code reviews and technical work\nMaintains compliance with change control, SDLC and development standards\nContribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience.\nPreferred Qualifications:\nExpertise in architecting and designing Master Data Management (MDM) solutions.\nPractical experience with AWS Cloud, Databricks, Apache Spark, workflow orchestration, and optimizing big data processing performance.\nFamiliarity with enterprise source systems and consumer systems for master and reference data, such as CRM, ERP, and Data Warehouse/Business Intelligence.\nAt least 2 to 3 years of experience as an MDM developer using Informatica MDM or Reltio MDM, along with strong proficiency in SQL.\n\n\nGood-to-Have Skills:\nExperience with ETL tools such as Apache Spark, and various Python packages related to data processing, machine learning model development.\nGood understanding of data modeling, data warehousing, and data integration concepts.\nExperience with development using Python, React JS, cloud data platforms.\nCertified Data Engineer / Data Analyst (preferred on Databricks or cloud environments).\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Business Intelligence', 'Data Warehouse', 'cloud data platforms', 'Databricks', 'ETL', 'React JS', 'Python']",2025-06-12 00:53:31
QA - Core Banking Data migration,"NTT DATA, Inc.",8 - 10 years,Not Disclosed,"['Noida', 'Hyderabad', 'Bengaluru']","Req ID: 309049\n\nWe are currently seeking a QA - Core Banking Data migration to join our team in Noida, Uttar Pradesh (IN-UP), India (IN).\n\nJob TitleQA Data Migration ""“ Core Banking (Transact)\n\n: We are looking for a highly skilled Quality Analyst with expertise in the banking sector, specifically in Core Banking (Transact) along with migration experience. The successful candidate will be responsible for ensuring the quality and integrity of data during migration from legacy systems to new platforms.\n\nKey Responsibilities:\nDevelop and execute test plans, test cases and test scripts to ensure data integrity, accuracy and completeness during migration.\nWork across multiple functional projects to understand data usage and implications for data migration.\nIdentify, Document and track data quality issues and collaborate with cross functional teams to resolve them.\nValidate data migrated to new systems, ensuring it meets business requirements and free from defects.\nIdentify, report and track defects found during testing and collaborate with development teams to resolve them.\n\n\n\nSkills and Qualifications:\n8-10 years of overall experience with a minimum of 3+ years as Core banking QA.\nProven experience as a Quality Analyst in the banking sector.\nIn-depth knowledge of Core Banking Transact and migration processes.\nFamiliarity with agile methodologies and project management principles.\nStrong analytical and problem-solving skills.\nExcellent communication and interpersonal abilities.\nAbility to work independently and as part of a team.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['project management', 'data integrity', 'process migration', 'core banking', 'agile methodology', 'capa', 'operations management', 'data analysis', 'data validation', 'business analysis', 'data migration', 'gmp', 'sql', 'change control', 'oos', 'quality assurance', 'qms', 'banking sector']",2025-06-12 00:53:35
"Data Analyst, Staff",Qualcomm,4 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Miscellaneous Group, Miscellaneous Group > Data Analyst\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nAbout the Team\n\nQualcomm's People Analytics team plays a crucial role in transforming data into strategic workforce insights that drive HR and business decisions. As part of this lean but high-impact team, you will have the opportunity to analyze workforce trends, ensure data accuracy, and collaborate with key stakeholders to enhance our data ecosystem. This role is ideal for a generalist who thrives in a fast-paced, evolving environment""”someone who can independently conduct data analyses, communicate insights effectively, and work cross-functionally to enhance our People Analytics infrastructure.\n\nWhy Join Us\n\n\nEnd-to-End ImpactWork on the full analytics cycle""”from data extraction to insight generation""”driving meaningful HR and business decisions.\n\n\nCollaboration at ScalePartner with HR leaders, IT, and other analysts to ensure seamless data integration and analytics excellence.\n\n\nData-Driven CultureBe a key player in refining our data lake, ensuring data integrity, and influencing data governance efforts.\n\n\nProfessional GrowthGain exposure to multiple areas of people analytics, including analytics, storytelling, and stakeholder engagement.\n\n\nKey Responsibilities\n\n\nPeople Analytics & Insights\nAnalyze HR and workforce data to identify trends, generate insights, and provide recommendations to business and HR leaders.\nDevelop thoughtful insights to support ongoing HR and business decision-making.\nPresent findings in a clear and compelling way to stakeholders at various levels, including senior leadership.\n\n\nData Quality & Governance\nEnsure accuracy, consistency, and completeness of data when pulling from the data lake and other sources.\nIdentify and troubleshoot data inconsistencies, collaborating with IT and other teams to resolve issues.\nDocument and maintain data definitions, sources, and reporting standards to drive consistency across analytics initiatives.\n\n\nCollaboration & Stakeholder Management\nWork closely with other analysts on the team to align methodologies, share best practices, and enhance analytical capabilities.\nAct as a bridge between People Analytics, HR, and IT teams to define and communicate data requirements.\nPartner with IT and data engineering teams to improve data infrastructure and expand available datasets.\n\n\nQualifications\n\nRequired4-7 years experience in a People Analytics focused role\n\n\nAnalytical & Technical Skills\nStrong ability to analyze, interpret, and visualize HR and workforce data to drive insights.\nExperience working with large datasets and ensuring data integrity.\nProficiency in Excel and at least one data visualization tool (e.g., Tableau, Power BI).\n\n\nCommunication & Stakeholder Management\nAbility to communicate data insights effectively to both technical and non-technical audiences.\nStrong documentation skills to define and communicate data requirements clearly.\nExperience collaborating with cross-functional teams, including HR, IT, and business stakeholders.\n\n\nPreferred:\n\n\nTechnical Proficiency\nExperience with SQL, Python, or R for data manipulation and analysis.\nFamiliarity with HR systems (e.g., Workday) and cloud-based data platforms.\n\n\nPeople Analytics Expertise\nPrior experience in HR analytics, workforce planning, or related fields.\nUnderstanding of key HR metrics and workforce trends (e.g., turnover, engagement, diversity analytics).\n\n\nAdditional Information\nThis is an office-based position (4 days a week onsite) with possible locations that may include India and Mexico",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'people analytics', 'documentation', 'tableau', 'data integration tools', 'hiring', 'data warehousing', 'data architecture', 'sourcing', 'jquery', 'staffing', 'plsql', 'oracle 10g', 'java', 'etl tool', 'html', 'etl', 'mongodb', 'python', 'oracle', 'power bi', 'hrsd', 'r', 'node.js', 'hr analytics', 'angularjs']",2025-06-12 00:53:38
Data & Gen AI Specialist,Altimetrik,1 - 4 years,Not Disclosed,['Bengaluru'],"Job Title: Data & GenAI AWS Specialist\nExperience: 1-4 Years\nLocation: Bangalore\nMandatory Qualification: B.E./ B.Tech/ M.Tech/ MS from IIT or IISc ONLY\nJob Overview:\nWe are seeking a seasoned Data & GenAI Specialist with deep expertise in AWS Managed Services (PaaS) to join our innovative team. The ideal candidate will have extensive experience in designing sophisticated, scalable architectures for data pipelines and Generative AI (GenAI) solutions leveraging cloud services.",,,,"['Generative Ai', 'Cloud', 'Data Science', 'Open Source', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Snowflake', 'Machine Learning', 'AWS']",2025-06-12 00:53:42
Data Modelling,Tech Mahindra,7 - 12 years,Not Disclosed,['Chennai'],"Role: Data Modelling\nFulltime (Work from office Monday to Friday)\nLocation: Chennai\n\n\nMinimum Qualifications:\nBachelors Degree or experience in Engineering,\n5+ years of experience in data architecture or a related field, with a strong understanding of cloud-based data solutions\nProficiency in designing and implementing Medallion Architecture with bronze, silver, and gold layers\nExperienced curating and Erwin modeling data into Star Schemas\nStrong background in semantic modeling and creating meaningful, user-friendly data sets\nExperience with AWS, Synapse, and Power BI.\nWork visa sponsorship is not available for this position",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Warehouse', 'Data Modeling', 'ERwin', 'Azure Cloud', 'GCP', 'AWS']",2025-06-12 00:53:45
Data Scientist,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Senior Data Scientist is an advanced subject matter expert, tasked with taking accountability in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nResponsible for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources\nProvides meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nDirects scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualizes the output of the models.\nResponsible for ensuring that industry standards best practices are applied to development activities.\nKnowledge and Attributes:\nAdvanced understanding of data modelling, statistical methods and machine learning techniques.\nStrong ability to thrive in a dynamic, fast-paced environment.\nStrong quantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nAdvanced understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nStrong ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nStrong ability to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\nAcademic Qualifications and Certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming certification preferred.\nAgile certification preferred.\nRequired Experience:\nAdvanced demonstrated experience in a data science position in a corporate environment and/or related industry.\nAdvanced demonstrated experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nAdvanced demonstrated experience in programming languages (R, Python, etc.).\nAdvanced demonstrated experience working with and creating data architectures.\nAdvanced demonstrated experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nAdvanced demonstrated experience visualizing and/or presenting data for stakeholder use and reuse across the business.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'R', 'data modelling', 'data mining', 'statistical modelling', 'machine learning', 'Python', 'SQL']",2025-06-12 00:53:49
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nAs part of the cybersecurity organization, In this vital role you will be responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The role sits at the intersection of data infrastructure and business insight delivery, requiring the Data Engineer to design and build robust data pipelines while also translating data into meaningful visualizations for stakeholders across the organization. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nBuild data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nDevelop and maintain interactive dashboards and reports using tools like Tableau, ensuring data accuracy and usability\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\n\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, GitLab, LucidChart, etc.\nHands-on experience with data visualization and dashboarding toolsTableau, Power BI, or similar is a plus\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\n\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\n\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\n\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data engineering', 'data analysis', 'data modeling', 'analysis tools', 'data warehousing', 'troubleshooting', 'data architecture', 'data integration', 'etl process']",2025-06-12 00:53:53
Data Visualization Expert - Quick sight,"NTT DATA, Inc.",4 - 5 years,Not Disclosed,['Chennai'],"We are currently seeking a Data Visualization Expert - Quick sight to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\n\n\n What awaits you/ Job Profile  \n\n\n\n Location Bangalore and Chennai, Hybrid mode,Immediate to 10 Days Notice period \nDevelop reports using Amazon Quicksight\nData Visualization DevelopmentDesign and develop data visualizations using Amazon Quicksight to present complex data in a clear and understandable format. Create interactive dashboards and reports that allow end-users to explore data and draw meaningful conclusions.\nData AnalysisCollaborate with data analysts and business stakeholders to understand data requirements, gather insights, and transform raw data into actionable visualizations.\nDashboard User Interface (UI) and User Experience (UX)Ensure that the data visualizations are user-friendly, intuitive, and aesthetically pleasing. Optimize the user experience by incorporating best practices in UI/UX design.\nData IntegrationWork closely with data engineers and data architects to ensure seamless integration of data sources into Quicksight, enabling real-time and up-to-date visualizations.\nPerformance OptimizationIdentify and address performance bottlenecks in data queries and visualization rendering to ensure quick and responsive dashboards.\nData Security and GovernanceEnsure compliance with data security policies and governance guidelines when handling sensitive data within Quicksight.\nTraining and DocumentationProvide training and support to end-users and stakeholders on how to interact with and interpret visualizations effectively. Create detailed documentation of the visualization development process.\nStay Updated with Industry TrendsKeep up to date with the latest data visualization trends, technologies, and best practices to continuously enhance the quality and impact of visualizations.\nUsing the Agile Methodology, attending daily standups and use of the Agile tools\nCollaborating with cross-functional teams and stakeholders to ensure data security, privacy, and compliance with regulations.\nusing Scrum/Kanban\nProficiency in Software Development best practices - Secure coding standards, Unit testing frameworks, Code coverage, Quality gates.\nAbility to lead and deliver change in a very productive way\nLead Technical discussions with customers to find the best possible solutions.\nW orking closely with the Project Manager, Solution Architect and managing client communication (as and when required)\n\n What should you bring along  \n\n\n\nMust Have\nPerson should have relevant work experience in analytics, reporting and business intelligence tools.\n4-5 years of hands-on experience in data visualization.\nRelatively 2-year Experience developing visualization using Amazon Quicksight.\nExperience working with various data sources and databases.\nAbility to work with large datasets and design efficient data models for visualization.\n\n\n\nNice to Have\nAI Project implementation and AI methods.\n\n Must have technical skill  \n\nQuick sight , SQL , AWS\n\n Good to have Technical skills  \n\nTableau, Data Engineer\n\n\n\n",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'data visualization', 'quicksight', 'software development', 'aws', 'hive', 'amazon redshift', 'unit testing', 'data warehousing', 'dashboards', 'business intelligence', 'spark', 'kanban', 'hadoop', 'etl', 'python', 'data analysis', 'ux', 'power bi', 'sql server', 'tableau', 'scrum', 'athena', 'agile', 'ssis']",2025-06-12 00:53:56
Data & AI Technical Solution ArchitectsData & AI Technical Solution,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Pune'],"Title : Data & AI Technical Solution ArchitectsData & AI Technical Solution Architects\n\nReq ID: 323749\n\nWe are currently seeking a Data & AI Technical Solution ArchitectsData & AI Technical Solution Architects to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'needs assessment', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'gcp', 'gcp cloud', 'ml']",2025-06-12 00:54:01
Associate Data Engineer,"NTT DATA, Inc.",1 - 3 years,Not Disclosed,"['New Delhi', 'Chennai', 'Bengaluru']","Your day at NTT DATA\nWe are seeking an experienced Data Engineer to join our team in delivering cutting-edge Generative AI (GenAI) solutions to clients. The successful candidate will be responsible for designing, developing, and deploying data pipelines and architectures that support the training, fine-tuning, and deployment of LLMs for various industries. This role requires strong technical expertise in data engineering, problem-solving skills, and the ability to work effectively with clients and internal teams.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesign, develop, and manage data pipelines and architectures to support GenAI model training, fine-tuning, and deployment\nData Ingestion and Integration: Develop data ingestion frameworks to collect data from various sources, transform, and integrate it into a unified data platform for GenAI model training and deployment.\nGenAI Model Integration: Collaborate with data scientists to integrate GenAI models into production-ready applications, ensuring seamless model deployment, monitoring, and maintenance.\nCloud Infrastructure Management: Design, implement, and manage cloud-based data infrastructure (e.g., AWS, GCP, Azure) to support large-scale GenAI workloads, ensuring cost-effectiveness, security, and compliance.\nWrite scalable, readable, and maintainable code using object-oriented programming concepts in languages like Python, and utilize libraries like Hugging Face Transformers, PyTorch, or TensorFlow\nPerformance Optimization: Optimize data pipelines, GenAI model performance, and infrastructure for scalability, efficiency, and cost-effectiveness.\nData Security and Compliance: Ensure data security, privacy, and compliance with regulatory requirements (e.g., GDPR, HIPAA) across data pipelines and GenAI applications.\nClient Collaboration: Collaborate with clients to understand their GenAI needs, design solutions, and deliver high-quality data engineering services.\nInnovation and R&D: Stay up to date with the latest GenAI trends, technologies, and innovations, applying research and development skills to improve data engineering services.\nKnowledge Sharing: Share knowledge, best practices, and expertise with team members, contributing to the growth and development of the team.\n\nBachelors degree in computer science, Engineering, or related fields (Masters recommended)\nExperience with vector databases (e.g., Pinecone, Weaviate, Faiss, Annoy) for efficient similarity search and storage of dense vectors in GenAI applications\n5+ years of experience in data engineering, with a strong emphasis on cloud environments (AWS, GCP, Azure, or Cloud Native platforms)\nProficiency in programming languages like SQL, Python, and PySpark\nStrong data architecture, data modeling, and data governance skills\nExperience with Big Data Platforms (Hadoop, Databricks, Hive, Kafka, Apache Iceberg), Data Warehouses (Teradata, Snowflake, BigQuery), and lakehouses (Delta Lake, Apache Hudi)\nKnowledge of DevOps practices, including Git workflows and CI/CD pipelines (Azure DevOps, Jenkins, GitHub Actions)\nExperience with GenAI frameworks and tools (e.g., TensorFlow, PyTorch, Keras)\nNice to have:\nExperience with containerization and orchestration tools like Docker and Kubernetes\nIntegrate vector databases and implement similarity search techniques, with a focus on GraphRAG is a plus\nFamiliarity with API gateway and service mesh architectures\nExperience with low latency/streaming, batch, and micro-batch processing\nFamiliarity with Linux-based operating systems and REST APIs",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Iceberg', 'Faiss', 'PySpark', 'Kafka', 'Pinecone', 'GitHub Actions', 'Snowflake', 'Apache Hudi', 'AWS', 'Azure DevOps', 'Python', 'Azure', 'BigQuery', 'Hadoop', 'Annoy', 'Teradata', 'SQL', 'Jenkins', 'Hive', 'Cloud Native platforms', 'GCP', 'Delta Lake', 'Databricks', 'Weaviate']",2025-06-12 00:54:05
Master Data Management Manager,JINDAL STEEL & POWER,7 - 12 years,15-25 Lacs P.A.,['Gurugram'],"ABOUT THE ROLE\n\nRole is related to Master data management and Transformation of the current process to have simplification, automation in shared service environment including Account payables, Receivables, GL and other activities for Jindal Group of companies\n\nKEY ATTRIBUTE",,,,"['SAP', 'PMP', 'Rpa', 'SAP FICO', 'Sap Hana', 'Process Transformation', 'Shared Services']",2025-06-12 00:54:09
Big Data Engineer - Hadoop,Info Origin Technologies Pvt Ltd,3 - 7 years,Not Disclosed,"['Hyderabad', 'Gurugram']","Role: Hadoop Data Engineer\nLocation: Gurgaon / Hyderabad\nWork Mode: Hybrid\nEmployment Type: Full-Time\nInterview Mode: First Video then In Person\nJob Description\nJob Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'Hadoop', 'Pyspark', 'Big Data', 'Python', 'SQL']",2025-06-12 00:54:13
Azure Data Engineer,HTC Global Services,4 - 8 years,Not Disclosed,['Bengaluru( Murugeshpalya )'],"Job Summary:\nWe are looking for a highly skilled Azure Data Engineer with experience in building and managing scalable data pipelines using Azure Data Factory, Synapse, and Databricks. The ideal candidate should be proficient in big data tools and Azure services, with strong programming knowledge and a solid understanding of data architecture and cloud platforms.\n\nKey Responsibilities:",,,,"['Power Bi', 'Azure Databricks', 'Azure Data Factory', 'Synapse', 'Python', 'Java', 'Scala', 'Kafka', 'big data tools', 'SQL', 'EventHub', 'Azure cloud services', 'Spark']",2025-06-12 00:54:19
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Chennai'],"Req ID: 324631\n\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 00:54:23
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Chennai'],"Req ID: 324632\n\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 00:54:26
Senior Data Engineer,Qualcomm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Data Engineer\n\nGeneral Summary:\n\nDeveloper will play an integral role in the PTEIT Machine Learning Data Engineering team. Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics. Design, develop and support CI/CD of data pipelines and services. - 5+ years of experience with Python or equivalent programming using OOPS, Data Structures and Algorithms - Develop new services in AWS using server-less and container-based services. - 3+ years of hands-on experience with AWS Suite of services (EC2, IAM, S3, CDK, Glue, Athena, Lambda, RedShift, Snowflake, RDS) - 3+ years of expertise in scheduling data flows using Apache Airflow - 3+ years of strong data modelling (Functional, Logical and Physical) and data architecture experience in Data Lake and/or Data Warehouse - 3+ years of experience with SQL databases - 3+ years of experience with CI/CD and DevOps using Jenkins - 3+ years of experience with Event driven architecture specially on Change Data Capture - 3+ years of Experience in Apache Spark, SQL, Redshift (or) Big Query (or) Snowflake, Databricks - Deep understanding building the efficient data pipelines with data observability, data quality, schema drift, alerting and monitoring. - Good understanding of the Data Catalogs, Data Governance, Compliance, Security, Data sharing - Experience in building the reusable services across the data processing systems. - Should have the ability to work and contribute beyond defined responsibilities - Excellent communication and inter-personal skills with deep problem-solving skills.\n\nMinimum Qualifications:\n3+ years of IT-related work experience with a Bachelor's degree in Computer Engineering, Computer Science, Information Systems or a related field.\nOR\n5+ years of IT-related work experience without a Bachelors degree.\n\n2+ years of any combination of academic or work experience with programming (e.g., Java, Python).\n1+ year of any combination of academic or work experience with SQL or NoSQL Databases.\n1+ year of any combination of academic or work experience with Data Structures and algorithms.\n5 years of Industry experience and minimum 3 years experience in Data Engineering development with highly reputed organizations- Proficiency in Python and AWS- Excellent problem-solving skills- Deep understanding of data structures and algorithms- Proven experience in building cloud native software preferably with AWS suit of services- Proven experience in design and develop data models using RDBMS (Oracle, MySQL, etc.)\n\nDesirable - Exposure or experience in other cloud platforms (Azure and GCP) - Experience working on internals of large-scale distributed systems and databases such as Hadoop, Spark - Working experience on Data Lakehouse platforms (One House, Databricks Lakehouse) - Working experience on Data Lakehouse File Formats (Delta Lake, Iceberg, Hudi)\n\nBachelor's or Master's degree in Computer Science, Software Engineering, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data quality', 'data structures', 'aws', 'schema', 'continuous integration', 'glue', 'amazon redshift', 'event driven architecture', 'ci/cd', 'data engineering', 'sql', 'alerts', 'java', 'data modeling', 'spark', 'devops', 'data flow', 'nosql databases', 'sql database']",2025-06-12 00:54:29
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Pune'],"Req ID: 324609\n\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 00:54:33
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Pune'],"Req ID: 324653\n\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 00:54:37
QA-Data Migration Professional,"NTT DATA, Inc.",8 - 10 years,Not Disclosed,['Chennai'],"Job TitleQA Data Migration ""“ Core Banking (Transact)\n\n: We are looking for a highly skilled Quality Analyst with expertise in the banking sector, specifically in Core Banking (Transact) along with migration experience. The successful candidate will be responsible for ensuring the quality and integrity of data during migration from legacy systems to new platforms.\n\nKey Responsibilities:\nDevelop and execute test plans, test cases and test scripts to ensure data integrity, accuracy and completeness during migration.\nWork across multiple functional projects to understand data usage and implications for data migration.\nIdentify, Document and track data quality issues and collaborate with cross functional teams to resolve them.\nValidate data migrated to new systems, ensuring it meets business requirements and free from defects.\nIdentify, report and track defects found during testing and collaborate with development teams to resolve them.\n\n\n\nSkills and Qualifications:\n8-10 years of overall experience with a minimum of 3+ years as Core banking QA.\nProven experience as a Quality Analyst in the banking sector.\nIn-depth knowledge of Core Banking Transact and migration processes.\nFamiliarity with agile methodologies and project management principles.\nStrong analytical and problem-solving skills.\nExcellent communication and interpersonal abilities.\nAbility to work independently and as part of a team.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['automation testing', 'test cases', 'process migration', 'core banking', 'agile methodology', 'project management', 'operations management', 'software testing', 'team management', 'regression testing', 'manual testing', 'selenium webdriver', 'functional testing', 'quality assurance']",2025-06-12 00:54:41
Data Migration Consultant,Excellerate Global Solutions,6 - 10 years,8-18 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Data migration SAP ABAP , S4 HANA , FSCM ,FICO",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Temporary/Contractual","['Sap Data Migration', 'SAP FICO', 'SAP ABAP', 'FSCM', 'Sap Hana', 'S4 Hana Finance']",2025-06-12 00:54:44
Data Engineer- MS Fabric,InfoCepts,5 - 9 years,Not Disclosed,['India'],"Position: Data Engineer – MS Fabric\n  Purpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\n",,,,"['components', 'data', 'scala', 'delta', 'pyspark', 'data warehousing', 'rules', 'azure data factory', 'sql', 'parquet', 'analytics', 'sql azure', 'spark', 'oracle adf', 'data pipeline architecture', 'etl', 'python', 'azure synapse', 'microsoft azure', 'power bi', 'data bricks', 'data quality', 'system', 't', 'fabric', 'data integration', 'etl process']",2025-06-12 00:54:47
Digital Solution Architect Sr. Advisor,"NTT DATA, Inc.",10 - 15 years,Not Disclosed,['Bengaluru'],"Req ID: 323226\n\nWe are currently seeking a Digital Solution Architect Sr. Advisor to join our team in Bengaluru, India, Karntaka (IN-KA), India (IN).\n\nKey Responsibilities:\nDesign data platform architectures (data lakes, lakehouses, DWH) using modern cloud-native tools (e.g., Databricks, Snowflake, BigQuery, Synapse, Redshift).\nArchitect data ingestion, transformation, and consumption pipelines using batch and streaming methods.\nEnable real-time analytics and machine learning through scalable and modular data frameworks.\nDefine data governance models, metadata management, lineage tracking, and access controls.\nCollaborate with AI/ML, application, and business teams to identify high-impact use cases and optimize data usage.\nLead modernization initiatives from legacy data warehouses to cloud-native and distributed architectures.\nEnforce data quality and observability practices for mission-critical workloads.\n\nRequired\n\nSkills:\n\n10+ years in data architecture, with strong grounding in modern data platforms and pipelines.\nDeep knowledge of SQL/NoSQL, Spark, Delta Lake, Kafka, ETL/ELT frameworks.\nHands-on experience with cloud data platforms (AWS, Azure, GCP).\nUnderstanding of data privacy, security, lineage, and compliance (GDPR, HIPAA, etc.).\nExperience implementing data mesh/data fabric concepts is a plus.\nExpertise in technical solutions writing and presenting using tools such as Word, PowerPoint, Excel, Visio etc.\nHigh level of executive presence to be able to articulate the solutions to CXO level executives.\n\nPreferred Qualifications:\nCertifications in Snowflake, Databricks, or cloud-native data platforms.\nExposure to AI/ML data pipelines, MLOps, and real-time data applications.\nFamiliarity with data visualization and BI tools (Power BI, Tableau, Looker, etc.).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'nosql', 'spark', 'kafka', 'etl', 'metadata management', 'microsoft azure', 'power bi', 'data warehousing', 'data architecture', 'elt', 'machine learning', 'distributed architecture', 'tableau', 'gcp', 'visio', 'data governance', 'platform architecture', 'data visualization', 'aws']",2025-06-12 00:54:51
Senior Data Manager/ Lead,Codeforce 360,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description:\nWe are looking for a highly experienced and dynamic Senior Data Manager / Lead to oversee a team of Data Engineers and Data Scientists. This role demands a strong background in data platforms such as Snowflake and proficiency in Python, combined with excellent people management and project leadership skills. While hands-on experience in the technologies is beneficial, the primary focus of this role is on team leadership, strategic planning, and project delivery .\n\nJob Title : Senior Data Manager / Lead\nLocation: Hyderabad (Work From Office)\nShift Timing: 10AM-7PM\nKey Responsibilities:\nLead, mentor, and manage a team of Data Engineers and Data Scientists.\nOversee the design and implementation of data pipelines and analytics solutions using Snowflake and Python.\nCollaborate with cross-functional teams (product, business, engineering) to align data solutions with business goals.\nEnsure timely delivery of projects, with high quality and performance.\nConduct performance reviews, training plans, and support career development for the team.\nSet priorities, allocate resources, and manage workloads within the data team.\nDrive adoption of best practices in data management, governance, and documentation.\nEvaluate new tools and technologies relevant to data engineering and data science.\n\nRequired Skills & Qualifications:\n6+ years of experience in data-related roles, with at least 23 years in a leadership or management position.\nStrong understanding of Snowflake architecture, performance tuning, data sharing, security, etc.\nSolid knowledge of Python for data engineering or data science tasks.\nExperience in leading data migration, ETL/ELT, and analytics projects.\nAbility to translate business requirements into technical solutions.\nExcellent leadership, communication, and stakeholder management skills.\nExposure to tools like Databricks, Dataiku, Airflow, or similar platforms is a plus.\nBachelors or Master’s degree in Computer Science, Engineering, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Bricks', 'Python', 'Airflow', 'Data Migration', 'Dataiku', 'Data Warehousing', 'ETL', 'ELT', 'SQL']",2025-06-12 00:54:54
"Walk-In Senior Data Engineer - DataStage, Azure & Power BI",Net Connect,6 - 10 years,5-11 Lacs P.A.,['Hyderabad( Madhapur )'],"Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\nBelow JD for your reference\n\nJob Description:\n\nWe are hiring an experienced Senior Data Engineer with strong expertise in IBM DataStage, , and . The ideal candidate will be responsible for end-to-end data integration, transformation, and reporting solutions that drive business decisions.",,,,"['Azure Data Factory', 'Datastage', 'Etl Datastage']",2025-06-12 00:54:58
Data Architecture,Top B2B MNC in Management Consulting Dom...,5 - 8 years,Not Disclosed,['Bengaluru'],"About the Company\nGreetings from Teamware Solutions a division of Quantum Leap Consulting Pvt. Ltd\n\nAbout the Role\nWe are hiring a Data Architecture\n\nLocation: Bangalore\nWork Model: Hybrid\nExperience: 5-9 Years\nNotice Period: Immediate to 15 Days\n\nJob Description:\nData Architecture, Data Governance, Data Modeling\n\nAdditional Information:\nMandatory Skills: Data Architecture, Data Governance, Data Modeling\nNice to have skills Certification in Data Engineering\nInterview Mode Virtual Interview\nminimum 5 yrs relevant experience and maximum 9 yrs for this requirement. Someone with more experience in building PySpark data streaming jobs on Azure Databricks\nwho have done real projects, have expertise, and hands-on experience also\nAlso, Data governance and data modeling experience with a minimum of 4 years is mandatory\nCommunication should be excellent\n\n\nPlease let me know if you are interested in this position and send me your resumes to netra.s@twsol.com",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Architecture', 'Data Modeling', 'Data Governance', 'Data Engineering']",2025-06-12 00:55:01
Data Engineer - SAS Migration,Crisil,2 - 4 years,Not Disclosed,['Mumbai'],"The SAS to Databricks Migration Developer will be responsible for migrating existing SAS code, data processes, and workflows to the Databricks platform. This role requires expertise in both SAS and Databricks, with a focus on converting SAS logic into scalable PySpark and Python code. The developer will design, implement, and optimize data pipelines, ensuring seamless integration and functionality within the Databricks environment. Collaboration with various teams is essential to understand data requirements and deliver solutions that meet business needs",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'pyspark', 'data warehousing', 'data migration', 'sql', 'spark', 'gcp', 'mysql', 'hadoop', 'bigquery', 'big data', 'etl', 'python', 'sas', 'teradata', 'airflow', 'microsoft azure', 'data engineering', 'sql server', 'dataproc', 'data bricks', 'cloud data flow', 'kafka', 'migration', 'sqoop', 'data flow']",2025-06-12 00:55:04
Senior Data Engineer,Straive,6 - 10 years,Not Disclosed,['Mumbai (All Areas)'],Senior Data Engineer\nYou will have the following responsibilities:\nDesign\nAnalyse relevant internally and externally sourced data (raw data) to generate BI and Advanced\nAnalytics datasets based on your stakeholders requirements,,,,"['AWS', 'Postgresql', 'Snowflake', 'Data Warehousing', 'ETL', 'Aws Rds Oracle']",2025-06-12 00:55:08
"Sr. Data Engineer, R&D Data Catalyst Team",Amgen Inc,7 - 9 years,Not Disclosed,['Hyderabad'],"The R&D Data Catalyst Team is responsible for buildingData Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nThe Data Engineer will be responsible for the end-to-end development of an enterprise analytics and data mastering solution leveraging Databricks and Power BI. This role requiresexpertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that research cohort-building and advanced research pipeline.The ideal candidate will have experience creating and surfacing large unifiedrepositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\nYou will collaborate closely with stakeholders, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a strong background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nRoles & Responsibilities:\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with stakeholders to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\nBasic Qualifications and Experience:\nMasters degree with 1 to 3years of experience in Data Engineering OR\nBachelors degree with 4 to 5 years of experience in Data Engineering\nDiploma and 7 to 9 years of experience in Data Engineering.\nFunctional Skills:\nMust-Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nStrong communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood-to-Have Skills:\nExperience in developing differentiated and deliverable solutions\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'ETL processes', 'DAX', 'Business Objects', 'data warehouse design', 'ETL', 'PowerBI Models', 'AWS', 'Power Query']",2025-06-12 00:55:11
Sr Data Engineering Manager,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a Senior Data Engineering Manager with a strong background in Regulatory or Integrated Product Teams within the Biotech or Pharmaceutical domain. This role will lead the end-to-end data strategy and execution for regulatory product submissions, lifecycle management, and compliance reporting, ensuring timely and accurate delivery of regulatory data assets across global markets.You will be embedded in a cross-functional Regulatory Integrated Product Team (IPT) and serve as the data and technology lead, driving integration between scientific, regulatory, and engineering functions to support submission-ready data and regulatory intelligence solutions.\nRoles & Responsibilities:\nFunctional Skills:\nLead the engineering strategy and implementation for end-to-end regulatory operations, including data ingestion, transformation, integration, and delivery across regulatory systems.\nServe as the data engineering SME in the Integrated Product Team (IPT) to support regulatory submissions, agency interactions, and lifecycle updates.\nCollaborate with global regulatory affairs, clinical, CMC, quality, safety, and IT teams to gather submission data requirements and translate them into data engineering solutions.\nManage and oversee the development of data pipelines, data models, and metadata frameworks that support submission data standards (e.g., eCTD, IDMP, SPL, xEVMPD).\nEnable integration and reporting across regulatory information management systems (RIMS), EDMS, clinical trial systems, and lab data platforms.\nImplement data governance, lineage, validation, and audit trails for regulatory data workflows, ensuring GxP and regulatory compliance.\nGuide the development of automation solutions, dashboards, and analytics that improve visibility into submission timelines, data quality, and regulatory KPIs.\nEnsure interoperability between regulatory data platforms and enterprise data lakes or lakehouses for cross-functional reporting and insights.\nCollaborate with IT, data governance, and enterprise architecture teams to ensure alignment with overall data strategy and compliance frameworks.\nDrive innovation by evaluating emerging technologies in data engineering, graph data, knowledge management, and AI for regulatory intelligence.\nLead, mentor, and coach a small team of data engineers and analysts, fostering a culture of excellence, innovation, and delivery.\nDrive Agile and Scaled Agile (SAFe) methodologies, managing sprint backlogs, prioritization, and iterative improvements to enhance team velocity and project delivery.\nStay up-to-date with emerging data technologies, industry trends, and best practices, ensuring the organization leverages the latest innovations in data engineering and architecture.\nMust-Have Skills:\n812 years of experience in data engineering or data architecture, with 3+ years in a senior or managerial capacity, preferably within the biotech or pharmaceutical industry.\nProven experience supporting regulatory functions, including submissions, tracking, and reporting for FDA, EMA, and other global authorities.\nExperience with ETL/ELT tools, data pipelines, and cloud-based data platforms (e.g., Databricks, AWS, Azure, or GCP).\nFamiliarity with regulatory standards and data models such as eCTD, IDMP, HL7, CDISC, and xEVMPD.\nDeep understanding of GxP data compliance, audit requirements, and regulatory submission processes.\nExperience with tools like Power BI, Tableau, or Qlik for regulatory dashboarding and visualization is a plus.\nStrong project management, stakeholder communication, and leadership skills, especially in matrixed, cross-functional environments.\nAbility to translate technical capabilities into regulatory and business outcomes.Prepare team members for stakeholder discussions by helping assess data costs, access requirements, dependencies, and availability for business scenarios.\nGood-to-Have Skills:\nPrior experience working on integrated product teams or regulatory transformation programs.\nKnowledge of Regulatory Information Management Systems (RIMS), Veeva Vault RIM, or Master Data Management (MDM) in regulated environments.\nFamiliarity with Agile/SAFe methodologies and DevOps/DataOps best practices.\nEducation and Professional Certifications\n12 to 15 years of experience in Computer Science, IT or related field\nScaled Agile SAFe certification preferred\nProject Management certifications preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'engineering strategy', 'DevOps', 'Project Management', 'DataOps', 'Agile', 'data strategy']",2025-06-12 00:55:14
Solutions Architect,"NTT DATA, Inc.",3 - 7 years,Not Disclosed,"['New Delhi', 'Chennai', 'Bengaluru']","Your day at NTT DATA\nWe are seeking an experienced Data Architect to join our team in designing and delivering innovative data solutions to clients. The successful candidate will be responsible for architecting, developing, and implementing data management solutions and data architectures for various industries. This role requires strong technical expertise, excellent problem-solving skills, and the ability to work effectively with clients and internal teams to design and deploy scalable, secure, and efficient data solutions.\nWhat you'll be doing\nWe are seeking an experienced Data Architect to join our team in designing and delivering innovative data solutions to clients. The successful candidate will be responsible for architecting, developing, and implementing data management solutions and data architectures for various industries. This role requires strong technical expertise, excellent problem-solving skills, and the ability to work effectively with clients and internal teams to design and deploy scalable, secure, and efficient data solutions.\nExperience and Leadership:\nProven experience in data architecture, with a recent role as a Lead Data Solutions Architect, or a similar senior position in the field.\nProven experience in leading architectural design and strategy for complex data solutions and then overseeing their delivery.\nExperience in consulting roles, delivering custom data architecture solutions across various industries.\nArchitectural Expertise:\nStrong expertise in designing and overseeing delivery of data streaming and event-driven architectures, with a focus on Kafka and Confluent platforms.\nIn-depth knowledge in architecting and implementing data lakes and lakehouse platforms, including experience with Databricks and Unity Catalog.\nProficiency in conceptualising and applying Data Mesh and Data Fabric architectural patterns.\nExperience in developing data product strategies, with a strong inclination towards a product-led approach in data solution architecture.\nExtensive familiarity with cloud data architecture on platforms such as AWS, Azure, GCP, and Snowflake.\nUnderstanding of cloud platform infrastructure and its impact on data architecture.\nData Technology Skills:\nA solid understanding of big data technologies such as Apache Spark, and knowledge of Hadoop ecosystems.\nKnowledge of programming languages such as Python or R is beneficial.\nExposure to ETL/ ELT processes, SQL, NoSQL databases is a nice-to-have, providing a well-rounded background.\nExperience with data visualization tools and DevOps principles/tools is advantageous.\nFamiliarity with machine learning and AI concepts, particularly in how they integrate into data architectures.\nDesign and Lifecycle Management:\nProven background in designing modern, scalable, and robust data architectures.\nComprehensive grasp of the data architecture lifecycle, from concept to deployment and consumption.\nData Management and Governance:\nStrong knowledge of data management principles and best practices, including data governance frameworks.\nExperience with data security and compliance regulations (GDPR, CCPA, HIPAA, etc.)\nLeadership and Communication:\nExceptional leadership skills to manage and guide a team of architects and technical experts.\nExcellent communication and interpersonal skills, with a proven ability to influence architectural decisions with clients and guide best practices\nProject and Stakeholder Management:\nExperience with agile methodologies (e.g. SAFe, Scrum, Kanban) in the context of architectural projects.\nAbility to manage project budgets, timelines, and resources, maintaining focus on architectural deliverables.\nLocation: Delhi or Bangalore\nWorkplace type:\nHybrid Working",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'snowflake', 'python', 'data management', 'big data technologies', 'microsoft azure', 'data architecture', 'sql', 'data bricks', 'spark', 'gcp', 'devops', 'kanban', 'kafka', 'architectural patterns', 'scrum', 'agile', 'hadoop', 'conceptualization', 'aws', 'etl', 'data lake']",2025-06-12 00:55:18
Lead AWS Glue Data Engineer,Allegis Group,8 - 13 years,Not Disclosed,[],"Lead AWS Glue Data Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\n\nWe are seeking a skilled Lead AWS Data Engineer with 8+ years of strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena.\nImplement data migration and transformation processes using AWS DMS and Glue.\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\n\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDS', 'Glue', 'DMS', 'Lambda', 'Redshift', 'Athena']",2025-06-12 00:55:21
Technical Lead - Sr. Data Engineer,Edgematics Consulting,8 - 13 years,Not Disclosed,['Pune'],"About This Role :\n\nWe are looking for a talented and experienced Data Engineer with Tech Lead with hands-on expertise in any ETL Tool with full knowledge about CI/CD practices with leading a team technically more than 5 and client facing and create Data Engineering, Data Quality frameworks. As a tech lead must ensure to build ETL jobs, Data Quality Jobs, Big Data Jobs performed performance optimization by understanding the requirements, create re-usable assets and able to perform production deployment and preferably worked in DWH appliances Snowflake / redshift / Synapse\n\nResponsibilities\nWork with a team of engineers in designing, developing, and maintaining scalable and efficient data solutions using Any Data Integration (any ETL tool like Talend / Informatica) and any Big Data technologies.\nDesign, develop, and maintain end-to-end data pipelines using Any ETL Data Integration (any ETL tool like Talend / Informatica) to ingest, process, and transform large volumes of data from heterogeneous sources.\nHave good experience in designing cloud pipelines using Azure Data Factory or AWS Glues/Lambda.\nImplemented Data Integration end to end with any ETL technologies.\nImplement database solutions for storing, processing, and querying large volumes of structured and unstructured and semi-structured data\nImplement Job Migrations of ETL Jobs from Older versions to New versions.\nImplement and write advanced SQL scripts in SQL Database at medium to expert level. \nWork with technical team with client and provide guidance during technical challenges.\nIntegrate and optimize data flows between various databases, data warehouses, and Big Data platforms.\nCollaborate with cross-functional teams to gather data requirements and translate them into scalable and efficient data solutions.\nOptimize ETL, Data Load performance, scalability, and cost-effectiveness through optimization techniques.\nInteract with Client on a daily basis and provide technical progress and respond to technical questions.\nImplement best practices for data integration.\nImplement complex ETL data pipelines or similar frameworks to process and analyze massive datasets.\nEnsure data quality, reliability, and security across all stages of the data pipeline.\nTroubleshoot and debug data-related issues in production systems and provide timely resolution.\nStay current with emerging technologies and industry trends in data engineering technologies, CI/CD, and incorporate them into our data architecture and processes.\nOptimize data processing workflows and infrastructure for performance, scalability, and cost-effectiveness.\nProvide technical guidance and foster a culture of continuous learning and improvement.\nImplement and automate CI/CD pipelines for data engineering workflows, including testing, deployment, and monitoring.\nPerform migration to production deployment from lower environments, test & validate\n\nMust Have Skills\nMust be certified in any ETL tools, Database, Cloud.(Snowflake certified is more preferred)\nMust have implemented at least 3 end-to-end projects in Data Engineering.\nMust have worked on performance management optimization and tuning for data loads, data processes, data transformation in big data\nMust be flexible to write code using JAVA/Scala/Python etc. as required\nMust have implemented CI/CD pipelines using tools like Jenkins, GitLab CI, or AWS CodePipeline.\nMust have managed a team technically of min 5 members and guided the team technically.\nMust have the Technical Ownership capability of Data Engineering delivery.\nStrong communication capabilities with client facing.\nBachelor's or Master's degree in Computer Science, Engineering, or a related field.\n5 years of experience in software engineering or a related role, with a strong focus on Any ETL Tool, database, integration.\nProficiency in Any ETL tools like Talend , Informatica etc for Data Integration for building and orchestrating data pipelines.\nHands-on experience with relational databases such as MySQL, PostgreSQL, or Oracle, and NoSQL databases such as MongoDB, Cassandra, or Redis.\nSolid understanding of database design principles, data modeling, and SQL query optimization.\nExperience with data warehousing, Data Lake , Delta Lake concepts and technologies, data modeling, and relational databases.",Industry Type: IT Services & Consulting,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'ETL', 'Azure Aws', 'Data Management', 'Big Data', 'Ci/Cd', 'Data Integration', 'Data Quality', 'Data Pipeline', 'Data Warehousing', 'Data Modeling', 'Data Governance']",2025-06-12 00:55:26
Data Engineer,Dun & Bradstreet,5 - 9 years,Not Disclosed,['Hyderabad'],"Key Responsibilities:\n1. Design, build, and deploy new data pipelines within our Big Data Eco-Systems using Streamsets/Talend/Informatica BDM etc. Document new/existing pipelines, Datasets.\n2. Design ETL/ELT data pipelines using StreamSets, Informatica or any other ETL processing engine. Familiarity with Data Pipelines, Data Lakes and modern Data Warehousing practices (virtual data warehouse, push down analytics etc.)\n3. Expert level programming skills on Python\n4. Expert level programming skills on Spark\n5. Cloud Based Infrastructure: GCP\n6. Experience with one of the ETL Informatica, StreamSets in creation of complex parallel loads, Cluster Batch Execution and dependency creation using Jobs/Topologies/Workflows etc.,\n7. Experience in SQL and conversion of SQL stored procedures into Informatica/StreamSets, Strong exposure working with web service origins/targets/processors/executors, XML/JSON Sources and Restful APIs.\n8. Strong exposure working with relation databases DB2, Oracle & SQL Server including complex SQL constructs and DDL generation.\n9. Exposure to Apache Airflow for scheduling jobs\n10. Strong knowledge of Big data Architecture (HDFS), Cluster installation, configuration, monitoring, cluster security, cluster resources management, maintenance, and performance tuning\n11. Create POCs to enable new workloads and technical capabilities on the Platform.\n12. Work with the platform and infrastructure engineers to implement these capabilities in production.\n13. Manage workloads and enable workload optimization including managing resource allocation and scheduling across multiple tenants to fulfill SLAs.\n14. Participate in planning activities, Data Science and perform activities to increase platform skills\n\nKey Requirements:\n1. Minimum 6 years of experience in ETL/ELT Technologies, preferably StreamSets/Informatica/Talend etc.,\n2. Minimum of 6 years hands-on experience with Big Data technologies e.g. Hadoop, Spark, Hive.\n3. Minimum 3+ years of experience on Spark\n4. Minimum 3 years of experience in Cloud environments, preferably GCP\n5. Minimum of 2 years working in a Big Data service delivery (or equivalent) roles focusing on the following disciplines:\n6. Any experience with NoSQL and Graph databases\n7. Informatica or StreamSets Data integration (ETL/ELT)\n8. Exposure to role and attribute based access controls\n9. Hands on experience with managing solutions deployed in the Cloud, preferably on GCP\n10. Experience working in a Global company, working in a DevOps model is a plus",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'GCP', 'Data engineer', 'Spark', 'ETL']",2025-06-12 00:55:28
"Data Engineering : Sr Software Engineer, Tech Lead & Sr Tech Lead",Reflion Tech,7 - 12 years,22.5-37.5 Lacs P.A.,"['Mumbai( Ghansoli )', 'Navi Mumbai', 'Mumbai (All Areas)']","Hiring: Data Engineering Senior Software Engineer / Tech Lead / Senior Tech Lead\n\n- Hybrid (3 Days from office) | Shift: 2 PM 11 PM IST\n- Experience: 5 to 12+ years (based on role & grade)\n\nOpen Grades/Roles:\nSenior Software Engineer: 58 Years\nTech Lead: 7–10 Years\nSenior Tech Lead: 10–12+ Years\n\nJob Description – Data Engineering Team\n\nCore Responsibilities (Common to All Levels):\n\nDesign, build and optimize ETL/ELT pipelines using tools like Pentaho, Talend, or similar\nWork on traditional databases (PostgreSQL, MSSQL, Oracle) and MPP/modern systems (Vertica, Redshift, BigQuery, MongoDB)\nCollaborate cross-functionally with BI, Finance, Sales, and Marketing teams to define data needs\nParticipate in data modeling (ER/DW/Star schema), data quality checks, and data integration\nImplement solutions involving messaging systems (Kafka), REST APIs, and scheduler tools (Airflow, Autosys, Control-M)\nEnsure code versioning and documentation standards are followed (Git/Bitbucket)\n\nAdditional Responsibilities by Grade\n\nSenior Software Engineer (5–8 Yrs):\nFocus on hands-on development of ETL pipelines, data models, and data inventory\nAssist in architecture discussions and POCs\nGood to have: Tableau/Cognos, Python/Perl scripting, GCP exposure\n\nTech Lead (7–10 Yrs):\nLead mid-sized data projects and small teams\nDecide on ETL strategy (Push Down/Push Up) and performance tuning\nStrong working knowledge of orchestration tools, resource management, and agile delivery\n\nSenior Tech Lead (10–12+ Yrs):\nDrive data architecture, infrastructure decisions, and internal framework enhancements\nOversee large-scale data ingestion, profiling, and reconciliation across systems\nMentoring junior leads and owning stakeholder delivery end-to-end\nAdvantageous: Experience with AdTech/Marketing data, Hadoop ecosystem (Hive, Spark, Sqoop)\n\n- Must-Have Skills (All Levels):\n\nETL Tools: Pentaho / Talend / SSIS / Informatica\nDatabases: PostgreSQL, Oracle, MSSQL, Vertica / Redshift / BigQuery\nOrchestration: Airflow / Autosys / Control-M / JAMS\nModeling: Dimensional Modeling, ER Diagrams\nScripting: Python or Perl (Preferred)\nAgile Environment, Git-based Version Control\nStrong Communication and Documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SQL', 'ETL', 'Orchestration', 'Postgresql', 'Peri', 'Informatica', 'ETL Tool', 'SSIS', 'Elt', 'Modeling', 'MongoDB', 'Data Architecture', 'Talend', 'Pentaho', 'Python']",2025-06-12 00:55:32
Senior Data Engineer,Jeavio,5 - 10 years,Not Disclosed,[],"We are seeking an experienced Senior Data Engineer to join our team. The ideal candidate will have a strong background in data engineering and AWS infrastructure, with hands-on experience in building and maintaining data pipelines and the necessary infrastructure components. The role will involve using a mix of data engineering tools and AWS services to design, build, and optimize data architecture.\n\nKey Responsibilities:\nDesign, develop, and maintain data pipelines using Airflow and AWS services.\nImplement and manage data warehousing solutions with Databricks and PostgreSQL.\nAutomate tasks using GIT / Jenkins.\nDevelop and optimize ETL processes, leveraging AWS services like S3, Lambda, AppFlow, and DMS.\nCreate and maintain visual dashboards and reports using Looker.\nCollaborate with cross-functional teams to ensure smooth integration of infrastructure components.\nEnsure the scalability, reliability, and performance of data platforms.\nWork with Jenkins for infrastructure automation.\n\nTechnical and functional areas of expertise:\nWorking as a senior individual contributor on a data intensive project\nStrong experience in building high performance, resilient & secure data processing pipelines preferably using Python based stack.\nExtensive experience in building data intensive applications with a deep understanding of querying and modeling with relational databases preferably on time-series data.\nIntermediate proficiency in AWS services (S3, Airflow)\nProficiency in Python and PySpark\nProficiency with ThoughtSpot or Databricks.\nIntermediate proficiency in database scripting (SQL)\nBasic experience with Jenkins for task automation\n\nNice to Have :\nIntermediate proficiency in data analytics tools (Power BI / Tableau / Looker / ThoughSpot)\nExperience working with AWS Lambda, Glue, AppFlow, and other AWS transfer services.\nExposure to PySpark and data automation tools like Jenkins or CircleCI.\nFamiliarity with Terraform for infrastructure-as-code.\nExperience in data quality testing to ensure the accuracy and reliability of data pipelines.\nProven experience working directly with U.S. client stakeholders.\nAbility to work independently and take the lead on tasks.\n\nEducation and experience:\nBachelors or masters in computer science or related fields.\n5+ years of experience\n\nStack/Skills needed:\nDatabricks\nPostgreSQL\nPython & Pyspark\nAWS Stack\nPower BI / Tableau / Looker / ThoughSpot\nFamiliarity with GIT and/or CI/CD tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'AWS', 'Data Bricks', 'Python', 'Etl Pipelines', 'Airflow', 'Database Scripting', 'Postgresql', 'Looker', 'SQL']",2025-06-12 00:55:35
Senior Data Engineer,The Main Stage Productions,4 - 6 years,Not Disclosed,['Bengaluru'],"Design and implement cloud-native data architectures on AWS, including data lakes, data warehouses, and streaming pipelines using services like S3, Glue, Redshift, Athena, EMR, Lake Formation, and Kinesis.\nDevelop and orchestrate ETL/ELT pipelines\n\nRequired Candidate profile\nParticipate in pre-sales and consulting activities such as:\nEngaging with clients to gather requirements and propose AWS-based data engineering solutions.\nSupporting RFPs/RFIs, technical proposals",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS Glue', 'GitHub Actions', 'PySpark', 'Scala', 'CodePipeline', 'Step Functions', 'data engineering']",2025-06-12 00:55:37
APAC Presales Solution Architect,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Bengaluru'],"Req ID: 328244\n\nWe are currently seeking a APAC Presales Solution Architect to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesSenior Data and AI Architect ""“ Presales\nGrade 11\n\nSeeking a senior data solution architect to closely work with India and APAC sales teams for technical solutioning and presales work.\n\nThe Consultant is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n\nMinimum Skills Required""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\n""¢ Academic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\n""¢ Required Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop On-prem, Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n\n""¢ Additional\n""¢ Knowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\n""¢ Problem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\n""¢ Interaction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject m""",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['client engagement', 'data architecture', 'change management', 'artificial intelligence', 'service orientation', 'needs assessment', 'architecting', 'python', 'project management', 'rfqs', 'togaf', 'aiml', 'market research', 'presales', 'solution architecting', 'gen', 'solution design', 'aws', 'rfp', 'ml']",2025-06-12 00:55:41
Senior Data Engineer : 7+ Years,Jayam Solutions Pvt Ltd - CMMI Level III Company,5 - 9 years,Not Disclosed,['Hyderabad( Madhapur )'],"Job Description:\nPosition: Sr.Data Engineer\nExperience: Minimum 7 years\nLocation: Hyderabad\nJob Summary:\n\nWhat Youll Do\n\nDesign and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.\nParticipate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.\nIndependently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.\nMonitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.\nMaintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.\nAcquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.\nEngage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.\nDrive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.\n\nWhat Youll Bring\n\nBachelors degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.\n7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.\nExtensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.\nCloud - AWS\nApache Flink/Spark , Redis\nDatabase platform- Databricks.\nProficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.\nExpertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.\nExperience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).\nAbility to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.\nStrong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.\nExceptional attention to detail and professionalism in report and dashboard creation.\nExcellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.\nOutstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.\n\nPlease feel free to contact us: 9440806850\nEmail ID : careers@jayamsolutions.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Flink', 'Redis', 'Spark', 'Python', 'SCALA', 'Ci/Cd', 'Devops', 'AWS']",2025-06-12 00:55:43
Senior Data Engineer,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nOR\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Git', 'PySpark', 'CI/CD', 'Databricks', 'ETL', 'NOSQL', 'AWS', 'data integration', 'SQL', 'Apache Spark', 'Python']",2025-06-12 00:55:47
Senior Data Engineer,Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data engineering', 'performance tuning', 'data security', 'data processing', 'Hadoop', 'Apache Spark', 'SQL', 'CI/CD', 'troubleshooting', 'big data', 'aws', 'ETL', 'Python']",2025-06-12 00:55:50
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nRole Description:\nWe are seeking a Senior Data Engineer with expertise in Graph Data technologies to join our data engineering team and contribute to the development of scalable, high-performance data pipelines and advanced data models that power next-generation applications and analytics. This role combines core data engineering skills with specialized knowledge in graph data structures, graph databases, and relationship-centric data modeling, enabling the organization to leverage connected data for deep insights, pattern detection, and advanced analytics use cases. The ideal candidate will have a strong background in data architecture, big data processing, and Graph technologies and will work closely with data scientists, analysts, architects, and business stakeholders to design and deliver graph-based data engineering solutions.\nRoles & Responsibilities:\nDesign, build, and maintain robust data pipelines using Databricks (Spark, Delta Lake, PySpark) for complex graph data processing workflows.\nOwn the implementation of graph-based data models, capturing complex relationships and hierarchies across domains.\nBuild and optimize Graph Databases such as Stardog, Neo4j, Marklogic or similar to support query performance, scalability, and reliability.\nImplement graph query logic using SPARQL, Cypher, Gremlin, or GSQL, depending on platform requirements.\nCollaborate with data architects to integrate graph data with existing data lakes, warehouses, and lakehouse architectures.\nWork closely with data scientists and analysts to enable graph analytics, link analysis, recommendation systems, and fraud detection use cases.\nDevelop metadata-driven pipelines and lineage tracking for graph and relational data processing.\nEnsure data quality, governance, and security standards are met across all graph data initiatives.\nMentor junior engineers and contribute to data engineering best practices, especially around graph-centric patterns and technologies.\nStay up to date with the latest developments in graph technology, graph ML, and network analytics.\nWhat we expect of you\nMust-Have Skills:\nHands-on experience in Databricks, including PySpark, Delta Lake, and notebook-based development.\nHands-on experience with graph database platforms such as Stardog, Neo4j, Marklogic etc.\nStrong understanding of graph theory, graph modeling, and traversal algorithms\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies with strong problem-solving and analytical skills\nExcellent collaboration and communication skills, with experience working with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SPARQL', 'Maven', 'PySpark', 'GSQL', 'Subversion', 'AWS services', 'Stardog', 'Cypher', 'SAFe', 'Jenkins', 'DevOps', 'Git', 'Neo4j', 'Delta Lake', 'Graph Databases', 'Spark', 'Marklogic', 'Gremlin']",2025-06-12 00:55:53
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Maven', 'SparkSQL Apache Spark', 'PySpark', 'Subversion', 'OLAP', 'Scaled Agile methodologies', 'SQL', 'Scaled Agile Framework', 'Jenkins', 'NOSQL database', 'Git', 'Databricks', 'Data Fabric', 'Data Mesh', 'AWS', 'Python']",2025-06-12 00:55:57
Technical Architect,PwC India,10 - 15 years,Not Disclosed,"['Mumbai', 'Navi Mumbai', 'Gurugram']","Role Description\nWe are looking for a suitable candidate for the opening of Data/Technical Architect role for Data Management, preferably for one who has worked in Insurance or Banking and Financial Services domain and holds relevant experience of 10+ years. The candidate should be willing to take up the role of Senior Manager/Associate Director in an organization based on overall experience.\nLocation : Mumbai and Gurugram\nRelevant experience : 10+ years",,,,"['Data Architecture', 'Technical Architecture', 'Java', 'Bigquery', 'SCALA', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'Data Bricks', 'Python']",2025-06-12 00:56:00
Immediate Joiner- Data Engineer,Healthedge,1 - 4 years,Not Disclosed,['Bengaluru'],"Data Engineer\nYou will be working with agile cross functional software development teams developing cutting age software to solve a significant problem in the Provider Data Management space. This hire will have experience building large scale complex data systems involving multiple cross functional data sets and teams. The ideal candidate will be excited about working on new product development, is comfortable pushing the envelope and challenging the status quo, sets high standards for him/herself and the team, and works well with ambiguity.\nWhat you will do:\nBuild data pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements.\nWork closely with data architect, SMEs and other technology partners to develop & execute data architecture and product roadmap.\nBuild analytical tools to utilize the data pipeline, providing actionable insight into key business performance including operational efficiency and business metrics.\nWork with stakeholders including the leadership, product, customer teams to support their data infrastructure needs while assisting with data-related technical issues.\nAct as a subject matter expert to other team members for technical guidance, solution design and best practices within the customer organization.\nKeep current on big data and data visualization technology trends, evaluate, work on proof-of-concept and make recommendations on cloud technologies.\nWhat you bring:\n2+ years of data engineering experience working in partnership with large data sets (preferably terabyte scale)\nExperience in building data pipelines using any of the ETL tools such as Glue, ADF, Notebooks, Stored Procedures, SQL/Python constructs or similar.\nDeep experience working with industry standard RDBMS such Postgres, SQL Server, Oracle, MySQL etc. and any of the analytical cloud databases such as Big Query, Redshift, Snowflake or similar\nAdvanced SQL expertise and solid programming experience with Python and/or Spark\nExperience working with orchestration tools such as Airflow and building complex dependency workflows.\nExperience, developing and implementing Data Warehouse or Data Lake Architectures, OLAP technologies, data modeling with star/snowflake-schemas to enable analytics & reporting.\nGreat problem-solving capabilities, troubleshooting data issues and experience in stabilizing big data systems.\nExcellent communication and presentation skills as youll be regularly interacting with stakeholders and engineering leadership.\nBachelors or master's in quantitative disciplines such as Computer Science, Computer Engineering, Analytics, Mathematics, Statistics, Information Systems, or other scientific fields.\nBonus points:\nHands-on deep experience with cloud data migration, and experience working with analytic platforms like Fabric, Databricks on the cloud.\nCertification in one of the cloud platforms (AWS/GCP/Azure)\nExperience or demonstrated understanding with real-time data streaming tools like Kafka, Kinesis or any similar tools.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Pyspark', 'Cloud', 'Python']",2025-06-12 00:56:03
Director Data Science,Astar Data,10 - 17 years,Not Disclosed,['Bengaluru'],"Sigmoid enables business transformation using data and analytics, leveraging real-time insights to make accurate and fast business decisions, by building modern data architectures using cloud and open source. Some of the worlds largest data producers engage with Sigmoid to solve complex business problems. Sigmoid brings deep expertise in data engineering, predictive analytics, artificial intelligence, and DataOps. Sigmoid has been recognized as one of the fastest growing technology companies in North America, 2021, by Financial Times, Inc. 5000, and Deloitte Technology Fast 500.\nOffices: New York | Dallas | San Francisco | Lima | Bengaluru\nThe below role is for our Bengaluru office.\n\nWhy Join Sigmoid?\n• Sigmoid provides the opportunity to push the boundaries of what is possible by seamlessly\ncombining technical expertise and creativity to tackle intrinsically complex business\nproblems and convert them into straight-forward data solutions.\n• Despite being continuously challenged, you are not alone. You will be part of a fast-paced\ndiverse environment as a member of a high-performing team that works together to\nenergize and inspire each other by challenging the status quo\n• Vibrant inclusive culture of mutual respect and fun through both work and play\nRoles and Responsibilities:\n• Convert broad vision and concepts into a structured data science roadmap, and guide a\nteam to successfully execute on it.\n• Handling end-to-end client AI & analytics programs in a fluid environment. Your role will be a\ncombination of hands-on contribution, technical team management, and client interaction.\n• Proven ability to discover solutions hidden in large datasets and to drive business results\nwith their data-based insights\n• Contribute to internal product development initiatives related to data science.\n• Drive excellent project management required to deliver complex projects, including\neffort/time estimation.\n• Be proactive, with full ownership of the engagement. Build scalable client engagement level\nprocesses for faster turnaround & higher accuracy\n• Define Technology/ Strategy and Roadmap for client accounts, and guides implementation\nof that strategy within projects\n• Manage the team-members, to ensure that the project plan is being adhered to over the\ncourse of the project\n• Build a trusted advisor relationship with the IT management at clients and internal accounts\nleadership.\nMandated Skills:\n• A B-Tech/M-Tech/MBA from a top tier Institutepreferably in a quantitativesubject\n• 10+ years of hands-onexperience in applied Machine Learning, AI and analytics\n• Experience of scientific programming in scripting languages like Python, R, SQL, NoSQL,\nSpark with ML tools & Cloud Technology (AWS, Azure, GCP)\n• Experience in Python libraries such as numpy, pandas, scikit-learn, tensor-flow, scrapy, BERT\netc. Strong grasp of depth and breadth of machine learning, deep learning, data mining, and\nstatistical concepts and experience in developing models and solutions in these areas\n• Expertise with client engagement, understanding complex problem statements, and offering\nsolutions in the domains of Supply Chain, Manufacturing, CPG, Marketing etc.\nDesired Skills:\nDeep understanding of ML algorithms for common use cases in both structured and\nunstructured data ecosystems.\nComfortable with large scale data processing and distributed computing\nProviding required inputs to sales, and pre-sales activities\nA self-starter who can work well with minimalguidance\nExcellent written and verbal communication skills",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Machine Learning', 'Algorithm Development', 'Pattern Recognition', 'Opencv', 'Image Processing', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Computer Vision', 'Deep Learning']",2025-06-12 00:56:06
Data Engineering Lead,Yotta Techports,10 - 15 years,30-35 Lacs P.A.,['Hyderabad'],"Responsibilities:\nLead and manage an offshore team of data engineers, providing strategic guidance, mentorship, and support to ensure the successful delivery of projects and the development of team members.\nCollaborate closely with onshore stakeholders to understand project requirements, allocate resources efficiently, and ensure alignment with client expectations and project timelines.\nDrive the technical design, implementation, and optimization of data pipelines, ETL processes, and data warehouses, ensuring scalability, performance, and reliability.\nDefine and enforce engineering best practices, coding standards, and data quality standards to maintain high-quality deliverables and mitigate project risks.\nStay abreast of emerging technologies and industry trends in data engineering, and provide recommendations for tooling, process improvements, and skill development.\nAssume a data architect role as needed, leading the design and implementation of data architecture solutions, data modeling, and optimization strategies.\nDemonstrate proficiency in AWS services such as:\nExpertise in cloud data services, including AWS services like Amazon Redshift, Amazon EMR, and AWS Glue, to design and implement scalable data solutions.\nExperience with cloud infrastructure services such as AWS EC2, AWS S3, to optimize data processing and storage.\nKnowledge of cloud security best practices, IAM roles, and encryption mechanisms to ensure data privacy and compliance.\nProficiency in managing or implementing cloud data warehouse solutions, including data modeling, schema design, performance tuning, and optimization techniques.\nDemonstrate proficiency in modern data platforms such as Snowflake and Databricks, including:\nDeep understanding of Snowflake's architecture, capabilities, and best practices for designing and implementing data warehouse solutions.\nHands-on experience with Databricks for data engineering, data processing, and machine learning tasks, leveraging Spark clusters for scalable data processing.\nAbility to optimize Snowflake and Databricks configurations for performance, scalability, and cost-effectiveness.\nManage the offshore team's performance, including resource allocation, performance evaluations, and professional development, to maximize team productivity and morale.\n\nQualifications:\nBachelor's degree in Computer Science, Engineering, or a related field; advanced degree preferred.\n10+ years of experience in data engineering, with a proven track record of leadership and technical expertise in managing complex data projects.\nProficiency in programming languages such as Python, Java, or Scala, as well as expertise in SQL and relational databases (e.g., PostgreSQL, MySQL).\nStrong understanding of distributed computing, cloud technologies (e.g., AWS), and big data frameworks (e.g., Hadoop, Spark).\nExperience with data architecture design, data modeling, and optimization techniques.\nExcellent communication, collaboration, and leadership skills, with the ability to effectively manage remote teams and engage with onshore stakeholders.\nProven ability to adapt to evolving project requirements and effectively prioritize tasks in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team Handling', 'Snowflake', 'Data Services', 'Cloud Infrastructure', 'Data Bricks']",2025-06-12 00:56:09
Data Engineer Graph – Research Data and Analytics,Amgen Inc,2 - 4 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be part Researchs Semantic Graph Team is seeking a qualified individual to design, build, and maintain solutions for scientific data that drive business decisions for Research. The successful candidate will construct scalable and high-performance data engineering solutions for extensive scientific datasets and collaborate with Research partners to address their data requirements. The ideal candidate should have experience in the pharmaceutical or biotech industry, leveraging their expertise in semantics, taxonomies, and linked data principles to ensure data harmonization and interoperability. Additionally, this individual should demonstrate robust technical skills, proficiency with data engineering technologies, and a thorough understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nDesign, develop, and implement data pipelines, ETL/ELT processes, and data integration solutions\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks\nDevelop and maintain semantic data models for biopharma scientific data, data dictionaries, and other documentation to ensure data accuracy and consistency\nOptimize large datasets for query performance\nCollaborate with global multi-functional teams including research scientists to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nCollaborate with Data Architects, Business SMEs, Software Engineers and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve [complex] data-related challenges\nAdhere to standard processes for coding, testing, and designing reusable code/component\nExplore new tools and technologies that will help to improve ETL platform performance\nParticipate in sprint planning meetings and provide estimations on technical implementation\nMaintain comprehensive documentation of processes, systems, and solutions\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. T\nBasic Qualifications and Experience:\nDoctorate Degree OR Masters degree with 2- 4years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 4- 6years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 7- 9 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\n\n\nPreferred Qualifications and Experience:\n4+ years of experience in designing and supporting biopharma scientific research data analytics (software platforms)\n\n\nFunctional Skills:\nMust-Have Skills:\nProficiency in SQL and Python for data engineering, test automation frameworks (pytest), and scripting tasks\nHands on experience with data technologies and platforms, such as Databricks, workflow orchestration, performance tuning on big data processing.\nExcellent problem-solving skills and the ability to work with large, complex datasets\n\n\nGood-to-Have Skills:\nA passion for tackling complex challenges in drug discovery with technology and data\nExperience with system administration skills, such as managing Linux and Windows servers, configuring network infrastructure, and automating tasks with shell scripting. Examples include setting up and maintaining virtual machines, troubleshooting server issues, and ensuring data security through regular updates and backups.\nSolid understanding of data modeling, data warehousing, and data integration concepts\nSolid experience using RDBMS (e.g. Oracle, MySQL, SQL server, PostgreSQL)\nKnowledge of cloud data platforms (AWS preferred)\nExperience with data visualization tools (e.g. Dash, Plotly, Spotfire)\nExperience with diagramming and collaboration tools such as Miro, Lucidchart or similar tools for process mapping and brainstorming\nExperience writing and maintaining user documentation in Confluence\nUnderstanding of data governance frameworks, tools, and standard processes\n\n\nProfessional Certifications:\nDatabricks Certified Data Engineer Professional preferred\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analytics', 'PostgreSQL', 'MySQL', 'ETL', 'ELT', 'Oracle', 'SQL server', 'AWS']",2025-06-12 00:56:15
Data Engineer,Tekskills India pvt ltd,7 - 9 years,8-15 Lacs P.A.,['Hyderabad'],"Role & Responsibilities Role Overview: We are seeking a talented and forward-thinking Data Engineer for one of the large financial services GCC based in Hyderabad with responsibilities that include designing and constructing data pipelines, integrating data from multiple sources, developing scalable data solutions, optimizing data workflows, collaborating with cross-functional teams, implementing data governance practices, and ensuring data security and compliance.\n\nTechnical Requirements: • Proficiency in ETL, Batch, and Streaming Process • Experience with BigQuery, Cloud Storage, and CloudSQL • Strong programming skills in Python, SQL, and Apache Beam for data processing • Understanding of data modeling and schema design for analytics • Knowledge of data governance, security, and compliance in GCP • Familiarity with machine learning workflows and integration with GCP ML tools • Ability to optimize performance within data pipelines\n\nFunctional Requirements: • Ability to collaborate with Data Operations, Software Engineers, Data Scientists, and Business SMEs to develop Data Product Features • Experience in leading and mentoring peers within an existing development team • Strong communication skills to craft and communicate robust solutions • Proficient in working with Engineering Leads, Enterprise and Data Architects, and Business Architects to build appropriate data foundations • Willingness to work on contemporary data architecture in Public and Private Cloud environments This role offers a compelling opportunity for a seasoned Data Engineering to drive transformative cloud initiatives within the financial sector, leveraging unparalleled experience and expertise to deliver innovative cloud solutions that align with business imperatives and regulatory requirements. Qualification o Engineering Grad / Postgraduate CRITERIA o Proficient in ETL, Python, and Apache Beam for data processing efficiency. o Demonstrated expertise in BigQuery, Cloud Storage, and CloudSQL utilization. o Strong collaboration skills with cross-functional teams for data product development. o Comprehensive knowledge of data governance, security, and compliance in GCP. o Experienced in optimizing performance within data pipelines for efficiency.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'GCP', 'Apache beam', 'Bigquery', 'Cloud sql', 'Cloudstorage', 'Python']",2025-06-12 00:56:17
Data Engineer,Aqilea Softech,5 - 9 years,13-20 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Job Title: Data Engineer\nCompany : Aqilea India(Client : H&M India)\nEmployment Type: Full Time\nLocation: Bangalore(Hybrid)\nExperience: 4.5 to 9 years\nClient : H&M India\n\nAt H&M, we welcome you to be yourself and feel like you truly belong. Help us reimagine the future of an entire industry by making everyone look, feel, and do good. We take pride in our history of making fashion accessible to everyone and led by our values we strive to build a more welcoming, inclusive, and sustainable industry. We are privileged to have more than 120,000 colleagues, in over 75 countries across the world. Thats 120 000 individuals with unique experiences, skills, and passions. At H&M, we believe everyone can make an impact, we believe in giving people responsibility and a strong sense of ownership. Our business is your business, and when you grow, we grow.\nWebsite : https://career.hm.com/\n\nWe are seeking a skilled and forward-thinking Data Engineer to join our Emerging Tech team. This role is designed for someone passionate about working with cutting-edge technologies such as AI, machine learning, IoT, and big data to turn complex data sets into actionable insights.\nAs the Data Engineer in Emerging Tech, you will be responsible for designing, implementing, and optimizing data architectures and processes that support the integration of next-generation technologies. Your role will involve working with large-scale datasets, building predictive models, and utilizing emerging tools to enable data-driven decision-making across the business. You ll collaborate with technical and business teams to uncover insights, streamline data pipelines, and ensure the best use of advanced analytics technologies.\n\nKey Responsibilities:\nDesign and build scalable data architectures and pipelines that support machine learning, analytics, and IoT initiatives.\nDevelop and optimize data models and algorithms to process and analyse large-scale, complex data sets.\nImplement data governance, security, and compliance measures to ensure high-quality\nCollaborate with cross-functional teams (engineering, product, and business) to translate business requirements into data-driven solutions.\nEvaluate, integrate, and optimize new data technologies to enhance analytics capabilities and drive business outcomes.\nApply statistical methods, machine learning models, and data visualization techniques to deliver actionable insights.\nEstablish best practices for data management, including data quality, consistency, and scalability.\nConduct analysis to identify trends, patterns, and correlations within data to support strategic business initiatives.\nStay updated on the latest trends and innovations in data technologies and emerging data management practices.\n\nSkills Required :\nBachelors or masters degree in data science, Computer Science, Engineering, Statistics, or a related field.\n4.5-9 years of experience in data engineering, data science, or a similar analytical role, with a focus on emerging technologies.\nProficiency with big data frameworks (e.g., Hadoop, Spark, Kafka) and experience with modern cloud platforms (AWS, Azure, or GCP).\nSolid skills in Python, SQL, and optionally R, along with experience using machine learning libraries such as Scikit-learn, TensorFlow, or PyTorch.\nExperience with data visualization tools (e.g., Tableau or Power BI or D3.js) to communicate insights effectively.\nFamiliarity with IoT and edge computing data architectures is a plus.\nUnderstanding of data governance, compliance, and privacy standards.\nAbility to work with both structured and unstructured data.\nExcellent problem-solving, communication, and collaboration skills, with the ability to work in a fast-paced, cross-functional team environment.\nA passion for emerging technologies and a continuous desire to learn and innovate.\nInterested Candidates can share your Resumes to mail id karthik.prakadish@aqilea.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Powerbi', 'Hadoop', 'Kafka', 'Tableau', 'Azure', 'GCP', 'Data Engineer', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-12 00:56:21
"Sustainable, Client and Regulatory Reporting Data Product Owner",Capital Markets,15 - 20 years,Not Disclosed,['Bengaluru'],"Hiring, Sustainable, Client and Regulatory Reporting Data Product Owner - ISS Data (Associate Director)\nAbout your team\n\nThe Technology function provides IT services that are integral to running an efficient run-the business operating model and providing change-driven solutions to meet outcomes that deliver on our business strategy. These include the development and support of business applications that underpin our revenue, operational, compliance, finance, legal, marketing and customer service functions. The broader organisation incorporates Infrastructure services that the firm relies on to operate on a day-to-day basis including data centre, networks, proximity services, security, voice, incident management and remediation.\nThe Technology group is responsible for providing Technology solutions to the Investment Solutions & Services business (which covers Investment Management, Asset Management Operations & Distribution business units globally)\n\nThe Technology team supports and enhances existing applications as well as designs, builds and procures new solutions to meet requirements and enable the evolving business strategy.\nAs part of this group, a dedicated Data Programme team has been mobilised as a key foundational programme to support the execution of the overarching Investment Solutions and Service strategy.\n\nAbout your role\nThe Investment Reporting Data Product Owner role is instrumental in the creation and execution of a future state data reporting product to enable Regulatory, Client, Vendor, Internal & MI reporting and analytics. The successful candidate will have an in- depth knowledge of all data domains that represent institutional clients , the investment life cycle , regulatory and client reporting data requirements.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned with our cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our internal business stakeholders and our clients.\n\nKey Responsibilities\n\nLeadership and Management:\nLead the ISS distribution, Client Propositions, Sustainable Investing and Regulatory reporting data outcomes defining the data roadmap and capabilities and supporting the execution and delivery of the data solutions as a Data Product lead within the ISS Data Programme.\nLine management responsibilities for junior data analysts within the chapter, coaching, influencing and motivating them for high performance.\nDefine the data product vision and strategy with end-to-end thought leadership.\nLead and define the data product backlog , documentation, enable peer-reviews, analysis effort estimation, maintain backlog, and support end to end planning.\nBe a catalyst of change for driving efficiencies, scale and innovation.\n\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering.\n\nCoordination and Communication:\nSenior management level communication to influence senior tech and business stakeholders globally, get alignment on the roadmaps.\nCoordinate with internal and external teams to communicate with those impacted by data flows.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\n\nAbout you\nThe Investment Reporting Data Product Owner role is instrumental in the creation and execution of a future state data reporting product to enable Regulatory, Client, Vendor, Internal & MI reporting and analytics. The successful candidate will have an in- depth knowledge of all data domains that represent institutional clients , the investment life cycle , regulatory and client reporting data requirements.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned with cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our internal business stakeholders and our clients.\n\nKey Responsibilities\n\nLeadership and Management:\nLead the ISS distribution, Client Propositions, Sustainable Investing and Regulatory reporting data outcomes defining the data roadmap and capabilities and supporting the execution and delivery of the data solutions as a Data Product lead within the ISS Data Programme.\nLine management responsibilities for junior data analysts within the chapter, coaching, influencing and motivating them for high performance.\nDefine the data product vision and strategy with end-to-end thought leadership.\nLead and define the data product backlog , documentation, enable peer-reviews, analysis effort estimation, maintain backlog, and support end to end planning.\nBe a catalyst of change for driving efficiencies, scale and innovation.\n\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering.\n\nCoordination and Communication:\nSenior management level communication to influence senior tech and business stakeholders globally, get alignment on the roadmaps.\nCoordinate with internal and external teams to communicate with those impacted by data flows.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\n\nYour Skills and Experience\n\nStrong leadership and senior management level communication, internal and external client management and influencing skills.\nAt least 15 years of proven experience as a senior business/technical/data analyst within technology and/or business change delivering data led business outcomes within the financial services/asset management industry.\n5-10 years as a data product owner adhering to agile methodology, delivering data solutions using industry leading data platforms such as Snowflake, State Street Alpha Data, Refinitiv Eikon, SimCorp Dimension, BlackRock Aladdin, FactSet etc.\nOutstanding knowledge of Client life cycle covering institutional & wholesale with a focus on CRM data, Transfer agency data.\nVery good understanding of the data generated by investment management processes and how that is leveraged in Go-to market capabilities such as client reporting, Sales, Marketing.\nExcellent knowledge of regulatory environment with a focus on European regulations and ESG specific ones such as MIFID II, EMIR, SFDR.\nWork effortlessly in different operating models such as insourcing, outsourcing and hybrid models.\nAutomation mindset that can drive efficiencies and quality in the reporting landscape.\nKnowledge of industry standard data calcs for fund factsheets, Institutional admin and investment reports would be an added advantage.\nIn Depth expertise in data and calculations across the investment industry covering the below.\nClient Specific data: This includes institutional and wholesale client, account and channels data, client preferences and data sets needed for client analytics. Knowledge of Salesforce desirable.\nTransfer Agency & Platform data: This includes granular client holdings at various levels, client transactions and relevant ref data. Knowledge of role of TPAs as TA and integrating external feeds/products with strategic inhouse data platforms.\nInvestment data: This includes investment life cycle data covering data domains such as trading, ABOR, IBOR, Security and fund reference.\nShould possess Problem Solving, Attention to detail, Critical thinking.\nTechnical Skills: Hands on SQL, Advanced Excel, Python, ML (optional) and knowledge of end-to-end tech solutions involving data platforms.\nKnowledge of data management, data governance, and data engineering practices\nHands on experience with data modelling techniques such as dimensional, data vault.\nWillingness to own and drive things, collaboration across business and tech stakeholders.",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Product Management,"Employment Type: Full Time, Permanent","['Data Transformation', 'ESG Framework', 'Snowflake', 'Asset Management', 'Product Owner', 'Product Manager', 'MIFID II', 'alphastate street', 'SQL', 'EMIR', 'Data Quality', 'Data Analysis', 'charles river', 'Agile', 'UK Regulatory Reporting', 'data roadmap', 'Capital Market Operations', 'Aladdin', 'SFDR.', 'Python', 'ML']",2025-06-12 00:56:24
Project Manager - Data Migration,TALWORX,9 - 14 years,22.5-35 Lacs P.A.,['Mumbai (All Areas)'],"Role & responsibilities\nDefine project scope, objectives and deliverables in collaboration with IT leads and business sponsors\nBuild & manage detailed migration plan. Coordinate with the internal client PMO function for project onboarding, configuration and tracking on approved PM tools\nCoordinate with cross functional teams & vendors\nOversee data mapping, transformation, validation and testing activities\nManage vendor coordination for tools & services supporting the migration\nEnsure compliance with data governance, security policies and regulatory requirements\nIdentify & manage migration related risks including data loss, downtime & performance issues\nEnsure adherence to QA/UAT protocols and change management process\nSupport audit readiness and documentation\nDefine, Setup and run governance forums\nCommunicate progress, risks, decisions to executive leadership and stakeholders\nDrive validation, optimization and improvement opportunities post migration to enhance data performance and usability\nMentor and guide technical teams throughout the migration journey\nLead and drive change management & knowledge transfer\n\nPreferred candidate profile\n8+ years of experience\nProven expertise delivering atleast one enterprise level end to end data warehouse management program\nSolid understanding of relational database systems, data structures and SQL\nStrong understanding of data modelling, ETL tools/processes, performance tuning and migration planning best practices\nExperience with cloud based database platforms is a plus\nStrong project management skills\nStrong knowledge of SDLC and project management methodologies (Agile, waterfall, Hybrid)\n\nPreferred Skills:\nBFSI or Life Sciences industry exposure\nExperience with data reconciliation and validation frameworks\nStrong interpersonal and communication skills\nAbility to lead cross functional, geographically distributed teams\nFamiliarity with Oracle DMS or Sybase",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['data warehouse', 'Data Migration', 'Oracle', 'SyBase', 'SQL', 'Waterfall', 'Database Management', 'Agile', 'oracle DMS', 'SDLC']",2025-06-12 00:56:26
OAC ODI Architect (Senior Oracle Analytics Consultant),Mastek,10 - 15 years,15-30 Lacs P.A.,"['Ahmedabad', 'Chennai']","We are looking for OAC ODI Architect to be based in Ahemdabad or Chennai\nMinimum Architect exp 4 Yrs\nOracle Analytics Consultant (OAC, ODI, FDI) Tech Architect\nLocation: [Specify Location or Remote] Chennai Ahmedabad\nExpected DOJ June\nEmployment Type: Full-time\nExperience Level: 10 - 15+ Years\nJob Summary:\nWe are seeking an experienced and results-driven Senior Oracle Analytics Consultant with over 10 years of hands-on experience in Oracle Analytics Cloud (OAC), Oracle Data Integrator (ODI), and Fusion Data Intelligence (FDI). The ideal candidate will have a deep understanding of enterprise data architecture, data integration best practices, and cloud-based analytics solutions. This role involves working closely with cross-functional teams to design, implement, and support advanced analytics and data integration solutions that drive business value.\nKey Responsibilities:\nLead the design, development, and deployment of analytics solutions using Oracle Analytics Cloud (OAC).\nArchitect and implement data integration pipelines using Oracle Data Integrator (ODI) for on-prem and cloud data sources.\nCollaborate with business and IT stakeholders to design and deploy Fusion Data Intelligence (FDI) based dashboards and KPIs.\nOptimize performance of OAC dashboards and reports, including data modeling and visualization best practices.\nDevelop and manage data models, RPDs, and semantic layers within OAC.\nBuild and maintain ETL mappings, packages, and workflows in ODI.\nIntegrate Oracle Fusion Applications with OAC and FDI for near-real-time reporting.\nDrive data governance and quality initiatives across analytics platforms.\nTroubleshoot technical issues and provide solutions in a timely manner.\nMentor junior developers and provide technical leadership on complex projects.\nQualifications:\nBachelors or Masters degree in Computer Science, Information Systems, or related field.\n10+ years of relevant experience with strong focus on:\nOracle Analytics Cloud (OAC) - Must\nOracle Data Integrator (ODI) - Must\nFusion Data Intelligence (FDI) Good to Have\nExpertise in Oracle Fusion ERP/HCM data models and subject areas.\nExperience integrating multiple data sources, including on-premise and cloud systems.\nStrong understanding of SQL, PL/SQL, and performance tuning.\nFamiliarity with data lake architecture, data warehousing, and ELT/ETL design patterns.\nProven experience working in Agile/DevOps environments.\nExcellent communication, analytical thinking, and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oac', 'ODI', 'Odi Architecture', 'FDI']",2025-06-12 00:56:29
Lead Data Analyst-Business Intelligence,Tresvista Financial Services,6 - 10 years,Not Disclosed,['Bengaluru'],"Roles and Responsibilities\nArchitect and incorporate an effective Data framework enabling end to end Data Solution.\nUnderstand business needs, use cases and drivers for insights and translate them into detailed technical specifications.\nCreate epics, features and user stories with clear acceptance criteria for execution and delivery by the data engineering team.\nCreate scalable and robust data solution designs that incorporate governance, security and compliance aspects.\nDevelop and maintain logical and physical data models and work closely with data engineers, data analysts and data testers for successful implementation of them.\nAnalyze, assess and design data integration strategies across various sources and platforms.\nCreate project plans and timelines while monitoring and mitigating risks and controlling progress of the project.\nConduct daily scrum with the team with a clear focus on meeting sprint goals and timely resolution of impediments.\nAct as a liaison between technical teams and business stakeholders and ensure.\nGuide and mentor the team for best practices on Data solutions and delivery frameworks.\nActively work, facilitate and support the stakeholders/ clients to complete User Acceptance Testing ensure there is strong adoption of the data products after the launch.\nDefining and measuring KPIs/KRA for feature(s) and ensuring the Data roadmap is verified through measurable outcomes\n\nPrerequisites\n5 to 8 years of professional, hands on experience building end to end Data Solution on Cloud based Data Platforms including 2+ years working in a Data Architect role.\nProven hands on experience in building pipelines for Data Lakes, Data Lake Houses, Data Warehouses and Data Visualization solutions\nSound understanding of modern Data technologies like Databricks, Snowflake, Data Mesh and Data Fabric.\nExperience in managing Data Life Cycle in a fast-paced, Agile / Scrum environment.\nExcellent spoken and written communication, receptive listening skills, and ability to convey complex ideas in a clear, concise fashion to technical and non-technical audiences\nAbility to collaborate and work effectively with cross functional teams, project stakeholders and end users for quality deliverables withing stipulated timelines\nAbility to manage, coach and mentor a team of Data Engineers, Data Testers and Data Analysts. Strong process driver with expertise in Agile/Scrum framework on tools like Azure DevOps, Jira or Confluence\nExposure to Machine Learning, Gen AI and modern AI based solutions.\n\nExperience\nTechnical Lead Data Analytics with 6+ years of overall experience out of which 2+ years is on Data architecture.\n\nEducation\nEngineering degree from a Tier 1 institute preferred.\n\nCompensation\nThe compensation structure will be as per industry standards",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Bricks', 'Data Lake', 'Data Warehousing', 'Python', 'Business Intelligence', 'Databricks Engineer', 'Machine Learning', 'Redshift Aws', 'Snowflake', 'Data Visualization', 'ETL', 'Data Mesh']",2025-06-12 00:56:32
Manager Data Engineer – Research Data and Analytics,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will create and develop data lake solutions for scientific data that drive business decisions for Research. You will build scalable and high-performance data engineering solutions for large scientific datasets and collaborate with Research collaborators. You will also provide technical leadership to junior team members. The ideal candidate possesses experience in the pharmaceutical or biotech industry, demonstrates deep technical skills, is proficient with big data technologies, and has a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nLead, manage, and mentor a high-performing team of data engineers\nDesign, develop, and implement data pipelines, ETL processes, and data integration solutions\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks\nDevelop and maintain data models for biopharma scientific data, data dictionaries, and other documentation to ensure data accuracy and consistency\nOptimize large datasets for query performance\nCollaborate with global multi-functional teams including research scientists to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nCollaborate with Data Architects, Business SMEs, Software Engineers and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve data-related challenges\nAdhere to best practices for coding, testing, and designing reusable code/component\nExplore new tools and technologies that will help to improve ETL platform performance\nParticipate in sprint planning meetings and provide estimations on technical implementation\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nDoctorate Degree OR\nMasters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications:\n3+ years of experience in implementing and supporting biopharma scientific research data analytics (software platforms)\n\n\nFunctional Skills:\nMust-Have Skills:\nProficiency in SQL and Python for data engineering, test automation frameworks (pytest), and scripting tasks\nHands on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing\nExcellent problem-solving skills and the ability to work with large, complex datasets\nAble to engage with business collaborators and mentor team to develop data pipelines and data models\n\n\nGood-to-Have Skills:\nA passion for tackling complex challenges in drug discovery with technology and data\nGood understanding of data modeling, data warehousing, and data integration concepts\nGood experience using RDBMS (e.g. Oracle, MySQL, SQL server, PostgreSQL)\nKnowledge of cloud data platforms (AWS preferred)\nExperience with data visualization tools (e.g. Dash, Plotly, Spotfire)\nExperience with diagramming and collaboration tools such as Miro, Lucidchart or similar tools for process mapping and brainstorming\nExperience writing and maintaining technical documentation in Confluence\nUnderstanding of data governance frameworks, tools, and best practices\n\n\nProfessional Certifications:\nDatabricks Certified Data Engineer Professional preferred\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Spotfire', 'PySpark', 'PostgreSQL', 'Plotly', 'SparkSQL', 'SQL server', 'SQL', 'process mapping', 'Dash', 'MySQL', 'ETL', 'Oracle', 'data governance frameworks', 'Python']",2025-06-12 00:56:35
Data Migration Engineer,A Global Engineering and Technology Solu...,4 - 6 years,Not Disclosed,['Pune'],"Roles and Responsibilities\nDesign, develop, test, and deploy data migration solutions using various tools such as PTC, WBM, CAD, PDM Link, Java Utilities, Loaders, SQL, and Oracle.\nCollaborate with cross-functional teams to identify business requirements and design data migration strategies that meet those needs.\nDevelop complex database queries to extract relevant data from legacy systems for migration into new platforms.\nConduct thorough testing of migrated data to ensure accuracy and integrity.\nProvide technical guidance on best practices for data management and governance.\nDesired Candidate Profile\n4-6 years of experience in Data Migration Engineering with expertise in one or more of the following areas: PTC/WBM/CAD/PDM Link/Java Utilities/Loaders/SQL/Oracle.\nBachelor's degree in Any Specialization (B.Tech/B.E.).\nStrong understanding of software development life cycle (SDLC) principles and methodologies.\nProficiency in writing efficient code using programming languages like Java or Python.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['WBM', 'Pdm Link', 'Java utilities', 'Oracle', 'Windchill', 'loaders', 'SQL']",2025-06-12 00:56:39
Data Engineer,Nemetschek,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\n5+ years in software development, with a focus on data-intensive applications, cloud solutions, and scalable data architectures.\nDevelopment experience in GoLang for building scalable and efficient data applications.\nExperience with Snowflake, Redshift, or similar data platforms including architecture, data modeling, performance optimization, and integrations.\nExperience designing and building data lakes and data warehouses, ensuring data integrity, scalability, and performance.\nProficient in developing and managing ETL pipelines, using modern tools and techniques to transform, load, and integrate data efficiently.\nExperience with high-volume event streams (such as Kafka, Kinesis) and near real-time data processing solutions for fast and accurate reporting.\nHands-on experience with Terraform for automating infrastructure deployment and configuration management in cloud environments.\nExperience with containerization technologies (Docker, Kubernetes) and orchestration.\nSolid grasp of database fundamentals (SQL, NoSQL, data modeling, performance tuning)\nExperience with CI/CD pipelines and automation tools for testing, deployment, and continuous improvement.\nExperience working in AWS cloud environments, specifically with big data solutions and serverless architectures\nAbility to mentor and guide junior engineers, fostering a culture of learning and innovation\nStrong communication skills to articulate technical concepts clearly to non-technical stakeholders.\nWHAT WE OFFER\nA young, dynamic, and innovation-oriented environment\nA wide variety of projects within different industries\nA very open and informal culture where knowledge sharing, and employee development are key.\nRoom for personal initiative, development, and growth\nRealistic career opportunities\nCompetitive package and fringe benefits.\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Golang', 'Snowflake', 'Javascript', 'ETL', 'AWS']",2025-06-12 00:56:42
Salesforce Data Cloud Developer,Avenoir Technologies,3 - 8 years,Not Disclosed,"['Bengaluru', 'Delhi / NCR', 'Mumbai (All Areas)']","Develop and implement solutions within Salesforce Data Cloud, focusing on data-driven insights and integrations.\nDesign, develop, and maintain custom solutions using Apex and Lightning Web Components (LWC).\nIntegrate Salesforce Data Cloud\n\nRequired Candidate profile\n3 years of experience as a Salesforce Developer with a strong focus on Salesforce Data Cloud.\nCompleted 2 successful Salesforce Data Cloud projects.\nStrong skills in Apex and Lightning Web Components.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Salesforce Data Cloud', 'Data Transformation', 'Einstein Analytics', 'Einstein Bots', 'modeling', 'duplicate management', 'LWC', 'data architecture', 'Tableau', 'identity resolution', 'Apex']",2025-06-12 00:56:45
Data Migration Expert - Talend,Maddisoft Solutions,10 - 20 years,Not Disclosed,['Hyderabad'],"Job Title: Data Migration Expert - Talend\nLocation: Hyderabad, India\n\nJob Description:\nMinimum of 9+ years of experience in data migration in Talend projects\nHandsons experience 5+ mandatory knowledge of Talend/SQL tools(Basic knowledge of SAP Routing and SAP Production order tables Individually is an add on)\nProficiency in data migration tools and methodologies, SAP ECC AND S/4HANA Migration Cockpit.\nHands-on experience Data Replication, Data Quality, Data Workbench, Talend or similar ETL tools.\nFamiliarity with Talend (ETL) is plus.\nStrong understanding of data modeling concepts, data mapping techniques, and data transformation rules.\nExcellent SQL skills for data extraction, manipulation, and analysis.\nExperience with SAP all Cross functional modules such as Finance (FI), Controlling (CO), Material Management (MM), Sales and Distribution (SD), or Production Planning (PP).\nStrong analytical, problem-solving, and troubleshooting skills.\nExcellent communication, presentation, and interpersonal skills.\nAbility to work independently and as part of a team in a fast-paced, dynamic environment.\nSAP certification in Data Migration or related field is a plus.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['SAP ECC', 'Data Migration', 'Talend']",2025-06-12 00:56:47
Azure Data Bricks (4-15 Yrs) - Bangalore,Happiest Minds Technologies,4 - 9 years,Not Disclosed,['Bengaluru'],"Hi,\n\nGreetings from Happiest Minds Technologies\n\nCurrently we are hiring for below positions and looking for immediate joiners.\n1. Azure Databricks Bangalore 5 to 10 Yrs - Bangalore\nAs a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools. Proficiency in Python PySpark, Pandas, NumPy, SciPy, Spark SQL, DataFrames, RDDs, Delta Lake, Databricks Notebooks, and MLflow is required, along with hands-on experience in Azure Data Lake, Blob Storage, and Synapse Analytics.",,,,"['Pyspark', 'Azure', 'Data Bricks', 'sql', 'ETL']",2025-06-12 00:56:50
"Data Conversion & ETL Test Lead (Ab Initio, SQL, Python)",Teamware Solutions a division of Quantu...,6 - 11 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","About the Company\nGreetings from Teamware Solutions a division of Quantum Leap Consulting Pvt. Ltd\n\nAbout the Role\nWe are hiring a Data Conversion & ETL Test Lead (Ab Initio, SQL, Python)\nLocation: Bangalore, Hyderabad\nWork Model: Hybrid - 2nd Shift 2-11 PM\nExperience: 6-12 Years\nNotice Period: Immediate to 15 Days\n\nData Conversion Test Lead / ETL Lead\nCollaborate with upstream/downstream to understand Data workflows and requirements.\nCollaborate with business analysts, data architects, and developers to understand ETL requirements and data mapping documents.\nExperience with ETL tools such as Ab initio or similar\nDesign and execute functional and integration test cases for ETL processes involving Oracle databases and related systems.\nValidate data transformations, mappings, and data loads (initial and incremental).\nPerform source-to-target data validation, data quality checks, and reconciliation.\nAnalyze ETL job logs, and error reports, and track defects through resolution using tools like JIRA or ALM.\nWork closely with Oracle implementation teams to test data from Oracle EBS/ERP, Oracle Fusion, or other modules as applicable.\nAutomate test cases where applicable using SQL or data testing tools.\nPerform data validation and testing to ensure data accuracy and integrity.\nSolid understanding of SQL and database concepts.\nProven experience in ETL testing and automation.\nStrong proficiency in Python programming.\nEnsure compliance with industry standards and best practices in data testing.\nKnowledge of data warehousing and data modeling concepts.\n\nMandatory skills:\nData Conversion Testing | Ab Initio | SQL | ETL | Data Quality | Data Validation | Upstream/Downstream Validations |\nOne year of experience in Python.\n\nNice to Have:\nOracle GL Implementation | R2R | P2P | Functional Testing\n\nPlease let me know if you are interested in this position and send me your resumes to netra.s@twsol.com",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Conversion Test', 'Ab Initio', 'ETL Testing', 'Data Validation', 'Data Quality', 'Oracle GL', 'ETL Lead', 'SQL']",2025-06-12 00:56:53
Digital Solution Architect Lead Advisor,"NTT DATA, Inc.",7 - 12 years,Not Disclosed,['Pune'],"Req ID: 301930\n\nWe are currently seeking a Digital Solution Architect Lead Advisor to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\n Position Overview  We are seeking a highly skilled and experienced Data Solution Architect to join our dynamic team. The ideal candidate will have a strong background in designing and implementing data solutions using AWS infrastructure and a variety of core and supplementary technologies. This role requires a deep understanding of data architecture, cloud services, and the ability to drive innovative solutions to meet business needs.\n\n\n\n Key Responsibilities  \n\n- Architect end-to-end data solutions using AWS services, including Lambda, SNS, S3, and EKS\n\n- Design and implement data streaming pipelines using Kafka/Confluent Kafka\n\n- Develop data processing applications using Python\n\n- Ensure data security and compliance throughout the architecture\n\n- Collaborate with cross-functional teams to understand business requirements and translate them into technical solutions\n\n- Optimize data flows for performance, cost-efficiency, and scalability\n\n- Implement data governance and quality control measures\n\n- Provide technical leadership and mentorship to development teams\n\n- Stay current with emerging technologies and industry trends\n\n\n\n Required Skills and Qualifications  \n\n\n\n- Bachelor's degree in Computer Science, Engineering, or related field\n\n- 7+ years of experience in data architecture and engineering\n\n- Strong expertise in AWS cloud services, particularly Lambda, SNS, S3, and EKS\n\n- Proficiency in Kafka/Confluent Kafka and Python\n\n- Experience with Synk for security scanning and vulnerability management\n\n- Solid understanding of data streaming architectures and best practices\n\n- Strong problem-solving skills and ability to think critically\n\n- Excellent communication skills to convey complex technical concepts to both technical and non-technical stakeholders",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'cloud services', 'data architecture', 'aws cloud', 'kafka', 'emerging technologies', 'kubernetes', 'aws iam', 'technical leadership', 'aws infrastructure', 'vulnerability management', 'eks', 'docker', 'ansible', 'java', 'lambda expressions', 'devops', 'data governance', 'sns', 'terraform', 'aws']",2025-06-12 00:56:58
Salesforce Marketing Cloud Architect,"NTT DATA, Inc.",10 - 15 years,Not Disclosed,['Hyderabad'],"We are currently seeking a Salesforce Marketing Cloud Architect to join our team in ""‹""‹""‹""‹""‹""‹""‹Hyderabad, Telangana, ""‹""‹""‹""‹""‹""‹""‹India.\n\nRoleSalesforce Marketing Cloud Architect\n\n\n\nResponsibilities:\nServe as a trusted advisor to key stakeholders within our Enterprise clients.\nEnsure scalable, best-practice solutions that meet or exceed customer expectations.\nEvaluate and translate business and technical requirements into well-architected solutions that effectively leverage Salesforce products.\nIdentify and mitigate solution and business design risks.\nBuild and maintain strong relationships with key stakeholders and team members.\nLead overall architecture alignment and coordinate efforts across multiple architects.\nCollaborate with project and engagement managers to support planning and execution in partnership with the client.\nOversee project vision, direction, and the review of key deliverables.\nDrive early solution evaluations, manage issues proactively, and engage with executive teams, engineering, and product management.\nDirect and mentor diverse teams in both technical and non-technical aspects, including communication strategies and executive influence.\nContribute to internal growth through initiatives, knowledge-sharing, and the development of strategic assets.\nSupport the Pre-Sales team in developing proposals, including target and transition architectures, security and compliance considerations, integration strategies, data migration plans, and implementation roadmaps.\n\n\n:\n\n\nProven Experience: 10+ years proven experience in enterprise consulting, including implementing enterprise software solutions in the Commerce space as a lead developer or architect\n\n\nSalesforce certificationsMarketing Architect\n\n\nMarketing Cloud Expertise:\nSalesforce Composable marketing offerings and able to shape end-to-end solutions using the Salesforce marketing platform\nKnowledgeable on key areas including\nProducts, Product Catalogues & Product Items\nPricing & Promotions\nOrders\nBaskets, Checkouts & Payment Methods\nShipping Methods\nShopper Login (SLAS)\n\n\nStrong knowledge on Headless Marketing concepts and design patterns\nExperience of Managed Runtime (fka Mobify) strongly beneficial\nCompetence with SFCC's PWA Kit & Node JS\nExperience building pixel-perfect/custom front ends with React\nExperience with headless integration patterns including SCAPI\nKnowledge of DevOps best practices\nKnowledge of Quality Code & Code Review best practices\nSuccessful candidates will join an existing SFCC delivery team and will be expected to reuse/leverage existing build components into an expanding delivery team.\nCandidates will be asked to contribute to overall multi-cloud solution architectures and end-to-end business processes, whilst taking technical ownership within the SFCC domain.\nExperience with integration technologies, master data management, and familiarity with other cloud platforms (e.g., AWS) is preferred.\nExtensive experience with Agile, Scrum, and Waterfall methodologies.\n\n\nPreferred Qualifications:\n\n\nSalesforce Commerce Knowledge: Knowledge in Salesforce Order Management and Salesforce B2B/D2C Commerce\n\n\nSalesforce Knowledge: Additional Salesforce certifications (Salesforce Administrator, Experience cloud, Experience implementing Salesforce Clouds, Multi-cloud scenarios - Sales, Service and\n\nMarketing)\n\n\nTechnical Familiarity: Familiarity with Salesforce""™s technical architectureAPIs, Standard and Custom Objects, APEX, AI concepts, Customer Data Platforms or Data LakesAdditional Salesforce certifications such as Marketing Cloud Consultant, Integration Architect, AI Associate, or AI Specialist.\nIn-depth knowledge of web services, data modelling, and enterprise application integration, including experience with ESBs, ETL tools, and common integration design patterns with systems like CMS, ERP, HRIS, and data warehouses.\nAgile Methodology certification, such as Scaled Agile Framework (SAFe), is a plus.\n\nRoles and Responsibilities-\n\nUtilisation of relevant modules within Marketing Cloud Builder customer journeys Analytics of campaigns and making recommendations Personalisation using AMPscript Implementing SFMC Configuration beyond declarative methods (beyond point-and-click) Setting up new modules Integrating with other clouds and third-party platforms\n\n\n\nSkills required- A good level of hands-on experience with SFMC Experience using Journey Builder with the ability to build more complex journeys for common use cases. Some knowledge of AMPscript Ability to build data extensions Previous experience with integration projects Experience with SFMC Connector Good knowledge of APIs Advanced AMPscript knowledge - Email Specialist certification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['web services', 'waterfall', 'design patterns', 'scrum', 'agile', 'ampscript', 'erp', 'hris', 'cms', 'technology integration', 'master data management', 'salesforce', 'enterprise application integration', 'marketing', 'node.js', 'b2b', 'data modeling', 'esb', 'devops', 'pwa', 'sfmc', 'aws', 'etl', 'marketing cloud']",2025-06-12 00:57:01
