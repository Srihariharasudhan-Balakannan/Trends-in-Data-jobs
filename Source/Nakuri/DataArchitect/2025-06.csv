title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Data Architect,Acesoft,6 - 10 years,19-22.5 Lacs P.A.,['Bengaluru'],"Hi all,\nWe are hiring fore the Data Architecture\nExperience: 6 - 9 years\nLocation: Bangalore\nNotice Period: Immediate - 15 Days\nSkills:\nData Architecture\nAzure Data Factory\nAzure Data Bricks\nAzure Cloud\nArchitecture\n\nIf you are interested drop your resume at mojesh.p@acesoftlabs.com\nCall: 9701971793",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Azure Data Factory', 'Architecture', 'Azure Databricks', 'Data Architecture', 'Azure Synapse', 'Data Modeling']",2025-06-12 14:22:11
Data Architect,CGI,8 - 12 years,15-30 Lacs P.A.,['Hyderabad'],"Job Title: Data Architect / Data Modeler\nExperience Level: 8+ Years\nLocation: Hyderabad\nJob Summary\nWe are seeking a highly experienced Data Architect to join our growing Data & Analytics team. This role demands a strategic thinker and technical expert who can design and build robust, scalable, and efficient data solutions. You will play a critical role in architecting end-to-end data pipelines, designing optimized data models, and delivering business-centric data infrastructure using cutting-edge technologies such as Python, PySpark, SQL, Snowflake, and/or Databricks.\nThe ideal candidate will have a deep understanding of data engineering best practices and a proven track record of enabling data-driven decision-making through innovative and scalable data solutions.\nKey Responsibilities\nArchitect & Design Scalable Data Pipelines\nLead the design and implementation of high-performance, scalable, and maintainable data pipelines that support batch and real-time processing.\nData Modeling & Data Architecture\nDesign and implement optimized data models and database schemas to support analytics, reporting, and machine learning use cases.\nCloud Data Platforms\nDevelop and manage modern cloud-based data architectures using platforms like Snowflake or Databricks, ensuring performance, security, and cost-efficiency.\nData Integration & ETL Development\nBuild robust ETL/ELT workflows to ingest, transform, and provision data from a variety of internal and external sources.\nCollaboration with Stakeholders\nWork closely with data analysts, data scientists, product managers, and business leaders to translate business requirements into technical specifications and data solutions.\nData Quality & Governance\nImplement and advocate for best practices in data quality, security, compliance, lineage, and governance.\nPerformance Optimization\nOptimize data storage and query performance using advanced SQL, partitioning, indexing, caching strategies, and compute resource tuning.\nMentorship & Best Practices\nProvide mentorship to junior engineers, establish coding standards, and contribute to the growth and maturity of the data engineering practice.\nRequired Qualifications\nBachelors or Masterâ€™s degree in Computer Science, Engineering, Data Science, or a related field.\n8+ years of experience in data engineering or related roles.\nStrong expertise in Python and PySpark for data processing and transformation.\nProficient in advanced SQL with a deep understanding of query optimization and performance tuning.\nHands-on experience with Snowflake and/or Databricks in a production environment.\nExperience in designing and implementing data warehouses and data lakes.\nSolid understanding of distributed computing frameworks, big data ecosystems, and modern data architecture patterns.\nExperience with CI/CD, version control systems (e.g., Git), and workflow orchestration tools (e.g., Airflow, dbt, etc.).\nStrong communication skills with the ability to clearly articulate technical concepts to non-technical stakeholders.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'sql', 'snowflake', 'Data Architecture']",2025-06-12 14:22:13
Data Architect,Ford,14 - 17 years,Not Disclosed,['Chennai'],"We are looking for Data Solution Architect to join FC India IT Architecture team. In this role, you will define analytics solutions and guide engineering teams to implement big data solutions on the cloud. Work involves migrating data from legacy on-prem warehouses to Google cloud data platform. This role will provide architecture assistance to data engineering teams in India, with key responsibility of supporting applications globally. This role will also drive business adoption of the new platform and sunset of legacy platforms.\nGoogle Professional Solution Architect certification.\n8+ years of relevant work experience in analytics application and data architecture, with deep understanding of cloud hosting concepts and implementations.\n5+ years experience in Data and Solution Architecture in analytics space. Solid knowledge of cloud data architecture, data modelling principles, and expertise in Data Modeling tools.\nExperience in migrating legacy analytics applications to Cloud platform and business adoption of these platforms to build insights and dashboards through deep knowledge of traditional and cloud Data Lake, Warehouse and Mart concepts.\nGood understanding of domain driven design and data mesh principles.\nExperience with designing, building, and deploying ML models to solve business challenges using Python/BQML/Vertex AI on GCP.\nKnowledge of enterprise frameworks and technologies. Strong in architecture design patterns, experience with secure interoperability standards and methods, architecture tolls and process.\nDeep understanding of traditional and cloud data warehouse environment, with hands on programming experience building data pipelines on cloud in a highly distributed and fault-tolerant manner. Experience using Dataflow, pub/sub, Kafka, Cloud run, cloud functions, Bigquery, Dataform, Dataplex , etc.\nStrong understanding on DevOps principles and practices, including continuous integration and deployment (CI/CD), automated testing & deployment pipelines.\nGood understanding of cloud security best practices and be familiar with different security tools and techniques like Identity and Access Management (IAM), Encryption, Network Security, etc. Strong understanding of microservices architecture.\nNice to Have\nBachelor s degree in Computer science/engineering, Data science or related field.\nStrong leadership, communication, interpersonal, organizing, and problem-solving skills\nGood presentation skills with ability to communicate architectural proposals to diverse audiences (user groups, stakeholders, and senior management).\nExperience in Banking and Financial Regulatory Reporting space.\nAbility to work on multiple projects in a fast paced & dynamic environment.\nExposure to multiple, diverse technologies, platforms, and processing environments.\nUtilize Google Cloud Platform & Data Services to modernize legacy applications.\nUnderstand technical business requirements and define architecture solutions that align to Ford Motor & Credit Companies Patterns and Standards.\nCollaborate and work with global architecture teams to define analytics cloud platform strategy and build Cloud analytics solutions within enterprise data factory.\nProvide Architecture leadership in design & delivery of new Unified data platform on GCP.\nUnderstand complex data structures in analytics space as well as interfacing application systems. Develop and maintain conceptual, logical & physical data models. Design and guide Product teams on Subject Areas and Data Marts to deliver integrated data solutions.\nProvide architectural guidance for optimal solutions considering regional Regulatory needs.\nProvide architecture assessments on technical solutions and make recommendations that meet business needs and align with architectural governance and standard.\nGuide teams through the enterprise architecture processes and advise teams on cloud-based design, development, and data mesh architecture.\nProvide advisory and technical consulting across all initiatives including PoCs, product evaluations and recommendations, security, architecture assessments, integration considerations, etc.\nLeverage cloud AI/ML Platforms to deliver business and technical requirements.",Industry Type: Auto Components,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Data modeling', 'Access management', 'Enterprise architecture', 'Consulting', 'Network security', 'Data structures', 'Analytics', 'Python']",2025-06-12 14:22:15
Snowflake Data Architect,Kasmo Digital,10 - 16 years,Not Disclosed,['Hyderabad'],"Required Skills & Qualifications:\n10-12 years of experience in data architecture, data warehousing, and cloud technologies.\nStrong expertise in Snowflake architecture, data modeling, and optimization.\nSolid hands-on experience with cloud platforms: AWS, Azure, and GCP.\nIn-depth knowledge of SQL, Python, PySpark, and related data engineering tools.\nExpertise in data modeling (both dimensional and normalized models).\nStrong experience with data integration, ETL processes, and pipeline development.\nCertification in Snowflake, AWS, Azure, or related cloud technologies.\nExperience working with large-scale data processing frameworks and platforms.\nExperience in data visualization tools and BI platforms (e.g., Tableau, Power BI).\nExperience in Agile methodologies and project management.\nStrong problem-solving skills with the ability to address complex technical challenges.\nExcellent communication skills and ability to work collaboratively with cross-functional teams.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Visualization', 'Data Modeling', 'Data Warehousing', 'SQL', 'Data Architecture', 'Python']",2025-06-12 14:22:17
Data Architect,Ford,8 - 13 years,Not Disclosed,['Chennai'],"We are looking for Data Solution Architect to join FC India IT Architecture team. In this role, you will define analytics solutions and guide engineering teams to implement big data solutions on the cloud. Work involves migrating data from legacy on-prem warehouses to Google cloud data platform. This role will provide architecture assistance to data engineering teams in India, with key responsibility of supporting applications globally. This role will also drive business adoption of the new platform and sunset of legacy platforms.\nGoogle Professional Solution Architect certification.\n8+ years of relevant work experience in analytics application and data architecture, with deep understanding of cloud hosting concepts and implementations.\n5+ years experience in Data and Solution Architecture in analytics space. Solid knowledge of cloud data architecture, data modelling principles, and expertise in Data Modeling tools.\nExperience in migrating legacy analytics applications to Cloud platform and business adoption of these platforms to build insights and dashboards through deep knowledge of traditional and cloud Data Lake, Warehouse and Mart concepts.\nGood understanding of domain driven design and data mesh principles.\nExperience with designing, building, and deploying ML models to solve business challenges using Python/BQML/Vertex AI on GCP.\nKnowledge of enterprise frameworks and technologies. Strong in architecture design patterns, experience with secure interoperability standards and methods, architecture tolls and process.\nDeep understanding of traditional and cloud data warehouse environment, with hands on programming experience building data pipelines on cloud in a highly distributed and fault-tolerant manner. Experience using Dataflow, pub/sub, Kafka, Cloud run, cloud functions, Bigquery, Dataform, Dataplex , etc.\nStrong understanding on DevOps principles and practices, including continuous integration and deployment (CI/CD), automated testing & deployment pipelines.\nGood understanding of cloud security best practices and be familiar with different security tools and techniques like Identity and Access Management (IAM), Encryption, Network Security, etc. Strong understanding of microservices architecture.\nNice to Have\nBachelor s degree in Computer science/engineering, Data science or related field.\nStrong leadership, communication, interpersonal, organizing, and problem-solving skills\nGood presentation skills with ability to communicate architectural proposals to diverse audiences (user groups, stakeholders, and senior management).\nExperience in Banking and Financial Regulatory Reporting space.\nAbility to work on multiple projects in a fast paced & dynamic environment.\nExposure to multiple, diverse technologies, platforms, and processing environments.\nUtilize Google Cloud Platform & Data Services to modernize legacy applications.\nUnderstand technical business requirements and define architecture solutions that align to Ford Motor & Credit Companies Patterns and Standards.\nCollaborate and work with global architecture teams to define analytics cloud platform strategy and build Cloud analytics solutions within enterprise data factory.\nProvide Architecture leadership in design & delivery of new Unified data platform on GCP.\nUnderstand complex data structures in analytics space as well as interfacing application systems. Develop and maintain conceptual, logical & physical data models. Design and guide Product teams on Subject Areas and Data Marts to deliver integrated data solutions.\nProvide architectural guidance for optimal solutions considering regional Regulatory needs.\nProvide architecture assessments on technical solutions and make recommendations that meet business needs and align with architectural governance and standard.\nGuide teams through the enterprise architecture processes and advise teams on cloud-based design, development, and data mesh architecture.\nProvide advisory and technical consulting across all initiatives including PoCs, product evaluations and recommendations, security, architecture assessments, integration considerations, etc.\nLeverage cloud AI/ML Platforms to deliver business and technical requirements.",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Data modeling', 'Access management', 'Enterprise architecture', 'Consulting', 'Network security', 'Data structures', 'Analytics', 'Python']",2025-06-12 14:22:19
Data Architect,Opus Technologies,10 - 16 years,35-50 Lacs P.A.,['Pune'],"Role & responsibilities\nDefine and evolve data engineering & analytics offerings aligned with payments domain needs (e.g., transaction analytics, fraud detection, customer insights).\nLead reference architecture creation for data modernization, real-time analytics, and cloud-native data platforms (e.g., Azure Synapse, GCP BigQuery).\nBuild reusable components, PoCs, and accelerators for ingestion, transformation, data quality, and governance.\nSupport pre-sales engagements with solution design, estimation, and client workshops.\nGuide delivery teams on data platform implementation, optimization, and security.\nMentor and upskill talent pool through learning paths and certifications.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Spark', 'Python', 'Azure Data Factory']",2025-06-12 14:22:22
Data Architect,"NTT DATA, Inc.",3 - 7 years,Not Disclosed,['Bengaluru'],"Additional Career Level Description\n\n\nKnowledge and application\nApplies advanced wide-ranging experience and in-depth professional knowledge to develop and resolve complex models and procedures in creative way .\nDirects the application of existing principles and guides development of new policies and ideas.\nDetermines own methods and procedures on new assignments .\n\n\n\nProblem solving\nUnderstands and works on complex issues where analysis of situation or data requires an in-depth evaluation of variable factors, solutions may need to be devised from limited informatio n.\nExercises judgment in selecting methods, evaluating, adapting of complex techniques and evaluation criteria for obtaining results.\n\n\n\nInteraction\nFrequently advises key people outside own area of expertise on complex matters, using persuasion in delivering messages.\n\n\n\nImpact\nDevelops and manages operational initiatives to deliver tactical results and achieve medium-term goals.\n\n\n\nAccountability\nMay be accountable through team for delivery of tactical business targets .\nWork is reviewed upon completion and is consistent with departmental objectives.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Architecture', 'data modeling', 'Data Architect', 'artificial intelligence', 'sql']",2025-06-12 14:22:24
Data Architect / Engagement Lead,Ignitho,7 - 10 years,Not Disclosed,['Chennai( Sholinganallur )'],"Job Title: Data Architect / Engagement Lead\nLocation: Chennai\nReports To: CEO\n\nAbout the Company:\nIgnitho Inc. is a leading AI and data engineering company with a global presence, including US, UK, India, and Costa Rica offices.\nVisit our website to learn more about our work and culture: www.ignitho.com.\nIgnitho is a portfolio company of Nuivio Ventures Inc., a venture builder dedicated to developing Enterprise AI product companies across various domains, including AI, Data Engineering, and IoT.\nLearn more about Nuivio at: www.nuivio.com.\n\nJob Summary:\nAs the Data Architect and Engagement Lead, you will define the data architecture strategy and lead client engagements, ensuring alignment between data solutions and business goals. This dual role blends technical leadership with client-facing responsibilities.\n\nKey Responsibilities:\nDesign scalable data architectures, including storage, processing, and integration layers.\nLead technical discovery and requirements gathering sessions with clients.\nProvide architectural oversight for data and AI solutions.\nAct as a liaison between technical teams and business stakeholders.\nDefine data governance, security, and compliance standards.\n\nRequired Qualifications:\nBachelors or Masters in computer science, Information Systems, or similar.\n7+ years of experience in data architecture, with client-facing experience.\nDeep knowledge of data modelling, cloud data platforms (Snowflake / BigQuery/ Redshift / Azure), and orchestration tools.\nExcellent communication, stakeholder management, and technical leadership skills.\nFamiliarity with AI/ML systems and their data requirements is a strong plus.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Data Modeling', 'Azure Cloud', 'Bigquery', 'Redshift Aws', 'Artificial Intelligence', 'Snowflake', 'Machine Learning']",2025-06-12 14:22:26
Data Architect,HARMAN,10 - 15 years,Not Disclosed,['Bengaluru'],"Introduction: HARMAN Technology Services (HTS)\n.\nCombine the physical and digital, making technology a more dynamic force to solve challenges and serve humanity s needs\nWork at the convergence of cross channel UX, cloud, insightful data, IoT and mobility\nEmpower companies to create new digital business models, enter new markets, and improve customer experiences",,,,"['Cloud computing', 'Data analysis', 'Project management', 'Manager Technology', 'Healthcare', 'Oracle', 'microsoft', 'RFP', 'Information technology', 'Automotive']",2025-06-12 14:22:28
Data Architect,"NTT DATA, Inc.",8 - 13 years,Not Disclosed,['Chennai'],"Req ID: 324664\n\nWe are currently seeking a Data Architect to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDevelop and articulate long-term strategic goals for data architecture vision and establish data standards for enterprise systems.\n\nUtilize various cloud technologies, including Azure, AWS, GCP, and data platforms like Databricks and Snowflake.\n\nConceptualize and create an end-to-end vision outlining the seamless flow of data through successive stages.\n\nInstitute processes for governing the identification, collection, and utilization of corporate metadata, ensuring accuracy and validity.\n\nImplement methods and procedures for tracking data quality, completeness, redundancy, compliance, and continuous improvement.\n\nEvaluate and determine governance, stewardship, and frameworks for effective data management across the enterprise.\n\nDevelop comprehensive strategies and plans for data capacity planning, data security, life cycle data management, scalability, backup, disaster recovery, business continuity, and archiving.\n\nIdentify potential areas for policy and procedure enhancements, initiating changes where required for optimal data management.\n\nFormulate and maintain data models and establish policies and procedures for functional design.\n\nOffer technical recommendations to senior managers and technical staff in the development and implementation of databases and documentation.\n\nStay informed about upgrades and emerging database technologies through continuous research.\n\nCollaborate with project managers and business leaders on all projects involving enterprise data.\n\nDocument the data architecture and environment to ensure a current and accurate understanding of the overall data landscape.\n\nDesign and implement data solutions tailored to meet customer needs and specific use cases.\n\nProvide thought leadership by recommending the most suitable technologies and solutions for various use cases, spanning from the application layer to infrastructure.\n\nBasic Qualifications:\n\n8+ years of hands-on experience with various database technologies\n\n6+ years of experience with Cloud-based systems and Enterprise Data Architecture, driving end-to end technology solutions.\n\nExperience with Azure, Databricks, Snowflake\n\nKnowledgeable on concepts of GenAI\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nPossess certifications in AWS, Azure, and GCP to complement extensive hands-on experience.\n\nDemonstrated expertise with certifications in Snowflake.\n\nValuable ""Big 4"" Management Consulting experience or exposure to multiple industries.\n\nUndergraduate or graduate degree preferred.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['enterprise information architecture', 'microsoft azure', 'gcp', 'database creation', 'aws', 'snowflake', 'data life cycle management', 'metadata', 'data management', 'data security', 'data warehousing', 'data architecture', 'sql', 'data bricks', 'data quality', 'database implementation']",2025-06-12 14:22:31
Data Architect,Coforge,11 - 16 years,Not Disclosed,"['Noida', 'Greater Noida', 'Delhi / NCR']","-Data Architect Department:\nData & Analytics The Data Architect having more than 14 years of experience and should play a pivotal role in designing, developing, and governing scalable data architectures to support enterprise-wide data integration, analytics, and reporting.\nThis role will focus on creating unified data models, optimizing data pipelines, and ensuring compliance with regulatory standards (GDPR) using cloud-based platforms.\nThe ideal candidate is a strategic thinker with deep expertise in data modeling, cloud data platforms, and governance.",,,,"['Data Migration', 'Data Warehousing', 'Data Modeling', 'Informatica', 'SSIS', 'ETL Tool']",2025-06-12 14:22:33
AWS Data Architect (Standard),Infogain,12 - 14 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nDesign and implement scalable, reliable, and high-performance data architectures to support business\nneeds.\nDevelop and maintain real-time data streaming solutions using Kafka and other streaming\ntechnologies.\nUtilize AWS cloud services to build and manage data infrastructure, ensuring security, performance,\nand cost optimization.\nCreate efficient and optimized data models for structured and unstructured datasets.\nDevelop, optimize, and maintain SQL queries for data processing, analysis, and reporting.\nWork with cross-functional teams to define data requirements and implement solutions that align with\nbusiness goals.\nImplement ETL/ELT pipelines using Python and other relevant tools.\nEnsure data quality, consistency, and governance across the organization.\nTroubleshoot and resolve issues related to data pipelines and infrastructure.\nRequired Skills and Qualifications:\nExperience in Data Engineering and Architecture.\nProficiency in Python for data processing and automation.\nStrong expertise in AWS (S3, Redshift, Glue, Lambda, EMR, etc.) for cloud-based data solutions.\nHands-on experience with Kafka for real-time data streaming.\nDeep understanding of data modeling principles for transactional and analytical workloads.\nStrong knowledge of SQL for querying and performance optimization.\nExperience in building and maintaining ETL/ELT pipelines.\nFamiliarity with big data technologies like Spark, Hadoop, or Snowflake is a plus.\nStrong problem-solving skills and ability to work in a fast-paced environment.\nExcellent communication and stakeholder management skills\nEXPERIENCE\n12-14 Years\nSKILLS\nPrimary Skill: Data Engineering\nSub Skill(s): Data Engineering\nAdditional Skill(s): Kafka, Python, Data Modeling, ETL, Data Architecture, SQL, Redshift, Pyspark",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data modeling', 'Analytical', 'Data processing', 'Data quality', 'Stakeholder management', 'AWS', 'SQL', 'Python', 'Data architecture']",2025-06-12 14:22:36
Data Architect,.,7 - 12 years,20-35 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Job Description\nWe are seeking a highly skilled Azure Data Engineer with strong expertise in Data Architecture, PySpark/Python, Azure Databricks, and data streaming solutions. The ideal candidate will have hands-on experience in designing and implementing large-scale data pipelines, along with solid knowledge of data governance and data modeling.\nKey Responsibilities\nDesign, develop, and optimize PySpark/Python-based data streaming jobs on Azure Databricks.\nBuild scalable and efficient data pipelines for batch and real-time processing.\nImplement data governance policies, ensuring data quality, security, and compliance.\nDevelop and maintain data models (dimensional, relational, NoSQL) to support analytics and reporting.\nCollaborate with cross-functional teams (data scientists, analysts, and business stakeholders) to deliver data solutions.\nTroubleshoot performance bottlenecks and optimize Spark jobs for efficiency.\nEnsure best practices in CI/CD, automation, and monitoring of data workflows.\nMentor junior engineers and lead technical discussions (for senior/managerial roles).\nMandatory Skills & Experience\n5+ years of relevant experience as a Data Engineer/Analyst/Architect (8+ years for Manager/Lead positions).\nExpert-level proficiency in PySpark/Python and Azure Databricks (must have worked on real production projects).\nStrong experience in building and optimizing streaming data pipelines (Kafka, Event Hubs, Delta Lake, etc.).\n4+ years of hands-on experience in data governance & data modeling (ER, star schema, data vault, etc.).\nIn-depth knowledge of Azure Data Factory, Synapse, ADLS, and SQL/NoSQL databases.\nExperience with Delta Lake, Databricks Workflows, and performance tuning.\nFamiliarity with data security, metadata management, and lineage tracking.\nExcellent communication skills (must be able to articulate technical concepts clearly).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure Databricks', 'Data Modeling', 'Data Governance', 'Python', 'ETL']",2025-06-12 14:22:38
GCP Data Architect (Standard),Infogain,12 - 14 years,Not Disclosed,['Bengaluru'],"Position Summary Experienced Senior Data Engineer utilizing Big Data & Gogle Cloud technologies to develop large scale, on-cloud data processing pipelines and data warehouses. What you ll do Consult customers across the world on their data engineering needs around Adobes Customer Data Platform. Support pre-sales discsusions around complex and large scale cloud, data engineering solutions. Design custom solutions on cloud integrating Adobes solutions in scalable and performant manner. Deliver complex, large scale, enterprise grade on-clould data engineer and integration solutions in hand-on manner. Good to have Experience of consulting India customers. Multi-cloud expertise preferable AWS and GCP\nEXPERIENCE\n12-14 Years\nSKILLS\nPrimary Skill: Data Engineering\nSub Skill(s): Data Engineering\nAdditional Skill(s): Python, BigQuery",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SUB', 'GCP', 'Data Architect', 'Consulting', 'Cloud', 'Presales', 'Data processing', 'Adobe', 'big data', 'Python']",2025-06-12 14:22:40
Data Architect - AWS,Happiest Minds Technologies,10 - 15 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Roles and responsibilities\nWork closely with the Product Owners and stake holders to design the Technical Architecture for data platform to meet the requirements of the proposed solution.\nWork with the leadership to set the standards for software engineering practices within the machine learning engineering team and support across other disciplines\nPlay an active role in leading team meetings and workshops with clients.\nChoose and use the right analytical libraries, programming languages, and frameworks for each task.",,,,"['SQL', 'data architect', 'Python', 'Pyspark', 'Apache Airflow', 'GLUE', 'Kinesis', 'Amazon Redshift', 'Data Architecture Principles', 'Data Modeling', 'Data Warehousing', 'Athena', 'Lambda', 'AWS']",2025-06-12 14:22:43
Data Architect,Calibo,12 - 16 years,Not Disclosed,[],"About the Role:\n\nWe are looking for a highly skilled Data Engineering Architect with strong Data Engineering pipeline implementation experience to serve as the lead Solution/Technical Architect and Subject Matter Expert for customer experience data solutions across multiple data sources. The ideal candidate will collaborate with the Enterprise Architect and the client IT team to establish and implement strategic initiatives.\n\nResponsibilities and Technical Skills:\n12+ years of relevant experience in designing and Architecting ETL, ELT, Reverse ETL, Data Management or Data Integration, Data Warehouse, Data Lake, and Data Migration.\nMust have expertise in building complex ETL pipelines and large Data Processing, Data Quality and Data security\nExperience in delivering quality work on time with multiple, competing priorities.\nExcellent troubleshooting and problem-solving skills must be able to consistently identify critical elements, variables and alternatives to develop solutions.\nExperience in identifying, analyzing and translating business requirements into conceptual, logical and physical data models in complex, multi-application environments.\nExperience with Agile and Scaled Agile Frameworks.\nExperience in identifying and documenting data integration issues, and challenges such as duplicate data, non-conformed data, and unclean data. Multiple platform development experience.\nStrong experience in performance tuning of ETL processes using Data Platforms\nMust have experience in handling Data formats like Delta Tables, Parquet files, Iceberg etc.\nExperience in Cloud technologies such as AWS/Azure or Google Cloud.\nApache Spark design and development experience using Scala, Java, Python or Data Frames with Resilient Distributed Datasets (RDDs).\nDevelopment experience in databases like Oracle, AWS Redshift, AWS RDS, Postgres Databricks and/or Snowflake.\nHands-on professional work experience with Python is highly desired.\nExperience in Hadoop ecosystem tools for real-time or batch data ingestion.\nStrong communication and teamwork skills to interface with development team members, business analysts, and project management. Excellent analytical skills.\nIdentification of data sources, internal and external, and defining a plan for data management as per business data strategy.\nCollaborating with cross-functional teams for the smooth functioning of the enterprise data system.\nManaging end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nPlanning and execution of big data solutions using Databricks, Big Data, Hadoop, Big Query, Snowflake, MongoDB, DynamoDB, PostgreSQL and SQL Server\nHands-on experience in defining and implementing various Machine Learning models for different business needs.\nIntegrating technical functionality, ensuring data accessibility, accuracy, and security.\nProgramming / Scripting Languages like Python / Java / Go, Microservices\nMachine Learning / AI tools like Scikit-learn / TensorFlow / PyTorch",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud', 'ETL', 'AWS', 'Data Handling', 'Spark']",2025-06-12 14:22:45
Data Architect (Data Bricks),Diacto Technologies Pvt Ltd,5 - 9 years,Not Disclosed,['Pune( Baner )'],"Job Overview:\nDiacto is seeking an experienced and highly skilled Data Architect to lead the design and development of scalable and efficient data solutions. The ideal candidate will have strong expertise in Azure Databricks, Snowflake (with DBT, GitHub, Airflow), and Google BigQuery. This is a full-time, on-site role based out of our Baner, Pune office.\n\nQualifications:\nB.E./B.Tech in Computer Science, IT, or related discipline\nMCS/MCA or equivalent preferred\n\nKey Responsibilities:\nDesign, build, and optimize robust data architecture frameworks for large-scale enterprise solutions\nArchitect and manage cloud-based data platforms using Azure Databricks, Snowflake, and BigQuery\nDefine and implement best practices for data modeling, integration, governance, and security\nCollaborate with engineering and analytics teams to ensure data solutions meet business needs\nLead development using tools such as DBT, Airflow, and GitHub for orchestration and version control\nTroubleshoot data issues and ensure system performance, reliability, and scalability\nGuide and mentor junior data engineers and developers\n\nExperience and Skills Required:\n5 to12 years of experience in data architecture, engineering, or analytics roles\nHands-on expertise in Databricks, especially Azure Databricks\nProficient in Snowflake, with working knowledge of DBT, Airflow, and GitHub\nExperience with Google BigQuery and cloud-native data processing workflows\nStrong knowledge of modern data architecture, data lakes, warehousing, and ETL pipelines\nExcellent problem-solving, communication, and analytical skills\n\nNice to Have:\nCertifications in Azure, Snowflake, or GCP\nExperience with containerization (Docker/Kubernetes)\nExposure to real-time data streaming and event-driven architecture\n\nWhy Join Diacto Technologies?\nCollaborate with experienced data professionals and work on high-impact projects\nExposure to a variety of industries and enterprise data ecosystems\nCompetitive compensation, learning opportunities, and an innovation-driven culture\nWork from our collaborative office space in Baner, Pune\nHow to Apply:\nOption 1 (Preferred)\n\nCopy and paste the following link on your browser and submit your application for the automated interview process: -\n\nhttps://app.candidhr.ai/app/candidate/gAAAAABoRrTQoMsfqaoNwTxsE_qwWYcpcRyYJk7NzSUmO3LKb6rM-8FcU58CUPYQKc65n66feHor-TGdCEfyouj0NmKdgYcNbA==/\n\nOption 2\n\n1. Please visit our website's career section at https://www.diacto.com/careers/\n2. Scroll down to the ""Who are we looking for?"" section\n3. Find the listing for "" Data Architect (Data Bricks)"" and\n4. Proceed with the virtual interview by clicking on ""Apply Now.""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Azure Databricks', 'Airflow', 'Etl Pipelines', 'Github', 'google BigQuery', 'DBT', 'Data Security', 'Data Modeling', 'Elt', 'Data Governance']",2025-06-12 14:22:48
Data Architect,Salesforce,10 - 15 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","The Data Excellence Data Architect is a demonstrated expert in technical and/or functional aspects of customer and partner engagements that lead to the successful delivery of data management projects. The Data Architect plays a critical role for setting customers up for success by prescriptively helping to shape and then execute in the Salesforce data space. This role also provides subject matter expertise related to the data management solutions and ensures successful project delivery. This includes helping identify and proactively manage risk areas, and ensuring issues are seen through to complete resolution as it relates to implementations. Will have the ability to configure and drive solutions to meet the customer s business and technical requirements. Additionally, this role will include helping align on the development of client-specific implementation proposals, SOWs, and staffing plans, engaging with SMEs across the organization to gain consensus on an acceptable proposal, developing best practices within the data excellence community, developing of shared assets.\n  Responsibilities\nServe as the Subject Matter Expert for Salesforce data excellence practice\nRecognized as a valuable and trusted advisor by our customers and other members of Salesforce community and continue to build a reputation for excellence in professional services\nLead development of multi-year Data platform capabilities roadmaps for internal business units like Marketing, Sales, Services, and Finance.\nFacilitate enterprise information & data strategy development, opportunity identification, business cases, technology adoption opportunities, operating model development, and innovation opportunities.\nMaximize value derived from data & analytics by leveraging data assets through data exploitation, envisioning data-enabled strategies as we'll as enabling business outcomes through analytics, data & analytics governance, and enterprise information policy.\nTranslating business requirements into technical specifications, including data streams, integrations, transformations, databases, and data warehouses\nDefining the data architecture framework, standards and principles, including modeling, metadata, security, reference data such as product codes and client categories, and master data such as clients, vendors, materials, and employees\nDefining data flows, ie, which parts of the organization generate data, which require data to function, how data flows are managed, and how data changes in transition\nDesign and implement effective data solutions and models to store and retrieve data from different data sources\nPrepare accurate dataset, architecture, and identity mapping design for execution and management purposes.\nExamine and identify data structural necessities by evaluating client operations, applications, and programming.\nResearch and properly evaluate new sources of information and new technologies to determine possible solutions and limitations in reliability or usability\nAssess data implementation procedures to ensure they comply with internal and external regulations.\nLead or participate in the architecture governance, compliance, and security activities (architectural reviews, technology sourcing) to ensure technology solutions are consistent with the target state architecture.\nPartner with stakeholders early in the project lifecycle to identify business, information, technical, and security architecture issues and act as a strategic consultant throughout the technology lifecycle.\nOversee the migration of data from legacy systems to new solutions.\nPreferred Qualifications and Skills:\nBA/BS degree or foreign equivalent\nOverall 10+ years of experience in Marketing data & Data management space.\nMinimum 1 year of hands-on full lifecycle CDP implementation experience on platforms like Salesforce CDP(formerly 360 Audiences), Tealium AudienceStream, Adobe AEP, Segment, Arm Treasure Data, BlueShift, SessionM, RedPoint, etc\n5+ years of experience with data management, data transformation, ETL, preferably using cloud-based tools/infrastructure\nExperience with Data architecture (ideally with marketing data) using batch and/or real-time ingestion\nRelevant Salesforce experience in Sales & Service Cloud as we'll as Marketing Cloud, related certifications is a plus (Marketing Cloud Consultant, Administrator, Advanced Administrator, Service Cloud Consultant, Sales Cloud Consultant, etc)\nExperience with Technologies and Processes for Marketing, Personalization, and Data Orchestration.\nExperience with master data management (MDM), data governance, data security, data quality and related tools desired.\nDemonstrate deep data integration and/or migration experience with Salesforce.com and other cloud-enabled tools\nDemonstrate expertise in complex SQL statements and RDBMS systems such as Oracle, Microsoft SQL Server, PostGres\nDemonstrate experience with complex coding through ETL tools such as Informatica, SSIS, Pentaho, and Talend\nKnowledge of Data Governance and Data Privacy concepts and regulations a plus\nRequired Skills\nAbility to work independently and be a self-starter\nComfort and ability to learn new technologies quickly & thoroughly\nSpecializes in gathering and analyzing information related to data integration, subscriber management, and identify resolution\nExcellent analytical & problem-solving skills\nDemonstrated ability to influence a group audience, facilitate solutions and lead discussions such as implementation methodology, Road-mapping, Enterprise Transformation strategy, and executive-level requirement gathering sessions\nTravel to client site (up to 50%)",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['metadata', 'RDBMS', 'Coding', 'Data quality', 'Informatica', 'Oracle', 'SSIS', 'Adobe', 'Pentaho', 'Salesforce']",2025-06-12 14:22:51
Data Architect Telecom Domain databrick BSS OSS,fast growing Data Driven IT solutions an...,10 - 20 years,45-55 Lacs P.A.,"['Noida', 'Hyderabad', 'Gurugram']","Data Architect Telecom Domain\nTo design comprehensive data architecture and technical solutions specifically for telecommunications industry challenges, leveraging TMforum frameworks and modern data platforms. To work closely with customers, and technology partners to deliver data solutions that address complex telecommunications business requirements including customer experience management, network optimization, revenue assurance, and digital transformation initiatives.\nResponsibilities:\nDesign and articulate enterprise-scale telecom data architectures incorporating TMforum standards and frameworks, including SID (Shared Information/Data Model), TAM (Telecom Application Map), and eTOM (enhanced Telecom Operations Map)\nDevelop comprehensive data models aligned with TMforum guidelines for telecommunications domains such as Customer, Product, Service, Resource, and Partner management\nCreate data architectures that support telecom-specific use cases including customer journey analytics, network performance optimization, fraud detection, and revenue assurance\nDesign solutions leveraging Microsoft Azure and Databricks for telecom data processing and analytics\nConduct technical discovery sessions with telecom clients to understand their OSS/BSS architecture, network analytics needs, customer experience requirements, and digital transformation objectives\nDesign and deliver proof of concepts (POCs) and technical demonstrations showcasing modern data platforms solving real-world telecommunications challenges\nCreate comprehensive architectural diagrams and implementation roadmaps for telecom data ecosystems spanning cloud, on-premises, and hybrid environments\nEvaluate and recommend appropriate big data technologies, cloud platforms, and processing frameworks based on telecom-specific requirements and regulatory compliance needs.\nDesign data governance frameworks compliant with telecom industry standards and regulatory requirements (GDPR, data localization, etc.)\nStay current with the latest advancements in data technologies including cloud services, data processing frameworks, and AI/ML capabilities\nContribute to the development of best practices, reference architectures, and reusable solution components for accelerating proposal development\nQualifications:\nBachelor's or Master's degree in Computer Science, Telecommunications Engineering, Data Science, or a related technical field\n10+ years of experience in data architecture, data engineering, or solution architecture roles with at least 5 years in telecommunications industry\nDeep knowledge of TMforum frameworks including SID (Shared Information/Data Model), eTOM, TAM, and their practical implementation in telecom data architectures\nDemonstrated ability to estimate project efforts, resource requirements, and implementation timelines for complex telecom data initiatives\nHands-on experience building data models and platforms aligned with TMforum standards and telecommunications business processes\nStrong understanding of telecom OSS/BSS systems, network management, customer experience management, and revenue management domains\nHands-on experience with data platforms including Databricks, and Microsoft Azure in telecommunications contexts\nExperience with modern data processing frameworks such as Apache Kafka, Spark and Airflow for real-time telecom data streaming\nProficiency in Azure cloud platform and its respective data services with an understanding of telecom-specific deployment requirements\nKnowledge of system monitoring and observability tools for telecommunications data infrastructure\nExperience implementing automated testing frameworks for telecom data platforms and pipelines\nFamiliarity with telecom data integration patterns, ETL/ELT processes, and data governance practices specific to telecommunications\nExperience designing and implementing data lakes, data warehouses, and machine learning pipelines for telecom use cases\nProficiency in programming languages commonly used in data processing (Python, Scala, SQL) with telecom domain applications\nUnderstanding of telecommunications regulatory requirements and data privacy compliance (GDPR, local data protection laws)\nExcellent communication and presentation skills with ability to explain complex technical concepts to telecom stakeholders\nStrong problem-solving skills and ability to think creatively to address telecommunications industry challenges\nGood to have TMforum certifications or telecommunications industry certifications\nRelevant data platform certifications such as Databricks, Azure Data Engineer are a plus\nWillingness to travel as required\nif you will all or most of the criteria contact bdm@intellisearchonline.net M 9341626895",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Telecom Bss', 'Data Architect', 'Telecom OSS', 'ETOM', 'Data Bricks']",2025-06-12 14:22:53
Data Architect - Supply Chain,Exxon Mobil Corporation,3 - 8 years,Not Disclosed,['Bengaluru'],"About us\nWe invite you to bring your ideas to ExxonMobil to help create sustainable solutions that improve quality of life and meet society s evolving needs. Learn more about our What and our Why and how we can work together .\nExxonMobil s affiliates in India\nExxonMobil s affiliates have offices in India in Bengaluru, Mumbai and the National Capital Region.\nExxonMobil s affiliates in India supporting the Product Solutions business engage in the marketing, sales and distribution of performance as well as specialty products across chemicals and lubricants businesses. The India planning teams are also embedded with global business units for business planning and analytics.",,,,"['ERP', 'SAP', 'Networking', 'Data management', 'Data modeling', 'XML', 'Agile', 'JSON', 'SQL', 'Python']",2025-06-12 14:22:55
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Bengaluru'],"Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 14:22:57
"Data Analyst, Staff",Qualcomm,4 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Miscellaneous Group, Miscellaneous Group > Data Analyst\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nAbout the Team\n\nQualcomm's People Analytics team plays a crucial role in transforming data into strategic workforce insights that drive HR and business decisions. As part of this lean but high-impact team, you will have the opportunity to analyze workforce trends, ensure data accuracy, and collaborate with key stakeholders to enhance our data ecosystem. This role is ideal for a generalist who thrives in a fast-paced, evolving environment""â€someone who can independently conduct data analyses, communicate insights effectively, and work cross-functionally to enhance our People Analytics infrastructure.\n\nWhy Join Us\n\n\nEnd-to-End ImpactWork on the full analytics cycle""â€from data extraction to insight generation""â€driving meaningful HR and business decisions.\n\n\nCollaboration at ScalePartner with HR leaders, IT, and other analysts to ensure seamless data integration and analytics excellence.\n\n\nData-Driven CultureBe a key player in refining our data lake, ensuring data integrity, and influencing data governance efforts.\n\n\nProfessional GrowthGain exposure to multiple areas of people analytics, including analytics, storytelling, and stakeholder engagement.\n\n\nKey Responsibilities\n\n\nPeople Analytics & Insights\nAnalyze HR and workforce data to identify trends, generate insights, and provide recommendations to business and HR leaders.\nDevelop thoughtful insights to support ongoing HR and business decision-making.\nPresent findings in a clear and compelling way to stakeholders at various levels, including senior leadership.\n\n\nData Quality & Governance\nEnsure accuracy, consistency, and completeness of data when pulling from the data lake and other sources.\nIdentify and troubleshoot data inconsistencies, collaborating with IT and other teams to resolve issues.\nDocument and maintain data definitions, sources, and reporting standards to drive consistency across analytics initiatives.\n\n\nCollaboration & Stakeholder Management\nWork closely with other analysts on the team to align methodologies, share best practices, and enhance analytical capabilities.\nAct as a bridge between People Analytics, HR, and IT teams to define and communicate data requirements.\nPartner with IT and data engineering teams to improve data infrastructure and expand available datasets.\n\n\nQualifications\n\nRequired4-7 years experience in a People Analytics focused role\n\n\nAnalytical & Technical Skills\nStrong ability to analyze, interpret, and visualize HR and workforce data to drive insights.\nExperience working with large datasets and ensuring data integrity.\nProficiency in Excel and at least one data visualization tool (e.g., Tableau, Power BI).\n\n\nCommunication & Stakeholder Management\nAbility to communicate data insights effectively to both technical and non-technical audiences.\nStrong documentation skills to define and communicate data requirements clearly.\nExperience collaborating with cross-functional teams, including HR, IT, and business stakeholders.\n\n\nPreferred:\n\n\nTechnical Proficiency\nExperience with SQL, Python, or R for data manipulation and analysis.\nFamiliarity with HR systems (e.g., Workday) and cloud-based data platforms.\n\n\nPeople Analytics Expertise\nPrior experience in HR analytics, workforce planning, or related fields.\nUnderstanding of key HR metrics and workforce trends (e.g., turnover, engagement, diversity analytics).\n\n\nAdditional Information\nThis is an office-based position (4 days a week onsite) with possible locations that may include India and Mexico",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'people analytics', 'documentation', 'tableau', 'data integration tools', 'hiring', 'data warehousing', 'data architecture', 'sourcing', 'jquery', 'staffing', 'plsql', 'oracle 10g', 'java', 'etl tool', 'html', 'etl', 'mongodb', 'python', 'oracle', 'power bi', 'hrsd', 'r', 'node.js', 'hr analytics', 'angularjs']",2025-06-12 14:23:00
Data & Analytics Specialist,Hoffmann La Roche,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\n.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\nA healthier future drives us to innovate. Together, more than 100 000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. Our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our Diagnostics products. We empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-12 14:23:02
Data & Gen AI Specialist,Altimetrik,1 - 4 years,Not Disclosed,['Bengaluru'],"Job Title: Data & GenAI AWS Specialist\nExperience: 1-4 Years\nLocation: Bangalore\nMandatory Qualification: B.E./ B.Tech/ M.Tech/ MS from IIT or IISc ONLY\nJob Overview:\nWe are seeking a seasoned Data & GenAI Specialist with deep expertise in AWS Managed Services (PaaS) to join our innovative team. The ideal candidate will have extensive experience in designing sophisticated, scalable architectures for data pipelines and Generative AI (GenAI) solutions leveraging cloud services.",,,,"['Generative Ai', 'Cloud', 'Data Science', 'Open Source', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Snowflake', 'Machine Learning', 'AWS']",2025-06-12 14:23:04
Data & Analytics Specialist,Roche Diagnostics,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\nAt Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we ve become one of the world s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\n.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-12 14:23:07
Lead Data Management Analyst,Wells Fargo,5 - 10 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Lead Data Management Analyst. We believe in the power of working together because great ideas can come from anyone. Through collaboration, any employee can have an impact and make a difference for the entire company. Explore opportunities with us for a career in a supportive environment where you can learn and grow. This role requires a blend of technical expertise, analytical thinking, and strategic decision making to drive impactful insights.\nAt Wells Fargo, we are looking for talented people who will put our customers at the center of everything we do. We are seeking candidates who embrace diversity, equity and inclusion in a workplace where everyone feels valued and inspired. Help us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.",,,,"['Data Management', 'Hive', 'Power BI', 'DB2', 'SQL Server', 'Tableau', 'Oracle', 'Teradata', 'Analytics', 'Python', 'Business Analysis']",2025-06-12 14:23:09
Data migration Manager,Robert Bosch Engineering and Business Solutions Private Limited,5 - 9 years,Not Disclosed,['Hyderabad'],"10+ years of experience in SAP Data Migration with Minimum Migration Management experience of 2 migration S/4 HANA rollout projects\nGood experience in migration management including planning, tracking, reporting, technical migration consulting, migration cutover planning etc.\nExperience of migration with SAP BODS (past experience of at least 3 projects using SAP BODS)\nGood SAP process knowledge required for data migration to work with cross-functional teams in gathering migration requirements and migration rules\nDeep knowledge in understanding the business objects requirements, dependencies, and technical knowledge (focus on migration cockpit and BODS) to coordinate and support the technical migration team during the technical design and release\nShould be proficient in Business Analysis, Business Knowledge Technical Solution Design\nPrior customer-facing roles to ensure client management is mandatory\nExhibit effective communication, presentation, and people skills along with demonstrated experience working with cross-functional teams, including teams working on interfaces.\nShould be a good collaborator, leader to drive the migration team (servant leadership mindset)\nAdditional migration tool knowledge is an added advantage\nRoles and Responsibilities\nComplete coordination of migration project starting from preparation till go-live and stabilization\nDrive the Migration concept and understand the high-level process of data migration, objects and dependencies and owning Migration cutover planning\nProcess overview desirable\nProvide technical migration solutions and guidance to technical team where ever necessary\nClose engagement with IT experts / Functional consultants, Process experts in gathering migration requirement\nClose collaboration, support and value addition to Central Migration Manager as applicable\nWorking closely between the technical migration team and cross functional team during development phase, technical unit testing\nPreparation of migration planning and cutover during each phase\nPlanning, tracking reporting of status to project management periodically.\nComplete responsibility and ownership of technical deliverables\nSingle point of contact for technical data migration within the project",Industry Type: Automobile,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution design', 'Business objects', 'Data migration', 'SAP', 'Business analysis', 'Technical design', 'Project management', 'Consulting', 'Unit testing', 'Client management']",2025-06-12 14:23:12
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nAs part of the cybersecurity organization, In this vital role you will be responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The role sits at the intersection of data infrastructure and business insight delivery, requiring the Data Engineer to design and build robust data pipelines while also translating data into meaningful visualizations for stakeholders across the organization. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nBuild data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nDevelop and maintain interactive dashboards and reports using tools like Tableau, ensuring data accuracy and usability\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\n\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, GitLab, LucidChart, etc.\nHands-on experience with data visualization and dashboarding toolsTableau, Power BI, or similar is a plus\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\n\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\n\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\n\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data engineering', 'data analysis', 'data modeling', 'analysis tools', 'data warehousing', 'troubleshooting', 'data architecture', 'data integration', 'etl process']",2025-06-12 14:23:14
IN_Manager_Azure Data Engineer_Data Analytics_Advisory,PwC Service Delivery Center,5 - 10 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nMust have\nCandidates with minimum 5 years of relevant experience for 1012 years of total experience (Architect / Managerial level).\nDeep expertise with technologies such as Data factory, Data Bricks (Advanced), SQLDB (writing complex Stored Procedures), Synapse, Python scripting (mandatory), Pyspark scripting, Azure Analysis Services.\nMust be certified with DP 203 (Azure Data Engineer Associate), Databricks Certified Data Engineer Professional (Architect / Managerial level)\nStrong troubleshooting and debugging skills. Proven experience in working source control technologies (such as GITHUB, Azure DevOps), build and release pipelines.\nExperience in writing complex PySpark queries to perform data analysis.\nMandatory skill sets\nAzure Databricks, Pyspark, Datafactory\nPreferred skill sets\nAzure Databricks, Pyspark, Datafactory, Python, Azure Devops\nYears of experience required\n712yrs\nEducation qualification\nB.Tech / M.Tech / MBA / MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nMicrosoft Azure\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling {+ 32 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Data modeling', 'Debugging', 'Database administration', 'Agile', 'Stored procedures', 'Apache', 'Business intelligence', 'Troubleshooting', 'Python']",2025-06-12 14:23:17
Data Engineer III,Expedia Group,5 - 10 years,Not Disclosed,['Bengaluru'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nData Engineer III\nIntroduction to the Team\nExpedia Technology teams partner with our Product teams to create innovative products, services, and tools to deliver high-quality experiences for travelers, partners, and our employees. A singular technology platform powered by data and machine learning provides secure, differentiated, and personalized experiences that drive loyalty and traveler satisfaction.\nExpedia Group is seeking a skilled and motivated Data Engineer III to join our Finance Business Intelligence team supporting the Product & Technology Finance organization. In this role, you will help drive data infrastructure and analytics solutions that support strategic financial planning, reporting, and operational decision-making across the Global Finance community. You ll work closely with Finance and Technology partners to ensure data accuracy, accessibility, and usability in support of Expedia s business objectives.\nAs a Data Engineer III, you have strong experience working with a variety of datasets, data environments, tools, and analytical techniques. You enjoy a fun, collaborative and stimulating team environment. Successful candidates should be able to own projects end-to-end, including identifying problems and solutions, building and maintain data pipelines and dashboards, distilling key insights and communicate to stakeholders.\nIn this role, you will:\nDevelop new and improve existing end to end Business Intelligence products (data pipelines, Tableau dashboards, and Machine Learning predictive forecasting models).\nDrive internal efficiencies through streamline code/documentation/Tableau development to maintain high data integrity.\nTroubleshoot and resolve production issues with the team products (automation opportunities, optimizations, back-end data issues, data reconciliations).\nProactively reach out to subject matter experts /stakeholders and collaborate to solve problems.\nRespond to ad hoc data requests and conduct analysis to provide valuable insights to stakeholders.\nCollaborate and coordinate with team members/stakeholders to translate complex data into meaningful insights, that improve the analytical capabilities of the business.\nApply knowledge of database design to support migration of data pipelines from on prem to cloud environment (including data extraction, ingestion, processing of large data sets)\nSupport dashboard development on cloud environment to enable self-service reporting.\nCommunicate clearly on current work status and design considerations\nThink broadly and comprehend the how, why, and what behind data architecture designs\nExperience & Qualifications:\nBachelor s in Computer Science, Mathematics, Statistics, Information Systems, or related field\n5+ years experience in a Data Analyst, Data Engineer or Business Analyst role\nProven expertise in SQL, with practical experience utilizing query engines including SQL Server, Starburst, Trino, Querybook and data science tools such as Python/R, SparkSQL.\nProficient visualization skills (Tableau, Looker, or similar) and excel modeling/report automation.\nExceptional understanding of relational and dimensional datasets, data warehouse and data mining and applies database design principles to solve data requirements\nExperience building robust data extract, load and transform (ELT) processes, that source data from multiple databases.\nDemonstrated record of defining and executing key analysis and solving problems with minimal supervision.\nDynamic individual contributor who consistently enhances operational playbooks to address business problems.\n3+ year working in a hybrid environment that uses both on-premise and cloud technologies is preferred.\nExperience working in an environment that manipulates large datasets on the cloud platform preferred.\nBackground in analytics, finance or a comparable reporting and analytics role preferred.\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Database design', 'Machine learning', 'Business intelligence', 'Data mining', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-12 14:23:19
Data Engineer - Databricks,KPI Partners,3 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About KPI Partners.\nKPI Partners is a leading provider of data analytics solutions, dedicated to helping organizations transform data into actionable insights. Our innovative approach combines advanced technology with expert consulting, allowing businesses to leverage their data for improved performance and decision-making.\n\nJob Description.\nWe are seeking a skilled and motivated Data Engineer with experience in Databricks to join our dynamic team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and data processing solutions that support our analytics initiatives. You will collaborate closely with data scientists, analysts, and other engineers to ensure the consistent flow of high-quality data across our platforms.",,,,"['python', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'data pipeline', 'data architecture', 'data engineering', 'sql', 'data bricks', 'cloud', 'analytics', 'data quality', 'data modeling', 'gcp', 'teamwork', 'integration', 'aws', 'etl', 'programming', 'communication skills', 'etl scripts']",2025-06-12 14:23:22
Big Data Engineer - Hadoop,Info Origin Technologies Pvt Ltd,3 - 7 years,Not Disclosed,"['Hyderabad', 'Gurugram']","Role: Hadoop Data Engineer\nLocation: Gurgaon / Hyderabad\nWork Mode: Hybrid\nEmployment Type: Full-Time\nInterview Mode: First Video then In Person\nJob Description\nJob Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'Hadoop', 'Pyspark', 'Big Data', 'Python', 'SQL']",2025-06-12 14:23:24
Hadoop Data Engineer,Info Origin,0 - 2 years,Not Disclosed,['Gurugram'],"Job Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'Scalability', 'data security', 'spark', 'Hadoop', 'Data processing', 'big data', 'SQL', 'Python']",2025-06-12 14:23:26
Data Engineer,Centrilogic,15 - 20 years,Not Disclosed,['Hyderabad'],"Data Engineer\n\nPurpose:\n\nOver 15 years, we have become a premier global provider of multi-cloud management, cloud-native application development solutions, and strategic end-to-end digital transformation services.\nHeadquartered in Canada and with regional headquarters in the U.S. and the United Kingdom, Centrilogic delivers smart, streamlined solutions to clients worldwide.\n\nWe are looking for a passionate and experienced Data Engineer to work with our other 70 Software, Data and DevOps engineers to guide and assist our clients data modernization journey.\n\nOur team works with companies with ambitious missions - clients who are creating new, innovative products, often in uncharted markets. We work as embedded members and leaders of our clients development and data teams. We bring experienced senior engineers, leading-edge technologies and mindsets, and creative thinking. We show our clients how to move to the modern frameworks of data infrastructures and processing, and we help them reach their full potential with the power of data.\n\nIn this role, youll be the day-to-day primary point of contact with our clients to modernize their data infrastructures, architecture, and pipelines.\n\nPrincipal Responsibilities:\n\nConsulting clients on cloud-first strategies for core bet-the-company data initiatives\nProviding thought leadership on both process and technical matters\nBecoming a real champion and trusted advisor to our clients on all facets of Data Engineering\nDesigning, developing, deploying, and supporting the modernization and transformation of our client s end-to-end data strategy, including infrastructure, collection, transmission, processing, and analytics\nMentoring and educating clients teams to keep them up to speed with the latest approaches, tools and skills, and setting them up for continued success post-delivery\n\nRequired Experience and Skills:\n\nMust have either Microsoft Certified Azure Data Engineer Associate or Fabric Data Engineer Associate certification.\nMust have experience working in a consulting or contracting capacity on large data management and modernization programs.\nExperience with SQL Servers, data engineering, on platforms such as Azure Data Factory, Databricks, Data Lake, and Synapse.\nStrong knowledge and demonstrated experience with Delta Lake and Lakehouse Architecture.\nStrong knowledge of securing Azure environment, such as RBAC, Key Vault, and Azure Security Center.\nStrong knowledge of Kafka and Spark and extensive experience using them in a production environment.\nStrong and demonstrable experience as DBA in large-scale MS SQL environments deployed in Azure.\nStrong problem-solving skills, with the ability to get to the route of an issue quickly.\nStrong knowledge of Scala or Python.\nStrong knowledge of Linux administration and networking.\nScripting skills and Infrastructure as Code (IaC) experience using PowerShell, Bash, and ARM templates.\nUnderstanding of security and corporate governance issues related with cloud-first data architecture, as well as accepted industry solutions.\nExperience in enabling continuous delivery for development teams using scripted cloud provisioning and automated tooling.\nExperience working with Agile development methodology that is fit for purpose.\nSound business judgment and demonstrated leadership",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['MS SQL', 'Networking', 'Data management', 'Powershell', 'Consulting', 'Application development', 'microsoft', 'Analytics', 'Python', 'Data architecture']",2025-06-12 14:23:28
Azure Data Engineer,HTC Global Services,4 - 8 years,Not Disclosed,['Bengaluru( Murugeshpalya )'],"Job Summary:\nWe are looking for a highly skilled Azure Data Engineer with experience in building and managing scalable data pipelines using Azure Data Factory, Synapse, and Databricks. The ideal candidate should be proficient in big data tools and Azure services, with strong programming knowledge and a solid understanding of data architecture and cloud platforms.\n\nKey Responsibilities:",,,,"['Power Bi', 'Azure Databricks', 'Azure Data Factory', 'Synapse', 'Python', 'Java', 'Scala', 'Kafka', 'big data tools', 'SQL', 'EventHub', 'Azure cloud services', 'Spark']",2025-06-12 14:23:31
Microsoft Fabrics Data Engineer,Swits Digital,5 - 10 years,Not Disclosed,['Bengaluru'],"Job TItle: Microsoft Fabric Data Engineer\nLocation: Bangalore\nJob Type: Conract (24 Months)\nJob Description:\nWe are seeking a highly skilled and experienced Microsoft Fabric Data Engineer/Architect to design, develop, and maintain robust, scalable, and secure data solutions within the Microsoft Fabric ecosystem. This role will leverage the full suite of Microsoft Azure data services, including Azure Data Bricks, Azure Data Factory, and Azure Data Lake, to build end-to-end data pipelines, data warehouses, and data lakehouses that enable advanced analytics and business intelligence.\nRequired Skills & Qualifications:\nBachelors degree in Computer Science, Engineering, or a related field.\n5+ years of experience in data architecture and engineering, with a strong focus on Microsoft Azure data platforms.\nProven hands-on expertise with Microsoft Fabric and its components, including:\nOneLake\nData Factory (Pipelines, Dataflows Gen2)\nSynapse Analytics (Data Warehousing, SQL analytics endpoint)\nLakehouses and Warehouses\nNotebooks (PySpark)\nExtensive experience with Azure Data Bricks, including Spark development (PySpark, Scala, SQL).\nStrong proficiency in Azure Data Factory for building and orchestrating ETL/ELT pipelines.\nDeep understanding and experience with Azure Data Lake Storage Gen2.\nProficiency in SQL (T-SQL, Spark SQL), Python, and/or other relevant scripting languages.\nSolid understanding of data warehousing concepts, dimensional modeling, and data lakehouse architectures.\nExperience with data governance principles and tools (e.g., Microsoft Purview).\nFamiliarity with CI/CD practices, version control (Git), and DevOps for data pipelines.\nExcellent problem-solving, analytical, and communication skills.\nAbility to work independently and collaboratively in a fast-paced, agile environment.\nPreferred Qualifications:\nMicrosoft certifications in Azure Data Engineering (e.g., DP-203, DP-600: Microsoft Fabric Analytics Engineer Associate).\nExperience with Power BI for data visualization and reporting.\nFamiliarity with real-time analytics and streaming data processing.\nExposure to machine learning workflows and integrating ML models with data solutions",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Analytical', 'microsoft azure', 'data visualization', 'microsoft', 'Business intelligence', 'Data warehousing', 'Analytics', 'Data architecture', 'Python']",2025-06-12 14:23:33
Big Data Engineer,Apexon,11 - 16 years,Not Disclosed,['Bengaluru'],"We enable #HumanFirstDigital\n\nJob Summary:\nWe are looking for a highly experienced and strategic Data Engineer to drive the design, development, and optimization of our enterprise data platform. This role requires deep technical expertise in AWS, StreamSets, and Snowflake, along with solid experience in Kubernetes, Apache Airflow, and unit testing. The ideal candidate will lead a team of data engineers and play a key role in delivering scalable, secure, and high-performance data solutions for both historical and incremental data loads.\nKey Responsibilities:\nLead the architecture, design, and implementation of end-to-end data pipelines using StreamSets and Snowflake.\nOversee the development of scalable ETL/ELT processes for historical data migration and incremental data ingestion.\nGuide the team in leveraging AWS services (S3, Lambda, Glue, IAM, etc.) to build cloud-native data solutions.\nProvide technical leadership in deploying and managing containerized applications using Kubernetes.\nDefine and implement workflow orchestration strategies using Apache Airflow.\nEstablish best practices for unit testing, code quality, and data validation.\nCollaborate with data architects, analysts, and business stakeholders to align data solutions with business goals.\nMentor junior engineers and foster a culture of continuous improvement and innovation.\nMonitor and optimize data workflows for performance, scalability, and cost-efficiency.\nRequired Skills & Qualifications:\nHigh proficiency in AWS, including hands-on experience with core services (S3, Lambda, Glue, IAM, CloudWatch).\nExpert-level experience with StreamSets, including Data Collector, Transformer, and Control Hub.\nStrong Snowflake expertise, including data modeling, SnowSQL, and performance tuning.\nMedium-level experience with Kubernetes, including container orchestration and deployment.\nWorking knowledge of Apache Airflow for workflow scheduling and monitoring.\nExperience with unit testing frameworks and practices in data engineering.\nProven experience in building and managing ETL pipelines for both batch and real-time data.\nStrong command of SQL and scripting languages such as Python or Shell.\nExperience with CI/CD pipelines and version control tools (e.g., Git, Jenkins).\nPreferred Qualifications:\nAWS certification (e.g., AWS Certified Data Analytics, Solutions Architect).\nExperience with data governance, security, and compliance frameworks.\nFamiliarity with Agile methodologies and tools like Jira and Confluence.\nPrior experience in a leadership or mentoring role within a data engineering team.\nOur Commitment to Diversity & Inclusion:\nOur Perks and Benefits:\nOur benefits and rewards program has been thoughtfully designed to recognize your skills and contributions, elevate your learning/upskilling experience and provide care and support for you and your loved ones. As an Apexon Associate, you get continuous skill-based development, opportunities for career advancement, and access to comprehensive health and well-being benefits and assistance.\nWe also offer:\no Group Health Insurance covering family of 4\no Term Insurance and Accident Insurance\no Paid Holidays & Earned Leaves\no Paid Parental LeaveoLearning & Career Development\no Employee Wellness\nJob Location : Bengaluru, India",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Agile', 'Wellness', 'Workflow', 'Healthcare', 'Unit testing', 'Apache', 'SQL', 'Python']",2025-06-12 14:23:35
Senior Data Engineer,Qualcomm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Data Engineer\n\nGeneral Summary:\n\nDeveloper will play an integral role in the PTEIT Machine Learning Data Engineering team. Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics. Design, develop and support CI/CD of data pipelines and services. - 5+ years of experience with Python or equivalent programming using OOPS, Data Structures and Algorithms - Develop new services in AWS using server-less and container-based services. - 3+ years of hands-on experience with AWS Suite of services (EC2, IAM, S3, CDK, Glue, Athena, Lambda, RedShift, Snowflake, RDS) - 3+ years of expertise in scheduling data flows using Apache Airflow - 3+ years of strong data modelling (Functional, Logical and Physical) and data architecture experience in Data Lake and/or Data Warehouse - 3+ years of experience with SQL databases - 3+ years of experience with CI/CD and DevOps using Jenkins - 3+ years of experience with Event driven architecture specially on Change Data Capture - 3+ years of Experience in Apache Spark, SQL, Redshift (or) Big Query (or) Snowflake, Databricks - Deep understanding building the efficient data pipelines with data observability, data quality, schema drift, alerting and monitoring. - Good understanding of the Data Catalogs, Data Governance, Compliance, Security, Data sharing - Experience in building the reusable services across the data processing systems. - Should have the ability to work and contribute beyond defined responsibilities - Excellent communication and inter-personal skills with deep problem-solving skills.\n\nMinimum Qualifications:\n3+ years of IT-related work experience with a Bachelor's degree in Computer Engineering, Computer Science, Information Systems or a related field.\nOR\n5+ years of IT-related work experience without a Bachelors degree.\n\n2+ years of any combination of academic or work experience with programming (e.g., Java, Python).\n1+ year of any combination of academic or work experience with SQL or NoSQL Databases.\n1+ year of any combination of academic or work experience with Data Structures and algorithms.\n5 years of Industry experience and minimum 3 years experience in Data Engineering development with highly reputed organizations- Proficiency in Python and AWS- Excellent problem-solving skills- Deep understanding of data structures and algorithms- Proven experience in building cloud native software preferably with AWS suit of services- Proven experience in design and develop data models using RDBMS (Oracle, MySQL, etc.)\n\nDesirable - Exposure or experience in other cloud platforms (Azure and GCP) - Experience working on internals of large-scale distributed systems and databases such as Hadoop, Spark - Working experience on Data Lakehouse platforms (One House, Databricks Lakehouse) - Working experience on Data Lakehouse File Formats (Delta Lake, Iceberg, Hudi)\n\nBachelor's or Master's degree in Computer Science, Software Engineering, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data quality', 'data structures', 'aws', 'schema', 'continuous integration', 'glue', 'amazon redshift', 'event driven architecture', 'ci/cd', 'data engineering', 'sql', 'alerts', 'java', 'data modeling', 'spark', 'devops', 'data flow', 'nosql databases', 'sql database']",2025-06-12 14:23:37
Data Migration Consultant,Excellerate Global Solutions,6 - 10 years,8-18 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Data migration SAP ABAP , S4 HANA , FSCM ,FICO",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Temporary/Contractual","['Sap Data Migration', 'SAP FICO', 'SAP ABAP', 'FSCM', 'Sap Hana', 'S4 Hana Finance']",2025-06-12 14:23:40
Data Engineer- MS Fabric,InfoCepts,5 - 9 years,Not Disclosed,['India'],"Position: Data Engineer â€“ MS Fabric\n  Purpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\n",,,,"['components', 'data', 'scala', 'delta', 'pyspark', 'data warehousing', 'rules', 'azure data factory', 'sql', 'parquet', 'analytics', 'sql azure', 'spark', 'oracle adf', 'data pipeline architecture', 'etl', 'python', 'azure synapse', 'microsoft azure', 'power bi', 'data bricks', 'data quality', 'system', 't', 'fabric', 'data integration', 'etl process']",2025-06-12 14:23:43
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Pune'],"ZSs Insights & Analytics group partners with clients to design and deliver solutions to help them tackle a broad range of business challenges. Our teams work on multiple projects simultaneously, leveraging advanced data analytics and problem-solving techniques. Our recommendations and solutions are based on rigorous research and analysis underpinned by deep expertise and thought leadership.\nWhat you'll Do\nDevelop advanced and efficient statistically effective algorithms that solve problems of high",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 14:23:45
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,"['Noida', 'Gurugram']","Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 14:23:47
Data Modelling,Tech Mahindra,7 - 12 years,Not Disclosed,['Chennai'],"Role: Data Modelling\nFulltime (Work from office Monday to Friday)\nLocation: Chennai\n\n\nMinimum Qualifications:\nBachelors Degree or experience in Engineering,\n5+ years of experience in data architecture or a related field, with a strong understanding of cloud-based data solutions\nProficiency in designing and implementing Medallion Architecture with bronze, silver, and gold layers\nExperienced curating and Erwin modeling data into Star Schemas\nStrong background in semantic modeling and creating meaningful, user-friendly data sets\nExperience with AWS, Synapse, and Power BI.\nWork visa sponsorship is not available for this position",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Warehouse', 'Data Modeling', 'ERwin', 'Azure Cloud', 'GCP', 'AWS']",2025-06-12 14:23:49
IT Manager - Data Engineering & Analytics,ZS,12 - 15 years,Not Disclosed,['Pune'],"IT MANAGER, DATA ENGINEERING AND ANALYTICS will lead a team of data engineers and analysts responsible for designing, developing, and maintaining robust data systems and integrations. This role is critical for ensuring the smooth collection, transformation, integration and visualization of data, making it easily accessible for analytics and decision-making across the organization. The Manager will collaborate closely with analysts, developers, business leaders and other stakeholders to ensure that the data infrastructure meets business needs and is scalable, reliable, and efficient.\n",,,,"['Data modeling', 'Project management', 'Analytical', 'Financial planning', 'Management consulting', 'Data quality', 'Troubleshooting', 'Stakeholder management', 'Analytics', 'SQL']",2025-06-12 14:23:51
Cloud Data Engineer - GCP,Synechron,2 - 3 years,Not Disclosed,"['Hyderabad', 'Gachibowli']","Job Summary\nSynechron is seeking a highly motivated and skilled Senior Cloud Data Engineer GCP to join our cloud solutions team. In this role, you will collaborate closely with clients and internal stakeholders to design, implement, and manage scalable, secure, and high-performance cloud-based data solutions on Google Cloud Platform (GCP). You will leverage your technical expertise to ensure the integrity, security, and efficiency of cloud data architectures, enabling the organization to derive maximum value from cloud data assets. This role contributes directly to our mission of delivering innovative digital transformation solutions and supports the organizations strategic objectives of scalable and sustainable cloud infrastructure.\nSoftware Requirements\nRequired Skills:\nProficiency with Google Cloud Platform (GCP) services (Compute Engine, Cloud Storage, BigQuery, Cloud Pub/Sub, Dataflow, etc.)\nBasic scripting skills with Python, Bash, or similar languages\nFamiliarity with virtualization and cloud networking concepts\nUnderstanding of cloud security best practices and compliance standards\nExperience with infrastructure as code tools (e.g., Terraform, Deployment Manager)\nStrong knowledge of data management, data pipelines, and ETL processes\nPreferred Skills:\nExperience with other cloud platforms (AWS, Azure)\nKnowledge of SQL and NoSQL databases\nFamiliarity with containerization (Docker, GKE)\nExperience with data visualization tools\nOverall Responsibilities\nDesign, implement, and operate cloud data solutions that are secure, scalable, and optimized for performance\nCollaborate with clients and internal teams to identify infrastructure and data architecture requirements\nManage and monitor cloud infrastructure and ensure operational reliability\nResolve technical issues related to cloud data workflows and storage solutions\nParticipate in project planning, timelines, and technical documentation\nContribute to best practices and continuous improvement initiatives within the organization\nEducate and support clients in adopting cloud data services and best practices\nTechnical Skills (By Category)\nProgramming Languages:\nEssential: Python, Bash scripts\nPreferred: SQL, Java, or other data processing languages\nDatabases & Data Management:\nEssential: BigQuery, Cloud SQL, Cloud Spanner, Cloud Storage\nPreferred: NoSQL databases like Firestore, MongoDB\nCloud Technologies:\nEssential: Google Cloud Platform core services (Compute, Storage, BigQuery, Dataflow, Pub/Sub)\nPreferred: Cloud monitoring, logging, and security tools\nFrameworks & Libraries:\nEssential: Data pipeline frameworks, Cloud SDKs, APIs\nPreferred: Apache Beam, Data Studio\nDevelopment Tools & Methodologies:\nEssential: Infrastructure as Code (Terraform, Deployment Manager)\nPreferred: CI/CD tools (Jenkins, Cloud Build)\nSecurity Protocols:\nEssential: IAM policies, data encryption, network security best practices\nPreferred: Compliance frameworks such as GDPR, HIPAA\nExperience Requirements\n2-3 years of experience in cloud data engineering, cloud infrastructure, or related roles\nHands-on experience with GCP is preferred; experience with AWS or Azure is a plus\nBackground in designing and managing cloud data pipelines, storage, and security solutions\nProven ability to deliver scalable data solutions in cloud environments\nExperience working with cross-functional teams on cloud deployments\nAlternative experience pathways: academic projects, certifications, or relevant internships demonstrating cloud data skills\nDay-to-Day Activities\nDevelop and deploy cloud data pipelines, databases, and analytics solutions\nCollaborate with clients and team members to plan and implement infrastructure architecture\nPerform routine monitoring, maintenance, and performance tuning of cloud data systems\nTroubleshoot technical issues affecting data workflows and resolve performance bottlenecks\nDocument system configurations, processes, and best practices\nEngage in continuous learning on new cloud features and data management tools\nParticipate in project meetings, code reviews, and knowledge sharing sessions\nQualifications\nBachelors or Masters degree in computer science, engineering, information technology, or a related field\nRelevant certifications (e.g., Google Cloud Professional Data Engineer, Cloud Architect) are preferred\nTraining in cloud security, data management, or infrastructure design is advantageous\nCommitment to professional development and staying updated with emerging cloud technologies\nProfessional Competencies\nCritical thinking and problem-solving skills to resolve complex cloud architecture challenges\nAbility to work collaboratively with multidisciplinary teams and clients\nStrong communication skills for technical documentation and stakeholder engagement\nAdaptability to evolving cloud technologies and project priorities\nOrganized with a focus on quality and detail-oriented delivery\nProactive learner with a passion for innovation in cloud data solutions\nAbility to manage multiple tasks effectively and prioritize in a fast-paced environment",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Jenkins', 'Java', 'NoSQL', 'Bash scripts', 'Data Studio', 'Data Management', 'CI/CD', 'Apache Beam', 'MongoDB', 'Cloud Build']",2025-06-12 14:23:53
"Sr. Solution Architect, Startups",Amazon,8 - 13 years,Not Disclosed,['Mumbai'],"Sales, Marketing and Global Services (SMGS)\nAWS Sales, Marketing, and Global Services (SMGS) is responsible for driving revenue, adoption, and growth from the largest and fastest growing smalland mid-market accounts to enterprise-level customers including public sector.\n\nDo you like startups? Are you interested in Cloud Computing & Generative AI? Yes? We have a role you might find interesting.\n\nStartups are the large enterprises of the future. These young companies are founded by ambitious people who have a desire to build something meaningful and to challenge the status quo. To address underserved customers, or to challenge incumbents. They usually operate in an environment of scarcity: whether that s capital, engineering resource, or experience. This is where you come in.\n\nThe Startup Solutions Architecture team is dedicated to working with these early stage startup companies as they build their businesses. We re here to make sure that they can deploy the best, most scalable, and most secure architectures possible and that they spend as little time and money as possible doing so.\n\nWe are looking for technical builders who love the idea of working with early stage startups to help them as they grow. In this role, you ll work directly with a variety of interesting customers and help them make the best (and sometimes the most pragmatic) technical decisions along the way. You ll have a chance to build enduring relationships with these companies and establish yourself as a trusted advisor.\n\nAs well as spending time working directly with customers, you ll also get plenty of time to sharpen the saw and keep your skills fresh. We have more than 175 services across a range of different categories and it s important that we can help startups take advantages of the right ones. You ll also play an important role as an advocate with our product teams to make sure we are building the right products for the startups you work with. And for the customers you don t get to work with on a 1:1 basis you ll get the chance to share your knowledge more broadly by working on technical content and presenting at events.\n\nA day in the life\nYou re surrounded by innovation. You re empowered with a lot of ownership. Your growth is accelerated. The work is challenging. You have a voice here and are encouraged to use it. Your experience and career development is in your hands. We live our leadership principles every day. At Amazon, its always ""Day 1"".\n\nDiverse Experiences\nAmazon values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn t followed a traditional path, or includes alternative experiences, don t let it stop you from applying.\n\nWhy AWS\nAmazon Web Services (AWS) is the world s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating that s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses.\n\nWork/Life Balance\nWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there s nothing we can t achieve in the cloud.\n\nInclusive Team Culture\nHere at AWS, it s in our nature to learn and be curious. Our employee-led affinity groups foster a culture of inclusion that empower us to be proud of our differences. Ongoing events and learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences, inspire us to never stop embracing our uniqueness.\n\nMentorship and Career Growth\nWe re continuously raising our performance bar as we strive to become Earth s Best Employer. That s why you ll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional.\n8+ years of specific technology domain areas (e.g. software development, cloud computing, systems engineering, infrastructure, security, networking, data & analytics) experience\n3+ years of design, implementation, or consulting in applications and infrastructures experience\n10+ years of IT development or implementation/consulting in the software or Internet industries experience\nExperience in a technical role within a sales organization 5+ years of infrastructure architecture, database architecture and networking experience\nKnowledge of cloud architecture\nExperience working with end user or developer communities",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database architecture', 'Cloud computing', 'Career development', 'Web services', 'Networking', 'Consulting', 'Manager Technology', 'Data analytics', 'infrastructure security', 'Senior Solution Architect']",2025-06-12 14:23:55
Data Architecture,Top B2B MNC in Management Consulting Dom...,5 - 8 years,Not Disclosed,['Bengaluru'],"About the Company\nGreetings from Teamware Solutions a division of Quantum Leap Consulting Pvt. Ltd\n\nAbout the Role\nWe are hiring a Data Architecture\n\nLocation: Bangalore\nWork Model: Hybrid\nExperience: 5-9 Years\nNotice Period: Immediate to 15 Days\n\nJob Description:\nData Architecture, Data Governance, Data Modeling\n\nAdditional Information:\nMandatory Skills: Data Architecture, Data Governance, Data Modeling\nNice to have skills Certification in Data Engineering\nInterview Mode Virtual Interview\nminimum 5 yrs relevant experience and maximum 9 yrs for this requirement. Someone with more experience in building PySpark data streaming jobs on Azure Databricks\nwho have done real projects, have expertise, and hands-on experience also\nAlso, Data governance and data modeling experience with a minimum of 4 years is mandatory\nCommunication should be excellent\n\n\nPlease let me know if you are interested in this position and send me your resumes to netra.s@twsol.com",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Architecture', 'Data Modeling', 'Data Governance', 'Data Engineering']",2025-06-12 14:23:57
"Senior Manager- Middle and Back Office Data Analyst- ISS,",Fidelity International,10 - 15 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Title: Middle and Back Office Data Analyst - ISS Data (Senior Manager)\nDepartment: Technology\nLocation: Bangalore & Gurgaon (hybrid / flexible working permitted)\nReports To: Middle and Back Office Data Product Owner\nLevel: Senior Manager\nWe re proud to have been helping our clients build better financial futures for over 50 years. How have we achieved this? By working together - and supporting each other - all over the world. So, join our [insert name of team/ business area] team and feel like you re part of something bigger.\nAbout your team\nThe Technology function provides IT services that are integral to running an efficient run-the business operating model and providing change-driven solutions to meet outcomes that deliver on our business strategy. These include the development and support of business applications that underpin our revenue, operational, compliance, finance, legal, marketing and customer service functions. The broader organisation incorporates Infrastructure services that the firm relies on to operate on a day-to-day basis including data centre, networks, proximity services, security, voice, incident management and remediation.\nThe ISS Technology group is responsible for providing Technology solutions to the Investment Solutions & Services (ISS) business (which covers Investment Management, Asset Management Operations & Distribution business units globally)\n\nThe ISS Technology team supports and enhances existing applications as well as designs, builds and procures new solutions to meet requirements and enable the evolving business strategy.\nAs part of this group, a dedicated ISS Data Programme team has been mobilised as a key foundational programme to support the execution of the overarching ISS strategy.\nAbout your role\nThe Middle and Back Office Data Analyst role is instrumental in the creation and execution of a future state design for Fund Servicing & Oversight data across Fidelity s key business areas. The successful candidate will have an in- depth knowledge of data domains that represent Middle and Back-office operations and technology.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned to deliver Fidelity s cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our clients.\nData Product - Requirements Definition and Delivery of Data Outcomes\nAnalysis of data product requirements to enable business outcomes, contributing to the data product roadmap\nCapture both functional and non-functional data requirements considering the data product and consumers perspectives.\nConduct workshops with both the business and tech stakeholders for requirements gathering, elicitation and walk throughs.\nResponsible for the definition of data requirements, epics and stories within the product backlog and providing analysis support throughout the SDLC.\nResponsible for supporting the UAT cycles, attaining business sign off on outcomes being delivered\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering principles.\nCoordination and Communication:\nExcellent communication skills to influence technology and business stakeholders globally, attaining alignment and sign off on the requirements.\nCoordinate with internal and external stakeholders to communicate data product deliveries and the change impact to the operating model.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\nAbout you\nAt least 10 years of proven experience as a business/technical/data analyst within technology and/or business changes within the financial services /asset management industry.\nMinimum 5 years as a senior business/technical/data analyst adhering to agile methodology, delivering data solutions using industry leading data platforms such as Snowflake, State Street Alpha Data, Refinitiv Eikon, SimCorp Dimension, BlackRock Aladdin, FactSet etc.\nProven experience. of delivering data driven business outcomes using industry leading data platforms such as Snowflake.\nExcellent knowledge of data life cycle that drives Middle and Back Office capabilities such as trade execution, matching, confirmation, trade settlement, record keeping, accounting, fund & cash positions, custody, collaterals/margin movements, corporate actions , derivations and calculations such as holiday handling, portfolio turnover rates, funds of funds look through .\nIn Depth expertise in data and calculations across the investment industry covering the below.\nAsset-specific data: This includes data related to financial instruments reference data like asset specifications, maintenance records, usage history, and depreciation schedules.\nMarket data: This includes data like security prices, exchange rates, index constituents and licensing restrictions on them.\nABOR & IBOR data: This includes calculation engines covering input data sets, calculations and treatment of various instruments for ABOR and IBOR data leveraging platforms such as Simcorp, Neoxam, Invest1, Charles River, Aladdin etc. Knowledge of TPAs, how data can be structured in a unified way from heterogenous structures.\nShould possess Problem Solving, Attention to detail, Critical thinking.\nTechnical Skills: Excellent hands-on SQL, Advanced Excel, Python, ML (optional) and proven experience and knowledge of data solutions.\nKnowledge of data management, data governance, and data engineering practices\nHands on experience on data modelling techniques such as dimensional, data vault etc.\nWillingness to own and drive things, collaboration across business and tech stakeholders.\nFeel rewarded",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['IT services', 'Data analysis', 'Data management', 'Incident management', 'Scrum', 'Customer service', 'Asset management', 'SDLC', 'SQL', 'Python']",2025-06-12 14:24:00
Lead AWS Glue Data Engineer,Allegis Group,8 - 13 years,Not Disclosed,[],"Lead AWS Glue Data Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\n\nWe are seeking a skilled Lead AWS Data Engineer with 8+ years of strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena.\nImplement data migration and transformation processes using AWS DMS and Glue.\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\n\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDS', 'Glue', 'DMS', 'Lambda', 'Redshift', 'Athena']",2025-06-12 14:24:02
Sr Data Engineer,Lowes Services India Private limited,5 - 10 years,Not Disclosed,['Bengaluru'],"We are seeking a seasoned Senior Data Engineer to join our Marketing Data Platform team. This role is pivotal in designing, building, and optimizing scalable data pipelines and infrastructure that support our marketing analytics and customer engagement strategies. The ideal candidate will have extensive experience with big data technologies, cloud platforms, and a strong understanding of marketing data dynamics.\n\nData Pipeline Development & Optimization\nDesign, develop, and maintain robust ETL/ELT pipelines using Apache PySpark on GCP services like Dataproc and Cloud Composer.\nEnsure data pipelines are scalable, efficient, and reliable to handle large volumes of marketing data.\nData Warehousing & Modeling\nImplement and manage data warehousing solutions using BigQuery, ensuring optimal performance and cost-efficiency.\nDevelop and maintain data models that support marketing analytics and reporting needs.\nCollaboration & Stakeholder Engagement\nWork closely with marketing analysts, data scientists, and cross-functional teams to understand data requirements and deliver solutions that drive business insights.\nTranslate complex business requirements into technical specifications and data architecture.\nData Quality & Governance\nImplement data quality checks and monitoring to ensure the accuracy and integrity of marketing data.\nAdhere to data governance policies and ensure compliance with data privacy regulations.\nContinuous Improvement & Innovation\nStay abreast of emerging technologies and industry trends in data engineering and marketing analytics.\nPropose and implement improvements to existing data processes and infrastructure\n  Years of Experience\n5 Years in Data Engineer space\n  Education Qualification & Certifications\nB.Tech or MCA\n  Experience\nProven experience with Apache PySpark, GCP (including Dataproc, BigQuery, Cloud Composer), and data pipeline orchestration.\nTechnical Skills\nProficiency in SQL and Python.\nExperience with data modeling, ETL/ELT processes, and data warehousing concepts.",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['orchestration', 'Data modeling', 'data governance', 'Data quality', 'Apache', 'Continuous improvement', 'Monitoring', 'SQL', 'Python', 'Data architecture']",2025-06-12 14:24:05
Data Engineer,Dun & Bradstreet,5 - 9 years,Not Disclosed,['Hyderabad'],"Key Responsibilities:\n1. Design, build, and deploy new data pipelines within our Big Data Eco-Systems using Streamsets/Talend/Informatica BDM etc. Document new/existing pipelines, Datasets.\n2. Design ETL/ELT data pipelines using StreamSets, Informatica or any other ETL processing engine. Familiarity with Data Pipelines, Data Lakes and modern Data Warehousing practices (virtual data warehouse, push down analytics etc.)\n3. Expert level programming skills on Python\n4. Expert level programming skills on Spark\n5. Cloud Based Infrastructure: GCP\n6. Experience with one of the ETL Informatica, StreamSets in creation of complex parallel loads, Cluster Batch Execution and dependency creation using Jobs/Topologies/Workflows etc.,\n7. Experience in SQL and conversion of SQL stored procedures into Informatica/StreamSets, Strong exposure working with web service origins/targets/processors/executors, XML/JSON Sources and Restful APIs.\n8. Strong exposure working with relation databases DB2, Oracle & SQL Server including complex SQL constructs and DDL generation.\n9. Exposure to Apache Airflow for scheduling jobs\n10. Strong knowledge of Big data Architecture (HDFS), Cluster installation, configuration, monitoring, cluster security, cluster resources management, maintenance, and performance tuning\n11. Create POCs to enable new workloads and technical capabilities on the Platform.\n12. Work with the platform and infrastructure engineers to implement these capabilities in production.\n13. Manage workloads and enable workload optimization including managing resource allocation and scheduling across multiple tenants to fulfill SLAs.\n14. Participate in planning activities, Data Science and perform activities to increase platform skills\n\nKey Requirements:\n1. Minimum 6 years of experience in ETL/ELT Technologies, preferably StreamSets/Informatica/Talend etc.,\n2. Minimum of 6 years hands-on experience with Big Data technologies e.g. Hadoop, Spark, Hive.\n3. Minimum 3+ years of experience on Spark\n4. Minimum 3 years of experience in Cloud environments, preferably GCP\n5. Minimum of 2 years working in a Big Data service delivery (or equivalent) roles focusing on the following disciplines:\n6. Any experience with NoSQL and Graph databases\n7. Informatica or StreamSets Data integration (ETL/ELT)\n8. Exposure to role and attribute based access controls\n9. Hands on experience with managing solutions deployed in the Cloud, preferably on GCP\n10. Experience working in a Global company, working in a DevOps model is a plus",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'GCP', 'Data engineer', 'Spark', 'ETL']",2025-06-12 14:24:07
Data Engineer - SAS Migration,Crisil,2 - 4 years,Not Disclosed,['Mumbai'],"The SAS to Databricks Migration Developer will be responsible for migrating existing SAS code, data processes, and workflows to the Databricks platform\n\nThis role requires expertise in both SAS and Databricks, with a focus on converting SAS logic into scalable PySpark and Python code\n\nThe developer will design, implement, and optimize data pipelines, ensuring seamless integration and functionality within the Databricks environment\n\nCollaboration with various teams is essential to understand data requirements and deliver solutions that meet business needs",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'scala', 'pyspark', 'data warehousing', 'data migration', 'azure data factory', 'sql', 'sql azure', 'java', 'spark', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'sas', 'microsoft azure', 'power bi', 'machine learning', 'sql server', 'data bricks', 'migration', 'sqoop', 'aws', 'ssis']",2025-06-12 14:24:11
Senior Data Engineer,The Main Stage Productions,4 - 6 years,Not Disclosed,['Bengaluru'],"Design and implement cloud-native data architectures on AWS, including data lakes, data warehouses, and streaming pipelines using services like S3, Glue, Redshift, Athena, EMR, Lake Formation, and Kinesis.\nDevelop and orchestrate ETL/ELT pipelines\n\nRequired Candidate profile\nParticipate in pre-sales and consulting activities such as:\nEngaging with clients to gather requirements and propose AWS-based data engineering solutions.\nSupporting RFPs/RFIs, technical proposals",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS Glue', 'GitHub Actions', 'PySpark', 'Scala', 'CodePipeline', 'Step Functions', 'data engineering']",2025-06-12 14:24:13
"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon",One of the largest insurance providers.,5 - 10 years,Not Disclosed,['Gurugram'],"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon\n\nSummary: An excellent opportunity for someone having a minimum of five years of experience with expertise in building data pipelines. A person must have experience in Python, Pyspark and AWS.\n\nLocation- Gurgaon (Hybrid)\n\nYour Future Employer- One of the largest insurance providers.\n\nResponsibilities-\nTo design, develop, and maintain large-scale data pipelines that can handle large datasets from multiple sources.\nReal-time data replication and batch processing of data using distributed computing platforms like Spark, Kafka, etc.\nTo optimize the performance of data processing jobs and ensure system scalability and reliability.\nTo collaborate with DevOps teams to manage infrastructure, including cloud environments like AWS.\nTo collaborate with data scientists, analysts, and business stakeholders to develop tools and platforms that enable advanced analytics and reporting.\n\nRequirements-\nHands-on experience with AWS services such as S3, DMS, Lambda, EMR, Glue, Redshift, RDS (Postgres) Athena, Kinesics, etc.\nExpertise in data modeling and knowledge of modern file and table formats.\nProficiency in programming languages such as Python, PySpark, and SQL/PLSQL for implementing data pipelines and ETL processes.\nExperience data architecting or deploying Cloud/Virtualization solutions (Like Data Lake, EDW, Mart ) in the enterprise.\nCloud/hybrid cloud (preferably AWS) solution for data strategy for Data lake, BI and Analytics.\nWhat is in for you-\nA stimulating working environment with equal employment opportunities.\nGrowing of skills while working with industry leaders and top brands.\nA meritocratic culture with great career progression.\n\nReach us- If you feel that you are the right fit for the role please share your updated CV at randhawa.harmeen@crescendogroup.in\n\nDisclaimer- Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Pipeline', 'AWS', 'Data Ingestion', 'Data Engineering', 'Data Processing']",2025-06-12 14:24:15
CBS & Data Migration - M/Sr.M - Navi Mumbai,Wikilabs India,7 - 12 years,Not Disclosed,['Navi Mumbai'],"Job Title: Manager / Senior Manager CBS & Data Migration\n\nLocation: Navi Mumbai\n\nExperience: 7-12 years\n\nIndustry: Banking\n\nJob Responsibilities:\nCore Banking System (CBS) Implementation & Management:\nLead end-to-end implementation and maintenance of Core Banking Systems (CBS).\nWork closely with vendors, IT teams, and business units to ensure smooth CBS operations.\nProvide technical support, troubleshooting, and system optimization.\nEnsure compliance with banking regulatory standards and security protocols.\n\nData Migration & Management:\nPlan, execute, and oversee large-scale data migration projects for banking platforms.\nEnsure data accuracy, integrity, and security during migration.\nCollaborate with cross-functional teams to define data mapping, transformation, and validation strategies.\nConduct post-migration audits and performance tuning.\n\nTechnical & Project Coordination:\nWork on SQL, Oracle, and other database management systems to support migration.\nHandle data extraction, transformation, and loading (ETL) processes.\nCollaborate with software vendors for CBS and data migration improvements.\nMonitor project timelines, risks, and deliverables.\n\nKey Skills Required:\nCBS Implementation & Support (Finacle, TCS BaNCS, Flexcube, Temenos, etc.)\nBanking Data Migration & Transformation\nETL Processes & Data Mapping\nSQL / Oracle Database Management\nRegulatory Compliance & Risk Management\nProject Management & Vendor Coordination\n\nPreferred Qualifications:\nB.Tech / B.E. / MCA / M.Sc. (IT) or equivalent\nExperience in Banking, Fintech, or IT Services\n\n\nImmediate joiners or candidates with a notice period of 30 days preferred.",Industry Type: Banking,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['CBS Implementation', 'Data Migration', 'Flexcube', 'Finacle', 'Data Mapping', 'CBS Migration', 'Banking IT', 'Core Banking System Migration']",2025-06-12 14:24:17
Data Engineer,Grid Dynamics,4 - 9 years,Not Disclosed,['Bengaluru'],"Required Qualifications:\n4+ years of professional experience in data engineering and data analysis roles.\nStrong proficiency in SQL and experience with database management systems such as MySQL, PostgreSQL, Oracle, and MongoDB.\nHands-on experience with big data tools like Hadoop and Apache Spark.\nProficient in Python programming.\nExperience with data visualization tools such as Tableau, Power BI, and Jupyter Notebooks.\nProven ability to design, build, and maintain scalable ETL pipelines using tools like Apache Airflow, DBT, Composer (GCP), Control-M, Cron, and Luigi.\nFamiliarity with data engineering tools including Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nExperience working with cloud data warehouses and services (Snowflake, Redshift, BigQuery, AWS Glue, GCP Dataflow, Azure Data Factory).\nUnderstanding of data modeling concepts and data lake/data warehouse architectures.\nExperience supporting CI/CD practices with Git, Docker, Terraform, and DevOps workflows.\nKnowledge of both relational and NoSQL databases, including PostgreSQL, BigQuery, MongoDB, and DynamoDB.\nExposure to Agile and DevOps methodologies.\nExperience with at least one cloud platform:\nGoogle Cloud Platform (BigQuery, Dataflow, Composer, Cloud Storage, Pub/Sub)\nAmazon Web Services (S3, Glue, Redshift, Lambda, Athena)\nMicrosoft Azure (Data Factory, Synapse Analytics, Blob Storage)\nEssential functions\nKey Responsibilities:\nDesign, develop, and maintain robust, scalable ETL pipelines using Apache Airflow, DBT, Composer (GCP), Control-M, Cron, Luigi, and similar tools.\nBuild and optimize data architectures including data lakes and data warehouses.\nIntegrate data from multiple sources ensuring data quality and consistency.\nCollaborate with data scientists, analysts, and stakeholders to translate business requirements into technical solutions.\nAnalyze complex datasets to identify trends, generate actionable insights, and support decision-making.\nDevelop and maintain dashboards and reports using Tableau, Power BI, and Jupyter Notebooks for visualization and pipeline validation.\nManage and optimize relational and NoSQL databases such as MySQL, PostgreSQL, Oracle, MongoDB, and DynamoDB.\nWork with big data tools and frameworks including Hadoop, Spark, Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nUtilize cloud data services and warehouses like AWS Glue, GCP Dataflow, Azure Data Factory, Snowflake, Redshift, and BigQuery.\nSupport CI/CD pipelines and DevOps workflows using Git, Docker, Terraform, and related tools.\nEnsure data governance, security, and compliance standards are met.\nParticipate in Agile and DevOps processes to enhance data engineering workflows.\nQualifications\nData Engineer with experience in MySQL or SQL or PL/SQL and any cloud experience like GCP or AWS or Azure\nWould be a plus\nPreferred Skills:\nStrong problem-solving and communication skills.\nAbility to work independently and collaboratively in a team environment.\nExperience with service development, REST APIs, and automation testing is a plus.\nFamiliarity with version control systems and workflow automation.\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Automation', 'Data modeling', 'MySQL', 'Workflow', 'Informatica', 'Oracle', 'Apache', 'SSIS', 'Analytics']",2025-06-12 14:24:20
Senior Data Engineer,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nOR\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Git', 'PySpark', 'CI/CD', 'Databricks', 'ETL', 'NOSQL', 'AWS', 'data integration', 'SQL', 'Apache Spark', 'Python']",2025-06-12 14:24:22
Oracle BRM Data Migration Engineer,Techstar Group,5 - 10 years,Not Disclosed,"['Noida', 'Hyderabad', 'Bengaluru']","Work Location : Hyderabad, Bangalore, Noida, Pune\nQualifications and Skills :\n- Proven expertise in Oracle BRM (Mandatory skill) with a strong understanding of its architecture and modules to effectively manage data migration processes.\n\n- Hands-on experience in data migration activities, particularly with Oracle BRM, ensuring high efficiency and accuracy throughout migration projects.\n\n- Knowledge in SQL for querying and managing databases, crucial for data migration and integration tasks.\n\n- Strong knowledge of ETL tools and processes for efficient data extraction, transformation, and loading from various sources.\n\n- Ability to perform detailed data mapping, ensuring logical transformation and compatibility between source and target system data structures.\n\n- Experience in data cleansing techniques to ensure data integrity and consistency throughout the migration process.\n\n- Understanding of data quality principles and practices, essential to maintain high standards of data accuracy and dependability.\n\n- Proficiency in scripting for automation of data migration tasks, enhancing efficiency and reducing potential for errors.\n\n- Excellent analytical and problem-solving skills to identify and address data-related challenges and opportunities.\n\n- Handling the execution of the data migration and validations.\n\n- Handle the develop Migration strategy documents and techniques. Execute data integrity testing post migration.\n\n- Understanding BRM : Having a working knowledge of BRM data migration components, the BRM 12 schema, and the data model\n\n- Data migration strategy : Developing a migration strategy and implementation plan\n\n- Data loading : Being able to load data and integrate it with systems\n\n- Post-migration analysis : Performing post-migration analysis on events, invoices, open items, bills, and dunning\n\n- Data reconciliation : Developing scripts to reconcile migrated data\n\n- Working Knowledge of all the BRM Data migration components.\n\n- Must have hands-on in BRM to verify the sanity of the Data migration.\n\n- Advantage - Programming skills on Java technologies. Exp. in C/C++, Oracle 12c/19c, PL/SQL, PCM Java, BRM Webservice, Scripting language (perl/python)\n\nRoles and Responsibilities :\n\n- Analyze client data and formulating effective data migration plans tailored to Oracle BRM specifications.\n\n- Collaborate with cross-functional teams to gather and interpret data migration requirements accurately.\n\n- Develop and implement efficient data migration scripts and processes, ensuring minimal disruption to business operations.\n\n- Conduct thorough testing and validation of data migration outputs to guarantee data accuracy and conformity.\n\n- Monitor and troubleshoot migration activities to ensure seamless execution and rectify any issues promptly.\n\n- Document data migration processes, maps, and transformations for knowledge sharing and continuous improvement.\n\n- Liaise with stakeholders to present progress updates and discuss ongoing improvements to data migration practices.\n\n- Contribute to the development of data migration best practices and reusable frameworks within the organization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle BRM', 'Java', 'Data Quality', 'Oracle Apps', 'C++', 'PL/SQL', 'Data Migration', 'Data reconciliation', 'ETL', 'SQL']",2025-06-12 14:24:24
Senior Data Engineer,Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data engineering', 'performance tuning', 'data security', 'data processing', 'Hadoop', 'Apache Spark', 'SQL', 'CI/CD', 'troubleshooting', 'big data', 'aws', 'ETL', 'Python']",2025-06-12 14:24:27
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nRole Description:\nWe are seeking a Senior Data Engineer with expertise in Graph Data technologies to join our data engineering team and contribute to the development of scalable, high-performance data pipelines and advanced data models that power next-generation applications and analytics. This role combines core data engineering skills with specialized knowledge in graph data structures, graph databases, and relationship-centric data modeling, enabling the organization to leverage connected data for deep insights, pattern detection, and advanced analytics use cases. The ideal candidate will have a strong background in data architecture, big data processing, and Graph technologies and will work closely with data scientists, analysts, architects, and business stakeholders to design and deliver graph-based data engineering solutions.\nRoles & Responsibilities:\nDesign, build, and maintain robust data pipelines using Databricks (Spark, Delta Lake, PySpark) for complex graph data processing workflows.\nOwn the implementation of graph-based data models, capturing complex relationships and hierarchies across domains.\nBuild and optimize Graph Databases such as Stardog, Neo4j, Marklogic or similar to support query performance, scalability, and reliability.\nImplement graph query logic using SPARQL, Cypher, Gremlin, or GSQL, depending on platform requirements.\nCollaborate with data architects to integrate graph data with existing data lakes, warehouses, and lakehouse architectures.\nWork closely with data scientists and analysts to enable graph analytics, link analysis, recommendation systems, and fraud detection use cases.\nDevelop metadata-driven pipelines and lineage tracking for graph and relational data processing.\nEnsure data quality, governance, and security standards are met across all graph data initiatives.\nMentor junior engineers and contribute to data engineering best practices, especially around graph-centric patterns and technologies.\nStay up to date with the latest developments in graph technology, graph ML, and network analytics.\nWhat we expect of you\nMust-Have Skills:\nHands-on experience in Databricks, including PySpark, Delta Lake, and notebook-based development.\nHands-on experience with graph database platforms such as Stardog, Neo4j, Marklogic etc.\nStrong understanding of graph theory, graph modeling, and traversal algorithms\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies with strong problem-solving and analytical skills\nExcellent collaboration and communication skills, with experience working with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SPARQL', 'Maven', 'PySpark', 'GSQL', 'Subversion', 'AWS services', 'Stardog', 'Cypher', 'SAFe', 'Jenkins', 'DevOps', 'Git', 'Neo4j', 'Delta Lake', 'Graph Databases', 'Spark', 'Marklogic', 'Gremlin']",2025-06-12 14:24:29
"Data & Analytics Analyst, VP",NatWest Markets,15 - 20 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Join us as a Data & Analytics Analyst\nTake on a new challenge in Data & Analytics and help us shape the future of our business\nYou ll take accountability for the analysis of complex data to identify business issues and opportunities, and supporting the delivery of high quality business solutions\nWere committed to mapping a career path that works for you, with a focus on helping you build new skills and engage with the latest ideas and technologies in data analytics\nWere offering this role at vice president level\nWhat youll do\nAs a Data & Analytics Analyst, you ll be driving the production of high quality analytical input to support the development and implementation of innovative processes and problem resolution. You ll be capturing, validating and documenting business and data requirements, making sure they are in line with key strategic principles.\nWe ll look to you to interrogate, interpret and visualise large volumes of data to identify, support and challenge business opportunities and identify solutions.\nYou ll also be:\nPerforming data extraction, storage, manipulation, processing and analysis\nConducting and supporting options analysis, identifying the most appropriate solution\nAccountable for the full traceability and linkage of business requirements of analytics outputs\nSeeking opportunities to challenge and improve current business processes, ensuring the best result for the customer\nCreating and executing quality assurance at various stages of the project in order to validate the analysis and to ensure data quality, identify data inconsistencies, and resolve as needed\nStrong sense of ownership with a focus on delivering high-quality outcomes\nExceptional attention to detail\nEmphasis on measurable outcomes and impact of work\nExpertise in data analytics and reporting\nThe skills youll need\nYou ll need a background in business analysis tools and techniques, along with the ability to influence through communications tailored to a specific audience. Additionally, you ll need the ability to use core technical skills.\nYou ll also demonstrate:\nClear and effective communication\nProficiency in SQL, and tools such as Excel and Power BI\nExperience in Informatica, Snowflake or others\nResponsible for performance metrics and data solutions across the entire data architecture team\nSkilled in data visualization, report generation and presentation to both technical and business audiences\nOver 15 years of professional experience\nHours\n45\nJob Posting Closing Date:\n23/06/2025",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'Business analysis', 'Analytical', 'Data quality', 'Data analytics', 'Informatica', 'Business solutions', 'SQL', 'Data extraction', 'Data architecture']",2025-06-12 14:24:32
Technical Architect,PwC India,10 - 15 years,Not Disclosed,"['Mumbai', 'Navi Mumbai', 'Gurugram']","Role Description\nWe are looking for a suitable candidate for the opening of Data/Technical Architect role for Data Management, preferably for one who has worked in Insurance or Banking and Financial Services domain and holds relevant experience of 10+ years. The candidate should be willing to take up the role of Senior Manager/Associate Director in an organization based on overall experience.\nLocation : Mumbai and Gurugram\nRelevant experience : 10+ years",,,,"['Data Architecture', 'Technical Architecture', 'Java', 'Bigquery', 'SCALA', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'Data Bricks', 'Python']",2025-06-12 14:24:34
Immediate Joiner- Data Engineer,Healthedge,1 - 4 years,Not Disclosed,['Bengaluru'],"Data Engineer\nYou will be working with agile cross functional software development teams developing cutting age software to solve a significant problem in the Provider Data Management space. This hire will have experience building large scale complex data systems involving multiple cross functional data sets and teams. The ideal candidate will be excited about working on new product development, is comfortable pushing the envelope and challenging the status quo, sets high standards for him/herself and the team, and works well with ambiguity.\nWhat you will do:\nBuild data pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements.\nWork closely with data architect, SMEs and other technology partners to develop & execute data architecture and product roadmap.\nBuild analytical tools to utilize the data pipeline, providing actionable insight into key business performance including operational efficiency and business metrics.\nWork with stakeholders including the leadership, product, customer teams to support their data infrastructure needs while assisting with data-related technical issues.\nAct as a subject matter expert to other team members for technical guidance, solution design and best practices within the customer organization.\nKeep current on big data and data visualization technology trends, evaluate, work on proof-of-concept and make recommendations on cloud technologies.\nWhat you bring:\n2+ years of data engineering experience working in partnership with large data sets (preferably terabyte scale)\nExperience in building data pipelines using any of the ETL tools such as Glue, ADF, Notebooks, Stored Procedures, SQL/Python constructs or similar.\nDeep experience working with industry standard RDBMS such Postgres, SQL Server, Oracle, MySQL etc. and any of the analytical cloud databases such as Big Query, Redshift, Snowflake or similar\nAdvanced SQL expertise and solid programming experience with Python and/or Spark\nExperience working with orchestration tools such as Airflow and building complex dependency workflows.\nExperience, developing and implementing Data Warehouse or Data Lake Architectures, OLAP technologies, data modeling with star/snowflake-schemas to enable analytics & reporting.\nGreat problem-solving capabilities, troubleshooting data issues and experience in stabilizing big data systems.\nExcellent communication and presentation skills as youll be regularly interacting with stakeholders and engineering leadership.\nBachelors or master's in quantitative disciplines such as Computer Science, Computer Engineering, Analytics, Mathematics, Statistics, Information Systems, or other scientific fields.\nBonus points:\nHands-on deep experience with cloud data migration, and experience working with analytic platforms like Fabric, Databricks on the cloud.\nCertification in one of the cloud platforms (AWS/GCP/Azure)\nExperience or demonstrated understanding with real-time data streaming tools like Kafka, Kinesis or any similar tools.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Pyspark', 'Cloud', 'Python']",2025-06-12 14:24:37
Senior Data Manager/ Lead,Codeforce 360,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description:\nWe are looking for a highly experienced and dynamic Senior Data Manager / Lead to oversee a team of Data Engineers and Data Scientists. This role demands a strong background in data platforms such as Snowflake and proficiency in Python, combined with excellent people management and project leadership skills. While hands-on experience in the technologies is beneficial, the primary focus of this role is on team leadership, strategic planning, and project delivery .\n\nJob Title : Senior Data Manager / Lead\nLocation: Hyderabad (Work From Office)\nShift Timing: 10AM-7PM\nKey Responsibilities:\nLead, mentor, and manage a team of Data Engineers and Data Scientists.\nOversee the design and implementation of data pipelines and analytics solutions using Snowflake and Python.\nCollaborate with cross-functional teams (product, business, engineering) to align data solutions with business goals.\nEnsure timely delivery of projects, with high quality and performance.\nConduct performance reviews, training plans, and support career development for the team.\nSet priorities, allocate resources, and manage workloads within the data team.\nDrive adoption of best practices in data management, governance, and documentation.\nEvaluate new tools and technologies relevant to data engineering and data science.\n\nRequired Skills & Qualifications:\n6+ years of experience in data-related roles, with at least 23 years in a leadership or management position.\nStrong understanding of Snowflake architecture, performance tuning, data sharing, security, etc.\nSolid knowledge of Python for data engineering or data science tasks.\nExperience in leading data migration, ETL/ELT, and analytics projects.\nAbility to translate business requirements into technical solutions.\nExcellent leadership, communication, and stakeholder management skills.\nExposure to tools like Databricks, Dataiku, Airflow, or similar platforms is a plus.\nBachelors or Masterâ€™s degree in Computer Science, Engineering, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Bricks', 'Python', 'Airflow', 'Data Migration', 'Dataiku', 'Data Warehousing', 'ETL', 'ELT', 'SQL']",2025-06-12 14:24:39
Data Engineer,Axis Finance (AFL),7 - 11 years,Not Disclosed,"['Mumbai', 'Mumbai (All Areas)']","Key Responsibilities:\nShould have experience in below\nDesign, develop, and implement a Data Lake House architecture on AWS, ensuring scalability, flexibility, and performance.\nBuild ETL/ELT pipelines for ingesting, transforming, and processing structured and unstructured data.\nCollaborate with cross-functional teams to gather data requirements and deliver data solutions aligned with business needs.\nDevelop and manage data models, schemas, and data lakes for analytics, reporting, and BI purposes.\nImplement data governance practices, ensuring data quality, security, and compliance.\nPerform data integration between on-premise and cloud systems using AWS services.\nMonitor and troubleshoot data pipelines and infrastructure for reliability and scalability.\nSkills and Qualifications:\n7 + years of experience in data engineering, with a focus on cloud data platforms.\nStrong experience with AWS services: S3, Glue, Redshift, Athena, Lambda, IAM, RDS, and EC2.\nHands-on experience in building data lakes, data warehouses, and lake house architectures.\nShould have experience in ETL/ELT pipelines using tools like AWS Glue, Apache Spark, or similar.\nExpertise in SQL and Python or Java for data processing and transformations.\nFamiliarity with data modeling and schema design in cloud environments.\nUnderstanding of data security and governance practices, including IAM policies and data encryption.\nExperience with big data technologies (e.g., Hadoop, Spark) and data streaming services (e.g., Kinesis, Kafka).\nHave lending domain knowledge will be added advantage\nPreferred Skills:\nExperience with Databricks or similar platforms for data engineering.\nFamiliarity with DevOps practices for deploying data solutions on AWS (CI/CD pipelines).\nKnowledge of API integration and cloud data migration strategies.",Industry Type: NBFC,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['pipeline tools', 'lending domain', 'AWS', 'data models', 'spark', 'devops', 'databricks', 'date engineering platforms', 'hadoop', 'data lake house', 'API integration']",2025-06-12 14:24:41
Data Engineer,Aqilea Softech,5 - 9 years,13-20 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Job Title: Data Engineer\nCompany : Aqilea India(Client : H&M India)\nEmployment Type: Full Time\nLocation: Bangalore(Hybrid)\nExperience: 4.5 to 9 years\nClient : H&M India\n\nAt H&M, we welcome you to be yourself and feel like you truly belong. Help us reimagine the future of an entire industry by making everyone look, feel, and do good. We take pride in our history of making fashion accessible to everyone and led by our values we strive to build a more welcoming, inclusive, and sustainable industry. We are privileged to have more than 120,000 colleagues, in over 75 countries across the world. Thats 120 000 individuals with unique experiences, skills, and passions. At H&M, we believe everyone can make an impact, we believe in giving people responsibility and a strong sense of ownership. Our business is your business, and when you grow, we grow.\nWebsite : https://career.hm.com/\n\nWe are seeking a skilled and forward-thinking Data Engineer to join our Emerging Tech team. This role is designed for someone passionate about working with cutting-edge technologies such as AI, machine learning, IoT, and big data to turn complex data sets into actionable insights.\nAs the Data Engineer in Emerging Tech, you will be responsible for designing, implementing, and optimizing data architectures and processes that support the integration of next-generation technologies. Your role will involve working with large-scale datasets, building predictive models, and utilizing emerging tools to enable data-driven decision-making across the business. You ll collaborate with technical and business teams to uncover insights, streamline data pipelines, and ensure the best use of advanced analytics technologies.\n\nKey Responsibilities:\nDesign and build scalable data architectures and pipelines that support machine learning, analytics, and IoT initiatives.\nDevelop and optimize data models and algorithms to process and analyse large-scale, complex data sets.\nImplement data governance, security, and compliance measures to ensure high-quality\nCollaborate with cross-functional teams (engineering, product, and business) to translate business requirements into data-driven solutions.\nEvaluate, integrate, and optimize new data technologies to enhance analytics capabilities and drive business outcomes.\nApply statistical methods, machine learning models, and data visualization techniques to deliver actionable insights.\nEstablish best practices for data management, including data quality, consistency, and scalability.\nConduct analysis to identify trends, patterns, and correlations within data to support strategic business initiatives.\nStay updated on the latest trends and innovations in data technologies and emerging data management practices.\n\nSkills Required :\nBachelors or masters degree in data science, Computer Science, Engineering, Statistics, or a related field.\n4.5-9 years of experience in data engineering, data science, or a similar analytical role, with a focus on emerging technologies.\nProficiency with big data frameworks (e.g., Hadoop, Spark, Kafka) and experience with modern cloud platforms (AWS, Azure, or GCP).\nSolid skills in Python, SQL, and optionally R, along with experience using machine learning libraries such as Scikit-learn, TensorFlow, or PyTorch.\nExperience with data visualization tools (e.g., Tableau or Power BI or D3.js) to communicate insights effectively.\nFamiliarity with IoT and edge computing data architectures is a plus.\nUnderstanding of data governance, compliance, and privacy standards.\nAbility to work with both structured and unstructured data.\nExcellent problem-solving, communication, and collaboration skills, with the ability to work in a fast-paced, cross-functional team environment.\nA passion for emerging technologies and a continuous desire to learn and innovate.\nInterested Candidates can share your Resumes to mail id karthik.prakadish@aqilea.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Powerbi', 'Hadoop', 'Kafka', 'Tableau', 'Azure', 'GCP', 'Data Engineer', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-12 14:24:43
"Walk-In Senior Data Engineer - DataStage, Azure & Power BI",Net Connect,6 - 10 years,5-11 Lacs P.A.,['Hyderabad( Madhapur )'],"Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\nBelow JD for your reference\n\nJob Description:\n\nWe are hiring an experienced Senior Data Engineer with strong expertise in IBM DataStage, , and . The ideal candidate will be responsible for end-to-end data integration, transformation, and reporting solutions that drive business decisions.",,,,"['Azure Data Factory', 'Datastage', 'Etl Datastage']",2025-06-12 14:24:45
Data & Analytics Subject Matter Expert,Trianz,10 - 15 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role Overview\nWe are looking for a Data & Analytics Subject Matter Expert with deep expertise in Data Engineering, Business Intelligence (BI), and AWS cloud ecosystems . This role demands strategic thinking, hands-on execution, and collaboration across technical and business teams to deliver impactful data-driven solutions.\nKey Responsibilities\n1. Data Architecture & Engineering\nDesign and implement scalable, high-performance data solutions on AWS.\nBuild robust data pipelines, ETL/ELT workflows, and data lake architectures.\nEnforce data quality, security, and governance practices.\n2. Business Intelligence & Insights\nDevelop interactive dashboards and visualizations using Power BI, Tableau, or QuickSight.\nDefine data models and KPIs to support data-driven decision-making.\nCollaborate with business teams to extract insights that drive action.\n3. Cloud & Advanced Analytics\nDeploy data warehousing solutions using Redshift, Glue, S3, Athena, and other AWS services.\nOptimize storage and processing strategies for performance and cost-efficiency.\nExplore AI/ML integrations for predictive and advanced analytics (preferred).\n4. Collaboration & Best Practices\nPartner with cross-functional teams (engineering, data science, business) to align on data needs.\nChampion best practices in data governance, compliance, and architecture.\nTranslate business requirements into scalable technical solutions.\nRequired Qualifications\nEducation\nBachelor s or Master s in Computer Science, Information Technology, Data Science, or related discipline.\nExperience\n10+ years of experience in data engineering, BI, and analytics domains.\nProven experience with AWS data tools and modern data architectures.\nTechnical Skills\nStrong command of AWS services: Redshift, Glue, S3, Athena, Lambda, Kinesis.\nProficient in SQL, Python, or Scala.\nExperience building and maintaining ETL/ELT workflows and data models.\nExpertise in BI tools like Power BI, Tableau, QuickSight, or Looker.\nFamiliarity with AI/ML models and frameworks is a plus.\nCertifications\nPreferred: AWS Certified Data Analytics - Specialty.\nAdditional certifications in AWS, data engineering, or analytics are a plus.\nWhy Join Trianz\nJoin a high-growth, innovation-led firm delivering transformation at scale.\nCollaborate with global teams on cutting-edge cloud and analytics projects.\nEnjoy a competitive compensation structure and clear career progression pathways.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['power bi', 'Data analytics', 'Subject Matter Expert', 'Business intelligence', 'AWS', 'Information technology', 'Analytics', 'SQL', 'Data architecture', 'Python']",2025-06-12 14:24:48
"Sr. Data Engineer, R&D Data Catalyst Team",Amgen Inc,7 - 9 years,Not Disclosed,['Hyderabad'],"The R&D Data Catalyst Team is responsible for buildingData Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nThe Data Engineer will be responsible for the end-to-end development of an enterprise analytics and data mastering solution leveraging Databricks and Power BI. This role requiresexpertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that research cohort-building and advanced research pipeline.The ideal candidate will have experience creating and surfacing large unifiedrepositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\nYou will collaborate closely with stakeholders, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a strong background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nRoles & Responsibilities:\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with stakeholders to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\nBasic Qualifications and Experience:\nMasters degree with 1 to 3years of experience in Data Engineering OR\nBachelors degree with 4 to 5 years of experience in Data Engineering\nDiploma and 7 to 9 years of experience in Data Engineering.\nFunctional Skills:\nMust-Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nStrong communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood-to-Have Skills:\nExperience in developing differentiated and deliverable solutions\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'ETL processes', 'DAX', 'Business Objects', 'data warehouse design', 'ETL', 'PowerBI Models', 'AWS', 'Power Query']",2025-06-12 14:24:50
Sr Data Engineering Manager,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a Senior Data Engineering Manager with a strong background in Regulatory or Integrated Product Teams within the Biotech or Pharmaceutical domain. This role will lead the end-to-end data strategy and execution for regulatory product submissions, lifecycle management, and compliance reporting, ensuring timely and accurate delivery of regulatory data assets across global markets.You will be embedded in a cross-functional Regulatory Integrated Product Team (IPT) and serve as the data and technology lead, driving integration between scientific, regulatory, and engineering functions to support submission-ready data and regulatory intelligence solutions.\nRoles & Responsibilities:\nFunctional Skills:\nLead the engineering strategy and implementation for end-to-end regulatory operations, including data ingestion, transformation, integration, and delivery across regulatory systems.\nServe as the data engineering SME in the Integrated Product Team (IPT) to support regulatory submissions, agency interactions, and lifecycle updates.\nCollaborate with global regulatory affairs, clinical, CMC, quality, safety, and IT teams to gather submission data requirements and translate them into data engineering solutions.\nManage and oversee the development of data pipelines, data models, and metadata frameworks that support submission data standards (e.g., eCTD, IDMP, SPL, xEVMPD).\nEnable integration and reporting across regulatory information management systems (RIMS), EDMS, clinical trial systems, and lab data platforms.\nImplement data governance, lineage, validation, and audit trails for regulatory data workflows, ensuring GxP and regulatory compliance.\nGuide the development of automation solutions, dashboards, and analytics that improve visibility into submission timelines, data quality, and regulatory KPIs.\nEnsure interoperability between regulatory data platforms and enterprise data lakes or lakehouses for cross-functional reporting and insights.\nCollaborate with IT, data governance, and enterprise architecture teams to ensure alignment with overall data strategy and compliance frameworks.\nDrive innovation by evaluating emerging technologies in data engineering, graph data, knowledge management, and AI for regulatory intelligence.\nLead, mentor, and coach a small team of data engineers and analysts, fostering a culture of excellence, innovation, and delivery.\nDrive Agile and Scaled Agile (SAFe) methodologies, managing sprint backlogs, prioritization, and iterative improvements to enhance team velocity and project delivery.\nStay up-to-date with emerging data technologies, industry trends, and best practices, ensuring the organization leverages the latest innovations in data engineering and architecture.\nMust-Have Skills:\n812 years of experience in data engineering or data architecture, with 3+ years in a senior or managerial capacity, preferably within the biotech or pharmaceutical industry.\nProven experience supporting regulatory functions, including submissions, tracking, and reporting for FDA, EMA, and other global authorities.\nExperience with ETL/ELT tools, data pipelines, and cloud-based data platforms (e.g., Databricks, AWS, Azure, or GCP).\nFamiliarity with regulatory standards and data models such as eCTD, IDMP, HL7, CDISC, and xEVMPD.\nDeep understanding of GxP data compliance, audit requirements, and regulatory submission processes.\nExperience with tools like Power BI, Tableau, or Qlik for regulatory dashboarding and visualization is a plus.\nStrong project management, stakeholder communication, and leadership skills, especially in matrixed, cross-functional environments.\nAbility to translate technical capabilities into regulatory and business outcomes.Prepare team members for stakeholder discussions by helping assess data costs, access requirements, dependencies, and availability for business scenarios.\nGood-to-Have Skills:\nPrior experience working on integrated product teams or regulatory transformation programs.\nKnowledge of Regulatory Information Management Systems (RIMS), Veeva Vault RIM, or Master Data Management (MDM) in regulated environments.\nFamiliarity with Agile/SAFe methodologies and DevOps/DataOps best practices.\nEducation and Professional Certifications\n12 to 15 years of experience in Computer Science, IT or related field\nScaled Agile SAFe certification preferred\nProject Management certifications preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'engineering strategy', 'DevOps', 'Project Management', 'DataOps', 'Agile', 'data strategy']",2025-06-12 14:24:52
Data Engineer (AWS),Neoware Technology Solutions,4 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Engineer (AWS) - Neoware Technology Solutions Private Limited Data Engineer (AWS)\nRequirements\n4 - 10 years of hands-on experience in designing, developing and implementing data engineering solutions.\nStrong SQL development skills, including performance tuning and query optimization.\nGood understanding of data concepts.\nProficiency in Python and a solid understanding of programming concepts.\nHands-on experience with PySpark or Spark Scala for building data pipelines.\nUnderstanding of streaming data pipelines for near real-time analytics.\nExperience with Azure services including Data Factory, Functions, Databricks, Synapse Analytics, Event Hub, Stream Analytics and Data Lake Storage.\nFamiliarity with at least one NoSQL database.\nKnowledge of modern data architecture patterns and industry trends in data engineering.\nUnderstanding of data governance concepts for data platforms and analytical solutions.\nExperience with Git for managing version control for source code.\nExperience with DevOps processes, including experience implementing CI/CD pipelines for data engineering solutions.\nStrong analytical and problem-solving skills.\nExcellent communication and teamwork skills.\nResponsibilities\nAzure Certifications related to Data Engineering are highly preferred.\nExperience with Amazon AppFlow, EKS, API Gateway, NoSQL database services.\nStrong understanding and experience with BI/visualization tools like Power BI.\nChennai, Bangalore\nFull time\n4+ years\nOther positions Principal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Version control', 'GIT', 'NoSQL', 'query optimization', 'Analytical', 'data governance', 'Analytics', 'Python', 'Data architecture']",2025-06-12 14:24:54
Data Engineer (Azure),Neoware Technology Solutions,4 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Engineer (Azure) - Neoware Technology Solutions Private Limited Data Engineer (Azure)\nRequirements\n4 - 10 years of hands-on experience in designing, developing and implementing data engineering solutions.\nStrong SQL development skills, including performance tuning and query optimization.\nGood understanding of data concepts.\nProficiency in Python and a solid understanding of programming concepts.\nHands-on experience with PySpark or Spark Scala for building data pipelines.\nEnsure data consistency and address ambiguities or inconsistencies across datasets.\nUnderstanding of streaming data pipelines for near real-time analytics.\nExperience with Azure services including Data Factory, Functions, Databricks, Synapse Analytics, Event Hub, Stream Analytics and Data Lake Storage.\nFamiliarity with at least one NoSQL database.\nKnowledge of modern data architecture patterns and industry trends in data engineering.\nUnderstanding of data governance concepts for data platforms and analytical solutions.\nExperience with Git for managing version control for source code.\nExperience with DevOps processes, including experience implementing CI/CD pipelines for data engineering solutions.\nStrong analytical and problem-solving skills.\nExcellent communication and teamwork skills.\nResponsibilities\nAzure Certifications related to Data Engineering are highly preferred.\nExperience with Azure Kubernetes Service (AKS), Container Apps and API Management.\nStrong understanding and experience with BI/visualization tools like Power BI.\nChennai, Bangalore\nFull time\n4+ years\nOther positions\nChennai / Bangalore / Mumbai\n3+ years\nPrincipal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Version control', 'GIT', 'query optimization', 'NoSQL', 'Analytical', 'SCALA', 'Analytics', 'Python', 'Data architecture']",2025-06-12 14:24:56
"Data Engineering : Sr Software Engineer, Tech Lead & Sr Tech Lead",Reflion Tech,7 - 12 years,22.5-37.5 Lacs P.A.,"['Mumbai( Ghansoli )', 'Navi Mumbai', 'Mumbai (All Areas)']","Hiring: Data Engineering Senior Software Engineer / Tech Lead / Senior Tech Lead\n\n- Hybrid (3 Days from office) | Shift: 2 PM 11 PM IST\n- Experience: 5 to 12+ years (based on role & grade)\n\nOpen Grades/Roles:\nSenior Software Engineer: 58 Years\nTech Lead: 7â€“10 Years\nSenior Tech Lead: 10â€“12+ Years\n\nJob Description â€“ Data Engineering Team\n\nCore Responsibilities (Common to All Levels):\n\nDesign, build and optimize ETL/ELT pipelines using tools like Pentaho, Talend, or similar\nWork on traditional databases (PostgreSQL, MSSQL, Oracle) and MPP/modern systems (Vertica, Redshift, BigQuery, MongoDB)\nCollaborate cross-functionally with BI, Finance, Sales, and Marketing teams to define data needs\nParticipate in data modeling (ER/DW/Star schema), data quality checks, and data integration\nImplement solutions involving messaging systems (Kafka), REST APIs, and scheduler tools (Airflow, Autosys, Control-M)\nEnsure code versioning and documentation standards are followed (Git/Bitbucket)\n\nAdditional Responsibilities by Grade\n\nSenior Software Engineer (5â€“8 Yrs):\nFocus on hands-on development of ETL pipelines, data models, and data inventory\nAssist in architecture discussions and POCs\nGood to have: Tableau/Cognos, Python/Perl scripting, GCP exposure\n\nTech Lead (7â€“10 Yrs):\nLead mid-sized data projects and small teams\nDecide on ETL strategy (Push Down/Push Up) and performance tuning\nStrong working knowledge of orchestration tools, resource management, and agile delivery\n\nSenior Tech Lead (10â€“12+ Yrs):\nDrive data architecture, infrastructure decisions, and internal framework enhancements\nOversee large-scale data ingestion, profiling, and reconciliation across systems\nMentoring junior leads and owning stakeholder delivery end-to-end\nAdvantageous: Experience with AdTech/Marketing data, Hadoop ecosystem (Hive, Spark, Sqoop)\n\n- Must-Have Skills (All Levels):\n\nETL Tools: Pentaho / Talend / SSIS / Informatica\nDatabases: PostgreSQL, Oracle, MSSQL, Vertica / Redshift / BigQuery\nOrchestration: Airflow / Autosys / Control-M / JAMS\nModeling: Dimensional Modeling, ER Diagrams\nScripting: Python or Perl (Preferred)\nAgile Environment, Git-based Version Control\nStrong Communication and Documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SQL', 'ETL', 'Orchestration', 'Postgresql', 'Peri', 'Informatica', 'ETL Tool', 'SSIS', 'Elt', 'Modeling', 'MongoDB', 'Data Architecture', 'Talend', 'Pentaho', 'Python']",2025-06-12 14:24:59
Senior Data Engineer,Jeavio,5 - 10 years,Not Disclosed,[],"We are seeking an experienced Senior Data Engineer to join our team. The ideal candidate will have a strong background in data engineering and AWS infrastructure, with hands-on experience in building and maintaining data pipelines and the necessary infrastructure components. The role will involve using a mix of data engineering tools and AWS services to design, build, and optimize data architecture.\n\nKey Responsibilities:\nDesign, develop, and maintain data pipelines using Airflow and AWS services.\nImplement and manage data warehousing solutions with Databricks and PostgreSQL.\nAutomate tasks using GIT / Jenkins.\nDevelop and optimize ETL processes, leveraging AWS services like S3, Lambda, AppFlow, and DMS.\nCreate and maintain visual dashboards and reports using Looker.\nCollaborate with cross-functional teams to ensure smooth integration of infrastructure components.\nEnsure the scalability, reliability, and performance of data platforms.\nWork with Jenkins for infrastructure automation.\n\nTechnical and functional areas of expertise:\nWorking as a senior individual contributor on a data intensive project\nStrong experience in building high performance, resilient & secure data processing pipelines preferably using Python based stack.\nExtensive experience in building data intensive applications with a deep understanding of querying and modeling with relational databases preferably on time-series data.\nIntermediate proficiency in AWS services (S3, Airflow)\nProficiency in Python and PySpark\nProficiency with ThoughtSpot or Databricks.\nIntermediate proficiency in database scripting (SQL)\nBasic experience with Jenkins for task automation\n\nNice to Have :\nIntermediate proficiency in data analytics tools (Power BI / Tableau / Looker / ThoughSpot)\nExperience working with AWS Lambda, Glue, AppFlow, and other AWS transfer services.\nExposure to PySpark and data automation tools like Jenkins or CircleCI.\nFamiliarity with Terraform for infrastructure-as-code.\nExperience in data quality testing to ensure the accuracy and reliability of data pipelines.\nProven experience working directly with U.S. client stakeholders.\nAbility to work independently and take the lead on tasks.\n\nEducation and experience:\nBachelors or masters in computer science or related fields.\n5+ years of experience\n\nStack/Skills needed:\nDatabricks\nPostgreSQL\nPython & Pyspark\nAWS Stack\nPower BI / Tableau / Looker / ThoughSpot\nFamiliarity with GIT and/or CI/CD tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'AWS', 'Data Bricks', 'Python', 'Etl Pipelines', 'Airflow', 'Database Scripting', 'Postgresql', 'Looker', 'SQL']",2025-06-12 14:25:01
Data Engineer,Nemetschek,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\n5+ years in software development, with a focus on data-intensive applications, cloud solutions, and scalable data architectures.\nDevelopment experience in GoLang for building scalable and efficient data applications.\nExperience with Snowflake, Redshift, or similar data platforms including architecture, data modeling, performance optimization, and integrations.\nExperience designing and building data lakes and data warehouses, ensuring data integrity, scalability, and performance.\nProficient in developing and managing ETL pipelines, using modern tools and techniques to transform, load, and integrate data efficiently.\nExperience with high-volume event streams (such as Kafka, Kinesis) and near real-time data processing solutions for fast and accurate reporting.\nHands-on experience with Terraform for automating infrastructure deployment and configuration management in cloud environments.\nExperience with containerization technologies (Docker, Kubernetes) and orchestration.\nSolid grasp of database fundamentals (SQL, NoSQL, data modeling, performance tuning)\nExperience with CI/CD pipelines and automation tools for testing, deployment, and continuous improvement.\nExperience working in AWS cloud environments, specifically with big data solutions and serverless architectures\nAbility to mentor and guide junior engineers, fostering a culture of learning and innovation\nStrong communication skills to articulate technical concepts clearly to non-technical stakeholders.\nWHAT WE OFFER\nA young, dynamic, and innovation-oriented environment\nA wide variety of projects within different industries\nA very open and informal culture where knowledge sharing, and employee development are key.\nRoom for personal initiative, development, and growth\nRealistic career opportunities\nCompetitive package and fringe benefits.\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Golang', 'Snowflake', 'Javascript', 'ETL', 'AWS']",2025-06-12 14:25:03
Data Modelling Expert (with Avaloq experience),Luxoft,5 - 10 years,Not Disclosed,['Bengaluru'],"Design, implement, and maintain conceptual, logical, and physical data models within the Avaloq Core Banking system.\nCollaborate with business analysts, product owners, and Avaloq parameterisation teams to translate business requirements into robust data models.\nEnsure alignment of data models with Avaloqs object model and industry best practices.\nPerform data profiling, quality checks, and lineage tracing to support regulatory and compliance requirements (e.g., Basel III, MiFID II, ESG).\nSupport integration of Avaloq data with downstream systems (e.g., CRM, data warehouses, reporting platforms).\nProvide expert input on data governance, metadata management, and model documentation.\nContribute to change requests, upgrades, and data migration projects involving Avaloq.\nCollaborate with cross-functional teams to drive data consistency, reusability, and scalability.\nReview and validate existing data models, identify gaps or optimisation opportunities.\nEnsure data models meet performance, security, and privacy requirements.\nSkills\nMust have\nProven experience (5+ years) in data modelling or data architecture, preferably within financial services.\n3+ years of hands-on experience with Avaloq Core Banking Platform, especially its data structures and object model.\nStrong understanding of relational databases and data modelling tools (e.g., ER/Studio, ERwin, or similar).\nProficient in SQL and data manipulation in Avaloq environments.\nKnowledge of banking products, client lifecycle data, and regulatory data requirements.\nFamiliarity with data governance, data quality, and master data management concepts.\nExperience working in Agile or hybrid project delivery environments.\nNice to have\nExposure to Avaloq Scripting or parameterisation is a strong plus.\nExperience integrating Avaloq with data lakes, BI/reporting tools, or regulatory platforms.\nUnderstanding of data privacy regulations (GDPR, FINMA, etc.).\nCertification in Avaloq or relevant financial data management domains is advantageous.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nAvaloq Software Engineer (Data)\nAvaloq\nIndia\nRemote India\nAvaloq Technical Lead\nAvaloq\nAustralia\nSydney\nSenior Avaloq Engineer\nAvaloq\nAustralia\nSydney\nBengaluru, India\nReq. VR-114445\nAvaloq\nBCM Industry\n21/05/2025\nReq. VR-114445\nApply for Data Modelling Expert (with Avaloq experience) in Bengaluru\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data migration', 'metadata', 'Data management', 'Agile', 'Data structures', 'Data quality', 'Financial services', 'SQL', 'CRM', 'Core banking']",2025-06-12 14:25:05
Data Modelling Expert (with Avaloq experience),Luxoft,5 - 10 years,Not Disclosed,[],"Design, implement, and maintain conceptual, logical, and physical data models within the Avaloq Core Banking system.\nCollaborate with business analysts, product owners, and Avaloq parameterisation teams to translate business requirements into robust data models.\nEnsure alignment of data models with Avaloqs object model and industry best practices.\nPerform data profiling, quality checks, and lineage tracing to support regulatory and compliance requirements (e.g., Basel III, MiFID II, ESG).\nSupport integration of Avaloq data with downstream systems (e.g., CRM, data warehouses, reporting platforms).\nProvide expert input on data governance, metadata management, and model documentation.\nContribute to change requests, upgrades, and data migration projects involving Avaloq.\nCollaborate with cross-functional teams to drive data consistency, reusability, and scalability.\nReview and validate existing data models, identify gaps or optimisation opportunities.\nEnsure data models meet performance, security, and privacy requirements.\nSkills\nMust have\nProven experience (5+ years) in data modelling or data architecture, preferably within financial services.\n3+ years of hands-on experience with Avaloq Core Banking Platform, especially its data structures and object model.\nStrong understanding of relational databases and data modelling tools (e.g., ER/Studio, ERwin, or similar).\nProficient in SQL and data manipulation in Avaloq environments.\nKnowledge of banking products, client lifecycle data, and regulatory data requirements.\nFamiliarity with data governance, data quality, and master data management concepts.\nExperience working in Agile or hybrid project delivery environments.\nNice to have\nExposure to Avaloq Scripting or parameterisation is a strong plus.\nExperience integrating Avaloq with data lakes, BI/reporting tools, or regulatory platforms.\nUnderstanding of data privacy regulations (GDPR, FINMA, etc.).\nCertification in Avaloq or relevant financial data management domains is advantageous.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nAvaloq Technical Lead\nAvaloq\nAustralia\nSydney\nSenior Avaloq Engineer\nAvaloq\nAustralia\nSydney\nAvaloq Engineer\nAvaloq\nIndia\nBengaluru\nRemote India, India\nReq. VR-114445\nAvaloq\nBCM Industry\n21/05/2025\nReq. VR-114445\nApply for Data Modelling Expert (with Avaloq experience) in Remote India\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data migration', 'metadata', 'Data management', 'Agile', 'Data structures', 'Data quality', 'Financial services', 'SQL', 'CRM', 'Core banking']",2025-06-12 14:25:08
Azure Data Bricks (4-15 Yrs) - Bangalore,Happiest Minds Technologies,4 - 9 years,Not Disclosed,['Bengaluru'],"Hi,\n\nGreetings from Happiest Minds Technologies\n\nCurrently we are hiring for below positions and looking for immediate joiners.\n1. Azure Databricks Bangalore 5 to 10 Yrs - Bangalore\nAs a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools. Proficiency in Python PySpark, Pandas, NumPy, SciPy, Spark SQL, DataFrames, RDDs, Delta Lake, Databricks Notebooks, and MLflow is required, along with hands-on experience in Azure Data Lake, Blob Storage, and Synapse Analytics.",,,,"['Pyspark', 'Azure', 'Data Bricks', 'sql', 'ETL']",2025-06-12 14:25:10
Senior Data Engineer : 7+ Years,Jayam Solutions Pvt Ltd - CMMI Level III Company,5 - 9 years,Not Disclosed,['Hyderabad( Madhapur )'],"Job Description:\nPosition: Sr.Data Engineer\nExperience: Minimum 7 years\nLocation: Hyderabad\nJob Summary:\n\nWhat Youll Do\n\nDesign and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.\nParticipate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.\nIndependently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.\nMonitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.\nMaintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.\nAcquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.\nEngage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.\nDrive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.\n\nWhat Youll Bring\n\nBachelors degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.\n7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.\nExtensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.\nCloud - AWS\nApache Flink/Spark , Redis\nDatabase platform- Databricks.\nProficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.\nExpertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.\nExperience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).\nAbility to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.\nStrong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.\nExceptional attention to detail and professionalism in report and dashboard creation.\nExcellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.\nOutstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.\n\nPlease feel free to contact us: 9440806850\nEmail ID : careers@jayamsolutions.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Flink', 'Redis', 'Spark', 'Python', 'SCALA', 'Ci/Cd', 'Devops', 'AWS']",2025-06-12 14:25:12
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Maven', 'SparkSQL Apache Spark', 'PySpark', 'Subversion', 'OLAP', 'Scaled Agile methodologies', 'SQL', 'Scaled Agile Framework', 'Jenkins', 'NOSQL database', 'Git', 'Databricks', 'Data Fabric', 'Data Mesh', 'AWS', 'Python']",2025-06-12 14:25:14
Data Engineer,XL India Business Services Pvt. Ltd,1 - 7 years,Not Disclosed,['Gurugram'],"Senior Engineer, Data Modeling Gurgaon/Bangalore, India AXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained industrious advantage\n\nOur Chief Data Office also known as our Innovation, Data Intelligence & Analytics team (IDA) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking a Data Engineer\n\nThe role will support the team s efforts towards creating, enhancing, and stabilizing the Enterprise data lake through the development of the data pipelines\n\nThis role requires a person who is a team player and can work well with team members from other disciplines to deliver data in an efficient and strategic manner\n\nWhat you ll be doing What will your essential responsibilities include? Act as a data engineering expert and partner to Global Technology and data consumers in controlling complexity and cost of the data platform, whilst enabling performance, governance, and maintainability of the estate\n\nUnderstand current and future data consumption patterns, architecture (granular level), partner with Architects to make sure optimal design of data layers\n\nApply best practices in Data architecture\n\nFor example, balance between materialization and virtualization, optimal level of de-normalization, caching and partitioning strategies, choice of storage and querying technology, performance tuning\n\nLeading and hands-on execution of research into new technologies\n\nFormulating frameworks for assessment of new technology vs business benefit, implications for data consumers\n\nAct as a best practice expert, blueprint creator of ways of working such as testing, logging, CI/CD, observability, release, enabling rapid growth in data inventory and utilization of Data Science Platform\n\nDesign prototypes and work in a fast-paced iterative solution delivery model\n\nDesign, Develop and maintain ETL pipelines using Py spark in Azure Databricks using delta tables\n\nUse Harness for deployment pipeline\n\nMonitor Performance of ETL Jobs, resolve any issue that arose and improve the performance metrics as needed\n\nDiagnose system performance issue related to data processing and implement solution to address them\n\nCollaborate with other teams to make sure successful integration of data pipelines into larger system architecture requirement\n\nMaintain integrity and quality across all pipelines and environments\n\nUnderstand and follow secure coding practice to make sure code is not vulnerable\n\nYou will report to the Application Manager\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Effective Communication skills\n\nBachelor s degree in computer science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience\n\nRelevant years of extensive work experience in various data engineering & modeling techniques (relational, data warehouse, semi-structured, etc), application development, advanced data querying skills\n\nRelevant years of programming experience using Databricks\n\nRelevant years of experience using Microsoft Azure suite of products (ADF, synapse and ADLS)\n\nSolid knowledge on network and firewall concepts\n\nSolid experience writing, optimizing and analyzing SQL\n\nRelevant years of experience with Python\n\nAbility to break complex data requirements and architect solutions into achievable targets\n\nRobust familiarity with Software Development Life Cycle (SDLC) processes and workflow, especially Agile\n\nExperience using Harness\n\nTechnical lead responsible for both individual and team deliveries\n\nDesired Skills and Abilities: Worked in big data migration projects\n\nWorked on performance tuning both at database and big data platforms\n\nAbility to interpret complex data requirements and architect solutions\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExcellent basics on parquet files and delta files\n\nEffective Knowledge of Azure cloud computing platform\n\nFamiliarity with Reporting software - Power BI is a plus\n\nFamiliarity with DBT is a plus\n\nPassion for data and experience working within a data-driven organization\n\nYou care about what you do, and what we do\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides dynamic compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Coding', 'Agile', 'Workflow', 'Application development', 'SDLC', 'SQL', 'Python', 'Firewall']",2025-06-12 14:25:17
"Data Engineer, AVP",NatWest Markets,16 - 18 years,Not Disclosed,['Gurugram'],"Join us as a Data Engineer\nWe re looking for someone to build effortless, digital first customer experiences to help simplify our organisation and keep our data safe and secure\nDay-to-day, you ll develop innovative, data-driven solutions through data pipelines, modelling and ETL design while inspiring to be commercially successful through insights\nIf you re ready for a new challenge, and want to bring a competitive edge to your career profile by delivering streaming data ingestions, this could be the role for you\nWere offering this role at assistant vice president level\nWhat you ll do\nYour daily responsibilities will include you developing a comprehensive knowledge of our data structures and metrics, advocating for change when needed for product development. You ll also provide transformation solutions and carry out complex data extractions.\nWe ll expect you to develop a clear understanding of data platform cost levels to build cost-effective and strategic solutions. You ll also source new data by using the most appropriate tooling before integrating it into the overall solution to deliver it to our customers.\nYou ll also be responsible for:\nDriving customer value by understanding complex business problems and requirements to correctly apply the most appropriate and reusable tools to build data solutions\nParticipating in the data engineering community to deliver opportunities to support our strategic direction\nCarrying out complex data engineering tasks to build a scalable data architecture and the transformation of data to make it usable to analysts and data scientists\nBuilding advanced automation of data engineering pipelines through the removal of manual stages\nLeading on the planning and design of complex products and providing guidance to colleagues and the wider team when required\nThe skills you ll need\nTo be successful in this role, you ll have an understanding of data usage and dependencies with wider teams and the end customer. You ll also have experience of extracting value and features from large scale data.\nWe ll expect you to have experience of ETL technical design, data quality testing, cleansing and monitoring, data sourcing, exploration and analysis, and data warehousing and data modelling capabilities.\nYou ll also need:\nExperience of using programming languages alongside knowledge of data and software engineering fundamentals\nGood knowledge of modern code development practices\nGreat communication skills with the ability to proactively engage with a range of stakeholders\nHours\n45\nJob Posting Closing Date:\n16/06/2025",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Usage', 'Technical design', 'Programming', 'Data structures', 'Data quality', 'Assistant Vice President', 'Data warehousing', 'Monitoring', 'Data architecture']",2025-06-12 14:25:19
"Director, Enterprise Data Architecture",Horizon Therapeutics,10 - 12 years,Not Disclosed,['Hyderabad'],"Career Category Engineering Job Description\nABOUT AMGEN\nAmgen harnesses the best of biology and technology to fight the world s toughest diseases, and make people s lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what s known today.\nABOUT THE ROLE\nRole Description:\nThe Director for Data Architecture and Solutions will lead Amgen s enterprise data architecture and solutions strategy, overseeing the design, integration, and deployment of scalable, secure, and future-ready data systems. This leader will define the architectural vision and guide a high-performing team of architects and technical experts to implement data and analytics solutions that drive business value and innovation.\nThis role demands a strong blend of business acumen, deep technical expertise, and strategic thinking to align data capabilities with the companys mission and growth. The Director will also serve as a key liaison with executive leadership, influencing technology investment and enterprise data direction\n.\nRoles & Responsibilities:\nDevelop and own the enterprise data architecture and solutions roadmap, aligned with Amgen s business strategy and digital transformation goals.\nProvide executive leadership and oversight of data architecture initiatives across business domains (R&D, Commercial, Manufacturing, etc.).\nLead and grow a high-impact team of data and solution architects. Coach, mentor, and foster innovation and continuous improvement in the team.\nDesign and promote modern data architectures (data mesh, data fabric, lakehouse etc.) across hybrid cloud environments and enable for AI readiness.\nCollaborate with stakeholders to define solution blueprints, integrating business requirements with technical strategy to drive value.\nDrive enterprise-wide adoption of data modeling, metadata management, and data lineage standards.\nEnsure solutions meet enterprise-grade requirements for security, performance, scalability, compliance, and data governance.\nPartner closely with Data Engineering, Analytics, AI/ML, and IT Security teams to operationalize data solutions that enable advanced analytics and decision-making.\nChampion innovation and continuous evolution of Amgen s data and analytics landscape through new technologies and industry best practices.\nCommunicate architectural strategy and project outcomes to executive leadership and other non-technical stakeholders.\nFunctional Skills:\nMust-Have Skills:\n10+ years of experience in data architecture or solution architecture leadership roles, including experience at the enterprise level.\nProven experience leading architecture strategy and delivery in the life sciences or pharmaceutical industry.\nExpertise in cloud platforms (AWS, Azure, or GCP) and modern data technologies (data lakes, APIs, ETL/ELT frameworks).\nStrong understanding of data governance, compliance (e.g., HIPAA, GxP), and data privacy best practices.\nDemonstrated success managing cross-functional, global teams and large-scale data programs.\nExperience with enterprise architecture frameworks (TOGAF, Zachman, etc.).\nProven leadership skills with a track record of managing and mentoring high-performing data architecture teams.\nGood-to-Have Skills:\nMaster s or doctorate in Computer Science, Engineering, or related field.\nCertifications in cloud architecture (AWS, GCP, Azure).\nExperience integrating AI/ML solutions into enterprise Data Achitecture.\nFamiliarity with DevOps, CI/CD pipelines, and Infrastructure as Code (Terraform, CloudFormation).\nScaled Agile or similar methodology experience.\nLeadership and Communication Skills:\nStrategic thinker with the ability to influence at the executive level.\nStrong executive presence with excellent communication and storytelling skills.\nAbility to lead in a matrixed, global environment with multiple stakeholders.\nHighly collaborative, proactive, and business-oriented mindset.\nStrong organizational and prioritization skills to manage complex initiatives.\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nBasic Qualifications:\nDoctorate degree and 2 years of Information Systems experience, or\nMaster s degree and 6 years of Information Systems experience, or\nBachelor s degree and 8 years of Information Systems experience, or\nAssociates degree and 10 years of Information Systems experience, or\n4 years of managerial experience directly managing people and leadership experience leading teams, projects, or programs.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'metadata', 'Data modeling', 'Enterprise architecture', 'TOGAF', 'HIPAA', 'Agile', 'Analytics', 'Data architecture']",2025-06-12 14:25:22
Data Engineering Lead,Yotta Techports,10 - 15 years,30-35 Lacs P.A.,['Hyderabad'],"Responsibilities:\nLead and manage an offshore team of data engineers, providing strategic guidance, mentorship, and support to ensure the successful delivery of projects and the development of team members.\nCollaborate closely with onshore stakeholders to understand project requirements, allocate resources efficiently, and ensure alignment with client expectations and project timelines.\nDrive the technical design, implementation, and optimization of data pipelines, ETL processes, and data warehouses, ensuring scalability, performance, and reliability.\nDefine and enforce engineering best practices, coding standards, and data quality standards to maintain high-quality deliverables and mitigate project risks.\nStay abreast of emerging technologies and industry trends in data engineering, and provide recommendations for tooling, process improvements, and skill development.\nAssume a data architect role as needed, leading the design and implementation of data architecture solutions, data modeling, and optimization strategies.\nDemonstrate proficiency in AWS services such as:\nExpertise in cloud data services, including AWS services like Amazon Redshift, Amazon EMR, and AWS Glue, to design and implement scalable data solutions.\nExperience with cloud infrastructure services such as AWS EC2, AWS S3, to optimize data processing and storage.\nKnowledge of cloud security best practices, IAM roles, and encryption mechanisms to ensure data privacy and compliance.\nProficiency in managing or implementing cloud data warehouse solutions, including data modeling, schema design, performance tuning, and optimization techniques.\nDemonstrate proficiency in modern data platforms such as Snowflake and Databricks, including:\nDeep understanding of Snowflake's architecture, capabilities, and best practices for designing and implementing data warehouse solutions.\nHands-on experience with Databricks for data engineering, data processing, and machine learning tasks, leveraging Spark clusters for scalable data processing.\nAbility to optimize Snowflake and Databricks configurations for performance, scalability, and cost-effectiveness.\nManage the offshore team's performance, including resource allocation, performance evaluations, and professional development, to maximize team productivity and morale.\n\nQualifications:\nBachelor's degree in Computer Science, Engineering, or a related field; advanced degree preferred.\n10+ years of experience in data engineering, with a proven track record of leadership and technical expertise in managing complex data projects.\nProficiency in programming languages such as Python, Java, or Scala, as well as expertise in SQL and relational databases (e.g., PostgreSQL, MySQL).\nStrong understanding of distributed computing, cloud technologies (e.g., AWS), and big data frameworks (e.g., Hadoop, Spark).\nExperience with data architecture design, data modeling, and optimization techniques.\nExcellent communication, collaboration, and leadership skills, with the ability to effectively manage remote teams and engage with onshore stakeholders.\nProven ability to adapt to evolving project requirements and effectively prioritize tasks in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team Handling', 'Snowflake', 'Data Services', 'Cloud Infrastructure', 'Data Bricks']",2025-06-12 14:25:24
Full Stack Data Scientist,Vimo Getinsured,2 - 7 years,Not Disclosed,['Gurugram( Sector 61 Gurgaon )'],"About the Role\nAs a Data Science Engineer, you will need strong technical skills in data modeling, machine learning, data engineering, and software development. You will have the ability to conduct literature reviews and critically evaluate research papers to identify applicable techniques. Additionally, you should be able to design and implement efficient and scalable data processing pipelines, perform exploratory data analysis, and collaborate with other teams to integrate data science models into production systems. Passion for conversational AI and a desire to solve some of the most complex problems in the Natural Language Processing space are essential. You will work on highly scalable, stable, and automated deployments, aiming for high performance. Taking on the challenge of building and scaling a truly remarkable AI platform to impact the lives of millions of customers will be part of your responsibilities. Working in a challenging yet enjoyable environment, where learning new things is the norm, you should think of solutions beyond boundaries. You should also drive outcomes with full ownership, deeply believe in customer obsession, and thrive in a fast-paced environment of learning and innovation.\nYou will work in a challenging, consumer-facing problem space, where you can make an immediate impact. You will get to work with the latest technologies, learn to use new tools and get the opportunity to have your say in the final product. Youll work alongside a great team in an open, collaborative environment. We are part of Vimo, a well-funded, stable mid-size company with excellent salaries, medical/dental/vision coverage, and perks. Vimo is an Equal Opportunity Employer.",,,,"['python', 'Langchain', 'Neural Networks', 'LLM', 'Linux', 'Data Structures', 'Natural Language Processing', 'Jupyter Notebook', 'Machine Learning', 'Deep Learning', 'Numpy', 'Data Science', 'pandas', 'Nltk', 'Langgraph', 'Transformers', 'BERT', 'langsmith']",2025-06-12 14:25:27
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nAs part of the cybersecurity organization, the Data Engineer is responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nCreate data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, Gitlab, LucidChart,etc.\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data engineering', 'data security', 'Agile', 'cloud data platforms', 'Databricks', 'data governance frameworks', 'ETL', 'AWS', 'SQL', 'Python']",2025-06-12 14:25:29
Data Engineer,Konrad Group,3 - 7 years,15-30 Lacs P.A.,['Gurugram( Sector 42 Gurgaon )'],"Who We Are\n\nKonrad is a next generation digital consultancy. We are dedicated to solving complex business problems for our global clients with creative and forward-thinking solutions. Our employees enjoy a culture built on innovation and a commitment to creating best-in-class digital products in use by hundreds of millions of consumers around the world. We hire exceptionally smart, analytical, and hard working people who are lifelong learners.\nAbout The Role\nAs a Data Engineer youll be tasked with designing, building, and maintaining scalable data platforms and pipelines. Your deep knowledge of data platforms such as Azure Fabric, Databricks, and Snowflake will be essential as you collaborate closely with data analysts, scientists, and other engineers to ensure reliable, secure, and efficient data solutions.\n\nWhat Youll Do\n\nDesign, build, and manage robust data pipelines and data architectures.\nImplement solutions leveraging platforms such as Azure Fabric, Databricks, and Snowflake.\nOptimize data workflows, ensuring reliability, scalability, and performance.\nCollaborate with internal stakeholders to understand data needs and deliver tailored solutions.\nEnsure data security and compliance with industry standards and best practices.\nPerform data modelling, data extraction, transformation, and loading (ETL/ELT).\nIdentify and recommend innovative solutions to enhance data quality and analytics capabilities.\n\nQualifications\n\nBachelors degree or higher in Computer Science, Data Engineering, Information Technology, or a related field.\nAt least 3 years of professional experience as a Data Engineer or similar role.\nProficiency in data platforms such as Azure Fabric, Databricks, and Snowflake.\nHands-on experience with data pipeline tools, cloud services, and storage solutions.\nStrong programming skills in SQL, Python, or related languages.\nExperience with big data technologies and concepts (Spark, Hadoop, Kafka).\nExcellent analytical, troubleshooting, and problem-solving skills.\nAbility to effectively communicate technical concepts clearly to non-technical stakeholders.\nAdvanced English\n\nNice to have\n\nCertifications related to Azure Data Engineering, Databricks, or Snowflake.\nFamiliarity with DevOps practices and CI/CD pipelines.\n\nPerks and Benefits\n\nComprehensive Health & Wellness Benefits Package \nSocials, Outings & Retreats\nCulture of Learning & Development\nFlexible Working Hours\nWork from Home Flexibility\nService Recognition Programs\n\nKonrad is committed to maintaining a diverse work environment and is proud to be an equal opportunity employer. All qualified applicants, regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status will receive consideration for employment. If you have any accessibility requirements or concerns regarding the hiring process or employment with us, please notify us so we can provide suitable accommodation.\nWhile we sincerely appreciate all applications, only those candidates selected for an interview will be contacted.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Azure Data Factory', 'Azure Databricks', 'Spark', 'Fabric', 'Python']",2025-06-12 14:25:32
Lead Data Engineer (Immediate joiner),Decision Point,4 - 9 years,15-30 Lacs P.A.,"['Gurugram', 'Chennai']","Role & responsibilities\nâ€¢ Assume ownership of Data Engineering projects from inception to completion.\nImplement fully operational Unified Data Platform solutions in production environments using technologies like Databricks, Snowflake, Azure Synapse etc.\nShowcase proficiency in Data Modelling and Data Architecture\nUtilize modern data transformation tools such as DBT (Data Build Tool) to streamline and automate data pipelines (nice to have).",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Synapse', 'Python', 'Etl Pipelines', 'Airflow', 'Bigquery', 'Advance Sql', 'Azure Cloud', 'GCP', 'Data Modeling', 'Data Architecture', 'AWS']",2025-06-12 14:25:34
Data Engineer,Xenonstack,2 - 5 years,Not Disclosed,['Mohali( Phase 8B Mohali )'],"At XenonStack, We committed to become the Most Value Driven Cloud Native, Platform Engineering and Decision Driven Analytics Company. Our Consulting Services and Solutions towards the Neural Company and its Key Drivers.\nXenonStacks DataOps team is looking for a Data Engineer who will be responsible for employing techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field.\nYou should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. The ideal candidate should be highly skilled in all aspects of Python, Java/Scala, SQL and analytical skills.\nJob Responsibilities:\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with co-workers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analysed quickly by BI & AI Teams.\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant co-workers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nRequirements:\nTechnical Requirements\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch, Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores.\nProfessional Attributes:\nExcellent communication skills & Attention to detail.\nAnalytical mind and problem-solving Aptitude with Strong Organizational skills & Visual Thinking.\nBenefits:\nDiscover the benefits of joining our team:\nDynamic and purposeful work culture in a people-oriented organization contributing to multi-million-dollar projects with guaranteed job security.\nOpen, authentic, and transparent communication fostering a warm work environment.\nRegular constructive feedback and exposure to diverse technologies.\nRecognition and rewards for exceptional performance achievements.\nAccess to certification courses & Skill Sessions to develop continually and refine your skills.\nAdditional allowances for team members assigned to specific projects.\nSpecial skill allowances to acknowledge and compensate for unique expertise.\nComprehensive medical insurance policy for your health and well-being.\nTo Learn more about the company -\nWebsite - http://www.xenonstack.com/",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Spark', 'ETL', 'Python', 'SQL', 'Java', 'Data Processing', 'Machine Learning']",2025-06-12 14:25:36
Technical Consultant(ETL + SQL + Data Migration),Insightsoftware,4 - 7 years,Not Disclosed,['Hyderabad'],"Insightsoftware (ISW) is a growing, dynamic computer software company that helps businesses achieve greater levels of financial intelligence across their organization with our world-class financial reporting solutions. At insightsoftware, you will learn and grow in a fast-paced, supportive environment that will take your career to the next level. The Data Conversion Specialist is a member of the insightsoftware Project Management Office (PMO) who demonstrates teamwork, results orientation, a growth mindset, disciplined execution, and a winning attitude.\nLocation: Hyderabad (Work from Office)\nWorking Hours: 5:00 PM - 2:00AM IST or 6:00 PM to 3:00 AM IS T, should be ok to work in night shift as per requirement.\nPosition Summary\nThe Consultant will integrate and map customer data from client source system(s) to our industry-leading platform. The role will include, but is not limited to:\nUsing strong technical data migration, scripting, and organizational skills to ensure the client data is converted efficiently and accurately to the insightsoftware (ISW) platform.\nPerforming extract, transform, load (ETL) activities to ensure accurate and timely data conversions.\nProviding in-depth research and analysis of complex scenarios to develop innovative solutions to meet customer needs whilst remaining within project governance.\nMapping and maintaining business requirements to the solution design using tools such as requirements traceability matrices (RTM).\nPresenting findings, requirements, and problem statements for ratification by stakeholders and working groups.\nIdentifying and documenting data gaps to allow change impact and downstream impact analysis to be conducted.\nExperience assessing data and analytic requirements to establish mapping rules from source to target systems to meet business objectives.\nExperience with real-time, batch, and ETL for complex data conversions.\nWorking knowledge of extract, transform, load (ETL) methodologies and tools such as Talend, Dell Boomi, etc.\nUtilize data mapping tools to prepare data for data loads based on target system specifications.\nWorking experience using various data applications/systems such as Oracle SQL, Excel, .csv files, etc.\nStrong SQL scripting experience.\nCommunicate with clients and/or ISW Project Manager to scope, develop, test, and implement conversion/integration\nEffectively communicate with ISW Project Managers and customers to keep project on target\nContinually drive improvements in the data migration process.\nCollaborate via phone and email with clients and/or ISW Project Manager throughout the conversion/integration process.\nDemonstrated collaboration and problem-solving skills.\nWorking knowledge of software development lifecycle (SDLC) methodologies including, but not limited to: Agile, Waterfall, and others.\nClear understanding of cloud and application integrations.\nAbility to work independently, prioritize tasks, and manage multiple tasks simultaneously.\nEnsure client s data is converted/integrated accurately and within deadlines established by ISW Project Manager.\nExperience in customer SIT, UAT, migration and go live support.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data migration', 'Data conversion', 'Financial reporting', 'Project management', 'project governance', 'Agile', 'Software development life cycle', 'data mapping', 'SDLC', 'Downstream']",2025-06-12 14:25:38
Project Manager - Data Migration,TALWORX,9 - 14 years,22.5-35 Lacs P.A.,['Mumbai (All Areas)'],"Role & responsibilities\nDefine project scope, objectives and deliverables in collaboration with IT leads and business sponsors\nBuild & manage detailed migration plan. Coordinate with the internal client PMO function for project onboarding, configuration and tracking on approved PM tools\nCoordinate with cross functional teams & vendors\nOversee data mapping, transformation, validation and testing activities\nManage vendor coordination for tools & services supporting the migration\nEnsure compliance with data governance, security policies and regulatory requirements\nIdentify & manage migration related risks including data loss, downtime & performance issues\nEnsure adherence to QA/UAT protocols and change management process\nSupport audit readiness and documentation\nDefine, Setup and run governance forums\nCommunicate progress, risks, decisions to executive leadership and stakeholders\nDrive validation, optimization and improvement opportunities post migration to enhance data performance and usability\nMentor and guide technical teams throughout the migration journey\nLead and drive change management & knowledge transfer\n\nPreferred candidate profile\n8+ years of experience\nProven expertise delivering atleast one enterprise level end to end data warehouse management program\nSolid understanding of relational database systems, data structures and SQL\nStrong understanding of data modelling, ETL tools/processes, performance tuning and migration planning best practices\nExperience with cloud based database platforms is a plus\nStrong project management skills\nStrong knowledge of SDLC and project management methodologies (Agile, waterfall, Hybrid)\n\nPreferred Skills:\nBFSI or Life Sciences industry exposure\nExperience with data reconciliation and validation frameworks\nStrong interpersonal and communication skills\nAbility to lead cross functional, geographically distributed teams\nFamiliarity with Oracle DMS or Sybase",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['data warehouse', 'Data Migration', 'Oracle', 'SyBase', 'SQL', 'Waterfall', 'Database Management', 'Agile', 'oracle DMS', 'SDLC']",2025-06-12 14:25:41
Assoc. Data Engineer - R&D Precision Medicine Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nThe R&D Precision Medicine team is responsible for Data Standardization, Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with access to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These data include clinical data, omics, and images. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\n\nThe Data Engineer will be responsible for full stack development of enterprise analytics and data mastering solutions leveraging Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that support research cohort-building and advanced AI pipelines. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\n\nYou will collaborate closely with partners, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a solid background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\n\nRoles & Responsibilities\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data management tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with partners to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is someone with these qualifications.\n\nBasic Qualifications:\nMasters degree with 1 to 3 years of experience in Data Engineering OR\nBachelors degree with 1 to 3 years of experience in Data Engineering\nMust-Have\n\nSkills:\nMinimum of 1 year of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 1 year of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nExperience using cloud platforms (AWS), data lakes, and data warehouses.\nWorking knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling and data anlysis\nGood-to-Have\n\nSkills:\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft CertifiedData Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data lakes', 'data pipelines', 'ETL processes', 'AWS', 'data warehouses', 'BI solutions']",2025-06-12 14:25:43
OAC ODI Architect (Senior Oracle Analytics Consultant),Mastek,10 - 15 years,15-30 Lacs P.A.,"['Ahmedabad', 'Chennai']","We are looking for OAC ODI Architect to be based in Ahemdabad or Chennai\nMinimum Architect exp 4 Yrs\nOracle Analytics Consultant (OAC, ODI, FDI) Tech Architect\nLocation: [Specify Location or Remote] Chennai Ahmedabad\nExpected DOJ June\nEmployment Type: Full-time\nExperience Level: 10 - 15+ Years\nJob Summary:\nWe are seeking an experienced and results-driven Senior Oracle Analytics Consultant with over 10 years of hands-on experience in Oracle Analytics Cloud (OAC), Oracle Data Integrator (ODI), and Fusion Data Intelligence (FDI). The ideal candidate will have a deep understanding of enterprise data architecture, data integration best practices, and cloud-based analytics solutions. This role involves working closely with cross-functional teams to design, implement, and support advanced analytics and data integration solutions that drive business value.\nKey Responsibilities:\nLead the design, development, and deployment of analytics solutions using Oracle Analytics Cloud (OAC).\nArchitect and implement data integration pipelines using Oracle Data Integrator (ODI) for on-prem and cloud data sources.\nCollaborate with business and IT stakeholders to design and deploy Fusion Data Intelligence (FDI) based dashboards and KPIs.\nOptimize performance of OAC dashboards and reports, including data modeling and visualization best practices.\nDevelop and manage data models, RPDs, and semantic layers within OAC.\nBuild and maintain ETL mappings, packages, and workflows in ODI.\nIntegrate Oracle Fusion Applications with OAC and FDI for near-real-time reporting.\nDrive data governance and quality initiatives across analytics platforms.\nTroubleshoot technical issues and provide solutions in a timely manner.\nMentor junior developers and provide technical leadership on complex projects.\nQualifications:\nBachelors or Masters degree in Computer Science, Information Systems, or related field.\n10+ years of relevant experience with strong focus on:\nOracle Analytics Cloud (OAC) - Must\nOracle Data Integrator (ODI) - Must\nFusion Data Intelligence (FDI) Good to Have\nExpertise in Oracle Fusion ERP/HCM data models and subject areas.\nExperience integrating multiple data sources, including on-premise and cloud systems.\nStrong understanding of SQL, PL/SQL, and performance tuning.\nFamiliarity with data lake architecture, data warehousing, and ELT/ETL design patterns.\nProven experience working in Agile/DevOps environments.\nExcellent communication, analytical thinking, and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oac', 'ODI', 'Odi Architecture', 'FDI']",2025-06-12 14:25:45
Software Data Operations Engineer (BS+2),MAQ Software,2 - 5 years,Not Disclosed,['Noida'],"MAQ LLC d.b.a MAQ Software hasmultiple openings at Redmond, WA for:\nSoftware Data Operations Engineer (BS+2)\n\nResponsible for gathering & analyzing business requirements from customers. Implement,test and integrate software applications for use by customers. Develop &review cost effective data architecture to ensure appropriateness with currentindustry advances in data management, cloud & user experience. Automateuser test scenarios, debug & fix errors in cloud-based data infrastructure,reporting applications to meet customer needs. Must be able to traveltemporarily to client sites and or relocate throughout the United States.\n\nRequirements:Bachelors Degree or foreign equivalent in Computer Science, ComputerApplications, Computer Information Systems, Information Technology or relatedfield with two years of work experience in job offered, software engineer, systemsanalyst or related job.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Data Operations', 'software applications', 'data management', 'Data Operations', 'data architecture', 'data infrastructure']",2025-06-12 14:25:47
Data Engineer,Trantor,5 - 10 years,Not Disclosed,[],"We are looking for a skilled and motivated Data Engineer with deep expertise in GCP,\nBigQuery, Apache Airflow to join our data platform team. The ideal candidate should have hands-on experience building scalable data pipelines, automating workflows, migrating large-scale datasets, and optimizing distributed systems. The candidate should have experience with building Web APIs using Python. This role will play a key part in designing and maintaining robust data engineering solutions across cloud and on-prem environments.\nKey Responsibilities\nBigQuery & Cloud Data Pipelines:\nDesign and implement scalable ETL pipelines for ingesting large-scale datasets.\nBuild solutions for efficient querying of tables in BigQuery.\nAutomated scheduled data ingestion using Google Cloud services and scheduled\nApache Airflow DAGs",,,,"['Airflow', 'Etl Pipelines', 'GCP', 'Bigquery', 'Python', 'SFTP', 'ETL', 'SQL']",2025-06-12 14:25:50
Manager Data Engineer â€“ Research Data and Analytics,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will create and develop data lake solutions for scientific data that drive business decisions for Research. You will build scalable and high-performance data engineering solutions for large scientific datasets and collaborate with Research collaborators. You will also provide technical leadership to junior team members. The ideal candidate possesses experience in the pharmaceutical or biotech industry, demonstrates deep technical skills, is proficient with big data technologies, and has a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nLead, manage, and mentor a high-performing team of data engineers\nDesign, develop, and implement data pipelines, ETL processes, and data integration solutions\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks\nDevelop and maintain data models for biopharma scientific data, data dictionaries, and other documentation to ensure data accuracy and consistency\nOptimize large datasets for query performance\nCollaborate with global multi-functional teams including research scientists to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nCollaborate with Data Architects, Business SMEs, Software Engineers and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve data-related challenges\nAdhere to best practices for coding, testing, and designing reusable code/component\nExplore new tools and technologies that will help to improve ETL platform performance\nParticipate in sprint planning meetings and provide estimations on technical implementation\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nDoctorate Degree OR\nMasters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications:\n3+ years of experience in implementing and supporting biopharma scientific research data analytics (software platforms)\n\n\nFunctional Skills:\nMust-Have Skills:\nProficiency in SQL and Python for data engineering, test automation frameworks (pytest), and scripting tasks\nHands on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing\nExcellent problem-solving skills and the ability to work with large, complex datasets\nAble to engage with business collaborators and mentor team to develop data pipelines and data models\n\n\nGood-to-Have Skills:\nA passion for tackling complex challenges in drug discovery with technology and data\nGood understanding of data modeling, data warehousing, and data integration concepts\nGood experience using RDBMS (e.g. Oracle, MySQL, SQL server, PostgreSQL)\nKnowledge of cloud data platforms (AWS preferred)\nExperience with data visualization tools (e.g. Dash, Plotly, Spotfire)\nExperience with diagramming and collaboration tools such as Miro, Lucidchart or similar tools for process mapping and brainstorming\nExperience writing and maintaining technical documentation in Confluence\nUnderstanding of data governance frameworks, tools, and best practices\n\n\nProfessional Certifications:\nDatabricks Certified Data Engineer Professional preferred\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Spotfire', 'PySpark', 'PostgreSQL', 'Plotly', 'SparkSQL', 'SQL server', 'SQL', 'process mapping', 'Dash', 'MySQL', 'ETL', 'Oracle', 'data governance frameworks', 'Python']",2025-06-12 14:25:52
"Manager, CFO Master Data Management Specialist",Fidelity International,13 - 16 years,Not Disclosed,['Gurugram'],"Job Title : Manager, CFO Master Data Management Specialist\nDept : CFO Data Management\nLocation : Gurgaon\nLevel : 5\nAbout your team\nThe CFO Data Management team is passionate about improving the data culture and maturity of the CFO function, and the FIL enterprise.\nWe work closely with a wide range of Technology and Change teams across the enterprise to embed the right data governance operating models, tools and data architecture into the CFO business that will help us to advance on our data journey.\nWe partner with data role holders across CFO, and the wider enterprise, to understand and catalogue our metadata, and improve data management, quality and accessibility, as key foundations for improving visualisation and AI capabilities.\nWe work with the CFO business to understand data challenges, as well as opportunities, and support them to develop lasting solutions.\nWe own the Finance Ledger master data including the Chart of Accounts, hierarchies and mappings, and partner with the CFO business to manage and maintain these in line with organisational change.\nAbout your role\nFIL has implemented Core Finance Platform (CFP) based upon Oracle Financials and Procurement Cloud ERP (Fusion / R13) - replacing legacy JD Edwards general ledger and Oracle Web Centre based procurement systems.\nAs part of the CFP implementation, the finance ledger Chart of Accounts was reviewed and fundamentally redesigned. The current Chart of Accounts segment values, descriptions and hierarchies are mastered and maintained using Oracle DRM, supported by workflow approval processes using K2. In addition, some of the legacy chart of account values are also required to be maintained, along with supporting mappings.\nThe ongoing support and maintenance of this dataset is critical, not only to Finance processes and reporting - both internal and external - but across the entire FIL enterprise, with many other business areas using these values and hierarchies to drive reporting.\nYou will be working closely with CFO business to understand, validate and action change requests in a structured way, working to clear month end timetables, and considering the impact on related datasets and processes such as cross validations rules, and allocations rules.\nAs part of this role, you will also be required to manage and administer the ARCS Account Attestation system which is a key control and reconciliation tool for the Financial Accounting team.\nYou will be required to build a good understanding of the CFP solution and associated systems to provide support and contribute to ongoing continuous improvement and change initiatives.\nYou will also contribute to other CFO Data Management activities and projects as required.\nKey Responsibilities\nMDM Operations - Maintenance of Chart of Accounts, Allocations Rules, CVRs and Mappings. MDM change activity - Chart of Accounts and mappings project work, process improvements and remediation activity (DRM) Data steward for Master Data (DRM) ARCS month end support & product ownership FIL Life reconciliation support (monthly) Regression testing of Oracle Quarterly Patches. Data Management Operations under CFO data program.\nSkills, Experience and Qualifications Required\nExperience of working in any Oracle ERP preferably Finance module (Oracle Cloud experience preferred, but experience with Oracle eBusiness Suite eBS -R12 acceptable)., knowledge of chart of accounts (CoAs). EPM systems and multi-dimensional reporting. Business Analysis and requirements writing for business process owner to be used by tech partners Finance background required, familiar with financial and management accounting (MA) concepts. Proven analytical skills, demonstrating accuracy and attention to detail. Good Microsoft office skills - PowerPoint, Project, Excel, Word Experience of data presentational tools such as PowerBI Strong presentation skills, both written & verbal Accounting or Management qualification highly desirable\nFeel rewarded\nFor starters, we ll offer you a comprehensive benefits package. We ll value your wellbeing and support your development. And we ll be as flexible as we can about where and when you work - finding a balance that works for all of us. It s all part of our commitment to making you feel motivated by the work you do and happy to be part of our team. For more about our work, our approach to dynamic working and how you could build your future here, .\nFor more about our work, our approach to dynamic working and how you could build your future here,",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Management accounting', 'Procurement', 'Business process', 'Data management', 'Business analysis', 'Reconciliation', 'JD Edwards', 'Workflow', 'Oracle financials', 'Continuous improvement']",2025-06-12 14:25:54
Snowflake Developer with Azure Data Factory,Net Connect,6 - 10 years,6-11 Lacs P.A.,['Hyderabad'],Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\n\nBelow JD for your reference\n\nJob Description:,,,,"['Azure Data Factory', 'Snowflake', 'SQL']",2025-06-12 14:25:56
Data Engineer 4,Comcast,5 - 11 years,Not Disclosed,['Chennai'],".\nResponsible for designing, building and overseeing the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs. Work with data modelers/analysts to understand the business problems they are trying to solve then create or augment data assets to feed their analysis. Integrates knowledge of business and functional priorities. Acts as a key contributor in a complex and crucial environment. May lead teams or projects and shares expertise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBachelors Degree\nWhile possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.\n7-10 Years\nComcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.",,,,"['Engineering services', 'Assurance', 'Process optimization', 'MySQL', 'Machine learning', 'Data structures', 'Data quality', 'Troubleshooting', 'Downstream', 'Python']",2025-06-12 14:25:58
Data Migration Expert - Talend,Maddisoft Solutions,10 - 20 years,Not Disclosed,['Hyderabad'],"Job Title: Data Migration Expert - Talend\nLocation: Hyderabad, India\n\nJob Description:\nMinimum of 9+ years of experience in data migration in Talend projects\nHandsons experience 5+ mandatory knowledge of Talend/SQL tools(Basic knowledge of SAP Routing and SAP Production order tables Individually is an add on)\nProficiency in data migration tools and methodologies, SAP ECC AND S/4HANA Migration Cockpit.\nHands-on experience Data Replication, Data Quality, Data Workbench, Talend or similar ETL tools.\nFamiliarity with Talend (ETL) is plus.\nStrong understanding of data modeling concepts, data mapping techniques, and data transformation rules.\nExcellent SQL skills for data extraction, manipulation, and analysis.\nExperience with SAP all Cross functional modules such as Finance (FI), Controlling (CO), Material Management (MM), Sales and Distribution (SD), or Production Planning (PP).\nStrong analytical, problem-solving, and troubleshooting skills.\nExcellent communication, presentation, and interpersonal skills.\nAbility to work independently and as part of a team in a fast-paced, dynamic environment.\nSAP certification in Data Migration or related field is a plus.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['SAP ECC', 'Data Migration', 'Talend']",2025-06-12 14:26:00
Technical Architect,XL India Business Services Pvt. Ltd,8 - 10 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","The Technical Architect role is the lead technologist for a product or platform\n\nThird party products with high out of box implementation most likely will have their own technical architecture roles, so company Technical Architects mainly manage internal assets\n\nTechnical Architects perform deliverable reviews and manage measurement of deliverable quality within the Delivery Team\n\nWhat you ll be DOING What will your essential responsibilities include? Specification of technologies, application architectures and data structures as a basis for application change for internal assets\n\nProducing quality, secure, scalable, high-performing, and resilient designs for new or improved services\n\nLead the systems analysts, developers, and testers in sympathetic change to the applications\n\nFor internal assets, support Application Managers to develop and maintain the Product Roadmap\n\nDefine and maintain development standards such as system and data design, coding, etc Maintain a capacity plan with historical performance metrics, a future forecast, and a capacity model to ensure services and infrastructure deliver performance and growth targets in a cost effective and proactive manner\n\nManage architecture exceptions for the application, including identifying, documenting, taking through exception approval process, and remediation where and when possible\n\nMonitor application services to ensure performance consistently meets non-functional requirements (response time, security, etc)\n\nWork with the Application Manager & Delivery Lead(s) in defining, analyzing, planning, measuring and improving product availability and continuity\n\nLeads the DevOps team and developers in targeted use of DevOps for their application platform assets\n\nYou will report to the Release Train Engineer\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: A moderate amount of experience with Guidewire Cloud and DevOps practices including CI/CD pipelines, infrastructure as code and containerization\n\nProven experience in designing complex, scalable architectures for large enterprise environments\n\nAbility to break down complex problems, evaluate multiple solutions & choose the most suitable outcome based on tradeoffs\n\nUphold technical integrity of internal assets at the logical and physical level, ensuring designs and changes done based on an outstanding architectural foundation\n\nDeep understanding of enterprise integration patterns (API, middleware and/or data migration)\n\nAbility to work closely with cross-functional teams, including developers, product owners, and operations to ensure alignment on technical goals and priorities\n\nDevelop, own and publish the product technical architecture and design documentation\n\nIdentify technical resources required to deliver the service and maintain plans for the short-, mid-, and long-term to support business cases for technical investments (upgrades, etc)\n\nTranslates the high-level designs into the technical specifications to facilitate efficient and effective development and unit testing\n\nDesired Skills and Abilities: Proficiency with multiple application delivery models including Agile, iterative and waterfall\n\nAdaptable to new/different strategies, programs, technologies, practices, cultures, etc Comfortable with change, able to easily make transitions\n\nAdvanced skills in developing tools, frameworks and processes intended to maximize software quality and minimize time-to-delivery\n\nBachelor s degree in the field of computer science, information systems, business management, or a related field preferred",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data migration', 'Coding', 'Agile', 'Data structures', 'Unit testing', 'software quality', 'Business strategy', 'Middleware', 'Analytics']",2025-06-12 14:26:03
IN Senior Associate AWS DataOps Engineer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary We are looking for a seasoned AWS DataOps Engineer\nResponsibilities\nDesign, implement, and manage scalable data pipelines and ETL processes on AWS. Collaborate with data teams to understand requirements and translate them into robust data solutions. Proven experience with AWS data services such as S3, Redshift, RDS, Glue, and Lambda. Strong understanding of data architecture, data modeling, and data warehousing concepts. Strong programming and scripting skills in languages like Python, SQL, or Shell scripting. Experience with data pipeline and ETL tools, such as Apache Airflow or AWS Data Pipeline. Ensure data quality, integrity, and security through automated testing, monitoring, and alerting systems. Optimize data storage and retrieval using AWS services such as S3, Redshift, RDS, and DynamoDB. Implement data governance and compliance standards to ensure data privacy and security. Automate data integration and deployment processes using tools like AWS Data Pipeline, Glue, and Step Functions. Monitor and troubleshoot data workflows to ensure reliability and performance. Provide technical support and guidance to data teams on best practices for data management and operations.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired\n48 Years\nEducation qualification BE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Master of Engineering, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nDevOps\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data management', 'Data modeling', 'Shell scripting', 'Database administration', 'Agile', 'Apache', 'Technical support', 'SQL', 'Python']",2025-06-12 14:26:05
IN Senior Associate AWS DataOps Engineer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary We are looking for a seasoned AWS DataOps Engineer\nResponsibilities\nDesign, implement, and manage scalable data pipelines and ETL processes on AWS. Collaborate with data teams to understand requirements and translate them into robust data solutions. Proven experience with AWS data services such as S3, Redshift, RDS, Glue, and Lambda. Strong understanding of data architecture, data modeling, and data warehousing concepts. Strong programming and scripting skills in languages like Python, SQL, or Shell scripting. Experience with data pipeline and ETL tools, such as Apache Airflow or AWS Data Pipeline. Ensure data quality, integrity, and security through automated testing, monitoring, and alerting systems. Optimize data storage and retrieval using AWS services such as S3, Redshift, RDS, and DynamoDB. Implement data governance and compliance standards to ensure data privacy and security. Automate data integration and deployment processes using tools like AWS Data Pipeline, Glue, and Step Functions. Monitor and troubleshoot data workflows to ensure reliability and performance. Provide technical support and guidance to data teams on best practices for data management and operations.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired\n48 Years\nEducation qualification BE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Master of Engineering, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nDevOps\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data management', 'Data modeling', 'Shell scripting', 'Database administration', 'Agile', 'Apache', 'Technical support', 'SQL', 'Python']",2025-06-12 14:26:08
Technical Architect,XL India Business Services Pvt. Ltd,5 - 10 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Technical Architect - Salesforce Platform Bangalore/Gurugram, India We are looking for a seasoned Technical Salesforce Platform Architect with expertise in design thinking, DevOps, CI/CD, Azure, API Integration, Data Streaming, and an effective background in commercial insurance\n\nAs a Technical Salesforce Platform Architect, you will lead the design and implementation of scalable solutions on the Salesforce platform while overseeing multiple solutions within a Salesforce org\n\nYou will collaborate closely with cross-functional teams in a scaled agile delivery setup, ensuring alignment with enterprise architecture principles and best practices\n\nWhat you ll be DOING What will your essential responsibilities include? Solution Design and Architecture: Lead the design and architecture of complex solutions on the Salesforce platform, leveraging design thinking methodologies to address business requirements effectively\n\nDefine architectural standards, patterns, and best practices for Salesforce development in alignment with enterprise architecture principles\n\nEnsure solutions are scalable, maintainable, and comply with industry regulations in the commercial insurance domain\n\nTechnical Leadership: Provide technical leadership and guidance to development teams, promoting DevOps practices and CI/CD pipelines for efficient delivery\n\nCollaborate with stakeholders to prioritize features and enhancements, ensuring alignment with business goals and objectives\n\nMentored and coached team members on Salesforce development, DevOps, and enterprise architecture concepts\n\nIntegration and Data Management: Design and implement API integrations with external systems, leveraging Azure services and other technologies as needed\n\nArchitects data streaming solutions for real-time analytics and insights, ensuring data accuracy, integrity, and security\n\nCollaborate with data architects to define data models and ensure proper data governance practices within the Salesforce Org\n\nScaled Agile Delivery: Work within a scaled agile framework, participating in program increment planning, backlog grooming, and sprint ceremonies\n\nFacilitate collaboration between agile teams, resolving dependencies and ensuring alignment with the overall program roadmap\n\nDrive continuous improvement initiatives to enhance delivery processes and optimize team performance\n\nSalesforce Org Oversight: Oversee multiple solutions within the Salesforce org, ensuring cohesiveness, scalability, and maintainability across different business units\n\nConduct regular reviews and audits of Salesforce configurations, identifying opportunities for optimization and enhancement\n\nPartner with Salesforce administrators to establish governance policies, security controls, and release management processes\n\nYou will report to the Platform Lead\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Bachelor s degree in computer science, Information Technology, or a related field\n\nProven experience as a Salesforce Technical Architect in enterprise environments, preferably within the commercial insurance industry\n\nExtensive experience in delivering solutions on Salesforce including Sales Cloud, Service Cloud and Financial Services Cloud\n\nEffective knowledge of Salesforce platform capabilities, including design thinking methodologies, DevOps practices, and CI/CD pipelines\n\nDesired Skills and Abilities: Hands-on experience with Azure services, API integration, and data streaming technologies\n\nFamiliarity with enterprise architecture frameworks such as TOGAF or Zachman\n\nExcellent communication, leadership, and collaboration skills\n\nSalesforce certifications such as Certified Technical Architect (CTA) and Certified Application Architect are a plus\n\nExperience working in scaled agile delivery setups, preferably SAFe\n\nExcellent problem-solving and troubleshooting skills\n\nProven tech leader and mentor, able and willing to share knowledge and experience in order to develop colleagues\n\nEffective communication and collaboration skills",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Enterprise architecture', 'Agile', 'Troubleshooting', 'Information technology', 'Release management', 'Analytics', 'Financial services', 'Salesforce']",2025-06-12 14:26:10
Data Engineer - R&D Data Catalyst Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role, you will be responsible for the end-to-end development of an enterprise analytics and data mastering solution using Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and impactful enterprise solutions that research cohort-building and advanced research pipeline. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be extraordinarily skilled with data analysis and profiling.\nYou will collaborate closely with key customers, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a good background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with key customers to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The R&D Data Catalyst Team is responsible for building Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nBasic Qualifications:\nMasters degree and 1 to 3 years of Data Engineering experience OR\nBachelors degree and 3 to 5 years of Data Engineering experience OR\nDiploma and 7 to 9 years of Data Engineering experience\nMust Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3 years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood to Have Skills:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nThe highest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, remote teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data management', 'Power BI', 'data governance', 'data warehousing', 'Databricks', 'ETL', 'AWS']",2025-06-12 14:26:13
"Technical Architect, AVP",XL India Business Services Pvt. Ltd,13 - 20 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Technical Architect Bangalore, Karnataka, India We invent the new to help the world move forward\n\nCombining powerful analytics and deeper insights with bigger ideas and innovative solutions, we free up our clients potential, thereby fulfilling our own\n\nTake it seriously\n\nMake it fun\n\nKnow it matters\n\nThe Technical Architect role is the lead technologist for a product or platform\n\nThird party products with high out of box implementation most likely will have their own technical architecture roles, so company Technical Architects mainly manage internal assets\n\nTechnical Architects perform deliverable reviews and manage measurement of deliverable quality within the Delivery Team\n\nWhat you ll be DOING What will your essential responsibilities include? Specification of technologies, application architectures and data structures as a basis for application change for internal assets\n\nProducing quality, secure, scalable, high-performing, and resilient designs for new or improved services on both OnPrem and Cloud platform\n\nLead the systems analysts, developers, and testers in sympathetic change to the applications\n\nFor internal assets, support Application Managers to develop and maintain the Product Roadmap\n\nDefine and maintain development standards such as system and data design, coding, etc Maintain a capacity plan with historical performance metrics, a future forecast, and a capacity model to ensure services and infrastructure deliver performance and growth targets in a cost effective and proactive manner\n\nManage architecture exceptions for the application, including identifying, documenting, taking through exception approval process, and remediation where and when possible\n\nMonitor application services to ensure performance consistently meets non-functional requirements (response time, security, etc)\n\nManage AXA XL security standards for the applications, including clean code, vulnerability identification and remediation, penetration test etc Work with the Application Manager & Delivery Lead(s) in defining, analyzing, planning, measuring and improving product availability and continuity\n\nLeads the DevOps team and developers in targeted use of DevOps for their application platform assets\n\nYou will report to the Delivery Lead - Claims\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Proven experience in Microsoft technical suit like Dot Net (Core, Standard), SQL Server, C#, VBDot Net, ASPDot Net\n\nDeep understanding of enterprise integration patterns (API, middlewear and/or data migration, secured data flow)\n\nA moderate amount of Cloud based experience with Azure\n\nDevOps practices including CI/CD pipelines, infrastructure as code and containerization\n\nProven experience in designing complex, scalable architectures for large enterprise environments\n\nAbility to break down complex problems, evaluate multiple solutions & choose the most suitable outcome based on tradeoffs\n\nUphold technical integrity of internal assets at the logical and physical level, ensuring designs and changes done based on an outstanding architectural foundation\n\nAbility to work closely with cross-functional teams, including developers, product owners, and operations to ensure alignment on technicals goals and priorities\n\nDevelop, own and publish the product technical architecture and design documentation\n\nIdentify technical resources required to deliver the service and maintain plans for the short-, mid-, and long-term to support business cases for technical investments (upgrades, etc)\n\nTranslates the high-level designs into the technical specifications to facilitate efficient and effective development and unit testing\n\nDesired Skills and Abilities: Proficiency with multiple application delivery models including Agile, iterative and waterfall\n\nAdaptable to new/different strategies, programs, technologies, practices, cultures, etc Comfortable with change, able to easily make transitions\n\nAdvanced skills in developing tools, frameworks and processes intended to maximize software quality and minimize time-to-delivery\n\nBachelor s degree in the field of computer science, information systems, business management, or a related field preferred\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides competitive compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data migration', 'Coding', 'Agile', 'Data structures', 'Unit testing', 'software quality', 'microsoft', 'Analytics', 'SQL']",2025-06-12 14:26:15
IN Senior Associate AWS AI/ML/GenAI Developer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\n& Summary We are looking for a seasoned AWS\nAI/ML/GenAI Developer\nResponsibilities\nDesign and implement AI/ML/GenAI models using AWS services such as AWS Bedrock, SageMaker, Comprehend, Rekognition, and others.\nStrong programming skills in Python, R etc\nExperience with machine learning frameworks such as TensorFlow, PyTorch, or Scikitlearn.\nKnowledge of data preprocessing, feature engineering, and model evaluation techniques.\nDevelop and deploy generative AI solutions to solve complex business problems and improve operational efficiency.\nCollaborate with data scientists, engineers, and product teams to understand requirements and translate them into technical solutions.\nOptimize and finetune machine learning models for performance and scalability. Ensure the security, reliability, and scalability of AI/ML solutions by adhering to best practices.\nMaintain and update existing AI/ML models to ensure they meet evolving business needs.\nStay uptodate with the latest advancements in AI/ML and GenAI technologies and integrate relevant innovations into our solutions.\nProvide technical guidance and mentorship to junior developers and team members.\nExcellent problemsolving skills and ability to work in a fastpaced, collaborative environment.\nGood to have AWS Certified Machine Learning Specialty or other relevant AWS certifications.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired 48 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Bachelor of Engineering, Bachelor of Technology\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'GCP', 'Data modeling', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Corporate advisory', 'Operations', 'AWS']",2025-06-12 14:26:18
Architect,Trianz,12 - 17 years,Not Disclosed,['Bengaluru'],"Role Snowflake Architect\nJob Summary -\nAs a Data Architect, you are core to the D&AI (Data & AI) Practices success. Data is foundational to everything we do, and you are accountable for defining and delivering best-in-class Snowflake data management solutions across all major cloud platforms. This is a senior role with high visibility and reporting to the D&AI Practice Tower Lead.\nJob Responsibilities\nArchitectural Design: Architect secure, scalable, highly performant data engineering and management solutions, including data warehouses, data lake, ELT / ETL and real-time data engineering / pipeline solutions. Support Principal Data Architect in defining and maintaining Practice reference data engineering and data management architectures.\nSnowflake Implementation: Design and manage scalable end-to-end data solutions leveraging native Snowflake workloads including : Data Engineering; Data Lake; Data Warehouse; Applications; Unistore; AI/ML; Governed Collaboration, Marketplace, Streamlit.\nHyperscaler Design: Competently leverage data-related cloud platform (AWS or Azure) capabilities to architect and develop end-to-end data engineering and data management solutions.\nClient Engagement: Regular collaboration and partnership with clients to understand their challenges and needs then translate requirements into data solutions that drive customer value. Support proposal development.\nData Modeling: Create and maintain conceptual, logical, and physical data models that support both transactional and analytical needs. Ensure data models are optimized for performance and scalability.\nCreativity: Be an out-of-the-box thinker and passionate about applying your skills to new and existing solutions alike while always demonstrating a customer-first mentality.\nMandatory Skills\n12+ years hands-on data solution architecture and implementation experience on modern cloud platforms (AWS preferred) including microservice and event-driven architectures.\nSnowflake SnowPro Advanced Architect certification. An architectural certification on either AWS, Azure or GCP.\nHands-on experience with Snowflake capabilities including Snowpipe, Snowpark, Cortex, Polaris Catalog, native applications, Notebooks, Horizon, Marketplace, Streamlit.\nPractical experience with end-to-end data engineering and data management supporting functions including data modeling (conceptual, logical & physical), BI & analytics, data governance, data quality, data security / privacy / compliance, IAM, performance optimization. Advanced SQL and data profiling.\nPython, Scala or Java. Strong communication skills with the ability to convey technical concepts to non-technical users.\nStrong, self-management skills demonstrating ability to multitask and self-manage goals and activities.\nAdditional / Nice-to-have Qualifications-\nSnowflake SnowPro Advanced Data Engineer certification Snowflake SnowPro Advanced Data Scientist certification Snowflake SnowPro Advanced Administrator certification Snowflake SnowPro Advanced Data Analyst certification\nRequired Education Master or Bachelor (CS, IT, Applied Mathematics or demonstrated experience)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'Architect', 'Data management', 'Data modeling', 'data security', 'Analytical', 'Data quality', 'Analytics', 'SQL', 'Python']",2025-06-12 14:26:20
Lead/Solution Architect: Java Full Stack,Omega Healthcare,10 - 15 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role : Tech Lead\nEducational Qualification : ME / BE / MCA / MSc\nExperience Required : 10+ Years\nShifts : Day shift\nResponsibilities:\n10+ years of expertise working in Core Java,J2EE, Design Patterns, Spring, Spring Boot, Micro service architecture\nExpertise in Web services; SOAP and REST\nExperience with at least one UI technology like Angular, React JS, OJET\nGood knowledge in Postgres/ MySQL, Advanced PL SQL with JSON data management\nBuild tools like Ant, Maven and Gradle\nExperience in Azure Repos/SVN/ Git source control tool is must\nSolid understanding of Software development life cycle and OOPS concept\nWorking with APIs/Integrations is must\nExperience working with JDK 17 + is preferred\nExcellent understanding of architectural principles involved in SaaS and multi-tenant platforms\nStrong in development tools like Eclipse with SDS, IntelliJ, Git, Cradle, Sonar, Jenkins, Jira/ADO/ Artifactory\nORM technologies like Hibernate\nExperience with Kubernetes and Dockers is preferred\nExperience of messaging systems and data pipelines such as RabbitMQ and Kafka is preferred\nExperience using other API Management solutions like Apigee\nWell versed with scalability, automation, resiliency, high availability and user experience\nExperience in cloud computing application implementations on AWS is preferred\nStrong background in creating secure cloud architectures for customer facing applications that are enterprise grade and highly scalable is strongly preferred\nExperience in Agile, DevOps culture is a plus\nShould have managed a team of SSE/SE and lead at least 2 projects\nGood communication skills",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'cloud integration', 'solution architecture', 'Java Architecture', 'data architecture', 'Angular', 'application development']",2025-06-12 14:26:23
Teamcenter Architect,We are hiring for well knowned MNC,8 - 13 years,20-35 Lacs P.A.,['Bengaluru'],"10+ years of experience in Teamcenter PLM implementations, with at least 3+ years as a Technical Architect.\nLead end-to-end Teamcenter PLM technical architecture and deployment strategy.\nDefine and implement Teamcenter solutions including data modeling, workflow customization, and BMIDE configurations.\nCollaborate with business stakeholders to gather requirements and propose scalable Teamcenter solutions.\nDrive architectural decisions, performance tuning, and capacity planning.\nOversee Teamcenter installations, upgrades, environment setup, clustering, and disaster recovery planning.\nSupport Teamcenter integrations with CAD (NX, SolidWorks, CATIA), ERP, and other enterprise systems.\nConduct code reviews and provide governance for customizations, extensions, and data migration strategies.\nDeep expertise in:\nTeamcenter architecture, ITK, RAC, SOA, and Active Workspace.\nBMIDE, Workflows, Custom Handlers, ACLs, and Stylesheets.\nDeployment on multi-tier architecture and cloud platforms (optional but preferred).",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Temporary/Contractual","['PLM', 'ITK', 'Architect', 'Bmide', 'SOA', 'RAC', 'Awc 1', 'Architecting']",2025-06-12 14:26:25
Solution Architect,Hitachi Vantara,10 - 20 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Job Title: Senior Technical Consultant / Solution Architect Storage & Content Technologies\n\nJob Summary:\n\nWe are looking for a client-facing Technical Consultant or Solution Architect with strong leadership and problem-solving skills. You will act as a key advisor to both internal and customer technical teams, providing expert guidance on system architecture, implementation techniques, troubleshooting, and solution development. This role requires active engagement in architectural planning, change management, and team coordination, while also contributing to client growth opportunities through technical insight and solution proposals.",,,,"['Storage', 'Infrastructure', 'Presales', 'Solutioning']",2025-06-12 14:26:28
Job opening For Data Warehouse + ADF + ETL,bct,3 - 6 years,Not Disclosed,['Pune'],"Greetings of the Day !!!\n\nWe have job opening for Data Warehouse + ADF + ETL with one of our Client .If you are interested for this role , kindly share update resume along with below details in this email id : shaswati.m@bct-consulting.com\n\nJob Description:\nSenior Data Engineer\nAs a Senior Data Engineer, you will support the European World Area using the Windows & Azure suite of Analytics & Data platforms. The focus of the role is on the technical aspects and implementation of data gathering, integration and database design.\nWe look forward to seeing your application!\nIn This Role, Your Responsibilities Will Be:\nData Ingestion and Integration: Collaborate with Product Owners and analysts to understand data requirements & design, develop, and maintain data pipelines for ingesting, transforming, and integrating data from various sources into Azure Data Services.\nMigration of existing ETL packages: Migrate existing SSIS packages to Synapse pipelines\nData Modelling: Assist in designing and implementing data models, data warehouses, and databases in Azure Synapse Analytics, Azure Data Lake Storage, and other Azure services.\nData Transformation: Develop ETL (Extract, Transform, Load) processes using SQL Server Integration Services (SSIS), Azure Synapse Pipelines, or other relevant tools to prepare data for analysis and reporting.\nData Quality and Governance: Implement data quality checks and data governance practices to ensure the accuracy, consistency, and security of data assets.\nMonitoring and Optimization: Monitor and optimize data pipelines and workflows for performance, scalability, and cost efficiency.\nDocumentation: Maintain comprehensive documentation of processes, including data lineage, data dictionaries, and pipeline schedules.\nCollaboration: Work closely with cross-functional teams, including data analysts, data scientists, and business stakeholders, to understand their data needs and deliver solutions accordingly.\nAzure Services: Stay updated on Azure data services and best practices to recommend and implement improvements in our data architecture and processes\nFor This Role, You Will Need:\n3-5 years of experience in Data Warehousing with On-Premises or Cloud technologies\nStrong practical experience of Synapse pipelines / ADF.\nStrong practical experience of developing ETL packages using SSIS.\nStrong practical experience with T-SQL or any variant from other RDBMS.\nGraduate degree educated in computer science or a relevant subject.\nStrong analytical and problem-solving skills.\nStrong communication skills in dealing with internal customers from a range of functional areas.\nWillingness to work flexible working hours according to project requirements.\nTechnical documentation skills.\nFluent in English.\nPreferred Qualifications that Set You Apart:\nOracle PL/SQL.\nExperience in working on Azure Services like Azure Synapse Analytics, Azure Data Lake.\nWorking experience with Azure DevOps paired with knowledge of Agile and/or Scrum methods of delivery.\nLanguages: French, Italian, or Spanish would be an advantage.\nAgile certification.\nThanks,\nShaswati",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ADF', 'ETL', 'SSIS', 'Data ware house']",2025-06-12 14:26:30
Data Engineer,Databeat,3 - 7 years,Not Disclosed,['Hyderabad( Rai Durg )'],"Experience Required: 3+ years\n\nTechnical knowledge: AWS, Python, SQL, S3, EC2, Glue, Athena, Lambda, DynamoDB, RedShift, Step Functions, Cloud Formation, CI/CD Pipelines, Github, EMR, RDS,AWS Lake Formation, GitLab, Jenkins and AWS CodePipeline.\n\n\n\nRole Summary: As a Senior Data Engineer,with over 3 years of expertise in Python, PySpark, SQL to design, develop and optimize complex data pipelines, support data modeling, and contribute to the architecture that supports big data processing and analytics to cutting-edge cloud solutions that drive business growth. You will lead the design and implementation of scalable, high-performance data solutions on AWS and mentor junior team members.This role demands a deep understanding of AWS services, big data tools, and complex architectures to support large-scale data processing and advanced analytics.\nKey Responsibilities:\nDesign and develop robust, scalable data pipelines using AWS services, Python, PySpark, and SQL that integrate seamlessly with the broader data and product ecosystem.\nLead the migration of legacy data warehouses and data marts to AWS cloud-based data lake and data warehouse solutions.\nOptimize data processing and storage for performance and cost.\nImplement data security and compliance best practices, in collaboration with the IT security team.\nBuild flexible and scalable systems to handle the growing demands of real-time analytics and big data processing.\nWork closely with data scientists and analysts to support their data needs and assist in building complex queries and data analysis pipelines.\nCollaborate with cross-functional teams to understand their data needs and translate them into technical requirements.\nContinuously evaluate new technologies and AWS services to enhance data capabilities and performance.\nCreate and maintain comprehensive documentation of data pipelines, architectures, and workflows.\nParticipate in code reviews and ensure that all solutions are aligned to pre-defined architectural specifications.\nPresent findings to executive leadership and recommend data-driven strategies for business growth.\nCommunicate effectively with different levels of management to gather use cases/requirements and provide designs that cater to those stakeholders.\nHandle clients in multiple industries at the same time, balancing their unique needs.\nProvide mentoring and guidance to junior data engineers and team members.\n\n\n\nRequirements:\n3+ years of experience in a data engineering role with a strong focus on AWS, Python, PySpark, Hive, and SQL.\nProven experience in designing and delivering large-scale data warehousing and data processing solutions.\nLead the design and implementation of complex, scalable data pipelines using AWS services such as S3, EC2, EMR, RDS, Redshift, Glue, Lambda, Athena, and AWS Lake Formation.\nBachelor's or Masters degree in Computer Science, Engineering, or a related technical field.\nDeep knowledge of big data technologies and ETL tools, such as Apache Spark, PySpark, Hadoop, Kafka, and Spark Streaming.\nImplement data architecture patterns, including event-driven pipelines, Lambda architectures, and data lakes.\nIncorporate modern tools like Databricks, Airflow, and Terraform for orchestration and infrastructure as code.\nImplement CI/CD using GitLab, Jenkins, and AWS CodePipeline.\nEnsure data security, governance, and compliance by leveraging tools such as IAM, KMS, and AWS CloudTrail.\nMentor junior engineers, fostering a culture of continuous learning and improvement.\nExcellent problem-solving and analytical skills, with a strategic mindset.\nStrong communication and leadership skills, with the ability to influence stakeholders at all levels.\nAbility to work independently as well as part of a team in a fast-paced environment.\nAdvanced data visualization skills and the ability to present complex data in a clear and concise manner.\nExcellent communication skills, both written and verbal, to collaborate effectively across teams and levels.\n\nPreferred Skills:\nExperience with Databricks, Snowflake, and machine learning pipelines.\nExposure to real-time data streaming technologies and architectures.\nFamiliarity with containerization and serverless computing (Docker, Kubernetes, AWS Lambda).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'SQL', 'Data Pipeline', 'Python', 'Amazon Ec2', 'Data Engineering', 'Data Bricks', 'Aws Lambda', 'Amazon Redshift', 'Azure Cloud', 'Data Lake', 'Data Modeling', 'Athena']",2025-06-12 14:26:32
Solution Architect - Java,Epam Systems,10 - 13 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","We are looking for a Solution Architect - Java to join our team.\nYour expertise will shape the design of scalable software solutions. You will leverage your extensive experience in solution architecture and microservice architecture styles to drive innovation. If you are ready to take your career to the next level, we encourage you to apply.\n\nResponsibilities\nDesign secure, reliable, high availability, scalable solutions for the program\nDefine, plan, and support execution of the technology strategy for one or more products\nCollaborate closely with the global solution architecture and engineering team to define principles and best practices\nEngage with wider architecture and technology teams to ensure alignment with technical strategies and policies\nSupport development teams and work with stakeholders, promoting agile development\nCreate a culture of technical excellence and continuous improvement\nResearch, create, and evaluate technical solution alternatives for business needs using current and upcoming technologies\nDrive overall software implementation using expertise in microservices-based architectures for the fintech industry\nPartner with senior technical and product leaders to deliver on designs\nCollaborate with development teams, operations, and product owners\nProvide technical leadership and mentorship to development teams\nRepresent as the primary architect and technical advocate in program discussions\nRequirements\nExperience in product engineering with over 15 years in designing scalable software solutions\nBackground in computer science fundamentals, web applications, and microservices-based software architecture\nExperience with high transaction volume financial systems operating at global scale\nKnowledge of web technologies including HTML5, CSS, and JavaScript, along with front-end frameworks like AngularJS and ReactJS\nProficiency in designing and building back-end microservices using Java and Spring frameworks\nUnderstanding of storage technologies such as PostgreSQL and SQL Server for large-scale applications\nFamiliarity with cloud-native technologies and best practices, including Amazon Web Services and Microsoft Azure\nCapability to work effectively in an Agile environment focused on continuous improvement\nDesire to collaborate and provide mentorship to technology teams\nHands-on experience in building prototypes to solve complex business problems\nEnglish proficiency at a professional level",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['css', 'technical leadership', 'web services', 'web application', 'microservices', 'sql', 'spring', 'react.js', 'java', 'postgresql', 'computer science', 'gcp', 'design', 'software solutions', 'end', 'html', 'architecture', 'microsoft azure', 'javascript', 'sql server', 'high availability', 'framework', 'web technologies', 'agile', 'aws', 'angularjs']",2025-06-12 14:26:34
Lead Data Engineer,Prolegion,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Lead Data Engineer/Associate Architect to lead the design, implementation, and optimization of scalable data architectures. The ideal candidate will have a deep understanding of data modeling, ETL processes, cloud data solutions, and big data technologies. You will work closely with cross-functional teams to build robust, high-performance data pipelines and infrastructure to enable data-driven decision-making.\n\nExperience: 8 - 12+ years\nWork Location: Hyderabad (Hybrid)\nMandatory skills: Python, SQL, Snowflake\n\nResponsibilities:\nDesign and Develop scalable and resilient data architectures that support business needs, analytics, and AI/ML workloads.\nData Pipeline Development: Design and implement robust ETL/ELT processes to ensure efficient data ingestion, transformation, and storage.\nBig Data & Cloud Solutions: Architect data solutions using cloud platforms like AWS, Azure, or GCP, leveraging services such as Snowflake, Redshift, BigQuery, and Databricks.\nDatabase Optimization: Ensure performance tuning, indexing strategies, and query optimization for relational and NoSQL databases.\nData Governance & Security: Implement best practices for data quality, metadata management, compliance (GDPR, CCPA), and security.\nCollaboration & Leadership: Work closely with data engineers, analysts, and business stakeholders to translate business requirements into scalable solutions.\nTechnology Evaluation: Stay updated with emerging trends, assess new tools and frameworks, and drive innovation in data engineering.\n\nRequired Skills:\nEducation: Bachelors or Masters degree in Computer Science, Data Engineering, or a related field.\nExperience: 8 - 12+ years of experience in data engineering\nCloud Platforms: Strong expertise in AWS data services.\nBig Data Technologies: Experience with Hadoop, Spark, Kafka, and related frameworks.\nDatabases: Hands-on experience with SQL, NoSQL, and columnar databases such as PostgreSQL, MongoDB, Cassandra, and Snowflake.\nProgramming: Proficiency in Python, Scala, or Java for data processing and automation.\nETL Tools: Experience with tools like Apache Airflow, Talend, DBT, or Informatica.\nMachine Learning & AI Integration (Preferred): Understanding of how to architect data solutions for AI/ML applications\n\n,",Industry Type: Defence & Aerospace,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'Data modeling', 'Postgresql', 'Informatica', 'Apache', 'Analytics', 'SQL', 'Python']",2025-06-12 14:26:37
SAP DATA Steward,Maddisoft Solutions,10 - 20 years,Not Disclosed,['Hyderabad'],"Job Title: SAP DATA STEWARD (CONTRACT)\nLocation: HYDERABAD, INDIA\n\nAs the SAP Data Steward is responsible for creating, maintaining, and deactivating master data and data attributes in SAP with focus on data migration. The Data Stewards has an essential role in establishing in monitoring existing and new data against and ensuring timely and high-quality creation of new data in the system.\nBachelors Degree or Associates degree with additional 9+ years of work experience required or an equivalent combination of education and experience.\nRequires SAP functional knowledge on SAP Routings with Migration Perspective.\nShould have complete knowledge on SAP Routings tables. Need to have basic knowledge on linking between the tables and joins needed for getting the extract template ready as per Client's Format.\nExcellent attention to detail, exceptional interest in creating order and consistency required.\n10+ years of experience in data management and/or data governance activities and responsibilities.\nExperience working with SAP ECC required.\nDemonstrated expert-level experience and capability with MS Excel required.\nHigh degree of initiative and ownership, as well as a proven history of delivering results while working with several different departments in a fast-paced environment required.\nExperience creating and running business reports and data queries is preferred.\nConfident user of Microsoft Office (Word, Excel, Outlook, PowerPoint, Teams).\nExperience working with teams across multiple functions.\nAbility to multi-task and work under tight timelines required.\nExcellent communication skills both verbal and written.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['SAP ECC', 'Data Stewardship', 'Data Management', 'Data Governance', 'SAP Routing']",2025-06-12 14:26:39
SAP MDG Architect,Akana Services,10 - 15 years,Not Disclosed,[],"Key Responsibilities:\n*Good understanding of SAP MDG technical framework includes BADI, BAPI/RFC/FM, Workflows, BRF+, Enterprise Services, IDoc, Floorplan Manager, WebDynPro, Fiori, and MDG API framework, Workflows, AMDP, CDS, BRF+, BOPF.\n*Should have knowledge working with REST API, SOAP API, ODATA. SAP AIF Application Interface Framework, Inapp extensibility.\n*Knowledge of SAP data dictionary tables, views, relationships, and corresponding data architecture for ECC and S/4 HANA for various SAP master.\n*Implementation experience of SAP MDG in key domains such as Customer, Supplier, Material, Business Partners.\nRole Requirements and Qualifications:\n*10+ years of SAP experience, with at least 5 years focused on SAP MDG architecture and implementation.\n*Proven experience with multiple end-to-end SAP MDG implementations.\n*Deep understanding of SAP MDG Central Governance, Consolidation, and Mass Processing.\n*Expertise in MDG data modeling, BRF+ rules, workflows, FPM, and UI configuration.\n*Strong integration knowledge using SOA services, IDocs, and MDG Data Replication Framework.\n*Experience with SAP S/4HANA, SAP Fiori, and MDG on S/4HANA.\n*Solid understanding of data governance frameworks and master data management principles.\n*Ability to assess current master data landscapes and propose future-state architecture.\n*Excellent communication, leadership, and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['SAP Fiori', 'Sap Mdg', 'SAP S/4HANA']",2025-06-12 14:26:41
Sap Hybris Architect,Pepplo Consulting,14 - 20 years,40-45 Lacs P.A.,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Minimum 12 years of experience in IT, with proven expertise in at least 2 large-scale projects (More than 10 lakh users for a B2C application) involving system design & deployment of enterprise architecture, data architecture & cloud implementations.\n\nRequired Candidate profile\nKnowledge on Hybris ORM/WCMS/Backoffice/Cockpits/SOLR search engine\nExpert in Hybris B2B/B2C Accelerators, Hybris Workflow/Task Mgmt\nExpert in the catalog/order management/promotions/vouchers/coupons",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spring Mvc', 'Sap Hybris Commerce', 'Javascript', 'Html/Css', 'GIT', 'JSP', 'J2Ee', 'SVN', 'JQuery']",2025-06-12 14:26:43
Mobile B2C BSS OSS Solution Architect Service fulfilment,Prodapt Solutions,5 - 10 years,Not Disclosed,['Bengaluru'],"Overview\n\n*Need Telecom Solution Architect/Digital Solution Designer with telecom domain experience.\n*Should have handson in Lead to Cash journey - Processes and Systems for Cellular Mobile service for Retail customers\n*Should have solid understanding on different Service Fulfilment journey metrics.\n*Should have solid understanding of different BSS/OSS applications that support Lead to Cash journey.\n*Should have strong understanding of different Channels, Product Catalog, Order Orchestration, Provisioning & Activation\n*Should have conducted workshops to gather requirements from stakeholders, presenting different solution options to architects.\n*Should have expertise in:\n**Designing multi-channel Enterprise solutions among BSS domains CRM, Order Management, BPM, OmniChannel implementations, TMForum Open APIs , Catalog, Intelligence platforms etc\n**Knowledge on Event/Data driven architecture, micro service framework and API integration\n**Experience in designing enterprise systems in BSS\n\nResponsibilities\n\nMandatory to have excellent understanding of Telecom Domain, and experience of Consulting / Solution Design / Solution Architecture for Telecom Solutions (Products and Services development).\nAbility to analyze Telco processes, simplify them and develop solutions in alignment with architecture principles.\nExperience in working with business and application teams to collect requirements and deliver solutions.\nExperience in BSS/OSS is must, at least 5+ Years.\nUnderstands Open API or TMF API specification or API based integration with partners\nDefine L2/L3 data model using reference data architecture like SID\nDeveloping micro services-based architecture\nTMF - open digital architecture or similar framework to develop channels architecture\nExperience of using modelling tools for architecture development\nAs a Telecom Domain expert, you will be required to seek out the best ways to improve business processes and effectiveness through technology, strategy, analytic solutions.\nBrings in strong knowledge in the telecom domain, and having working experience across Business and Technology Roles, across B2B and B2C businesses.\nExperience with Business Analysis, Business Process Re-engineering, E2E Solution Design / Component or System Design / Solution Architecture, in Telecom OSS / BSS domain is required.\n\nGood to have working experience on Digital Transformation, OSS/BSS Cloud Implementations, Network Transformation programs.\nUnderstanding of Technology Landscape such as E2E OSS & BSS, Microservices based Architecture, RPA, Service Assurance, Billing, and Invoicing for B2C business.\nShould have a business focus, and an equal understanding of IT/Technology enablement, so as to be able to align the business goals/objectives to the IT delivery.\nGood experience in data modelling and integration patterns.\nHas good understanding of frameworks relevant for Telecommunications domain (for e.g. eTOM, SID, TAM, ITIL, ODA)\nAble to understand and translate business requirements, and working with product / technical and operations teams to help design and build new Products and Services\nAble to conduct workshops and design thinking sessions, to understand IT Process / Technology / Architecture and existing OSS/BSS implementations.\nAble to deliver intelligent business insights through translation of reports and analytics.\nUnderstanding of technologies such as 4G/5G/Metro Ethernet/ATM/MPLS/SONET/SDH etc. and their mapping to OSS systems is an added advantage.\nConduct proof-of-concept (POCs) for requirements.\nGood communication and Presentation skills.\n\nGood to have\nUnderstanding of autonomous networks and closed loop automation\nKnowledge of Architecture design methodologies and Industry reference frameworks â€“ TOGAF etc. â€“ with a clear demonstration of the deliverables created as a result of applying the framework\nKnowledge of 5G & IoT\nExperience in any of the COTS products\nExperience in Network optimization techniques\nUnderstanding of intelligent applying across OSS land scape\nExperience of working on network technologies\nKnowledge of DevOps and open-source tools\nExperience in any one of the programming languages - Java or Python\nExperience in any one of the DB technologies - Oracle, MySQL, Postgres, Cassandra\nWorking with development teams and product managers to build software solutions/web applications\n\n\nBachelor's degree (in any field) is mandatory.\nMSc/BE /Masters with specialization in IT/Computer Science is desired.\nAtleast 5 years of work experience.\n\nAdaptability and experience working in multi-channel delivery projects is preferred.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['product catalog', 'bss', 'open source', 'telecom', 'provisioning', 'python', 'etom', 'oracle', 'togaf', 'business analysis', 'solution delivery', 'data architecture', 'tmf', 'microservices', 'java', 'postgresql', 'solution design', 'cassandra', 'devops', 'open api', 'e2e', 'mysql', 'api', 'digital transformation']",2025-06-12 14:26:46
Mobile B2C BSS OSS Solution Architect Service Assurance,Prodapt Solutions,5 - 10 years,Not Disclosed,['Bengaluru'],"Overview\n\n*Need Telecom Solution Architect/Digital Solution Designer with telecom domain experience.\n*Should have hands-on in Service Assurance journey - Processes and Systems for Cellular Mobile service for Retail customers\n*Should have solid understanding on different Service Assurance journey metrics.\n*Should have solid understanding of different BSS OSS applications that support Service Assurance Journey\n*Should have solid understanding of Billing & Rating\n*Should have conducted workshops to gather requirements from stakeholders, presenting different solution options to architects.\n*Should have expertise in:\n**Designing multi-channel Enterprise solutions among BSS/OSS domains with in-depth knowledge in Network monitoring system, Fault Orchestration/Incident Management system, Test & Diagnostics system, Billing & Rating system, Provisioning & Activation system, Service Inventory, CRM, Field Engineering Management system, OmniChannel implementations, Network performance management system, TMForum Open APIs, Intelligence platforms etc\n**Knowledge on Event/Data driven architecture, micro service framework and API integration\n**Experience in designing enterprise systems in BSS/OSS\n\nResponsibilities\n\nMandatory to have excellent understanding of Telecom Domain, and experience of Consulting / Solution Design / Solution Architecture for Telecom Solutions (Products and Services development).\nAbility to analyze Telco processes, simplify them and develop solutions in alignment with architecture principles.\nExperience in working with business and application teams to collect requirements and deliver solutions.\nExperience in BSS/OSS is must, at least 5+ Years.\nUnderstands Open API or TMF API specification or API based integration with partners\nDefine L2/L3 data model using reference data architecture like SID\nDeveloping micro services-based architecture\nTMF - open digital architecture or similar framework to develop channels architecture\nExperience of using modelling tools for architecture development\nAs a Telecom Domain expert, you will be required to seek out the best ways to improve business processes and effectiveness through technology, strategy, analytic solutions.\nBrings in strong knowledge in the telecom domain, and having working experience across Business and Technology Roles, across B2B and B2C businesses.\nExperience with Business Analysis, Business Process Re-engineering, E2E Solution Design / Component or System Design / Solution Architecture, in Telecom OSS / BSS domain is required.\n\nGood to have working experience on Digital Transformation, OSS/BSS Cloud Implementations, Network Transformation programs.\nUnderstanding of Technology Landscape such as E2E OSS & BSS, Microservices based Architecture, RPA, Service Assurance, Billing, and Invoicing for B2C business.\nShould have a business focus, and an equal understanding of IT/Technology enablement, so as to be able to align the business goals/objectives to the IT delivery.\nGood experience in data modelling and integration patterns.\nHas good understanding of frameworks relevant for Telecommunications domain (for e.g. eTOM, SID, TAM, ITIL, ODA)\nAble to understand and translate business requirements, and working with product / technical and operations teams to help design and build new Products and Services\nAble to conduct workshops and design thinking sessions, to understand IT Process / Technology / Architecture and existing OSS/BSS implementations.\nAble to deliver intelligent business insights through translation of reports and analytics.\nUnderstanding of technologies such as 4G/5G/Metro Ethernet/ATM/MPLS/SONET/SDH etc. and their mapping to OSS systems is an added advantage.\nConduct proof-of-concept (POCs) for requirements.\nGood communication and Presentation skills.\n\nGood to have\nUnderstanding of autonomous networks and closed loop automation\nKnowledge of Architecture design methodologies and Industry reference frameworks â€“ TOGAF etc. â€“ with a clear demonstration of the deliverables created as a result of applying the framework\nKnowledge of 5G & IoT\nExperience in any of the COTS products\nExperience in Network optimization techniques\nUnderstanding of intelligent applying across OSS land scape\nExperience of working on network technologies\nKnowledge of DevOps and open-source tools\nExperience in any one of the programming languages - Java or Python\nExperience in any one of the DB technologies - Oracle, MySQL, Postgres, Cassandra\nWorking with development teams and product managers to build software solutions/web applications\n\n\nBachelor's degree (in any field) is mandatory.\nMSc/BE /Masters with specialization in IT/Computer Science is desired.\nAtleast 5 years of work experience.\n\nAdaptability and experience working in multi-channel delivery projects is preferred.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['bss', 'service assurance', 'software testing', 'billing', 'telecom', 'python', 'engineering management', 'field engineering', 'performance management system', 'open source', 'network monitoring', 'network performance', 'java', 'postgresql', 'incident management', 'provisioning', 'mysql', 'crm']",2025-06-12 14:26:48
Senior Business Analyst,ANZ,5 - 10 years,Not Disclosed,['Bengaluru'],"The Senior Business Analyst role will provide business analysis expertise to help deliver complex initiatives. The purpose of this role is to:\nWork in an agile and collaborative team, working with a range of business and technical stakeholders to deliver outcomes and value for customers.\nApply business analysis skills, techniques, and activities to understand, validate, synthesise, and articulate problems, opportunities, requirements, options, and solutions.\nIdentify opportunities to deliver continuous improvement to ensure the continuous delivery of customer centric benefits.\nShare knowledge and support capability growth of peers and Business Analysts.\nProvide passion about agile product development and will help drive continuous improvement in our scrum teams.\nBanking is changing and we re changing with it, giving our people great opportunities to try new things, learn and grow. Whatever your role at ANZ, you ll be building your future, while helping to build ours.\nRole Type: Permanent, 4.4\nRole Location: Bengaluru\nWork Hours: 7 AM to 4 PM\nWhat will your day look like?\n\nAs a Senior Business Analyst, you will:\nProviding technical business analysis expertise to help deliver complex initiatives.\nDeveloping requirements that are fit for purpose and accurate, display an understanding of the implications to the project or program, and manage the requirements throughout the life of the project.\nDeveloping presentations and documentation to communicate insights and recommendations in a compelling manner, and present regular project updates to the wider team.\nNurturing a growth mindset and curiosity within our teams.\nActively researching industry trends, standards, and methods, offering technical insights and direction.\nEnsuring quality control and data governance processes are followed.\nAdhering to established ANZ policy and procedures and display an understanding of project & program practices and governance.\nEnsuring that assigned business analysis activities are conducted efficiently and within agreed timeframes and ensure that the outputs and benefits are clear.\nWhat will you bring?\nThe must have knowledge, skill, and experience (KSE) the role requires are:\n5+ years experience as a Business Analyst and currently operating at senior level.\nUnderstanding and experience with Data warehousing, data projects or data migration is required.\nAbility to use SQL or SAS is highly recommended.\nAnalytical skills while being highly organised and adaptable.\nPositive can-do attitude and an excellent communicator both written, verbal and visual.\nProblem solver and critical thinker.\nNatural ability to build relationships with stakeholders within the business and technology.\nGrowth mindset.\nThe good to have knowledge, skill and experience (KSE) the role requires are:\nExperience delivering work and leading others in an Agile context (Scrum Master experience is beneficial).\nExperience in the creation of user stories into minimum viable increments, acceptance criteria and specification by example.\nExperience delivering data outcomes that drive decision making in a financial services or banking organisation.\nData warehousing experience.\nExperience with data modelling and databases will be advantageous.\nExpert in story mapping and building backlogs to support iterative development.",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAS', 'Business analysis', 'Agile', 'Senior Business Analyst', 'Scrum', 'Manager Quality Control', 'Continuous improvement', 'Data warehousing', 'Financial services', 'SQL']",2025-06-12 14:26:50
Software Dev Engineer,Amazon,1 - 3 years,Not Disclosed,['Bengaluru'],"AWS Infrastructure Services owns the design, planning, delivery, and operation of all AWS global infrastructure. In other words, we re the people who keep the cloud running. We support all AWS data centers and all of the servers, storage, networking, power, and cooling equipment that ensure our customers have continual access to the innovation they rely on. We work on the most challenging problems, with thousands of variables impacting the supply chain and we re looking for talented people who want to help.\n\nYou ll join a diverse team of software, hardware, and network engineers, supply chain specialists, security experts, operations managers, and other vital roles. You ll collaborate with people across AWS to help us deliver the highest standards for safety and security while providing seemingly infinite capacity at the lowest possible cost for our customers. And you ll experience an inclusive culture that welcomes bold ideas and empowers you to own them to completion.\n\nThese are all greenfield projects offering a rare opportunity to build systems from the ground up. You will be part of a new organization within AWS building large scale distributed applications. We will be designing and developing these systems from scratch utilizing technologies and frameworks like reactive micro services, serverless computing and distributed NoSQL data stores.\n\nIf you have a solid understanding of fundamental algorithms and system design and are able to produce bulletproof code, we are looking for you. To succeed in this role, you must be passionate about delivering high-quality designs and components. You must be creative in solving hard problems and unafraid to think out-of-the-box.\n\n\nThe candidate must be able to:\n1. Solve complex architecture and business problems to come up with extensible solutions.\n2. Write high quality code that are modular, functional and testable.\n3. Formally mentor junior engineers on design, coding and troubleshooting.\n4. Communicate, collaborate and work effectively in a global environment.\n\nAbout the team\nDiverse Experiences\nAmazon values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn t followed a traditional path, or includes alternative experiences, don t let it stop you from applying.\n\nWhy AWS\nAmazon Web Services (AWS) is the world s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating that s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses.\n\nWork/Life Balance\nWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there s nothing we can t achieve.\n\nInclusive Team Culture\nAWS values curiosity and connection. Our employee-led and company-sponsored affinity groups promote inclusion and empower our people to take pride in what makes us unique. Our inclusion events foster stronger, more collaborative teams. Our continual innovation is fueled by the bold ideas, fresh perspectives, and passionate voices our teams bring to everything we do.\n\nMentorship and Career Growth\nWe re continuously raising our performance bar as we strive to become Earth s Best Employer. That s why you ll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional. Several years of non-internship professional software development experience\nSeveral years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nKnowledge of engineering practices and patterns for the full software/hardware/networks development life cycle, including coding standards, code reviews, source control management, build processes, testing, certification, and livesite operations\nExperience programming with at least one software programming language, preferably Java among them.\nBachelors degree or equivalent Several years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nExperience building complex software systems that have been successfully delivered to customers\nExperience designing or architecting (design patterns, reliability and scaling) of new and existing systems\nExperience with full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations",,,,"['Supply chain', 'Cloud computing', 'Automation', 'NoSQL', 'Coding', 'Software development life cycle', 'System design', 'Troubleshooting', 'Internship', 'infrastructure services']",2025-06-12 14:26:53
Architect (ETL),Insightsoftware,10 - 15 years,Not Disclosed,['Hyderabad'],"Insightsoftware (ISW) is a growing, dynamic computer software company that helps businesses achieve greater levels of financial intelligence across their organization with our world-class financial reporting solutions. At insightsoftware, you will learn and grow in a fast-paced, supportive environment that will take your career to the next level. The Data Conversion Specialist is a member of the insightsoftware Project Management Office (PMO) who demonstrates teamwork, results orientation, a growth mindset, disciplined execution, and a winning attitude.\nLocation: Hyderabad (Work from Office)\nWorking Hours: 5:00PM - 2:00AM IST\nPosition Summary\nThe Team Lead, Data Conversion will build positive relationships with customers and serve as a primary point of contact for large scale complex product implementations and business-led change initiatives. The role will include, but is not limited to:\n10+ years of total experience in IT and at least 9+ yrs working experience in any ETL Tool.\nLeading the data conversion team to implement conversion implementation project deliverables.\nCollaborating with client operations and executive teams, confirming business requirements, and aligning to technical requirements.\nUnderstanding project status and facilitate achievement of project objectives and priorities.\nIdentifying process gaps and facilitating continuous refinement of conversion methodology.\nOverseeing data-related initiatives and identifying innovation and improvement opportunities.\nAdapting to and incorporating best practices across the conversion team while documenting and communicating to impacted stakeholders.\nActing as an escalation point for active client engagements.\nFacilitating the accurate and timely integration mapping of client data from source system to our industry-leading platform.\nApplying and staying within the requirements of project governance, standards, and processes.\nWorking in collaboration with project managers in the successful delivery of key projects and objectives.\nDeveloping a deep understanding of conversion products sufficient to facilitate decision-making at various levels.\nCommunicating appropriately to internal and external stakeholders about the project deliverables and key updates.\nTraining and mentoring other data conversion team members.\nExperience performing rapid assessments and being able to distill key themes as early as possible in an assessment process.\nExperience with assessing data and analytic requirements to establish mapping rules from Working experience using various data applications/systems such as Oracle SQL, Excel, .csv files, etc.\nDemonstrated experience automating manual data conversion, migration, or integration processes.\nExperience with real-time, batch, and ETL for complex data conversions.\nStrong knowledge of extract, transform, load (ETL) methodologies for large-scale data conversions, integrations, and/or migrations.\nUtilize data mapping tools to prepare data for data loads based on target system specifications.\nStrong SQL scripting experience.\nA track record of building and maintaining relationships with leaders, both internally and externally.\nDemonstrated ability to scope, develop, test, and implement data conversions.\nProven success implementing data conversion processes.\nAbility to determine priorities daily to achieve business objectives.\nExperience in customer SIT, UAT, migration and go live support.\nExperience in creating cloud migration strategies, defining delivery architecture and creating data migration plans.\nExperience performing rapid assessments and being able to distill key themes as early as possible in an assessment process.\nExperience with assessing data and analytic requirements to establish mapping rules from Working experience using various data applications/systems such as Oracle SQL, Excel, .csv files, etc.\nDemonstrated experience automating manual data conversion, migration, or integration processes.\nExperience with real-time, batch, and ETL for complex data conversions.\nStrong knowledge of extract, transform, load (ETL) methodologies for large-scale data conversions, integrations, and/or migrations.\nUtilize data mapping tools to prepare data for data loads based on target system specifications.\nStrong SQL scripting experience.\nA track record of building and maintaining relationships with leaders, both internally and externally.\nDemonstrated ability to scope, develop, test, and implement data conversions.\nProven success implementing data conversion processes.\nAbility to determine priorities daily to achieve business objectives.\nExperience in customer SIT, UAT, migration and go live support.\nExperience in creating cloud migration strategies, defining delivery architecture and creating data migration plans.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data migration', 'Data conversion', 'Financial reporting', 'Oracle SQL', 'Architecture', 'Project management', 'project governance', 'Team Leader', 'data mapping', 'SQL scripting']",2025-06-12 14:26:55
Senior Software Engineer - Adobe Experience Platform ( AEP ),Wells Fargo,4 - 7 years,Not Disclosed,['Hyderabad'],"About this role:\nWells Fargo is seeking a senior Software Engineer - Adobe Experience Platform (AEP)\nIn this role, you will:\nLead moderately complex initiatives and deliverables within technical domain environments\nContribute to large scale planning of strategies\nDesign, code, test, debug, and document for projects and programs associated with technology domain, including upgrades and deployments",,,,"['Software Engineering', 'Data Science', 'data model', 'data analysis', 'data modeling', 'configuration', 'APIs', 'AEP', 'CDP']",2025-06-12 14:26:57
IN Senior Associate AWS AI/ML/GenAI Developer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\n& Summary We are looking for a seasoned AWS\nAI/ML/GenAI Developer\nResponsibilities\nDesign and implement AI/ML/GenAI models using AWS services such as AWS Bedrock, SageMaker, Comprehend, Rekognition, and others.\nStrong programming skills in Python, R etc\nExperience with machine learning frameworks such as TensorFlow, PyTorch, or Scikitlearn.\nKnowledge of data preprocessing, feature engineering, and model evaluation techniques.\nDevelop and deploy generative AI solutions to solve complex business problems and improve operational efficiency.\nCollaborate with data scientists, engineers, and product teams to understand requirements and translate them into technical solutions.\nOptimize and finetune machine learning models for performance and scalability. Ensure the security, reliability, and scalability of AI/ML solutions by adhering to best practices.\nMaintain and update existing AI/ML models to ensure they meet evolving business needs.\nStay uptodate with the latest advancements in AI/ML and GenAI technologies and integrate relevant innovations into our solutions.\nProvide technical guidance and mentorship to junior developers and team members.\nExcellent problemsolving skills and ability to work in a fastpaced, collaborative environment.\nGood to have AWS Certified Machine Learning Specialty or other relevant AWS certifications.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired 48 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'GCP', 'Data modeling', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Corporate advisory', 'Operations', 'AWS']",2025-06-12 14:27:00
SQL DB Architect,Sutherland Global Services Inc,6 - 12 years,Not Disclosed,['Hyderabad'],"Job Title: SQL Database Architect (SQL Server, Azure SQL Server, SSIS, SSRS, Data Migration, Azure Data Factory, Power BI)\nJob Summary:\nWe are seeking a highly skilled SQL Database Architect with expertise in SQL Server, Azure SQL Server, SSIS, SSRS, Data Migration, and Power BI to design, develop, and maintain scalable database solutions. The ideal candidate will have experience in database architecture, data integration, ETL processes, cloud-based solutions, and business intelligence reporting. Excellent communication and documentation skills are essential for collaborating with cross-functional teams and maintaining structured database records.\nKey Responsibilities:\nDatabase Design & Architecture: Develop highly available, scalable, and secure database solutions using Azure SQL Server.\nETL & Data Integration: Design and implement SSIS packages for data movement, transformations, and automation.\nData Migration: Oversee database migration projects, including on-premises to cloud transitions and cloud to cloud transitions, data conversion, and validation processes.\nAzure Data Factory (ADF): Build and manage data pipelines, integrating various data sources and orchestrating ETL workflows in cloud environments\nReporting & Business Intelligence: Develop SSRS reports and leverage Power BI for creating interactive dashboards and data visualizations.\nPerformance Optimization: Analyze and optimize query performance, indexing strategies, and database configurations.\nCloud Integration: Architect Azure-based database solutions, including Azure SQL Database, Managed Instances, and Synapse Analytics.\nSecurity & Compliance: Ensure data security, encryption, and compliance with industry standards.\nBackup & Disaster Recovery: Design and implement backup strategies, high availability, and disaster recovery solutions.\nAutomation & Monitoring: Utilize Azure Monitor, SQL Profiler, and other tools to automate and monitor database performance.\nCollaboration & Communication: Work closely with developers, BI teams, DevOps, and business stakeholders, explaining complex database concepts in a clear and concise manner.\nDocumentation & Best Practices: Maintain comprehensive database documentation, including design specifications, technical workflows, and troubleshooting guides.\nRequired Skills & Qualifications:\nExpertise in SQL Server & Azure SQL Database.\nExperience with SSIS for ETL processes, data transformations, and automation.\nProficiency in SSRS for creating, deploying, and managing reports.\nStrong expertise in Data Migration, including cloud and on-premises database transitions.\nPower BI skills for developing dashboards, reports, and data visualizations.\nDatabase modeling, indexing, and query optimization expertise.\nKnowledge of cloud-based architecture, including Azure SQL Managed Instance.\nProficiency in T-SQL, stored procedures, Triggers, and database scripting.\nUnderstanding of security best practices, including role-based access control (RBAC).\nExcellent communication skills to explain database solutions to technical and non-technical stakeholders.\nStrong documentation skills to create and maintain database design specs, process documents, and reports.\nPreferred Qualifications:\nKnowledge of CI/CD pipelines for database deployments.\nFamiliarity with Power BI and other data visualization tools.\nExperience with Azure Data Factory and Synapse Analytics for advanced data engineering workflows\n\n\nBachelors or Masters degree in Business, Computer Science, Engineering, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data migration', 'Database design', 'SSRS', 'Stored procedures', 'SSIS', 'Business intelligence', 'Troubleshooting', 'Analytics', 'Monitoring']",2025-06-12 14:27:02
Solution Architect,TechVantage,10 - 20 years,Not Disclosed,['Thiruvananthapuram'],"Solution Architect\nOur company:\nTechvantage.ai is a next-generation technology and product engineering company at the forefront of innovation in Generative AI, Agentic AI , and autonomous intelligent systems . We design intelligent platforms that solve complex business problems and deliver measurable impact through cutting-edge AI technologies.\nRole Overview:\nWe are seeking an experienced Solution Architect with a strong foundation in software architecture and a working knowledge of AI-based products or platforms . In this role, you will be responsible for designing robust, scalable, and secure architectures that support AI-driven applications and enterprise systems.\nYou will work closely with cross-functional teams including data scientists, product managers, and engineering leads to bridge the gap between business needs, technical feasibility, and AI capability.\nWhat we are looking from an ideal candidate?\nArchitect end-to-end solutions for enterprise and product-driven platforms, including components such as data pipelines, APIs, AI model integration, cloud infrastructure, and user interfaces.\nGuide teams in selecting the right technologies, tools, and design patterns to build scalable systems.\nCollaborate with AI/ML teams to understand model requirements and ensure smooth deployment and integration into production.\nDefine system architecture diagrams, data flow, service orchestration, and infrastructure provisioning using modern tools.\nWork closely with stakeholders to translate business needs into technical solutions, with a focus on scalability, performance, and security.\nProvide leadership on best practices for software development, DevOps, and cloud-native architecture.\nConduct architecture reviews and ensure alignment with security, compliance, and performance standards.\nPreferred Skills: What skills do you need?\nRequirements:\n10+ years of experience in software architecture or solution design roles.\nProven experience designing systems using microservices , RESTful APIs , event-driven architecture , and cloud-native technologies.\nHands-on experience with at least one major cloud provider: AWS , GCP , or Azure .\nFamiliarity with AI/ML platforms or components , such as integrating AI models, MLOps pipelines, or inference services.\nUnderstanding of data architectures, including data lakes , streaming , and ETL pipelines.\nStrong experience with containerization ( Docker , Kubernetes ) and DevOps principles.\nAbility to lead technical discussions, make design trade-offs, and communicate with both technical and non-technical stakeholders.\nPreferred Qualifications:\nExposure to AI model lifecycle management , prompt engineering , or real-time inference workflows.\nExperience with infrastructure-as-code (Terraform, Pulumi).\nKnowledge of GraphQL, gRPC, or serverless architectures.\nPrevious experience working in AI-driven product companies or digital transformation programs.\nWhat We Offer:\nHigh-impact role in designing intelligent systems that shape the future of AI adoption\nWork with forward-thinking engineers, researchers, and innovators\nStrong focus on career growth, learning, and technical leadership\nCompensation is not a constraint for the right candidate",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System architecture', 'software architecture', 'Architect', 'Product engineering', 'orchestration', 'Architecture', 'GCP', 'Cloud', 'Infrastructure', 'Solution Architect']",2025-06-12 14:27:04
AWS Technical Architect,Puma Energy,10 - 14 years,35-40 Lacs P.A.,"['Mumbai', 'Pune']","We are seeking a highly experienced AWS Technical Architect to lead the design, implementation, and optimization of cloud infrastructure solutions on the Amazon Web Services (AWS) platform. The ideal candidate will have extensive hands-on experience with AWS services, a deep understanding of cloud architecture best practices, and a proven track record of delivering complex, enterprise-scale cloud solutions. Preferably from Energy, Retail sector.\nProvide technical leadership and strategic direction for the design and implementation of AWS-based solutions.\nCollaborate with cross-functional teams, including business stakeholders, to understand requirements and translate them into robust, scalable, and cost-effective technical solutions.\nDevelop and maintain reference architectures, design patterns, and implementation guidelines for AWS services.\nPerform capacity planning, cost optimization, and performance tuning of AWS environments.\nImplement and automate infrastructure as code (IaC) using tools like AWS CloudFormation, Terraform, or AWS CDK\nEnsure compliance with security, governance, and regulatory requirements.\nMentor and provide technical guidance to development teams on AWS best practices.\nStay up to date with the latest AWS services and features and drive their adoption within the organization.\nParticipate in the evaluation and selection of third-party tools and services for integration with AWS.\nImplement security best practices, including IAM, encryption, and network security measures.\nEnsure solutions comply with relevant regulatory standards and corporate policies.\nConduct regular audits and assessments to maintain cloud environment integrity.\nExperience\nShould have around 8-10 years of experience within IT.\nQualifications\n8-10 years of experience in designing and implementing cloud solutions, with a strong focus on AWS\nExtensive hands-on experience with a wide range of AWS services, including EC2, VPC, ELB, RDS, S3, CloudFront, Lambda, and more.\nDeep expertise in infrastructure as code (IaC) tools like AWS CloudFormation, Terraform, or AWS CDK\nStrong understanding of cloud architecture principles, including high availability, scalability, security, and cost optimization\nExperience with containerization technologies like Docker and container orchestration platforms (e.g., ECS, EKS)\nProficiency in at least one programming language (e.g., Python, Java, Node.js)\nExperience with CI/CD pipelines and automation tools (e.g., AWS CodePipeline, Jenkins)\nStrong problem-solving, analytical, and communication skills\nAWS Certified Solutions Architect (Professional) or equivalent certification is required.\nExperience in leading and mentoring technical teams.\nInternal and External",Industry Type: Oil & Gas,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS', 'Aws Migration', 'Solution Architecting', 'Aws Infrastructure', 'Cloud Architecture', 'Designing And Developing']",2025-06-12 14:27:06
SOFTWARE DEVELOPMENT MANAGER,Amazon,8 - 13 years,Not Disclosed,['Bengaluru'],"you'll join a diverse team of software, hardware, and network engineers, supply chain specialists, security experts, operations managers, and other vital roles. you'll collaborate with people across AWS to help us deliver the highest standards for safety and security while providing seemingly infinite capacity at the lowest possible cost for our customers. And you'll experience an inclusive culture that welcomes bold ideas and empowers you to own them to completion.\n\nCome and be part of the AWS Supply Chain team and build systems that enable AWS to rapidly grow and continue to be the pioneer and widely recognized leader in Cloud Computing, and have direct and immediate impact on the hundreds of thousands developers and businesses around the world who use AWS every day.\n\nSupply Chain Management (AWS_SCM) systems are at heart of ensuring that we can provide our customers with the right computing services, in the right region, at the right time! The SC.os team is responsible for building the end-to-end supply chain automation that enable control of our global footprint for our hyper-growth cloud computing.\n\nAs a technical leader on the team, you will be responsible guiding your team through the design, development, testing, and deployment of a range of products.\nIn addition, you will help build the roadmaps for software teams, developing innovative customer experiences.\nWe are looking for an experienced technical manager who is excited about building industry leading distributed systems in the Supply Chain domain.\nYou will be part of the team that architects, designs, and implements highly scalable distributed systems that provide availability, scalability and latency guarantees.\nThis is a hands on technology position. You need to not only be a top software manager with a good track record of delivering, but also excel in communication, leadership and customer focus.\nThis is a unique and rare opportunity to get in on the ground floor and help shape the supply chain software, product and business.\nA successful candidate will bring deep technical and software expertise and ability to work within a fast moving, startup environment in a large company to deliver solid code that has a broad business impact.\n\nAbout the team\nDiverse Experiences\nAmazon values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn t followe'd a traditional path, or includes alternative experiences, don t let it stop you from applying.\n\nWhy AWS\nAmazon Web Services (AWS) is the world s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating that s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses.\n\nWork/Life Balance\nWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there s nothing we can t achieve.\n\nMentorship and Career Growth\nwe're continuously raising our performance bar as we strive to become Earth s Best Employer. That s why you'll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional.",,,,"['Cloud computing', 'Automation', 'Supply chain management', 'Team management', 'development testing', 'Coding', 'Design development', 'SCM', 'Distribution system', 'infrastructure services']",2025-06-12 14:27:08
Solution Architect,Azine Technologies,5 - 8 years,Not Disclosed,['Ahmedabad'],"We are seeking an experienced Solution Architect to design scalable, high-performance technical solutions aligned with business goals. The role involves collaborating with cross-functional teams and leading architecture strategy and implementation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution Architecting', 'Data Architecture', 'Solution Design', 'System Integration', 'Enterprise Architecture', 'Software Development Life Cycle', 'Cloud Platforms', 'TOGAF', 'Cloud Architecture', 'IT Architecture', 'Enterprise Integration', 'IT Strategy']",2025-06-12 14:27:11
App Dev & Support Engineer III,Conduent,2 - 6 years,Not Disclosed,['Bengaluru'],"Job Track Description:\nCandidate needs to be Oracle Fusion HCM CloudSubject Mater expertin Integration (API/Extract), Fast Formula & Reporting tools.\nAnalyze client requirements and provide design solutions using Oracle Fusion HCM modules.\nEnsure Team customize and configure Fusion HCM applications to align with client business processes.\nEnsure Team develops and maintain reports, interfaces, conversions, and extensions as per project needs.\nLead data migration activities, ensuring accurate and timely transfer of data from legacy systems to Fusion HCM.\nCollaborate with cross-functional teams to integrate Fusion HCM with other enterprise systems.\nProvide technical guidance and support to junior team members.\nStay updated with Oracle Fusion HCM updates and best practices.\nEnsure Team adherence to SOPs - Change Management, Coding Standards, Knowledge Management, Oracle SR, Batch Job Monitoring\nRequires formal education and relevant expertise in professional & technical area.\nPerforms technical-based activities.\nContributes to and manages projects.\nUses deductive reasoning to solve problems and make recommendations.\nInterfaces with and influences key stakeholders.\nLeverages previous knowledge and expertise to achieve results.\nProficiency in independently managing and completing tasks.",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['oracle', 'engineering support', 'oracle fusion hcm', 'oracle fusion', 'change management', 'bi publisher', 'oracle core hr', 'oracle apps technical', 'core hr', 'sql', 'plsql', 'hdl', 'hcm', 'application support', 'linux', 'hcm extracts', 'attendance management', 'payroll', 'oracle hrms']",2025-06-12 14:27:13
AWS DevOps Senior Software Engineer,Care Allianz,3 - 7 years,Not Disclosed,['Thiruvananthapuram'],"Care Allianz is looking for AWS DevOps Senior Software Engineer to join our dynamic team and embark on a rewarding career journey.\n\nyou will be responsible for designing, implementing, and managing end - to - end DevOps processes and infrastructure on the Amazon Web Services (AWS) platform. This role involves leading a team, collaborating with development and operations teams, and ensuring the seamless integration of development and deployment pipelines. Key Responsibilities : DevOps Strategy : Develop and implement a comprehensive DevOps strategy for the organization, focusing on automation, continuous integration, and continuous delivery. Lead the adoption of best practices in DevOps processes. AWS Infrastructure : Design, configure, and manage AWS infrastructure, including EC2 instances, S3 buckets, VPCs, and other services. Implement infrastructure as code (IaC) using tools such as AWS CloudFormation or Terraform. Continuous Integration/Continuous Deployment (CI/CD) : Design, implement, and maintain CI/CD pipelines for multiple applications and environments. Ensure efficient and automated deployment processes, minimizing downtime and errors. Automation : Implement automation scripts and tools to streamline repetitive tasks and enhance operational efficiency. Utilize scripting languages such as Python, Shell, or PowerShell for automation. Monitoring and Logging : Implement monitoring and logging solutions to proactively identify and address issues. Set up and manage logging tools and monitoring dashboards for AWS services. Security and Compliance : Implement security best practices for infrastructure and applications on AWS. Ensure compliance with security policies, industry standards, and regulatory requirements. Team Leadership : Lead and mentor a team of DevOps engineers. Foster a culture of collaboration, innovation, and continuous improvement within the DevOps team. Collaboration : Collaborate with development, operations, and other cross - functional teams to align DevOps processes with overall business goals. Participate in Agile/Scrum processes and contribute to sprint planning and retrospectives.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Career development', 'Networking', 'devops', 'IT infrastructure', 'Manager Technology', 'Healthcare', 'Management', 'Financial services', 'Spectrum', 'Teaching']",2025-06-12 14:27:15
Solution Architect,Amgen Inc,9 - 14 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are seeking a seasoned Solution Architect to drive the architecture, development and implementation of data solutions to Amgen functional groups. The ideal candidate able to work in large scale Data Analytic initiatives, engage and work along with Business, Program Management, Data Engineering and Analytic Engineering teams. Be champions of enterprise data analytic strategy, data architecture blueprints and architectural guidelines. As a Solution Architect, you will play a crucial role in designing, building, and optimizing data solutions to Amgen functional groups such as R&D, Operations and GCO.\nRoles & Responsibilities:\nImplement and manage large scale data analytic solutions to Amgen functional groups that align with the Amgen Data strategy\nCollaborate with Business, Program Management, Data Engineering and Analytic Engineering teams to deliver data solutions\nResponsible for design, develop, optimize, delivery and support of Data solutions on AWS and Databricks architecture\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nProvide expert guidance and mentorship to the team members, fostering a culture of innovation and best practices.\nBe passionate and hands-on to quickly experiment with new data related technologies\nDefine guidelines, standards, strategies, security policies and change management policies to support the Enterprise Data platform.\nCollaborate and align with EARB, Cloud Infrastructure, Security and other technology leaders on Enterprise Data Architecture changes\nWork with different project and application groups to drive growth of the Enterprise Data Platform using effective written/verbal communication skills, and lead demos at different roadmap sessions\nOverall management of the Enterprise Data Platform on AWS environment to ensure that the service delivery is cost effective and business SLAs around uptime, performance and capacity are met\nEnsure scalability, reliability, and performance of data platforms by implementing best practices for architecture, cloud resource optimization, and system tuning.\nCollaboration with RunOps engineers to continuously increase our ability to push changes into production with as little manual overhead and as much speed as possible.\nMaintain knowledge of market trends and developments in data integration, data management and analytics software/tools\nWork as part of team in a SAFe Agile/Scrum model\nBasic Qualifications and Experience:\nMasters degree with 6 - 8 years of experience in Computer Science, IT or related field OR\nBachelors degree with 9 - 12 years of experience in Computer Science, IT or related field OR\nFunctional Skills:\nMust-Have Skills:\n7+ years of hands-on experience in Data integrations, Data Management and BI technology stack.\nStrong experience with one or more Data Management tools such as AWS data lake, Snowflake or Azure Data Fabric\nExpert-level proficiency with Databricks and experience in optimizing data pipelines and workflows in Databricks environments.\nStrong experience with Python, PySpark, and SQL for building scalable data workflows and pipelines.\nExperience with Apache Spark, Delta Lake, and other relevant technologies for large-scale data processing.\nFamiliarity with BI tools including Tableau and PowerBI\nDemonstrated ability to enhance cost-efficiency, scalability, and performance for data solutions\nStrong analytical and problem-solving skills to address complex data solutions\nGood-to-Have Skills:\nPreferred to have experience in life science or tech or consultative solution architecture roles\nExperience working with agile development methodologies such as Scaled Agile.\nProfessional Certifications\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'Data Engineering', 'PySpark', 'Tableau', 'SQL', 'Apache Spark', 'Enterprise Data platform', 'AWS data lake', 'Program Management', 'Databricks architecture', 'Azure Data Fabric', 'PowerBI', 'Snowflake', 'Delta Lake', 'Scaled Agile', 'AWS', 'Python']",2025-06-12 14:27:18
Senior MSBI Developer (SQL & SSRS/SSIS Expert),Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027513\n\nJob Summary\nSynechron is seeking an experienced and detail-oriented Senior MSBI Developerexpertise in MSBI (Microsoft Business Intelligence) to join our data and analytics team. In this role, you will contribute to designing, developing, and maintaining robust reporting and data integration solutions that support our business objectives. Your expertise will help deliver actionable insights, improve decision-making processes, and enhance overall data management efficiency within the organization.\n\nSoftware\n\nRequired\n\nSkills:\nMSBI Suite (including SSIS, SSRS, SSAS)\nSQL Server (including SQL Server Management Studio and Query Performance Tuning)\nVersionsRecent versions of SQL Server (2016 or later preferred)\nProven experience in creating complex reports, data transformation, and integration workflows\nPreferred\n\nSkills:\nPower BI or other visualization tools\nExperience with cloud-based data solutions (e.g., Azure SQL, Synapse Analytics)\nOverall Responsibilities\nDevelop, implement, and maintain MSBI solutions such as SSIS packages, SSRS reports, and data models to meet business requirements\nCollaborate with business stakeholders and data teams to gather reporting needs and translate them into scalable solutions\nOptimize and troubleshoot existing reports and data pipelines to improve performance and reliability\nEnsure data accuracy, security, and compliance within reporting processes\nDocument solution architectures, workflows, and processes for ongoing support and knowledge sharing\nParticipate in team initiatives to enhance data governance and best practices\nContribute to strategic planning for data platform evolution and modernization\nTechnical Skills (By Category)\n\nProgramming Languages:\nRequiredSQL (Advanced proficiency in query writing, stored procedures, and performance tuning)\nPreferredT-SQL scripting for data transformations and automation\nDatabases / Data Management:\nRequiredDeep knowledge of relational database concepts with extensive experience in SQL Server databases\nPreferredFamiliarity with data warehouse concepts, OLAP cubes, and data mart design\nCloud Technologies:\nDesiredBasic understanding of cloud-based data platforms like Azure Data Factory, Azure Synapse\nFrameworks and Libraries:\nNot directly applicable, focus on MSBI tools\nDevelopment Tools and Methodologies:\nExperience working within Agile development environments\nData pipeline development and testing best practices\nSecurity Protocols:\nImplement data security measures, role-based access controls, and ensure compliance with data privacy policies\nExperience\n8 to 10 years of professional experience in software development with substantial hands-on MSBI expertise\nDemonstrated experience in designing and deploying enterprise-level BI solutions\nDomain experience in finance, healthcare, retail, or similar industries is preferred\nAlternative candidacyExtensive prior experience with BI tools and proven success in similar roles may be considered in lieu of exact industry background\nDay-to-Day Activities\nDesign and develop SSIS data integration workflows to automate data loading processes\nCreate and optimize SSRS reports and dashboards for various organizational units\nEngage in troubleshooting and resolving technical issues in existing BI solutions\nCollaborate with data architects, developers, and business analysts to align data solutions with business needs\nConduct code reviews, testing, and validation of reports and data pipelines\nParticipate in scrum meetings, planning sessions, and stakeholder discussions\nEnsure documentation of solutions, processes, and workflows for ease of maintenance and scalability\nQualifications\nBachelors degree or equivalent in Computer Science, Information Technology, or related field\nRelevant certifications in Microsoft BI or SQL Server (e.g., Microsoft Certified Data Engineer Associate) preferred\nOngoing engagement in professional development related to BI, data management, and analytics tools\nProfessional Competencies\nAnalytical mindset with strong problem-solving abilities in data solution development\nCapable of working collaboratively across diverse teams and communicating technical concepts effectively\nStakeholder management skills to interpret and prioritize reporting needs\nAdaptability to evolving technologies and continuous learning mindset\nFocus on delivering high-quality, sustainable data solutions with attention to detail\nEffective time management, prioritizing tasks to meet project deadlines",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['performance tuning', 'stored procedures', 'sql scripting', 'query writing', 'msbi', 'sql server database', 'software development', 'ssas', 'power bi', 'microsoft azure', 'data mart design', 'olap cubes', 'sql server', 'sql azure', 'ssrs', 'data warehousing concepts', 'data transformation', 'ssis']",2025-06-12 14:27:20
Solution Architect,Kanerika Software,10 - 20 years,Not Disclosed,"['Indore', 'Hyderabad', 'Ahmedabad']","Job Summary:\n\nAs a Solution Architect, you will collaborate with our sales, presales and COE teams to provide technical expertise and support throughout the new business acquisition process. You will play a crucial role in understanding customer requirements, presenting our solutions, and demonstrating the value of our products.\nYou thrive in high-pressure environments, maintaining a positive outlook and understanding that career growth is a journey that requires making strategic choices. You possess good communication skills, both written and verbal, enabling you to convey complex technical concepts clearly and effectively. You are a team player, customer-focused, self-motivated, responsible individual who can work under pressure with a positive attitude. You must have experience in managing and handling RFPs/ RFIs, client demos and presentations, and converting opportunities into winning bids. You possess a strong work ethic, positive attitude, and enthusiasm to embrace new challenges. You can multi-task and prioritize (good time management skills), willing to display and learn. You should be able to work independently with less or no supervision. You should be process-oriented, have a methodical approach and demonstrate a quality-first approach.",,,,"['Azure Data Factory', 'Microsoft Fabric', 'Data Analytics', 'Azure Cloud']",2025-06-12 14:27:23
Immediate Joiner / Java Developer,Service based Top B2B MNC in IT Services...,5 - 10 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nBuild high performant, highly scalable, complex and\ndistributed systems end to end\nDevelop simple solutions to address complex\nproblems.\nGive to a future-ready, high quality, and performant\ncode base.\nBring technical solutions to the leadership team,\nfeedback on solutions recommended, new product\nideas with the team through design review, pair\nprogramming, code review and tech talk.\nAbility to share technical solutions and product\nideas with the broader team through design review,\ncode review, proof-of-concepts and show and tell\nParticipate in brainstorming sessions and give ideas\nto our technology, algorithms and products\nImplement new features in a highly collaborative\nenvironment with product managers, UI/UX guides,\nand software and hardware engineers.\nMinimum Qualifications\nBachelor's degree (or above) in\nengineering/computer science with an overall work\nexperience of 5-8 years in in Java/Kotlin\nDevelopment\nPossess advanced knowledge of object-oriented\ndesign and development and data architectures.\nConfirmed ability to communicate with different\nlevels in the organization, influence others and\nbuilding consensus, developing teamwork across\nteams, and seek problems/remove obstacles in a\ntimely manner\n\nBroad Information Technology experience,\nincluding understanding of tools, processes and\nstandard methodologies of project execution\nAdept in coordinating and participating in all\nactivities (analysis, scoping, and design,\ncoding/code reviews, test case reviews, defect\nmanagement, implementation planning /execution\nand support, leading resources and timelines)\nExperience working with and applying Design\npatterns to tackle problems.\nStrong CS fundamentals in algorithms and data\nstructures\nExtensive technical experience and development\nexpertise in Core Java, Koltin, Kotlin Co-routines,\nKafka, Vertx, Nodejs , Java Script, JQuery and\nAJAX, Asynchronous programming .\nExperience doing Object Oriented Analysis and\nDesign, using Domain Driven Design, and Design\nPatterns\nExperience with tools like Maven, Jenkins, Git\nExtensive technical experience and development\nexpertise in data-driven applications utilizing\nsignificant relational database engines/NO SQL\nDatabases as\npart of the overall application architecture\n(experience with any or all of the following helpful:\nMysql Oracle, SQL Server, MongoDB, Cassandra)\nExcellent debugging and testing skills\nAbility to perform performance and scalability\nanalysis as needed.\nGood knowledge of technology and product trends,\nincluding knowledge of what is happening in Open\n\nSource and in other parts of the software\ndevelopment industry\nAbility to work with large-scale distributed systems\nExcellent in problem-solving and multitasking skills\nAbility to work in fast paced environment with good\npartnership capabilities.\nImplement solutions focusing on reuse and industry\nstandards at a program, enterprise or operational\nscope.\nStrong understanding of system performance and\nscaling\nAbility to work in an agile and collaborative setup\nwithin an engineering team.\nPossess excellent communication, sharp analytical\nabilities with validated design skills, able to think\ncritically of the current system in terms of growth\nand stability\nProven ability to mentor other software developers\nto maintain architectural vision and software quality\nStrong desire to build, sense of ownership, urgency,\nand drive\nVerify stability, interoperability, portability, security\nand scalability of java system architecture.\n\nDesired Qualifications\nValidated experience of strong desire to work in\nproduct development\nBe highly flexible and adaptable and demonstrate\npassion for platform development\nExcellent partnership, written and verbal\ncommunication skills\n\nExpertise in delivering high-quality, innovative\napplication\nFamiliar with AWS or other Cloud environment\nConsistent track record of innovation and thought\nleadership.\nObsession with quality and customer experience -\nAttention to detail coupled with ability to think\nabstractly\nFamiliarity with running large scale web services;\nunderstanding of systems internals and networking\nare a plus\nConsistent track record of developing large\nenterprise grade software and delivering high\nquality products/releases on time\nAnalyze performance characteristics to identify\nbottlenecks, failure points, and security holes in\nlarge scale systems. Passionate about performance\nand scalability\nSelf-Starter with the ability to remain flexible and\nlearn new technologies quickly\nCan do attitude and flexibility in pursuing various\nassignments to make the company successful\nConsistent track record of innovation and thought\nleadership.\nObsession with quality and customer experience -\nAttention to detail coupled with ability to think\nabstractly\nThe ability to take convert raw requirements into\ngood design while exploring technical feasibility\ntradeoffs\nProven ability to achieve stretch goals in a highly\ninnovative and fast paced environment\n\nEvaluate current or emerging technologies to\nconsider monetary factors of java program.\n\n\nMandatory skills - Core Java, AWS, , Kafka, OOPS, Maven/Jenkins and No-SQL",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Core Java', 'Kafka', 'AWS', 'Jenkins', 'Maven', 'NoSQL', 'Manves', 'OOPS']",2025-06-12 14:27:25
"Delivery Lead (.NET, C#, AWS, Agile & Cloud Solutions)",Synechron,12 - 15 years,Not Disclosed,"['Pune', 'Bengaluru', 'Hinjewadi']","job requisition idJR1027350\n\nJob Summary\nSynechron is seeking a experienced and strategic Delivery Lead to oversee complex technology projects utilizing .NET, C#, and AWS. This role is instrumental in managing end-to-end project delivery, guiding cross-functional teams, and ensuring alignment with business objectives. The Delivery Lead will drive improvements in delivery efficiency, quality, and stakeholder satisfaction, contributing significantly to the organizations technological growth and operational excellence.\n\nSoftware\n\nRequired\n\nSkills:\nDevelopment and delivery experience with .NET (preferably version 4.7 or later)\nC# programming proficiency\nHands-on experience with Amazon Web Services (AWS) (EC2, S3, Lambda, etc.)\nProject management toolsJira, SharePoint, MS Excel, PowerPoint, Power BI\nAgile and Waterfall project management methodologies\nPreferred\n\nSkills:\nExperience with DevOps/CI-CD pipelines\nKnowledge of Azure cloud platform\nFamiliarity with software release management\nOverall Responsibilities\nEnd-to-End Project Delivery: Manage multiple projects from initiation to closure, ensuring delivery is on time, within scope, and within budget.\nTeam Leadership: Lead and mentor diverse project teams, fostering collaboration and high performance.\nStakeholder Management: Act as the primary point of contact for clients and internal stakeholders, translating business needs into technical solutions.\nGovernance & Compliance: Ensure adherence to organizational policies, standards, and industry best practices.\nTechnical Oversight: Provide guidance on architecture, technology choices, and solution design aligned with best practices.\nProcess Optimization: Continuously identify opportunities to improve delivery processes, increase efficiency, and reduce risk.\nFinancial Oversight: Monitor project budgets, optimize resource utilization, and report on financial performance.\nRisk & Issue Management: Identify, assess, and mitigate risks impacting project delivery.\nPerformance Measurement: Establish metrics and KPIs to measure project success and customer satisfaction.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: C#, .NET Framework/Core\nPreferred: Java, Python (for integration or automation)\nDatabases/Data Management:\nSQL Server, AWS RDS\nCloud Technologies:\nAWS cloud services (EC2, S3, Lambda, CloudWatch)\nFrameworks & Libraries:\n.NET Core / .NET Framework\nRESTful APIs, Microservices architecture\nDevelopment Tools & Methodologies:\nAgile, Scrum, Kanban\nDevOps tools (Jenkins, Azure DevOps, Git)\nSecurity Protocols:\nAWS security best practices\nData privacy and compliance standards\nExperience\n12 to 15 years of professional experience in managing IT projects and delivery teams.\nDemonstrable experience leading large-scale software development and implementation projects.\nStrong background with .NET/C# development, AWS cloud solutions, and cross-functional team management.\nExperience managing global or distributed teams.\nProven stakeholder management experience with senior management and clients.\nPrior exposure to Agile and Waterfall project methodologies.\nAlternative Experience: Candidates with extensive experience in software delivery, cloud migration, or enterprise application implementation may be considered.\n\nDay-to-Day Activities\nConduct project planning, resource allocation, and status reporting.\nHold regular stand-ups, progress reviews, and stakeholder meetings.\nReview development progress, remove blockers, and ensure adherence to quality standards.\nCollaborate with technical teams on architecture design and problem resolution.\nManage change requests, scope adjustments, and project adjustments.\nTrack project KPIs, update dashboards, and communicate progress to leadership.\nOversee risk registers and implement mitigation strategies.\nFacilitate retrospectives and process improvement initiatives.\nQualifications\nBachelors degree in Computer Science, Engineering, or related field; Masters preferred.\nProject Management certifications such as PMP, PMI-ACP, or ScrumMaster are advantageous.\nTraining or certification in AWS or cloud architecture is preferred.\nCommitment to continuous learning and professional development.\nProfessional Competencies\nStrong analytical and problem-solving skills\nEffective leadership and team management capabilities\nExcellent stakeholder communication and negotiation skills\nAbility to adapt to evolving project requirements and technologies\nStrategic thinking and organizational agility\nData-driven decision-making\nPrioritization and time management skills\nChange management and process improvement orientation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud migration', 'c#', 'software development', 'team management', '.net', 'continuous integration', 'azure devops', 'sql', 'java', 'git', 'it projects', 'aws cloud', 'devops', 'waterfall', 'kanban', 'jenkins', 'jira', 'rest', 'python', 'software delivery', 'amazon ec2', 'lambda expressions', 'scrum', 'agile', 'aws']",2025-06-12 14:27:27
Senior ODI Developer (OCI PaaS/IaaS Expertise),Oracle,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","Role Overview:\nWe are seeking a highly skilled Senior ODI Developer with strong hands-on experience in SQL, PL/SQL, and Oracle Data Integrator (ODI) projects, particularly on OCI (Oracle Cloud Infrastructure) PaaS or IaaS platforms. The ideal candidate will design, implement, and optimize ETL processes, leveraging cloud-based solutions to meet evolving business needs. Prior experience in banking or insurance projects is a significant advantage.\nKey Responsibilities:\nDesign, develop, and deploy ETL processes using Oracle Data Integrator (ODI) on OCI PaaS/IaaS.\nConfigure and manage ODI instances on OCI, ensuring optimal performance and scalability.\nDevelop and optimize complex SQL and PL/SQL scripts for data extraction, transformation, and loading.\nImplement data integration solutions, connecting diverse data sources like cloud databases, on-premise systems, APIs, and flat files.\nMonitor and troubleshoot ODI jobs running on OCI to ensure seamless data flow and resolve any issues promptly.\nCollaborate with data architects and business analysts to understand integration requirements and deliver robust solutions.\nConduct performance tuning of ETL processes, SQL queries, and PL/SQL procedures.\nPrepare and maintain detailed technical documentation for developed solutions.\nAdhere to data security and compliance standards, particularly in cloud-based environments.\nProvide guidance and best practices for ODI and OCI-based data integration projects.\nSkills and Qualifications:\nMandatory Skills:\nStrong hands-on experience with Oracle Data Integrator (ODI) development and administration.\nProficiency in SQL and PL/SQL for complex data manipulation and query optimization.\nExperience deploying and managing ODI solutions on OCI PaaS/IaaS environments.\nDeep understanding of ETL processes, data warehousing concepts, and cloud data integration.\nPreferred Experience:\nHands-on experience in banking or insurance domain projects, with knowledge of domain-specific data structures.\nFamiliarity with OCI services like Autonomous Database, Object Storage, Compute, and Networking.\nExperience in integrating on-premise and cloud-based data sources.\nOther Skills:\nStrong problem-solving and debugging skills.\nExcellent communication and teamwork abilities.\nKnowledge of Agile methodologies and cloud-based DevOps practices.\nEducation and Experience:\nBachelors degree in computer science, Information Technology, or a related field.\n5 to 8 years of experience in ODI development, with at least 2 years of experience in OCI-based projects.\nDomain experience in banking or insurance is an added advantage.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Oracle Data Integrator', 'OCI', 'Data Integrator', 'Data Warehousing', 'ETL', 'SQL']",2025-06-12 14:27:29
Professional Services Architect,Splunk,5 - 10 years,Not Disclosed,['Mumbai'],"Description\nJoin us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and seek to deliver the best experience for our customers. At Splunk, we re committed to our work, customers, having fun and most importantly to each other s success. Learn more about Splunk careers and how you can become a part of our journey!\n\nRole\nAs a Professional Services Architect, you will join a world-class team that provides technical and thought leadership through focused solutions within the Customer Success & Experience organisation. This role involves working closely with Splunk s portfolio of products, aligning closely with stakeholders, and acting as the subject-matter expert and Trusted Advisor across\nSplunk s solutions, including Splunk s core, Security, enterprise cloud solutions and Splunk ITSI.",,,,"['Product management', 'Unix', 'Business services', 'Automation', 'Analytical', 'Consulting', 'PHP', 'Application development', 'Windows', 'cisco']",2025-06-12 14:27:32
Java Developer(Immediate Joiner's within 10 Days),Service based Top B2B MNC in IT Services...,6 - 9 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nBuild high performant, highly scalable, complex and\ndistributed systems end to end\nDevelop simple solutions to address complex\nproblems.\nGive to a future-ready, high quality, and performant\ncode base.\nBring technical solutions to the leadership team,\nfeedback on solutions recommended, new product\nideas with the team through design review, pair\nprogramming, code review and tech talk.\nAbility to share technical solutions and product\nideas with the broader team through design review,\ncode review, proof-of-concepts and show and tell\nParticipate in brainstorming sessions and give ideas\nto our technology, algorithms and products\nImplement new features in a highly collaborative\nenvironment with product managers, UI/UX guides,\nand software and hardware engineers.\nMinimum Qualifications\nBachelor's degree (or above) in\nengineering/computer science with an overall work\nexperience of 5-8 years in in Java/Kotlin\nDevelopment\nPossess advanced knowledge of object-oriented\ndesign and development and data architectures.\nConfirmed ability to communicate with different\nlevels in the organization, influence others and\nbuilding consensus, developing teamwork across\nteams, and seek problems/remove obstacles in a\ntimely manner\n\nBroad Information Technology experience,\nincluding understanding of tools, processes and\nstandard methodologies of project execution\nAdept in coordinating and participating in all\nactivities (analysis, scoping, and design,\ncoding/code reviews, test case reviews, defect\nmanagement, implementation planning /execution\nand support, leading resources and timelines)\nExperience working with and applying Design\npatterns to tackle problems.\nStrong CS fundamentals in algorithms and data\nstructures\nExtensive technical experience and development\nexpertise in Core Java, Koltin, Kotlin Co-routines,\nKafka, Vertx, Nodejs , Java Script, JQuery and\nAJAX, Asynchronous programming .\nExperience doing Object Oriented Analysis and\nDesign, using Domain Driven Design, and Design\nPatterns\nExperience with tools like Maven, Jenkins, Git\nExtensive technical experience and development\nexpertise in data-driven applications utilizing\nsignificant relational database engines/NO SQL\nDatabases as\npart of the overall application architecture\n(experience with any or all of the following helpful:\nMysql Oracle, SQL Server, MongoDB, Cassandra)\nExcellent debugging and testing skills\nAbility to perform performance and scalability\nanalysis as needed.\nGood knowledge of technology and product trends,\nincluding knowledge of what is happening in Open\n\nSource and in other parts of the software\ndevelopment industry\nAbility to work with large-scale distributed systems\nExcellent in problem-solving and multitasking skills\nAbility to work in fast paced environment with good\npartnership capabilities.\nImplement solutions focusing on reuse and industry\nstandards at a program, enterprise or operational\nscope.\nStrong understanding of system performance and\nscaling\nAbility to work in an agile and collaborative setup\nwithin an engineering team.\nPossess excellent communication, sharp analytical\nabilities with validated design skills, able to think\ncritically of the current system in terms of growth\nand stability\nProven ability to mentor other software developers\nto maintain architectural vision and software quality\nStrong desire to build, sense of ownership, urgency,\nand drive\nVerify stability, interoperability, portability, security\nand scalability of java system architecture.\n\nDesired Qualifications\nValidated experience of strong desire to work in\nproduct development\nBe highly flexible and adaptable and demonstrate\npassion for platform development\nExcellent partnership, written and verbal\ncommunication skills\n\nExpertise in delivering high-quality, innovative\napplication\nFamiliar with AWS or other Cloud environment\nConsistent track record of innovation and thought\nleadership.\nObsession with quality and customer experience -\nAttention to detail coupled with ability to think\nabstractly\nFamiliarity with running large scale web services;\nunderstanding of systems internals and networking\nare a plus\nConsistent track record of developing large\nenterprise grade software and delivering high\nquality products/releases on time\nAnalyze performance characteristics to identify\nbottlenecks, failure points, and security holes in\nlarge scale systems. Passionate about performance\nand scalability\nSelf-Starter with the ability to remain flexible and\nlearn new technologies quickly\nCan do attitude and flexibility in pursuing various\nassignments to make the company successful\nConsistent track record of innovation and thought\nleadership.\nObsession with quality and customer experience -\nAttention to detail coupled with ability to think\nabstractly\nThe ability to take convert raw requirements into\ngood design while exploring technical feasibility\ntradeoffs\nProven ability to achieve stretch goals in a highly\ninnovative and fast paced environment\n\nEvaluate current or emerging technologies to\nconsider monetary factors of java program.\n\n\nMandatory skills - Core Java, AWS, , Kafka, OOPS, Maven/Jenkins and No-SQL",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Core Java', 'Kafka', 'AWS', 'Jenkins', 'Maven', 'NoSQL', 'Manves', 'OOPS']",2025-06-12 14:27:34
IT Auditor,KPMG Assurance and Consulting Services LLP,2 - 7 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nPerform testing of IT Application Controls, IPE, and Interface Controls through code reviews, IT General Controls review covering areas such as Change Management, Access Management, Backup Management, Incident and Problem Management, SDLC, Data Migration, Batch Job scheduling/monitoring and Business Continuity and Disaster Recovery\nRisk Based IT Internal Audit for Financial Services Entities\nIT SOX 404 Controls Testing, Quality Assurance\nInternal Financial Controls related to IT General Controls as part of Financial Statements Audits\nBusiness Systems Controls / IT Application Controls\nIT Risk & Control Self-Assessment\nAuditing Emerging Technologies such as Cloud Security, Intelligent Automation, RPA, IoT etc.\nWorking knowledge of programming languages(C/C++/Java/SQL)\n\nPreferred candidate profile\nA Bachelor's degree in engineering and approximately 2-7 years of related work experience; or a masters or MBA degree in business, computer science, information systems, engineering\nExpertise in code review skills (e.g., Java, C++, C, SQL, Oracle)\nExperience in performing IT audits of banking/financial sector applications\nGood to have knowledge of other IT regulations, standards and benchmarks used by the IT industry (e.g., NIST, PCI-DSS, ITIL, OWASP, SOX, COBIT, SSAE18/ISAE 3402 etc.)",Industry Type: Management Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['IT Audit', 'ITGC', 'Code Review', 'ITAC']",2025-06-12 14:27:36
ETL Developer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\nResponsibilities:\nDesign and implement the data modeling, data ingestion and data processing for various datasets\nDesign, develop and maintain ETL Framework for various new data source\nDevelop data ingestion using AWS Glue/ EMR, data pipeline using PySpark, Python and Databricks.\nBuild orchestration workflow using Airflow & databricks Job workflow\nDevelop and execute adhoc data ingestion to support business analytics.\nProactively interact with vendors for any questions and report the status accordingly\nExplore and evaluate the tools/service to support business requirement\nAbility to learn to create a data-driven culture and impactful data strategies.\nAptitude towards learning new technologies and solving complex problem.\nQualifications:\nMinimum of bachelors degree. Preferably in Computer Science, Information system, Information technology.\nMinimum 5 years of experience on cloud platforms such as AWS, Azure, GCP.\nMinimum 5 year of experience in Amazon Web Services like VPC, S3, EC2, Redshift, RDS, EMR, Athena, IAM, Glue, DMS, Data pipeline & API, Lambda, etc.\nMinimum of 5 years of experience in ETL and data engineering using Python, AWS Glue, AWS EMR /PySpark and Airflow for orchestration.\nMinimum 2 years of experience in Databricks including unity catalog, data engineering Job workflow orchestration and dashboard generation based on business requirements\nMinimum 5 years of experience in SQL, Python, and source control such as Bitbucket, CICD for code deployment.\nExperience in PostgreSQL, SQL Server, MySQL & Oracle databases.\nExperience in MPP such as AWS Redshift, AWS EMR, Databricks SQL warehouse & compute cluster.\nExperience in distributed programming with Python, Unix Scripting, MPP, RDBMS databases for data integration\nExperience building distributed high-performance systems using Spark/PySpark, AWS Glue and developing applications for loading/streaming data into Databricks SQL warehouse & Redshift.\nExperience in Agile methodology\nProven skills to write technical specifications for data extraction and good quality code.\nExperience with big data processing techniques using Sqoop, Spark, hive is additional plus\nExperience in data visualization tools including PowerBI, Tableau.\nNice to have experience in UI using Python Flask framework anglular\n\n\nMandatory Skills: Python for Insights. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'data bricks', 'aws glue', 'amazon ec2', 'python', 'spark', 'glue', 'amazon redshift', 'cloud platforms', 'aws', 'data engineering', 'sql']",2025-06-12 14:27:39
Sr SQL Developer,HTC Global Services,8 - 13 years,Not Disclosed,['Hyderabad'],"We are looking for a skilled SSIS/T-SQL Developer with 8 years of good expertise. The incumbent will be responsible for developing, deploying and maintaining SSIS packages, as well as writing and optimizing T-SQL queries, stored procedures and functions. The role involves working closely with stakeholders to understand data requirements and deliver high-quality data solutions.\nRequirements:\nStrong understanding of relational database concepts and data modelling.\nExperience with SQL query optimization and performance tuning.",,,,"['T-SQL', 'Performance tuning', 'SQL queries', 'Data migration', 'query optimization', 'Data modeling', 'Debugging', 'Stored procedures', 'SSIS', 'Data warehousing']",2025-06-12 14:27:41
Snowflake Developer,internal,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","We are seeking a skilled Snowflake Developer to design, develop, and manage scalable data solutions using the Snowflake cloud data platform. The ideal candidate will have deep experience in data warehousing, SQL development, ETL processes, and cloud data architecture. Understanding of Control M and Tableau will be an added advantage. 1. Snowflake (Cloud Data Warehouse):1. Good understanding of Snowflake ECO system2. Good experience on Data modeling and Dimensional Modeling and techniques and will be able to drive the Technical discussions with IT & Business and Architects / Data Modelers3. Need to guide the team and provide the technical solutions4. Need to prepare the technical solution and architectures as part of project requirements5. Virtual Warehouse (Compute) - Good Understanding of Warehouse creation & manage6. Data Modeling & Storage - Strong knowledge on LDM/PDM design7. Data Loading/Unloading and Data Sharing- Should have good knowledge8. SnowSQL (CLI)- Expertise and excellent understanding of Snowflake Internals and Integration9. Strong hands on experience on SNOWSQL queries and Stored procedures and performance tuning techniques10. Good knowledge on SNOWSQL Scripts preparation the data validation and Audits11. SnowPipe Good knowledge of Snow pipe implementation12. Expertise and excellent understanding of S3 - Internal data copy/movement13. Good knowledge on Security & Readers and Consumers accounts14. Good knowledge and hands on experience on Query performance tuning implementation techniques2. SQL Knowledge:1. Advance SQL knowledge and hands on experience on complex queries writing using with Analytical functions2. Strong knowledge on stored procedures3. Troubleshooting, problem solving and performance tuning of SQL queries accessing data warehouse",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Part Time, Temporary/Contractual","['Snowflake', 'S3 integration', 'SQL']",2025-06-12 14:27:43
Senior Database Engineer,Cvent,3 - 6 years,Not Disclosed,['Gurugram'],"In This Role, You Will: Design, develop, and manage databases on the AWS cloud platform Develop and maintain automation scripts or jobs to perform routine database tasks such as provisioning, backups, restores, and data migrations\nBuild and maintain automated testing frameworks for database changes and upgrades to minimize the risk of introducing errors\nImplement self-healing mechanisms to automatically recover from database failures or performance degradation\nIntegrate database automation tools with CI/CD pipelines to enable continuous delivery and deployment of database changes",,,,"['Computer science', 'Hospitality', 'Database design', 'database security', 'MySQL', 'Disaster recovery', 'Troubleshooting', 'Event marketing', 'Information technology', 'Python']",2025-06-12 14:27:45
Workday Consultant,Tata Consultancy Services,4 - 9 years,4-8.5 Lacs P.A.,['Bengaluru'],"Dear Candidate,\nGreetings from TATA Consultancy Services!!\nThank you for expressing your interest in exploring a career possibility with the TCS Family.\nHiring For:- Workday\nLocation: Brigade Bhuwalka, Bangalore\nExperience: 4 to 12 years\nExperience in the role include Workday HCM cloud solutions experience including implementation, integrations, data migrations, development in Workday Extend and support (AMS/AD)",,,,"['Workday', 'Workday Functional', 'Workday Hcm']",2025-06-12 14:27:47
"Software Development Engineer II, Global Payments Tech",Amazon,3 - 8 years,Not Disclosed,['Gurugram'],"Want to participate in building the next generation of online payment system that supports numerous countries and payment methods? We are seeking talented software engineers to join one of the fastest growing areas in Amazon s e-commerce services platform. We offer competitive salary and benefits, career and growth opportunities and an exciting and team-oriented atmosphere.\nWe want to move all the money in the world! We deliver game-changing financial processing power for the some of the world s largest technology platforms including Amazon.com, Amazon Kindle, and Amazon Web Services.\nWe build systems that process payments at an unprecedented scale, with accuracy, speed, and mission-critical availability. We innovate to improve customer experience across the globe, with support for currency choice, in-store payments, pay on delivery, credit and debit payments, seller disbursements, gift cards, and many new exciting and challenging ideas are in the works.\nThe Amazon Payments Platform processes millions of transactions every day across numerous countries and payment methods. Over 100 million customers and merchants send tens of billions of dollars moving at light-speed through our systems annually. Come challenge yourself in our team-oriented atmosphere, and watch yourself grow with one of the fastest growing areas of the Amazon e-commerce services platform.\n\n\nDefine, design, and implement multi-tier distributed software applications.\nEstimate engineering effort, plan implementation, and rollout system changes that meet requirements for functionality, performance, scalability, reliability, and adherence to development goals and principles.\nMust be able to independently design code and test major features, as well as work jointly with other team members to deliver complex changes.\nMust be able to effectively collaborate in a fast paced environment with multiple teams in a large organization (software development, QA, Project/Release Management, Build and Release, etc.,).\nProvide on-call production support for payment platform applications 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Production support', 'Software Development Engineer II', 'Coding', 'Architectural design', 'Software development life cycle', 'Customer experience', 'Application software', 'Internship', 'Release management']",2025-06-12 14:27:49
Senior Security Operations Analyst,ZS,4 - 9 years,Not Disclosed,['Pune'],"We are seeking an experienced professional to join our Pune, India office as a Senior Security Operations Analyst with a strong background in Security Information and Event Management (SIEM) platforms, specifically in Microsoft Sentinel and Wiz. The ideal candidate will be responsible for leading advanced threat detection, response, and monitoring activities. This role will be critical in enhancing our cybersecurity posture and ensuring the ZS environment remains secure against emerging threats.\n  What you'll do:",,,,"['Event management', 'Analytical', 'Cloud', 'Management consulting', 'Financial planning', 'Vulnerability management', 'AWS', 'Information technology', 'Operations', 'Analytics']",2025-06-12 14:27:52
"Advisor, Technical Professional Services",Fiserv,10 - 12 years,Not Disclosed,['Noida'],"The Mainframe Advisor is responsible to Design large and complex Data Migration, Data Warehousing, and Business Intelligence Solutions, working in close collaboration with the Project Manager and the Development team. This is a full-time position with career growth opportunities and a competitive benefits package. If you want to in financial institutions and businesses worldwide solve complex business challenges every day, this is the right opportunity for you.\nWhat you will do\nUnder general supervision, analyse conversion requirements and write Conversion Programs.\nInterpret client s existing systems, workflows, and processing parameters.\nMust take complete ownership of the technical delivery the assigned conversion/implementation.\nManages multiple clients and adhere to project timelines.\nMonitors project progress by tracking activity, resolving problems, publishing progress reports, recommending actions in accordance with stated procedure.\nAssists management with the planning and design of improvements to business processes.\nUses sound judgment and experience to solve moderately complex problems based on precedent, example and experience that is commiserate with that of Business Analyst.\nUtilizes system and data to resolve business issues in the most effective manner.\nAnalyses and identifies root cause; providing input to solutions that lead to success of the project.\nCommunicate progress and any potential problems to Project Manager for awareness and/or resolution.\nMaintain the tools used to ensure the efficiency and effectiveness of the conversion process (system studies, timelines, and questionnaires).\nWork in late night shift to provide overlap with US working hours as and when required.\nProvide post implementation support for 1 week. (US shift timing will depend on time zone of client) during Conversion Go Live. 4 times in a year.\nWhat you will need to have\nB. Tech/MCA\n10 to 12 years of experience in IT Industry.\nExcellent Programming skills on IBM Mainframe Programming, COBOL, TSO, JCL.\nGood understanding of mainframe files\nShould have good understanding of activities performed in conversion/implementation of core Banking application.\nGood knowledge in identifying valid business scenarios, business workflows and business process.\nKnowledge of Banking domain.\nExperience of estimating data migration projects.\nExperienced problem solving and debugging skills.\nGood verbal and written communication and interpersonal skills\nWhat would be great to have\nExperience supporting Banking Core Conversions.\nExperience on Account Processing core is a plus.\nExposure to Banking and Financial Services industry with a good understanding of Banking Products, Services & Procedures.\nStrong analytical skills, good verbal and written communication skills and the ability to interact professionally with a diverse group.\nLeadership and mentoring skills.\nProficiency with Excel.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business process', 'TSO', 'JCL', 'Data migration', 'Cobol', 'Debugging', 'Business intelligence', 'Financial services', 'Mainframes', 'Core banking']",2025-06-12 14:27:54
App Dev & Support Engineer II,Conduent,2 - 5 years,Not Disclosed,['Bengaluru'],"Job Track Description:\n\n\nCandidate needs to be Oracle Fusion HCM Cloud\\u202FSubject Mater expert\\u202Fin Integration ( API/Extract ), Fast Formula & Reporting tools. ( OTBI , BI Publisher )\n\n\nAnalyze client requirements and provide design solutions using Oracle Fusion HCM modules.\n\n\nEnsure Team customize and configure Fusion HCM applications to align with client business processes.\n\n\nEnsure Team develops and maintain reports, interfaces, conversions, and extensions as per project needs.\\u202F\\u202F\n\n\nLead data migration activities, ensuring accurate and timely transfer of data from legacy systems to Fusion HCM.\n\n\nCollaborate with cross-functional teams to integrate Fusion HCM with other enterprise systems.\n\n\nProvide technical guidance and support to junior team members.\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\n\n\nStay updated with Oracle Fusion HCM updates and best practices.\n\n\nEnsure Team adherence to SOPs - Change Management, Coding Standards, Knowledge Management, Oracle SR, Batch Job Monitoring\n\n\nRequires formal education and relevant expertise in professional & technical area.\n\n\nPerforms technical-based activities.\n\n\nContributes to and manages projects.\n\n\nUses deductive reasoning to solve problems and make recommendations.\n\n\nInterfaces with and influences key stakeholders.\n\n\nLeverages previous knowledge and expertise to achieve results.\\u202F\n\n\nProficiency in independently managing and completing tasks.",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['oracle', 'engineering support', 'oracle fusion hcm', 'change management', 'sql', 'bi publisher', 'networking', 'oracle fusion', 'plsql', 'production support', 'technical support', 'unix shell scripting', 'application support', 'linux', 'desktop support', 'hcm extracts', 'troubleshooting', 'unix']",2025-06-12 14:27:56
"Senior Engineer, Application Development",S&P Global Market Intelligence,5 - 8 years,Not Disclosed,['Hyderabad'],"Grade Level (for internal use):\n10\nMarket Intelligence\nThe Role: Senior Full Stack Developer\nGrade level :10\nThe Team: You will work with a team of intelligent, ambitious, and hard-working software professionals. The team is responsible for the architecture, design, development, quality, and maintenance of the next-generation financial data web platform. Other responsibilities include transforming product requirements into technical design and implementation. You will be expected to participate in the design review process, write high-quality code, and work with a dedicated team of QA Analysts, and Infrastructure Teams\nThe Impact: Market Intelligence is seeking a Software Developer to create software design, development, and maintenance for data processing applications. This person would be part of a development team that manages and supports the internal & external applications that is supporting the business portfolio. This role expects a candidate to handle any data processing, big data application development. We have teams made up of people that learn how to work effectively together while working with the larger group of developers on our\nplatform.\nWhats in it for you:\nOpportunity to contribute to the development of a world-class Platform Engineering team .\nEngage in a highly technical, hands-on role designed to elevate team capabilities and foster continuous skill enhancement.\nBe part of a fast-paced, agile environment that processes massive volumes of dataideal for advancing your software development and data engineering expertise while working with a modern tech stack.\nContribute to the development and support of Tier-1, business-critical applications that are central to operations.\nGain exposure to and work with cutting-edge technologies including AWS Cloud , EMR and Apache NiFi .\nGrow your career within a globally distributed team , with clear opportunities for advancement and skill development.\nResponsibilities:\nDesign and develop applications, components, and common services based on development models, languages and tools, including unit testing, performance testing and monitoring and implementation\nSupport business and technology teams as necessary during design, development and delivery to ensure scalable and robust solutions\nBuild data-intensive applications and services to support and enhance fundamental financials in appropriate technologies.( C#, .Net Core, Databricsk, Spark ,Python, Scala, NIFI , SQL)\nBuild data modeling, achieve performance tuning and apply data architecture concepts\nDevelop applications adhering to secure coding practices and industry-standard coding guidelines, ensuring compliance with security best practices (e.g., OWASP) and internal governance policies.\nImplement and maintain CI/CD pipelines to streamline build, test, and deployment processes; develop comprehensive unit test cases and ensure code quality\nProvide operations support to resolve issues proactively and with utmost urgency\nEffectively manage time and multiple tasks\nCommunicate effectively, especially written with the business and other technical groups\nWhat Were Looking For:\nBasic Qualifications:\nBachelorsMasters Degree in Computer Science, Information Systems or equivalent.\nMinimum 5 to 8 years of strong hand-development experience in C#, .Net Core, Cloud Native, MS SQL Server backend development. Proficiency with Object Oriented Programming.\nAdvance SQL programming skills\nPreferred experience or familiarity with tools and technologies such as Odata, Grafana, Kibana, Big Data platforms, Apache Kafka, GitHub, AWS EMR, Terraform, and emerging areas like AI/ML and GitHub Copilot.\nHighly recommended skillset in Databricks, SPARK, Scalatechnologies.\nUnderstanding of database performance tuning in large datasets\nAbility to manage multiple priorities efficiently and effectively within specific timeframes\nExcellent logical, analytical and communication skills are essential, with strong verbal and writing proficiencies\nKnowledge of Fundamentals, or financial industry highly preferred.\nExperience in conducting application design and code reviews\nProficiency with following technologies:\nObject-oriented programming\nPrograming Languages (C#, .Net Core)\nCloud Computing\nDatabase systems (SQL, MS SQL)\nNice to have: No-SQL (Databricks, Spark, Scala, python), Scripting (Bash, Scala, Perl, Powershell)\nPreferred Qualifications:\nHands-on experience with cloud computing platforms including AWS , Azure , or Google Cloud Platform (GCP) .\nProficient in working with Snowflake and Databricks for cloud-based data analytics and processing.\nBenefits:\nHealth & Wellness: Health care coverage designed for the mind and body.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GitHub Copilot', 'AI/ML', 'Kibana', 'python', 'GitHub', 'Scala', 'AWS EMR', 'Grafana', 'Odata', 'Big Data platforms', 'Terraform', 'Apache Kafka', 'Databricks', 'Spark']",2025-06-12 14:27:58
"Startup Account Manager, North",Amazon,10 - 15 years,Not Disclosed,['Gurugram'],"Sales, Marketing and Global Services (SMGS)\n\nAWS Sales, Marketing, and Global Services (SMGS) is responsible for driving revenue, adoption, and growth from the largest and fastest growing smalland mid-market accounts to enterprise-level customers including public sector.\n\nAmazon Web Services (AWS) offers a set of cloud services that enable all companies, from startups to enterprises, to run virtually everything in the cloud, including mobile applications, big data analytics, AI/ML platforms, and microservices/serverless infrastructures. AWS India Pvt. Ltd. , the reseller for cloud services in India, is looking for a Senior Startup Account Manager to help drive the growth of high-potential startups in India.\nYou need to possess passion about Startups, be a self-starter with a strong entrepreneurial spirit who is prepared to work in a fast-paced, often ambiguous environment, execute against ambitious goals, and consistently embrace the Amazon Culture. Your responsibilities will include driving growth and user adoption, migrations and ensuring startups select AWS as their preferred cloud provider in India. You will work closely with counterparts in business development, marketing, solution architecture and partner teams to lead execution of BD plays.\n\nThe candidate should have technical background that enables him/her to drive engagement at the CXO level as well as with software developers and IT architects. The candidate should be an exceptional analytical thinker who thrives in fast-paced dynamic environments and has excellent communication and presentation skills. The candidate should be visioning and executing via collaboration with an extended team to address all startup s needs.\n\n\nEnsure customer success with early and growth stage startups in India\nDrive growth and market share in a defined territory\nAccelerate customer adoption through well-developed BD engagements\nDevelop and execute against a comprehensive account/territory plan.\nCreate & articulate compelling value propositions around AWS services.\nAccelerate customer adoption by engaging Founders, CXO, Board of Directors and VC influencers\nWork with AWS partners to manage joint selling opportunities\nAssist customers in identifying use cases for priority adoption of AWS as well as best practices implementations\nDevelop long-term strategic relationships with key accounts.\n\nA day in the life\nMeet startup CXOs and help them Build on AWS\nLeverage AWS startup programs to support early stage startups to bring idea to market\nTrack investments, technology trends; build coverage plans and oversee execution\nCollaborate with cross functional teams such as Sales, VC BD, Solutions Architect, Partners, Marketing\nEnsure high standards and maintain sales pipeline hygiene\n\nAbout the team\nThe AWS Startups team partners with startups around the world to build, launch, grow, and help scale their business. We don t just support startups with cloud infrastructure, but also partner with our startup customers throughout their journey by providing resources to tackle challenges from early stage fundraising to building technical teams and developing startup culture.\n\nAbout AWS\n\nDiverse Experiences\nAWS values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn t followed a traditional path, or includes alternative experiences, don t let it stop you from applying.\n\nWhy AWS?\nAmazon Web Services (AWS) is the world s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating that s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses.\n\nInclusive Team Culture\nHere at AWS, it s in our nature to learn and be curious. Our employee-led affinity groups foster a culture of inclusion that empower us to be proud of our differences. Ongoing events and learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences, inspire us to never stop embracing our uniqueness.\n\nMentorship & Career Growth\nWe re continuously raising our performance bar as we strive to become Earth s Best Employer. That s why you ll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional.\n\nWork/Life Balance\nWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there s nothing we can t achieve in the cloud. 10+ years of technology experience with a focus on field BD (quota-carrying) Experience in working with Startups in identifying, developing, negotiating, and closing large-scale technology deals.\nExperience in positioning and selling technology to new customers and in new market segments. Experience in proactively growing customer relationships within an account while expanding their understanding of the customer s business.\nExcellent verbal and written communications skills Functioned in an environment where they managed an account list in technology which included large growth in net new opportunities.\nProven track record of consistent territory growth and quota attainment. BA/BS/B.Tech degree required. Masters or MBA is a plus.\nUnderstanding of AWS and/or technology as a service (Iaas,SaaS,PaaS) is preferred.",,,,"['Solution architecture', 'Territory growth', 'Cloud computing', 'big data analytics', 'Cloud Services', 'Analytical', 'PAAS', 'cxo', 'Mobile applications', 'Solution Architect']",2025-06-12 14:28:00
Senior Cloud Engineering Lead,ZS,6 - 11 years,Not Disclosed,['Pune'],"As a Senior Cloud Engineering Lead , you will be an individual contributor and subject matter expert who maintains and participates in the design and implementation of technology solutions. The engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients needs. This position will be working with the latest Amazon Web Services technologies around cloud architecture, infrastructure automation, and network security.\n  What you'll Do:",,,,"['Product management', 'Automation', 'Financial planning', 'Management consulting', 'Cloud', 'Network security', 'Agile', 'Application development', 'Python']",2025-06-12 14:28:02
Oracle/ Informatica/ PLSQL/ ETL/ Snaplogic,Photon,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Job Description:\n\nRole: Orcale, Informatica, PLSQL, ETL\nLocation: Chennai/ Bangalore\nExperience: 5+ Years\nMust have: Orcale, Informatica, PLSQL, ETL\n\nLooking for a candidate with expertise on Oracle Database,  Snaplogic and Oracle PL/SQL with knowledge on AWS cloud.",,,,"['Snaplogic', 'PLSQL', 'Informatica', 'ETL', 'ORACE']",2025-06-12 14:28:05
MSD365 Finance Technical,Hexaware Technologies,6 - 11 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']",MS Dynamics365 Finance Technical\nHybrid\nPAN India\nRequired:\nIntegration and Data Management:\nDesign and implement integrations with other systems.\nData consistency and accuracy during data migration and transformation.,,,,"['D365', 'MSD365', 'MICROSOFT DYNAMICS', 'X++', 'F&O']",2025-06-12 14:28:07
MS Dynamics365 Finance Functional,Hexaware Technologies,6 - 11 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","MS Dynamics365 Finance Functional\nHybrid\nPAN India\n\nDescription:\nWe are looking for a highly skilled Microsoft Dynamics Functional Consultant to join our team.\nThe ideal candidate will have in-depth knowledge of Microsoft Dynamics 365 (Finance and Operations, Customer Engagement, or other modules) and a strong understanding of business processes.",,,,"['MSD365', 'MS dynamics 365', 'MSDynamics365', 'F&O']",2025-06-12 14:28:09
Lead .Net Developer,Conduent,5 - 9 years,Not Disclosed,['Bengaluru'],"Must have skills: \nHands on experience in design and development of Windows based applications using C#, .Net Core, WPF and WCF, Java, Python and React.js /Angular.\nUnderstanding of Apigee /API Gateway concepts for managing and securing APIs.\nExperience with SignalR for real-time communication in.NET applications.\nWorking knowledge of Kubernetes for container orchestration and managing microservices deployments.\n\n :",,,,"['design patterns', 'data structures', 'presentation skills', 'product design', 'ooad', 'kubernetes', 'c++', 'api gateway', 'socket programming', 'unit testing', 'apigee', 'react.js', 'java', 'wcf', 'sdlc process', 'c#', 'python', 'microsoft azure', 'stl', 'mfc', 'wpf', 'angular', '.net core', 'nunit', 'aws', 'sdlc']",2025-06-12 14:28:11
Senior Sap Fico Consultant,IT service and consulting,6 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","We are actively looking out for the candidate having 6- years of experience into SAP FICO\nJob Title - Senior SAP FICO Consultant\nLocation - Bangalore / Pune/ Hyderabad\nWork mode - Hybrid\nNotice period - 15-30 days / Serving notice period\n\nRequired Skills :-\nSAP FI (Finance):\nGeneral Ledger (GL), Accounts Payable (AP), Accounts Receivable (AR), Asset Accounting (AA)\nBank Accounting, Electronic Bank Statement (EBS)\nIntegration with MM and SD\nTax configuration (GST/VAT, Withholding Tax, etc.)\nAutomatic payment program (APP)\nDocument Splitting, Parallel Ledger, Special Purpose Ledger\nSAP CO (Controlling):\nCost Center Accounting (CCA)\nInternal Orders\nProfit Center Accounting (PCA)\nProduct Costing, Profitability Analysis (COPA)\nActivity-Based Costing (ABC)\nIntegration with FI, PP, and SD\nTechnical/Functional Skills:\nStrong SAP FICO configuration and process design experience\nPreparation of Functional Specification Documents (FSDs)\nUnderstanding of ABAP debugging and basic technical design\nExperience with SAP S/4HANA is highly preferred (especially Universal Journal, New Asset Accounting, etc.)\nKnowledge of SAP Fiori apps relevant to finance\nExperience in LSMW, BAPIs, BDCs, and other data migration tools\nWorking knowledge of IDocs, interfaces, and third-party integrations\nGood understanding of authorization concepts and roles in FICO",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP FICO Implementation', 'Sap Configuration', 'SAP Implementation', 'IDOCS', 'mm', 'Sap S Hana', 'gst', 'Profit Center Accounting', 'Bdcs', 'ap', 'ar', 'sd', 'LSMW', 'FSD', 'General Ledger', 'Internal Orders', 'BAPIs', 'Copa', 'Cost Center Accounting', 'Activity Based Costing']",2025-06-12 14:28:14
Senior SAP MM Functional Consultant,Sopra Steria,14 - 18 years,Not Disclosed,['Chennai'],"Role & responsibilities\nLead the workshop and identify the GAP\nShould be autonomous, able to challenge the Business, to orient the business on core solutions without customizations\nConfigure and customize the SAP MM module in alignment with client-specific needs\nLead full-cycle SAP MM implementation projects, including requirements gathering, design, testing, training, and post-implementation support.",,,,"['SAP MM', 'implementation', 'Hana Implementation', 'Sap Mm Hana', 'SAP MM Implementation']",2025-06-12 14:28:16
SAP MM Senior Consultant,Sopra Steria,14 - 18 years,Not Disclosed,['Noida'],"We are looking for a highly experienced Senior SAP MM Consultant with deep expertise in S/4HANA Cloud to join our team. The ideal candidate will bring 14+ years of SAP MM implementation experience, including hands-on involvement in Greenfield S/4HANA projects (preferably in European contexts), and possess the ability to lead end-to-end project life cycles, collaborate with global stakeholders, and ensure solution excellence across business processes.\nLocation: Chennai / Noida\nExperience: 14 - 18 years",,,,"['Computer science', 'variant configuration', 'Data migration', 'SAP MM', 'VAT', 'Stakeholder management', 'Information technology', 'ABAP', 'Analytics', 'Logistics']",2025-06-12 14:28:18
Womens walkin- Automation testing/ETL/UX on 13th June @Pune,Infosys BPM,2 - 4 years,Not Disclosed,['Pune'],"Greeting from Infosys BPM Ltd,\n\nExclusive Women's Walkin drive\n\nWe are hiring for UX with JavaScript, ETL Testing + Python Programming, Automation Testing with Java, Selenium, BDD, Cucumber, ETL DB Testing, ETL Testing Automation skills. Please walk-in for interview on 13th June 2025 at Pune location\n\n\nNote: Please carry copy of this email to the venue and make sure you register your application before attending the walk-in. Please use below link to apply and register your application. Please mention Candidate ID on top of the Resume ***\n\n\nhttps://career.infosys.com/jobdesc?jobReferenceCode=PROGEN-HRODIRECT-215163\n\n\nInterview details\nInterview Date: 13th June 2025\nInterview Time: 10 AM till 1 PM\n\n\nInterview Venue:\nPune:: Hinjewadi Phase 1\nInfosys BPM Limited, Plot No. 1, Building B1, Ground floor, Hinjewadi Rajiv Gandhi Infotech Park, Hinjewadi Phase 1, Pune, Maharashtra-411057\n\n\nPlease find below Job Description for your reference:\n\n\nWork from Office***\nMin 2 years of experience on project is mandate***\nJob Description: UX with JavaScript\nTechnical Design tools (e.g., Photoshop, XD, Figma), strong knowledge of HTML, CSS, JavaScript, and experience with SharePoint customization.\nExperience with Wireframe, Prototype, intuitive, responsive design, Documentation,\nAble to Lead the Team\nNice to have Understanding of SharePoint Framework (SPFx) and modern SharePoint development.\n\nJob Description: ETL Testing + Python Programming\nExperience in Data Migration Testing (ETL Testing), Manual & Automation with Python Programming. Strong on writing complex SQLs for data migration validations.\nWork experience with Agile Scrum Methodology\nFunctional Testing- UI Test Automation using Selenium, Java\nFinancial domain experience\nGood to have AWS knowledge\n\nJob Description: Automation Testing with Java, Selenium, BDD, Cucumber\nHands on exp in Automation.\nJava, Selenium, BDD , Cucumber expertise is mandatory.\nBanking Domian Experience is good.\nFinancial domain experience\nAutomation Talent with TOSCA skills, Payment domain skills is preferable.\n\nJob Description: ETL DB Testing\nStrong experience in ETL testing, data warehousing, and business intelligence.\nStrong proficiency in SQL.\nExperience with ETL tools (e.g., Informatica, Talend, AWS Glue, Azure Data Factory).\nSolid understanding of Data Warehousing concepts, Database Systems and Quality Assurance.\nExperience with test planning, test case development, and test execution.\nExperience writing complex SQL Queries and using SQL tools is a must, exposure to various data analytical functions.\nFamiliarity with defect tracking tools (e.g., Jira).\nExperience with cloud platforms like AWS, Azure, or GCP is a plus.\nExperience with Python or other scripting languages for test automation is a plus.\nExperience with data quality tools is a plus.\nExperience in testing of large datasets.\nExperience in agile development is must\nUnderstanding of Oracle Database and UNIX/VMC systems is a must\n\nJob Description: ETL Testing Automation\nStrong experience in ETL testing and automation.\nStrong proficiency in SQL and experience with relational databases (e.g., Oracle, MySQL, PostgreSQL, SQL Server).\nExperience with ETL tools and technologies (e.g., Informatica, Talend, DataStage, Apache Spark).\nHands-on experience in developing and maintaining test automation frameworks.\nProficiency in at least one programming language (e.g., Python, Java).\nExperience with test automation tools (e.g., Selenium, PyTest, JUnit).\nStrong understanding of data warehousing concepts and methodologies.\nExperience with CI/CD pipelines and version control systems (e.g., Git).\nExperience with cloud-based data warehouses like Snowflake, Redshift, BigQuery is a plus.\nExperience with data quality tools is a plus.\n\n\n\nREGISTRATION PROCESS:\nThe Candidate ID & SHL Test(AMCAT ID) is mandatory to attend the interview. Please follow the below instructions to successfully complete the registration. (Talents without registration & assessment will not be allowed for the Interview).\n\nCandidate ID Registration process:\nSTEP 1: Visit: https://career.infosys.com/joblist\nSTEP 2: Click on ""Register"" and provide the required details and submit.\nSTEP 3: Once submitted, Your Candidate ID(100XXXXXXXX) will be generated.\nSTEP 4: The candidate ID will be shared to the registered Email ID.\n\nSHL Test(AMCAT ID) Registration process:\nThis assessment is proctored, and talent gets evaluated on Basic analytics, English Comprehension and writex (email writing).\n\nSTEP 1: Visit: https://apc01.safelinks.protection.outlook.com/?url=https://autologin-talentcentral.shl.com/?link=https://amcatglobal.aspiringminds.com/?data=JTdCJTIybG9naW4lMjIlM0ElN0IlMjJsYW5ndWFnZSUyMiUzQSUyMmVuLVVTJTIyJTJDJTIyaXNBdXRvbG9naW4lMjIlM0ExJTJDJTIycGFydG5lcklkJTIyJTNBJTIyNDE4MjQlMjIlMkMlMjJhdXRoa2V5JTIyJTNBJTIyWm1abFpUazFPV1JsTnpJeU1HVTFObU5qWWpRNU5HWTFOVEU1Wm1JeE16TSUzRCUyMiUyQyUyMnVzZXJuYW1lJTIyJTNBJTIydXNlcm5hbWVfc3E5QmgxSWI5NEVmQkkzN2UlMjIlMkMlMjJwYXNzd29yZCUyMiUzQSUyMnBhc3N3b3JkJTIyJTJDJTIycmV0dXJuVXJsJTIyJTNBJTIyJTIyJTdEJTJDJTIycmVnaW9uJTIyJTNBJTIyVVMlMjIlN0Q=&apn=com.shl.talentcentral&ibi=com.shl.talentcentral&isi=1551117793&efr=1&data=05|02|omar.muqtar@infosys.com|a7ffe71a4fe4404f3dac08dca01c0bb3|63ce7d592f3e42cda8ccbe764cff5eb6|0|0|638561289526257677|Unknown|TWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0=|0|||&sdata=s28G3ArC9nR5S7J4j/V1ZujEnmYCbysbYke41r5svPw=&reserved=0\n\nSTEP 2: Click on ""Start new test"" and follow the instructions to complete the assessment.\nSTEP 3: Once completed, please make a note of the AMCAT ID( Access you Amcat id by clicking 3 dots on top right corner of screen).\n\nNOTE:\nDuring registration, you'll be asked to provide the following information:\nPersonal Details: Name, Email Address, Mobile Number, PAN number.\nAvailability: Acknowledgement of work schedule preferences (Shifts, Work from Office, Rotational Weekends, 24/7 availability, Transport Boundary) and reason for career change.\nEmployment Details: Current notice period and total annual compensation (CTC) in the format 390000 - 4 LPA (example).\nCandidate Information: 10-digit candidate ID starting with 100XXXXXXX, Gender, Source (e.g., Vendor name, Naukri/LinkedIn/Found it, or Direct), and Location\nInterview Mode: Walk-in\nAttempt all questions in the SHL Assessment app.\nThe assessment is proctored, so choose a quiet environment.\nUse a headset or Bluetooth headphones for clear communication.\nA passing score is required for further interview rounds.\n5 or above toggles, multi face detected, face not detected, or any malpractice will be considered rejected\nOnce you've finished, submit the assessment and make a note of the AMCAT ID (15 Digit) used for the assessment.\n\nDocuments to Carry:\nPlease have a note of Candidate ID & AMCAT ID along with registered Email ID.\nPlease do not carry laptops/cameras to the venue as these will not be allowed due to security restrictions.\nPlease carry 2 set of updated Resume/CV (Hard Copy).\nPlease carry original ID proof for security clearance.\nPlease carry individual headphone/Bluetooth for the interview.\n\nPointers to note:\nPlease do not carry laptops/cameras to the venue as these will not be allowed due to security restrictions.\nOriginal Government ID card is must for Security Clearance.\n\nRegards,\nInfosys BPM Recruitment team.",Industry Type: BPM / BPO,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['ETL DB', 'UX', 'Automation Testing', 'ETL', 'ETL Testing', 'Javascript', 'Python']",2025-06-12 14:28:20
Transition Analyst,Capco,5 - 7 years,8-10 Lacs P.A.,['Pune'],"About Us\nCapco, a Wipro company, is a global technology and management consulting firm. Awarded with Consultancy of the year in the British Bank Award and has been ranked Top 100 Best Companies for Women in India 2022 by Avtar & Seramount. With our presence across 32 cities across globe, we support 100+ clients across banking, financial and Energy sectors. We are recognized for our deep transformation execution and delivery.\nWHY JOIN CAPCO?\nYou will work on engaging projects with the largest international and local banks, insurance companies, payment service providers and other key players in the industry. The projects that will transform the financial services industry.\nMAKE AN IMPACT\nInnovative thinking, delivery excellence and thought leadership to help our clients transform their business. Together with our clients and industry partners, we deliver disruptive work that is changing energy and financial services.\n#BEYOURSELFATWORK\nCapco has a tolerant, open culture that values diversity, inclusivity, and creativity.\nCAREER ADVANCEMENT\nWith no forced hierarchy at Capco, everyone has the opportunity to grow as we grow, taking their career into their own hands.\nDIVERSITY & INCLUSION\nWe believe that diversity of people and perspective gives us a competitive advantage.\nMAKE AN IMPACT\nJob Title: Transition Analyst\nLocation: Pune\nExperience Required:\nBachelorâ€™s degree is required.\nMinimum 6 to 8 years of total work experience.\nMinimum 1 to 2 years of relevant experience in Project/Program Management or Support roles.\nTechnical & Functional Expertise:\nTechnical:\nProficiency in MS Office products including Office 365, Project Online, SharePoint, Power BI, and other analytics tools.\nStrong understanding of process workflow design, data architecture, and related tools.\nFunctional:\nStrong business acumen and functional understanding.\nExperience in planning and monitoring for program workstreams, project deliverables, and reporting.\nAbility to handle transition-related documentation, administrative tasks, risk management, due diligence, and stakeholder coordination.\nKey Responsibilities:\nSupport planning and execution of program and transition projects.\nTrack deliverables, manage risks, and ensure timely reporting.\nEnsure compliance with GBS methodologies and toolkits.\nManage travel and logistics for transition-related requirements.\nCoordinate with operational teams and business functions for successful transitions.\nLead the documentation of SOPs and manage sign-off processes.\nCollaborate with various business units including Procurement, Finance, and IT.\nSupport project reporting, dashboard preparation, and Power BI-based analytics.\nHandle highly confidential material with discretion and professionalism.\nParticipate in customer-facing meetings and internal stakeholder communications.\nFacilitate workshops, team meetings, and process improvement initiatives.\nKey Challenges:\nNavigating fragmented systems and tools.\nEngaging a wide range of stakeholders across global functions.\nManaging services at a large scale with geographical and cultural diversity.\nAdapting to evolving digital technologies and technical tools.\nEnsuring alignment with global process design standards.\nSkills & Competencies:\nCore Skills:\nProject planning and reporting skills\nWorkflow and process documentation\nRisk identification and mitigation\nData visualization and reporting tools (especially Power BI)\nSoft Skills:\nExcellent multitasking and prioritization skills\nStrong interpersonal, presentation, and written communication skills\nFluency in English (spoken and written)\nKnowledge of local regulations and compliance standards\nFamiliarity with Puneâ€™s local business environment\nAbility to work effectively in a regional service center ecosystem",Industry Type: BPM / BPO,Department: Consulting,"Employment Type: Full Time, Permanent","['Project Management', 'Transition Management', 'Process Transition', 'Process Migration', 'Transition Planning', 'Business Transition', 'Project Governance', 'Project Transition']",2025-06-12 14:28:23
Transistion Analyst,Capco,1 - 8 years,Not Disclosed,['Pune'],"About Us\nCapco, a Wipro company, is a global technology and management consulting firm. Awarded with Consultancy of the year in the British Bank Award and has been ranked Top 100 Best Companies for Women in India 2022 by\nAvtar & Seramount\n. With our presence across 32 cities across globe, we support 100+ clients across banking, financial and Energy sectors. We are recognized for our deep transformation execution and delivery.\nWHY JOIN CAPCO?\nYou will work on engaging projects with the largest international and local banks, insurance companies, payment service providers and other key players in the industry. The projects that will transform the financial services industry.\nMAKE AN IMPACT\nInnovative thinking, delivery excellence and thought leadership to help our clients transform their business. Together with our clients and industry partners, we deliver disruptive work that is changing energy and financial services.\n#BEYOURSELFATWORK\nCapco has a tolerant, open culture that values diversity, inclusivity, and creativity.\nCAREER ADVANCEMENT\nWith no forced hierarchy at Capco, everyone has the opportunity to grow as we grow, taking their career into their own hands.\nDIVERSITY & INCLUSION\nWe believe that diversity of people and perspective gives us a competitive advantage.\nMAKE AN IMPACT\nJob Title: Transition Analyst\nLocation: Pune\nExperience Required:\nBachelor s degree is required.\nMinimum 6 to 8 years of total work experience.\nMinimum 1 to 2 years of relevant experience in Project/Program Management or Support roles.\nTechnical & Functional Expertise:\nTechnical:\nProficiency in MS Office products including Office 365, Project Online, SharePoint, Power BI, and other analytics tools.\nStrong understanding of process workflow design, data architecture, and related tools.\nFunctional:\nStrong business acumen and functional understanding.\nExperience in planning and monitoring for program workstreams, project deliverables, and reporting.\nAbility to handle transition-related documentation, administrative tasks, risk management, due diligence, and stakeholder coordination.\nExperience in knowledge transfer, SOP documentation, and hyper care support.\nKey Responsibilities:\nSupport planning and execution of program and transition projects.\nTrack deliverables, manage risks, and ensure timely reporting.\nEnsure compliance with GBS methodologies and toolkits.\nManage travel and logistics for transition-related requirements.\nCoordinate with operational teams and business functions for successful transitions.\nLead the documentation of SOPs and manage sign-off processes.\nCollaborate with various business units including Procurement, Finance, and IT.\nSupport project reporting, dashboard preparation, and Power BI-based analytics.\nHandle highly confidential material with discretion and professionalism.\nParticipate in customer-facing meetings and internal stakeholder communications.\nFacilitate workshops, team meetings, and process improvement initiatives.\nKey Challenges:\nNavigating fragmented systems and tools.\nEngaging a wide range of stakeholders across global functions.\nManaging services at a large scale with geographical and cultural diversity.\nAdapting to evolving digital technologies and technical tools.\nEnsuring alignment with global process design standards.\nSkills & Competencies:\nCore Skills:\nProject planning and reporting skills\nWorkflow and process documentation\nRisk identification and mitigation\nData visualization and reporting tools (especially Power BI)\nSoft Skills:\nExcellent multitasking and prioritization skills\nStrong interpersonal, presentation, and written communication skills\nFluency in English (spoken and written)\nKnowledge of local regulations and compliance standards\nFamiliarity with Pune s local business environment\nAbility to work effectively in a regional service center ecosystem",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Procurement', 'Administration', 'Management consulting', 'Project planning', 'MS Office', 'Risk management', 'Financial services', 'Analytics', 'Monitoring', 'Logistics']",2025-06-12 14:28:25
Associate Director-Oracle Techno-Functional,Acuity Knowledge Partners,10 - 17 years,Not Disclosed,['Gurugram'],"Acuity Knowledge Partners\nAcuity Knowledge Partners (Acuity) is a leading provider of bespoke research, analytics and technology solutions to the financial services sector, including asset managers, corporate and investment banks, private equity and venture capital firms, hedge funds and consulting firms. Its global network of over 6,000 analysts and industry experts, combined with proprietary technology, supports more than 600 financial institutions and consulting companies to operate more efficiently and unlock their human capital, driving revenue higher and transforming operations. Acuity is headquartered in London and operates from 10 locations worldwide.",,,,"['Oracle Cloud', 'Oracle Fusion', 'Oracle Finance', 'Oracle Fusion Technical', 'Oracle Cloud Applications', 'Erp Cloud', 'Oic', 'Fusion Financials', 'Oracle Fusion Financials', 'Oracle ERP', 'Oracle Peoplesoft Financials']",2025-06-12 14:28:27
Senior Database Engineer,Cvent,3 - 6 years,Not Disclosed,['Gurugram'],"Design, develop, and manage databases on the AWS cloud platform Develop and maintain automation scripts or jobs to perform routine database tasks such as provisioning, backups, restores, and data migrations\nBuild and maintain automated testing frameworks for database changes and upgrades to minimize the risk of introducing errors\nImplement self-healing mechanisms to automatically recover from database failures or performance degradation\nIntegrate database automation tools with CI/CD pipelines to enable continuous delivery and deployment of database changes",,,,"['Hospitality', 'Database design', 'database security', 'MySQL', 'Disaster recovery', 'Troubleshooting', 'Event marketing', 'Information technology', 'Python']",2025-06-12 14:28:29
Oracle EBS /Cloud/ Fusion SCM Consultant,Infosys,2 - 5 years,Not Disclosed,['Pune'],"Job Title\nOracle EBS /Cloud/ Fusion SCM Consultant\n\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys consulting team, your primary role would be to actively aid the consulting team in different phases of the project including problem definition, effort estimation, diagnosis, solution generation and design and deployment\nYou will explore the alternatives to the recommended solutions based on research that includes literature surveys, information available in public domains, vendor evaluation information, etc. and build POCs\nYou will create requirement specifications from the business needs, define the to-be-processes and detailed functional designs based on requirements.\nYou will support configuring solution requirements on the products; understand if any issues, diagnose the root-cause of such issues, seek clarifications, and then identify and shortlist solution alternatives\nYou will also contribute to unit-level and organizational initiatives with an objective of providing high quality value adding solutions to customers. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! Technical and Professional :\nMinimum 2 years of implementation experience with Oracle EBS/Cloud/Fusion in Order Management, Procurement, Inventory, Bill of Material, Manufacturing modules\nHave at least 1 full life cycle implementations experience, with hands-on configuration, implementation, and support of Oracle EBS/Cloud/Fusion SCM\nResponsible for leading the requirements elicitation, fit-gap analysis, development, configuration, functional testing, and post-production support\nHave experience with data migration using FBDI\nStrong experience in gathering requirements, designing solutions for Very High transaction volumes and should have good experience of Performance Testing of solutions\nShould have experience of designing and delivering complex custom solutions in highly integrated applications landscape\nExperience in handling integration with external partners/ applications like E-Commerce Portals, Part Catalogs, trading partners - Suppliers & Customers, EDI Preferred Skills:\nTechnology-Oracle eBS Functional-Oracle Order Management Technology-Oracle Cloud-Oracle Planning Cloud Additional Responsibilities:\nAbility to work with clients to identify business challenges and contribute to client deliverables by refining, analyzing, and structuring relevant data\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate\nAbility to assess the current processes, identify improvement areas and suggest the technology solutions\nOne or two industry domain knowledge Educational Master Of Commerce,Master Of Engineering,Master Of Science,Master Of Technology,Master of Business Administration,Bachelor Of Commerce,Bachelor Of Science,Bachelor of Engineering,Bachelor Of Technology Service LineEnterprise Package Application Services* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle EBS', 'solution design', 'order management', 'procurement', 'Cloud', 'oracle e-business suite', 'Fusion SCM Consultant']",2025-06-12 14:28:31
Sr. Application Developer,XL India Business Services Pvt. Ltd,3 - 7 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Senior Application Developer - Salesforce Bangalore, Karnataka India AXA XL are creating a new delivery model based on agile and a new vendor model to enable more efficient delivery of technology with the business fully embedded in what we deliver\n\nThis Digital Factory will first deliver new capabilities for Fusion Transformation and then will be rolled out to other capabilities within GT (Global Technology) and IDA (Innovation, Data & Analytics)\n\nWe are seeking a Senior Salesforce Developer with extensive expertise in automated software testing and a passion for coaching others to join our dynamic team\n\nAs a Senior Salesforce Developer, you will play a key role in architecting, designing, and implementing our insurance workbench application on the Salesforce platform, with a focus on test-driven development, automated testing, and continuous integration\n\nWhat you ll be DOING What will your essential responsibilities include? Responsibilities: Drive the development and maintenance of our insurance workbench application on the Salesforce platform, leveraging your expertise to design and integrate relevant modules, customizations, and configurations that optimize workflows and data management\n\nManage and resolve support requests, issues, and escalations from end-users\n\nDiagnose and solve technical issues related to Salesforce applications\n\nPerform administrative tasks such as user account maintenance, reports, dashboards, and workflows\n\nConfigure Salesforce settings for users, roles, security, profiles, and workflow rules\n\nAssist in training new users and enhancing the Salesforce skill set across the organization\n\nCommunicate regularly with the Salesforce user base regarding new features, enhancements, and changes\n\nUpdate, maintain, and create customized reports for different departments and management\n\nEnsure data integrity and perform data imports/exports as needed\n\nWork with management to identify new opportunities to leverage Salesforce for additional business processes\n\nDocument system configurations, processes, and procedures\n\nTest Salesforce updates and new releases to ensure smooth implementation\n\nProvide feedback and suggestions for system improvements\n\nLead and mentor a team of developers, providing technical guidance, conducting code reviews, and promoting best practices in Salesforce development, automated testing, and software quality assurance\n\nBuild and lead the automated software testing chapter for the insurance workbench suite\n\nLead the development of APIs to facilitate data exchange and connectivity with external systems and partners\n\nChampion a product-led Agile approach to development, applying design thinking principles to create intuitive user experiences (UX) and customer experiences (CX) within the workbench application\n\nDrive continuous improvement initiatives, staying ahead of emerging technologies and industry trends, and adapting to changing product requirements\n\nSolution Design and Architecture: Lead the design and architecture of complex solutions on the Salesforce platform, leveraging design thinking methodologies to address business requirements effectively\n\nDefine architectural standards, patterns, and best practices for Salesforce development in alignment with enterprise architecture principles\n\nEnsure solutions are scalable, maintainable, and comply with industry regulations in the commercial insurance domain\n\nTechnical Leadership: Provide technical leadership and guidance to development teams, promoting DevOps practices and CI/CD pipelines for efficient delivery\n\nCollaborate with stakeholders to prioritize features and enhancements, ensuring alignment with business goals and objectives\n\nMentor and coach team members on Salesforce development, DevOps, and enterprise architecture concepts\n\nIntegration and Data Management: Design and implement API integrations with external systems, leveraging Azure services and other technologies as needed\n\nArchitect data streaming solutions for real-time analytics and insights, ensuring data accuracy, integrity, and security\n\nCollaborate with data architects to define data models and ensure proper data governance practices within the Salesforce Org\n\nScaled Agile Delivery: Work within a scaled agile framework, participating in program increment planning, backlog grooming, and sprint ceremonies\n\nFacilitate collaboration between agile teams, resolving dependencies and ensuring alignment with the overall program roadmap\n\nDrive continuous improvement initiatives to enhance delivery processes and optimize team performance\n\nSalesforce Org Oversight: Oversee multiple solutions within the Salesforce org, ensuring cohesiveness, scalability, and maintainability across different business units\n\nConduct regular reviews and audits of Salesforce configurations, identifying opportunities for optimization and enhancement\n\nPartner with Salesforce administrators to establish governance policies, security controls, and release management processes\n\nYou will be reporting to Platform Delivery Lead\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Proven experience as a Senior Salesforce Developer, with a track record of delivering complex solutions in enterprise environments, preferably within the insurance domain\n\nStrong knowledge of Salesforce administration and declarative capabilities\n\nExperience with Salesforce configuration, customization, and data management\n\nExtensive experience with APEX, LWC , Flows and familiarity with cloud-based integration, design thinking, UX/CX\n\nExperience in Salesforce administration, including user account maintenance, security settings, and workflow rules\n\nExperience in application support, including triaging incidents and assisting support teams in resolving issues\n\nProven experience in optimizing the Salesforce platform for performance, security, and audit\n\nExperience in understanding and implementing Salesforce AI capabilities will be an added advantage\n\nHands on experience in Implementing Integrations with various systems\n\nSalesforce devloper 1 certificate mandatory and developer 2 certificate preferable\n\nPrefereable experience in Omni Studio\n\nSignificant leadership and communication skills, with the ability to effectively guide and collaborate with development squads, stakeholders, and business users\n\nA deep understanding of Agile principles, with a passion for driving continuous improvement and innovation\n\nDesired Skills and Abilities: Outstanding ability to coach and mentor team members, helping them to develop expertise in automated testing and software quality assurance\n\nLeading a automation testing chapter\n\nExcellent analytical abilities to identify issues and devise effective solutions in a dynamic environment\n\nA Bachelors or Masters degree in Computer Science, Software Engineering, or equivalent work experience",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Application support', 'Data management', 'Enterprise architecture', 'Agile', 'Workflow', 'Release management', 'Software quality assurance', 'Salesforce', 'Auditing']",2025-06-12 14:28:34
Senior Consultant Technical - ETL + SQL Expertise,Insightsoftware,7 - 11 years,Not Disclosed,['Hyderabad'],"Insightsoftware (ISW) is a growing, dynamic computer software company that helps businesses achieve greater levels of financial intelligence across their organization with our world-class financial reporting solutions. At insightsoftware, you will learn and grow in a fast-paced, supportive environment that will take your career to the next level. The Data Conversion Specialist is a member of the insightsoftware Project Management Office (PMO) who demonstrates teamwork, results orientation, a growth mindset, disciplined execution, and a winning attitude.\nLocation: Hyderabad (Work from Office- Hybrid)\nWorking Hours: 2:30 PM to 11:30 PM for 3 Days 5:00 PM - 2:00AM IST or 6:00 PM to 3:00 AM IST for 2 Days, should be ok to work in night shift as per requirement.\nPosition Summary\nThe Senior Consultant will integrate and map customer data from client source system(s) to our industry-leading platform. The role will include, but is not limited to:\nUsing strong technical data migration, scripting, and organizational skills to ensure the client data is converted efficiently and accurately to the insightsoftware (ISW) platform.\nPerforming extract, transform, load (ETL) activities to ensure accurate and timely data conversions.\nProviding in-depth research and analysis of complex scenarios to develop innovative solutions to meet customer needs whilst remaining within project governance.\nMapping and maintaining business requirements to the solution design using tools such as requirements traceability matrices (RTM).\nPresenting findings, requirements, and problem statements for ratification by stakeholders and working groups.\nIdentifying and documenting data gaps to allow change impact and downstream impact analysis to be conducted.\nExperience assessing data and analytic requirements to establish mapping rules from source to target systems to meet business objectives.\nExperience with real-time, batch, and ETL for complex data conversions.\nWorking knowledge of extract, transform, load (ETL) methodologies and tools such as Talend, Dell Boomi, etc.\nUtilize data mapping tools to prepare data for data loads based on target system specifications.\nWorking experience using various data applications/systems such as Oracle SQL, Excel, .csv files, etc.\nStrong SQL scripting experience.\nCommunicate with clients and/or ISW Project Manager to scope, develop, test, and implement conversion/integration\nEffectively communicate with ISW Project Managers and customers to keep project on target\nContinually drive improvements in the data migration process.\nCollaborate via phone and email with clients and/or ISW Project Manager throughout the conversion/integration process.\nDemonstrated collaboration and problem-solving skills.\nWorking knowledge of software development lifecycle (SDLC) methodologies including, but not limited to: Agile, Waterfall, and others.\nClear understanding of cloud and application integrations.\nAbility to work independently, prioritize tasks, and manage multiple tasks simultaneously.\nEnsure client s data is converted/integrated accurately and within deadlines established by ISW Project Manager.\nExperience in customer SIT, UAT, migration and go live support.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data migration', 'Data conversion', 'Financial reporting', 'Project management', 'project governance', 'Agile', 'Software development life cycle', 'data mapping', 'SDLC', 'Downstream']",2025-06-12 14:28:36
Senior Workday Integration Developer,Luxoft,5 - 10 years,Not Disclosed,['Gurugram'],"Act as the Tech Business Analyst in the end-to-end implementation of Workday integrations.\nGather integration requirements from downstream systems and document detailed design specifications.\nSupport all project phases, including design, build, testing (SIT), PRR, and deployment.\nDocument field mappings, transformation logic, user stories, and cost trackers.\nCreate and update process models using enterprise modeling tools.\nFacilitate alignment between business and technical teams to ensure smooth delivery.\nConduct business analysis such as scenario, data, gap, and change impact analysis.\nDevelop process flows, data models, business rules, and reporting structures as part of solution design.\nMaintain internal documentation repositories (e.g., Confluence) and manage stakeholder updates.\nCollaborate with cross-functional teams, including integration, development, QA, and business stakeholders.\nSupport vendor coordination, testing, cutover, and go-live planning.\nApply agile practices to ensure high-quality, user-centric outcomes.\nSkills\nMust have\n5+ years of experience as a Senior Workday Integration Developer/ Consultant with proven expertise in Workday integrations, configuration, and system optimization.\nStrong experience in Workday Integrations, including design and delivery.\nProven integration experience with: oCustomer Master ADA [MEID] oBenevity, Clarity, ODI oAspect & ControliQ (Bidirectional Time & Attendance interfaces) oODB and GPR\nUK\nProficient in data mapping, transformation logic, and integration architecture.\nExperience with Workday security frameworks, authentication protocols (OAuth, SAML), and data governance.\nAbility to translate business requirements into scalable technical solutions.\nExcellent analytical, documentation, and process modeling skills.\nStrong workshop facilitation and stakeholder engagement capabilities.\nProficiency working within agile delivery environments.\nNice to have\nWorkday Certified Integration Developer with exposure to large-scale implementations.\nExperience with reporting and data migration in cloud-based HR platforms.\nWorking knowledge of enterprise modeling tools and data lake environments.\nAdvanced Excel and experience with data analytics and validation.\nFamiliarity with compliance, regulatory reporting, and change management practices.\nStrong communication and stakeholder management across technical and non-technical teams.\nDemonstrated ability to work independently, manage risks, and adapt in fast-paced environments.\nExperience collaborating with external vendors and integration partners.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Change management', 'Data migration', 'Business analysis', 'Analytical', 'Agile', 'business rules', 'data mapping', 'SAML', 'Stakeholder management', 'Downstream']",2025-06-12 14:28:39
Senior Software Engineer-7916,WebMD,5 - 10 years,Not Disclosed,['Navi Mumbai'],"Position: Senior Software Engineer (Data Engineer)\nNo. of Positions: 1\nAbout WebMD:\nHeadquartered in El Segundo, Calif., Internet Brands is a fully integrated online media and software services\norganization focused on four high-value vertical categories: Health, Automotive, Legal, and Home/Travel. The\ncompanys award-winning consumer websites lead their categories and serve more than 250 million monthly\nvisitors, while a full range of web presence offerings has established deep, long-term relationships with SMB and\nenterprise clients. Internet Brands,powerful, proprietary operating platform provides the flexibility and\nscalability to fuel the companys continued growth. Internet Brands is a portfolio company of KKR and Temasek.\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health information services, serving\npatients, physicians, health care professionals, employers, and health plans through our public and private online\nportals, mobile platforms, and health-focused publications. The WebMD Health Network includes WebMD\nHealth, Medscape, Jobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals\nConsumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape Education, and other owned\nWebMD sites. WebMD, Medscape, CME Circle, Medpulse, eMedicineÂ®, MedicineNetÂ®, theheart.orgÂ®, and\nRxListÂ® are among the trademarks of WebMD Health Corp. or its subsidiaries.\nFor Company details, visit our website: www.webmd.com / www.internetbrands.com\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex,\nsexual orientation, gender identity, national origin, disability, or veteran status\nEducation: B.E. Computer Science/IT degree (or any other engineering discipline)\nExperience: 5+ years\nWork timings: 2:00 PM to 11:00 PM IST.\nDescription:\nWe are seeking an experienced and passionate Senior Software Developer to join our team. In this role,\nyou will work closely with cross-functional teams, developers, stakeholders, and business units to\ngather and analyze business requirements, design, build and implement ETL Solutions, and maintain\nthe infrastructure. The ideal candidate will have a strong background in business analysis, SQL, Unix,\nPython, ETL Tools to ensure the successful execution of projects.\nResponsibilities:\nLead requirements gathering sessions with key stakeholders to understand business needs and\nobjectives.\nCollaborate with and across Agile teams to design, develop, test, implement and support ETL\nprocesses for data transformation and preparation.\nManage data pipelines for analytics and operational use.\nEnsure data quality, data accuracy and integrity across multiple sources and systems.\nPerform unit tests and conduct reviews with other team members to make sure your code is\nrigorously designed, elegantly coded, and effectively tuned for performance.\nShould be able to come up with multiple approaches to any ETL\nproblem statement/solution/technical challenge and take well informed decision to pick the\nbest solution.\nAutomate ETL Processes using Cron and/or using Job Scheduler tools like AirFlow.\nAdhere to company standards and Serve as a key contributor to the design and development of\nexception handling, code/data standardization procedures, resolution steps and Quality Assurance\ncontrols.\nMaintain a version repository and ensure version control.\nCreate visual aids such as diagrams, charts, and screenshots to enhance documentation.\nWork with Infrastructure/systems team and developers to ensure all modules are up-to- date and\nare compatible with the code.",,,,"['ETL', 'SQL', 'UNIX', 'Python']",2025-06-12 14:28:41
"R&D Member of Technical Staff II, Product Development",Aveva,2 - 6 years,Not Disclosed,['Hyderabad'],"job requisition idR010044\n\nAVEVA is creating software trusted by over 90% of leading industrial companies.\n\nJob TitleR&D Member of Technical Staff II, Product Development\n\nLocationHyderabad\n\nEmployment Type Full-time\n\nBenefits Gratuity, Medical and accidental insurance, very attractive leave entitlement, emergency leave days, childcare support, maternity, paternity and adoption leaves, education assistance program, home office set up support (for hybrid roles), well-being support\n\nThe job\n\nAs a part of this function, the Cloud Unified Engineering team of R&D Technology and Execution Development, the development engineer will be responsible for ensuring the high quality of AVEVA Software Product deliveries to customers on the CONNECT cloud platform.\n\nKey responsibilities\n\nWe are looking for a Developer with skills to design and develop required functionality in the Cloud Unified Engineering solution / platform. This will include implement new user stories, write unit tests and fixing the reported defects by system test, DevOps teams.\n\nAlso, proactively identify improvements and enhancements to existing unit test cases test suites. This role reports into Development manager located in Hyderabad, India.\n\nEssential requirements\n\nDevelopment experience with exposure to programming skills, e.g. C#, .NET.\nProven experience on Amazon Web Services (AWS)Desired skills and competenciesKnowledge of PowerShell or Node JS scripting.Knowledge of AWS CloudFormation and Infrastructure as Code (IaC).\nKnowledge of API, REST, microservices and serverless architecture\nKnowledge and experience of operational support, software development and deployment methodologies and principles.\nHands-on in AWS administration, AWS APIs and tools or equivalent Azure experience.\nStrong written, verbal and presentation skills, able to convey information clearly and concisely to technical and non-technical audiences.R&D at AVEVA Our global team of 2000+ developers work on an incredibly diverse portfolio of over 75 industrial automation and engineering products, which cover everything from data management to 3D design. AI and cloud are at the centre of our strategy, and we have over 150 patents to our name.Our track record of innovation is no fluke its the result of a structured and deliberate focus on learning, collaboration and inclusivity. If you want to build applications that solve big problems, join us.Find out moreaveva.com/en/about/careers/r-and-d-careers/ India Benefits include:Gratuity, Medical and accidental insurance, very attractive leave entitlement, emergency leave days, childcare support, maternity, paternity and adoption leaves, education assistance program, home office set up support (for hybrid roles), well-being support Its possible were hiring for this position in multiple countries, in which case the above benefits apply to the primary location. Specific benefits vary by country, but our packages are similarly comprehensive.Find out moreaveva.com/en/about/careers/benefits/ Hybrid workingBy default, employees are expected to be in their local AVEVA office three days a week, but some positions are fully office-based. Roles supporting particular customers or markets are sometimes remote.Hiring processInterestedGreat! Get started by submitting your cover letter and CV through our application portal. AVEVA is committed to recruiting and retaining people with disabilities. Please let us know in advance if you need reasonable support during your application process.Find out moreaveva.com/en/about/careers/hiring-process",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['presentation skills', 'aws administration', 'microsoft azure', 'aws cloudformation', 'aws', 'c#', 'rest', 'software development', 'web services', 'artificial intelligence', 'microservices', 'node.js', 'devops', 'powershell', 'product development', '.net', 'api', 'product engineering']",2025-06-12 14:28:43
Senior Etl Informatica Developer,VBeyond,6 - 8 years,19-25 Lacs P.A.,"['Noida', 'Chennai', 'Bengaluru']","We are seeking a highly skilled and experienced Senior ETL & Reporting QA Analyst to join our dynamic team. The ideal candidate will bring strong expertise in ETL and Report Testing, with a solid command of SQL, and hands-on experience in Informatica, as well as BI Reporting tools. A strong understanding of the Insurance domain is crucial to this role. This position will be instrumental in ensuring the accuracy, reliability, and performance of our data pipelines and reporting solutions.\n\nKey Responsibilities:\nDesign, develop, and execute detailed test plans and test cases for ETL processes, data migration, and data warehousing solutions.\nPerform data validation and data reconciliation using complex SQL queries across various source and target systems.\nValidate Informatica ETL workflows and mappings to ensure accurate data transformation and loading.\nConduct end-to-end report testing and dashboard validations using Cognos (preferred), or comparable BI tools such as Tableau or Power BI.\nCollaborate with cross-functional teams including Business Analysts, Developers, and Data Engineers to understand business requirements and transform them into comprehensive test strategies.\nIdentify, log, and track defects to closure using test management tools and actively participate in defect triage meetings.\nMaintain and enhance test automation scripts and frameworks where applicable.\nEnsure data integrity, consistency, and compliance across reporting environments, particularly in the insurance domain context.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Informatica', 'ETL', 'Power Bi', 'Insurance Domain', 'Tableau', 'SQL']",2025-06-12 14:28:46
Java + Springboot Lead,Infosys,5 - 9 years,Not Disclosed,['Chennai'],"Job Title\nJava + Springboot Lead\n\nResponsibilities\nLead and mentor a team of Java & Springboot Developers in the design, development, and maintenance of applications.Work with business stakeholders and technical teams to gather and analyze requirements for Java & Springboot applications.Design, develop, and enhance software solutions using Java & Springboot, including Microservices, MVC, Spring Data, and Spring Security.Write efficient and well-structured code to implement business logic and functionality on the Java platform.Perform unit testing and debugging to ensure the quality and reliability of developed applications.Maintain and enhance existing Java & Springboot applications by troubleshooting issues, implementing bug fixes, and optimizing performance.Collaborate with other developers, database administrators, and system administrators to integrate Java & Springboot applications with other systems and databases.Develop and maintain technical documentation, including system design, coding standards, and user manuals.Stay updated with the latest Java & Springboot technologies and industry trends, and recommend improvements or alternative solutions to enhance system performance and efficiency.Collaborate with cross-functional teams to support system integration, data migration, and software deployment activities.Participate in code reviews and provide constructive feedback to ensure adherence to coding standards and best practices.Proactively identify and address potential risks or issues related to Java & Springboot applications and propose appropriate solutions.Provide leadership and guidance to the team and create a positive and productive work environment.Manage the team's workload and ensure that projects are completed on time and within budget.Delegate tasks and responsibilities to team members and provide regular feedback.Identify and develop the team's strengths and weaknesses and provide opportunities for professional growth.\n\nTechnical and Professional :\nPrimary skills:Technology-Java-Springboot\nPreferred Skills:\nTechnology-Java-Springboot Additional Responsibilities:Bachelor's degree in Computer Science, Information Technology, or a related field.Minimum of 5 years of experience as a Java & Springboot Developer, with at least 3 years of team handling experienceStrong understanding of Java programming concepts, including object-oriented programming, data structures, and algorithms.Proficiency in Springboot framework, including Microservices, MVC, Spring Data, and Spring Security.Extensive experience with Java development tools, such as Eclipse and IntelliJ IDEA.Deep familiarity with relational databases, particularly MySQL and PostgreSQL.Expert knowledge of Java performance tuning and optimization techniques.Excellent problem-solving and analytical skills.Strong written and verbal communication skills, with the ability to effectively communicate technical concepts to both technical and non-technical stakeholders.Detail-oriented with a commitment to delivering high-quality software solutions.Proven ability to lead and mentor a team of developers.Leadership and management skills Educational Master Of Comp. Applications,Bachelor Of Comp. Applications,Bachelor of Engineering,Bachelor Of Technology Service LineApplication Development and Maintenance* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'postgresql', 'performance tuning', 'java development', 'mysql', 'spring boot framework', 'relational databases']",2025-06-12 14:28:48
Solution Design Lead & implimantation,Excellerate Global Solutions,13 - 23 years,10-20 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Solution Design & Implementation:\nLead and participate in the full project lifecycle of SAP S/4HANA Public Cloud implementations, focusing on Procurement (Sourcing & Procurement/MM), Finance (FI/CO), and Sales & Distribution (SD) modules.\nConduct in-depth business process analysis, gather requirements, and translate them into robust and scalable SAP S/4HANA Public Cloud solutions aligned with SAP best practices.\nDesign, configure, and customize SAP S/4HANA Public Cloud functionalities for Order-to-Cash (O2C), Procure-to-Pay (P2P), Record-to-Report (R2R), and other relevant cross-functional processes.\nEnsure seamless integration between Procurement, Finance, and SD modules, as well as with other SAP Cloud modules (e.g., EWM, PP) and third-party applications where applicable.\nLeverage SAP Activate methodology for project delivery, guiding clients through fit-to-standard workshops and solution design.\nFunctional Expertise:\nProcurement (MM): Expertise in Material Master, Vendor Master, Purchase Requisitions, Purchase Orders, Contracts, Sourcing, Inventory Management, Invoice Verification, and supplier collaboration.\nFinance (FI/CO): Strong knowledge of General Ledger, Accounts Payable, Accounts Receivable, Asset Accounting, Bank Accounting, Cost Center Accounting, Profit Center Accounting, Internal Orders, Product Costing, Profitability Analysis (CO-PA), and treasury functions. Understanding of the Universal Journal (ACDOCA) and its impact.\nSales & Distribution (SD): Proficiency in Sales Order Management, Pricing, Delivery Processing, Billing, Credit Management, Returns Management, and ATP (Available-to-Promise).\nTechnical Acumen (Public Cloud Specific):\nUnderstanding of SAP S/4HANA Public Cloud architecture, standard scope, extensibility options (e.g., in-app extensibility, side-by-side extensions using SAP BTP).\nFamiliarity with SAP Fiori applications and user interfaces for relevant modules.\nKnowledge of data migration strategies and tools within the Public Cloud environment (e.g., Migration Cockpit).\nExperience with SAP Cloud ALM for implementation, operations, and monitoring.\nClient Engagement & Leadership:\nAct as a trusted advisor to clients, effectively communicating complex technical and functional concepts to both business and IT stakeholders.\nLead workshops, facilitate discussions, and drive decisions throughout the project lifecycle.\nProvide expert guidance on cloud transformation strategies, change management, and user adoption.\nMentor and guide junior consultants, fostering a culture of knowledge sharing and continuous improvement.\nTesting, Training & Support:\nDevelop and execute comprehensive test plans (unit, integration, UAT) to ensure the solution meets business requirements and is defect-free.\nPrepare detailed training materials and conduct engaging training sessions for end-users.\nProvide post-implementation support, troubleshoot issues, and drive resolution in collaboration with technical teams.\nContinuous Improvement & Innovation:\nStay updated with the latest SAP S/4HANA Public Cloud releases, functionalities, and industry best practices.\nIdentify opportunities for process optimization and leverage new SAP innovations (e.g., AI, Machine Learning capabilities within S/4HANA) to enhance client value.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Sap Hana', 'Solution Design', 'SAP FICO', 'SAP SD', 'SAP MM', 'SAP Finance']",2025-06-12 14:28:50
"Spark, Java, Kafka- Hyderabad",Cognizant,12 - 15 years,Not Disclosed,['Hyderabad'],"Skill: Java, Spark, Kafka\nExperience: 10 to 16 years\nLocation: Hyderabad\n As Data Engineer, you will :\n       Support in designing and rolling out the data architecture and infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources\n       Identify data source, design  and implement data schema/models and integrate data that meet the requirements of the business stakeholders",,,,"['hive', 'cloudera', 'modeling', 'scala', 'data warehousing', 'apache pig', 'data pipeline', 'data architecture', 'scalability', 'sql', 'java', 'data modeling', 'spark', 'mysql', 'hadoop', 'etl', 'big data', 'hbase', 'python', 'oozie', 'data processing', 'airflow', 'elt', 'data engineering', 'nosql', 'mapreduce', 'kafka', 'feasibility analysis', 'hdfs', 'sqoop', 'aws']",2025-06-12 14:28:52
MD365 Finance Functional Consultant,Protiviti India,3 - 8 years,6-16 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nWe are seeking an experienced Microsoft Dynamics 365 Finance Functional Consultant to join our team in Bangalore. The ideal candidate will have hands-on experience in implementing, configuring, and supporting D365 Finance solutions for diverse business requirements. This role involves working closely with clients to understand their financial processes and translate them into effective D365 Finance configurations.\nKey Responsibilities\nImplementation & Configuration",,,,"['Finance', 'D365 Functional', 'Microsoft dynamics']",2025-06-12 14:28:54
Sr. Associate Full Stack Software Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will be responsible for designing, developing, and maintaining software applications and solutions that meet business needs and ensuring the availability and performance of critical systems and applications. This role involves working closely with product managers, designers, data engineers, and other engineers to create high-quality, scalable software solutions and automating operations, monitoring system health, and responding to incidents to minimize downtime.\n\nYou will play a key role in a regulatory submission content automation initiative which will modernize and digitize the regulatory submission process, positioning Amgen as a leader in regulatory innovation. The initiative demonstrates innovative technologies, including Generative AI, Structured Content Management, and integrated data to automate the creation, and management of regulatory content.\n\n\n\nRoles & Responsibilities:\nPossesses strong rapid prototyping skills and can quickly translate concepts into working code\nContribute to both front-end and back-end development using cloud technology\nDevelop innovative solution using generative AI technologies\nEnsure code quality and consistency to standard methodologies\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations\nIdentify and resolve technical challenges effectively\nStay updated with the latest trends and advancements\nWork closely with product team, business team, and other collaborators\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDevelop and implement unit tests, integration tests, and other testing strategies to ensure the quality of the software\nIdentify and resolve software bugs and performance issues\nWork closely with multi-functional teams, including product management, design, and QA, to deliver high-quality software on time\nCustomize modules to meet specific business requirements\nWork on integrating with other systems and platforms to ensure seamless data flow and functionality\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nMasters degree and 1 to 3 years of experience in Computer Science, IT or related field OR\nBachelors degree and 3 to 5 years of experience in Computer Science, IT or related field OR\nDiploma and 7 to 9 years of experience in Computer Science, IT or related field\nPreferred Qualifications:\n\n\n\nFunctional\n\nSkills:\nMust-Have Skills:\nProficiency in Python/PySpark development, Fast API, PostgreSQL, Databricks, DevOps Tools, CI/CD, Data Ingestion.\nCandidates should be able to write clean, efficient, and maintainable code.\nKnowledge of HTML, CSS, and JavaScript, along with popular front-end frameworks like React or Angular, is required to build interactive and responsive web applications\nIn-depth knowledge of data engineering concepts, ETL processes, and data architecture principles. Solid understanding of cloud computing principles, particularly within the AWS ecosystem\nSolid understanding of software development methodologies, including Agile and Scrum\nExperience with version control systems like Git\nHands on experience with various cloud services, understand pros and cons of various cloud service in well architected cloud design principles\nStrong problem solving, analytical skills; Ability to learn quickly; Good communication and interpersonal skills\nExperienced with API integration, serverless, microservices architecture.\nExperience in SQL/NOSQL database, vector database for large language models\n\n\n\nGood-to-Have\n\nSkills:\nSolid understanding of cloud platforms (e.g., AWS, GCP, Azure) and containerization technologies (e.g., Docker, Kubernetes)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience with data processing tools like Hadoop, Spark, or similar\n\n\n\nSoft\n\nSkills:\nExcellent analytical and solving skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'PySpark development', 'Data Ingestion', 'PostgreSQL', 'Fast API', 'CI/CD', 'DevOps Tools', 'Databricks']",2025-06-12 14:28:57
Sr. Databricks Developer,Newscape Consulting,7 - 9 years,Not Disclosed,['Pune( Baner )'],"We are looking for a highly skilled Senior Databricks Developer to join our data engineering team. You will be responsible for building scalable and efficient data pipelines using Databricks, Apache Spark, Delta Lake, and cloud-native services (Azure/AWS/GCP). You will work closely with data architects, data scientists, and business stakeholders to deliver high-performance, production-grade solutions.\nKey Responsibilities :\n- Design, build, and maintain scalable and efficient data pipelines on Databricks using PySpark, Spark SQL, and optionally Scala.\n- Work with Databricks components including Workspace, Jobs, DLT (Delta Live Tables), Repos, and Unity Catalog.\n- Implement and optimize Delta Lake solutions aligned with Lakehouse and Medallion architecture best practices.\n- Collaborate with data architects, engineers, and business teams to understand requirements and deliver production-grade solutions.\n- Integrate CI/CD pipelines using tools such as Azure DevOps, GitHub Actions, or similar for Databricks deployments.\n- Ensure data quality, consistency, governance, and security by using tools like Unity Catalog or Azure Purview.\n- Use orchestration tools such as Apache Airflow, Azure Data Factory, or Databricks Workflows to schedule and monitor pipelines.\n- Apply strong SQL skills and data warehousing concepts in data modeling and transformation logic.\n- Communicate effectively with technical and non-technical stakeholders to translate business requirements into technical solutions.\nRequired Skills and Qualifications :\n- Hands-on experience in data engineering, with specifically in Databricks.\n- Deep expertise in Databricks Workspace, Jobs, DLT, Repos, and Unity Catalog.\n- Strong programming skills in PySpark, Spark SQL; Scala experience is a plus.\n- Proficient in working with one or more cloud platforms : Azure, AWS, or GCP.\n- Experience with Delta Lake, Lakehouse architecture, and medallion architecture patterns.\n- Proficient in building CI/CD pipelines for Databricks using DevOps tools.\n- Familiarity with orchestration and ETL/ELT tools such as Airflow, ADF, or Databricks Workflows.\n- Strong understanding of data governance, metadata management, and lineage tracking.\n- Excellent analytical, communication, and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'ETL', 'Delta Lake', 'Azure Data Lake', 'Apache', 'Data Bricks']",2025-06-12 14:28:59
"Lead FullStack Developer - Java, React.JS",S&P Global Market Intelligence,10 - 15 years,Not Disclosed,['Hyderabad'],"About the Role:\nGrade Level (for internal use):\n11\nThe Team:\nWe are looking for a highly motivated, enthusiastic, and skilled engineering lead for Commodity Insights. We strive to deliver solutions that are sector-specific, data-rich, and hyper-targeted for evolving business needs. Our Software development Leaders are involved in the full product life cycle, from design through release.\nThe resource would be joining a strong innovative team working on the content management platforms which support a large revenue stream for S&P Commodity Insights.Working very closely with the Product owner and Development Manager, teams are responsible for the development of user enhancements and maintaining good technical hygiene.\nThe successful candidate will assist in the design, development, release and support of content platforms. Skills required include ReactJS, Spring Boot, RESTful microservices, AWS services (S3, ECS, Fargate, Lambda, etc.), CSS HTML, AJAX JSON, XML and SQL (PostgreSQL/Oracle), .\nThe candidate should be aware of GEN AI or LLM models like Open AI and Claude etc.\nThe candidate should be enthusiast in working on prompt building related to GenAI and business-related prompts.\nThe candidate should be able to develop and optimize prompts for AI models to improve accuracy and relevance.\nThe candidate must be able to work well with a distributed team, demonstrate an ability to articulate technical solutions for business requirements, have experience with content management/packaging solutions, and embrace a collaborative approach for the implementation of solutions.\nResponsibilities:\nLead and mentor a team through all phases of the software development lifecycle, adhering to agile methodologies (Analyze, design, develop, test, debug, and deploy). Ensure high-quality deliverables and foster a collaborative environment.\nBe proficient with the use of developer tools supporting the CI/CD process including configuring and executing automated pipelines to build and deploy software components\nActively contribute to team planning and ceremonies and commit to team agreement and goals\nEnsure code quality and security by understanding vulnerability patterns, running code scans, and be able to remediate issues.\nMentoringthe junior developers.\nMake sure that code review tasks on all user storiesare added and timely completed.\nPerform reviews and integration testing to assure quality of project development eorts\nDesign database schemas, conceptual data models, UI workows and application architectures that t into the enterprise architecture\nSupport the user base, assisting with tracking down issues and analyzing feedback to identify product improvements\nUnderstand and commit to the culture of S&P Global: the vision, purpose and values of the organization\nBasic Qualifications:\n10+ years experience in an agile team development role, delivering software solutions using Scrum\nJava, J2EE, Javascript, CSS/HTML, AJAX\nReactJS, Spring Boot, Microservices, RESTful services, OAuth\nXML, JSON, data transformation\nSQL and NoSQL Databases (Oracle, PostgreSQL)\nWorking knowledge of Amazon Web Services (Lambda, Fargate, ECS, S3, etc.)\nExperience on GEN AI or LLM models like Open AI and Claude is preferred.\nExperience with agile workflow tools (e.g. VSTS, JIRA)\nExperience with source code management tools (e.g. git), build management tools (e.g. Maven) and continuous integration/delivery processes and tools (e.g. Jenkins, Ansible)\nSelf-starter able to work to achieve objectives with minimum direction\nComfortable working independently as well as in a team\nExcellent verbal and written communication skills\nPreferred Qualifications:\nAnalysis of business information patterns, data analysis and data modeling\nWorking with user experience designers to deliver end-user focused benefits realization\nFamiliar with containerization (Docker, Kubernetes)\nMessaging/queuing solutions (Kafka, etc.)\nFamiliar with application security development/operations best practices (including static/dynamic code analysis tools)\nAbout S&P Global Commodity Insights\nAt S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\nWere a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the worlds foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the worlds leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit .\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\n\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nWe take care of you, so you cantake care of business. We care about our people. Thats why we provide everything youand your careerneed to thrive at S&P Global.\n\nHealth & Wellness: Health care coverage designed for the mind and body.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nGlobal Hiring and Opportunity at S&P Global:\nAt S&P Global, we are committed to fostering a connected andengaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n-----------------------------------------------------------\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\nIf you need an accommodation during the application process due to a disability, please send an email to:and your request will be forwarded to the appropriate person.\n\nUS Candidates Only:The EEO is the Law Poster describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision -\n-----------------------------------------------------------\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.2 - Middle Professional Tier II (EEO Job Group), SWP Priority Ratings - (Strategic Workforce Planning)",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'S3', 'PostgreSQL', 'Spring Boot', 'J2EE', 'Microservices', 'OAuth', 'Fargate', 'ECS', 'ReactJS', 'Javascript', 'CSS/HTML', 'RESTful services', 'Oracle', 'Lambda', 'AJAX']",2025-06-12 14:29:02
Application Developer - Dot Net IFRS 17 // Mumbai // 4-8 Yrs,2coms,4 - 9 years,Not Disclosed,['Mumbai'],"SUMMARY\nAbout the client:\n\nOur client is an IT Technology & Services Management MNC , supporting millions of internal and external customers with state of-the-art IT solutions to everyday problems & dedicated to bringing digital innovations to every aspect of the landscape of insurance. Our Client is a part of one of the major insurance groups based out of Germany and Europe. The Group is represented in around 26 countries worldwide, with Over 37,000 people worldwide, focusing mainly on Europe and Asia & offers a comprehensive range of insurances, pensions, investments and services by focusing on all cutting edge technologies majorly on Could, Digital, Robotics Automation, IoT, Voice Recognition, Big Data science, advanced mobile solutions and much more to accommodate the customers future needs around the globe.\n\n\n\n\n\nRequirements\nCompentences for .NET\nNET Framework (incl. .net core), SQL Server, Entity Framework\nPrioritize business impact and urgency\nAbility to learn new technology and methodology quickly.\nKnowledge User Task API, GUI (Graphical User Interfaces) Design Compentences for NTS (New Tech Stack)\nNTS (New Tech Stack), Container runtime environment with Docker containers and Kubernetes, Cloud with AWS (Amazon Web Services), CI/CD - Continuous Integration / Continuous Deployment with Jenkins, Knowledge Source Management Github and Nexus\nOpenshift, Kerberos Authentication, Competences for Cluster Workflow\nDesign + implementation of process model, Design + implementation of input interfaces in REST format\nDevelopment of flow services below process model\n\nEducational Qualifications:\nBachelorâ€™s or Masterâ€™s  degree in Computer Science /Engineering/Information Technology\nCandidate with non-computer science degree must have minimum 1 year of relevant experience\nMBA in IT / Insurance/Finance   can also apply for Requirements Engineer and Test Engineer role.\n\n\n\nBenefits",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'kubernetes', 'nexus', 'web services', 'openshift', 'ci/cd', 'kerberos', 'ccm', 'business objects administration', 'docker', 'profibus', 'jenkins', 'cd', 'rest', 'e-discovery', 'github', 'entity framework', 'intools', 'sql server', 'application development', 'net application', '.net core', '.net', 'abb dcs', 'aws']",2025-06-12 14:29:04
Project Manager,Nomura,5 - 9 years,Not Disclosed,['Mumbai'],"Job Title: Project Manager\nJob Code: 9922\nCountry: IN\nCity: Mumbai\nSkill Category: India CMT\nDescription:\nRole description\nThe candidate will be a member of the international change team responsible for delivery and support of core transformation initiatives. Strong skills in business requirements & product coverage analysis, data mapping across platforms, support solution design and technical implementation deliverables. The ability to help prototype, visualise, reengineer and automate processes in cross functional teams would be key.\nThe candidate must have strong analytical and problem solving skills and be able to work in a dynamic environment to rapidly produce robust solutions. The candidate will have to liaise with key stakeholders across Finance, MiddleOffice, Risk and Technology, have the ability to work independently to see initiatives through from inception to delivery and have a strong attention to detail.\nKey objectives critical to success\nStrong analytical and communication skills with the initiative to identify and solve problems\nAbility to define requirements, solutions, scope and work across endtoend implementation lifecycle\nAbility to proactively reengineer processes and deliver digital solutions to improve department efficiency\nDemonstrate confidence in engaging and presenting complex solutions to senior individuals\nManage relationships with stakeholders in Finance, Risk and Technology functions across all regions\nSkills, experience, qualifications and knowledge required\nFinancial services experience, ideally in a transformational, middle office, collateral, product control role\nGood understanding of data preparation, modelling and visual data delivery through dashboards\nExperience with implementing data management capability centralisation of data stores, data sourcing, data quality and controls definition and simplification\nAn understanding of profiling current state, define future state, consolidate data insights and present these succinctly to senior stakeholders\nAbility to be proactive and use initiative to improve and reengineer processes and systems and help support legacy solutions e.g. a centralised adjustment mechanism and op model\nStrong communication skills to accurately elicit requirements from stakeholders, present project updates to senior management and provide training to project end users\nAbility to take ownership of end to end project management from inception to delivery.\nProficient in effectively documenting complex business and technical requirements and workflows using a variety of tools and ability to present these to business and technology stakeholders\nExperience of Agile process and tools, e.g. JIRA, Confluence etc\nCoordinate UAT testing for new process implementation with an ability to identify risks involved and communicate with respective teams for resolution\nWillingness to help others across the department in improving their digital skills through training sessions or workshops\nDesirable skills and experience\nUnderstanding of collateral management, P&L reporting, balance sheet substantiation, and trade lifecycle controls\nSome experience of using digital tools would be beneficial that would support key platform data migrations e.g. Python, Alteryx, VBA\nBasic understanding of equities, fixed income and derivatives products\nWe are committed to providing equal opportunities throughout employment including in the recruitment, training and development of employees. We prohibit discrimination in the workplace whether on grounds of gender, marital or domestic partnership status, pregnancy, carer s responsibilities, sexual orientation, gender identity, gender expression, race, color, national or ethnic origins, religious belief, disability or age.\n*Applying for this role does not amount to a job offer or create an obligation on Nomura to provide a job offer. The expression ""Nomura"" refers to Nomura Services India Private Limited together with its affiliates.",Industry Type: Financial Services,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Prototype', 'Data management', 'Project management', 'Fixed income', 'Agile', 'Data quality', 'JIRA', 'Financial services', 'Python', 'Recruitment']",2025-06-12 14:29:07
"Business Systems Analyst, Collaboration Tools",Zendesk,2 - 7 years,Not Disclosed,['Bengaluru'],"Business Analyst, Collaboration Tools\nZendesk is re-envisioning how we use our collaboration tools to provide an outstanding employee experience! We want to be innovators with how employees are served and provide a showcase example that our industry peers can follow. Successful candidates should be experienced with managing Google Workspace (formerly G Suite) as well as other collaboration tools, successful at working with internal partners, and proficient in implementing leading SaaS applications. The successful candidate will collaborate with various internal partners, including but not limited to, Product Development, People & Places, Go To Market, and Internal Communications.\nThis role resides within the Enterprise-Wide Applications team and will have a variety of responsibilities, including assisting with strategic projects from start to finish, implementing governance within our collaboration applications, collaborating with leadership to develop and maintain our long-term roadmap to ensure scalable, secure, and innovative solutions to facilitate Zendesk s growth. Partner with our Global Service Desk to train the team on standard methodologies and create and maintain both internal and employee-facing documentation for supporting Google Workspace and other collaboration applications to ensure best-in-class service for all of Zendesk!\nThis position is in-office, but candidates only have to work 2-3 days onsite per week. We require fluency in written and spoken English.\nWhat youll be doing\nPartner closely with various Zendesk organizations to turn short and long-term business needs into high quality, scalable, secure systems to enable Zendesk s critical initiatives.\nDevelop positive relationships with business partners. Understand their goals, workflows, and processes. Use your deep system knowledge to drive system strategy/vision, design, and implementation to mutually beneficial ends.\nWork in partnership with management and business partners to prioritize and shape our team s roadmap and long-range planning.\nAssist with system improvement projects from design through implementation and support. The individual in this role will be working directly with users to collect requirements, implement, test and deploy new features on a periodic basis.\nIdentify manual processes and problems for both the business and the employees who use Enterprise-Wide Applications and work to determine solutions.\nSupport our Global Service Desk team in technical troubleshooting employee issues.\nWhat you bring to the role\nBasic Qualifications:\n2+ years of experience administering Google Workspace\nProven track record of having implemented, improved and supported enterprise-class SaaS systems. This includes planning, analysis and design, configuration, development, data migrations, system testing, cutover plan, and production support.\nDemonstrable ability to work closely with a diverse and distributed team\nSolid grasp of IT fundamentals, including SDLC, agile methodologies, and change management.\nBachelors degree or equivalent work experience\nPreferred Qualifications:\n2+ years of consulting experience, preferably focused on process optimization, system implementations, and application integrations.\nExperience handling multiple SaaS collaboration applications\nCertified Google Workspace Admin or Developer\nFamiliar with Google Workspace APIs, using GAM, and experienced using SDLC to build and maintain integrations with Integration Platform as a Service (iPaaS) technology.\nZendesk endeavors to make reasonable accommodations for applicants with disabilities and disabled veterans pursuant to applicable federal and state law.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System testing', 'Change management', 'Process optimization', 'Business Analyst', 'Production support', 'Consulting', 'Agile', 'Customer service', 'Troubleshooting', 'SDLC']",2025-06-12 14:29:41
Lead Support Analyst,Nomura,10 - 11 years,Not Disclosed,['Mumbai'],"Job Title: Lead Support Analyst\nJob Code: 10182\nCountry: IN\nCity: Mumbai\nSkill Category: IT\\Technology\nDescription:\nRole & Responsibilities:\nInstinet Cloud Operations team manages daily operations for Instinet s cloud portfolio primarily based on the Amazon Web Services (AWS) platform. The team collaborates with Applications and Infrastructure teams to operate the cloud environment.\nThis is a handson technical role . The selected person would be part of Mumbai Instinet Cloud Operations and would report to Instinet IT Cloud Ops Head. It s a very high ownership as the selected person would be part of a team that s responsible for a significant part of overall global Cloud Ops coverage. In addition, the selected person would independently work with global stakeholders and various Application groups towards high quality delivery out of Mumbai Cloud Ops team.\nDuties & responsibilities (includes but is not limited to):\nManage incoming service requests via JIRA Service Desk. Perform triage, scheduling and execution of incoming requests. Coordinate with internal teams to successfully resolve requests.\nRealtime monitoring and alerting, proactive notifications, problem isolation and remediation.\nCloud environment change management, coordination and provisioning/execution\nAutomation for BAU activities such as account creation, landing zone management, application inventory, trusted advisor recommendations, cost oversight, operational guard rails etc.\nSoftware Lifecycle Maintenance (development and bug fixes) of established automation frameworks such as Backup management, cost management, landing zone etc.\nMonthly reporting of KPIs; apply continuous improvement methodology to support required KPIs.\nWilling to own deliveries and work handson across all aspects of Cloud Ops delivery out of Mumbai\nWilling to learn new technologies/tool as required, in order to effectively deliver output.\nShifts:\nMon to Friday 7.30 am IST to 4.30 pm IST\nMon to Friday 4 pm IST to 01:00 am IST\nKey Skills:\nMandatory\nDesirable\nDomain\nAWS Certified SysOps Administrator certification or a higher AWS Professional certification.\nPrior experience of working in IBs is a plus but not mandatory\nTechnical\nExperience utilizing service desk tools such as JIRA, Service Now etc.\nStrong Operations and/or NOC (Network Operations Centre) experience including monitoring, alert response, problem isolation and remediation.\nStrong experience with operating applications within Amazon Web Services (AWS). Lambda, Config, IAM ,Ec2,RDS , Landing zone Others\nAbility to utilize and perform administration via a command line interface in Linux or Unix.\nKnowledge and experience utilizing Git via command line\nStrong Unix/Linux skills well versed in tasks like file editing, system resource monitoring, running and scheduling processes, and troubleshooting system issues.\nScripting shell scripting or Python scripting\nKnowledge and experience with creating, maintaining and updating YAML and JSON configuration files.\nPrior release management experience\nStrong verbal and written communication skills (as this is a senior global stakeholder facing role)\nQuick learner\nHighly proactive and takes initiative to identify problem areas to evolve solutions.\nClient focused and attentive to businesscritical issues\nWe are committed to providing equal opportunities throughout employment including in the recruitment, training and development of employees. We prohibit discrimination in the workplace whether on grounds of gender, marital or domestic partnership status, pregnancy, carer s responsibilities, sexual orientation, gender identity, gender expression, race, color, national or ethnic origins, religious belief, disability or age.\n*Applying for this role does not amount to a job offer or create an obligation on Nomura to provide a job offer. The expression ""Nomura"" refers to Nomura Services India Private Limited together with its affiliates.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Change management', 'Automation', 'Linux', 'Scheduling', 'Troubleshooting', 'Continuous improvement', 'Operations', 'Monitoring', 'Recruitment']",2025-06-12 14:29:44
Senior Cloud Engineer,Infrassist Technologies Pvt. Ltd.,2 - 7 years,1-6 Lacs P.A.,['Ahmedabad'],"Number of open positions 2\n\nTitle: Cloud Engineer or Sr. Cloud Engineer\n\nExperience: 3-5 years relevant work experience\n\nQualification, Experience & Skills Required\n\nGood to have Professional Certification in Domain (1. Microsoft 365 Certified: Administrator Expert / Endpoint Administrator 2. Azure Administrator)\nExcellent oral & written communication skills Ready to work in morning & afternoon shifts along with occasional night shifts (if required).\nThe purpose of the role is to be the main customer facing point of contact for all Microsoft Office 365 & Azure Cloud projects implementation & consulting work.\nAbility to work intuitively, along or as part of a team environment, Self-starter with ability to plan and execute work.\nProven hands-on experience in Implementing, Configuring, Testing, Migrating & Securing Microsoft 365 & Azure Cloud Solutions.\nMicrosoft Windows Active Directory, Azure AD Connect Azure AD Premium Features deployment mainly User & Device based Conditional Access, Multi-Factor Authentication Policies.\nEmail & data Migrations from different source environments to Office 365 (Exchange & SharePoint Online, Microsoft Teams & One Drive for Business respectively) using Microsoft methods, tools and/or 3rd party migration solutions bit titan, SkyKick, AvePoint, Sharegate etc.\nMicrosoft Mobile Device Management (Intune MDM & MAM) Solutions, Windows Autopilot Technologies",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Azure Certified', 'Cloud Administration', 'microsoft 365', 'Endpoint', 'Azure Virtual Desktop', 'Entra Id', 'Microsoft Certified', 'Sharegate', 'Networking', 'Migration', 'avepoint', 'Intune', 'Autopilot', 'microsoft mobile device management', 'Security', 'Active Directory', 'MDM', 'Mam', 'skykick', 'Bittitan']",2025-06-12 14:29:47
Senior Devops Engineer,Epam Systems,5 - 8 years,18-22.5 Lacs P.A.,"['Chennai', 'Coimbatore']","Proven experience as a DevOps Engineer, with a focus on designing and implementing cloud-based infrastructure.\nStrong expertise in cloud platforms such as Amazon Web Services (AWS)\nExperience with infrastructure-as-code tools such as Terraform.\nProficiency in scripting languages (e.g., Python/Shell) for automation and infrastructure management.\nDeep understanding of CI/CD concepts and experience with CI/CD tools such as Jenkins, GitLab CI/CD\nFamiliarity with containerization and orchestration technologies like Docker and Kubernetes.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Jenkins', 'Gitlab', 'Terraform', 'Docker', 'Bash Scripting', 'Shell Scripting', 'Ansible', 'Groovy', 'Kubernetes', 'Python']",2025-06-12 14:29:49
Oracle Database Developer,Mobile Programming,6 - 10 years,Not Disclosed,['Bengaluru'],"We are hiring an experienced Oracle Database Developer for a prestigious client, Siemens. The ideal candidate should have deep expertise in Oracle databases with a solid foundation in SQL and PL/SQL development.\nThis role demands experience in database migrations, trigger management, and a basic understanding of PostgreSQL.\nKey Responsibilities:Design, develop, and maintain Oracle database solutions using SQL and PL/SQLHandle database migrations, develop and manage triggers, stored procedures, and packages Optimize and troubleshoot performance issues Collaborate with development and QA teams to ensure seamless data integrationWork with PostgreSQL for basic database interactions and data migration tasks Technical Skills\n(Mandatory):Oracle DB (11g, 12c, 19c)SQL, PL/SQLDatabase Migrations Triggers\nAdditional:Basic knowledge of PostgreSQL",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle Database', 'QA', 'PostgreSQL', 'PL/SQL Database Migrations', 'SQL']",2025-06-12 14:29:51
Technical Lead - PLM,Tata Technologies,8 - 13 years,Not Disclosed,['Pune'],"TC Migration Lead:Minimum 8 + years of experience in developing Teamcenter\nHands-on experience in Teamcenter implementations in various phases of project like business blueprinting, configuration, cut-over and post go-live support\nStrong expertise of Teamce nter product architecture and its integration frameworks like T4EA/T4S/T4O/CAD Integrations etc. Technical capabilities in Part Management, BOM Management, Change Management, Classification, Workflow, Organization\nProficiency in Teamcenter ITK, RAC, BMIDE, and SOA customization\nExperience with Teamcenter workflows, access control, and BMIDE configurations\nExperience with SQL/Oracle and d atabase management systems\nExperience in C/C++ programming language and TC server side (ITK & SOA), BMIDE, Java, HTML, XML and Web services and Active Workspace Configuration and declarative programming\nExperience with data migration, mapping, transformation, data validation to Teamcenter from legacy applications\nShould have exposure to CAD, NX and Teamcenter",Industry Type: Building Material (Cement),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Teamcenter', 'c++', 'caa', 'oracle', 'soa', 'web services', 'product life cycle management', 'engineering change management', 'bmide', 'cad', 'data migration', 'change management', 'catia', 'sql', 'plm', 'java', 'rac', 'xml', 'pdm', 'teamcenter itk', 'html', 'sap plm', 'teamcenter unified']",2025-06-12 14:29:53
MDM Developer,NetApp,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nParticipates in reviewing, analyzing, and modifying client/server applications and systems.\nJob Requirements\nDevelop and maintain integrations between CDM and other systems, such as CRM, order management, and other boundary systems.\nCustomize and extend MDM functionality using Oracle tools, such as Oracle Integration Cloud, Oracle Application Composer, Oracle Visual Builder.",,,,"['data management', 'web services', 'unit testing', 'groovy scripting', 'data migration', 'tools', 'oracle fusion', 'master data management', 'sql', 'plsql', 'cloud', 'operations', 'java', 'spark', 'oracle erp', 'visual', 'etl', 'pubsub', 'rest', 'python', 'oracle', 'software testing', 'application', 'mdm', 'integration', 'oracle sql', 'data integration']",2025-06-12 14:29:56
Marketing Analytics - TL / Assistant Manager,Leading Client,3 - 8 years,Not Disclosed,['Bengaluru'],Candidate Expectations\n3--12 years of work experience\nFlexible to work in shift as per client requirement\nCertification(s) Preferred:\nAdobe Experience Platform\nAdobe Real-Time Customer Data Platform\nAdobe Customer Journey Analytics\nJob Responsibilities\nUtilizing previous experience in CDPs (Customer data platforms) such as Adobe Experience Platform (RTCDP) or similar products;\nHaving an enhanced understanding of customer profile segmentation and experience in 360 degree view of customers in CDP for further analytical processing and decision making;\nShowcasing proven track record of managing successful CDP implementation/management and delivering capabilities that drive business growth;\nDemonstrating work experience in data architecture and data modeling; preferably with experience working in CDP CRM paid media social and offline data;\nEnsuring an enhanced understanding of data-driven marketing analytics and relevance / usage of real-time event data and associated attributes;\nSetting strategic direction and driving execution through collaboration with a cross-functional team;\nDemonstrating familiarity with CRM and Marketing Automation platforms (i.e. Salesforce Sales Cloud Salesforce Marketing Cloud etc.);\nHaving extensive hands on expertise in implementing/administering Adobe Experience Cloud Products and marketing automation platforms and technologies;\nEnsuring an enhanced understanding of identity resolution components and the enabling technology i.e. profile merge rules identity graph identity service provider etc.;\nWorking with audience-based digital marketing strategy either at an agency or brand;\nDriving project management through a full lifecycle including the ability to prioritize sequence execute and deliver projects on time and on budget;\nTranslating business requirements and objectives into segmentation strategies and audience building logic;\nOwning Adobe AEP and driving proactive platform ownership that is directly aligned to the day-to-day delivery of marketing tactics and closely connected to the other digital marketing teams to develop capabilities and manage technology roadmap;\nHaving oversight of the CDP technology solution (e.g. oversee steady state support teams interact with business owner plan for break / fix and other enhancements / maintenance);\nDriving how customer and prospect information is unified across sales marketing and service channels;\nCollaborating with Insights team to refine and optimize measurement process;\nImplementing and refining Adobe Products and marketing automation system;\nCollaborating with Marketing leadership to evolve the use of data within the marketing function allowing us to stay ahead responding to clients in real-time and limiting redundancy.\nContact Person : - Subhikshaa\nContact Number : - 9840114687,Industry Type: BPM / BPO,Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['Adobe Experience Platform', 'Adobe Customer Journey Analytics', 'Salesforce Sales Cloud', 'Sales Accreditation', 'Paid Media', 'Adobe Real-Time Customer Data Platform', 'Salesforce Marketing Cloud', 'Adobe Experience Cloud', 'Marketing Automation platforms', 'CRM']",2025-06-12 14:29:58
Team center developer,tekskill,4 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Responsibilities\nDevelop and customize solutions in Siemens Teamcenter based on business requirements.\nImplement BMIDE configurations including data model changes, business objects, and properties.\nDesign and develop ITK and SOA-based server-side customizations.\nWork on Active Workspace Client (AWC) customizations including stylesheets, XRTs, and client extensions.\nTroubleshoot and resolve issues related to Teamcenter performance, customization, and user support.\nParticipate in data migration activities using tools like Import/Export (IPEC), BMIDE XML, or custom scripts.\nSupport Teamcenter integrations with CAD tools (NX, CATIA, etc.) and other enterprise applications (SAP, CRM).\nDevelop and maintain workflows, handlers, and process templates.\nCollaborate with functional teams to understand requirements and deliver scalable PLM solutions.\nProvide technical documentation, code reviews, and mentoring for junior team members if needed.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ITK', 'team center', 'SOA', 'Bmide']",2025-06-12 14:30:00
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Bengaluru'],"Responsibilities :\n\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:30:02
ZuroCPQ Developer,Mobile Programming,5 - 10 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:Implement and configure ZuroCPQ to meet business requirements.Customize and extend the functionality of the ZuroCPQ application based on business needs.\nCollaborate with cross-functional teams to understand the requirements and ensure that the CPQ solutions align with sales processes.\nDevelop workflows and pricing rules for complex quotes and configurations.Integrate ZuroCPQ with other enterprise applications, including CRM and ERP systems.Provide technical support and troubleshooting for ZuroCPQ users.\nCreate and maintain technical documentation for the solutions developed.Participate in the design and review process for system enhancements and new features.Monitor and ensure system performance and data integrity.\nRequired Skills and Qualifications:\n5 to 10 years of experience in implementing, configuring, and customizing CPQ solutions.Hands-on experience with ZuroCPQ is essential.\nProficient in pricing and quoting tools, and experience integrating CPQ solutions with CRM (Salesforce) and ERP systems.\nStrong understanding of Salesforce and Salesforce CPQ (if applicable).Ability to work in a collaborative environment with sales, product, and engineering teams.Strong analytical skills and the ability to solve complex business challenges.\nFamiliarity with API integration and data migration.\nUnderstanding of workflow automation and business process optimization.\nAbility to write clean and maintainable code and deliver effective solutions.Strong problem-solving skills and attention to detail.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ZuroCPQ', 'Salesforce CPQ', 'ERP systems', 'CPQ', 'data migration', 'API integration', 'CRM', 'Salesforce']",2025-06-12 14:30:05
Informatica/ETL PowerCenter Developer,Saama Technologies,7 - 12 years,Not Disclosed,"['Pune', 'Chennai', 'Coimbatore']","Role Description: The Informatica/ETL PowerCenter Developer would need to have at least 7+ years of experience.\n\nResponsibilities and Qualifications:  \nParticipates in ETL Design of new or changing mappings and workflows with the team and prepares technical specifications.\nCreates ETL Mappings, Mapplets, Workflows, Worklets using Informatica PowerCenter 10.x and prepare corresponding documentation.",,,,"['Etl Informatica', 'PLSQL', 'Informatica Powercenter', 'SQL']",2025-06-12 14:30:07
Informatica MDM-Manager,Mclaren Strategic Ventures India,9 - 14 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Bengaluru']","Role & responsibilities\nD for IDMC Manager: 8+ years of experience in MDM development, with at least 2 years on the Informatica IDMC platform. Key Responsibilities: â€¢ Lead the development and implementation of MDM solutions on the Informatica IDMC platform. â€¢ Design and configure Business 360 (B360), Customer 360 (C360), Product 360 (P360), and Reference 360 (R360) solutions. â€¢ Implement match/merge logic, survivorship rules, business validations, and workflow orchestration using Cloud Application Integration (CAI). â€¢ Configure Data Quality (DQ) services and perform data profiling to support cleansing and validation processes. â€¢ Integrate MDM with enterprise systems such as ERP, CRM, and data warehouses using IDMCs standard connectors, APIs, and real-time integration patterns. â€¢ Collaborate with data architects, business stakeholders, and project teams to gather requirements and translate them into scalable MDM solutions. â€¢ Utilize data modeling best practices to design and maintain golden records for domains like Customer, Product. â€¢ Work with real-time REST APIs exposed by Informatica for operations such as search, create, update, and delete. â€¢ Provide leadership in solution design reviews, troubleshooting, performance tuning, and production support. JD for IDMC Developer: 4+ years of experience in MDM development, with at least 1 year on the Informatica IDMC platform. Key Responsibilities: â€¢ Develop and implement Master Data Management (MDM) solutions on the Informatica IDMC platform. â€¢ Configure and maintain IDMC components such as Customer 360 (C360), Product 360 (P360), Business 360 (B360), and Reference 360 (R360). â€¢ Design and implement match rules, business rules, survivorship rules, and data validations. â€¢ Build and orchestrate workflows using Cloud Application Integration (CAI). â€¢ Perform data profiling and implement data quality (DQ) rules and transformations. â€¢ Develop data pipelines and workflows using IDMC Data Integration service. â€¢ Integrate IDMC MDM with external systems like ERP, CRM, and data lakes using connectors and APIs. â€¢ Use real-time REST APIs provided by IDMC for operations such as search, create, update, and delete. â€¢ Collaborate with data architects, analysts, and business users to gather requirements and deliver scalable solutions.""\nAdditional Information: SPOC - Rajalaxmi (rajalaxmi.k.kar@pwc.com)\nMandatory Skills IDMC MDM\nNice to have skills CDQ, IDQ\nInterview Mode Virtual Interview\nWork Model Remote\nCleanroom: No\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Informatica Mdm', 'Informatica Master Data Management', 'Master Data Management']",2025-06-12 14:30:09
Backend Developer - AWS / Java,Sadup Soft,7 - 11 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Designation : AWS Java back end developer\n\nExperience : 7-11yrs\n\nLocation : Remote, Delhi NCR, Bangalore, Chennai, Pune, Kolkata, Ahmedabad, Mumbai, Hyderabad\n\nMust have skills :\n\n- 8-10 years of experience\n\n- 6+ years of experience in JAVA\n\n- Exposure to AWS\n\n- Willingness to work in Kotlin\n\n- Preference is to have the person from product-based company.\n\nRoles &responsibilities :\n\nAs an senior software engineer in our team, you will contribute not only to all aspects of backend, but also get involved in all aspects of software engineering: infra, data, application, deployment, monitoring, architecture, code reviews, pair and mob programming, etc.\n\nYou will also be leading a few initiatives with your design and architectural know-how and working incollaboration with other Architects and Managers.\n\nHere, we own everything we do: we own our successes as well as our mistakes and setbacks. In this spirit, we are looking for someone reliable, curious, and >\nYour day-to-day :\n\n- You will focus on delivering high quality solutions using your Java and Kotlin skills\n\n- You will work as part of an agile team, planning, refining, delivering and inspecting sprint deliveries\n\n- You work closely with product and design to understand context and vision\n\n- You will work with your peers across teams to make platform improvements and increase learning opportunities for all in a supportive environment\n\nWhat do you need to bring :\n\nToday we work with technologies like :\n\n- Java and Kotlin\n\n- Drop wizard, Spring Boot\n\n- Docker, Terraform\n\n- PostgreSQL\n\n- AWS\n\n- Everything is hosted on Amazon Web Services - we use managed cloud services as much as we can.\n\nWe believe that you have :\n\n- 6 to 8 years of experience in software development\n- 5+ years of experience building distributed systems\n\n- 5 + years of experience with SQL and data modelling\n\n- Experience working with continuously delivered systems\n\n- A result and delivery-oriented mindset\n\n- A genuine learning and knowledge sharing spirit\n\n- Unix/Linux/Docker experience\n\n- Interest in or experience of FinTech industry",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Unix', 'Linux', 'PostgreSQL', 'Spring Boot', 'AWS', 'Backend Architecture', 'Kotlin']",2025-06-12 14:30:11
SAP MDM Specialist,Sadup Soft,7 - 10 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Job Title : SAP MDM Consultant.\n\nLocation : Remote,Delhi NCR, Bangalore, Chennai, Pune, Kolkata, Ahmedabad, Mumbai, Hyderabad\n\nExperience : 7-10 years.\n\nKey Responsibilities :\n\n- Own and manage all Master Data Management (MDM) activities for SAP projects, ensuring alignment with business objectives.\n\n- Develop and implement comprehensive MDM strategies and roadmaps.\n\n- Lead the design and implementation of data governance frameworks and processes.\n\n- Lead data migration and cutover activities in SAP S/4HANA projects, including Greenfield implementations, system migrations, and rollouts.\n\n- Develop and execute data migration plans, ensuring data accuracy and consistency.\n\n- Manage data cleansing, transformation, and validation processes.\n\n- Establish and implement MDM best practices and data management capabilities.\n\n- Define and enforce data management principles, policies, and lifecycle strategies.\n\n- Ensure data compliance with regulatory requirements and internal policies.\n\n- Work closely with MDM Leads and stakeholders to drive data governance initiatives.\n\n- Develop and implement data quality metrics and reporting mechanisms.\n\n- Monitor data quality and identify areas for improvement.\n\n- Implement data quality controls and validation rules.\n\n- Track and manage MDM objects, ensuring timely delivery and adherence to project timelines.\n\n- Participate in daily stand-ups, issue tracking, and dashboard updates.\n\n- Collaborate with cross-functional teams, including functional consultants, developers, and business stakeholders.\n\n- Identify risks and process improvements for MDM.\n\n- Conduct training sessions for teams on S/4HANA MDM best practices and processes.\n\n- Develop and maintain training materials and documentation.\n\nRequired Skills & Qualifications :\n\n- 7-10 years of experience in SAP Master Data Management (MDM).\n\n- Strong knowledge of SAP S/4HANA, Data Migration, and Rollouts.\n\n- Expertise in data governance, lifecycle management, and compliance.\n\n- Experience in defining data management principles, policies, and lifecycle strategies.\n\n- Ability to monitor data quality with consistent metrics and reporting.\n\n- Familiarity with KANBAN boards, ticketing tools, and dashboards.\n\n- Strong problem-solving and communication skills.\n\n- Ability to track and manage MDM objects, ensuring timely delivery.\n\nPreferred Skills :\n\n- Experience in training teams on MDM best practices.\n\nAutomation & Productivity Tools :\n\n- Knowledge of automation and productivity improvement tools.\n\n- Familiarity with ABAP and SQL.\n\n- Experience with SAP Data Services or other data migration tools.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP MDM', 'SAP ERP', 'S/4 HANA', 'SAP Implementation', 'SAP projects']",2025-06-12 14:30:13
SAP DMS Specialist,Sadup Soft,5 - 10 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Locations : Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote\n\nAbout the Role :\nWe are seeking a highly skilled SAP DMS Specialist to manage, configure, and optimize the Document Management System (DMS) within the SAP environment. The successful candidate will be responsible for overseeing document lifecycle management, ensuring seamless integration of DMS with other SAP modules, and maintaining security and compliance standards. This role requires collaboration with cross-functional teams to implement solutions that enhance document management processes, improve data accessibility, and support business operations.\n\nResponsibilities :\n\n- Provide hands-on expertise in SAP DMS configuration and management, with 5-10 years of relevant experience.\n\n- Manage the complete document lifecycle within SAP DMS, including creation, storage, retrieval, version control, and archiving.\n\n- Configure and maintain metadata within SAP DMS to ensure accurate document categorization and efficient searching.\n\n- Implement and manage version control strategies within SAP DMS to track document changes and maintain audit trails.\n\n- Possess strong experience with SAP ECC and SAP S/4HANA, including a deep understanding of how DMS integrates with other SAP modules such as Materials Management (MM), Plant Maintenance (PM), and Quality Management (QM).\n\n- Design, implement, and manage SAP DMS workflows to automate document routing, approval processes, and other document-related tasks.\n\n- Define and implement document categorization strategies within SAP DMS to organize and classify documents effectively.\n\n- Configure and manage various storage solutions integrated with SAP DMS, ensuring optimal performance and accessibility.\n\n- Implement and maintain SAP DMS security protocols, including user roles, authorizations, and access controls, to protect sensitive information.\n\n- Apply knowledge of compliance standards and document retention policies within SAP systems to ensure adherence to regulatory requirements.\n\n- Collaborate with business users and IT teams to gather requirements and translate them into effective SAP DMS solutions.\n\n- Develop and maintain comprehensive documentation for SAP DMS configurations, processes, and user guides.\n\n- Provide end-user training and support for SAP DMS functionalities.\n\n- Troubleshoot and resolve issues related to SAP DMS functionality and integrations.\n\n- Participate in upgrades and enhancements of the SAP DMS system.\n\n- Stay up-to-date with the latest SAP DMS features and best practices.\n\nQualifications : Bachelor's degree in Computer Science, Information Technology, or a related field.\n\nRequired Skills :\n\n- 5-10 years of hands-on experience in SAP DMS configuration and management.\n\n- Deep expertise in document lifecycle management, metadata configuration, and version control in SAP DMS.\n\n- Proven strong experience with SAP ECC and SAP S/4HANA.\n\n- Extensive experience with the integration of SAP DMS with other SAP modules (e.g., MM, PM, QM).\n\n- Proficient in designing, implementing, and managing SAP DMS workflows.\n\n- Strong understanding of document categorization principles and experience in implementing them within SAP DMS.\n\n- Experience in configuring and managing various storage solutions integrated with SAP DMS.\n\n- Comprehensive understanding and experience with SAP DMS security protocols and access controls.\n\n- Solid knowledge of compliance standards and document retention policies within SAP systems.\n\n- Excellent analytical and problem-solving skills.\n\n- Strong communication and interpersonal skills, with the ability to collaborate effectively with cross functional teams.\n\n- Ability to work independently and manage tasks effectively in a remote environment.\n\nPreferred Skills :\n\n- Experience with SAP Engineering Control Center (ECTR).\n\n- Knowledge of SAP Content Server.\n\n- Experience with OpenText Extended ECM for SAP Solutions.\n\n- Familiarity with data migration tools and techniques related to SAP DMS.\n\n- Experience in developing custom solutions or enhancements within SAP DMS.\n\n- SAP DMS certification.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP DMS', 'SAP ECC', 'SAP S/4 HANA', 'SAP PM', 'SAP QM', 'Document Management System', 'SAP MM', 'SAP Integration', 'SAP System Configuration', 'SAP Implementation', 'SAP Support']",2025-06-12 14:30:16
SAP ABAP Developer,Mobile Programming,6 - 9 years,Not Disclosed,['Bengaluru'],"We are looking for an experienced SAP ABAP Developer to join our team in Bengaluru. As an SAP ABAP Developer, you will be responsible for designing, developing, and maintaining custom SAP applications and solutions using the ABAP programming language. You will work on enhancements, custom reports, interfaces, and data migration tasks.\nYour role will involve collaborating with functional teams to ensure the seamless integration of SAP modules and a smooth business workflow.\nKey Responsibilities:\nDevelop and maintain custom ABAP programs and SAP modules to support business needs.\nDesign and implement SAP enhancements, reports, and interfaces.Work on data migration and system integration between SAP and other enterprise systems.Collaborate with functional teams to understand business requirements and translate them into technical solutions.\nPerform unit testing, integration testing, and ensure the quality of deliverables.\nProvide support for SAP troubleshooting and issue resolution.Ensure that all developed solutions comply with SAP standards and best practices.Maintain up-to-date documentation for all technical solutions and processes.Participate in code reviews and contribute to improving the development process.\nRequired Skills and Qualifications:6-9 years of hands-on experience as an SAP ABAP Developer.\nProficiency in ABAP programming language with a deep understanding of SAP architecture.\nExperience with SAP modules (e.g., MM, SD, FICO, etc.) and their integration.\nExpertise in ABAP Workbench, Smart Forms, ALV Reports, and BAPI development.\nExperience in Data Migration using LSMW, IDOC, and BAPI.\nHands-on experience with SAP NetWeaver, SAP Fiori, and HANA integration.\nStrong problem-solving and debugging skills.\nKnowledge of performance optimization and security best practices for ABAP programs.Ability to work with functional teams to define requirements and design solutions.\nStrong communication skills to collaborate with business stakeholders and technical teams.\nTechnical Skills:\nSAP ABAP | ABAP Workbench | Smart Forms | ALV Reports | BAPI | LSMW | IDOC | SAP NetWeaver | SAP Fiori | SAP HANA | SAP Modules (MM, SD, FICO) | Data Migration | SAP OData | ABAP Objects | UI5 | Object-Oriented Programming | SAP PI/PO | Performance Optimization | Debugging | SAP Workflow | Integration",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'SAP Fiori', 'ABAP Workbench', 'integration testing', 'SAP HANA', 'SAP ABAP', 'unit testing', 'BAPI development']",2025-06-12 14:30:18
SAP PP Consultant - S/4 HANA Module,Zettamine Labs,6 - 11 years,Not Disclosed,"['Mumbai', 'Pune', 'Bengaluru']","Location : Pune, Mumbai, Bangalore\n\nNotice Period : Immediate\n\nJob Overview :\n\nWe are looking for an experienced SAP PP (Production Planning) Consultant to join our team. The ideal candidate will have strong expertise in SAP PP module, including configuration, implementation, and integration with other SAP modules.\n\nKey Responsibilities :\n\n- Implement and configure SAP PP to optimize production planning and control.\n\n- Collaborate with stakeholders to gather business requirements and translate them into system solutions.\n\n- Perform end-to-end process mapping for demand planning, capacity planning, and shop floor control.\n\n- Ensure seamless integration with SAP MM, SD, and QM modules.\n\n- Troubleshoot and resolve technical issues in SAP PP environments.\n\n- Provide end-user training and documentation support.\n\n- Work closely with SAP ERP teams to enable master data integration and production planning\nrequirements.\n\n- Support SAP S/4HANA migration and implementation projects.\n\n- Optimize Bill of Materials (BOM), Routing, and Work Centers for efficient production\nprocesses.\n\n- Lead data migration from legacy systems to SAP PP.\n\nRequired Skills and Experience :\n\n- 6-12 years of experience in SAP PP implementation.\n\n- Strong knowledge of production planning, demand management, and shop floor control.\n\n- Hands-on experience in SAP PP configuration, development (ABAP), and integration.\n\n- Expertise in SAP S/4HANA integration for production planning.\n\n- Experience in capacity planning, MRP, and scheduling.\n\n- Strong understanding of warehouse business processes related to production planning.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['S/4 HANA Module', 'SAP QM', 'S/4 HANA', 'SAP SD', 'SAP PP', 'SAP ABAP', 'SAP MM', 'SAP Integration', 'SAP Implementation', 'SAP WM']",2025-06-12 14:30:21
"Engineer, ETL",XL India Business Services Pvt. Ltd,2 - 5 years,Not Disclosed,['Gurugram'],"Engineer, ETL Gurgaon/Bangalore, India AXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained industrious advantage\n\nOur Chief Data Office also known as our Innovation, Data Intelligence & Analytics team (IDA) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking an Data Engineer\n\nThe role will support the team s efforts towards creating, enhancing, and stabilizing the Enterprise data lake through the development of the data pipelines\n\nThis role requires a person who is a team player and can work well with team members from other disciplines to deliver data in an efficient and strategic manner\n\nWhat you ll be DOING What will your essential responsibilities include? Act as a data engineering expert and partner to Global Technology and data consumers in controlling complexity and cost of the data platform, whilst enabling performance, governance, and maintainability of the estate\n\nUnderstand current and future data consumption patterns, architecture (granular level), partner with Architects to ensure optimal design of data layers\n\nApply best practices in Data architecture\n\nFor example, balance between materialization and virtualization, optimal level of de-normalization, caching and partitioning strategies, choice of storage and querying technology, performance tuning\n\nLeading and hands-on execution of research into new technologies\n\nFormulating frameworks for assessment of new technology vs business benefit, implications for data consumers\n\nAct as a best practice expert, blueprint creator of ways of working such as testing, logging, CI/CD, observability, release, enabling rapid growth in data inventory and utilization of Data Science Platform\n\nDesign prototypes and work in a fast-paced iterative solution delivery model\n\nDesign, Develop and maintain ETL pipelines using Pyspark in Azure Databricks using delta tables\n\nUse Harness for deployment pipeline\n\nMonitor Performance of ETL Jobs, resolve any issue that arose and improve the performance metrics as needed\n\nDiagnose system performance issue related to data processing and implement solution to address them\n\nCollaborate with other teams to ensure successful integration of data pipelines into larger system architecture requirement\n\nMaintain integrity and quality across all pipelines and environments\n\nUnderstand and follow secure coding practice to make sure code is not vulnerable\n\nYou will report to Technical Lead\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Effective Communication skills\n\nBachelor s degree in computer science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience\n\nRelevant years of extensive work experience in various data engineering & modeling techniques (relational, data warehouse, semi-structured, etc), application development, advanced data querying skills\n\nRelevant years of programming experience using Databricks\n\nRelevant years of experience using Microsoft Azure suite of products (ADF, synapse and ADLS)\n\nSolid knowledge on network and firewall concepts\n\nSolid experience writing, optimizing and analyzing SQL\n\nRelevant years of experience with Python\n\nAbility to break complex data requirements and architect solutions into achievable targets\n\nRobust familiarity with Software Development Life Cycle (SDLC) processes and workflow, especially Agile\n\nExperience using Harness\n\nTechnical lead responsible for both individual and team deliveries\n\nDesired Skills and Abilities: Worked in big data migration projects\n\nWorked on performance tuning both at database and big data platforms\n\nAbility to interpret complex data requirements and architect solutions\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExcellent basics on parquet files and delta files\n\nEffective Knowledge of Azure cloud computing platform\n\nFamiliarity with Reporting software - Power BI is a plus\n\nFamiliarity with DBT is a plus\n\nPassion for data and experience working within a data-driven organization\n\nYou care about what you do, and what we do",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'System architecture', 'Coding', 'Agile', 'Workflow', 'Application development', 'SDLC', 'SQL', 'Python', 'Firewall']",2025-06-12 14:30:23
Technial Lead - backend,Cheq Digital,7 - 9 years,40-55 Lacs P.A.,['Bengaluru'],"About CheQ\nHey Future Teammate! Ready to dive into the fintech revolution with us at CheQ? We're not your typical 9-to-5 crew; we're the dynamic force turning credit management into a fun and rewarding journey.\nImagine instant repayments that are not just manageable but downright enjoyable. Founded by ex-Flipkart executive, Aditya Soni, CheQ has processed over $2 Billion in credit repayments in a short span. Yes, that's buying all IPL teams together! We've built a user base of over a million and raised a whopping $16 M, backed by 3one4 Capital, Venture Highway and marquee angels like Ram Shriram & Dr. LloydMarquee fintech angels like Naveen Kukreja of PaisaBazar and Shailaz Nag of Dotpe trust our venture. We're on a mission to make credit easy, enjoyable, and filled with awesome rewards. Picture being part of a team that turns the credit maze into an adventure. If you're ready to make credit management a journey worth taking, where work feels like play, hit us up. Let's transform the game together! #JoinCheQ #FintechRevolution\n\nEngineering @CheQ\nEngineers at CheQ are true builders. They bring in a myriad of ideas, and create the infrastructure, systems and products that drive our services. Our engineers build Zero to One products, microservices, DBs and everything else (from farm to table).\n\nWith their magic they take a product, feature or experience from ideation to production, always keeping the customers and their interests in mind. This includes understanding consumer & business needs, critiquing & challenging product and design counterparts and owning the execution & delivery of the scope.\n\nWhat youll be doing\nWe are much more than our job descriptions,but here is where you will begin:\nCollaborate with stakeholders, including product owners, project managers, and scrum masters, to define and clarify project requirements.\nTranslate business requirements into technical specifications and ensure all stakeholders have a clear understanding of the project scope and objectives.\nFacilitate effective communication and coordination among cross-functional teams to ensure alignment and successful project delivery.\nDesign, develop, and maintain scalable and efficient software solutions that meet business needs.\nWrite clean, maintainable, and well-documented code while adhering to best practices and coding standards.\nPerform code reviews and provide constructive feedback to team members to ensure code quality and consistency.\nWork closely with the DevOps team to establish and maintain CI/CD pipelines for seamless product building, deployment, and testing across all release cycles from development to production.\nEnhance and apply a strong understanding of modern security principles and practices to the development and deployment of applications.\nImplement security measures such as authentication, authorization, data encryption, and vulnerability assessments to protect applications from security threats.\nRequirements\nLike us, youâ€™ll be deeply committed to delivering impactful outcomes for customers.\n7+ years of demonstrated ability to develop resilient, high-performance, and scalable code tailored to application usage demands.\nAbility to lead by example with hands-on development while managing project timelines and deliverables.\nExperience in agile methodologies and practices, including sprint planning and execution, to drive team performance and project success.\nExperience building RESTful web services using NodeJS.\nExperience writing batch/cron jobs using Python and Shell scripting.\nExperience in web application development using JavaScript and JavaScript libraries.\nHave a basic understanding of Typescript, JavaScript, HTML, CSS, JSON and REST based applications.\nExperience/Familiarity with RDBMS and NoSQL Database technologies like MySQL, MongoDB, Redis, ElasticSearch and other similar databases.\nUnderstanding of code versioning tools such as Git.\nUnderstanding of building applications deployed on the cloud using Google cloud platform or Amazon Web Services (AWS), Docker and kubernetes.\nExperienced in JS-based build /Package tools like Grunt, Gulp, Bower, Webpack and NPM.\nBenefits\nYou define your work\nWe acknowledge that your work does not define you. Itâ€™s you who will define your work here. We do not encourage trade-offs between work and life.\nPropelled by courage & care\nWe dare each other with the art of possible and then watch each otherâ€™s back delivering the solution with speed, agility, heart and rigor.\nLearn with the best\nWith a strong leadership team from diverse backgrounds, you can expect to get the best of many worlds\n\nAnd much more\n\nIndustry competitive compensation\nESOPs of a boat that you must onboard before it becomes a ship\nWork on real problems of India that will create Impact at scale\nWork with all the jazz and fancy that new and innovative technologies bring\n\nWhat you will not get\nWe come from a place of honesty. So letâ€™s set our expectations right!\nPredictability of work\nYou will be a spider in the web; we will throw everything at you!\nClimbing the slow ladder of Career Growth\nWe all love to hop, skip & grow!\nBureaucracy and slow decision making\nWhat was that again??\nMeetings, meetings & only meetings\nWe believe in agility, empowerment and get the work done!",Industry Type: FinTech / Payments,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MySQL', 'Node.Js', 'MongoDB']",2025-06-12 14:30:26
Application Manager,XL India Business Services Pvt. Ltd,14 - 18 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Application Manager Bangalore, Karnataka, India We invent the new to help the world move forward\n\nCombining powerful analytics and deeper insights with bigger ideas and innovative solutions, we free up our clients potential, thereby fulfilling our own\n\nTake it seriously\n\nMake it fun\n\nKnow it matters\n\nApplication Managers oversee technical teams within a Delivery Team and help to manage day-to-day tasks to ensure high levels of productivity, accuracy, and work priority\n\nApplication manager is responsible for the technical solution delivery and maintenance of the same in production\n\nWhat you ll be DOING What will your essential responsibilities include? Technically lead and manage Business Analysts and Developers including assignment of work\n\nAssists Delivery Lead to manage SI Partners by helping to provide partner day to day direction on prioritization and decisions\n\nPerforms deliverable reviews and manage measurement of deliverable quality\n\nAssists to maintain application standards like app certification, vendor management, release coordination, apply security standard etc Act as liaison between SI partner team and stakeholders\n\nEnsure technical team alignment with business expectations and delivery roadmap\n\nWill liaise and consult with the Architecture team to ensure design alignment with AXA XL s architecture strategy\n\nProvide technical SME assistance for the insurance billing and payment solutions (Ex\n\nGuidewire, Majesco, SAP)\n\nEstimate work requests at various levels\n\nPartners with Release Management to Coordinate Release Activities\n\nWorks with Operational Change Management team to ensure training materials and release notes are being delivered\n\nMonitor and execute release and deployment activities\n\nEnsure full compliance to AXA standards of the products for the business (incl\n\nSecurity & Data Privacy)\n\nAct as liaison between SI Partner team and stakeholders\n\nSolid experience working in an Agile environment\n\nAssist in Coordinating and Participate in Agile Ceremonies as required\n\nMonitor Agile ceremonies and activities to ensure compliance with Digital Factory standards\n\nYou will report to the Delivery Lead - Claims\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Relevant years of hands-on work experience with complex applications\n\nRelevant years of experience working in an Agile environment\n\nProven experience in Microsoft technical suite like Dot Net (Core, Standard), SQL Server, C#, VBDot Net, ASPDot Net\n\nDeep understanding of enterprise integration patterns (API, middleware and/or data migration, secured data flow) Cloud based experience with Azure\n\nDevOps practices including CI/CD pipelines, infrastructure as code and containerization Proficient in use of JIRA, Confluence, Bitbucket, team city and Data dog\n\nTimely and accurate completion of deliverables in a manner that is auditable, testable, and maintainable\n\nImplementation consistent with solution design and business specifications\n\nEnsure for technical integrity of changes made to systems\n\nAdherence to development governance & SDLC standards\n\nTeam leadership abilities required, including experience leading and mentoring development professionals\n\nMust be able to set priorities and multi-task\n\nPrior work experience with Commercial Lines of Insurance\n\nDesired Skills and Abilities: Proficiency with multiple application delivery models including Agile, iterative and waterfall\n\nBroad understanding of application development and support technologies\n\nPrior work experience in an insurance or technology field preferred\n\nPrior experience working with multiple vendor partners\n\nAdaptable to new/different strategies, programs, technologies, practices, cultures, etc Comfortable with change, able to easily make transitions\n\nBachelor s degree in the field of computer science, information systems or a related field preferred\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides competitive compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Change management', 'SAP', 'Billing', 'Agile', 'Application development', 'microsoft', 'Middleware', 'SDLC', 'Analytics', 'SQL']",2025-06-12 14:30:28
PostgreSQL DBA / Developer,Srs Business Solutions India,3 - 8 years,6-11 Lacs P.A.,['Jaipur'],"Hello,\n\nWe are hiring for ""PostgreSQL DBA / Developer"" for Jaipur Location.\n\nJob Title PostgreSQL DBA / Developer\nWork Mode: Work from Office\nExp: 3+Years\nLoc: Jaipur\nNotice Period: Immediate joiners(notice period served candidate)\n\nNOTE: We are looking for Immediate joiners(notice period served candidate)\n\nApply only If you are Immediate joiners(notice period served candidate)\n\nApply only If you are from Jaipur or Rajasthan.\n\n\nRequired experience:\nStrong expertise in PostgreSQL database development or administration\nShould have experience in writing queries, replication, Optimization and data migration\n\nNOTE: Let your co-workers or Circle know about this opportunity if you lack these skills set.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Postgresql', 'Postgresql Database Administration', 'Postgresql development']",2025-06-12 14:30:30
Test Lead,Infosys,5 - 10 years,10-20 Lacs P.A.,[],"Role & responsibilities\n\nThe Test Lead oversees the testing strategy and execution for the Microsoft Fabric migration and Power BI reporting solutions. This offshore role ensures quality, reliability, and client satisfaction through rigorous validation.\nThe successful candidate will have a strong testing background and coordination skills.\nResponsibilities\nDevelop and execute the testing strategy for Microsoft Fabric and Power BI deliverables.\nValidate data migration, pipeline functionality, and report accuracy against requirements.\nCoordinate with the Offshore Project Manager to align testing with development milestones.\nCollaborate with onsite technical leads to validate results and resolve defects. â€¢ Oversee offshore testers, ensuring comprehensive coverage and quality standards.\nProactively identify risks and articulate solutions to minimize delivery issues.\nSkills\nBachelors degree in IT, computer science, or a related field.\n5+ years of experience in test leadership for data platforms and BI solutions.\nKnowledge of Microsoft Fabric, Power BI, and data migration testing.\nProficiency with testing tools (e.g., Azure DevOps, Selenium) and SQL.\nStrong communication and stakeholder management skills.\nDetail-oriented with a focus on quality and continuous improvement\n1. JD for Data Modeler\nThe Data Modeler designs and implements data models for Microsoft Fabric and Power BI, supporting the migration from Oracle/Informatica. This offshore role ensures optimized data structures for performance and reporting needs. The successful candidate will bring expertise in data modeling and a collaborative approach.\nResponsibilities\nDevelop conceptual, logical, and physical data models for Microsoft Fabric and Power BI solutions.\nImplement data models for relational, dimensional, and data lake environments on target platforms.\nCollaborate with the Offshore Data Engineer and Onsite Data Modernization Architect to ensure model alignment.\nDefine and govern data modeling standards, tools, and best practices.\nOptimize data structures for query performance and scalability.\nProvide updates on modeling progress and dependencies to the Offshore Project Manager.\nSkills\nBachelorâ€™s or masterâ€™s degree in computer science, data science, or a related field.\n5+ years of data modeling experience with relational and NoSQL platforms.\nProficiency with modeling tools (e.g., Erwin, ER/Studio) and SQL.\nExperience with Microsoft Fabric, data lakes, and BI data structures.\nStrong analytical and communication skills for team collaboration.\nAttention to detail with a focus on performance and consistency.\nmanagement, communication, and presentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Test lead', 'Migration', 'power bi', 'microsoft fabric']",2025-06-12 14:30:32
Oracle Recruitment Cloud Expert,UST,7 - 12 years,Not Disclosed,"['Kochi', 'Bengaluru', 'Thiruvananthapuram']","Role: Oracle Recruitment Cloud Expert\nLocation: PAN India\n\nKey Responsibilities\n1. Development & Implementation\nInterpret application/component designs and develop accordingly\nCode, debug, test, document, and communicate development stages",,,,"['Oracle Hcm Cloud', 'Oracle Hcm', 'Oracle Recruiting Cloud']",2025-06-12 14:30:34
MDM Testing - Associate Analyst,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for a skilled MDM Testing Associate Analyst who will responsible for ensuring the quality and integrity of Master Data Management (MDM) applications through rigorous testing processes. This role involves collaborating with cross-functional teams to define testing objectives, scope, and deliverables, and to ensure that master data is accurate, consistent, and reliable. and comply with Amgens standard operating procedures, policies, and guidelines. Your expertise will be instrumental in ensuring quality and adherence to required standards so that the engineering teams can build and deploy products that are compliant.\nRoles & Responsibilities:\nTest Planning: Develop and implement comprehensive testing strategies for MDM applications, including defining test objectives, scope, and deliverables. This includes creating detailed test plans, test cases, and test scripts.\nTest Execution: Execute test cases, report defects, and ensure that all issues are resolved before deployment. This involves performing functional, integration, regression, and performance testing.\nData Analysis: Analyze data to identify trends, patterns, and insights that can be used to improve business processes and decision-making. This includes validating data accuracy, completeness, and consistency.\nCollaboration: Work closely with the MDM, RefData and DQDG team and other departments to ensure that the organizations data needs are met. This includes coordinating with data stewards, data architects, and business analysts.\nDocumentation: Maintain detailed documentation of test cases, test results, and any issues encountered during testing. This includes creating test summary reports and defect logs.\nQuality Assurance: Develop and implement data quality metrics to ensure the accuracy and consistency of master data. This includes conducting regular data audits and implementing data cleansing processes.\nCompliance: Ensure that all master data is compliant with data privacy and protection regulations. This includes adhering to industry standards and best practices for data management.\nTraining and Support: Provide training and support to end-users to ensure proper use of MDM systems. This includes creating user manuals and conducting training sessions\nStay current on new technologies, validation trends, and industry best practices to improve validation efficiencies.\nCollaborate and communicate effectively with the product teams.\nBasic Qualifications and Experience:\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\n2+ years of experience in MDM implementations, primarily with testing (pharmaceutical, biotech, medical devices, etc.)\nExtensive experience on ETL/ELT and MDM testing (Creating test plan, test scripts and execution of test scripts and bugs tracking/reporting in JIRA)\nInformatica MDM: Proficiency in Informatica MDM Hub console, configuration, IDD (Informatica Data Director), IDQ, and data modeling\nor\nReltio MDM: Experience with Reltio components, including data modeling, integration, validation, cleansing, and unification.\nAdvanced SQL: Ability to write and optimize complex SQL queries, including subqueries, joins, and window functions.\nData Manipulation: Skills in data transformation techniques like pivoting and unpivoting.\nStored Procedures and Triggers: Proficiency in creating and managing stored procedures and triggers for automation.\nPython: Strong skills in using Python for data analysis, including libraries like Pandas and NumPy etc.\nAutomation: Experience in automating tasks using Python scripts.\nMachine Learning: Basic understanding of machine learning concepts and libraries like scikit-learn.\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nGood-to-Have Skills:\nETL Processes: Knowledge of ETL processes for extracting, transforming, and loading data from various sources.\nData Quality Management: Skills in data profiling and cleansing using tools like Informatica.\nData Governance: Understanding of data governance frameworks and implementation.\nData Stewardship: Ability to work with data stewards to enforce data policies and standards.\nSelenium: Experience with Selenium for automated testing of web applications.\nJIRA: Familiarity with JIRA for issue tracking and test case management.\nPostman: Skills in using Postman for API testing.\nUnderstanding of compliance and regulatory considerations in master data.\nIn depth knowledge of GDPR and HIPPA guidelines.\nProfessional Certifications:\nMDM certification (Informatica or Reltio)\nSQL Certified\nAgile or SAFe certified\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MDM Testing', 'ETL Processes', 'Data Stewardship', 'MDM', 'Agile', 'Data Quality Management', 'Data Governance', 'SQL']",2025-06-12 14:30:37
CRM Technical professional,big 4,4 - 9 years,Not Disclosed,"['Gurugram', 'Bengaluru', 'Mumbai (All Areas)']","Job Summary\n1.1 CRM Technical\n1.1.1 Must Have\nDesign, develop, and implement Microsoft Dynamics 365 solutions to meet business requirements.\nKnowledge of minimum two D365 CRM modules like Sales, Service, Field Service and Customer Insights\nCustomize and configure D365 applications, including workflows, plugins, and integrations.\nStrong technical skills in Power Pages, PowerApps, Power Automate, and related technologies\nExperience with CanvasApp development\nKnowledge of form script using JS, Business Process Flow, Business Rules\nKnowledge of solution deployment, solution management\nKnowledge of Integrations WebAPI\nExperience with development tools like Visual Studio, XRMToolBox, Postman etc\nDevelop and maintain technical documentation for D365 solutions.\nMicrosoft Certification\n1.1.2 Good To have:\n(Must meet 50% for Con level, above 70% match for Sr. Con and above)\nProficiency in programming languages such as C#, .NET, JavaScript, and SQL.\nExperience with D365 integrations using APIs and web services.\nKnowledge of CI/CD pipeline configuration for deployment\nDevelopment knowledge on PCF Controls for both canvas and model driven apps\nAbility to create custom connectors for Power Automate\nExperience with SharePoint integration scenarios with Power Apps and D365\nExperience with Azure platform including functions, LogicApps, ServiceBus, KeyVault\nPowerBI report development Connecting data sources, importing data, and transforming data for Business intelligence.\nExperience with Large enterprise level data migration using Azure Data Factory, Kingsway soft\nKnowledge of CoPilot configuration\n1.1.3 Power Pages:\nDesign, develop, and implement web applications using Microsoft Power Pages.\nCustomize and configure Power Pages to meet business requirements.\nKnowledge of Table Permission, Forms, Lists configuration\nKnowledge of HTML, JS and Liquid for Web Template configuration\nExperience with Power Pages Design Studio and its features",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power apps', 'Microsoft Dynamics 365', 'Power Pages', 'CanvasApp development', 'Microsoft Certification', 'XRMToolBox', 'Power Automate', 'Visual Studio', 'Postman', 'Integrations WebAPI']",2025-06-12 14:30:39
.net fullstack developer - Azure Expert,Kyndryl,10 - 15 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward â€“ always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.\n\nThe Role\nAre you ready to join the team of software engineering experts at Kyndryl? We are seeking a talented Software Engineering Technical Specialist to contribute to our software engineering space and provide critical skills required for the development of cutting-edge products.  \n\nAs a Software Engineering Technical Specialist, you will develop solutions in specific domains such as Security, Systems, Databases, Networking Solutions, and more. You will be a leader â€“ contributing knowledge, guidance, technical expertise, and team leadership skills. Your leadership will be demonstrated in your work, to your customers, and within your teams.\n\nAt Kyndryl, we value effective communication and collaboration skills. When you recognise opportunities for business change, you will have the ability to clearly and persuasively communicate complex technical and business concepts to both customers and team members. Youâ€™ll be the go-to person for problem-solving of customersâ€™ business and technical issues. You have a knack for effectively identifying and framing problems, leading the collection of elements of information, and integrating this information to produce timely and thoughtful decisions. Your aim throughout, is to improve the effectiveness, efficiency and delivery of services through the use of technology and technical methods and methodologies.\n\nDriving the design, development, integration, delivery, and evolution of highly scalable distributed software you will integrate with other layers and offerings. You will provide deeper functionality and solutions to address customer needs. You will work closely with software engineers, architects, product managers, and partner teams to get high-quality products and features through the agile software development lifecycle.\n\nYour continuous grooming of features/user stories to estimate, identify technical risks/dependencies and clearly communicate them to project stakeholders will ensure the features are delivered with the right quality and within timeline. You will maintain and drive the clearing of technical debt, vulnerabilities, and currency of the 3rd party components within the product.\n\nAs a Software Engineering Technical Specialist, you will also coach and mentor engineers to design and implement highly available, secure, distributed software in a scalable architecture. This is an opportunity to make a real impact and contribute to the success of Kyndryl's innovative software products. Join us and become a key player in our team of software engineering experts!",,,,"['kubernetes', 'server', 'css', 'methods', 'openshift', 'data migration', 'azure migration', 'sql', 'iis', 'apache', 'devops', 'html', 'software engineering', 'api', 'agile principles', 'communication skills', 'process', 'new relic', 'microsoft azure', 'sql server', 'nosql', 'framework', 'devops automation', 'collaboration', 'splunk', '.net', 'aws']",2025-06-12 14:30:41
NEO4j / Neptune Developer,Mobile Programming,3 - 5 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced NEO4j / Neptune Developer to join our team in Hyderabad\nIn this role, you will be responsible for designing, implementing, and optimizing graph-based solutions using NEO4j or Amazon Neptune databases\nYou will collaborate with cross-functional teams to integrate and deploy graph technologies that solve complex business problems\nThis is a fantastic opportunity for someone who thrives in a dynamic environment and is excited about leveraging graph databases to create innovative solutions\nKey Responsibilities:\nDesign, develop, and maintain graph database models using NEO4j or Amazon Neptune\nDevelop and implement graph query languages like Cypher (for NEO4j) or SPARQL for efficient data retrieval\nOptimize graph database performance for large-scale data and high-volume queries\nCollaborate with teams to identify business requirements and design graph-based data models\nIntegrate graph database solutions with existing systems and applications\nTroubleshoot and resolve performance issues or bugs within the graph database solutions\nContribute to the continuous improvement of the development process, tools, and techniques\nProvide support for data migration and integration of graph technologies with other enterprise systems\nWrite high-quality, clean, and maintainable code, ensuring best practices are followed\nRequired Skills and Qualifications:\n3-5 years of experience in developing with NEO4j or Amazon Neptune\nStrong knowledge of graph database modeling, relationships, and graph theory\nProficiency in Cypher query language (for NEO4j) and SPARQL (for Amazon Neptune)\nHands-on experience with graph analytics and performance tuning\nExperience integrating graph databases with other systems and services\nFamiliarity with NoSQL databases and distributed data architectures\nUnderstanding of cloud-based graph database solutions (eg, AWS Neptune)\nAbility to work in an Agile development environment\nStrong troubleshooting and problem-solving skills\nExcellent written and verbal communication skills\nTechnical Skills:\nNEO4j | Amazon Neptune | Cypher | SPARQL | Graph Database Modeling | NoSQL | Graph Analytics | Python | Java | AWS | Data Migration | ETL | Cloud Solutions | Agile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['NEO4j', 'Java', 'NoSQL', 'Data Migration', 'Agile', 'ETL', 'AWS', 'Graph Analytics', 'Python', 'Cloud Solutions']",2025-06-12 14:30:44
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Agra'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n- Configure and refine related workflows within NetSuite\n- Provide post-implementation support and troubleshooting\n- Understand and manage system configuration, data flows, and integration points\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\nQualifications :\n- Strong functional and technical knowledge of NetSuite\n- Experience with ERP data migration, especially from QuickBooks\n- Familiarity with third-party integration tools and NetSuites workflow engine\n- Ability to support post-go-live activities and optimize ERP performance\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:30:46
Fullstack Developer,Grid Dynamics,5 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai']","We are seeking an experienced Full Stack Java Developer skilled in designing, developing, and deploying scalable applications using Java/J2EE, Spring Boot, Microservices, and Angular. The ideal candidate will have hands-on experience with Gradle, Kafka, MySQL, Amazon S3, Docker, and AWS cloud services. You will play a key role in building robust backend systems and intuitive front-end interfaces, leveraging modern DevOps and cloud-native practices.\n\nKey Responsibilities:\nDesign and implement scalable microservices using Java/J2EE and Spring Boot frameworks.\nDevelop RESTful APIs to facilitate communication between distributed services.\nBuild and maintain front-end components using Angular (or similar frameworks).\nIntegrate and manage message-driven architectures using Kafka for event streaming and processing.\nUse Gradle for dependency management and build automation.\nImplement and optimize relational database solutions using MySQL.\nManage file storage and retrieval operations with Amazon S3.\nContainerize applications using Docker and orchestrate deployments on AWS.\nCollaborate with DevOps teams to automate CI/CD pipelines and infrastructure provisioning.\nEnsure application reliability, scalability, and security in a cloud-native (AWS) environment.\nParticipate in code reviews, unit testing, and integration testing to ensure code quality.\nDocument technical designs and implementation processes for future reference.\nEngage in Agile/Scrum ceremonies and collaborate with cross-functional teams.\n\nRequired Qualifications:\nBachelors degree in Computer Science, Information Technology, or a related field.\nProven experience (typically 5+ years) in Java/J2EE application development.\nProficiency in Spring Framework, including Spring Boot and Spring Data JPA.\nStrong understanding of microservices architecture and RESTful API design.\nHands-on experience with Gradle for build automation.\nProficient in integrating and configuring Kafka for messaging solutions.\nExperience with MySQL or other relational databases.\nFamiliarity with Amazon S3 for cloud storage operations.\nFront-end development skills with Angular (TypeScript, HTML, CSS).\nExperience with Docker for containerization and AWS for cloud deployments.\nKnowledge of CI/CD tools and DevOps practices.\nStrong problem-solving, communication, and teamwork skills.\n\nPreferred/Good to Have:\nExperience with additional AWS services (EC2, Lambda, EKS, etc.).\nFamiliarity with security best practices (OAuth2, JWT, etc.).\nExposure to observability tools (Prometheus, Grafana) and infrastructure-as-code (Terraform).\nExperience with Agile methodologies and mentoring junior developers.",Industry Type: IT Services & Consulting,Department: Research & Development,"Employment Type: Full Time, Permanent","['Java', 'Amazon Web Services', 'Kafka', 'Spring Boot', 'Spring', 'AWS', 'Microservices', 'Angular', 'SQL']",2025-06-12 14:30:48
CX Sales and Service Consultant,Gnostic Solutions,10 - 18 years,20-35 Lacs P.A.,[],"Overall, 8+ years of experience relevant to this position.\nDemonstrable experience as a techno-functional lead on at least two large-scale full-life cycle implementations of Oracle CX Applications, with strong implementation expertise in at least two of the following products is a must. \nCX Sales\nB2B Service Cloud\nField Service Cloud\nCPQ\nAsset Based Service\nSubscription Management\nIncentive Compensation\nFamiliarity and exposure to business processes such as Target to Lead, Opportunity to Cash, Request to Resolution.\nStrong techno-functional skills in proposing, designing optimal solutions, including ownership of the overall solution for customization/extension/integrations on Oracle CX Cloud Projects.\nExperience developing process flow diagrams, gathering requirements, conducting workshops, design and prototyping, testing, training, defining support procedures, and implementing practical business solutions.\nOracle CPQ - Hands on experience in configuring Oracle CPQ Cloud module Pricing, Quotation Configuration, Product Management-Items, BOMs, and System Configuration, Document Engine and Big Machines Language (BML), Utilities Libraries, Validation/Hiding/Constraint rules, Layout editor, Commerce layout, Custom CSS, Designing extensions and interfaces in Oracle CPQ Cloud module.\nOSC/B2B Cloud - Prior hands-on experience in functional configurations and customization in the following areas - custom objects, groovy scripting, assignment manager, workflows, triggers email alerts, Data Migration, OTBI Analytics etc.\nOracle Field Service - Prior hands-on experience in configurations and customization in the following areas Core Application, Profiles, Permissions, Business Rules, Routing Plans, Work Schedules, Smart Collaboration, Action Management, Mobility and Manage Displays, Reports, Data migration, Inbound/Outbound messages. Experience developing Oracle Field Service forms and plug-ins.\nExperience developing extensions using Redwood UI, VBCS, VBS, JET\nGood knowledge in web based front end development using â€“ HTML, JavaScript, and CSS\nExperience with React and/or other front-end JavaScript frameworks, would be a plus.\nStrong hands-on experience with design and development of integrations (point to point and Integration Cloud) using various",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['oracle cx applications cloud', 'service cloud', 'Field Service', 'vbcs', 'oracle cs', 'oracle cloud', 'Functional', 'B2B', 'Cpq Cloud', 'ofsc', 'Techno Functional', 'integration', 'fusion sales', 'B2B service', 'fusion service']",2025-06-12 14:30:51
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Kanpur'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'HubSpot', 'Data Migration', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:30:53
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Lucknow'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:30:55
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Agra'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:30:57
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Noida'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:30:59
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Faridabad'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:01
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Ludhiana'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:03
SAP RAR Consultant - Support & Integration,Sadup Soft,8 - 10 years,Not Disclosed,['Gurugram'],"- We are seeking a skilled SAP RAR Implementation Consultant with relevant 5+ years of experience to lead the implementation and integration of the SAP Revenue Accounting and Reporting (RAR) module.\n\n- The ideal candidate will have a strong understanding of revenue recognition standards (IFRS 15, ASC 606) and hands-on experience in configuring and deploying SAP RAR solutions.\n\n- SAP FI overall experience 8+ years\n\n- Analyze and document business requirements related to revenue recognition.\n\n- Collaborate with stakeholders to understand the intricacies of revenue recognition processes and translate them into system requirements.\n\n- Design and configure the SAP RAR module to meet business needs, ensuring seamless integration with SAP SD, SAP FI, and other relevant modules.\n\n- Customize the system to comply with IFRS 15 and ASC 606 standards.\n\n- Support data migration efforts to ensure accurate transfer of revenue data into the SAP RAR system.\n\n- Oversee data mapping, cleansing, and validation to maintain data integrity.\n\n- Develop and execute comprehensive test plans to validate the implementation of the SAP RAR module.\n\n- Identify and resolve issues during the testing phase to ensure a smooth deployment.\n\n- Provide training and support to end-users to facilitate the adoption of the SAP RAR system.\n\n- Address user queries and issues post-implementation to ensure ongoing system effectiveness.\n\n- Manage project timelines, deliverables, and documentation.\n\n- Ensure projects are completed on time, within scope, and in accordance with predefined quality standards.\n\n- Analyze and document business requirements for revenue recognition.\n\n- Design and configure the SAP RAR module, ensuring seamless integration with SAP SD, FI, and other modules.\n\n- Support data migration and ensure accurate revenue data transfer.\n\n- Develop and execute test plans to validate the implementation.\n\n- Provide training and post-implementation support to end-users.\n\n- Manage project timelines, deliverables, and documentation.\n\n- Proficiency in SAP RAR configuration and integration.\n\n- Experience with SAP S/4HANA is mandatory\n\n- Excellent communication and project management skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP RAR', 'SAP S/4 HANA', 'SAP SD', 'SAP Integration', 'SAP FI', 'SAP Implementation', 'SAP Support']",2025-06-12 14:31:06
Manager,XL India Business Services Pvt. Ltd,10 - 15 years,Not Disclosed,['Gurugram'],"Manager, Business Intelligence & Reporting Gurugram/Bangalore, India AXA XL recognizes digital, data and information assets are critical for the business, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained advantage\n\nOur Data, Intelligence & Analytics function is focused on driving innovation through optimizing how we leverage digital, data to drive strategy and differentiate ourselves from the competition\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and strengthen our digital reporting capabilities we are seeking a Manager, BI and Reporting\n\nIn this role you will support/manage BI & reporting\n\nWhat you ll be DOING What will your essential responsibilities include? Be an expert and manage BI & Reporting BAU and effective stakeholder management\n\nBe and expert and manage BI & Reporting products and Impart training to users for self-service reporting\n\nManage IDA BI & Reporting on various strategic initiatives as they arise and enable the development of the function and related capabilities\n\nOversee the design/production/change for BI reporting\n\nContribute to best practices and standards to ensure a consistent approach to BI reporting\n\nRaising data gaps to the IDA Data Quality team so that accurate information flows in our Data Architecture\n\nPartner with key areas within IDA, GT, and across business stakeholders for any BI requirement\n\nInstill a customer-first culture, prioritizing service for our business stakeholders above all\n\nAn understanding of AI will be an added advantage\n\nYou will report to the Senior Delivery Lead, Business Intelligence & Reporting\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Ability to communicate well within team, peers, teams across the globe, manage stakeholders effectively\n\nBrings in a collaborative spirit, can-do attitude and a Customer First mindset\n\nA minimum of a bachelor s or master s degree in a relevant discipline\n\nRelevant years of experience in a data role (analytics or engineer) supporting multiple specialty areas of Data and Analytics\n\nPassion for digital, data and experience working within a digital and data driven organization\n\nExperience on BI tool like Power BI etc Desired Skills and Abilities: Intermediate proficiency in SQL, Advance Excel and Power BI\n\nAble to help/guide his team members on any technical issues and develop them so that the team can self-directedly manage the same\n\nAbility to lead a project/team\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and enables business growth and is critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most inclusive workforce possible, and create a culture where everyone can bring their full selves to work and reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe\n\nRobust support for Flexible Working Arrangements Enhanced family-friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides competitive compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far-reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business intelligence reporting', 'Agile', 'digital strategy', 'Data quality', 'Business strategy', 'Stakeholder management', 'Manager Business Intelligence', 'Analytics', 'SQL', 'Data architecture']",2025-06-12 14:31:08
Cloud Engineer,S2 Tech,6 - 11 years,Not Disclosed,[],"Role & responsibilities\nMaintain the AWS resource library using the AWS Cloud Development Kit (CDK) written in typescript\nWrite tests and assertions to harden the resiliency of the code\nAdminister cloud network resources including subnets, routes, DNS, load balancers and firewalls\nTroubleshoot Microsoft .NET application performance and functionality\nIdentify and implement opportunities for quality improvement using test automation\nAdminister and interpret results from performance tools such as X-Ray, CloudWatch Logs, and Dynatrace APM to promote continuous improvement of products to our customers\nOptimize cloud configuration from both a technical and budgetary perspective\nReview and enhance runbooks, knowledge base documentation, and log defects\nEnforce security tool actions for both short term remediation and prevent recurrence\nIdentify and remediate application performance and database performance on AWS EC2, Lambda, and RDS (Relational Database Service) instances\nAssist with moving workloads from on premise data center to cloud\nConfigure and maintain Microsoft .NET workloads running on both EC2 instances and AWS Lambda\nLearn and stay up to date with current AWS best practices and service offerings\nRequired Skills\nA Bachelor's Degree from an accredited college or university with a major in Computer Science, Information Systems, Engineering, or other related scientific or technical discipline or three (3) years of equivalent experience in a related field\nAWS Certification\n\nPreferred candidate profile\nAt least five (5) years of experience administering Amazon Web Services\nAt least two (2) years of experience administering serverless workloads\nAWS Certification in one or more of the following\nAdvanced Networking\nDevOps Engineer Professional\nSolutions Architect - Professional\nAt least one (2) year of writing and supporting AWS CDK code\nAn advanced knowledge of typescript development\nAn advanced knowledge of .NET application development\nAt least two (2) years of experience building CI/CD automation processes\nExperience with software development life cycle (SDLC) and agile/iterative methodologies\nExperience in large-scale migrations such Data Center to Cloud\nStrong technical communication skills, both verbal and written\nExcel at being a team player who excels at knowledge sharing and understanding the interpersonal aspects of working in a team.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Migration', 'AWS', 'Devops', 'Subnetting', 'Cloud Development', 'Aws Certified', 'Javascript', 'DNS', 'Routing', 'Ci/Cd']",2025-06-12 14:31:10
SQL Developer,Sutherland Global Services Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"Job Title: SQL Developer with SSIS\nExperience Level: Mid-Level\nJob Summary:\nWe are seeking a skilled SQL Developer with expertise in SSIS (SQL Server Integration Services) to join our data engineering team. The ideal candidate will be responsible for developing, maintaining, and optimizing SQL queries, stored procedures, and SSIS packages to support data integration, transformation, and reporting needs.\nKey Responsibilities:",,,,"['Computer science', 'Performance tuning', 'Version control', 'GIT', 'query optimization', 'Analytical', 'data integrity', 'Stored procedures', 'SSIS', 'Reporting tools']",2025-06-12 14:31:13
Software Engineer,Maxwell Geosystems,2 - 3 years,Not Disclosed,"['Kochi( Marine Drive )', 'Ernakulam']","1. Design, develop, test, and maintain the web/mobile application using various technologies\n2. Write efficient, reusable, and scalable code to ensure high performance, security and reliability.\n3. Identify and resolve bugs and performance bottlenecks to ensure smooth functionality.\n4. Maintain clear and concise documentation for code, design, and technical processes.\n5. Work closely with other developers, product managers, and designers to understand project requirements and deliver high quality solutions.\n6. Write unit tests, conduct code reviews, and participate in deployment activities to ensure high quality code.\n7. Develop and maintain databases, including schema design, data migration, and query optimization.\n8. Mentoring of junior software engineers.",Industry Type: Engineering & Construction,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PHP', 'Python', 'MYSQL']",2025-06-12 14:31:15
LMS Manager,Rachna Sagar,3 - 5 years,Not Disclosed,['New Delhi'],"Location : Delhi\nSkill Set : Proficiency in LMS Platforms\nJob Summary:\nThe LMS Manager will manage the day-to-day operations of the Learning Management System (LMS), ensuring optimal performance, integration with existing systems, and user support. The role also involves content management, data analytics, project management, and coding for system improvements. This position requires a strong background in educational technology, server management, and technical skills related to LMS platforms and web development.\nKey Responsibilities:\nLMS Management: Oversee daily operations, troubleshoot issues, and ensure system efficiency.\nImplementation & Integration: Collaborate to add new features, integrate systems, and manage data migration.\nUser Support: Train and assist teachers, staff, students, and parents in using the LMS.\nContent Management: Maintain course materials, ensuring compliance with standards.\nAnalytics & Reporting: Generate reports for instructional insights and system effectiveness.\nProject & Vendor Management: Handle LMS-related projects and coordinate with vendors for upgrades and support.\nQuality Assurance: Ensure compliance with educational and regulatory requirements.\nDatabase & Server Handling: Manage cloud/shared hosting, data relationships, and server infrastructure.\nSales Support & Play Store Management: Address sales queries and oversee app publishing.\nCoding & Development: Develop LMS features (30% coding) using PHP, JavaScript, React, Node.js, etc.\nKey Skills & Requirements:\nEducation: Bachelors in Educational Technology, Computer Science, or related field.\nExperience: 3-5 years in LMS management, preferably in K-12 education.\nTechnical Skills: LMS platforms (Canvas, Moodle, Blackboard), database & server management, coding (PHP, React, Node.js).\nSoft Skills: Strong project management, communication, and problem-solving abilities.",Industry Type: Education / Training,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['LMS Management', 'User Support', 'Content Management', 'Coding', 'Sales Support', 'PHP', 'Node.js', 'Quality Assurance', 'React', 'Analytics']",2025-06-12 14:31:17
DBA PL/SQL Developer,Softeon,3 - 6 years,Not Disclosed,['Chennai'],"Job Summary:\n\nWe are seeking a skilled and experienced DBA PL/SQL Developer to join our IT team. The ideal candidate will have a strong understanding of Oracle Database Architecture and be proficient in PL/SQL programming. This role involves the design, development, implementation, and maintenance of database solutions to support enterprise applications, ensuring high performance, data integrity, and reliability.\n\nRole & responsibilities:\nDesign and implement database structures including tables, indexes, views, and constraints.\nDevelop and maintain PL/SQL code including procedures, functions, triggers, and packages.\nWrite, debug, and optimize complex SQL and PL/SQL queries for data manipulation and reporting.\nManage Oracle database architecture, including schema design, storage management, and space utilization.\nImplement and manage database backup, recovery, and disaster recovery procedures.\nMonitor and tune database performance for optimal efficiency.\nCollaborate with application developers and project managers to support application requirements.\nEnsure database security, data integrity, and access control compliance.\nExecute data migration and transformation jobs using scripts and tools.\nApply database best practices, standards, and version control processes.\nParticipate in database upgrades, patching, and maintenance activities.\nRequired Skills and Qualifications:\n\nBachelor's degree in Computer Science, Information Technology, or related field.\n3 6 years of hands-on experience with Oracle Database and PL/SQL development.\nIn-depth understanding of Oracle database architecture and administration.\nStrong experience in database design, normalization, and data modeling.\nProven expertise in SQL query optimization and PL/SQL performance tuning.\nKnowledge of backup and recovery strategies using RMAN or equivalent.\nExperience in database security and access control implementation.\nFamiliarity with version control tools (e.g., Git) and agile development methodologies.\nAbility to manage multiple priorities and communicate effectively with stakeholders.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Pl / Sql Development', 'Sql Database Development', 'Oracle SQL', 'Architecture', 'Oracle Database']",2025-06-12 14:31:19
JDE S&D - Freelancers /Business Analyst,Clarisity,10 - 15 years,Not Disclosed,[],"Oracle JDEdwards Sr. Consultant / Freelancers - Sales & Distribution , Business Analyst\n\nJob description:\n\nOracle JD Edwards Sales and Distribution Management- Inventory, Procurement, Sales & Advance Pricing\nApplications Modules - Primary\nJD Edwards EnterpriseOne Distribution Suite\n- Inventory Mgt.\n- Procurement Management\n- Sales Order Management\n- Advanced Pricing\nApplications/technology domain- secondary\n- Transport / Warehouse Management\n- Business Analysis\nSkills Required :\n- 14+ years of Domain /IT experience (at least 10 years of grounds up experience in JDE Distribution functional and as Business analyst)\n- Graduate / Post graduate with Exposure/knowledge in Manufacturing CPG/ Retail, Materials, Procurement & Sales industry domains\n- Should have experience in Oracle JDE EnterpriseOne Projects (Implementation / Roll Outs /Support / Upgrade) as a functional consultant.\n- Experience in JDE Inventory, Sales, Procurement Management & Advanced Pricing is a must\n- Experiences in Transport management will be an added advantage\n- Excellent business communication skills, negotiation skills, and presentation skills\n- Ability to work within large teams and learn new functionality and modules during the project\n- Should have prior experience of working in Onsite/Offshore model\n- Should have knowledge of ERP implementation activities such as Business requirements & analysis, configuration, Conference Room pilot, Gap fit analysis, Application customization functional specifications, testing, data migration and or conversion\n- Should have Industry recognized certifications.\n\nJob Location - Remote/ WFH\nMode - Freelancer / Contractor\nShift timings - 7 pm IST - 3 am IST (Mon-Fri)\nEarly joiners are preferred.\n\n\nWe look forward to receiving your application! All your information will be kept strictly confidential.\n\nPlease furnish below details while applying with your updated resume:\nTotal Experience (yrs) -\nRelevant Experience in JDE SD -\nCurrent CTC -\nExpected CTC -\nNotice period -\nCurrent Location -\n\nThanks & Regards,\nKavita Tikone | Manager - HR & Recruitment\nM: +91 9819386379\neMail: Kavita.tikone@clarisity.com\nWeb: www.clarisity.com",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Freelance/Homebased","['Procurement', 'Inventory Management', 'Advanced Pricing', 'Taxation', 'JDE Sales & distribution', 'Business Analysis']",2025-06-12 14:31:21
Laravel Backend Developer,Ai Mobile Idea Box,3 - 5 years,4-7 Lacs P.A.,[],"URGENT HIRING (REMOTE)\n\nPayrollNinja is a saas provider and software development house. We're looking for an experienced Backend Laravel developer to join our IT team. You will be responsible for the backend logic of our web applications.\n\nIf you have an excellent and strong coding skills and passion for developing challenging web applications or improving existing ones, let's talk. As a backend Laravel developer, you'll work closely with our team to ensure system consistency and improve user experience.\n\nUltimately, you should be able to develop and maintain functional and stable web applications to meet our company's needs.\n\nOur team are actively working on plantation based ERP which consists of many modules from operational to accounting. Potential to work on-site (Malaysia) if you've proven skillful.\n\n\nResponsibilities\nParticipate in the entire application lifecycle, focusing on coding and debugging.\nWrite clean code to develop functional web applications.\nTroubleshoot and debug applications.\nPerform tests to optimize performance and security.\nManage cutting-edge technologies to improve legacy applications.\nCollaborate with Front-end developers to integrate user-facing elements with server side logic.\nGather and address technical and design requirements.\nBuild reusable code and libraries for future use.\nContinuously propose system enhancement and implement security patches.\nFollow emerging technologies.\nCommunicate with client related to system development requirements and progress update.\n\nRequirements\nBachelors degree in computer programming, computer science, or a related field.\nProven work experience as a laravel backend developer (minimum 3 years)\nIn-depth understanding of the entire web development process (design, development and deployment)\nFluent in scripting languages like PHP, Java, Phyton or Node.js. (PHP is compulsory)\nExperience with SQL, MySQL and Oracle database systems (MySQL is compulsory)\nExpert in PHP Laravel framework and API integration\nFamiliar with Amazon Web Services (AWS)\nVersion control, such as Git, CVS or SVN\nFamiliarity with front-end languages (e.g. HTML, JavaScript, Jquery and CSS)\nExcellent analytical and time management skills\nTeamwork skills with a problem-solving attitude\nHigh attention to detail\nCritical thinker and can analyse client documents thoroughly.\nExcellent communication skills (English is compulsory)\nGood internet speed and fine working laptop condition is compulsory for remote job.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Php Laravel', 'Laravel', 'Api Integration', 'JQuery', 'Git Version Control', 'Software Development', 'GIT', 'Git Repository', 'MySQL', 'Web Application', 'Bootstrap', 'Html And Css', 'Ajax']",2025-06-12 14:31:23
Oracle Fusion Technical Consultant ( OIC Specialist ),Strawberry Infotech Pvt. Ltd.,4 - 9 years,Not Disclosed,['Gurugram'],"Role & responsibilities\nKey Responsibilities:\nDevelop and implement integrations using Oracle Integration Cloud (OIC) including REST, SOAP, File, and FTP adapters.\nWork on BIP Reports, OTBI Reports, Fast Formulas, and Personalizations in Oracle Fusion.\nCollaborate with functional consultants and business stakeholders to gather requirements and translate them into technical solutions.\nDesign and develop data migrations using FBDI/ADFDi, and support data conversion and transformation activities.\nTroubleshoot and resolve integration issues, provide performance tuning, and ensure system stability.\nImplement security policies, error handling, and logging for integrations.\nParticipate in unit testing, integration testing, and deployment activities.\nSupport post-implementation, production support, and ongoing enhancements.\nRequired Skills:\n4+ years of experience as an Oracle Fusion Technical Consultant.\nStrong expertise in Oracle Integration Cloud (OIC) REST APIs, SOAP Web Services, File/FTP, and Process Automation.\nExperience with Oracle Fusion SaaS modules like HCM, Finance, or SCM.\nGood knowledge of PaaS components (Visual Builder, PCS, etc.).\nStrong SQL/PLSQL skills and knowledge of Web Services and XML/XSDs.\nExperience with Reports (BIP, OTBI) and Data Extracts.\nHands-on experience in FBDI, HDL, and Data Migration processes.\nGood understanding of Oracle Fusion Cloud data models and structures.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oic', 'Oracle Fusion', 'Fusion Cloud']",2025-06-12 14:31:25
SAP Group Reporting,SRSInfoway,4 - 9 years,Not Disclosed,"['Noida', 'Hyderabad', 'Pune']","Job Title: SAP Group Reporting\nExperience: 4 to 7 Years\nLocation: Noida / Hyderabad / Pune\nEmployment Type: Full-time\nNotice Period: Immediate to 20 Days\n\nJob Summary:\nWe are seeking a highly skilled SAP Group Reporting Consultant with hands-on experience in implementing Group Reporting solutions, data migration, and strong knowledge of SAP BPC or SAP FICO. The ideal candidate should have deep expertise in financial consolidation concepts, configuration, and technical migration at the table level.\n\nKey Responsibilities:\nLead and implement SAP Group Reporting (GR) solutions, including configuration and testing.\nManage end-to-end data migration from legacy systems such as SAP BPC or SAP FICO into Group Reporting tables.\nUnderstand and configure GR master data, dimensions, consolidation units, and hierarchies.\nEnsure data integrity and accuracy throughout the migration and consolidation processes.\nWork closely with business stakeholders to understand financial consolidation requirements and design scalable GR solutions.\nConduct technical validations and reconciliation during migration.\nProvide support during UAT and post-go-live phases.\nParticipate in client interviews and demonstrate strong communication and consulting capabilities.\n\nMust-Have Skills:\nProven experience in SAP Group Reporting implementation projects.\nStrong knowledge in data migration from BPC or FICO to Group Reporting, including table-level understanding.\nSolid background in SAP BPC (Business Planning and Consolidation) or SAP FICO (Financial Accounting and Controlling).\nDeep understanding of FICO consolidation concepts and financial closing procedures.\nStrong analytical and troubleshooting skills.\nExcellent verbal and written communication skills must be able to represent the project during client interactions.\n\nPreferred Qualifications:\nSAP certification in Group Reporting, BPC, or FICO.\nExperience in S/4HANA Financials.\nExposure to SAP Analytics Cloud (SAC) for reporting (optional)\n\n\nMail: krishna.jothi@srsinfoway.com",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Sap Group Reporting', 'FICO', 'FI', 'Co']",2025-06-12 14:31:28
Onix is Hiring MDM Informatica Developer,Onix,3 - 8 years,Not Disclosed,"['Hyderabad', 'Pune']","Job Summary:\nWe are seeking a highly skilled Informatica MDM Developer to join our data integration and management team. The ideal candidate will have extensive experience in Informatica Master Data Management (MDM) solutions and a deep understanding of data quality, data governance, and master data modeling.\nKey Responsibilities:\nDesign, develop, and deploy Informatica MDM solutions (including Hub, IDD, SIF, and MDM Hub configurations).\nWork closely with data architects, business analysts, and stakeholders to understand master data requirements.\nConfigure and manage Trust, Merge, Survivorship rules, and Match/Merge logic.\nImplement data quality (DQ) checks and profiling using Informatica DQ tools.\nDevelop batch and real-time integration using Informatica MDM SIF APIs and ETL tools (e.g., Informatica PowerCenter).\nMonitor and optimize MDM performance and data processing.\nDocument MDM architecture, data flows, and integration touchpoints.\nTroubleshoot and resolve MDM issues across environments (Dev, Test, UAT, Prod).\nSupport data governance and metadata management initiatives.\nRequired Skills:\nStrong hands-on experience with Informatica MDM (10.x or later).\nProficient in match/merge rules, data stewardship, hierarchy management, and SIF APIs.\nExperience with Informatica Data Quality (IDQ) is a plus.\nSolid understanding of data modeling, relational databases, and SQL.\nFamiliarity with REST/SOAP APIs, web services, and real-time data integration.\nExperience in Agile/Scrum environments.\nExcellent problem-solving and communication skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['informatica', 'Informatica Mdm', 'Informatica Master Data Management', 'Mdm Informatica', 'Etl Informatica', 'MDM']",2025-06-12 14:31:30
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Aurangabad'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:32
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Ludhiana'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:31:34
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Madurai'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:36
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Vadodara'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:39
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Jaipur'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:41
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Chennai'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:43
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Bhubaneswar'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:45
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Allahabad'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:47
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Jamshedpur'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-12 14:31:49
Partner Alliance - Solutions,Prodapt Solutions,4 - 7 years,Not Disclosed,['Chennai'],"Overview\n\n2 roles - Presales solution engineers\n\nResponsibilities\n\nknown as a "" Presales Consultant,"" ""Sales Consultant,"" or â€œSales Engineerâ€ the Solution Consultant is responsible for leading the solution evaluation throughout the sales cycle and delivering thought leadership to companies to transform their customerâ€™s experience. You will be responsible for developing innovative demonstrations and proof of concepts to prove the value of Salesforce solutions within the context of our customers needs.\nYou will have a proven track record of building innovative multi-product solutions that co-exist with our customers existing environments.\nBeing able to translate our customers business goals into a working demonstrable vision will be a key skill.\n\nYour Responsibilities\nTo develop and present innovative customer solutions to key decision makers to address their business issues and needs whilst showing business value\nCoordinate and own the entire solution cycle through close collaboration with other Salesforce product teams.\nIndustry experience, incorporating and developing a point of view based on Salesforceâ€™s (or similar) solutions.\nTo fully understand and clearly articulate the benefits of Salesforce to customers at all levels, examples include; Administration/IT Staff, managers and ""C"" level executives.\nDevelop new go to market propositions that reflect changing market demands. Such as how AI can benefit our industries.\nDisplay initiative, and self-motivation while delivering high-quality results along with meeting all expectations for both internal and external customers.\nHave a strong interest in growing your career and participating in our internal training programs and mentorship initiatives.\n\nPersonality Attributes/Experience\nExperience will be evaluated based on alignment to the core competencies for the role (e.g. extracurricular leadership roles, military experience, volunteer work, etc.).\nThis role is within our Automotive sector to help to lead UK OEM and Retailer organisations to implement Digital Transformation strategies and deliver more success to their customers. An understanding of these industries is essential.\nAwareness of industry trends, challenges, and common business solutions in use for both Auto OEMs and retailers.\nTrack record of solution engineering, consultancy, or delivery for an enterprise software solution organisation that works within the Auto industry. We are open to a variety of backgrounds for the role.\nSolid oral, written, presentation and interpersonal communication and relationship skills.\nProven time management skills in a dynamic team environment.\nAbility to work as part of a team to solve problems in multifaceted, energizing environments.\nInquisitive, practical and passionate about technology and sharing knowledge.\nLikes to be the first to know something and to understand why and how things happen.\nGood at searching out information and experimenting likes to concentrate on a particular topic and solve puzzles.\nGood at explaining ideas and finding ways to keep peopleâ€™s attention.\nWilling and able to travel as needed.\n\n\nhiring a ServiceNow Presales Solution Consultant to join their team in London. The successful candidate will drive enterprise-wide transformations, manage relationships with C-level executives and create innovative solutions that elevate the digital experience.\n\nResponsibilities\nAs the ServiceNow Presales Solution Consultant, you will lead sal es and business development initiatives, uncovering new opportunities for ServiceNow solutions.\nLead the pre-sales journey, from crafting proposals to negotiating contracts and securing sign-offs.\nPartner with sales teams to define and present ServiceNow solutions to potential clients.\nServe as a trusted advisor to senior stakeholders, including C-suite executives and IT leaders.\nBuild and maintain strong relationships with ServiceNow leadership and strategic partners for seamless collaboration.\nCollaborate with key business functions (e.g. HR, IT, customer service) to customize ServiceNow solutions for their specific needs.\nStay up to date on market trends and ensure ServiceNow offerings are aligned with evolving business needs.\n\nSkillset\nMasterâ€™s or Bachelorâ€™s degree in Information Technology or similar.\nExtensive knowledge of ITSM, ITOM, CSM, HRSD, Employee Service Center, Case & Knowledge Management and Performance Analytics.\nProficient in system integration, data migration, automation, scripting and ServiceNow customization.\nUnderstanding of HR processes, employee engagement strategies and workflow design.\nSkilled in mapping customer journeys, improving service delivery and boosting customer satisfaction.\nExperienced in demonstrating the value of ServiceNow and advising clients on strategic implementation.\nStrong problem-solving abilities, with a knack for assessing business needs and proposing effective solutions.\nProven experience in driving adoption of new technology and training end-users for smooth transitions.\nCapable of engaging with stakeholders up to the CXO level, simplifying technical concepts for non-technical audiences, and fostering collaboration across teams.\nPreferred CertificationsServiceNow Certified System Administrator (CSA), ServiceNow Certified Implementation Specialist (CIS), ServiceNow Certified Application Developer (CAD), Customer Service Management (CSM) / CRM, Human Resources Service Delivery (HRSD).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['presentation skills', 'servicenow', 'itsm', 'itom', 'csm', 'performance analytics', 'cmdb', 'hrsd', 'business development', 'data migration', 'knowledge management', 'problem management', 'javascript', 'system integration', 'incident management', 'html', 'servicenow development', 'itil']",2025-06-12 14:31:51
Informatica SAP Expert,Randomtrees,3 - 6 years,Not Disclosed,['Hyderabad'],"The Data Steward will play a critical role in ensuring data integrity, quality, and governance within SAP systems.\nThe responsibilities include:\nData Governance:\no Define ownership and accountability for critical data assets to ensure they are effectively managed and maintain integrity throughout systems.\no Collaborate with business and IT teams to enforce data governance policies, ensuring alignment with enterprise data standards.\nData Quality Management:\no Promote data accuracy and adherence to defined data management and governance practices.\no Identify and resolve data discrepancies to enhance operational efficiency.\nData Integration and Maintenance:\no Manage and maintain master data quality for Finance and Material domains within the SAP system.\no Support SAP data migrations, validations, and audits to ensure seamless data integration.\nCompliance and Reporting:\no Ensure compliance with regulatory and company data standards.\no Develop and distribute recommendations and supporting documentation for new or proposed data standards, business rules, and policies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'SAP data migrations', 'Data Maintenance', 'Data Quality Management', 'Data Integration']",2025-06-12 14:31:53
SQL Server & Oracle Developer,Mobile Programming,3 - 8 years,Not Disclosed,['Mumbai'],"We are seeking a skilled SQL Server Oracle Developer to join our team.\nThe ideal candidate will have strong expertise in working with SQL Server and Oracle databases, capable of managing and optimizing complex database systems.\nYou will be responsible for writing efficient queries, designing and maintaining database structures, and collaborating with cross-functional teams to ensure seamless integration of database solutions.\nKey Responsibilities:\nDevelop, manage, and optimize SQL queries and stored procedures for SQL Server and Oracle databases.\nDesign, implement, and maintain database systems ensuring high performance, security, and scalability.\nCollaborate with developers to integrate backend services with databases.\nTroubleshoot and resolve performance bottlenecks and database-related issues.\nPerform database backup and recovery, ensuring data integrity and availability.Assist in database migrations and version upgrades.\nProvide support for database-related queries and performance tuning.Ensure compliance with data security and privacy policies.\nRequired Skills:3+ years of experience in SQL Server and Oracle database management.\nStrong proficiency in T-SQL and PL/SQL for writing queries, stored procedures, and functions.In-depth understanding of database design, normalization, and optimization techniques.Experience with performance tuning, query optimization, and database indexing.\nFamiliarity with data migration and database backup/recovery processes.Ability to troubleshoot and resolve complex database issues.\nKnowledge of database security practices and ensuring compliance.\nStrong communication and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'T-SQL', 'PL/SQL', 'Oracle database management', 'Oracle databases']",2025-06-12 14:31:55
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Nashik'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:31:58
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Surat'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:00
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Jaipur'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n- Configure and refine related workflows within NetSuite\n- Provide post-implementation support and troubleshooting\n- Understand and manage system configuration, data flows, and integration points\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\nQualifications :\n- Strong functional and technical knowledge of NetSuite\n- Experience with ERP data migration, especially from QuickBooks\n- Familiarity with third-party integration tools and NetSuites workflow engine\n- Ability to support post-go-live activities and optimize ERP performance\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:02
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Visakhapatnam'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:04
Specialist Salesforce Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role the Salesforce Specialist IS Engineer for the MyAmgen Product Team will help in driving high-quality, efficient delivery for the companys intranet. This role ensures the development team adheres to best practices and definitions of done while proactively identifying technical debt and collaborating with the Architect and Product Owner to prioritize its resolution. The engineer has expertise in the Salesforce development stack and data model and leverages AI and automation to enhance quality and speed. They take a hands-on approach to innovation, working on proofs of concept (PoCs) to validate the technical feasibility of backlog items. Additionally, the role involves designing and breaking down technical work to minimize dependencies and improve team flow during sprints. The Salesforce Development Lead also supports testing and validation to ensure reliable and impactful deliverables.\nRoles & Responsibilities:\nDeliver high-quality Salesforce solutions using LWC, Apex, Flows and other Salesforce technologies.\nEnsure alignment to established best practices and definitions of done, maintaining high-quality standards in deliverables.\nTake architectural design and translate to code deliverables\nCreate user stories that effectively describe business and technical needs\nProactively identify technical debt and collaborate with the Architect and Product Owner to prioritize and address it effectively.\nInnovate and improve development workflows by leveraging AI and automation tools to increase efficiency, speed, and quality.\nDesign and decompose technical tasks to minimize interdependencies and optimize the team's workflow during sprints.\nSupport the testing and validation of deliverables to ensure reliability, performance, and alignment with business goals.\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The Salesforce professional we seek is a type of person with these qualifications.\nBasic Qualifications:\nMasters degree and 4 to 6 years of experience in Computer Science, IT or related field OR\nBachelors degree and 6 to 8 years of experience in Computer Science, IT or related field OR\nDiploma and 10 to 12 years of Computer Science, IT or related field experience\nFunctional Skills:\nMust-Have Skills:\nExperience with Apex, JavaScript, and Lightning Web Components (LWC)to create scalable applications\nExperienced in using CI/CD automation to deploy Salesforce code and configurations\nStrong understanding of declarative tools like Flows and Process Builder\nProficiency in using Salesforce tools such as SOQL, SOSL, and Data Loader to query, manipulate and export data\nGood ability to lead development teams and collaborate with Product Owners and Architects for task prioritization and execution\nAbility to train and guide junior developers in best practices\nHands-on experience in testing, debugging, and validating deliverables for reliability and performance\nFamiliarity with Agile practices such as User Story Creation and, sprint planning\n\nGood-to-Have Skills:\nExperience with using Copado for CI/CD automations\nExperience using AI, automation, or cutting-edge Salesforce tools.\nProficiency in data migration, modeling, and security configurations.\nExperience in fine-tuning Salesforce org for scalability and speed.\nFamiliarity with industry-specific Salesforce tools like Health Cloud or Financial Services Cloud.\nAbility to identify and resolve technical debt while designing scalable, maintainable solutions\nExperience creating proofs of concept (PoCs) to validate new ideas or backlog items.\n\nProfessional Certifications (preferred):\nSalesforce Advanced Administrator\nSalesforce Developer I and II\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong written and verbal communications skills (English) in translating technology content into business-language at various levels\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong time and task management skills to estimate and successfully meet project timeline with ability to bring consistency and quality assurance across various projects.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Salesforce', 'SOQL', 'Lightning Web Components', 'SOSL', 'JavaScript', 'CI/CD', 'debugging', 'Apex']",2025-06-12 14:32:07
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Delhi / NCR'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:09
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Indore'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:11
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Ahmedabad'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:13
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Kolkata'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:15
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Pune'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'System Implementation', 'QuickBooks', 'NetSuite Implementation']",2025-06-12 14:32:17
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Hyderabad'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n- Configure and refine related workflows within NetSuite\n- Provide post-implementation support and troubleshooting\n- Understand and manage system configuration, data flows, and integration points\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\nQualifications :\n- Strong functional and technical knowledge of NetSuite\n- Experience with ERP data migration, especially from QuickBooks\n- Familiarity with third-party integration tools and NetSuites workflow engine\n- Ability to support post-go-live activities and optimize ERP performance\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:20
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Chennai'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n- Configure and refine related workflows within NetSuite\n- Provide post-implementation support and troubleshooting\n- Understand and manage system configuration, data flows, and integration points\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\nQualifications :\n- Strong functional and technical knowledge of NetSuite\n- Experience with ERP data migration, especially from QuickBooks\n- Familiarity with third-party integration tools and NetSuites workflow engine\n- Ability to support post-go-live activities and optimize ERP performance\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:22
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Thane'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:24
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Bhopal'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:26
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Mumbai'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:32:28
Transportation Master Team Lead,Amgen Inc,7 - 8 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will take ownership of Transportation Master data processes, ensuring the accuracy, consistency, and governance of critical data across the organization. This role will lead data validation, cleansing, and enrichment efforts, and collaborate with cross-functional teams to resolve complex data issues and drive process improvements. The Transportation Master Team Lead will also oversee key performance metrics, ensure compliance with data governance standards, and lead data migration and integration initiatives.\nRoles & Responsibilities:\nLead and manage day-to-day MDM operations, including data validation, cleansing, and enrichment processes\nOversee data governance practices, ensuring compliance with internal standards and regulatory requirements\nCollaborate with cross-functional teams, including IT and business units, to resolve complex data issues and improve data workflows.\nImplement and drive continuous improvements in MDM processes to enhance data accuracy, quality, and operational efficiency.\nLead data migration, integration projects, and system upgrades to ensure seamless data consistency across platforms.\nMonitor and report on key performance indicators related to master data quality and operational success.\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nBachelors in a STEM discipline and 7-8 years of experience in enterprise applications like SAP, and Oracle, proven experience in Transportation Master data management and data governance\nIndustry experience preferably in healthcare or biotech supply chain\nProficient in MS Office, visualization tools like Spotfire, Tableau, Power BI\nStrong analytical skills with the ability to collaborate cross-functionally and resolve data issues.\nExperience in leading and managing successful teams\nPreferred Qualifications:\nMust-Have Skills:\nExpertise in master data management processes and data governance\nSolid understanding of SAP ECC and proven experience in data implementation/integration projects\nMaster data knowledge in the Transportation domain, other domains such as Material, Customer, and Production Master are a plus.\nAbility to lead and collaborate with cross-functional teams to set data strategy for a master domain(s) and drive prioritization across different projects and day-to-day operations\nStrong understanding of data governance frameworks and regulatory compliance standards and regulations (e.g., GDPR, HIPAA, GxP).\nExcellent problem-solving and analytical skills, with a focus on driving continuous improvement in data accuracy and quality\nGood-to-Have Skills:\nSAP S/4, SAP MDG, SAP TM\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills.\nAbility to work effectively with global, virtual teams.\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data management processes', 'SAP ECC', 'Transportation Master data management', 'SAP MDG', 'HIPAA', 'data governance', 'SAP TM', 'SAP S/4', 'GDPR', 'GxP', 'STEM']",2025-06-12 14:32:30
SAP WM Consultant - E2E Implementation,Zettamine Labs,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Summary :\nWe are looking for a highly skilled and experienced SAP WM (Warehouse Management) Consultant to join our growing team.\nThe ideal candidate will have a strong background in SAP WM module implementations, support, and integrations, along with excellent problem-solving and communication skills.\nExperience with EWM is a plus.\nKey Responsibilities :\n- Gather business requirements and translate them into SAP WM solutions.\n- Configure and customize SAP WM modules to meet business needs.\n- Lead and support SAP WM module implementation, rollout, and support projects.\n- Collaborate with cross-functional teams including MM, SD, PP, and EWM.\n- Develop functional specifications for custom developments.\n- Perform system testing, integration testing, and support UAT.\n- Provide training and documentation for end-users and support teams.\n- Resolve incidents and provide ongoing maintenance and support.\n\nRequired Skills :\n- Strong expertise in SAP WM configuration and end-to-end implementation.\n- Hands-on experience with Inventory Management (IM), Inbound/Outbound Logistics, Putaway, Picking, and Stock Transfer Processes.\n- Integration experience with SAP MM, SD, and EWM.\n- Experience with RF devices, barcode scanning, and warehouse automation is an advantage.\n- Good understanding of IDoc handling, interfaces, and data migration.\n- Ability to analyze business processes and map them into SAP WM functionality.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP', 'Technical Consultant', 'SAP SD', 'SAP MM', 'SAP PP', 'SAP Integration', 'SAP EWM', 'SAP Implementation', 'IDoc', 'SAP Support', 'SAP WM']",2025-06-12 14:32:33
Power BI Developer,Luxoft,3 - 5 years,Not Disclosed,['Pune'],"Design, develop and maintain/support Power BI workflows to take data from multiple sources to make it ready for analytics and reporting.\nOptimize existing workflows to ensure performance, scalability and reliability.\nSupport the automation of manual processes to improve operational efficiency.\nDocument workflows, processes, and best practices for knowledge sharing.\nProvide training and mentorship to other team members on Alteryx development.\nCollaborate with other members of the team to deliver data solutions for the program.\nSkills\nMust have\nProficiency in Power BI Desktop, Power BI Service (5+ yrs of experience)\nExperience with creating interactive dashboards, custom visuals, and reports.\nData Modeling:\nStrong understanding of data modeling concepts, including relationships, calculated columns, measures, and hierarchies.\nExpertise in using DAX (Data Analysis Expressions) for complex calculations.\nSQL and Database Management:\nProficiency in SQL to extract, manipulate, and analyze data from databases.\nKnowledge of database design and querying.\nETL (Extract, Transform, Load) Tools:\nExperience with data transformation and cleaning using tools like Power Query, SSIS, or other ETL tools.\nNice to have\nData Architecture & Engineering: Design and implement efficient and scalable data warehousing solutions using Azure Databricks and Microsoft Fabric.\nBusiness Intelligence & Data Visualization: Create insightful Power BI dashboards to help drive business decisions.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nAlpharetta\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nRemote United States\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nPune, India\nReq. VR-114885\nData Science\nBCM Industry\n05/06/2025\nReq. VR-114885\nApply for Power BI Developer in Pune\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data analysis', 'Data modeling', 'Database design', 'SSIS', 'Business intelligence', 'microsoft', 'Analytics', 'SQL', 'Data architecture']",2025-06-12 14:32:35
Actuarial Consultant,"Top B2B Corporate, Management Consulting...",4 - 9 years,22.5-30 Lacs P.A.,['Pune'],"Summary:\nResponsible for providing actuarial support across Life, Health, and Retirement domains, including product development, pricing, valuation, financial reporting, and risk management.\n\nKey Responsibilities:\nSupport actuarial functions: pricing, projection, valuation, illustrations, and reinsurance.\nConduct data migration, validation, testing, and documentation.\nAssist in model conversion, regulatory modeling, and system improvements.\nEnhance valuation efficiency and reporting processes.\nAutomate tools and reports; analyze data for business insights.\nSimplify complex actuarial data for cross-functional teams.\nManage multiple projects with a focus on quality and timeliness.\n\nQualifications & Skills:\nBachelor's/Masters in Mathematics, Statistics, Actuarial Science, or related.\nPursuing actuarial exams (minimum 4 passes, CM1 required; CM2 preferred).\nYears of actuarial experience in Life, Health, or Retirement.\nProficient in valuation, modeling, reporting, and pricing.\nSkilled in actuarial software (Prophet, AXIS, MG ALFA, etc.).\nAdvanced Excel/VBA; exposure to R, SAS, or Python.\nStrong analytical and communication skills.",Industry Type: IT Services & Consulting,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Actuarial Reserving Methodologies', 'Health', 'Life', 'Retirement', 'Retirement Planning', 'Valuation']",2025-06-12 14:32:37
Database Administrator(6 months Contract),T2 Innovations,10 - 15 years,Not Disclosed,['Chennai'],"Job Summary:\nWe are looking for an experienced Database Administrator (DBA) with strong expertise in migrating databases from Oracle to MySQL. The role involves hands-on work in schema conversion, data migration, performance tuning, scripting, and post-migration support.\n\nKey Responsibilities:\nLead end-to-end Oracle to MySQL migration\nPerform schema conversion, data export/import, and validation\nUse tools like MySQL Workbench or Oracle SQL Developer\nAutomate tasks using Python, Perl, or Bash\nOptimize queries, indexing, and ensure performance\nMaintain data integrity, security, and compliance\nProvide post-migration monitoring and support\nKey Skills:\nOracle to MySQL Migration\nMySQL Administration\nSchema & Data Conversion\nQuery Optimization & Performance Tuning\nScripting (Python/Perl/Bash)\nPreferred:\nExperience with large datasets and automation tools\nFamiliarity with cloud databases (AWS RDS, GCP SQL)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Oracle to MySQL Migration', 'MySQL Administration', 'Schema & Data Conversion', 'perl', 'Bash', 'Performance Tuning', 'Scripting', 'Python']",2025-06-12 14:32:39
Python Technical Lead Developer - Only Male candidates,AM Infoweb,2 - 7 years,Not Disclosed,['Pune( Kalyani Nagar )'],"Job Title: Technical Lead Python Developer\nLocation: Kalyani Nagar, Pune\nShift Timing: 3:00 PM to 12:00 AM IST\nAbout the Role:\nWe are looking for a highly skilled and passionate Python Technical Lead to join our growing team at AM Infoweb. This role is purely backend and hands-on coding focused, and ideal for someone who is not only technically strong but also has the capability to design, architect, and lead backend projects.\n\nAs a Technical Lead, you'll take ownership of the backend architecture, manage complex data-driven systems, and guide the team through technical challenges using best-in-class tools and practices.\n\nKey Responsibilities:\nLead backend development efforts using Python and associated frameworks.\nDesign, develop, and maintain scalable backend architecture for web and AI-based applications.\nArchitect solutions around given datasets and business logic.\nGuide junior developers, conduct code reviews, and ensure best coding practices.\nCollaborate with cross-functional teams including AI engineers, frontend developers, DevOps, and product owners.\nWork with CI/CD pipelines to ensure rapid and stable deployment cycles.\nIntegrate and manage databases such as MySQL, PostgreSQL, and MongoDB.\nDeploy, monitor, and maintain services on AWS and Azure cloud platforms.\n\nTechnical Requirements:\nStrong experience with Python backend frameworks Django, Flask, FastAPI (at least two).\nExperience with relational and non-relational databases MySQL, PostgreSQL, MongoDB.\nSolid understanding of API development, RESTful services, and system architecture.\nExperience working with cloud platforms AWS, Azure.\nFamiliarity with CI/CD tools and DevOps practices.\nStrong problem-solving and communication skills.\n\nPreferred Qualifications:\n6+ years of experience in backend development.\n12 years in a technical leadership role.\nExposure to AI and machine learning integrations is a plus.\nExperience with microservices and containerization (Docker, Kubernetes) is an advantage.\n\nWhat You Get:\nInternational exposure to global projects and clients.\nWork with trending AI tools and intelligent bots.\nOnsite cafeteria and break facilities.\nFun and engaging team events and celebrations.\n\n* Know your organization - https://www.aminfoweb.in/\n* Know your workspace! - https://www.youtube.com/watch?v=T1UKFelepCk\n* Our Annual RNR'2022 - https://www.youtube.com/watch?v=T1UKFelepCk\n* Exploring the myths surrounding outsourced healthcare management - https://www.youtube.com/watch?v=fwf3jFa2T-A",Industry Type: Analytics / KPO / Research,Department: Product Management,"Employment Type: Full Time, Permanent","['Python', 'Data Structures And Algorithms', 'Lead Architect', 'Django', 'Postgresql', 'MySQL', 'FastAPI', 'MongoDB', 'Data Architecture', 'AWS', 'Flask', 'Backend Development']",2025-06-12 14:32:42
"Manager, Software Development",SAS Institute of Management,12 - 17 years,Not Disclosed,['Pune'],"The role is based in the Pune R&D Center, at SAS R&D Pune facility. We are looking for a\nSoftware Manager\nto development product within the Customer Intelligence line of business. You will be leading the development activities, optimizing allocation for team member, lead the team technically, involve all the stakeholders appropriately, and maintain project standards and best practices and deliver outcomes as planned.\nEssential Experience:\nAt least 12 years of working experience in software development\nAt least 2 years of management/leading experience of software development teams\nExperience of architecting technically sophisticated systems.\nStrong experience in Agile software development cycle\nExperience in enforcing best technical practices in software development\nExcellent written and oral communication skills.\nUnderstanding of manual and automation testing processes and architecture\nUnderstanding of full stack of technology\nExcellent knowledge of enterprise design patterns\nGood understanding of data modelling best practices\nExperience in leading/managing teams with energy and optimism\nAbility to motivate team members to function collaboratively\nGood in multi-tasking\nExperience of these items would be useful:\nCustomer Intelligence domain preferred\nExperience with Docker, Kubernetes\nManagement in development on any public cloud providers like Amazon Web Services, Microsoft Azure, and Google Cloud, etc.\nManaging Continuous Integration and Continuous Delivery (CI, CD) model\nWe are a friendly team, and we ll be offering you plenty of opportunities to develop your career. Interested? Then please get in touch to find out more!\nPrimary Responsibilities\nOrganizes, develops, prioritizes, and assigns resources to deliver high quality, testable and scalable software solutions within established timelines, while adhering to R&D best practices and processes.\nLeads development project designs and enforces technical standards to ensure solutions execute correctly across various supported environments (i.e. browsers, devices, operating systems).\nProactively leads and solicits the involvement of other project stakeholders (e.g. managers, developers, user interface and visual designers, product managers) to ensure implementation satisfies functional requirements and is consistent with established R&D standards.\nManages and leads project scoping and scheduling; tracks progress of individual tasks and alerts executive management and stakeholders of concerns meeting schedules, while following established R&D standards.\nManages product quality standards by ensuring functional, unit and performance testing is comprehensive and thorough; works closely with development and testing teams to verify test plans. Organizes, prioritizes, and assigns resources to implement and resolve code changes related to enhancements, redesigns and/or bug fixes.\nMaintains accountability for the entire life cycle of the code including support for both internal and external consumers.\nEnsures the veracity of design and technical documentation to satisfy both internal and external consumers.\nContributes to product or project direction through collaboration with Product Management, Marketing, Sales, Customers, and others within SAS.\nProvides technical leadership as appropriate for projects and to the team through mentoring, training, and managing the activities of the team.\nManages all aspects of the department including teamwork, performance management, feedback, professional growth through collaboration with SAS human resources, SAS education and executive leadership.",Industry Type: Aviation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Product quality', 'SAS', 'Performance management', 'Enterprise applications', 'Performance testing', 'Agile', 'Scheduling', 'Management', 'Technical documentation']",2025-06-12 14:32:45
AWS Admin,Hakkda,10 - 15 years,Not Disclosed,['Jaipur'],"ABOUT HAKKODA\n\nHakkoda, an IBM Company, is a modern data consultancy that empowers data driven organizations to realize the full value of the Snowflake Data Cloud. We provide consulting and managed services in data architecture, data engineering, analytics and data science. We are renowned for bringing our clients deep expertise, being easy to work with, and being an amazing place to work! We are looking for curious and creative individuals who want to be part of a fast-paced, dynamic environment, where everyone s input and efforts are valued. We hire outstanding individuals and give them the opportunity to thrive in a collaborative atmosphere that values learning, growth, and hard work. Our team is distributed across North America, Latin America, India and Europe. If you have the desire to be a part of an exciting, challenging, and rapidly-growing Snowflake consulting services company, and if you are passionate about making a difference in this world, we would love to talk to you!.\n\nWe are seeking a highly skilled and experienced AWS Administrator to join a long-term project (12+ months), fully allocated and 100% hands-on. This role will backfill a senior AWS Admin with 10-15 years of experience and requires deep technical capability across AWS infrastructure service. This is not a team leadership role the ideal candidate will operate independently, take full ownership of AWS administration tasks, and contribute directly to maintaining and optimizing cloud operations.\nRole & Responsibilities\nAWS Infrastructure Management: Provision, configure, and maintain AWS services such as EC2, S3, IAM, VPC, Lambda, RDS, CloudWatch, CloudTrail, and more.\nMonitoring & Incident Response: Set up monitoring, logging, and alerting solutions. Respond to and resolve infrastructure issues proactively.\nSecurity & IAM: Manage IAM roles, policies, and user access with a strong focus on security best practices and compliance requirements.\nAutomation & Scripting: Automate routine tasks using scripting (Bash, Python) and AWS CLI/SDK.\nInfrastructure as Code (IaC): Use tools like Terraform or CloudFormation to manage and automate infrastructure deployments and changes.\nCost Optimization: Monitor resource usage and implement cost-control strategies to optimize AWS spending.\nBackup & Disaster Recovery: Manage backup strategies and ensure systems are resilient and recoverable.\nDocumentation: Maintain detailed and up-to-date documentation of AWS environments, standard operating procedures, and runbooks.\nSkils & Qualifications\n10+ years of hands-on AWS administration experience.\nStrong understanding of AWS core services (EC2, S3, IAM, VPC, Lambda, RDS, etc.).\nExperience with scripting (Python, Bash, or PowerShell) and automation tooling.\nProven expertise in using Terraform or CloudFormation .\nDeep knowledge of IAM policy creation and security best practices.\nExperience with monitoring tools such as CloudWatch, Prometheus, or third-party APM tools.\nFamiliarity with CI/CD pipelines and DevOps principles.\nStrong troubleshooting skills with the ability to resolve complex infrastructure issues independently.\nExcellent communication skills with the ability to work effectively with remote teams.\nComfortable working during US Eastern Time zone hours.\nPreferred Qualifications:\nAWS Certifications (e.g., SysOps Administrator Associate , Solutions Architect Associate/Professional ).\nExperience in hybrid environments or with other cloud platforms (Azure, GCP).\nFamiliarity with Snowflake, GitLab, or similar DevOps tooling.\nBenefits:\n\n- Health Insurance\n- Paid leave\n- Technical training and certifications\n- Robust learning and development opportunities\n- Incentive\n- Toastmasters\n- Food Program\n- Fitness Program\n- Referral Bonus Program\n\nHakkoda is committed to fostering diversity, equity, and inclusion within our teams. A diverse workforce enhances our ability to serve clients and enriches our culture. We encourage candidates of all races, genders, sexual orientations, abilities, and experiences to apply, creating a workplace where everyone can succeed and thrive.\n\nReady to take your career to the next level? Apply today and join a team that s shaping the future!!\n\nHakkoda is an IBM subsidiary which has been acquired by IBM and will be integrated in the IBM organization. Hakkoda will be the hiring entity. By Proceeding with this application, you understand that Hakkoda will share your personal information with other IBM subsidiaries involved in your recruitment process, wherever these are located. More information on how IBM protects your personal information, including the safeguards in case of cross-border data transfer, are available here.",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Managed services', 'Infrastructure management', 'Consulting', 'Disaster recovery', 'SDK', 'Troubleshooting', 'Analytics', 'Python', 'Data architecture']",2025-06-12 14:32:47
Databricks Developer,Luxoft,5 - 10 years,Not Disclosed,['Pune'],"Design, build, and manage data pipelines using Azure Data Integration Services (Azure DataBricks, ADF, Azure Functions.)\nCollaborate closely with the security team to develop robust data solutions that support our security initiatives.\nImplement, monitor, and optimize data processes, ensuring adherence to security and data governance best practices.\nTroubleshoot and resolve data-related issues, ensuring data quality and accessibility.\nDevelop strategies for data acquisitions and integration of the new data into our existing architecture.\nDocument procedures and workflows associated with data pipelines, contributing to best practices.\nShare knowledge about latest Azure Data Integration Services trends and techniques.\nImplement and manage CI/CD pipelines to automate data and UI testcases and integrate testing with development pipelines.\nImplement and manage CI/CD pipelines to automate development and integrate test pipelines.\nConduct regular reviews of the system, identify possible security risks, and implement preventive measures.\nSkills\nMust have\nExcellent command of English\nBachelors or Masters degree in Computer Science, Information Technology, or related field.\n5+ years of experience in data integration and pipeline development using Azure Data Integration Services including Azure Data Factory and Azure Databricks.\nHands-on with Python and Spark\nStrong understanding of security principles in the context of data integration.\nProven experience with SQL and other data query languages.\nAbility to write, debug, and optimize data transformations and datasets.\nExtensive experience in designing and implementing ETL solutions using Azure Databricks, Azure Data Factory or similar technologies.\nFamiliar with automated testing frameworks using Squash\nNice to have\nData Architecture & Engineering: Design and implement efficient and scalable data warehousing solutions using Azure Databricks and Microsoft Fabric.\nBusiness Intelligence & Data Visualization: Create insightful Power BI dashboards to help drive business decisions.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nAlpharetta\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nRemote United States\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nPune, India\nReq. VR-114884\nData Science\nBCM Industry\n05/06/2025\nReq. VR-114884\nApply for Databricks Developer in Pune\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'data governance', 'Data quality', 'Engineering Design', 'Business intelligence', 'microsoft', 'Information technology', 'SQL', 'Python', 'Data architecture']",2025-06-12 14:32:49
"Manager, Software Development",SAS Research and Developement (India) Pvt Ltd,12 - 17 years,Not Disclosed,['Pune'],"Nice to meet you!\nWe re a leader in data and AI. Through our software and services, we inspire customers around the world to transform data into intelligence - and questions into answers.\nWe re also a debt-free multi-billion-dollar organization on our path to IPO-readiness. If youre looking for a dynamic, fulfilling career coupled with flexibility and world-class employee experience, youll find it here.\nAbout the role:\nThe role is based in the Pune R&D Center, at SAS R&D Pune facility. We are looking for a\nSoftware Manager\nto development product within the Customer Intelligence line of business. You will be leading the development activities, optimizing allocation for team member, lead the team technically, involve all the stakeholders appropriately, and maintain project standards and best practices and deliver outcomes as planned.\nEssential Experience:\nAt least 12 years of working experience in software development\nAt least 2 years of management/leading experience of software development teams\nExperience of architecting technically sophisticated systems.\nStrong experience in Agile software development cycle\nExperience in enforcing best technical practices in software development\nExcellent written and oral communication skills.\nUnderstanding of manual and automation testing processes and architecture\nUnderstanding of full stack of technology\nExcellent knowledge of enterprise design patterns\nGood understanding of data modelling best practices\nExperience in leading/managing teams with energy and optimism\nAbility to motivate team members to function collaboratively\nGood in multi-tasking\nExperience of these items would be useful:\nCustomer Intelligence domain preferred\nExperience with Docker, Kubernetes\nManagement in development on any public cloud providers like Amazon Web Services, Microsoft Azure, and Google Cloud, etc.\nManaging Continuous Integration and Continuous Delivery (CI, CD) model\nWe are a friendly team, and we ll be offering you plenty of opportunities to develop your career. Interested? Then please get in touch to find out more!\nPrimary Responsibilities\nOrganizes, develops, prioritizes, and assigns resources to deliver high quality, testable and scalable software solutions within established timelines, while adhering to R&D best practices and processes.\nLeads development project designs and enforces technical standards to ensure solutions execute correctly across various supported environments (i.e. browsers, devices, operating systems).\nProactively leads and solicits the involvement of other project stakeholders (e.g. managers, developers, user interface and visual designers, product managers) to ensure implementation satisfies functional requirements and is consistent with established R&D standards.\nManages and leads project scoping and scheduling; tracks progress of individual tasks and alerts executive management and stakeholders of concerns meeting schedules, while following established R&D standards.\nManages product quality standards by ensuring functional, unit and performance testing is comprehensive and thorough; works closely with development and testing teams to verify test plans. Organizes, prioritizes, and assigns resources to implement and resolve code changes related to enhancements, redesigns and/or bug fixes.\nMaintains accountability for the entire life cycle of the code including support for both internal and external consumers.\nEnsures the veracity of design and technical documentation to satisfy both internal and external consumers.\nContributes to product or project direction through collaboration with Product Management, Marketing, Sales, Customers, and others within SAS.\nProvides technical leadership as appropriate for projects and to the team through mentoring, training, and managing the activities of the team.\nManages all aspects of the department including teamwork, performance management, feedback, professional growth through collaboration with SAS human resources, SAS education and executive leadership.\nCANDIDATE PROFILE\nBackground and Experience\nSAS is seeking a Software Manager with overall experience of 12+ years, with minimum 2 years management/leading experience. The candidate should have development and management experience in Saas enterprise applications. The candidate should have excellent organizing and prioritizing skills, and sharp technical skills to choose the right direction technically. Candidate should have proven track record of delivery management and people management. Candidate should also have excellent verbal and written communication skills and possess the confidence to pull the work in the right direction, keeping all stake holders involved and engaged.\n#highlightedjob",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Product quality', 'SAS', 'Performance management', 'Enterprise applications', 'Performance testing', 'Agile', 'Scheduling', 'Management', 'Technical documentation']",2025-06-12 14:32:52
Cyber Intel Fusion Analyst,Outreach,2 - 4 years,Not Disclosed,['Hyderabad'],"Outreach is the first and only AI Sales Execution Platform built for intelligent revenue workflows. Built on the world s largest foundation of customer interactions and go-to-market team data, Outreach s leading revenue AI technology helps go-to-market professionals and their companies win by intelligently accelerating decision making and elevating sellers to do their best work. Our powerful platform gives revenue teams the tools they need to design, measure, and improve a revenue strategy for every stage of the customer journey, improving efficiency and effectiveness across the entire revenue cycle. Over 6, 000 customers, including Zoom, McKesson, Snowflake, SAP, and Okta use Outreach to power workflows, put customers at the center of their business, improve revenue results, and win in the market.\nOutreach is a privately held company based in Seattle, Washington, with offices worldwide. To learn more, please visit www. outreach. io .\n\nThe Role\n\nBridging Intelligence and Action\nThe Cyber Intel Fusion Analyst is a pivotal role within our security program.\n\nThis position serves as a critical bridge, linking strategic threat intelligence with tactical security operations.\nThe analyst will be instrumental in evolving our security practices beyond traditional, siloed functions while ensuring that intelligence capabilities are not merely insightful but are directly integrated and operationalized within our security framework.\nThis proactive operationalization of intelligence is key to anticipating emerging threats and developing innovative countermeasures to counter sophisticated cyber threats before they can impact our services or compromise sensitive information.\nThe ability to quickly fuse intelligence into operational defense mechanisms provides a distinct security advantage, crucial for maintaining service reliability and customer trust.\nYour Daily Adventures Will Include\nCore Responsibilities: Shaping Our Defenses\nThe responsibilities of the Cyber Intel Fusion Analyst are multifaceted, demanding a blend of analytical acumen, technical expertise, and collaborative skill.\nIntelligence Cycle Management Requirements Definition: The analyst will manage the intelligence analysis cycle as it pertains to team operations. This includes working closely with team operators and other stakeholders to identify and refine intelligence requirements that drive threat emulation assessments and inform defensive strategies. A key function involves identifying intelligence requirements for diverse areas such as security operations, cloud security, enterprise security, and application security, including those related to artificial intelligence. This broad scope necessitates an understanding of the unique intelligence needs of various teams, positioning the analyst as a strategic partner who can tailor and deliver relevant intelligence to enhance the effectiveness of multiple security functions.\nTactical Intelligence Analysis Adversary Understanding: A core function is providing tactical cyber intelligence analysis, meticulously identifying specific adversary tactics, techniques, and procedures (TTPs). This analysis will be consistently tied back to established frameworks like the MITRE ATTCK Framework, leveraging intelligence provided by relevant organizations. The role involves recognizing and researching attacks and attack patterns based onpublished open-source intelligence (OSINT) and other intelligence sources. The analyst will be adept at handling and organizing disparate data concerning detections, attacks, and attackers to accurately identify adversary groups and their modus operandi, thereby driving assessments pertinent to the company. This process transforms general threat data into a refined understanding of adversaries specifically targeting our environment, such as those focusing on SaaS platforms if applicable.\nDeveloping Actionable Intelligence Driving Threat Emulation: The analyst is tasked with developing, producing, and managing Adversary Response Playbooks. These playbooks are crucial for supporting and driving threat emulation assessments, ensuring our defenses are tested against realistic adversary behaviors. 1 This involves translating analyzed intelligence on adversary TTPs and campaign indicators into actionable detection strategies, such as developing custom SIEM correlation rules or contributing to Security Orchestration, Automation, and Response (SOAR) playbooks. This operationalization of intelligence is fundamental, turning analytical findings into tangible, proactive defensive measures that strengthen our security posture.\nCollaboration, Liaison Stakeholder Management: Effective relationship management is paramount. The analyst will manage relationships with organizations, both internal and external, that provide requested intelligence to the team or receive information from it. A significant part of the role includes representing the team in cyber threat intelligence-related meetings and matters, acting as a crucial liaison. This collaboration extends across multiple organizational functions, potentially including cloud engineering teams, DevSecOps personnel, SOC analysts, incident responders, and even executive leadership. By effectively sharing tailored intelligence, the analyst acts as a force multiplier, enhancing the capabilities and preparedness of various teams across the organization.\nOur Vision of You\nCore Competencies: Mastery of the Intelligence Cycle: Expertise in managing the intelligence analysis cycle, encompassing planning, collection (including OSINT and multi-source intelligence), processing, in-depth analysis of adversary TTPs, and the production and dissemination of timely, accurate, and actionable intelligence products tailored to diverse internal audiences.\nStrategic Requirements Identification: Proven ability to identify and refine intelligence requirements for a wide array of security functions, includingsecurity operations, cloud security, enterprise security, and application security (potentially including AI), ensuring intelligence efforts align with business and operational needs.\nTactical Intelligence TTP Expertise: Strong skills in tactical cyber intelligence analysis, identifying specific adversary TTPs and mapping them to frameworks like MITRE ATTCK . This includes researching current attacks, attack patterns, and understanding threats specific to modern environments (e. g. , SaaS-specific attack patterns).\nActionable Output Development: Demonstrable experience in developing, producing, and managing resources like Adversary Response Playbooks to support and drive threat emulation assessments, effectively translating intelligence into practical defensive measures.\nData Synthesis Adversary Profiling: Capability in handling and organizing disparate data about detections, attacks, and attackers to properly identify adversary groups and develop comprehensive threat actor profiles, particularly those relevant to the company s operational landscape.\nExceptional Collaboration Liaison Skills: Excellent relationship management abilities with internal and external intelligence providers and consumers, and proven experience acting as an effective liaison and team representative in intelligence matters.\nEducation and Experience: A minimum of 5 years of progressive, hands-on experience in the cybersecurity domain, with a demonstrable track record in roles that combine cyber threat intelligence analysis with security operations or incident response functions. Experience in environments with a significant cloud and SaaS focus is highly advantageous. This emphasis on combined experience highlights the need for individuals who have practically applied the ""fusion""concept.\nTechnical Prowess: The analyst must possess a robust set of technical skills to effectively investigate security incidents, analyze threat data, and implement defensive measures, especially within cloud environments.\n\nEssential technical competencies are outlined below:\n\nAn in-depth understanding of core networking protocols (TCP/IP, UDP, HTTP/S, DNS, SMTP, etc. ), network traffic analysis methodologies, and the function of common networking ports and protocols.\nProficiency with cloud security architectures (IaaS, PaaS, SaaS) and hands-onexperience with security tools native to major cloud platforms (e. g. , AWS, Azure, GCP).\nExpertise with Security Information and Event Management (SIEM) platforms for log correlation, advanced analysis, and the development of custom detection rules.\nHands-on experience with Endpoint Detection and Response (EDR/XDR) solutions for endpoint threat detection, investigation, and response.\nStrong skills in comprehensive log analysis from diverse cloud and on-premises sources, including operating systems (Windows, Linux, macOS), applications, network devices, and cloud service logs (e. g. , CloudTrail, Azure Monitor).\nA solid understanding of Windows and Linux operating systems (including distributions such as RHEL, Ubuntu, CentOS) and macOS, encompassing system administration fundamentals, security configurations, logging mechanisms, and common attack vectors.\nScripting skills for automation of analytical tasks, data manipulation, tool integration, or the development of custom detection scripts using languages such as Python, PowerShell, or Bash.\nDeep understanding and practical application of threat intelligence frameworks such as the MITRE ATTCK Framework, the Cyber Kill Chain , and the Diamond Model of Intrusion Analysis.\nThe following outlines core technical competencies and representative toolsets relevant to this role:\nCategory Examples/Specific Tools (Tailored for SaaS)\nCloud Platform Security: AWS (GuardDuty, Security Hub, Macie, Inspector), Azure (Sentinel, Defender for Cloud), GCP (Security Command Center)\nSIEM: Google SecOps, CrowdStrike NG SIEM, Sumologic CloudSiem\nEDR/XDR: CrowdStrike Falcon, JAMF Protect\nNetwork Analysis: Wireshark, Zeek (formerly Bro), Suricata, Cloud-native traffic mirroring/analysis tools\nVulnerability Management: CrowdStrike Exposure Management, Wiz, Cloud-native vulnerability scanners\nScripting Languages: Python, PowerShell, Bash\nOperating Systems: Windows (Client/Server), Linux (various distributions such as RHEL, Ubuntu, CentOS), macOS\nThreat Intelligence Platforms: (TIPs) MISP, ThreatConnect, Anomali ThreatStream, Recorded Future.\nAnalytical and Communication Skills: Exceptional analytical and problem-solving skills, with a demonstrated ability to correlate disparate datasets, identify subtle patterns of malicious activity, and make sound, evidence-based judgments, often under pressure.\nExcellent written and verbal communication skills, with the proven ability to articulate complex technical information, security concepts, and intelligence findings clearly and concisely to diverse audiences, including technical peers and management.\nWork Requirements:\nThis position requires participation in an on-call rotation to provide expert support during critical security incidents. This role does not involve regular shift work.\nBonus Points: Preferred Qualifications\nWhile not mandatory, the following qualifications will significantly differentiate strong candidates and indicate a deeper specialization.\n\nAdvanced industry-recognized cybersecurity certifications. Examples include:\n\nGIAC Cyber Threat, SANS/GIAC Cyber Threat Intelligence , Intelligence (GCTI), GIAC Certified Intrusion, SANS/GIAC Network Security Monitoring, Analyst (GCIA), Intrusion Detection, GIAC Certified Incident, SANS/GIAC Incident Response, Handler (GCIH), CISSP (ISC) Broad Cybersecurity, Management Operations, AWS Certified Security - Amazon Web Services AWS Cloud Security Specialty, Azure Security Engineer, Microsoft Azure Cloud Security, Associate (AZ-500), CompTIA Cybersecurity, CompTIA Cybersecurity Analysis, Analyst (CySA+), Intrusion Detection, Offensive Security Certified, Offensive Security Penetration Testing, Professional (OSCP), (Understanding Attacker Methods)\nPractical experience utilizing Threat Intelligence Platforms (TIPs) such as MISP, ThreatConnect, Anomali ThreatStream, or Recorded Future.\nExperience with Security Orchestration, Automation, and Response (SOAR) platforms and playbook development.\nKnowledge of malware analysis (static and dynamic) and reverse engineering techniques, and familiarity with associated tools.\nFamiliarity with DevSecOps principles and experience securing CI/CD pipelines.\nUnderstanding of compliance frameworks relevant to SaaS environments (e. g. , SOC 2, ISO 27001/27701/42001, GDPR, HIPAA).\nWhy You ll Love It Here\n\nHighly competitive salary\n25 days annual vacation time + sick time and casual leave\nGroup medical policy coverage available to employees and up to 5 eligible family members\nOPD benefit covered up to INR 10, 000\nLife insurance and personal accident insurance at 3x annual CTC\n26 weeks of maternity leave pay, and 15 days of paternity leave pay\nOpportunity to be part of company success via the RSU program\nDiversity and inclusion programs that promote employee resource groups like OWN+ (Outreach Womens Network), Adelante (Latinx community), OBX (Outreach Black Connection), Mosaic (AAPI community), Pride (LGBTQIA+), Gender+, Disability Community, and Veterans/Military\nEmployee referral bonuses to encourage the addition of great new people to the team\nFun company and team outings because we play just as hard as we work\n\nOur success is reliant on building teams that include people from different backgrounds and experiences who can elevate assumptions and ideas with fresh perspectives. Were dedicated to hiring the whole human, not just a resume. To that end, we look for a diverse pool of applicants-including those from historically marginalized groups. We would like to invite you to apply even if you dont think you meet all of the requirements listed below. We dont want a few lines in a job description to get between us and the opportunity to meet you.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Automation', 'SAP', 'Linux', 'Information security', 'DNS', 'Network security', 'HTTP', 'Windows', 'Open source', 'Python']",2025-06-12 14:32:54
Starburst Engineer,Luxoft,1 - 13 years,Not Disclosed,['Pune'],"Design, develop, and maintain scalable data solutions using Starburst.\nCollaborate with cross-functional teams to integrate Starburst with existing data sources and tools.\nOptimize query performance and ensure data security and compliance.\nImplement monitoring and alerting systems for data platform health.\nStay updated with the latest developments in data engineering and analytics.\nSkills\nMust have\nBachelors degree or Masters in a related technical field; or equivalent related professional experience.\nPrior experience as a Software Engineer applying new engineering principles to improve existing systems including leading complex, well defined projects.\nStrong knowledge of Big-Data Languages including:\nSQL\nHive\nSpark/Pyspark\nPresto\nPython\nStrong knowledge of Big-Data Platforms, such as:o The Apache Hadoop ecosystemo AWS EMRo Qubole or Trino/Starburst\nGood knowledge and experience in cloud platforms such as AWS, GCP, or Azure.\nContinuous learner with the ability to apply previous experience and knowledge to quickly master new technologies.\nDemonstrates the ability to select among technology available to implement and solve for need.\nAble to understand and design moderately complex systems.\nUnderstanding of testing and monitoring tools.\nAbility to test, debug, fix issues within established SLAs.\nExperience with data visualization tools (e.g., Tableau, Power BI).\nUnderstanding of data governance and compliance standards.\nNice to have\nData Architecture & Engineering: Design and implement efficient and scalable data warehousing solutions using Azure Databricks and Microsoft Fabric.\nBusiness Intelligence & Data Visualization: Create insightful Power BI dashboards to help drive business decisions.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nAlpharetta\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nRemote United States\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nPune, India\nReq. VR-114886\nData Science\nBCM Industry\n05/06/2025\nReq. VR-114886\nApply for Starburst Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data security', 'data governance', 'Engineering Design', 'Apache', 'Business intelligence', 'microsoft', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-12 14:32:57
Principal Software Engineer (Full Stack),Insightsoftware,10 - 14 years,Not Disclosed,['Hyderabad'],"Insightsoftware is a growing, dynamic software company that helps businesses achieve greater levels of financial intelligence across their organization with our world-class financial reporting solutions. At Insightsoftware, you will learn and grow in a fast-paced, supportive environment that will take your career to the next level. We are looking for future Insighters who can demonstrate teamwork, results orientation, a growth mindset, disciplined execution, and a winning attitude to join our growing team.\nRole : Principal Software Engineer\ninsightsoftware is seeking a Principal Software Engineer to join our team. The ideal candidate will have a strong full-stack development background, with expertise in Java coding, particularly using Spring Boot for backend development. Additionally, proficiency in front-end technologies, especially ReactJS/Vue.js, is essential.\nThis critical role would require working with the development team to ensure accessibility for all users by developing a front end that functions across browsers, platforms, and devices while meeting accessibility and security requirements.\nWe provide powerful tools to our clients which empower enterprises and organizations in business planning, consolidation, and performance monitoring.\nAre you ready to lead in this new era of technology and solve Enterprise Performance Management s most challenging problems?\nOur Engineering team\nOur engineers participate in all phases of the application development lifecycle, focusing on design, coding, and keeping the production platform up and running.\nOur engineering culture also values curiosity, humility, trust and team spirit.\nOur technical stack is Java with Spring Boot and Hibernate frameworks, SQL, ORM, JPA, Vue.js, JavaScript, SCSS, Amazon Web Services, Maven\nThis is What You Will do in This Role:\nDesign, develop, and maintain complex web applications using Vue.js and associated frameworks.\nLead the architecture and design of scalable frontend applications.\nCollaborate with backend developers to integrate APIs and ensure seamless functionality.\nMentor junior developers, conduct code reviews, and ensure adherence to best practices.\nOptimize applications for maximum speed, scalability, and performance.\nStay updated with the latest trends and advancements in Vue.js and frontend development.\nDebug and troubleshoot application issues to maintain high quality and performance.\nWork with Quality Assurance (QA) team to get the product tested, address any issues\nRequirements -\n10-14 years of web application development experience in a fast-paced agile environment experience required\nProficiency in Springboot and ReactJS/ Vue.js ecosystem (Vue Router, Vuex/Pinia, Composition API).\nStrong experience with JavaScript, TypeScript, HTML, and CSS.\nFamiliarity with modern build tools (e.g., Webpack, Vite).\nKnowledge of RESTful APIs and integration techniques.\nFamiliarity with version control systems like Git.\nStrong problem-solving skills and attention to detail.\nExcellent communication and leadership skills.\nCollaborate with team members to define project requirements, priorities, and timelines.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Maven', 'Hibernate', 'Manager Quality Assurance', 'Front end', 'Performance management', 'Coding', 'Agile', 'HTML', 'JPA', 'SQL']",2025-06-12 14:32:59
Staff Software Engineer,Fourkites,10 - 15 years,Not Disclosed,['Chennai'],"At FourKites we have the opportunity to tackle complex challenges with real-world impacts. Whether it s medical supplies from Cardinal Health or groceries for Walmart, the FourKites platform helps customers operate global supply chains that are efficient, agile and sustainable.\nJoin a team of curious problem solvers that celebrates differences, leads with empathy and values inclusivity\n.\nAs a Staff Software Engineer, you will get an opportunity to work on features end to end (backend & frontend) using the latest technologies such as RoR, Java, GoLang, Angular, React, Redis, PostgreSQL. You will develop products that can change the logistics landscape and will be used by some of the biggest corporations in the world. You will develop integrations with our strategic partners to help expand our ecosystem. You will work closely with our US team and customers to develop features that help shape the logistics and supply chain industry.\nWho you are:\nBachelor s degree in Computer Science & Engineering or related field from a reputed institution.\nMinimum of 8 years of experience in Software Engineering and Web application development.\nGood understanding of software design, Microservices architecture, object-oriented principles, and design patterns.\nExperience with Design and development of highest quality software/services using RoR/Golang/Java.\nGood knowledge of RESTful APIs and microservices architecture\nStrong understanding of Java, Spring Framework, and object-oriented programming principles\nExperience in one of Azure, Amazon Web Services or other cloud services.\nExperience with databases such as MySQL, PostgreSQL, or MongoDB\nFamiliarity with front-end technologies such as HTML, CSS, and JavaScript is a plus\nStrong knowledge of Git (branches, submodules, rebasing) and other Agile tools such as JIRA & Confluence.\nAgile SDLC experience\nExcellent oral and written communication skills\nWhat you ll be doing:\nDesign, architect, implement, test, profile, release, and optimize highest quality software/services using RoR/Golang/Java.\nPartner with product manages to analyse product requirements and plan engineering execution\nDocument HLD/LLD for easy knowledge sharing and future scaling\nPerform design and code reviews\nImplement code with very high coverage of unit tests and component tests\nCross-training peers and mentoring teammates\nPossess expert knowledge in performance, security, scalability, architecture, and best practices\nFunctionally decompose complex problems into simple, straight-forward solutions\nCollaborate with UX designers to develop responsive user interface components\nWorking knowledge of SQL based (any RDBMS) and NOSQL data stores (any one) with the ability to write intermediate level SQL\nExperience in building Web application backends using Java Spring Boot or similar\nExperience with frontend libraries/frameworks such as React/Angular is a plus.\nEducation Qualification: Graduate from B.E/ B.Tech / MCA / M.Tech Background.\nWho we are:\nFourKites , the leader in AI-driven supply chain transformation for global enterprises and pioneer of real-time visibility, turns supply chain data into automated action. FourKites Intelligent Control Tower breaks down enterprise silos by creating a real-time digital twin of orders, shipments, inventory and assets. This comprehensive view, combined with AI-powered digital workers, enables companies to prevent disruptions, automate routine tasks, and optimize performance across As the leader in AI-driven supply chain transformation, FourKites pioneered the Intelligent Control Tower powered by the world s largest real-time visibility network. Our platform creates comprehensive digital twins of your supply chain with AI-powered digital workers to automate resolution, improve collaboration and drive outcomes across all stakeholders. Unlike traditional control towers, we enable true real-time execution and intelligent fulfillment, transforming both your supply and customer\nBenefits\nMedical benefits start on first day of employment\n36 PTO days( Sick, Casual and Earned) , 5 recharge days, 2 volunteer days\nHome Office setups and Technology reimbursement\nLifestyle & Family benefits\nAnnual Swags/ Festive Swags\nOngoing learning & development opportunities ( Professional development program, Toast Master club etc.)",Industry Type: Courier / Logistics,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Software design', 'RDBMS', 'Postgresql', 'MySQL', 'Javascript', 'Agile', 'HTML', 'SDLC', 'SQL']",2025-06-12 14:33:01
PLM PROCESS LEAD - TECHNICAL,Mondelez,8 - 13 years,Not Disclosed,['Mumbai'],"P rovide s software and application knowledge to support implementation of the given solutions specifically in SAP S/4 PLM domain collaborating with stakeholders in R D .\n  How will this person contribute\nPerson will ensure that delivered services are optimized to meet business demands and the service operations strategy, plan, measure, report and communicate service improvement initiatives, and serve as a consultant on issues and resolutions. W ill also recommend actions that can be taken to optimize investments and benefits and to mitigate risks. This role will require to identify suppliers, evaluate them, on-board new vendors, establish and run vendor governance; collaborate with management and follow-up on requisitions, purchase orders, invoices, and payments; work with project resources to provide design collateral and to configure software components so they are aligned with security policy and governance; and ensure adherence to development and configuration standards and processes.\n  What Person will bring to the table\nThis role is a Solution Delivery Expert mainly in SAP S/4 PLM area\nSDE will be the process system expert in Product Lifecycle Management domains and responsible for the delivery of Projects, and also ensure all projects are delivered with quality and benefits.\nWorking collaboratively with multiple vendors\nLeading complex projects - project management\nStakeholder management and influencing skills\nManaging infrastructure services delivery, support and excellence\nWorking in global IT function with regional or global responsibilities in an environment like Mondel z International\nWorking with IT outsourcing providers using frameworks such as the IT Infrastructure Library\nWorking with internal and external teams and leading when necessary\nWhat extra ingredients person will bring:\nDeep Knowledge SAP PLM practices And associated general knowledge of SAP ECC /MDG ), Work flows , Data Migration and Integration with SAP and Non-SAP Applications\nEducation / Certifications:\nUniversity Degree in Engineering, Computer Science or related fields.\nAt least 8 years of experience in similar roles\nAny data certification is a plus\nFluent in English",Industry Type: FMCG,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP ECC', 'Data migration', 'SAP', 'SAP PLM', 'Project management', 'Product life cycle management', 'IT infrastructure', 'Stakeholder management', 'Application software', 'it outsourcing']",2025-06-12 14:33:03
PHP Backend (Platform) Developer,Arbor,6 - 11 years,9-14 Lacs P.A.,['Thiruvananthapuram'],"Role & responsibilities\nAbout you\nExperience of PHP at scale through frameworks such as Symfony /Laravel\nExperience of distributed cloud systems, and specifically Amazon Web Services\nEnterprise Software design patterns and their implementation in real-world enterprise systems\nExperience of message queuing and/or streaming systems such as SQS, ActiveMQ, Apache Kafka, AWS Kinesis, AWS Firehose\nUnderstanding of relational database technologies and their cloud versions (e.g. AWS MySQL Aurora)\nExperience with DataDog, Prometheus or similar observability tools\nA positive and proactive attitude to problem solving\nA team player, willing to muck in and help others when needed, driven personality who asks questions and actively participates in discussions\nCore responsibilities\nDevelop core platform components to aid reusability and stability of the system\nWork with Head of Platform Engineering/SRE to identify and progress platform improvements related to stability, scalability, and performance\nWork with the QA automation framework to ensure functionality is delivered to a high quality\nWork with DevOps Engineers to understand application impacts and system performance and stability, and work with engineering teams to rectify\nAssist in incident response and resolution, and subsequent post-mortems and retrospectives\nContribute to the platform code base and framework which is used by Product Engineers across Engineering\n\nPreferred candidate profile\n\nBonus skills\nPast experience with enterprise solutions running at scale\nFamiliarity with Scrum methodology or other agile development processes\nExperience with Docker and containerization\nExperience with AWS or other Cloud Infrastructure\nFamiliarity with software best practices such as Refactoring, Clean Code, Domain-Driven Design, Test-Driven Development, etc.",Industry Type: Education / Training,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['PHP', 'Symfony Framework', 'Symfony', 'OOPS', 'MVC Framework', 'Oops Design Patterns', 'Yii', 'Zend Framework', 'Oops Programming', 'Laravel', 'Core PHP']",2025-06-12 14:33:06
NetApp Storage Administrator,SVN System Technologies,3 - 6 years,Not Disclosed,['Thane'],"SVN System Technologies is looking for NetApp Storage Administrator to join our dynamic team and embark on a rewarding career journey We are seeking a skilled NetApp Storage Administrator to join our IT team\n\nThe NetApp Storage Administrator will be responsible for the design, implementation, and maintenance of NetApp storage solutions to ensure optimal performance, availability, and reliability\n\nThe ideal candidate will have a strong background in storage administration, NetApp technologies, and a proactive approach to managing storage infrastructure\n\nResponsibilities:Storage Design and Implementation:Design and implement NetApp storage solutions based on organizational requirements\n\nConfigure and optimize storage arrays for performance and efficiency\n\nStorage Administration:Administer and manage NetApp storage systems, including filers, aggregates, and volumes\n\nPerform routine monitoring and maintenance tasks to ensure system health\n\nData Migration and Storage Expansion:Plan and execute data migration activities between storage systems\n\nManage storage expansion projects to accommodate growing data needs\n\nBackup and Disaster Recovery:Implement and manage backup and disaster recovery solutions for NetApp storage\n\nPerform regular data backups and test recovery processes\n\nPerformance Optimization:Monitor and analyze storage performance metrics\n\nIdentify and implement optimization strategies for improved performance\n\nSecurity and Access Control:Configure and manage security settings for NetApp storage, including access controls\n\nEnsure compliance with data privacy and security policies\n\nTroubleshooting:Investigate and resolve storage-related issues and incidents\n\nCollaborate with other IT teams to address cross-functional infrastructure challenges\n\nDocumentation:Create and maintain comprehensive documentation for NetApp storage configurations, procedures, and best practices\n\nDevelop and update standard operating procedures (SOPs) for storage administration",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PDF', 'JPEG', 'netapp storage administrator']",2025-06-12 14:33:08
Principal Site Reliability Engineer,Swimlane,10 - 15 years,Not Disclosed,['Hyderabad'],"Do you have an interest and desire to work with cutting edge technologies to solve challenging problems? Do you love cyber-security? Would you like to help build a platform that helps security teams process millions of security alerts every day? Are you interested in a role where you can use the latest JavaScript technologies and frameworks, and contribute to open source?\n\nAs the most senior technical individual contributor within an entire division of Engineering at Swimlane, you will be deeply involved with and guide the reliability, availability, security, quality, and extensibility of our offerings. You must have an extreme ownership mentality. You will work very closely with other senior Engineering, Product, and Support resources to quickly advance the number and state of our offerings. You will create, test, and operate new services, as well as enhance existing ones.\n\nJob Requirements:\n8+ years experience developing in at least one common, general languages (i.e., C, C#, Java, Python, etc)\n8+ years experience as a senior or principal engineer\n15+ years experience as an engineer managing mission-critical services at scale\n6+ years experience with Amazon Web Services (AWS), Microsoft Azure, and/or Google Compute Platform (GCP)\nDeep, demonstrable experience with various Cloud-native monitoring, logging, and dashboarding platforms (e.g., New Relic, Datadog, Prometheus, etc)\nExcellent understanding of and ability to work with Hashicorp Terraform\nStrong understanding of modern continuous integration/continuous deployment (CI/CD) platforms (e.g., GitHub Actions, GitLab pipelines, AWS CodeBuild / Codedeploy / Codepipeline , etc)\nSubstantial experience with managing Kubernetes resources\nExcellent written and verbal English communication skills\nStrong ability to work in a fast-paced environment that does also have security and compliance requirements\nMust be available for on-call escalations\nMust be able to mentor resources of different levels\nAbility to manage up, down, and across the organization\nAutomate, automate, automate!\n\nDont meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Swimlane, we are dedicated to building a diverse, inclusive and authentic workplace, so if you are excited about this role but your past experience doesnot align perfectly with every qualification in the job description, we encourage you to apply anyway! You may just be the right candidate for this or other roles!\n\nWho we are, and what we offer:\nSwimlane is a rapidly growing, innovative startup that provides cloud-scale, low-code security automation for organizations of all industries and sizes. Our technology is relied upon by major security-forward companies around the globe and we are consistently rated as the #1 trusted low-code security automation platform. Our mission is to prevent breaches and enable continuous compliance via a low-code security automation platform that serves as the system of record for the entire security organization.\n\nWhat s the best thing about working at Swimlane? If you ask the team, they will tell you its the people. Swimlaners are innovative, collaborative and driven by the purpose of revolutionizing the way security teams automate and respond to alerts. Headquartered in beautiful Louisville,Colorado, directly between Denver and Boulder, Swimlanes staff spans 28 states and 16 countries!\n\nThe Perks of being a Swimlaner\nCompetitive Benefits & Compensation\nTraining & Professional Development Opportunities\nMacbook Pro\nGreat Company Culture\nWe value collaboration and innovation\nGive-back Volunteering Opportunities\n\nHere at Swimlane, our core focus is to Automate the World of Security and we strive to represent our five core values in everything we do:\nPunch above your weight class - We make the most of our circumstances and constantly surprise and impress with our ability to deliver.\nBe a happy innovator - The hard problems are the fun problems to solve, we re excited to take on difficult challenges and find creative solutions.\nAlways be leveling up - We are continuously improving, embracing change, and consuming information to better ourselves and each other.\nMove at the speed of WOW - We work with an extreme sense of urgency, but we never compromise quality.\nHave honesty and integrity in all the things - We make decisions with the best of intentions, doing what is right for as many stakeholders as possible.",Industry Type: Hardware & Networking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'github', 'cyber security', 'Compliance', 'GCP', 'Cloud', 'Javascript', 'Open source', 'Monitoring', 'Python']",2025-06-12 14:33:10
PHP Developer,Apto Innovations Private Limited,0 - 2 years,Not Disclosed,['Kozhikode'],"We are looking for an experienced developer to join our team.\nCandidate with in depth knowledge of PHP.\nVersioning tools such as Git, Bit Bucket\nKnowledge of operating systems, hosting and deployment on cloud\nplatforms like Amazon Web Services. will be added advantage\nWhat we expect from you\nWriting clean, well-designed code\nDebugging code and fixing bugs\nManaging code repositories and deploying builds\nTo know about PHP, Laravel, MySQL/ MariaDB, HTML5, CSS3, JavaScript, AJAX/ Fetch, jQuery, etc.\nRoleFull Stack Developer\nIndustry TypeBanking\nFunctional AreaEngineering - Software QA\nEmployment TypeFull Time, Permanent",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['css', 'web services', 'hosting', 'bitbucket', 'ajax', 'jquery', 'docker', 'iot', 'git', 'devops', 'linux', 'jenkins', 'debugging', 'json', 'html', 'mysql', 'shell scripting', 'architecture', 'laravel', 'python', 'operating systems', 'microsoft azure', 'cloud platforms', 'php development', 'javascript', 'mariadb', 'php', 'aws']",2025-06-12 14:33:12
OIC Developer,Sutherland Global Services Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"Job Description:\nWork Hours : 2:30pm IST to 11:30pm IST\nDuration : From 1st June till 31st December 2026\nResource Type : Contract Resource (Later can be converted FTE)\nRequired Skills :\nProven expertise in Oracle Integration Cloud (OIC), including integration patterns, connectivity agents, and process automation.",,,,"['Computer science', 'Process automation', 'Training', 'Performance tuning', 'Data migration', 'Analytical', 'Integration testing', 'Cloud', 'Oracle', 'SSIS']",2025-06-12 14:33:15
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Nagpur'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 14:33:18
SBU Support Planner-2,Solenis,5 - 7 years,Not Disclosed,['Hyderabad'],"Solenis is a leading global provider of water and hygiene solutions. The company s product portfolio includes a broad array of water treatment chemistries, process aids, functional additives, cleaners, disinfectants, and state-of-the-art monitoring, control and delivery systems. These technologies are used by customers to improve operational efficiencies, enhance product quality, protect plant assets, minimize environmental impact, and create cleaner and safer environments. Headquartered in Wilmington, Delaware, the company has 70 manufacturing facilities strategically located around the globe and employs a team of over 16, 500 professionals in 130 countries across six continents. Solenis is a 2025 Best Managed Company Gold Standard honoree. For more information about Solenis, please visit www. solenis. com .",,,,"['Procurement', 'Supply chain', 'Data analysis', 'Demand planning', 'Raw material', 'Forecasting', 'Monitoring', 'Analytics', 'Logistics', 'Capacity planning']",2025-06-12 14:33:20
DevOps Engineer,Easemytrip,1 - 6 years,Not Disclosed,"['Noida', 'Gurugram']","About the Role:\nAs a DevOps Engineer at EaseMyTrip.com, you will be pivotal in optimizing and maintaining our IT infrastructure and deployment processes. Your role involves managing cloud environments, implementing automation, and ensuring seamless deployment of applications across various platforms. You will collaborate closely with development teams to enhance system reliability, security, and efficiency, supporting our mission to provide exceptional travel experiences through robust technological solutions. This position is critical for maintaining high operational standards and driving continuous innovation.\n\nRole & responsibilities:\nCloud Computing Mastery: Expert in managing Amazon Web services (AWS) environments, with skills in GCP and Azure for comprehensive cloud solutions and automation.\nWindows Server Expertise: Profound knowledge of configuring and maintaining Windows Server systems and Internet Information Services (IIS).\nDeployment of .NET Applications: Experienced in deploying diverse .NET applications such as ASP.Net, MVC, Web API, and WCF using Jenkins.\nProficiency in Version Control: Skilled in utilizing GitLab or GitHub for effective version control and collaboration.\nLinux Server Management: Capable of administering Linux servers with a focus on security and performance optimizations.\nScripting and Automation: Ability to write and maintain scripts for automation of routine tasks to improve efficiency and reliability.\nMonitoring and Optimization: Implement monitoring tools to ensure high availability and performance of applications and infrastructure.\nSecurity Best Practices: Knowledge of security protocols and best practices to safeguard systems and data.\nContinuous Integration/Continuous Deployment (CI/CD): Develop and maintain CI/CD pipelines to streamline software updates and deployments.\nCollaboration and Support: Work closely with development teams to troubleshoot deployment issues and enhance the overall operational efficiency.\n\nPreferred candidate profile:\nMigration Project Leadership: Experienced in leading significant migration projects from planning through to execution.\nDatabase Expertise: Strong foundation in both SQL and NoSQL database technologies.\nExperience with Diverse Tech Stacks: Managed projects involving various technologies, including 2-tier, 3-tier, and microservices architectures.\nProficiency in Automation Tools: Hands-on experience with automation and deployment tools such as Jenkins, Bamboo, and Code Deploy.\nAdvanced Code Management: Highly skilled in managing code revisions and maintaining code integrity across multiple platforms.\nStrategic DevOps Experience: Proven track record in developing and implementing DevOps strategies at an enterprise level.\nConfiguration Management Skills: Proficient in using tools like Ansible, Chef, or Puppet for configuration management.\nTechnology Versatility: Experience working with a range of programming languages and frameworks, including .NET, MVC, LAMP, Python, and NodeJS.\nProblem Solving and Innovation: Ability to solve complex technical issues and innovate new solutions to enhance system reliability and performance.\nEffective Communication: Strong communication skills to collaborate with cross-functional teams and articulate technical challenges and solutions clearly.",Industry Type: Travel & Tourism,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Code Deploy', 'Aws Cloud', 'Ci/Cd', 'Devops', 'Devops Automation', 'Apm Tools', 'Cloud Automation Devops', 'Devops Jenkins', 'Aws Codedeploy', 'Aws Api Gateway', 'Vpc', 'Load Balancing', 'Route3', 'Code Pipeline', 'Auto Scaling', 'Pipeline', 'Elastic Cache', 'Aws Devops', 'Aws Lambda', 'Build', 'New Relic', 'Azure Devops', 'Cloudfront']",2025-06-12 14:33:23
Specialist Application Management- Kochi,StradaGlobal,8 - 13 years,Not Disclosed,['Kochi'],"Our story\nStrada is a technology-enabled, people powered company committed to delivering world-class payroll, human capital management, and financial management solutions to organizations globally. With a team of more than 8,000 experts and over 30 years of expertise, Strada blends leading-edge technology with human ingenuity to help businesses across the globe design and deliver at scale. Supporting over 1,400 customers in 33 countries, Strada partners with customers at every stage of their journey, to help drive their vision forward. Its why were so driven to connect passion with purpose. Our teams experience in human insights and cloud technology gives companies and employees around the world the ability to power confident decisions, for life.",,,,"['Application Support', 'Application Support Management', 'Application Management', 'Power Bi', 'Application Implementation', 'Cpq', 'Servicenow', 'Workday Financials', 'JIRA', 'Salesforce']",2025-06-12 14:33:25
Software Engineer Staff,Fourkites,9 - 14 years,30-45 Lacs P.A.,[],"At FourKites we have the opportunity to tackle complex challenges with real-world impacts. Whether its medical supplies from Cardinal Health or groceries for Walmart, the FourKites platform helps customers operate global supply chains that are efficient, agile and sustainable.\nJoin a team of curious problem solvers that celebrates differences, leads with empathy and values inclusivity.\nAs a Staff Software Engineer, you will get an opportunity to work on features end to end (backend & frontend) using the latest technologies such as RoR, Java, GoLang, Angular, React, Redis, PostgreSQL. You will develop products that can change the logistics landscape and will be used by some of the biggest corporations in the world. You will develop integrations with our strategic partners to help expand our ecosystem. You will work closely with our US team and customers to develop features that help shape the logistics and supply chain industry.\nWho you are:\nBachelorâ€™s degree in Computer Science & Engineering or related field from a reputed institution.\nMinimum of 8 years of experience in Software Engineering and Web application development.\nGood understanding of software design, Microservices architecture, object-oriented principles, and design patterns.\nExperience with Design and development of highest quality software/services using RoR/Golang/Java.\nGood knowledge of RESTful APIs and microservices architecture\nStrong understanding of Java, Spring Framework, and object-oriented programming principles\nExperience in one of Azure, Amazon Web Services or other cloud services.\nExperience with databases such as MySQL, PostgreSQL, or MongoDB\nFamiliarity with front-end technologies such as HTML, CSS, and JavaScript is a plus\nStrong knowledge of Git (branches, submodules, rebasing) and other Agile tools such as JIRA & Confluence.\nAgile SDLC experience\nExcellent oral and written communication skills\nWhat youâ€™ll be doing:\nDesign, architect, implement, test, profile, release, and optimize highest quality software/services using RoR/Golang/Java.\nPartner with product manages to analyse product requirements and plan engineering execution\nDocument HLD/LLD for easy knowledge sharing and future scaling\nPerform design and code reviews\nImplement code with very high coverage of unit tests and component tests\nCross-training peers and mentoring teammates\nPossess expert knowledge in performance, security, scalability, architecture, and best practices\nFunctionally decompose complex problems into simple, straight-forward solutions\nCollaborate with UX designers to develop responsive user interface components\nWorking knowledge of SQL based (any RDBMS) and NOSQL data stores (any one) with the ability to write intermediate level SQL\nExperience in building Web application backends using Java Spring Boot or similar\nExperience with frontend libraries/frameworks such as React/Angular is a plus.\nEducation Qualification: Graduate from B.E/ B.Tech / MCA / M.Tech Background.\nWho we are:\nFourKitesÂ®, the leader in AI-driven supply chain transformation for global enterprises and pioneer of real-time visibility, turns supply chain data into automated action. FourKitesâ€™ Intelligent Control Towerâ„¢ breaks down enterprise silos by creating a real-time digital twin of orders, shipments, inventory and assets. This comprehensive view, combined with AI-powered digital workers, enables companies to prevent disruptions, automate routine tasks, and optimize performance across As the leader in AI-driven supply chain transformation, FourKites pioneered the Intelligent Control Towerâ„¢ powered by the worldâ€™s largest real-time visibility network. Our platform creates comprehensive digital twins of your supply chain with AI-powered digital workers to automate resolution, improve collaboration and drive outcomes across all stakeholders. Unlike traditional control towers, we enable true real-time execution and intelligent fulfillment, transforming both your supply and customer\nBenefits\nMedical benefits start on first day of employment\n36 PTO days( Sick, Casual and Earned) , 5 recharge days, 2 volunteer days\nHome Office setups and Technology reimbursement\nLifestyle & Family benefits\nAnnual Swags/ Festive Swags\nOngoing learning & development opportunities ( Professional development program, Toast Master club etc.)",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Golang', 'Casandra', 'SQL Database', 'Ci/Cd', 'Elastic Search', 'Python']",2025-06-12 14:33:27
Snowflake Expert,Tekskills India,6 - 11 years,Not Disclosed,['Hyderabad'],"Location: Hyderabad (Preferred) / Bangalore (If strong candidates are available)\nType: Contractual\nKey Responsibilities:\nUse data mappings and models provided by the data modeling team to build robust Snowflake data pipelines.\nDesign and implement pipelines adhering to 2NF/3NF normalization standards.\nDevelop and maintain ETL processes for integrating data from multiple ERP and source systems.\nBuild scalable and secure Snowflake data architecture supporting Data Quality (DQ) needs.\nRaise CAB requests via Carriers change process and manage production deployments.\nProvide UAT support and ensure smooth transition of finalized pipelines to support teams.\nCreate and maintain comprehensive technical documentation for traceability and handover.\nCollaborate with data modelers, business stakeholders, and governance teams to enable DQ integration.\nOptimize complex SQL queries, perform performance tuning, and ensure data ops best practices.\nRequirements:\nStrong hands-on experience with Snowflake\nExpert-level SQL skills and deep understanding of data transformation\nSolid grasp of data architecture and 2NF/3NF normalization techniques\nExperience with cloud-based data platforms and modern data pipeline design\nExposure to AWS data services like S3, Glue, Lambda, Step Functions (preferred)\nProficiency with ETL tools and working in Agile environments\nFamiliarity with Carrier CAB process or similar structured deployment frameworks\nProven ability to debug complex pipeline issues and enhance pipeline scalability\nStrong communication and collaboration skills",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'ETL', 'aws services', 'SQL']",2025-06-12 14:33:29
Hiring SCM Functional Lead - US Shift - Ahmedabad/Pune/Remote,Synoptek,7 - 12 years,Not Disclosed,[],"Lead Functional Consultant, SCM & Manufacturing\nThis is an amazing opportunity to work within one of the fastest growing Managed Services Providers. We are a company with a heart and soul dedicated to the ongoing success and growth of our employees and continued business success of the customers we support. We foster a fun and connected environment with employee benefits extending beyond general compensation and into company sponsored events and an invested culture of learning.\nThe Lead Functional Consultant, SCM & Manufacturing, is responsible for leading the end-to-end project lifecycle, from project conception to successful implementation, specifically focusing on Supply Chain Management (SCM) and Manufacturing solutions. This role requires a deep understanding of Microsoft Dynamics AX/ F&SCM, strong leadership skills, and the ability to coordinate and manage both functional teams and project scope effectively.",,,,"['Microsoft Dynamics', 'SCM', 'dynamics', 'Supply Chain Management']",2025-06-12 14:33:32
Sap MM Consultant,Trident Group,4 - 9 years,18-25 Lacs P.A.,"['Barnala', 'Budhni']","Job Title: SAP MM Consultant\nJob Location: Punjab, Madhya Pradesh\nJob Type: Full-Time\nJob Summary: We are looking for an experienced SAP MM Consultant to join our team. The ideal candidate will have a deep understanding of the SAP Material Management module, along with strong analytical skills and the ability to translate business requirements into effective ERP solutions. Your role will involve configuring and implementing the SAP MM module, providing user support, and ensuring seamless integration with other SAP modules.",,,,"['SAP MM Configuration', 'SAP MM Module', 'Problem Solving Skills', 'SAP MM Implementation']",2025-06-12 14:33:34
Salesforce Developer,Varun Info Tech Consulting,2 - 4 years,3-6 Lacs P.A.,['Hyderabad( Madhapur )'],"Roles and Responsibilities\nDesign, develop, test, deploy, and maintain Salesforce solutions to meet business requirements.\nCollaborate with cross-functional teams to gather requirements and deliver high-quality solutions.\nTroubleshoot issues related to Salesforce platform, including data migration, integration, and performance optimization.\nDevelop custom components using Apex programming language and LWC (Lightning Web Components).\nEnsure adherence to coding standards, best practices, and company policies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Salesforce Sales Cloud', 'Salesforce Service Cloud', 'Salesforce Certification', 'Sales Force Development', 'Salesforce Integration']",2025-06-12 14:33:37
SAP Implementation Engineer,Emkay Placement Consultants,10 - 12 years,10-13 Lacs P.A.,['Kolkata'],"Lead SAP team for implementing SAP modules FICO, MM, SD, PM, PP, QM, PS .Understand business & technology ,digital platforms & drive new initiatives such as GRC, SAP Rise ,Minimize SAP run cost ,ROI analyses for SAP spending and initiatives,\n\nRequired Candidate profile\nimplementing SAP modules FICO, MM, SD, PM, PP, QM, PS .Understand modern business & technology framework, drive new initiatives such as GRC, ROI analyses for SAP spending and initiatives,",Industry Type: Building Material (Cement),Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Sap Data Migration', 'Sap Hana', 'SAP Support', 'Roi Analysis', 'SAP Implementation', 'SAP ECC', 'PP', 'PS', 'Time Management', 'SAP PP', 'Sap S Hana', 'FICO', 'SD', 'Execution', 'MM', 'ECC', 'SAP Security', 'Scheduling', 'ROI', 'Schedule', 'SAP Basis', 'GRC', 'Digitization', 'SAP Production Planning', 'Planning', 'Qm', 'SAP Quality Management']",2025-06-12 14:33:39
Salesforce Developer,Leading Client,5 - 10 years,Not Disclosed,['Hyderabad'],"Name of Projects in which the skills were used (add rows if necessary)\nNo: of months worked in each Project of work done using the skill (Mandatory/ optional)\nApex Customization - M\nSalesforce Configuration - M\nLWC - M\nJavascript/Jquery - M\nSlack - O\nMulesoft - O\nDesign, develop, and deploy customized Salesforce solutions using Apex, Visualforce, and Lightning components Collaborate with stakeholders to gather requirements and translate them into technical designs Create and maintain workflows, process builders, triggers, and validation rules Perform data migration using Data Loader or third-party tools Integrate Salesforce with external systems via REST/SOAP APIs Ensure unit testing, deployment through CI/CD tools, and post-deployment support Maintain platform security and ensure adherence to Salesforce best practices",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Salesforce', 'Lightning Web Components', 'Slack', 'JavaScript', 'Jquery', 'Salesforce Lightning', 'Mulesoft']",2025-06-12 14:33:41
D365 F&O Finance Functional Consultant,Claranet Group,5 - 8 years,Not Disclosed,[],"We at Claranet are looking for F&O Finance Functional Professionals who will be involved in all aspects of implementing supporting Dynamics solutions through the project life cycle to go live and ongoing support. You can contribute to Solution Design sessions, help with configuration, help with data migration deliverables, create required interface design and functional design documents, troubleshoot customization, etc.\n\nRole- D365 F&O Functional Consultant (Full Time)\nOffice Location Hitech, Hyderabad - India\nWorking mode : Remote\n\nInterested Candidates can your CV on Rakesh.koyya@claranet.com\n\nKey Responsibilities:\nAnalyze business processes (Finance, PMA) to identify opportunities for improvement.\nIdentify creative workarounds to meet requirements without the development of custom code.\nUnderstand the functional capabilities and limitations for out-of-the-box functionality as well as custom code.\nYou will be the one to identify our customers' requirements and match them with technological capabilities and with Microsoft continuous release plans.\n\nKey Competencies:\nDeep functional knowledge of Microsoft Dynamics 365 Finance and PMA.\nExperience of customized solutions to complex business problems.\nDemonstrable consultancy experience.\nStrong working knowledge of business processes and ER framework.\nRelevant Microsoft certification.\nExcellent documentation and communication skills.\nA logical approach to problem-solving and the structured introduction of change into operational systems.\nAbility to multitask and prioritize.\nGood interpersonal skills.\nAttention to detail.\n\nSkills Required:\nHold 6-8 years of experience within D365 F&O implementation, support\nSpecialized in Finances, PMA, Integration, ER reports\nQualified Chartered Accountant / MBA (Finance) desirable\nAre fluent in English.\nStrong communication and consulting skills\nGlobal Exposure must (Interacting with Global clients/customers)\n\nCompany Benefits:\nGroup Medical Insurance (including parents coverage), Group Term Life Policy and Personal Accidental Policy\nTax Saving flexible benefits\nFlexible working hours\nStatutory Benefits (PF, Gratuity)",Industry Type: Miscellaneous,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['PMA', 'Microsoft Dynamics', 'F&O', 'finance', 'Integration', 'configuration', 'Migration', 'Customization', 'implementation', 'Troubleshooting', 'ER', 'Data Migration', 'functional consultant']",2025-06-12 14:33:43
SAP SD Consultant,Trident Group,4 - 9 years,18-25 Lacs P.A.,"['Barnala', 'Budhni']","Job Title: SAP SD Consultant\nJob Location: Punjab, Madhya Pradesh\nJob Type: Full-Time\nJob Summary: We are seeking a dedicated SAP SD (Sales and Distribution) Consultant to implement and support our SAP software solutions. The ideal candidate will have a strong understanding of the SAP SD module, along with excellent analytical skills and the ability to translate business requirements into effective ERP solutions. Your role will involve configuring and implementing the SAP SD module, providing user support, and ensuring seamless integration with other SAP modules.",,,,"['SAP MM Configuration', 'SAP MM Module', 'SAP MM Implementation', 'Material Management', 'Problem Solving Skills']",2025-06-12 14:33:45
Netsuite Developer,Advy Chemical,2 - 4 years,Not Disclosed,['Thane'],"Resolve day-to-day NetSuite issues and user queries efficiently.\nDevelop and maintain NetSuite customizations using SuiteScript (1.0 / 2.0 / 2.1) including Client Scripts, User Events, Suitelets, RESTlets, Scheduled Scripts, and Map/Reduce.\nDesign and configure SuiteFlow workflows, Saved Searches, dashboards, and reports using SuiteAnalytics.\nCustomize forms, fields, roles, permissions, and advanced PDF templates through SuiteBuilder.\nIntegrate NetSuite with external systems using APIs (REST/SOAP) or middleware tools.\nPerform data migrations, sandbox-to-production deployments, and manage post-Go-Live support and improvements.\nMaintain technical documentation, apply version control, and ensure structured deployment processes.\nCollaborate with business stakeholders to translate functional requirements into scalable NetSuite solutions.",Industry Type: Pharmaceutical & Life Sciences,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['ERP System', 'Netsuite Erp', 'Suitescript', 'Suite Builder', 'Javascript', 'Accounting Software', 'Suite Bundler', 'Suiteflow']",2025-06-12 14:33:47
Dot Net Fullstack Developer,BriskWin IT (BWIT),6 - 11 years,Not Disclosed,"['Pune', 'Chennai', 'Mumbai (All Areas)']","Req 1. Microsoft Cloud .Net + Angular + React (Basics)\nExp: 6 to 12 Yrs\nNP : Immediate to 15 Days (Serving Notice Max 20 Days)\nLocation Chennai / Mumbai / Pune / Bangalore\n\nReq 2. API Technical Lead Azure Cloud Integration\nExp: 9+ yrs\nNP : Immediate to 30 Days\nLocation: Chennai, Mumbai, Pune, Noida, Ahmadabad, Bangalore\n\nKey Skills & Experience:\n\nProven experience in a technical leadership role focused on backed/API development, with strong proficiency in Java.\nIn-depth knowledge of Microsoft Azure services, including:\nAzure Table Storage\nAzure Data Lake Storage Gen2\nAzure Functions\nAzCopy for efficient data migration to Azure( Good to have )\nStrong grasp of Restful API design principles, cloud-native architectures, and server less computing models",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['.Net Core', 'Cloud', 'React.Js', 'Angular']",2025-06-12 14:33:50
Sap Abap Technical Consultant,Varun Info Tech Consulting,2 - 3 years,6-9 Lacs P.A.,['Hyderabad( Madhapur )'],"Roles and Responsibilities\nIntegrate SAP with Salesforce using OData Rest APIs.\nDesign, develop, test, and deploy SAP ABAP applications using OOPS concepts.\nExpert knowledge in LSMW and LTMC for data migration.\n\nImplement & Support Adobe forms.\n\nFamiliarity with Fiori App developments and enhancements.\nDesired Candidate Profile\n2 to 3 years of experience in SAP ABAP development, Preferred on HANA platform.\nBachelor's degree in Computers or Information Technology (B. Tech/B.E.).\nStrong understanding of Object-Oriented Programming (OOP) principles and design patterns.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Odata', 'LSMW', 'Sap Abap Hana', 'Sap Fiori', 'OO ABAP', 'Adobe Forms', 'Cds Views', 'LTMC']",2025-06-12 14:33:52
Data Architect,Accenture,15 - 20 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Architect\n\n\n\n\n\nProject Role Description :Define the data requirements and structure for the application. Model and design the application data structure, storage and integration.\n\n\n\nMust have skills :Data Architecture Principles\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n18 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Architect, you will define the data requirements and structure for the application. Your typical day involves modeling and designing the application data structure, storage, and integration, ensuring that the data architecture aligns with business objectives and supports efficient data management practices. You will collaborate with various stakeholders to gather requirements and translate them into effective data solutions, while also addressing any challenges that arise in the data architecture process.\nRoles & Responsibilities:- Expected to be a Subject Matter Expert with deep knowledge and experience.- Should have influencing and advisory skills.- Engage with multiple teams and responsible for team decisions.- Expected to provide solutions to problems that apply across multiple teams, and provide solutions to business area problems.- Facilitate workshops and discussions to gather data requirements and ensure alignment with business goals.- Develop and maintain documentation related to data architecture, including data models and integration processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Architecture Principles.- Strong understanding of data modeling techniques and best practices.- Experience with data integration tools and methodologies.- Knowledge of database management systems and data storage solutions.- Familiarity with data governance and data quality frameworks.\nAdditional Information:- The candidate should have minimum 18 years of experience in Data Architecture Principles.- This position is based at our Mumbai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data architecture', 'database management', 'data architecture principles', 'data modeling', 'data integration', 'python', 'data management', 'data warehousing', 'power bi', 'sql server', 'sql', 'plsql', 'data quality', 'tableau', 'dwbi', 'data governance', 'etl', 'ssis', 'informatica']",2025-06-13 05:41:39
Data Architect,Diverse Lynx,8 - 13 years,Not Disclosed,['Bengaluru'],"Data Architect-\nTotal Yrs. of Experience* 15+ Relevant Yrs. of experience* 8+ Detailed JD *(Roles and Responsibilities)\nLeadership qualities and ability to lead a team of 8 data engineers + PowerBI resources\nShould be able to engage with business users and IT to provide consultation on data and visualization needs\nExcellent communication, articulation, and presentation skills\nExposure to data architecture, ETL architecture\nDesign, develop, and maintain scalable data pipelines using Python, ADF, and Databricks\nImplement ETL process to extract, transform, and load data from various sources into Snowflake\nEnsure data is processed efficiently and is made available for analytics and reporting\n8+ years of experience in data engineering, with a focus on Python, ADF, Snowflake, Databricks, and ETL processes.\nProficiency in SQL and experience with cloud-based data storage and processing.\nStrong problem-solving skills and the ability to work in a fast-paced environment\nExperience with Agile methodologies and working in a collaborative team environment.\nCertification in Snowflake, Azure, or other relevant technologies is an added advantage\nBachelors degree in computer science engineering, Information Systems or equivalent field\nMandatory skills* Python, Snowflake, Azure Data Factory, Databricks, SQL Desired skills* 1. Strong Oral and written communication\n2. Proactive and accountable of the deliverables quality and timely submission Domain* Retail Work Location* India\nLocation- PAN India\nYrs of Exp-15+Yrs",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Retail', 'Focus', 'Data Architect', 'Cloud', 'Agile', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-13 05:41:41
Data Architect,Clifyx Technology,12 - 13 years,Not Disclosed,['Bengaluru'],"ECMS #\n528867\nNumber of Openings\n1\nDuration of project\n6 months Initially\nNo of years experience\n12 Years\nDetailed job description - Skill Set:\nScroll down for the JD\nMandatory Skills\nDatabricks, Python, Pyspark, Architect\nVendor Billing range (local currency /Day)\n10000 INR/DAY\nWork Location\nBANGALORE/PUNE\nHybrid/Remote/WFO\nHybrid\nBGV Pre/Post onboarding\nPre on-boarding.\nAny particular shift timings\nGeneral shift whereas need to extend couple of hours if required.\nRole Data Architect\n\nIn the role of Data Architect, you will interface with key stakeholders and apply your knowledge for understanding the business and business data across source systems. You will play an important role in creating a detailed business data understanding, outlining problems, opportunities, and data solutions for a business.\nBasic\nBachelors degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education.\nAt least 12+ years of experience with Information Technology and 5 + years in Data Architect\nExtensive experience in Design and Architecture of large data transformation systems.\nPreferred\nUnderstanding of the business area that the project is involved with.\nWorking with data stewards to understand the data sources.\nClear understanding of data entities, relationships, cardinality etc for the inbound sources based on inputs from the data stewards / source system experts.\nPerformance tuning understanding the overall requirement, reporting impact.\nData Modeling for the business and reporting models as per the reporting needs or delivery needs to other downstream systems.\nHave experience to components and languages like Databricks, Python, PySpark, SCALA, R.\nAbility to ask strong questions to help the team see areas that may lead to problems.\nAbility to validate the data by writing sql queries and compare against the source system and transformation mapping.\nWork closely with teams to collect and translate information requirements into data to develop data-centric solutions.\nEnsure that industry-accepted data architecture principles and standards are integrated and followed for modeling, stored procedures, replication, regulations, and security, among other concepts, to meet technical and business goals.\nContinuously improve the quality, consistency, accessibility, and security of our data activity across company needs.\nExperience on Azure DevOps project tracking tool or equivalent tools like JIRA.\nShould have Outstanding verbal, non-verbal communication.\nShould have experience and desire to work in a Global delivery environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Data Architect', 'Billing', 'Stored procedures', 'JIRA', 'Information technology', 'Downstream', 'Python', 'Data architecture']",2025-06-13 05:41:42
Data Architect,Calibo,12 - 16 years,Not Disclosed,[],"About the Role:\n\nWe are looking for a highly skilled Data Engineering Architect with strong Data Engineering pipeline implementation experience to serve as the lead Solution/Technical Architect and Subject Matter Expert for customer experience data solutions across multiple data sources. The ideal candidate will collaborate with the Enterprise Architect and the client IT team to establish and implement strategic initiatives.\n\nResponsibilities and Technical Skills:\n12+ years of relevant experience in designing and Architecting ETL, ELT, Reverse ETL, Data Management or Data Integration, Data Warehouse, Data Lake, and Data Migration.\nMust have expertise in building complex ETL pipelines and large Data Processing, Data Quality and Data security\nExperience in delivering quality work on time with multiple, competing priorities.\nExcellent troubleshooting and problem-solving skills must be able to consistently identify critical elements, variables and alternatives to develop solutions.\nExperience in identifying, analyzing and translating business requirements into conceptual, logical and physical data models in complex, multi-application environments.\nExperience with Agile and Scaled Agile Frameworks.\nExperience in identifying and documenting data integration issues, and challenges such as duplicate data, non-conformed data, and unclean data. Multiple platform development experience.\nStrong experience in performance tuning of ETL processes using Data Platforms\nMust have experience in handling Data formats like Delta Tables, Parquet files, Iceberg etc.\nExperience in Cloud technologies such as AWS/Azure or Google Cloud.\nApache Spark design and development experience using Scala, Java, Python or Data Frames with Resilient Distributed Datasets (RDDs).\nDevelopment experience in databases like Oracle, AWS Redshift, AWS RDS, Postgres Databricks and/or Snowflake.\nHands-on professional work experience with Python is highly desired.\nExperience in Hadoop ecosystem tools for real-time or batch data ingestion.\nStrong communication and teamwork skills to interface with development team members, business analysts, and project management. Excellent analytical skills.\nIdentification of data sources, internal and external, and defining a plan for data management as per business data strategy.\nCollaborating with cross-functional teams for the smooth functioning of the enterprise data system.\nManaging end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nPlanning and execution of big data solutions using Databricks, Big Data, Hadoop, Big Query, Snowflake, MongoDB, DynamoDB, PostgreSQL and SQL Server\nHands-on experience in defining and implementing various Machine Learning models for different business needs.\nIntegrating technical functionality, ensuring data accessibility, accuracy, and security.\nProgramming / Scripting Languages like Python / Java / Go, Microservices\nMachine Learning / AI tools like Scikit-learn / TensorFlow / PyTorch",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud', 'ETL', 'AWS', 'Data Handling', 'Spark']",2025-06-13 05:41:44
Data Architect,Exavalu,7 - 12 years,Not Disclosed,[],"We are seeking an experienced Azure Data Architect to lead the design, implementation, and optimization of scalable, secure, and cost-effective cloud data solutions on Microsoft Azure. The ideal candidate will have deep expertise in data architecture, cloud computing, and modern data platforms, with a proven ability to align technology strategies with business goals.\nKey Responsibilities:\nDesign and implement modern data platform solutions using Azure services including Azure Data Lake, Azure Synapse Analytics, Azure Data Factory, and Azure Databricks.\nDefine data architecture frameworks, standards, and principles.\nLead data modernization and cloud migration initiatives.\nBuild secure and scalable data pipelines to ingest, transform, and store large datasets.\nCollaborate with data engineers, data scientists, and business stakeholders to understand data requirements and provide architectural guidance.\nOptimize performance, security, and cost of data platforms.\nEvaluate and recommend new Azure services and tools based on evolving project needs.\nEnsure data governance, data quality, and compliance standards are met.\nRequirements Required Qualifications:\nBachelors or Masters degree in Computer Science, Information Systems, or a related field.\n7+ years of experience in data architecture or data engineering roles.\n3+ years of hands-on experience with Azure data services.\nStrong knowledge of relational and non-relational databases (SQL Server, Cosmos DB, etc).\nProficiency with data modeling, data warehousing, and ETL processes.\nFamiliarity with big data and analytics tools like Databricks, Spark, and Synapse.\nExperience with scripting and programming languages (Python, SQL, PowerShell).\nExcellent understanding of security, identity, and governance in Azure.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Cloud computing', 'Architecture', 'Data modeling', 'Data quality', 'Cosmos', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-13 05:41:46
Data Architect,Armakuni,8 - 12 years,Not Disclosed,['Ahmedabad'],"DataArchitecture Design: Develop and maintain a comprehensive data architecture strategy that aligns with the business objectives and technology landscape.\nDataModeling:Createand managelogical, physical, and conceptual data models to support various business applications and analytics. DatabaseDesign: Design and implement database solutions, including data warehouses, data lakes, and operational databases.\nDataIntegration: Oversee the integration of data from disparate sources into unified, accessible systems using ETL/ELT processes. DataGovernance:Implementand enforce data governance policies and procedures to ensure data quality, consistency, and security.\nTechnologyEvaluation: Evaluate and recommend data management tools, technologies, and best practices to improve data infrastructure and processes.\nCollaboration: Work closely with data engineers, data scientists, business analysts, and other stakeholders to understand data requirements and deliver effective solutions. Trusted by the world s leading brands\nDocumentation:Createand maintain documentation related to data architecture, data flows, data dictionaries, and system interfaces. PerformanceTuning: Optimize database performance through tuning, indexing, and query optimization.\nSecurity: Ensure data security and privacy by implementing best practices for data encryption, access controls, and compliance with relevant regulations (e.g., GDPR, CCPA)\n\nRequirements:\nHelpingproject teams withsolutions architecture,troubleshooting, and technical implementation assistance.\nExperiencewithbig data technologies (e.g., Hadoop, Spark, Kafka, Airflow).\nExpertisewithcloud platforms (e.g., AWS, Azure, Google Cloud) and their data services.\nKnowledgeofdataintegration tools (e.g., Informatica, Talend, FiveTran, Meltano).\nUnderstandingofdatawarehousing concepts and tools (e.g., Snowflake, Redshift, Synapse, BigQuery). Experiencewithdata governanceframeworks and tools.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['query optimization', 'Data management', 'data security', 'Postgresql', 'MySQL', 'Data quality', 'Informatica', 'Troubleshooting', 'Analytics', 'Data architecture']",2025-06-13 05:41:47
Data Architect,Orangepeople,6 - 12 years,Not Disclosed,['Chennai'],"Are you a visionary who thrives on designing future-ready data ecosystems? Let s build the next big thing together! Were working with top retail and healthcare leaders to transform how they harness data and we re looking for a Data Architect to guide that journey.\n\n\n\n\nWe are looking for an experienced Data Architect with deep knowledge of Databricks and cloud-native data architecture. This role will drive the design and implementation of scalable, high-performance data platforms to support advanced analytics, business intelligence, and data science initiatives within a retail or healthcare environment.\n\n\n\n\n\n\nKey Responsibilities:\n\n\n\nDefine and implement enterprise-level data architecture strategies using Databricks.\n\nDesign end-to-end data ecosystems including ingestion, transformation, storage, and access layers.\n\nLead data governance, data quality, and security initiatives across the organization.\n\nWork with stakeholders to align data architecture with business goals and compliance requirements.\n\nGuide the engineering team on best practices in data modeling, pipeline development, and system optimization.\n\nChampion the use of Delta Lake, Lakehouse architecture, and real-time analytics.\n\n\n\n\n\nRequired Qualifications:\n\n\n\n8+ years of experience in data architecture or solution architecture roles.\n\nStrong expertise in Databricks, Spark, Delta Lake, and data warehousing concepts.\n\nSolid understanding of modern data platform tools (Snowflake, Azure Synapse, BigQuery, etc.).\n\nExperience with cloud architecture (Azure preferred), data governance, and MDM.\n\nStrong understanding of healthcare or retail data workflows and regulatory requirements.\n\nExcellent communication and stakeholder management skills.\n\n\n\n\n\n\nBenefits:\n\n\n\nHealth Insurance, Accident Insurance.\n\nThe salary will be determined based on several factors including, but not limited to, location, relevant education, qualifications, experience, technical skills, and business needs.\n\n\n\n\n\n\nAdditional Responsibilities:\n\n\n\nParticipate in OP monthly team meetings, and participate in team-building efforts.\n\nContribute to OP technical discussions, peer reviews, etc.\n\nContribute content and collaborate via the OP-Wiki/Knowledge Base.\n\nProvide status reports to OP Account Management as requested.\n\n\n\n\n\nAbout us:\n\n\nOP is a technology consulting and solutions company, offering advisory and managed services, innovative platforms, and staffing solutions across a wide range of fields including AI, cyber security, enterprise architecture, and beyond. Our most valuable asset is our people: dynamic, creative thinkers, who are passionate about doing quality work. As a member of the OP team, you will have access to industry-leading consulting practices, strategies & and technologies, innovative training & education. An ideal OP team member is a technology leader with a proven track record of technical excellence and a strong focus on process and methodology.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'Managed services', 'Data modeling', 'Enterprise architecture', 'Healthcare', 'Data quality', 'Account management', 'Business intelligence', 'Analytics', 'Data architecture']",2025-06-13 05:41:49
Data Architect - AWS,Qentelli,4 - 8 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are looking for a highly skilled AWS Data Architect to design and implement scalable, secure, and high-performing data architecture solutions on AWS. The ideal candidate will have hands-on experience in building data lakes, data warehouses, and data pipelines, along with a solid understanding of data governance and cloud security best practices.\n  Roles and Responsibilities:\nDesign and implement data architecture solutions on AWS using services such as S3, Redshift, Glue, Lake Formation, Athena, and Lambda.\nDevelop scalable ETL/ELT workflows and data pipelines using AWS Glue, Apache Spark, or AWS Data Pipeline.\nDefine and implement data governance, security, and compliance strategies, including IAM policies, encryption, and data cataloging.\nCreate and manage data lakes and data warehouses that are scalable, cost-effective, and secure.\nCollaborate with data engineers, analysts, and business stakeholders to develop robust data models and reporting solutions.\nEvaluate and recommend tools, technologies, and best practices to optimize data architecture and ensure high-quality solutions.\nEnsure data quality, performance tuning, and optimization for large-scale data storage and processing\n  Required Skills and Qualifications:\nProven experience in AWS data services such as S3, Redshift, Glue, etc.\nStrong knowledge of data modeling, data warehousing, and big data architecture.\nHands-on experience with ETL/ELT tools and data pipeline frameworks.\nGood understanding of data security and compliance in cloud environments.\nExcellent problem-solving skills and ability to work collaboratively with cross-functional teams.\nStrong verbal and written communication skills.\n  Preferred Skills:\nAWS Certified Data Analytics â€“ Specialty or AWS Solutions Architect Certification.\nExperience in performance tuning and optimizing large datasets.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud security', 'performance tuning', 'glue', 'amazon redshift', 'big data technologies', 'verbal communication', 'data warehousing', 'data architecture', 'aws glue', 's', 'data modeling', 'lambda expressions', 'spark', 'written communication', 'data governance', 'athena', 'aws', 'etl', 'communication skills']",2025-06-13 05:41:51
Data Architect Telecom Domain databrick BSS OSS,fast growing Data Driven IT solutions an...,10 - 20 years,45-55 Lacs P.A.,"['Noida', 'Hyderabad', 'Gurugram']","Data Architect Telecom Domain\nTo design comprehensive data architecture and technical solutions specifically for telecommunications industry challenges, leveraging TMforum frameworks and modern data platforms. To work closely with customers, and technology partners to deliver data solutions that address complex telecommunications business requirements including customer experience management, network optimization, revenue assurance, and digital transformation initiatives.\nResponsibilities:\nDesign and articulate enterprise-scale telecom data architectures incorporating TMforum standards and frameworks, including SID (Shared Information/Data Model), TAM (Telecom Application Map), and eTOM (enhanced Telecom Operations Map)\nDevelop comprehensive data models aligned with TMforum guidelines for telecommunications domains such as Customer, Product, Service, Resource, and Partner management\nCreate data architectures that support telecom-specific use cases including customer journey analytics, network performance optimization, fraud detection, and revenue assurance\nDesign solutions leveraging Microsoft Azure and Databricks for telecom data processing and analytics\nConduct technical discovery sessions with telecom clients to understand their OSS/BSS architecture, network analytics needs, customer experience requirements, and digital transformation objectives\nDesign and deliver proof of concepts (POCs) and technical demonstrations showcasing modern data platforms solving real-world telecommunications challenges\nCreate comprehensive architectural diagrams and implementation roadmaps for telecom data ecosystems spanning cloud, on-premises, and hybrid environments\nEvaluate and recommend appropriate big data technologies, cloud platforms, and processing frameworks based on telecom-specific requirements and regulatory compliance needs.\nDesign data governance frameworks compliant with telecom industry standards and regulatory requirements (GDPR, data localization, etc.)\nStay current with the latest advancements in data technologies including cloud services, data processing frameworks, and AI/ML capabilities\nContribute to the development of best practices, reference architectures, and reusable solution components for accelerating proposal development\nQualifications:\nBachelor's or Master's degree in Computer Science, Telecommunications Engineering, Data Science, or a related technical field\n10+ years of experience in data architecture, data engineering, or solution architecture roles with at least 5 years in telecommunications industry\nDeep knowledge of TMforum frameworks including SID (Shared Information/Data Model), eTOM, TAM, and their practical implementation in telecom data architectures\nDemonstrated ability to estimate project efforts, resource requirements, and implementation timelines for complex telecom data initiatives\nHands-on experience building data models and platforms aligned with TMforum standards and telecommunications business processes\nStrong understanding of telecom OSS/BSS systems, network management, customer experience management, and revenue management domains\nHands-on experience with data platforms including Databricks, and Microsoft Azure in telecommunications contexts\nExperience with modern data processing frameworks such as Apache Kafka, Spark and Airflow for real-time telecom data streaming\nProficiency in Azure cloud platform and its respective data services with an understanding of telecom-specific deployment requirements\nKnowledge of system monitoring and observability tools for telecommunications data infrastructure\nExperience implementing automated testing frameworks for telecom data platforms and pipelines\nFamiliarity with telecom data integration patterns, ETL/ELT processes, and data governance practices specific to telecommunications\nExperience designing and implementing data lakes, data warehouses, and machine learning pipelines for telecom use cases\nProficiency in programming languages commonly used in data processing (Python, Scala, SQL) with telecom domain applications\nUnderstanding of telecommunications regulatory requirements and data privacy compliance (GDPR, local data protection laws)\nExcellent communication and presentation skills with ability to explain complex technical concepts to telecom stakeholders\nStrong problem-solving skills and ability to think creatively to address telecommunications industry challenges\nGood to have TMforum certifications or telecommunications industry certifications\nRelevant data platform certifications such as Databricks, Azure Data Engineer are a plus\nWillingness to travel as required\nif you will all or most of the criteria contact bdm@intellisearchonline.net M 9341626895",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Telecom Bss', 'Data Architect', 'Telecom OSS', 'ETOM', 'Data Bricks']",2025-06-13 05:41:52
Senior Snowflake Data Architect,TechStar Group,9 - 14 years,15-30 Lacs P.A.,"['Pune', 'Chennai', 'Bengaluru']","JD:\nSenior Snowflake Data Architect:  \nDesign , implements, and optimizes data solutions within the Snowflake cloud data platform, ensuring data security, governance, and performance, while also collaborating with cross-functional teams and providing technical leadership.\nData architect include determining a data strategy.\nunderstanding data management technologies\noversee data inventory.\nmaintain a finger on the pulse of an organization's data management systems.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Management', 'Data Security', 'Data Governance']",2025-06-13 05:41:54
Data Platform Architect,Accenture,15 - 25 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Platform Architect\n\n\n\n\n\nProject Role Description :Architects the data platform blueprint and implements the design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Data Architecture Principles\n\n\n\n\nGood to have skills :Amazon Web Services (AWS), Teradata Vantage, Data GovernanceMinimum\n\n\n\n15 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Architect, you will architect the data platform blueprint and implement the design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Expected to be a SME with deep knowledge and experience.- Should have Influencing and Advisory skills.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead the design and implementation of the data platform architecture.- Collaborate with cross-functional teams to ensure data platform alignment with business objectives.- Provide technical guidance and mentorship to junior team members.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Architecture Principles.- Good To Have\n\n\n\n\nSkills:\nExperience with Amazon Web Services (AWS), Teradata Vantage, Data Governance.- Strong understanding of data architecture principles and best practices.- Experience in designing and implementing scalable data solutions.- Knowledge of cloud platforms and data governance frameworks.\nAdditional Information:- The candidate should have a minimum of 15 years of experience in Data Architecture Principles.- This position is based at our Pune office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['vantage', 'data architecture', 'data architecture principles', 'data governance', 'aws', 'mainframes', 'oracle', 'data warehousing', 'microsoft azure', 'dbms', 'sql server', 'sql', 'jcl', 'java', 'data modeling', 'cobol', 'gcp', 'platform architecture', 'hadoop', 'etl', 'big data', 'informatica', 'vsam']",2025-06-13 05:41:56
Azure Data Architect,Technip Energies,5 - 10 years,Not Disclosed,['Noida'],"Develop RESTful APIs using Azure APIM\nDevelop integration workflow using LogicApp, synpase and service bus.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation using Azure Data Factory and synapse pipelines.\nCollaborate closely with Product Owners to understand data pipeline requirements and design effective data workflows.\nTranslate business requirements into technical specifications for data pipelines.\nCreate and maintain data storage solutions using Azure Cosmos DB and Azure Data Lake Storage",,,,"['Service level', 'Talent acquisition', 'data security', 'Analytical', 'TPS', 'Data Architect', 'Agile', 'Workflow', 'Project delivery', 'Analytics']",2025-06-13 05:41:58
Enterprise Data Architect - Azure / ETL,Leading Client,10 - 12 years,Not Disclosed,['Bengaluru'],"Key Responsibilities :\n\nAs an Enterprise Data Architect, you will :\n\n- Lead Data Architecture : Design, develop, and implement comprehensive enterprise data architectures, primarily leveraging Azure and Snowflake platforms.\n\n- Data Transformation & ETL : Oversee and guide complex data transformation and ETL processes for large and diverse datasets, ensuring data integrity, quality, and performance.\n\n- Customer-Centric Data Design : Specialize in designing and optimizing customer-centric datasets from various sources, including CRM, Call Center, Marketing, Offline, and Point of Sale systems.\n\n- Data Modeling : Drive the creation and maintenance of advanced data models, including Relational, Dimensional, Columnar, and Big Data models, to support analytical and operational needs.\n\n- Query Optimization : Develop, optimize, and troubleshoot complex SQL and NoSQL queries to ensure efficient data retrieval and manipulation.\n\n- Data Warehouse Management : Apply advanced data warehousing concepts to build and manage high-performing, scalable data warehouse solutions.\n\n- Tool Evaluation & Implementation : Evaluate, recommend, and implement industry-leading ETL tools such as Informatica and Unifi, ensuring best practices are followed.\n\n- Business Requirements & Analysis : Lead efforts in business requirements definition and management, structured analysis, process design, and use case documentation to translate business needs into technical specifications.\n\n- Reporting & Analytics Support : Collaborate with reporting teams, providing architectural guidance and support for reporting technologies like Tableau and PowerBI.\n\n- Software Development Practices : Apply professional software development principles and best practices to data solution delivery.\n\n- Stakeholder Collaboration : Interface effectively with sales teams and directly engage with customers to understand their data challenges and lead them to successful outcomes.\n\n- Project Management & Multi-tasking : Demonstrate exceptional organizational skills, with the ability to manage and prioritize multiple simultaneous customer projects effectively.\n\n- Strategic Thinking & Leadership : Act as a self-managed, proactive, and customer-focused leader, driving innovation and continuous improvement in data architecture.\n\nPosition Requirements :\n\nof strong experience with data transformation & ETL on large data sets.\n\n- Experience with designing customer-centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale, etc.).\n\n- 5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data).\n\n- 5+ years of complex SQL or NoSQL experience.\n\n- Extensive experience in advanced Data Warehouse concepts.\n\n- Proven experience with industry ETL tools (i.e., Informatica, Unifi).\n\n- Solid experience with Business Requirements definition and management, structured analysis, process design, and use case documentation.\n\n- Experience with Reporting Technologies (i.e., Tableau, PowerBI).\n\n- Demonstrated experience in professional software development.\n\n- Exceptional organizational skills and ability to multi-task simultaneous different customer projects.\n\n- Strong verbal & written communication skills to interface with sales teams and lead customers to successful outcomes.\n\n- Must be self-managed, proactive, and customer-focused.\n\nTechnical Skills :\n\n- Cloud Platforms : Microsoft Azure\n\n- Data Warehousing : Snowflake\n\n- ETL Methodologies : Extensive experience in ETL processes and tools\n\n- Data Transformation : Large-scale data transformation\n\n- Data Modeling : Relational, Dimensional, Columnar, Big Data\n\n- Query Languages : Complex SQL, NoSQL\n\n- ETL Tools : Informatica, Unifi (or similar enterprise-grade tools)\n\n- Reporting & BI : Tableau, PowerBI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Enterprise Architect', 'Data Architect', 'Big Data', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'ETL', 'Reporting Tools', 'SQL']",2025-06-13 05:41:59
Enterprise Data Architect - Azure/ETL,Leading Client,10 - 12 years,Not Disclosed,['Delhi'],"Key Responsibilities :\n\nAs an Enterprise Data Architect, you will :\n\n- Lead Data Architecture : Design, develop, and implement comprehensive enterprise data architectures, primarily leveraging Azure and Snowflake platforms.\n\n- Data Transformation & ETL : Oversee and guide complex data transformation and ETL processes for large and diverse datasets, ensuring data integrity, quality, and performance.\n\n- Customer-Centric Data Design : Specialize in designing and optimizing customer-centric datasets from various sources, including CRM, Call Center, Marketing, Offline, and Point of Sale systems.\n\n- Data Modeling : Drive the creation and maintenance of advanced data models, including Relational, Dimensional, Columnar, and Big Data models, to support analytical and operational needs.\n\n- Query Optimization : Develop, optimize, and troubleshoot complex SQL and NoSQL queries to ensure efficient data retrieval and manipulation.\n\n- Data Warehouse Management : Apply advanced data warehousing concepts to build and manage high-performing, scalable data warehouse solutions.\n\n- Tool Evaluation & Implementation : Evaluate, recommend, and implement industry-leading ETL tools such as Informatica and Unifi, ensuring best practices are followed.\n\n- Business Requirements & Analysis : Lead efforts in business requirements definition and management, structured analysis, process design, and use case documentation to translate business needs into technical specifications.\n\n- Reporting & Analytics Support : Collaborate with reporting teams, providing architectural guidance and support for reporting technologies like Tableau and PowerBI.\n\n- Software Development Practices : Apply professional software development principles and best practices to data solution delivery.\n\n- Stakeholder Collaboration : Interface effectively with sales teams and directly engage with customers to understand their data challenges and lead them to successful outcomes.\n\n- Project Management & Multi-tasking : Demonstrate exceptional organizational skills, with the ability to manage and prioritize multiple simultaneous customer projects effectively.\n\n- Strategic Thinking & Leadership : Act as a self-managed, proactive, and customer-focused leader, driving innovation and continuous improvement in data architecture.\n\nPosition Requirements :\n\nof strong experience with data transformation & ETL on large data sets.\n\n- Experience with designing customer-centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale, etc.).\n\n- 5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data).\n\n- 5+ years of complex SQL or NoSQL experience.\n\n- Extensive experience in advanced Data Warehouse concepts.\n\n- Proven experience with industry ETL tools (i.e., Informatica, Unifi).\n\n- Solid experience with Business Requirements definition and management, structured analysis, process design, and use case documentation.\n\n- Experience with Reporting Technologies (i.e., Tableau, PowerBI).\n\n- Demonstrated experience in professional software development.\n\n- Exceptional organizational skills and ability to multi-task simultaneous different customer projects.\n\n- Strong verbal & written communication skills to interface with sales teams and lead customers to successful outcomes.\n\n- Must be self-managed, proactive, and customer-focused.\n\nTechnical Skills :\n\n- Cloud Platforms : Microsoft Azure\n\n- Data Warehousing : Snowflake\n\n- ETL Methodologies : Extensive experience in ETL processes and tools\n\n- Data Transformation : Large-scale data transformation\n\n- Data Modeling : Relational, Dimensional, Columnar, Big Data\n\n- Query Languages : Complex SQL, NoSQL\n\n- ETL Tools : Informatica, Unifi (or similar enterprise-grade tools)\n\n- Reporting & BI : Tableau, PowerBI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Architect', 'Azure', 'Enterprise Architect', 'Big Data', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'ETL', 'Reporting Tools', 'SQL']",2025-06-13 05:42:01
Enterprise Data Architect - Azure/ETL,Leading Client,10 - 12 years,Not Disclosed,['Nagpur'],"Key Responsibilities :\n\nAs an Enterprise Data Architect, you will :\n\n- Lead Data Architecture : Design, develop, and implement comprehensive enterprise data architectures, primarily leveraging Azure and Snowflake platforms.\n\n- Data Transformation & ETL : Oversee and guide complex data transformation and ETL processes for large and diverse datasets, ensuring data integrity, quality, and performance.\n\n- Customer-Centric Data Design : Specialize in designing and optimizing customer-centric datasets from various sources, including CRM, Call Center, Marketing, Offline, and Point of Sale systems.\n\n- Data Modeling : Drive the creation and maintenance of advanced data models, including Relational, Dimensional, Columnar, and Big Data models, to support analytical and operational needs.\n\n- Query Optimization : Develop, optimize, and troubleshoot complex SQL and NoSQL queries to ensure efficient data retrieval and manipulation.\n\n- Data Warehouse Management : Apply advanced data warehousing concepts to build and manage high-performing, scalable data warehouse solutions.\n\n- Tool Evaluation & Implementation : Evaluate, recommend, and implement industry-leading ETL tools such as Informatica and Unifi, ensuring best practices are followed.\n\n- Business Requirements & Analysis : Lead efforts in business requirements definition and management, structured analysis, process design, and use case documentation to translate business needs into technical specifications.\n\n- Reporting & Analytics Support : Collaborate with reporting teams, providing architectural guidance and support for reporting technologies like Tableau and PowerBI.\n\n- Software Development Practices : Apply professional software development principles and best practices to data solution delivery.\n\n- Stakeholder Collaboration : Interface effectively with sales teams and directly engage with customers to understand their data challenges and lead them to successful outcomes.\n\n- Project Management & Multi-tasking : Demonstrate exceptional organizational skills, with the ability to manage and prioritize multiple simultaneous customer projects effectively.\n\n- Strategic Thinking & Leadership : Act as a self-managed, proactive, and customer-focused leader, driving innovation and continuous improvement in data architecture.\n\nPosition Requirements :\n\nof strong experience with data transformation & ETL on large data sets.\n\n- Experience with designing customer-centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale, etc.).\n\n- 5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data).\n\n- 5+ years of complex SQL or NoSQL experience.\n\n- Extensive experience in advanced Data Warehouse concepts.\n\n- Proven experience with industry ETL tools (i.e., Informatica, Unifi).\n\n- Solid experience with Business Requirements definition and management, structured analysis, process design, and use case documentation.\n\n- Experience with Reporting Technologies (i.e., Tableau, PowerBI).\n\n- Demonstrated experience in professional software development.\n\n- Exceptional organizational skills and ability to multi-task simultaneous different customer projects.\n\n- Strong verbal & written communication skills to interface with sales teams and lead customers to successful outcomes.\n\n- Must be self-managed, proactive, and customer-focused.\n\nTechnical Skills :\n\n- Cloud Platforms : Microsoft Azure\n\n- Data Warehousing : Snowflake\n\n- ETL Methodologies : Extensive experience in ETL processes and tools\n\n- Data Transformation : Large-scale data transformation\n\n- Data Modeling : Relational, Dimensional, Columnar, Big Data\n\n- Query Languages : Complex SQL, NoSQL\n\n- ETL Tools : Informatica, Unifi (or similar enterprise-grade tools)\n\n- Reporting & BI : Tableau, PowerBI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Architect', 'Azure', 'Enterprise Architect', 'Big Data', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'ETL', 'Reporting Tools', 'SQL']",2025-06-13 05:42:03
Enterprise Data Architect - Azure/ETL,Leading Client,10 - 12 years,Not Disclosed,['Ahmedabad'],"Key Responsibilities :\n\nAs an Enterprise Data Architect, you will :\n\n- Lead Data Architecture : Design, develop, and implement comprehensive enterprise data architectures, primarily leveraging Azure and Snowflake platforms.\n\n- Data Transformation & ETL : Oversee and guide complex data transformation and ETL processes for large and diverse datasets, ensuring data integrity, quality, and performance.\n\n- Customer-Centric Data Design : Specialize in designing and optimizing customer-centric datasets from various sources, including CRM, Call Center, Marketing, Offline, and Point of Sale systems.\n\n- Data Modeling : Drive the creation and maintenance of advanced data models, including Relational, Dimensional, Columnar, and Big Data models, to support analytical and operational needs.\n\n- Query Optimization : Develop, optimize, and troubleshoot complex SQL and NoSQL queries to ensure efficient data retrieval and manipulation.\n\n- Data Warehouse Management : Apply advanced data warehousing concepts to build and manage high-performing, scalable data warehouse solutions.\n\n- Tool Evaluation & Implementation : Evaluate, recommend, and implement industry-leading ETL tools such as Informatica and Unifi, ensuring best practices are followed.\n\n- Business Requirements & Analysis : Lead efforts in business requirements definition and management, structured analysis, process design, and use case documentation to translate business needs into technical specifications.\n\n- Reporting & Analytics Support : Collaborate with reporting teams, providing architectural guidance and support for reporting technologies like Tableau and PowerBI.\n\n- Software Development Practices : Apply professional software development principles and best practices to data solution delivery.\n\n- Stakeholder Collaboration : Interface effectively with sales teams and directly engage with customers to understand their data challenges and lead them to successful outcomes.\n\n- Project Management & Multi-tasking : Demonstrate exceptional organizational skills, with the ability to manage and prioritize multiple simultaneous customer projects effectively.\n\n- Strategic Thinking & Leadership : Act as a self-managed, proactive, and customer-focused leader, driving innovation and continuous improvement in data architecture.\n\nPosition Requirements :\n\nof strong experience with data transformation & ETL on large data sets.\n\n- Experience with designing customer-centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale, etc.).\n\n- 5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data).\n\n- 5+ years of complex SQL or NoSQL experience.\n\n- Extensive experience in advanced Data Warehouse concepts.\n\n- Proven experience with industry ETL tools (i.e., Informatica, Unifi).\n\n- Solid experience with Business Requirements definition and management, structured analysis, process design, and use case documentation.\n\n- Experience with Reporting Technologies (i.e., Tableau, PowerBI).\n\n- Demonstrated experience in professional software development.\n\n- Exceptional organizational skills and ability to multi-task simultaneous different customer projects.\n\n- Strong verbal & written communication skills to interface with sales teams and lead customers to successful outcomes.\n\n- Must be self-managed, proactive, and customer-focused.\n\nTechnical Skills :\n\n- Cloud Platforms : Microsoft Azure\n\n- Data Warehousing : Snowflake\n\n- ETL Methodologies : Extensive experience in ETL processes and tools\n\n- Data Transformation : Large-scale data transformation\n\n- Data Modeling : Relational, Dimensional, Columnar, Big Data\n\n- Query Languages : Complex SQL, NoSQL\n\n- ETL Tools : Informatica, Unifi (or similar enterprise-grade tools)\n\n- Reporting & BI : Tableau, PowerBI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Architect', 'Azure', 'Enterprise Architect', 'Big Data', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'ETL', 'Reporting Tools', 'SQL']",2025-06-13 05:42:04
Enterprise Data Architect - Azure/ETL,Leading Client,10 - 12 years,Not Disclosed,['Pune'],"Key Responsibilities :\n\nAs an Enterprise Data Architect, you will :\n\n- Lead Data Architecture : Design, develop, and implement comprehensive enterprise data architectures, primarily leveraging Azure and Snowflake platforms.\n\n- Data Transformation & ETL : Oversee and guide complex data transformation and ETL processes for large and diverse datasets, ensuring data integrity, quality, and performance.\n\n- Customer-Centric Data Design : Specialize in designing and optimizing customer-centric datasets from various sources, including CRM, Call Center, Marketing, Offline, and Point of Sale systems.\n\n- Data Modeling : Drive the creation and maintenance of advanced data models, including Relational, Dimensional, Columnar, and Big Data models, to support analytical and operational needs.\n\n- Query Optimization : Develop, optimize, and troubleshoot complex SQL and NoSQL queries to ensure efficient data retrieval and manipulation.\n\n- Data Warehouse Management : Apply advanced data warehousing concepts to build and manage high-performing, scalable data warehouse solutions.\n\n- Tool Evaluation & Implementation : Evaluate, recommend, and implement industry-leading ETL tools such as Informatica and Unifi, ensuring best practices are followed.\n\n- Business Requirements & Analysis : Lead efforts in business requirements definition and management, structured analysis, process design, and use case documentation to translate business needs into technical specifications.\n\n- Reporting & Analytics Support : Collaborate with reporting teams, providing architectural guidance and support for reporting technologies like Tableau and PowerBI.\n\n- Software Development Practices : Apply professional software development principles and best practices to data solution delivery.\n\n- Stakeholder Collaboration : Interface effectively with sales teams and directly engage with customers to understand their data challenges and lead them to successful outcomes.\n\n- Project Management & Multi-tasking : Demonstrate exceptional organizational skills, with the ability to manage and prioritize multiple simultaneous customer projects effectively.\n\n- Strategic Thinking & Leadership : Act as a self-managed, proactive, and customer-focused leader, driving innovation and continuous improvement in data architecture.\n\nPosition Requirements :\n\nof strong experience with data transformation & ETL on large data sets.\n\n- Experience with designing customer-centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale, etc.).\n\n- 5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data).\n\n- 5+ years of complex SQL or NoSQL experience.\n\n- Extensive experience in advanced Data Warehouse concepts.\n\n- Proven experience with industry ETL tools (i.e., Informatica, Unifi).\n\n- Solid experience with Business Requirements definition and management, structured analysis, process design, and use case documentation.\n\n- Experience with Reporting Technologies (i.e., Tableau, PowerBI).\n\n- Demonstrated experience in professional software development.\n\n- Exceptional organizational skills and ability to multi-task simultaneous different customer projects.\n\n- Strong verbal & written communication skills to interface with sales teams and lead customers to successful outcomes.\n\n- Must be self-managed, proactive, and customer-focused.\n\nTechnical Skills :\n\n- Cloud Platforms : Microsoft Azure\n\n- Data Warehousing : Snowflake\n\n- ETL Methodologies : Extensive experience in ETL processes and tools\n\n- Data Transformation : Large-scale data transformation\n\n- Data Modeling : Relational, Dimensional, Columnar, Big Data\n\n- Query Languages : Complex SQL, NoSQL\n\n- ETL Tools : Informatica, Unifi (or similar enterprise-grade tools)\n\n- Reporting & BI : Tableau, PowerBI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Complex SQL', 'NoSQL', 'PowerBI', 'Big Data', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Tableau', 'ETL', 'Reporting Tools', 'SQL']",2025-06-13 05:42:06
Enterprise Data Architect - Azure/ETL,Leading Client,10 - 12 years,Not Disclosed,['Mumbai'],"Key Responsibilities :\n\nAs an Enterprise Data Architect, you will :\n\n- Lead Data Architecture : Design, develop, and implement comprehensive enterprise data architectures, primarily leveraging Azure and Snowflake platforms.\n\n- Data Transformation & ETL : Oversee and guide complex data transformation and ETL processes for large and diverse datasets, ensuring data integrity, quality, and performance.\n\n- Customer-Centric Data Design : Specialize in designing and optimizing customer-centric datasets from various sources, including CRM, Call Center, Marketing, Offline, and Point of Sale systems.\n\n- Data Modeling : Drive the creation and maintenance of advanced data models, including Relational, Dimensional, Columnar, and Big Data models, to support analytical and operational needs.\n\n- Query Optimization : Develop, optimize, and troubleshoot complex SQL and NoSQL queries to ensure efficient data retrieval and manipulation.\n\n- Data Warehouse Management : Apply advanced data warehousing concepts to build and manage high-performing, scalable data warehouse solutions.\n\n- Tool Evaluation & Implementation : Evaluate, recommend, and implement industry-leading ETL tools such as Informatica and Unifi, ensuring best practices are followed.\n\n- Business Requirements & Analysis : Lead efforts in business requirements definition and management, structured analysis, process design, and use case documentation to translate business needs into technical specifications.\n\n- Reporting & Analytics Support : Collaborate with reporting teams, providing architectural guidance and support for reporting technologies like Tableau and PowerBI.\n\n- Software Development Practices : Apply professional software development principles and best practices to data solution delivery.\n\n- Stakeholder Collaboration : Interface effectively with sales teams and directly engage with customers to understand their data challenges and lead them to successful outcomes.\n\n- Project Management & Multi-tasking : Demonstrate exceptional organizational skills, with the ability to manage and prioritize multiple simultaneous customer projects effectively.\n\n- Strategic Thinking & Leadership : Act as a self-managed, proactive, and customer-focused leader, driving innovation and continuous improvement in data architecture.\n\nPosition Requirements :\n\nof strong experience with data transformation & ETL on large data sets.\n\n- Experience with designing customer-centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale, etc.).\n\n- 5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data).\n\n- 5+ years of complex SQL or NoSQL experience.\n\n- Extensive experience in advanced Data Warehouse concepts.\n\n- Proven experience with industry ETL tools (i.e., Informatica, Unifi).\n\n- Solid experience with Business Requirements definition and management, structured analysis, process design, and use case documentation.\n\n- Experience with Reporting Technologies (i.e., Tableau, PowerBI).\n\n- Demonstrated experience in professional software development.\n\n- Exceptional organizational skills and ability to multi-task simultaneous different customer projects.\n\n- Strong verbal & written communication skills to interface with sales teams and lead customers to successful outcomes.\n\n- Must be self-managed, proactive, and customer-focused.\n\nTechnical Skills :\n\n- Cloud Platforms : Microsoft Azure\n\n- Data Warehousing : Snowflake\n\n- ETL Methodologies : Extensive experience in ETL processes and tools\n\n- Data Transformation : Large-scale data transformation\n\n- Data Modeling : Relational, Dimensional, Columnar, Big Data\n\n- Query Languages : Complex SQL, NoSQL\n\n- ETL Tools : Informatica, Unifi (or similar enterprise-grade tools)\n\n- Reporting & BI : Tableau, PowerBI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Complex SQL', 'NoSQL', 'PowerBI', 'Big Data', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Tableau', 'ETL', 'Reporting Tools', 'SQL']",2025-06-13 05:42:08
Enterprise Data Architect - Azure/ETL,Leading Client,10 - 12 years,Not Disclosed,['Indore'],"Key Responsibilities :\n\nAs an Enterprise Data Architect, you will :\n\n- Lead Data Architecture : Design, develop, and implement comprehensive enterprise data architectures, primarily leveraging Azure and Snowflake platforms.\n\n- Data Transformation & ETL : Oversee and guide complex data transformation and ETL processes for large and diverse datasets, ensuring data integrity, quality, and performance.\n\n- Customer-Centric Data Design : Specialize in designing and optimizing customer-centric datasets from various sources, including CRM, Call Center, Marketing, Offline, and Point of Sale systems.\n\n- Data Modeling : Drive the creation and maintenance of advanced data models, including Relational, Dimensional, Columnar, and Big Data models, to support analytical and operational needs.\n\n- Query Optimization : Develop, optimize, and troubleshoot complex SQL and NoSQL queries to ensure efficient data retrieval and manipulation.\n\n- Data Warehouse Management : Apply advanced data warehousing concepts to build and manage high-performing, scalable data warehouse solutions.\n\n- Tool Evaluation & Implementation : Evaluate, recommend, and implement industry-leading ETL tools such as Informatica and Unifi, ensuring best practices are followed.\n\n- Business Requirements & Analysis : Lead efforts in business requirements definition and management, structured analysis, process design, and use case documentation to translate business needs into technical specifications.\n\n- Reporting & Analytics Support : Collaborate with reporting teams, providing architectural guidance and support for reporting technologies like Tableau and PowerBI.\n\n- Software Development Practices : Apply professional software development principles and best practices to data solution delivery.\n\n- Stakeholder Collaboration : Interface effectively with sales teams and directly engage with customers to understand their data challenges and lead them to successful outcomes.\n\n- Project Management & Multi-tasking : Demonstrate exceptional organizational skills, with the ability to manage and prioritize multiple simultaneous customer projects effectively.\n\n- Strategic Thinking & Leadership : Act as a self-managed, proactive, and customer-focused leader, driving innovation and continuous improvement in data architecture.\n\nPosition Requirements :\n\nof strong experience with data transformation & ETL on large data sets.\n\n- Experience with designing customer-centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale, etc.).\n\n- 5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data).\n\n- 5+ years of complex SQL or NoSQL experience.\n\n- Extensive experience in advanced Data Warehouse concepts.\n\n- Proven experience with industry ETL tools (i.e., Informatica, Unifi).\n\n- Solid experience with Business Requirements definition and management, structured analysis, process design, and use case documentation.\n\n- Experience with Reporting Technologies (i.e., Tableau, PowerBI).\n\n- Demonstrated experience in professional software development.\n\n- Exceptional organizational skills and ability to multi-task simultaneous different customer projects.\n\n- Strong verbal & written communication skills to interface with sales teams and lead customers to successful outcomes.\n\n- Must be self-managed, proactive, and customer-focused.\n\nTechnical Skills :\n\n- Cloud Platforms : Microsoft Azure\n\n- Data Warehousing : Snowflake\n\n- ETL Methodologies : Extensive experience in ETL processes and tools\n\n- Data Transformation : Large-scale data transformation\n\n- Data Modeling : Relational, Dimensional, Columnar, Big Data\n\n- Query Languages : Complex SQL, NoSQL\n\n- ETL Tools : Informatica, Unifi (or similar enterprise-grade tools)\n\n- Reporting & BI : Tableau, PowerBI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Complex SQL', 'NoSQL', 'PowerBI', 'Big Data', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Tableau', 'ETL', 'Reporting Tools', 'SQL']",2025-06-13 05:42:10
Enterprise Data Architect - Azure/ETL,Leading Client,10 - 12 years,Not Disclosed,['Lucknow'],"Key Responsibilities :\n\nAs an Enterprise Data Architect, you will :\n\n- Lead Data Architecture : Design, develop, and implement comprehensive enterprise data architectures, primarily leveraging Azure and Snowflake platforms.\n\n- Data Transformation & ETL : Oversee and guide complex data transformation and ETL processes for large and diverse datasets, ensuring data integrity, quality, and performance.\n\n- Customer-Centric Data Design : Specialize in designing and optimizing customer-centric datasets from various sources, including CRM, Call Center, Marketing, Offline, and Point of Sale systems.\n\n- Data Modeling : Drive the creation and maintenance of advanced data models, including Relational, Dimensional, Columnar, and Big Data models, to support analytical and operational needs.\n\n- Query Optimization : Develop, optimize, and troubleshoot complex SQL and NoSQL queries to ensure efficient data retrieval and manipulation.\n\n- Data Warehouse Management : Apply advanced data warehousing concepts to build and manage high-performing, scalable data warehouse solutions.\n\n- Tool Evaluation & Implementation : Evaluate, recommend, and implement industry-leading ETL tools such as Informatica and Unifi, ensuring best practices are followed.\n\n- Business Requirements & Analysis : Lead efforts in business requirements definition and management, structured analysis, process design, and use case documentation to translate business needs into technical specifications.\n\n- Reporting & Analytics Support : Collaborate with reporting teams, providing architectural guidance and support for reporting technologies like Tableau and PowerBI.\n\n- Software Development Practices : Apply professional software development principles and best practices to data solution delivery.\n\n- Stakeholder Collaboration : Interface effectively with sales teams and directly engage with customers to understand their data challenges and lead them to successful outcomes.\n\n- Project Management & Multi-tasking : Demonstrate exceptional organizational skills, with the ability to manage and prioritize multiple simultaneous customer projects effectively.\n\n- Strategic Thinking & Leadership : Act as a self-managed, proactive, and customer-focused leader, driving innovation and continuous improvement in data architecture.\n\nPosition Requirements :\n\nof strong experience with data transformation & ETL on large data sets.\n\n- Experience with designing customer-centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale, etc.).\n\n- 5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data).\n\n- 5+ years of complex SQL or NoSQL experience.\n\n- Extensive experience in advanced Data Warehouse concepts.\n\n- Proven experience with industry ETL tools (i.e., Informatica, Unifi).\n\n- Solid experience with Business Requirements definition and management, structured analysis, process design, and use case documentation.\n\n- Experience with Reporting Technologies (i.e., Tableau, PowerBI).\n\n- Demonstrated experience in professional software development.\n\n- Exceptional organizational skills and ability to multi-task simultaneous different customer projects.\n\n- Strong verbal & written communication skills to interface with sales teams and lead customers to successful outcomes.\n\n- Must be self-managed, proactive, and customer-focused.\n\nTechnical Skills :\n\n- Cloud Platforms : Microsoft Azure\n\n- Data Warehousing : Snowflake\n\n- ETL Methodologies : Extensive experience in ETL processes and tools\n\n- Data Transformation : Large-scale data transformation\n\n- Data Modeling : Relational, Dimensional, Columnar, Big Data\n\n- Query Languages : Complex SQL, NoSQL\n\n- ETL Tools : Informatica, Unifi (or similar enterprise-grade tools)\n\n- Reporting & BI : Tableau, PowerBI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Enterprise Architect', 'Data Architect', 'Big Data', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'ETL', 'Reporting Tools', 'SQL']",2025-06-13 05:42:11
Enterprise Data Architect - Azure/ETL,Leading Client,10 - 12 years,Not Disclosed,['Chennai'],"Key Responsibilities :\n\nAs an Enterprise Data Architect, you will :\n\n- Lead Data Architecture : Design, develop, and implement comprehensive enterprise data architectures, primarily leveraging Azure and Snowflake platforms.\n\n- Data Transformation & ETL : Oversee and guide complex data transformation and ETL processes for large and diverse datasets, ensuring data integrity, quality, and performance.\n\n- Customer-Centric Data Design : Specialize in designing and optimizing customer-centric datasets from various sources, including CRM, Call Center, Marketing, Offline, and Point of Sale systems.\n\n- Data Modeling : Drive the creation and maintenance of advanced data models, including Relational, Dimensional, Columnar, and Big Data models, to support analytical and operational needs.\n\n- Query Optimization : Develop, optimize, and troubleshoot complex SQL and NoSQL queries to ensure efficient data retrieval and manipulation.\n\n- Data Warehouse Management : Apply advanced data warehousing concepts to build and manage high-performing, scalable data warehouse solutions.\n\n- Tool Evaluation & Implementation : Evaluate, recommend, and implement industry-leading ETL tools such as Informatica and Unifi, ensuring best practices are followed.\n\n- Business Requirements & Analysis : Lead efforts in business requirements definition and management, structured analysis, process design, and use case documentation to translate business needs into technical specifications.\n\n- Reporting & Analytics Support : Collaborate with reporting teams, providing architectural guidance and support for reporting technologies like Tableau and PowerBI.\n\n- Software Development Practices : Apply professional software development principles and best practices to data solution delivery.\n\n- Stakeholder Collaboration : Interface effectively with sales teams and directly engage with customers to understand their data challenges and lead them to successful outcomes.\n\n- Project Management & Multi-tasking : Demonstrate exceptional organizational skills, with the ability to manage and prioritize multiple simultaneous customer projects effectively.\n\n- Strategic Thinking & Leadership : Act as a self-managed, proactive, and customer-focused leader, driving innovation and continuous improvement in data architecture.\n\nPosition Requirements :\n\nof strong experience with data transformation & ETL on large data sets.\n\n- Experience with designing customer-centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale, etc.).\n\n- 5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data).\n\n- 5+ years of complex SQL or NoSQL experience.\n\n- Extensive experience in advanced Data Warehouse concepts.\n\n- Proven experience with industry ETL tools (i.e., Informatica, Unifi).\n\n- Solid experience with Business Requirements definition and management, structured analysis, process design, and use case documentation.\n\n- Experience with Reporting Technologies (i.e., Tableau, PowerBI).\n\n- Demonstrated experience in professional software development.\n\n- Exceptional organizational skills and ability to multi-task simultaneous different customer projects.\n\n- Strong verbal & written communication skills to interface with sales teams and lead customers to successful outcomes.\n\n- Must be self-managed, proactive, and customer-focused.\n\nTechnical Skills :\n\n- Cloud Platforms : Microsoft Azure\n\n- Data Warehousing : Snowflake\n\n- ETL Methodologies : Extensive experience in ETL processes and tools\n\n- Data Transformation : Large-scale data transformation\n\n- Data Modeling : Relational, Dimensional, Columnar, Big Data\n\n- Query Languages : Complex SQL, NoSQL\n\n- ETL Tools : Informatica, Unifi (or similar enterprise-grade tools)\n\n- Reporting & BI : Tableau, PowerBI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Enterprise Architect', 'Data Architect', 'Big Data', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'ETL', 'Reporting Tools', 'SQL']",2025-06-13 05:42:13
Enterprise Data Architect - Azure/ETL,Leading Client,10 - 12 years,Not Disclosed,['Jaipur'],"Key Responsibilities :\n\nAs an Enterprise Data Architect, you will :\n\n- Lead Data Architecture : Design, develop, and implement comprehensive enterprise data architectures, primarily leveraging Azure and Snowflake platforms.\n\n- Data Transformation & ETL : Oversee and guide complex data transformation and ETL processes for large and diverse datasets, ensuring data integrity, quality, and performance.\n\n- Customer-Centric Data Design : Specialize in designing and optimizing customer-centric datasets from various sources, including CRM, Call Center, Marketing, Offline, and Point of Sale systems.\n\n- Data Modeling : Drive the creation and maintenance of advanced data models, including Relational, Dimensional, Columnar, and Big Data models, to support analytical and operational needs.\n\n- Query Optimization : Develop, optimize, and troubleshoot complex SQL and NoSQL queries to ensure efficient data retrieval and manipulation.\n\n- Data Warehouse Management : Apply advanced data warehousing concepts to build and manage high-performing, scalable data warehouse solutions.\n\n- Tool Evaluation & Implementation : Evaluate, recommend, and implement industry-leading ETL tools such as Informatica and Unifi, ensuring best practices are followed.\n\n- Business Requirements & Analysis : Lead efforts in business requirements definition and management, structured analysis, process design, and use case documentation to translate business needs into technical specifications.\n\n- Reporting & Analytics Support : Collaborate with reporting teams, providing architectural guidance and support for reporting technologies like Tableau and PowerBI.\n\n- Software Development Practices : Apply professional software development principles and best practices to data solution delivery.\n\n- Stakeholder Collaboration : Interface effectively with sales teams and directly engage with customers to understand their data challenges and lead them to successful outcomes.\n\n- Project Management & Multi-tasking : Demonstrate exceptional organizational skills, with the ability to manage and prioritize multiple simultaneous customer projects effectively.\n\n- Strategic Thinking & Leadership : Act as a self-managed, proactive, and customer-focused leader, driving innovation and continuous improvement in data architecture.\n\nPosition Requirements :\n\nof strong experience with data transformation & ETL on large data sets.\n\n- Experience with designing customer-centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale, etc.).\n\n- 5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data).\n\n- 5+ years of complex SQL or NoSQL experience.\n\n- Extensive experience in advanced Data Warehouse concepts.\n\n- Proven experience with industry ETL tools (i.e., Informatica, Unifi).\n\n- Solid experience with Business Requirements definition and management, structured analysis, process design, and use case documentation.\n\n- Experience with Reporting Technologies (i.e., Tableau, PowerBI).\n\n- Demonstrated experience in professional software development.\n\n- Exceptional organizational skills and ability to multi-task simultaneous different customer projects.\n\n- Strong verbal & written communication skills to interface with sales teams and lead customers to successful outcomes.\n\n- Must be self-managed, proactive, and customer-focused.\n\nTechnical Skills :\n\n- Cloud Platforms : Microsoft Azure\n\n- Data Warehousing : Snowflake\n\n- ETL Methodologies : Extensive experience in ETL processes and tools\n\n- Data Transformation : Large-scale data transformation\n\n- Data Modeling : Relational, Dimensional, Columnar, Big Data\n\n- Query Languages : Complex SQL, NoSQL\n\n- ETL Tools : Informatica, Unifi (or similar enterprise-grade tools)\n\n- Reporting & BI : Tableau, PowerBI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Enterprise Architect', 'Data Architect', 'Big Data', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'ETL', 'Reporting Tools', 'SQL']",2025-06-13 05:42:15
Azure Data Architect,T.EN GLOBAL BUSINESS SERVICES PRIVATE LIMITED,5 - 10 years,Not Disclosed,['Noida'],"Be part of the solution at Technip Energies and embark on a one-of-a-kind journey. You will be helping to develop cutting-edge solutions to solve real-world energy problems.\nWe are currently seeking an Azure Data Architect , to join our Digi team based in Noida .\nAbout us:\nTechnip Energies is a global technology and engineering powerhouse. With leadership positions in LNG, hydrogen, ethylene, sustainable chemistry, and CO2 management, we are contributing to the development of critical markets such as energy, energy derivatives, decarbonization, and circularity. Our complementary business segments, Technology, Products and Services (TPS) and Project Delivery, turn innovation into scalable and industrial reality.\nThrough collaboration and excellence in execution, our 17,000+ employees across 34 countries are fully committed to bridging prosperity with sustainability for a world designed to last.\nAbout the opportunity we offer:\nDevelop RESTful APIs using Azure APIM\nDevelop integration workflow using LogicApp, synpase and service bus.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation using Azure Data Factory and synapse pipelines.\nCollaborate closely with Product Owners to understand data pipeline requirements and design effective data workflows.\nTranslate business requirements into technical specifications for data pipelines.\nCreate and maintain data storage solutions using Azure Cosmos DB and Azure Data Lake Storage\nDesign and implement data models to optimize data storage and retrieval.\nEnsure data security and compliance with data governance policies.\nAnalyze data pipeline performance metrics to identify bottlenecks and areas for improvement.\nMonitor data pipelines to ensure data consistency, availability, and adherence to service-level agreements.\nIntegrate data pipelines with Azure DevOps to automate data pipeline deployment and testing processes.\nLeverage Azure DevOps tools for continuous integration and continuous delivery (CI/CD) of data pipelines\nWork effectively in an Agile development environment\nCollaborate with cross-functional teams to deliver value in an Agile manner.\nAbout you:\n5 years work experience (minimum 3 years Experience in Microsoft Azure) (Azure Administrator, Data Platform, Data Lake, Synapse Pipelines, Synapse Analytics, API Management and other data cloud architecture)\nDevelopment environments: Git, Azure DevOps, Template ARM\nLanguages: C#, .NET, Python\nStrong analytical problem solver with an organized approach\nFluent English\nExcellent methodology (communication, documentation, collaborative approach)\nAct independently and as a top-level contributor in resolving project strategy, scope, and direction\nExcellent organizational skills and a proven ability to get results\nData mindset\nNice to have :\nMicrosoft Azure certifications\nScala, JAVA\nData-related projects: 5 years minimum\nYour career with us:\nWorking at Technip Energies is an inspiring journey, filled with groundbreaking projects and dynamic collaborations. Surrounded by diverse and talented individuals, you will feel welcomed, respected, and engaged. Enjoy a safe, caring environment where you can spark new ideas, reimagine the future, and lead change. As your career grows, you will benefit from learning opportunities at T.EN University, such as The Future Ready Program, and from the support of your manager through check-in moments like the Mid-Year Development Review, fostering continuous growth and development\nWhat s next\nOnce receiving your application, our Talent Acquisition professionals will screen and match your profile against the role requirements. We ask for your patience as the team completes the volume of applications with reasonable timeframe. Check your application progress periodically via personal account from created candidate profile during your application.\nWe invite you to get to know more about our company by visiting and follow us on LinkedIn , Instagram , Facebook , X and YouTube for company updates.\n#LI-AP1",Industry Type: Engineering & Construction,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Service level', 'Talent acquisition', 'data security', 'Analytical', 'TPS', 'Data Architect', 'Agile', 'Workflow', 'Project delivery', 'Analytics']",2025-06-13 05:42:16
Senior Technical Architect - Web Data & Analytics,Quicken Software Development,10 - 12 years,Not Disclosed,['Bengaluru'],"We are seeking a Senior Technical Architect Web Data & Analytics with expertise in Google web data tools, web data architecture, and JavaScript-based tracking. This role involves providing strategic guidance, technical direction, and actionable insights to drive business decisions. The ideal candidate excels in data implementation best practices, collaborates effectively across teams in a global environment, and possesses strong communication and execution skills.\nResponsibilities:\nTechnical Lead for the Web Data & Analytics team, ensuring best practices in data collection, governance, implementation, tracking, and reporting.\nCollaborate with stakeholders, including Web Development, Data Science, and Marketing Leadership, to understand analytics needs.\nDefine and implement a scalable web data architecture for efficient tracking, tagging, and data collection across analytics and marketing platforms.\nProvide hands-on expertise in Google Analytics (GA4), Google Tag Manager (GTM), and JavaScript-based tracking solutions.\nDevelop and enhance a scalable data layer to facilitate standardized and efficient data transmission across platforms.\nTrack and analyze user journeys and purchase funnels, monitor key performance indicators (KPIs), and deliver insights on user behavior and site performance.\nContinuously improve dashboards and data visualizations using Google Data Studio, Tableau, or other BI tools.\nInterpret data trends and provide actionable insights to enable data-driven decision-making.\nManage analytics projects and ensure timely execution of strategic initiatives.\nStay up-to-date with the latest web analytics trends, privacy regulations (GDPR, CCPA), industry standards, and best practices.\nEnsure data integrity, accuracy, and consistency across all analytics platforms and reporting tools.\nIdentify and address gaps in data collection, recommending enhancements to improve data quality and insights.\nProvide technical guidance and best practices on Google tools for data collection, tagging, and reporting.\nDevelop compelling data reports, dashboards, and visualizations with supporting narratives for senior leadership presentations.\nQualifications:\n10+ years of experience in Web Analytics, with at least 2+ years in a technical leadership role.\nStrong understanding of digital marketing analytics, attribution modeling, and customer segmentation.\nExpertise in web data architecture, tracking strategy development, and data layer design.\nHands-on experience with event-driven tracking, structured data layers, and tagging frameworks.\nProven expertise in Google Analytics (GA4), Google Tag Manager (GTM), and web tracking tools such as Amplitude, Glassbox, or similar technologies.\nFamiliarity with audience segmentation through Customer Data Platforms (CDPs) or Google Analytics.\nDeep understanding of JavaScript, HTML, and CSS, with the ability to debug and troubleshoot tracking issues.\nProficiency in SQL and familiarity with scripting languages such as Python or JavaScript for automation.\nExperience managing BigQuery or other marketing databases, including data extraction, transformation, and analysis.\nStrong analytical skills in integrating user data from multiple sources, such as user behavior analysis and A/B testing.\nAbility to generate and present actionable insights that drive Conversion Rate Optimization (CRO).\nExcellent communication and presentation skills, with the ability to translate complex data into meaningful business insights.\nStrong project management skills, with the ability to manage multiple priorities and deadlines.\nBachelor s degree in Marketing, Computer Science, or a related field.\nAdvanced certification in GA4 is a plus.\nWhat we offer:\nCompetitive salary and performance bonus\nFantastic culture, strong believers in Autonomy/Mastery/Purpose\nCustomer-driven, we make money by building the best products for our users. No confusion about how to win create amazing products!\nAbility to work with and lead incredible talent\nBuild products that make a huge difference in people s lives\nWork on a highly recognizable brand that defined the personal finance category\nQuicken is excited to build a team of innovative and talented people dedicated to helping people improve their financial lives. We believe in giving our employees the benefits and perks to keep them happy and healthy. Whether it s generous healthcare benefits, a 5% matching 401(k) Plan, a gym subsidy, onsite lunches, or unlimited snacks, we believe happy employees are successful employees. (Benefits and perks may vary based on location.) Come join our Quicken team!",Industry Type: Film / Music / Entertainment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Google Analytics', 'Web analytics', 'Web development', 'Javascript', 'Healthcare', 'HTML', 'SQL', 'Python']",2025-06-13 05:42:19
Data Modeler,Synechron,0 - 2 years,Not Disclosed,['Bengaluru'],"Synechron is seeking a knowledgeable and proactive Data Modeler to guide the design and development of data structures that support our clients' business objectives. In this role, you will collaborate with cross-functional teams to translate business requirements into scalable and efficient data models, ensuring data accuracy, consistency, and integrity. You will contribute to creating sustainable and compliant data architectures that leverage emerging technologies such as cloud, IoT, mobile, and blockchain. Your work will be instrumental in enabling data-driven decision-making and operational excellence across projects.Software Required\n\nSkills:\nStrong understanding of data modeling concepts, methodologies, and tools Experience with data modeling for diverse technology platforms including cloud, mobile, IoT, and blockchain Familiarity with database management systems (e.g., relational, NoSQL) Knowledge of SDLC and Agile development practices Proficiency in modeling tools such as ERwin, PowerDesigner, or similar Preferred Skills:\nExperience with data integration tools and ETL processes Knowledge of data governance and compliance standards Familiarity with cloud platforms (AWS, Azure, GCP) and how they impact data architectureOverall Responsibilities Collaborate with business analysts, data engineers, and stakeholders to understand data requirements and translate them into robust data models Design logical and physical data models optimized for performance, scalability, and maintainability Develop and maintain documentation for data structures, including data dictionaries and metadata Conduct reviews of data models and code to ensure adherence to quality standards and best practices Assist in designing data security and privacy measures in alignment with organizational policies Stay informed about emerging data modeling trends and incorporate best practices into project delivery Support data migration, integration, and transformation activities as needed Provide technical guidance and mentorship related to data modeling standardsTechnical Skills (By Category) Data Modeling & Data Management: EssentialLogical/physical data modeling, ER diagrams, data dictionaries PreferredDimensional modeling, data warehousing, master data management Programming Languages: PreferredSQL (expertise in writing complex queries) OptionalPython, R for data analysis and scripting Databases & Data Storage Technologies: EssentialRelational databases (e.g., Oracle, SQL Server, MySQL) PreferredNoSQL (e.g., MongoDB, Cassandra), cloud-native data stores Cloud Technologies: PreferredBasic understanding of cloud data solutions (AWS, Azure, GCP) Frameworks & Libraries: Not typically required, but familiarity with data integration frameworks is advantageous Development Tools & Methodologies: EssentialData modeling tools (ERwin, PowerDesigner), version control (Git), Agile/Scrum workflows Security & Compliance: Knowledge of data security best practices, regulatory standards like GDPR, HIPAAExperience Minimum of 8+ years of direct experience in data modeling, data architecture, or related roles Proven experience designing data models for complex systems across multiple platforms (cloud, mobile, IoT, blockchain) Experience working in Agile environments using tools like JIRA, Confluence, Git Preference for candidates with experience supporting data governance and data quality initiativesNoteEquivalent demonstrated experience in relevant projects or certifications can qualify candidates.Day-to-Day Activities Participate in daily stand-ups and project planning sessions Collaborate with cross-functional teams to understand and analyze business requirements Create, review, and refine data models and associated documentation Develop data schemas, dictionaries, and standards to ensure consistency Support data migration, integration, and performance tuning activities Conduct peer reviews and provide feedback on data models and solutions Keep current with the latest industry developments in data architecture and modeling Troubleshoot and resolve data-related technical issuesQualifications Bachelors or Masters degree in Computer Science, Data Science, Information Technology, or related fields Demonstrated experience with data modeling tools and techniques in diverse technological environments Certifications related to data modeling, data management, or cloud platforms (preferred)Professional Competencies Strong analytical and critical thinking skills to develop optimal data solutions Effective communication skills for translating technical concepts to non-technical stakeholders Ability to work independently and in collaborative team environments Skilled problem solver able to handle complex data challenges Adaptability to rapidly evolving technologies and project requirements Excellent time management and prioritization skills to deliver quality outputs consistently",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data modeling', 'modeling tools', 'relational databases', 'scrum', 'agile', 'confluence', 'hipaa', 'data warehousing', 'data architecture', 'erwin', 'sql', 'git', 'gcp', 'mysql', 'etl', 'mongodb', 'jira', 'python', 'oracle', 'microsoft azure', 'sql server', 'nosql', 'gdpr', 'cassandra', 'aws', 'data integration', 'sdlc']",2025-06-13 05:42:21
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Data Modeling Techniques and Methodologies\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on data architecture and collaborating with cross-functional teams to optimize data processes.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead data modeling initiatives to design and implement data structures.- Optimize data storage and retrieval processes.- Develop and maintain data pipelines for efficient data flow.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Modeling Techniques and Methodologies.- Strong understanding of database management systems.- Experience with data warehousing and ETL processes.- Knowledge of data governance and compliance.- Hands-on experience with data visualization tools.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Modeling Techniques and Methodologies.- This position is based at our Bengaluru office.PFB Education details- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database management system', 'data warehousing', 'data modeling', 'data visualization', 'etl', 'hive', 'python', 'data architecture', 'data engineering', 'sql', 'database management', 'data quality', 'tableau', 'spark', 'data governance', 'data structures', 'hadoop', 'big data', 'data flow', 'etl process']",2025-06-13 05:42:23
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Data Architecture Principles\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. You will create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on various data-related tasks and collaborating with teams to optimize data processes.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Develop innovative data solutions to meet business requirements.- Optimize data pipelines for efficiency and scalability.- Implement data governance policies to ensure data quality and security.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Architecture Principles.- Strong understanding of data modeling and database design.- Experience with ETL tools and processes.- Knowledge of cloud platforms and big data technologies.- Good To Have\n\n\n\n\nSkills:\nData management and governance expertise.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Architecture Principles.- This position is based at our Bengaluru office.Education information - - A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'data architecture', 'database design', 'data architecture principles', 'data modeling', 'hive', 'python', 'big data technologies', 'cloud platforms', 'data engineering', 'sql', 'data quality', 'etl tool', 'spark', 'data governance', 'hadoop', 'etl', 'big data', 'etl process']",2025-06-13 05:42:25
Data Solutions Architect (Immediate Joiner),Expro,7 - 12 years,Not Disclosed,['Hyderabad( HITEC City )'],"Overall Purpose of the Job\nWe are seeking an experienced and innovative Data Solutions Architect to join our team. The ideal candidate will have a strong background in designing and implementing data systems, ensuring seamless integration and scalability of data solutions, and leading the design of data architectures that meet business needs. This role requires proficiency in leveraging the Azure IoT Framework to enable IoT-driven data solutions. As a Data Solutions Architect, you will work closely with cross-functional teams to create robust, secure, and scalable data solutions that empower the business to leverage data for strategic decision-making.\n\nRole & responsibilities\nDesign and implement scalable, secure, and high-performance data architectures that meet business requirements.\nLead the integration of Azure IoT Framework into the architecture, enabling real-time data ingestion, processing, and analysis from IoT devices.\nCollaborate with stakeholders (business, IT, and data science teams) to understand data needs and translate them into effective solutions.\nOversee the full data lifecycle, including data collection, transformation, storage, and consumption.\nEvaluate and recommend tools, platforms, and technologies for data storage, processing, and analytics, ensuring that the solutions are aligned with business goals.\nDefine data integration strategies, ensuring that data from disparate sources, including IoT devices and sensors, can be ingested, processed, and unified effectively.\nCreate and enforce data governance practices, ensuring compliance with data privacy, security, and regulatory requirements.\nLead and mentor teams in the development and implementation of data solutions, ensuring adherence to architectural best practices and design patterns.\nEnsure the scalability, reliability, and performance of data systems to handle increasing volumes of data, including large datasets from IoT sources.\nStay updated on emerging trends and technologies in data management, cloud platforms, Azure IoT, and data engineering.\n\nRequired Skills & Qualifications:\n\nBachelors or masters degree in computer science, Data Engineering, Information Technology, or a related field.\nProven experience (typically 5+ years) in designing, implementing, and managing large-scale data architectures.\nExpertise in Azure IoT Framework, including services such as Azure IoT Hub, Azure Stream Analytics, and Azure Digital Twins.\nStrong understanding of data management, ETL processes, and database design (e.g., SQL, NoSQL, data lakes, and data warehouses).\nHands-on experience with modern data technologies and tools such as Hadoop, Spark, Kafka, and ETL frameworks.\nProficiency with data governance, security, and privacy standards.\nFamiliarity with machine learning, AI integration, and analytics platforms is a plus.\nStrong leadership skills with the ability to mentor and guide technical teams.\nExcellent problem-solving abilities and a deep understanding of system architecture and distributed systems.\nExcellent communication skills, both written and verbal, with the ability to translate complex technical concepts into business-friendly terms.\n\nPreferred Skills:\nExperience with cloud-native data platforms and services (e.g., Azure Synapse Analytics, Azure Databricks).\nKnowledge of data visualization and business intelligence tools (e.g., Tableau, Power BI, Looker).\nExperience with DevOps and CI/CD practices for data pipelines and data-driven applications.\nFamiliarity with microservices architectures and APIs.\nCertifications in Azure technologies (e.g., Microsoft Certified: Azure Solutions Architect Expert, Microsoft Certified: Azure IoT Developer).\nExperience with edge computing and real-time data processing for IoT solutions.\nRequired Immediate joiner.",Industry Type: Oil & Gas,Department: Other,"Employment Type: Full Time, Permanent","['Datafactory', 'Azure Event Hub', 'SQL Server', 'Data Bricks', 'Azure IoT Hub', 'Sap Data Services']",2025-06-13 05:42:27
Data Modeler,Accenture,15 - 20 years,Not Disclosed,['Mumbai'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Data Modeling Techniques and Methodologies\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\nProject Role :Data Architect & Modeler\n\nProject Role Description Data Model, Design, build and lead the complex ETL data integration pipelines to meet business process and application requirements. Management Level :9Work Experience :6+ yearsWork Location :AnyMust have skills :Data Architecture Principles\nGood to have skills :Data Modeling, Data Architect, Informatica PowerCenter, Informatica Data Quality, SAP BusinessObjects Data Services, SQL, PL/SQL, SAP HANA DB, MS Azure, Python, ErWin, SAP Power Designer Job :Data Architect, Modeler, and data Integration LeadKey Responsibilities:1) Working on building Data models, Forward and Reverse Engineering.2) Working on Data and design analysis and working with data analysts team on data model design.3) Working on presentations on design, end to end flow and data models.4) Work on new and existing data models using Power designer tools and other designing tools like Visio5) Work with functional SMEs, BAs to review requirements, mapping documents\nTechnical Experience:1) Should have good understanding of ETL design concepts like CDC, SCD, Transpose/ pivot, Updates, Validation2) Should have strong understanding of SQL concepts, Data warehouse concepts and can easily understand data technically and functionally.3) Good understanding of various file formats like xml, delimited, fixed width etc.4) Understand the concepts of data quality, data cleansing, data profiling5) Good to have Python and other new data technologies and cloud exposure.6) Having Insurance background is a plus.\nEducational Qualification :15 years of fulltime education with BE/B Tech or equivalent\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Modeling Techniques and Methodologies.- Strong understanding of relational and non-relational database design principles.- Experience with data integration and ETL processes.- Familiarity with data governance and data quality frameworks.- Ability to translate business requirements into technical specifications.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'data architecture principles', 'data modeling', 'data warehousing concepts', 'sql joins', 'python', 'ms azure', 'sap', 'informatica powercenter', 'informatica data quality', 'data warehousing', 'erwin', 'data architecture', 'plsql', 'modeler', 'hana db', 'etl', 'sap hana', 'data integration']",2025-06-13 05:42:29
Data Modeler,Accenture,15 - 20 years,Not Disclosed,['Mumbai'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Data Building Tool\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Modeler, you will engage with key business representatives, data owners, end users, application designers, and data architects to model both current and new data. Your typical day will involve collaborating with various stakeholders to understand their data needs, analyzing existing data structures, and designing effective data models that support business objectives. You will also be responsible for ensuring that the data models are aligned with best practices and organizational standards, facilitating smooth data integration and accessibility across different systems. This role requires a proactive approach to problem-solving and a commitment to delivering high-quality data solutions that enhance decision-making processes within the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate training sessions and workshops to enhance team capabilities.- Continuously evaluate and improve data modeling processes to ensure efficiency.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Building Tool.- Strong understanding of data modeling techniques and methodologies.- Experience with data integration and ETL processes.- Familiarity with database management systems and SQL.- Ability to translate business requirements into technical specifications.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in Data Building Tool.- This position is based in Mumbai.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'database management', 'data modeling', 'etl', 'data integration', 'python', 'oracle', 'data analysis', 'data warehousing', 'sme', 'data architecture', 'business intelligence', 'sql server', 'plsql', 'unix shell scripting', 'etl tool', 'modeler', 'informatica', 'unix', 'etl process']",2025-06-13 05:42:30
Data Modeler,Accenture,12 - 15 years,Not Disclosed,['Kolkata'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Data Building Tool\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Modeler, you will engage with key business representatives, data owners, end users, application designers, and data architects to model both current and new data. Your typical day will involve collaborating with various stakeholders to understand their data needs, analyzing existing data structures, and designing effective data models that support business objectives. You will also be responsible for ensuring that the data models are aligned with best practices and organizational standards, facilitating smooth data integration and accessibility across different systems. This role requires a proactive approach to problem-solving and a commitment to delivering high-quality data solutions that enhance decision-making processes within the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate workshops and meetings to gather requirements and feedback from stakeholders.- Develop and maintain comprehensive documentation of data models and architecture.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Building Tool.- Strong understanding of data modeling techniques and methodologies.- Experience with data integration and ETL processes.- Familiarity with database management systems and SQL.- Ability to translate business requirements into technical specifications.\nAdditional Information:- The candidate should have minimum 12 years of experience in Data Building Tool.- This position is based at our Kolkata office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'database management', 'data modeling', 'etl', 'data integration', 'python', 'oracle', 'data analysis', 'data warehousing', 'sme', 'data architecture', 'business intelligence', 'sql server', 'plsql', 'unix shell scripting', 'etl tool', 'modeler', 'informatica', 'unix', 'etl process']",2025-06-13 05:42:32
Data Engineer,Accenture,5 - 10 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on data architecture and engineering tasks to support business operations and decision-making.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Develop and maintain data pipelines for efficient data processing.- Implement ETL processes to ensure seamless data migration and deployment.- Collaborate with cross-functional teams to design and optimize data solutions.- Conduct data quality assessments and implement improvements for data integrity.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data architecture principles.- Experience in designing and implementing data solutions.- Proficient in SQL and other data querying languages.- Knowledge of cloud platforms such as AWS or Azure.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Chennai office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'sql', 'data architecture principles', 'etl', 'aws', 'hive', 'python', 'data processing', 'airflow', 'microsoft azure', 'pyspark', 'data warehousing', 'data integrity', 'data migration', 'data engineering', 'data quality', 'spark', 'hadoop', 'business operations', 'big data', 'etl process']",2025-06-13 05:42:34
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Informatica Data Quality\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems. You will collaborate with team members to enhance data workflows and contribute to the overall efficiency of data management practices.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist in the design and implementation of data architecture to support data initiatives.- Monitor and optimize data pipelines for performance and reliability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Informatica Data Quality.- Strong understanding of data integration techniques and ETL processes.- Experience with data profiling and data cleansing methodologies.- Familiarity with database management systems and SQL.- Knowledge of data governance and data quality best practices.\nAdditional Information:- The candidate should have minimum 3 years of experience in Informatica Data Quality.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['informatica data quality', 'sql', 'etl', 'data integration', 'etl process', 'hive', 'python', 'data management', 'data architecture', 'data engineering', 'data cleansing', 'database management', 'profiling', 'spark', 'data governance', 'hadoop', 'big data', 'informatica', 'data profiling']",2025-06-13 05:42:36
IN Senior Associate GenAI S/W Engineer- Data and Analytics,PwC Service Delivery Center,1 - 7 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\n& Summary\nJob Overview\nWe are seeking a highly skilled and versatile polyglot Full Stack Developer with expertise in modern frontend and backend technologies, cloudbased solutions, AI/ML and Gen AI. The ideal candidate will have a strong foundation in fullstack development, cloud platforms (preferably Azure), and handson experience in Gen AI, AI and machine learning technologies.\nKey Responsibilities\nDevelop and maintain web applications using Angular / React.js , .NET , and Python .\nDesign, deploy, and optimize Azure native PaaS and SaaS services, including but not limited to Function Apps , Service Bus , Storage Accounts , SQL Databases , Key vaults, ADF, Data Bricks and REST APIs with Open API specifications.\nImplement security best practices for data in transit and rest. Authentication best practices SSO, OAuth 2.0 and Auth0.\nUtilize Python for developing data processing and advanced AI/ML models using libraries like pandas , NumPy , scikitlearn and Langchain , Llamaindex , Azure OpenAI SDK\nLeverage Agentic frameworks like Crew AI, Autogen etc.\nWell versed with RAG and Agentic Architecture.\nStrong in Design patterns Architectural, Data, Object oriented\nLeverage azure serverless components to build highly scalable and efficient solutions.\nCreate, integrate, and manage workflows using Power Platform , including Power Automate , Power Pages , and SharePoint .\nApply expertise in machine learning , deep learning , and Generative AI to solve complex problems.\nPrimary Skills\nProficiency in React.js , .NET , and Python .\nStrong knowledge of Azure Cloud Services , including serverless architectures and data security.\nExperience with Python Data Analytics libraries\npandas\nNumPy\nscikitlearn\nMatplotlib\nSeaborn\nExperience with Python Generative AI Frameworks\nLangchain\nLlamaIndex\nCrew AI\nAutoGen\nFamiliarity with REST API design , Swagger documentation , and authentication best practices .\nSecondary Skills\nExperience with Power Platform tools such as Power Automate, Power Pages, and SharePoint integration.\nKnowledge of Power BI for data visualization (preferred).\nPreferred Knowledge Areas Nice to have\nIndepth understanding of Machine Learning , deep learning, supervised, unsupervised algorithms.\nMandatory skill sets\nAI, ML\nPreferred skill sets\nAI, ML\nYears of experience required\n3 7 years\nEducation qualification\nBE/BTECH, ME/MTECH, MBA, MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration, Bachelor of Engineering, Master of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nGame AI\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 28 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Front end', 'Architecture', 'Data modeling', 'data security', 'Machine learning', 'data visualization', 'Apache', 'SQL', 'Python', 'Data architecture']",2025-06-13 05:42:38
IN Senior Associate Azure Data Engineer Data & Analaytics,PwC Service Delivery Center,2 - 5 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nDesign, develop, and optimize data pipelines and ETL processes using PySpark or Scala to extract, transform, and load large volumes of structured and unstructured data from diverse sources.\nImplement data ingestion, processing, and storage solutions on Azure cloud platform, leveraging services such as Azure Databricks, Azure Data Lake Storage, and Azure Synapse Analytics.\nDevelop and maintain data models, schemas, and metadata to support efficient data access, query performance, and analytics requirements.\nMonitor pipeline performance, troubleshoot issues, and optimize data processing workflows for scalability, reliability, and costeffectiveness.\nImplement data security and compliance measures to protect sensitive information and ensure regulatory compliance.\nRequirement\nProven experience as a Data Engineer, with expertise in building and optimizing data pipelines using PySpark, Scala, and Apache Spark.\nHandson experience with cloud platforms, particularly Azure, and proficiency in Azure services such as Azure Databricks, Azure Data Lake Storage, Azure Synapse Analytics, and Azure SQL Database.\nStrong programming skills in Python and Scala, with experience in software development, version control, and CI/CD practices.\nFamiliarity with data warehousing concepts, dimensional modeling, and relational databases (e.g., SQL Server, PostgreSQL, MySQL).\nExperience with big data technologies and frameworks (e.g., Hadoop, Hive, HBase) is a plus.\nMandatory skill sets\nSpark, Pyspark, Azure\nPreferred skill sets\nSpark, Pyspark, Azure\nYears of experience required\n4 8\nEducation qualification\nB.Tech / M.Tech / MBA / MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nMicrosoft Azure\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'Data modeling', 'Postgresql', 'MySQL', 'Database administration', 'Agile', 'Apache', 'Business intelligence', 'SQL', 'Python']",2025-06-13 05:42:39
"Senior Data Engineer ( T-SQL & SSIS,Data Warehousing & ETL Specialist)",Synechron,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job Summary\nSynechron is seeking a highly skilled Senior Data Engineer specializing in T-SQL and SSIS to lead and advance our data integration and warehousing initiatives. In this role, you will design, develop, and optimize complex ETL processes and database solutions to support enterprise data needs. Your expertise will enable efficient data flow, ensure data integrity, and facilitate actionable insights, contributing to our organizations commitment to data-driven decision-making and operational excellence.\nSoftware Requirements\nOverall Responsibilities\nTechnical Skills (By Category)\nExperience Requirements\nDay-to-Day Activities\nQualifications\nProfessional Competencies",,,,"['Data Engineering', 'T-SQL', 'Azure Data Factory', 'query optimization', 'performance tuning', 'database security', 'AWS Glue', 'Data Warehousing', 'SSIS', 'ETL']",2025-06-13 05:42:41
Data Engineer- MS Fabric,InfoCepts,5 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Position: Data Engineer - MS Fabric\nPurpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\nLocation: Bangalore/ Pune/ Nagpur/ Chennai\nType of Employment: FTE",,,,"['Performance tuning', 'Data modeling', 'Coding', 'XML', 'Scheduling', 'Data quality', 'JSON', 'Business intelligence', 'Data architecture', 'Python']",2025-06-13 05:42:44
Data Engineer - Databricks,KPI Partners,3 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About KPI Partners.\nKPI Partners is a leading provider of data analytics solutions, dedicated to helping organizations transform data into actionable insights. Our innovative approach combines advanced technology with expert consulting, allowing businesses to leverage their data for improved performance and decision-making.\n\nJob Description.\nWe are seeking a skilled and motivated Data Engineer with experience in Databricks to join our dynamic team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and data processing solutions that support our analytics initiatives. You will collaborate closely with data scientists, analysts, and other engineers to ensure the consistent flow of high-quality data across our platforms.",,,,"['python', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'data pipeline', 'data architecture', 'data engineering', 'sql', 'data bricks', 'cloud', 'analytics', 'data quality', 'data modeling', 'gcp', 'teamwork', 'integration', 'aws', 'etl', 'programming', 'communication skills', 'etl scripts']",2025-06-13 05:42:45
Senior PySpark Data Engineer,Synechron,7 - 12 years,Not Disclosed,"['Pune', 'Hinjewadi']","Job Summary\nSynechron is seeking an experienced and technically proficient Senior PySpark Data Engineer to join our data engineering team. In this role, you will be responsible for developing, optimizing, and maintaining large-scale data processing solutions using PySpark. Your expertise will support our organizations efforts to leverage big data for actionable insights, enabling data-driven decision-making and strategic initiatives.\nSoftware Requirements\nRequired Skills:\nProficiency in PySpark\nFamiliarity with Hadoop ecosystem components (e.g., HDFS, Hive, Spark SQL)\nExperience with Linux/Unix operating systems\nData processing tools like Apache Kafka or similar streaming platforms\nPreferred Skills:\nExperience with cloud-based big data platforms (e.g., AWS EMR, Azure HDInsight)\nKnowledge of Python (beyond PySpark), Java or Scala relevant to big data applications\nFamiliarity with data orchestration tools (e.g., Apache Airflow, Luigi)\nOverall Responsibilities\nDesign, develop, and optimize scalable data processing pipelines using PySpark.\nCollaborate with data engineers, data scientists, and business analysts to understand data requirements and deliver solutions.\nImplement data transformations, aggregations, and extraction processes to support analytics and reporting.\nManage large datasets in distributed storage systems, ensuring data integrity, security, and performance.\nTroubleshoot and resolve performance issues within big data workflows.\nDocument data processes, architectures, and best practices to promote consistency and knowledge sharing.\nSupport data migration and integration efforts across varied platforms.\nStrategic Objectives:\nEnable efficient and reliable data processing to meet organizational analytics and reporting needs.\nMaintain high standards of data security, compliance, and operational durability.\nDrive continuous improvement in data workflows and infrastructure.\nPerformance Outcomes & Expectations:\nEfficient processing of large-scale data workloads with minimum downtime.\nClear, maintainable, and well-documented code.\nActive participation in team reviews, knowledge transfer, and innovation initiatives.\nTechnical Skills (By Category)\nProgramming Languages:\nRequired: PySpark (essential); Python (needed for scripting and automation)\nPreferred: Java, Scala\nDatabases/Data Management:\nRequired: Experience with distributed data storage (HDFS, S3, or similar) and data warehousing solutions (Hive, Snowflake)\nPreferred: Experience with NoSQL databases (Cassandra, HBase)\nCloud Technologies:\nRequired: Familiarity with deploying and managing big data solutions on cloud platforms such as AWS (EMR), Azure, or GCP\nPreferred: Cloud certifications\nFrameworks and Libraries:\nRequired: Spark SQL, Spark MLlib (basic familiarity)\nPreferred: Integration with streaming platforms (e.g., Kafka), data validation tools\nDevelopment Tools and Methodologies:\nRequired: Version control systems (e.g., Git), Agile/Scrum methodologies\nPreferred: CI/CD pipelines, containerization (Docker, Kubernetes)\nSecurity Protocols:\nOptional: Basic understanding of data security practices and compliance standards relevant to big data management\nExperience Requirements\nMinimum of 7+ years of experience in big data environments with hands-on PySpark development.\nProven ability to design and implement large-scale data pipelines.\nExperience working with cloud and on-premises big data architectures.\nPreference for candidates with domain-specific experience in finance, banking, or related sectors.\nCandidates with substantial related experience and strong technical skills in big data, even from different domains, are encouraged to apply.\nDay-to-Day Activities\nDevelop, test, and deploy PySpark data processing jobs to meet project specifications.\nCollaborate in multi-disciplinary teams during sprint planning, stand-ups, and code reviews.\nOptimize existing data pipelines for performance and scalability.\nMonitor data workflows, troubleshoot issues, and implement fixes.\nEngage with stakeholders to gather new data requirements, ensuring solutions are aligned with business needs.\nContribute to documentation, standards, and best practices for data engineering processes.\nSupport the onboarding of new data sources, including integration and validation.\nDecision-Making Authority & Responsibilities:\nIdentify performance bottlenecks and propose effective solutions.\nDecide on appropriate data processing approaches based on project requirements.\nEscalate issues that impact project timelines or data integrity.\nQualifications\nBachelors degree in Computer Science, Information Technology, or related field. Equivalent experience considered.\nRelevant certifications are preferred: Cloudera, Databricks, AWS Certified Data Analytics, or similar.\nCommitment to ongoing professional development in data engineering and big data technologies.\nDemonstrated ability to adapt to evolving data tools and frameworks.\nProfessional Competencies\nStrong analytical and problem-solving skills, with the ability to model complex data workflows.\nExcellent communication skills to articulate technical solutions to non-technical stakeholders.\nEffective teamwork and collaboration in a multidisciplinary environment.\nAdaptability to new technologies and emerging trends in big data.\nAbility to prioritize tasks effectively and manage time in fast-paced projects.\nInnovation mindset, actively seeking ways to improve data infrastructure and processes.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PySpark', 'S3', 'Unix operating systems', 'Spark SQL', 'Luigi', 'HDFS', 'AWS EMR', 'Apache Airflow', 'Hive', 'Linux', 'Azure HDInsight', 'Apache Kafka', 'AWS']",2025-06-13 05:42:47
Data Engineer Sr. Analyst,Accenture,5 - 7 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Databricks including Spark-based ETL, Delta Lake\n\n\n\n\nGood to have skills:Pyspark\n\n\n\nJob\n\n\nSummary\n\nWe are seeking a highly skilled and experienced Senior Data Engineer to join our growing Data and Analytics team. The ideal candidate will have deep expertise in Databricks and cloud data warehousing, with a proven track record of designing and building scalable data pipelines, optimizing data architectures, and enabling robust analytics capabilities. This role involves working collaboratively with cross-functional teams to ensure the organization leverages data as a strategic asset. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesign, build, and maintain scalable data pipelines and ETL processes using Databricks and other modern tools.\nArchitect, implement, and manage cloud-based data warehousing solutions on Databricks (Lakehouse Architecture)\nDevelop and maintain optimized data lake architectures to support advanced analytics and machine learning use cases.\nCollaborate with stakeholders to gather requirements, design solutions, and ensure high-quality data delivery.\nOptimize data pipelines for performance and cost efficiency.\nImplement and enforce best practices for data governance, access control, security, and compliance in the cloud.\nMonitor and troubleshoot data pipelines to ensure reliability and accuracy.\nLead and mentor junior engineers, fostering a culture of continuous learning and innovation.\nExcellent communication skills\nAbility to work independently and along with client based out of western Europe.\n\n\n\nProfessional and Technical Skills\n3.5-5 years of experience in Data Engineering roles with a focus on cloud platforms.\nProficiency in Databricks, including Spark-based ETL, Delta Lake, and SQL.\nStrong experience with one or more cloud platforms (AWS preferred).\nHandson Experience with Delta lake, Unity Catalog, and Lakehouse architecture concepts.\nStrong programming skills in Python and SQL; experience with Pyspark a plus.\nSolid understanding of data modeling concepts and practices (e.g., star schema, dimensional modeling).\nKnowledge of CI/CD practices and version control systems (e.g., Git).\nFamiliarity with data governance and security practices, including GDPR and CCPA compliance.\n\n\n\n\nAdditional Information\nExperience with Airflow or similar workflow orchestration tools.\nExposure to machine learning workflows and MLOps.\nCertification in Databricks, AWS\nFamiliarity with data visualization tools such as Power BI\n\n(do not remove the hyperlink)Qualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data warehousing', 'sql', 'data modeling', 'python', 'data bricks', 'hive', 'kubernetes', 'catalog', 'pyspark', 'data architecture', 'docker', 'ansible', 'git', 'java', 'spark', 'devops', 'hadoop', 'etl', 'big data', 'data lake', 'airflow', 'power bi', 'cloud platforms', 'machine learning', 'data engineering', 'aws']",2025-06-13 05:42:49
Hadoop Data Engineer/ Senior Software Engineer,Hsbc,2 - 11 years,Not Disclosed,['Pune'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Software Engineer\nIn this role you will be\nExpertise in Scala-Spark/Python-Spark development and should be able to Work with Agile application dev team to implement data strategies.\nDesign and implement scalable data architectures to support the banks data needs.\nDevelop and maintain ETL (Extract, Transform, Load) processes.\nEnsure the data infrastructure is reliable, scalable, and secure.\nOversee the integration of diverse data sources into a cohesive data platform.\nEnsure data quality, data governance, and compliance with regulatory requirements.\nMonitor and optimize data pipeline performance.\nTroubleshoot and resolve data-related issues promptly.\nImplement monitoring and alerting systems for data processes.\nTroubleshoot and resolve technical issues optimizing system performance ensuring reliability.\nCreate and maintain technical documentation for new and existing system ensuring that information is accessible to the team.\nImplementing and monitoring solutions that identify both system bottlenecks and production issues.\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nExperience in data engineering or related field and hands-on experience of building and maintenance of ETL Data pipelines\nGood experience in Designing and Developing Spark Applications using Scala or Python.\nGood experience with database technologies (SQL, NoSQL), data warehousing solutions, and big data technologies (Hadoop, Spark)\nProficiency in programming languages such as Python, Java, or Scala.\nOptimization and Performance Tuning of Spark Applications\nGIT Experience on creating, merging and managing Repos.\nPerform unit testing and performance testing.\nGood understanding of ETL processes and data pipeline orchestration tools like Airflow, Control-M.\nStrong problem-solving skills and ability to work under pressure.\nExcellent communication and interpersonal skills.",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Performance testing', 'Agile', 'Control-M', 'Data quality', 'Unit testing', 'Financial services', 'SQL', 'Python', 'Technical documentation']",2025-06-13 05:42:51
IN_Director_Senior Data Architect_Data & Analytics_Advisory_ Bangalore,PwC Service Delivery Center,12 - 18 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nDirector\n& Summary\n\n\n.\n& Summary We are seeking an experienced Senior Data Architect to lead the design and development of our data architecture, leveraging cloudbased technologies, big data processing frameworks, and DevOps practices. The ideal candidate will have a strong background in data warehousing, data pipelines, performance optimization, and collaboration with DevOps teams.\nResponsibilities\n1. Design and implement endtoend data pipelines using cloudbased services (AWS/ GCP/Azure) and conventional data processing frameworks.\n2. Lead the development of data architecture, ensuring scalability, security, and performance.\n3. Collaborate with crossfunctional teams, including DevOps, to design and implement data lakes, data warehouses, and data ingestion/extraction processes. 4. Develop and optimize data processing workflows using PySpark, Kafka, and other big data processing frameworks.\n5. Ensure data quality, integrity, and security across all data pipelines and architectures.\n6. Provide technical leadership and guidance to junior team members.\n7. Design and implement data load strategies, data partitioning, and data storage solutions.\n8. Collaborate with stakeholders to understand business requirements and develop data solutions to meet those needs.\n9. Work closely with DevOps team to ensure seamless integration of data pipelines with overall system architecture.\n10. Participate in design and implementation of CI/CD pipelines for data workflows.\nDevOps Requirements\n1. Knowledge of DevOps practices and tools, such as Jenkins, GitLab CI/CD, or Apache Airflow.\n2. Experience with containerization using Docker.\n3. Understanding of infrastructure as code (IaC) concepts using tools like Terraform or AWS CloudFormation.\n4. Familiarity with monitoring and logging tools, such as Prometheus, Grafana, or ELK Stack.\nRequirements\n1. 1214 years of experience for Senior Data Architect in data architecture, data warehousing, and big data processing.\n2. Strong expertise in cloudbased technologies (AWS/ GCP/ Azure) and data processing frameworks (PySpark, Kafka, Flink , Beam etc.).\n3. Experience with data ingestion, data extraction, data warehousing, and data lakes.\n4. Strong understanding of performance optimization, data partitioning, and data storage solutions.\n5. Excellent leadership and communication skills.\n6. Experience with NoSQL databases is a plus.\nMandatory skill sets\n1. Experience with agile development methodologies.\n2. Certification in cloudbased technologies (AWS / GCP/ Azure) or data processing frameworks.\n3. Experience with data governance, data quality, and data security.\nPreferred skill sets\nKnowledge of AgenticAI and GenAI is added advantage\nYears of experience required\n12 to 18 years\nEducation qualification\nGraduate Engineer or Management Graduate\nEducation\nDegrees/Field of Study required Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Coaching and Feedback, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Influence, Innovation, Intellectual Curiosity, Learning Agility {+ 28 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['System architecture', 'Business transformation', 'GCP', 'Analytical', 'Consulting', 'Data processing', 'Data quality', 'Operations', 'Monitoring', 'Data architecture']",2025-06-13 05:42:53
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, encompassing the relevant data platform components. You will collaborate with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Your typical day will involve working on the data platform blueprint and design, collaborating with architects, and ensuring seamless integration between systems and data models.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects.- Ensure cohesive integration between systems and data models.- Implement data platform components.- Troubleshoot and resolve data platform issues.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'data architecture', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining']",2025-06-13 05:42:54
Data Platform Engineer,Accenture,12 - 15 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Collibra Data Governance\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, encompassing the relevant data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, while also engaging in discussions to refine and enhance the overall data architecture. You will be involved in various stages of the data platform lifecycle, ensuring that all components work harmoniously to support the organization's data needs and objectives.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities and foster a culture of continuous improvement.- Monitor and evaluate team performance, providing constructive feedback to ensure alignment with project goals.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Collibra Data Governance.- Strong understanding of data governance frameworks and best practices.- Experience with data integration tools and techniques.- Familiarity with data modeling concepts and methodologies.- Ability to analyze and interpret complex data sets to inform decision-making.\nAdditional Information:- The candidate should have minimum 12 years of experience in Collibra Data Governance.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data architecture', 'sql', 'data modeling', 'data governance', 'data analysis', 'oracle', 'data management', 'data warehousing', 'business analysis', 'machine learning', 'business intelligence', 'javascript', 'sql server', 'data quality', 'tableau', 'java', 'html', 'mysql', 'etl', 'informatica']",2025-06-13 05:42:56
Data Modeler,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Microsoft Power Business Intelligence (BI)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Modeler, you will engage with key business representatives, data owners, end users, application designers, and data architects to model both current and new data. Your typical day will involve collaborating with various stakeholders to understand their data needs, analyzing existing data structures, and designing effective data models that support business objectives. You will also be responsible for ensuring that the data models are aligned with best practices and meet the requirements of the organization, facilitating seamless data integration and accessibility across different platforms.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate training sessions for junior team members to enhance their understanding of data modeling.- Continuously evaluate and improve data modeling processes to ensure efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Power Business Intelligence (BI).- Strong understanding of data modeling concepts and best practices.- Experience with data integration techniques and tools.- Familiarity with database management systems and SQL.- Ability to communicate complex data concepts to non-technical stakeholders.\nAdditional Information:- The candidate should have minimum 5 years of experience in Microsoft Power Business Intelligence (BI).- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business intelligence', 'sql', 'database management', 'data modeling', 'data integration', '3d modeling', 'tekla structures', '3ds max', 'python', '3d modeler', 'oracle', 'texturing', 'bi', 'data warehousing', 'sme', 'photoshop', 'autocad', 'sql server', 'maya', 'plsql', 'modeler', 'etl', 'tekla', 'informatica']",2025-06-13 05:42:58
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analysis', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:43:00
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines and ETL processes using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls for the data platform.- Troubleshoot and resolve issues related to the data platform and data pipelines.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data visualization tools such as Tableau or Power BI.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data analytics', 'microsoft azure', 'design principles', 'hive', 'sql', 'java', 'spark', 'design patterns', 'oops', 'mysql', 'hadoop', 'etl', 'big data', 'c#', 'rest', 'python', 'data security', 'power bi', 'javascript', 'sql server', 'data bricks', 'tableau', 'kafka', 'sqoop', 'aws']",2025-06-13 05:43:02
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the implementation of data platform solutions.- Conduct performance tuning and optimization of data platform components.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of cloud-based data platforms.- Experience in designing and implementing data pipelines.- Knowledge of data governance and security best practices.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'data analytics', 'data modeling', 'spark', 'data governance', 'python', 'amazon redshift', 'data warehousing', 'microsoft azure', 'emr', 'machine learning', 'sql', 'nosql', 'amazon ec2', 'java', 'kafka', 'mysql', 'hadoop', 'sqoop', 'big data', 'aws', 'etl']",2025-06-13 05:43:04
Data Platform Engineer,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:43:06
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:43:08
Data Platform Engineer,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :A Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:Overall 7+ years of experience In Industry including 4 Years of experience As Developer using Big Data Technologies like Databricks/Spark and Hadoop Ecosystems - Hands on experience on Unified Data Analytics with Databricks, Databricks Workspace User Interface, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL - Good understanding of Spark Architecture with Databricks, Structured Streaming.\nSetting Up cloud platform with Databricks, Databricks Workspace- Working knowledge on distributed processing, data warehouse concepts, NoSQL, huge amount of data processing, RDBMS, Testing, Data management principles, Data mining and Data modellingAs a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data pipelines and data platform components.- Ensure data quality and integrity by implementing data validation and testing procedures.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with Apache Spark and Hadoop.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Proficiency in programming languages such as Python or Java.- Experience with data integration and ETL tools such as Apache NiFi or Talend.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science, software engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Chennai, Bengaluru, Hyderabad and Pune office.\n\nQualification\n\nA Engineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'software engineering', 'design principles', 'python', 'rdbms', 'data management', 'talend', 'microsoft azure', 'nosql', 'spark programming', 'data bricks', 'data quality', 'java', 'apache nifi', 'spark', 'hadoop', 'big data', 'aws', 'etl', 'data integration']",2025-06-13 05:43:09
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:43:11
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with Integration Architects and Data Architects to design and implement data platform components.- Ensure seamless integration between various systems and data models.- Develop and maintain data platform blueprints.- Optimize data platform performance and scalability.- Provide technical guidance and support to team members.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data platform architecture and design principles.- Experience with cloud-based data platforms like AWS or Azure.- Hands-on experience with data integration tools and technologies.- Knowledge of data governance and security best practices.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'microsoft azure', 'platform architecture', 'design principles', 'aws', 'kubernetes', 'c++', 'oracle', 'enterprise architecture', 'microservices', 'docker', 'infrastructure architecture', 'java', 'data modeling', 'gcp', 'design patterns', 'data governance', 'agile', 'hadoop']",2025-06-13 05:43:13
IN_Manager_Azure Data Engineer_Data & Analytics_Advisory_Bangalore,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nDesign, develop, and optimize data pipelines and ETL processes using PySpark or Scala to extract, transform, and load large volumes of structured and unstructured data from diverse sources. Implement data ingestion, processing, and storage solutions on Azure cloud platform, leveraging services such as Azure Databricks, Azure Data Lake Storage, and Azure Synapse Analytics. Develop and maintain data models, schemas, and metadata to support efficient data access, query performance, and analytics requirements. Monitor pipeline performance, troubleshoot issues, and optimize data processing workflows for scalability, reliability, and costeffectiveness. Implement data security and compliance measures to protect sensitive information and ensure regulatory compliance. Requirement Proven experience as a Data Engineer, with expertise in building and optimizing data pipelines using PySpark, Scala, and Apache Spark. Handson experience with cloud platforms, particularly Azure, and proficiency in Azure services such as Azure Databricks, Azure Data Lake Storage, Azure Synapse Analytics, and Azure SQL Database. Strong programming skills in Python and Scala, with experience in software development, version control, and CI/CD practices. Familiarity with data warehousing concepts, dimensional modeling, and relational databases (e.g., SQL Server, PostgreSQL, MySQL).\nExperience with big data technologies and frameworks (e.g., Hadoop, Hive, HBase) is a plus.\nMandatory skill sets\nSpark, Pyspark, Azure\nPreferred skill sets\nSpark, Pyspark, Azure\nYears of experience required\n4 8\nEducation qualification\nB.Tech / M.Tech / MBA / MCA\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Master of Business Administration, Master of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nPython (Programming Language)\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling {+ 32 more}\nNo",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAP', 'Data management', 'Data modeling', 'Postgresql', 'MySQL', 'Database administration', 'Agile', 'Apache', 'Business intelligence', 'Python']",2025-06-13 05:43:15
Deputy Manager - Data Engineer - Analytics,IBM,2 - 7 years,Not Disclosed,['Bengaluru'],"Develop, test and support future-ready data solutions for customers across industry verticals\nDevelop, test, and support end-to-end batch and near real-time data flows/pipelines\nDemonstrate understanding in data architectures, modern data platforms, big data, analytics, cloud platforms, data governance and information management and associated technologies\nCommunicates risks and ensures understanding of these risks.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMinimum of 2+ years of related experience required\nExperience in modeling and business system designs\nGood hands-on experience on DataStage, Cloud based ETL Services\nHave great expertise in writing TSQL code\nWell versed with data warehouse schemas and OLAP techniques\n\n\nPreferred technical and professional experience\nAbility to manage and make decisions about competing priorities and resources.\nAbility to delegate where appropriate\nMust be a strong team player/leader\nAbility to lead Data transformation project with multiple junior data engineers\nStrong oral written and interpersonal skills for interacting and throughout all levels of the organization.\nAbility to clearly communicate complex business problems and technical solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'cloud platforms', 'data architecture', 'data governance', 'big data', 'schema', 'python', 'data analytics', 'datastage', 'microsoft azure', 'warehouse', 't-sql', 'data engineering', 'ansible', 'docker', 'sql', 'java', 'devops', 'linux', 'olap', 'jenkins', 'shell scripting', 'etl', 'aws']",2025-06-13 05:43:17
Data Engineer,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Google Cloud Data Services\n\n\n\n\nGood to have skills :GCP Dataflow, Data EngineeringMinimum\n\n\n\n2 year(s) of experience is required\n\n\n\n\nEducational Qualification :standard 15 years\n\n\nSummary:As a Data Engineer, you will be responsible for designing, developing, and maintaining data solutions for data generation, collection, and processing. Your role involves creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across systems. You will play a crucial part in the data management process.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Develop and maintain data solutions for data generation, collection, and processing.- Create data pipelines to streamline data flow.- Ensure data quality and integrity throughout the data lifecycle.- Implement ETL processes for data migration and deployment.- Collaborate with cross-functional teams to optimize data processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Google Cloud Data Services.- Good To Have\n\n\n\n\nSkills:\nExperience with Data Engineering and GCP Dataflow.- Strong understanding of cloud-based data services.- Experience in designing and implementing data pipelines.- Knowledge of ETL processes and data migration techniques.\nAdditional Information:- The candidate should have a minimum of 2 years of experience in Google Cloud Data Services.- This position is based at our Bengaluru office.- A standard 15 years of education is required.\n\nQualification\n\nstandard 15 years",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['gcp', 'etl', 'data services', 'google', 'data engineering', 'hive', 'data management', 'data warehousing', 'data migration', 'business intelligence', 'sql', 'plsql', 'data modeling', 'spark', 'hadoop', 'big data', 'python', 'sql server', 'data quality', 'tableau', 'aws', 'ssis', 'data flow', 'informatica', 'etl process']",2025-06-13 05:43:19
Lead Data Management Analyst,Wells Fargo,5 - 10 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Lead Data Management Analyst. We believe in the power of working together because great ideas can come from anyone. Through collaboration, any employee can have an impact and make a difference for the entire company. Explore opportunities with us for a career in a supportive environment where you can learn and grow. This role requires a blend of technical expertise, analytical thinking, and strategic decision making to drive impactful insights.\nAt Wells Fargo, we are looking for talented people who will put our customers at the center of everything we do. We are seeking candidates who embrace diversity, equity and inclusion in a workplace where everyone feels valued and inspired. Help us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.",,,,"['Data Management', 'Hive', 'Power BI', 'DB2', 'SQL Server', 'Tableau', 'Oracle', 'Teradata', 'Analytics', 'Python', 'Business Analysis']",2025-06-13 05:43:20
"Specialist, Data Architecture",Fiserv,8 - 12 years,Not Disclosed,['Noida'],"Responsibilities\nRequisition ID R-10358179 Date posted 06/11/2025 End Date 07/15/2025 City Noida State/Region Uttar Pradesh Country India Location Type Onsite\nCalling all innovators find your future at Fiserv.\nJob Title\nSpecialist, Data Architecture\nWhat does a successful Lead, Data Conversions do\nA Conversion Lead is responsible for timely and accurate conversion of new and existing Bank/Client data to Fiserv systems, from both internal and external sources. This role is responsible to provide data analysis for client projects and to accommodate other ad hoc data updates to meet client requests.\nAs part of the overall Service Delivery organization, a Conversion Lead plays a critical role in mapping in data to support project initiatives for new and existing banks. Leads provide a specialized service to the Project Manager teams developing custom reporting, providing technical assistance, and ensuring project timelines are met.\nWorking with financial services data means a high priority on accuracy and adherence to procedures and guidelines.\nWhat you will do\nA Conversion Lead is responsible for timely and accurate conversion of new and existing Bank/Client data to Fiserv systems, from both internal and external sources. This role is responsible to provide data analysis for client projects and to accommodate other ad hoc data updates to meet client requests.\nAs part of the overall Service Delivery organization, a Conversion Lead plays a critical role in mapping in data to support project initiatives for new and existing banks/clients. Lead provides a specialized service to the Project Manager teams developing custom reporting, providing technical assistance, and ensuring project timelines are met.\nWorking with financial services data means a high priority on accuracy and adherence to procedures and guidelines.\nThe person stepping in as the backup would need to review the specifications history and then review and understand the code that was being developed to resolve the issue and or change. This would also have to occur on the switch back to the original developer.\nToday, the associate handling the project would log back in to support the effort and address the issue and or change.\nWhat you will need to have\nBachelor s degree in programming or related field\nWorking Hours (IST):\n12:00 p.m. 09:00 p.m. (IST)\nMonday through Friday\nHighest attention to detail and accuracy\nTeam player with ability to work independently\nAbility to manage and prioritize work queue across multiple workstreams\nStrong communication skills and ability to provide technical information to non-technical colleagues\nWhat would be great to have\nExperience with Data Modelling, Informatica, Power BI, MS Visual Basic, Microsoft Access and Microsoft Excel required.\nExperience with Card Management systems, debit card processing is a plus\nUnderstanding Applications and related database features that can be leveraged to improve performance\nExperience of creating testing artifacts (test cases, test plans) and knowledge of various testing types.\n8 12 years Experience and strong knowledge of MS SQL/PSQL, MS SSIS and Data warehousing concepts\nShould have strong database fundamentals and Expert knowledge in writing SQL commands, queries and stored procedures\nExperience in Performance Tuning of SQL complex queries.\nStrong communication skills and ability to provide technical information to non-technical colleagues.\nAbility to mentor junior team members\nAbility to manage and prioritize work queue across multiple workstreams.\nTeam player with ability to work independently.\nExperience in full software development life cycle using agile methodologies.\nShould have good understanding of Agile methodologies and can handle agile ceremonies.\nEfficient in Reviewing, Analyzing, coding, testing, and debugging of application programs.\nShould be able to work under pressure while resolving critical issues in Prod environment.\nGood communication skills and experience in working with Clients.\nGood understanding in Banking Domain.\nMinimum 8 years relevant experience in data processing (ETL) conversions or financial services industry\nThank you for considering employment with Fiserv. Please:\nApply using your legal name\nComplete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\nOur commitment to Diversity and Inclusion:\nFiserv is proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, gender, gender identity, sexual orientation, age, disability, protected veteran status, or any other category protected by law.\nNote to agencies:\nFiserv does not accept resume submissions from agencies outside of existing agreements. Please do not send resumes to Fiserv associates. Fiserv is not responsible for any fees associated with unsolicited resume submissions.\nWarning about fake job posts:\nPlease be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information. Any communications from a Fiserv representative will come from a legitimate Fiserv email address.\n\n\n\nShare this Job\nEmail\nLinkedIn\nX\nFacebook",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'MS SQL', 'Visual Basic', 'Coding', 'Debugging', 'Agile', 'Informatica', 'Stored procedures', 'SSIS', 'SQL']",2025-06-13 05:43:22
"Specialist, Data Architecture",Fiserv,3 - 5 years,Not Disclosed,['Pune'],"A Conversion Professional is responsible for timely and accurate conversion of new and existing Bank/Client data to Fiserv systems, from both internal and external sources. This role is responsible for providing data analysis for client projects and to accommodate other ad hoc data updates to meet client requests.\nAs part of the overall Service Delivery organization, a Conversion Professional plays a critical role in mapping in data to support project initiatives for new and existing banks.\nWorking with financial services data means a high priority on accuracy and adherence to procedures and guidelines.\nWhat will you do\nA Conversion Professional is responsible for timely and accurate conversion of new and existing Bank/Client data to Fiserv systems, from both internal and external sources. This role is responsible for providing data analysis for client projects and to accommodate other ad hoc data updates to meet client requests.\nAs part of the overall Service Delivery organization, a Conversion Professional plays a critical role in mapping in data to support project initiatives for new and existing banks.\nWorking with financial services data means a high priority on accuracy and adherence to procedures and guidelines.\nThe person stepping in as the backup would need to review the specifications history and then review and understand the code that was being developed to resolve the issue and or change. This would also have to occur on the switch back to the original developer.\nToday, the associate handling the project would log back in to support the effort and address the issue and or change.\nWhat you will need to have\nBachelor s degree in programming or related field\nMinimum 3 years relevant experience in data processing (ETL) conversions or financial services industry\n3 5 years Experience and strong knowledge of MS SQL/PSQL, MS SSIS and data warehousing concepts\nStrong communication skills and ability to provide technical information to non-technical colleagues.\nTeam players with ability to work independently.\nExperience in full software development life cycle using agile methodologies.\nShould have good understanding of Agile methodologies and can handle agile ceremonies.\nEfficient in Reviewing, coding, testing, and debugging of application/Bank programs.\nShould be able to work under pressure while resolving critical issues in Prod environment.\nGood communication skills and experience in working with Clients.\nGood understanding in Banking Domain.\nWhat would be great to have\nExperience with Informatica, Power BI, MS Visual Basic, Microsoft Access and Microsoft Excel required.\nExperience with Card Management systems, debit card processing is a plus\nStrong communication skills and ability to provide technical information to non-technical colleagues\nAbility to manage and prioritize work queue across multiple workstreams\nTeam player with ability to work independently\nHighest attention to detail and accuracy\nThank you for considering employment with Fiserv. Please:\nApply using your legal name\nComplete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\nOur commitment to Diversity and Inclusion:\nFiserv is proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, gender, gender identity, sexual orientation, age, disability, protected veteran status, or any other category protected by law.\nNote to agencies:\nFiserv does not accept resume submissions from agencies outside of existing agreements. Please do not send resumes to Fiserv associates. Fiserv is not responsible for any fees associated with unsolicited resume submissions.\nWarning about fake job posts:\nPlease be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information. Any communications from a Fiserv representative will come from a legitimate Fiserv email address.\n\n\n\nShare this Job\nEmail\nLinkedIn\nX\nFacebook",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MS SQL', 'Data analysis', 'Visual Basic', 'Coding', 'Debugging', 'Agile', 'Informatica', 'SSIS', 'Financial services', 'Data architecture']",2025-06-13 05:43:24
"Specialist, Data Architecture",Fiserv,3 - 5 years,Not Disclosed,['Pune'],"Good Hand on experience on ETL and BI tools like SSIS, SSRS, Power BI etc.\nReadiness to play an individual contributor role on the technical front\nExcellent communication skills\nReadiness to travel onsite for short term, as required\nA good experience in ETL development for 3-5 years and with hands-on experience in a migration or data warehousing project\nShould have strong database fundamentals and experience in writing Unit test cases and test scenarios\nExpert knowledge in writing SQL commands, queries and stored procedures\nGood knowledge of ETL tools like SSIS, Informatica, etc. and data warehousing concepts\nShould have good knowledge in writing macros\nGood client handling skills with preferred onsite experience\nThank you for considering employment with Fiserv. Please:\nApply using your legal name\nComplete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\nOur commitment to Diversity and Inclusion:\nFiserv is proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, gender, gender identity, sexual orientation, age, disability, protected veteran status, or any other category protected by law.\nNote to agencies:\nFiserv does not accept resume submissions from agencies outside of existing agreements. Please do not send resumes to Fiserv associates. Fiserv is not responsible for any fees associated with unsolicited resume submissions.\nWarning about fake job posts:\nPlease be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information. Any communications from a Fiserv representative will come from a legitimate Fiserv email address.\n\n\n\nShare this Job\nEmail\nLinkedIn\nX\nFacebook",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SSRS', 'Test scenarios', 'Informatica', 'Stored procedures', 'Test cases', 'SSIS', 'Macros', 'Data warehousing', 'SQL', 'Data architecture']",2025-06-13 05:43:26
"Data Engineer, Alexa AI Developer Tech",Amazon,3 - 8 years,Not Disclosed,['Pune'],"Alexa+ is our next-generation assistant powered by generative AI. Alexa+ is more conversational, smarter, personalized, and gets things done.\n\nOur goal is make Alexa+ an instantly familiar personal assistant that is always ready to help or entertain on any device. At the core of this vision is Alexa AI Developer Tech, a close-knit team that s dedicated to providing software developers with the tools, primitives, and services they need to easily create engaging customer experiences that expand the wealth of information, products and services available on Alexa+.\n\nYou will join a growing organization working on top technology using Generative AI and have an enormous opportunity to make an impact on the design, architecture, and implementation of products used every day, by people you know.\n\nWe re working hard, having fun, and making history; come join us!\n\n\nWork with a team of product and program managers, engineering leaders, and business leaders to build data architectures and platforms to support business\nDesign, develop, and operate high-scalable, high-performance, low-cost, and accurate data pipelines in distributed data processing platforms\nRecognize and adopt best practices in data processing, reporting, and analysis: data integrity, test design, analysis, validation, and documentation\nKeep up to date with big data technologies, evaluate and make decisions around the use of new or existing software products to design the data architecture\nDesign, build and own all the components of a high-volume data warehouse end to end.\nProvide end-to-end data engineering support for project lifecycle execution (design, execution and risk assessment)\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\nInterface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources\nOwn the functional and nonfunctional scaling of software systems in your ownership area. *Implement big data solutions for distributed computing.\n\nAbout the team\nAlexa AI Developer Tech is an organization within Alexa on a mission to empower developers to create delightful and engaging experiences by making Alexa more natural, accurate, conversational, and personalized. 3+ years of data engineering experience\n4+ years of SQL experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['Data modeling', 'Business design', 'Risk assessment', 'Test design', 'Architectural design', 'Data processing', 'data integrity', 'Design analysis', 'SQL', 'Data architecture']",2025-06-13 05:43:28
Big data Developer,Diverse Lynx,3 - 5 years,Not Disclosed,['Bengaluru'],"Total Yrs. of Experience 8+ Yrs Relevant Yrs. of experience 4+ Yrs Detailed JD (Roles and Responsibilities)\nGather operational Client on business processes and policies from multiple sources\nPrepare periodical and ad-hoc reports using operational data\nDevelop semantic core to align data with business processes\nSupport operations teams work streams for data processing, analysis and reporting\nAnalyse data and Create dashboards for the senior management\nDesign and implement optimal processes\nRegression testing of the releases\nMandatory skills\nBig Data : Spark, Hive, DataBricks\nLanguage : SQL, JAVA /Python\nBI Analytics: Power BI (DAX), Tableau , Dataiku\nOperating System : Unix\nExperience with Data Migration, Data Engineering, Data Analysis\nDesired/ Secondary skills\nBig Data : SCALA, HADOOP\nTools: DB Visualizer, JIRA, GIT, Bitbucket, Control-M\nStrong problem-solving skills and the ability to work independently and in a team environment.\nExcellent communication skills and the ability to work effectively with cross-functional teams.\nDomain eCommerce / Retail Max Vendor Rate in Per Day (Currency in relevance to work location) 11000 INR/Day Delivery Anchor for tracking the sourcing statistics, technical evaluation, interviews and feedback etc. Prem_dason@infosys.com Work Location given in ECMS ID Bangalore (Hyd, Pune, Trivandrum locations also ok)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Data analysis', 'Data migration', 'Control-M', 'Data processing', 'Regression testing', 'Operations', 'Analytics', 'SQL', 'Python']",2025-06-13 05:43:29
SNOWFLAKE DATA ENGINEER,Capgemini,6 - 11 years,Not Disclosed,['Chennai'],"Your Role \n\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.\n1. Applies scientific methods to analyse and solve software engineering problems.\n2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.\n3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.\n4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.\n5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.\n\n Your Profile \n4+ years of experience in data architecture, data warehousing, and cloud data solutions.\nMinimum 3+ years of hands-on experience with End to end Snowflake implementation.\nExperience in developing data architecture and roadmap strategies with knowledge to establish data governance and quality frameworks within Snowflake\nExpertise or strong knowledge in Snowflake best practices, performance tuning, and query optimisation.\nExperience with cloud platforms like AWS or Azure and familiarity with Snowflakes integration with these environments.\nStrong knowledge in at least one cloud(AWS or Azure) is mandatory\nSolid understanding of SQL, Python, and scripting for data processing and analytics.\nExperience in leading teams and managing complex data migration projects.\nStrong communication skills, with the ability to explain technical concepts to non-technical stakeholders.\n\nKnowledge on new Snowflake features,AI capabilities and industry trends to drive innovation and continuous improvement.\n\n  \n\n Skills (competencies) \n\nVerbal Communication",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'python', 'microsoft azure', 'sql', 'aws', 'head hunting', 'screening', 'performance tuning', 'data processing', 'data warehousing', 'hrsd', 'data architecture', 'data migration', 'sourcing', 'talent acquisition', 'it recruitment', 'technical recruitment', 'recruitment', 'data governance']",2025-06-13 05:43:31
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:43:33
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:43:35
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Indore'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:43:37
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:43:39
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:43:41
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:43:43
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:43:44
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:43:47
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:43:49
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with Integration Architects and Data Architects to design and implement data platform components.- Ensure seamless integration between various systems and data models.- Develop and maintain data platform blueprints.- Provide technical expertise in data platform design and implementation.- Troubleshoot and resolve data platform related issues.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data platform architecture and design principles.- Experience in implementing and optimizing data pipelines.- Knowledge of cloud-based data solutions.- Hands-on experience with data platform security and compliance measures.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'java', 'data modeling', 'platform architecture', 'design principles', 'hive', 'python', 'oracle', 'enterprise architecture', 'microsoft azure', 'data warehousing', 'sql', 'docker', 'infrastructure architecture', 'spark', 'gcp', 'design patterns', 'hadoop', 'agile', 'aws', 'big data']",2025-06-13 05:43:51
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data platform components using Databricks Unified Data Analytics Platform.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with other big data technologies such as Hadoop, Spark, and Kafka.- Strong understanding of data modeling and database design principles.- Experience with data security and access controls.- Experience with data pipeline development and maintenance.- Experience with troubleshooting and resolving issues related to data platform components.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'spark', 'data analytics', 'design principles', 'hive', 'sql', 'java', 'design patterns', 'oops', 'mysql', 'hadoop', 'big data', 'etl', 'c#', 'rest', 'python', 'microsoft azure', 'javascript', 'sql server', 'data bricks', 'amazon ec2', 'kafka', 'troubleshooting', 'sqoop', 'aws']",2025-06-13 05:43:53
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data platform components using Databricks Unified Data Analytics Platform.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data pipeline development and maintenance.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Chennai office.\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data analytics', 'microsoft azure', 'design principles', 'hive', 'data warehousing', 'sql', 'java', 'spark', 'design patterns', 'mysql', 'hadoop', 'big data', 'etl', 'rest', 'python', 'data security', 'javascript', 'sql server', 'data bricks', 'pipeline', 'kafka', 'sqoop', 'aws']",2025-06-13 05:43:55
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data platform components using Databricks Unified Data Analytics Platform.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data platform components and architecture.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data pipeline development and maintenance.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.-This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'data security', 'microsoft azure', 'data bricks', 'aws', 'hive', 'amazon redshift', 'pyspark', 'data warehousing', 'emr', 'sql', 'java', 'data modeling', 'spark', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'machine learning', 'sql server', 'nosql', 'pipeline', 'amazon ec2', 'kafka', 'sqoop']",2025-06-13 05:43:57
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the organization.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the organization.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Hyderabad office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:43:59
Data Platform Engineer,Accenture,2 - 7 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 2 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analysis', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:44:01
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analysis', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:44:03
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines and ETL processes using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls for the data platform.- Troubleshoot and resolve issues related to the data platform and data pipelines.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data visualization tools such as Tableau or Power BI.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data analytics', 'microsoft azure', 'design principles', 'hive', 'sql', 'java', 'spark', 'design patterns', 'oops', 'mysql', 'hadoop', 'etl', 'big data', 'c#', 'rest', 'python', 'data security', 'power bi', 'javascript', 'sql server', 'data bricks', 'tableau', 'kafka', 'sqoop', 'aws']",2025-06-13 05:44:05
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead data platform blueprint and design- Implement data platform components effectively- Ensure seamless integration between systems and data models\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform- Strong understanding of data platform architecture- Experience in data integration and data modeling- Knowledge of cloud platforms like AWS or Azure- Hands-on experience with SQL and NoSQL databases\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform- This position is based at our Hyderabad office- An Engineering graduate preferably Computer Science graduate with 15 years of full-time education is required\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'data modeling', 'data analytics', 'platform architecture', 'aws', 'kubernetes', 'continuous integration', 'openshift', 'docker', 'microservices', 'iot', 'java', 'git', 'gcp', 'devops', 'linux', 'jenkins', 'mysql', 'hadoop', 'big data', 'microsoft azure', 'cloud platforms', 'nosql', 'agile', 'data integration']",2025-06-13 05:44:07
Data Product Owner,Capgemini,9 - 14 years,Not Disclosed,['Hyderabad'],"\n\nThe Product Owner III will be responsible for defining and prioritizing features and user stories, outlining acceptance criteria, and collaborating with cross-functional teams to ensure successful delivery of product increments. This role requires strong communication skills to effectively engage with stakeholders, gather requirements, and facilitate product demos.\n\nThe ideal candidate should have a deep understanding of agile methodologies, experience in the insurance sector, and possess the ability to translate complex needs into actionable tasks for the development team.\n\n Key Responsibilities: \nDefine and communicate the  vision, roadmap, and backlog  for data products.\nManages team backlog items and prioritizes based on business value.\nPartners with the business owner to understand needs, manage scope and add/eliminate user stories while contributing heavy influence to build an effective strategy.\nTranslate business requirements into scalable data product features.\nCollaborate with data engineers, analysts, and business stakeholders to prioritize and deliver impactful solutions.\nChampion  data governance , privacy, and compliance best practices.\nAct as the voice of the customer to ensure usability and adoption of data products.\nLead Agile ceremonies (e.g., backlog grooming, sprint planning, demos) and maintain a clear product backlog.\nMonitor data product performance and continuously identify areas for improvement.\nSupport the integration of AI/ML solutions and advanced analytics into product offerings.\n\n\n  \n\n Required Skills & Experience: \nProven experience as a Product Owner, ideally in data or analytics domains.\nStrong understanding of  data engineering ,  data architecture , and  cloud platforms  (AWS, Azure, GCP).\nFamiliarity with  SQL , data modeling, and modern data stack tools (e.g., Snowflake, dbt, Airflow).\nExcellent  stakeholder management  and communication skills across technical and non-technical teams.\nStrong  business acumen  and ability to align data products with strategic goals.\nExperience with  Agile/Scrum methodologies  and working in cross-functional teams.\nAbility to  translate data insights into compelling stories and recommendations .\n\n\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data architecture', 'business acumen', 'data engineering', 'stakeholder management', 'agile methodology', 'snowflake', 'advanced analytics', 'microsoft azure', 'cloud platforms', 'user stories', 'demo', 'sql', 'data modeling', 'gcp', 'sprint planning', 'scrum', 'product performance', 'agile', 'aws', 'ml']",2025-06-13 05:44:08
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\n Your primary responsibilities include: \nDevelop & maintain data pipelines for batch & stream processing using informatica power centre or cloud ETL/ELT tools.\nLiaise with business team and technical leads, gather requirements, identify data sources, identify data quality issues, design target data structures, develop pipelines and data processing routines, perform unit testing and support UAT.\nWork with data scientist and business analytics team to assist in data ingestion and data-related technical issues.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter\nKnowledge of Cloud, Power BI, Data migration on cloud skills.\nExperience in Unix shell scripting and python\nExperience with relational SQL, Big Data etc\n\n\nPreferred technical and professional experience\nKnowledge of MS-Azure Cloud\nExperience in Informatica PowerCenter\nExperience in Unix shell scripting and python",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'elastic search', 'unix shell scripting', 'splunk', 'agile', 'big data', 'informatica']",2025-06-13 05:44:10
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter\nKnowledge of Cloud, Power BI, Data migration on cloud skills.\nExperience in Unix shell scripting and python\nExperience with relational SQL, Big Data etc\n\n\nPreferred technical and professional experience\nKnowledge of MS-Azure Cloud\nExperience in Informatica PowerCenter\nExperience in Unix shell scripting and python",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'data management', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'unix shell scripting', 'java', 'etl tool', 'big data', 'informatica', 'unix']",2025-06-13 05:44:12
Data Engineer Specialist,Accenture,3 - 4 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level :\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python, Pyspark\n\n\n\n\nGood to have skills:Redshift\n\n\n\nJob\n\n\nSummary: We are seeking a highly skilled and experienced Senior Data Engineer to join our growing Data and Analytics team. The ideal candidate will have deep expertise in Databricks and cloud data warehousing, with a proven track record of designing and building scalable data pipelines, optimizing data architectures, and enabling robust analytics capabilities. This role involves working collaboratively with cross-functional teams to ensure the organization leverages data as a strategic asset. Your responsibilities will include:\n\n\n\n\nRoles & Responsibilities\nDesign, build, and maintain scalable data pipelines and ETL processes using Databricks and other modern tools.\nArchitect, implement, and manage cloud-based data warehousing solutions on Databricks (Lakehouse Architecture)\nDevelop and maintain optimized data lake architectures to support advanced analytics and machine learning use cases.\nCollaborate with stakeholders to gather requirements, design solutions, and ensure high-quality data delivery.\nOptimize data pipelines for performance and cost efficiency.\nImplement and enforce best practices for data governance, access control, security, and compliance in the cloud.\nMonitor and troubleshoot data pipelines to ensure reliability and accuracy.\nLead and mentor junior engineers, fostering a culture of continuous learning and innovation.\nExcellent communication skills\nAbility to work independently and along with client based out of western Europe\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 3-4 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:5-8 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'data bricks', 'glue', 'amazon redshift', 'data warehousing', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'snowflake', 'scipy', 'data analysis', 'azure data lake', 'microsoft azure', 'power bi', 'javascript', 'pandas', 'tableau', 'lambda expressions', 'aws']",2025-06-13 05:44:14
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Indore'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Google BigQuery\n\n\n\n\nGood to have skills :Microsoft SQL Server, Google Cloud Data ServicesMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will be responsible for designing, developing, and maintaining data solutions for data generation, collection, and processing. You will create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Develop and maintain data pipelines.- Ensure data quality throughout the data lifecycle.- Implement ETL processes for data migration and deployment.- Collaborate with cross-functional teams to understand data requirements.- Optimize data storage and retrieval processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Google BigQuery.- Strong understanding of data engineering principles.- Experience with cloud-based data services.- Knowledge of SQL and database management systems.- Hands-on experience with data modeling and schema design.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Google BigQuery.- This position is based at our Mumbai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data engineering', 'sql', 'data modeling', 'bigquery', 'etl', 'schema', 'hive', 'data services', 'python', 'amazon redshift', 'data warehousing', 'google', 'data migration', 'knowledge of sql', 'sql server', 'database design', 'data quality', 'tableau', 'spark', 'etl tool', 'hadoop', 'big data', 'aws', 'etl process']",2025-06-13 05:44:16
Data Management Practitioner,Accenture,12 - 17 years,Not Disclosed,['Kolkata'],"Project Role :Data Management Practitioner\n\n\n\n\n\nProject Role Description :Maintain the quality and compliance of an organizations data assets. Design and implement data strategies, ensuring data integrity and enforcing governance policies. Establish protocols to handle data, safeguard sensitive information, and optimize data usage within the organization. Design and advise on data quality rules and set up effective data compliance policies.\n\n\n\nMust have skills :Data Architecture Principles\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :any graduate\n\n\nSummary:As a Data Management Practitioner, you will be responsible for maintaining the quality and compliance of an organization's data assets. Your role involves designing and implementing data strategies, ensuring data integrity, enforcing governance policies, and optimizing data usage within the organization.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Design and advise on data quality rules- Set up effective data compliance policies- Ensure data integrity and enforce governance policies- Optimize data usage within the organization\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Architecture Principles- Strong understanding of data management best practices- Experience in designing and implementing data strategies- Knowledge of data governance and compliance policies- Ability to optimize data usage for organizational benefit\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Architecture Principles- This position is based at our Kolkata office- A degree in any graduate is required\n\nQualification\n\nany graduate",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data management', 'data architecture', 'sql', 'data architecture principles', 'hive', 'data analysis', 'oracle', 'data warehousing', 'machine learning', 'business intelligence', 'sql server', 'plsql', 'tableau', 'data science', 'data modeling', 'hadoop', 'sqoop', 'etl', 'etl development']",2025-06-13 05:44:18
Cloud Data Engineer - GCP,Synechron,2 - 3 years,Not Disclosed,"['Hyderabad', 'Gachibowli']","Job Summary\nSynechron is seeking a highly motivated and skilled Senior Cloud Data Engineer GCP to join our cloud solutions team. In this role, you will collaborate closely with clients and internal stakeholders to design, implement, and manage scalable, secure, and high-performance cloud-based data solutions on Google Cloud Platform (GCP). You will leverage your technical expertise to ensure the integrity, security, and efficiency of cloud data architectures, enabling the organization to derive maximum value from cloud data assets. This role contributes directly to our mission of delivering innovative digital transformation solutions and supports the organizations strategic objectives of scalable and sustainable cloud infrastructure.\nSoftware Requirements\nOverall Responsibilities\nTechnical Skills (By Category)\nExperience Requirements\nDay-to-Day Activities\nQualifications\nProfessional Competencies",,,,"['GCP', 'Jenkins', 'Java', 'NoSQL', 'Bash scripts', 'Data Studio', 'Data Management', 'CI/CD', 'Apache Beam', 'MongoDB', 'Cloud Build']",2025-06-13 05:44:20
Lead Infrastructure Engineer - Data Platforms,General Mills,6 - 10 years,Not Disclosed,['Mumbai'],"Position Title\nLead Infrastructure Engineer - Data Platforms\nFunction/Group\nDigital and Technology\nLocation\nMumbai\nShift Timing\nGeneral\nRole Reports to\nDT Manager Cloud Data Platforms\nRemote/Hybrid/in-Office\nHybrid\nABOUT GENERAL MILLS\nWe make food the world loves: 100 brands. In 100 countries. Across six continents. With iconic brands like Cheerios, Pillsbury, Betty Crocker, Nature Valley, and H agen-Dazs, we ve been serving up food the world loves for 155 years (and counting). Each of our brands has a unique story to tell.\nHow we make our food is as important as the food we make. Our values are baked into our legacy and continue to accelerate\nus into the future as an innovative force for good. General Mills was founded in 1866 when Cadwallader Washburn boldly bought the largest flour mill west of the Mississippi. That pioneering spirit lives on today through our leadership team who upholds a vision of relentless innovation while being a force for good. For more details check out http://www.generalmills.com\nGeneral Mills India Center (GIC) is our global capability center in Mumbai that works as an extension of our global organization delivering business value, service excellence and growth, while standing for good for our planet and people.\nWith our team of 1800+ professionals, we deliver superior value across the areas of Supply chain (SC) , Digital Technology (DT) Innovation, Technology Quality (ITQ), Consumer and Market Intelligence (CMI), Sales Strategy Intelligence (SSI) , Global Shared Services (GSS) , Finance Shared Services (FSS) and Human Resources Shared Services (HRSS).For more details check out https://www.generalmills.co.in\nWe advocate for advancing equity and inclusion to create more equitable workplaces and a better tomorrow.\nJOB OVERVIEW\nFunction Overview\nThe Digital and Technology team at General Mills stands as the largest and foremost unit, dedicated to exploring the latest trends and innovations in technology while leading the adoption of cutting-edge technologies across the organization. Collaborating closely with global business teams, the focus is on understanding business models and identifying opportunities to leverage technology for increased efficiency and disruption. The teams expertise spans a wide range of areas, including AI/ML, Data Science, IoT, NLP, Cloud, Infrastructure, RPA and Automation, Digital Transformation, Cyber Security, Blockchain, SAP S4 HANA and Enterprise Architecture. The MillsWorks initiative embodies an agile@scale delivery model, where business and technology teams operate cohesively in pods with a unified mission to deliver value for the company. Employees working on significant technology projects are recognized as Digital Transformation change agents.\nThe team places a strong emphasis on service partnerships and employee engagement with a commitment to advancing equity and supporting communities. In fostering an inclusive culture, the team values individuals passionate about learning and growing with technology, exemplified by the Work with Heartphilosophy, emphasizing results over facetime. Those intrigued by the prospect of contributing to the digital transformation journey of a Fortune 500 company are encouraged to explore more details about the function through the provided Link\nPurpose of the role\nThe Digital and Technology team of General Mills India Centre is looking for a MS SQL Server / Cloud database administrator (DBA) with Operations-oriented/DevOps skill set to support Cloud/On Prem databases. This opportunity is in a fast-paced environment as we migrate and refactor enterprise-scale databases from our on-premises datacenters to a cloud environment. The ideal candidate will have experience operating in a fast-paced, complex, and multi-platform environment and will contribute to the strategic direction of our database infrastructure.\nKEY ACCOUNTABILITIES\nProvide technical leadership for migrating existing Microsoft SQL Server and NoSQL Databases to Public Cloud, developing and owning migration processes, documentation, and tooling. This includes defining strategies and roadmaps for database migrations, ensuring alignment with overall IT strategy and leveraging automation wherever possible to streamline the process.\nExpertly administer and manage mission-critical, complex, and high-volume Database Platforms in a 24/7 environment, implementing and maintaining automated monitoring and alerting systems to proactively identify and address potential issues.\nProactively identify and address potential database performance bottlenecks, proposing and implementing automated solutions to optimize database efficiency and scalability. This includes developing and deploying automated scripts for performance tuning and optimization.\nLead the design and implementation of highly available and scalable database solutions in the cloud (GCP preferred), ensuring compliance with security and governance standards and utilizing Infrastructure as Code (IaC) for automated provisioning and management of database infrastructure.\nAdminister and troubleshoot SQL Server, PostgreSQL, MySQL, and NoSQL DBs (MongoDB), implementing best practices and resolving complex performance issues through automated scripting and tooling.\nDevelop and maintain comprehensive documentation for database systems, processes, and procedures.\nChampion the adoption of DevOps practices, including CI/CD, infrastructure as code (Terraform), and automation (Ansible, Python, PowerShell) to streamline database management and deployment. Actively participate in the development and improvement of automated deployment pipelines.\nCollaborate with application stakeholders to understand their database requirements and provide technical guidance on database design and optimization, emphasizing automation opportunities to improve development workflows.\nContribute to the development and implementation of database security policies and procedures, ensuring compliance with industry best practices and leveraging automation for security auditing and vulnerability management.\nActively participate in Agile development processes, contributing to sprint planning, daily stand-ups, and retrospectives, focusing on automation opportunities to improve team efficiency and reduce manual effort.\nMentor and guide junior team members, fostering a culture of knowledge sharing and continuous learning, particularly around automation and DevOps practices.\nStay abreast of emerging technologies and trends in database administration and cloud computing, recommending and implementing innovative solutions to improve database performance and reliability, with a focus on automation and efficiency gains.\nMINIMUM QUALIFICATIONS\n9+ years of hands-on experience with leading design, refactoring, or migration of databases in cloud infrastructures and services for at least one of Microsoft Azure, Amazon Web Services, and Google GCP (preferred). Experience with automating database migration processes is highly desirable.\nExperience maintaining and administering with CI/CD tools such as Ansible, GitHub, and Artifactory in a cloud environment and developing/writing scripts using advanced DevOps languages such as Python, PowerShell. Demonstrated ability to design and implement automated workflows.\nExperience working with infrastructure as code such as Terraform, or equivalent. Proven ability to automate infrastructure provisioning and management.\nExperience with concepts, processes tools required for cloud adoption including cloud security, governance, and integration. Experience with automating security and compliance tasks is a plus.\nExperience with SQL Server AlwaysOn and Windows clustering. Experience with automating the management of high-availability clusters is preferred.\nExperience with agile techniques and methods.\nFamiliarity with user expectations for cloud Databases (to be able to design user-centric engineering services e.g., provisioning self-service workflow). Experience with automating user provisioning and access management is a plus.\nWorking knowledge of DevOps, Agile development processes, exploration, and POCs.\nAbility to work collaboratively across functional team boundaries.\nPREFERRED QUALIFICATIONS\nGood understanding hands-on experience of Linux OS and Windows Server (2012+).\nExperience working with high availability, disaster recovery, backup strategies, and server tuning strategies including parameters, resources, contention, etc.\nExcellent interpersonal and communication skills.\nAbility to work in a fast-paced team environment.\nFlexibility, reliability, initiative, responsibility, and a can-domentality.",Industry Type: Food Processing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'MS SQL', 'SAP', 'Linux', 'MySQL', 'Infrastructure', 'Workflow', 'Windows', 'SQL', 'Python']",2025-06-13 05:44:22
Senior Data Scientist | Snowflakes | Tableau | AI/ML,Cisco,0 - 2 years,Not Disclosed,['Bengaluru'],"Job posting may be removed earlier if the position is filled or if a sufficient number of applications are received.\n\nMeet the Team\n\nWe are a dynamic and innovative team of Data Engineers, Data Architects, and Data Scientists based in Bangalore, India. Our mission is to harness the power of data to provide actionable insights that empower executives to make informed, data-driven decisions. By analyzing and interpreting complex datasets, we enable the organization to understand the health of the business and identify opportunities for growth and improvement.\n\nYour Impact\n\nWe are seeking a highly experienced and skilled Senior Data Scientist to join our dynamic team. The ideal candidate will possess deep expertise in machine learning models, artificial intelligence (AI), generative AI, and data visualization. Proficiency in Tableau and other visualization tools is essential. This role requires hands-on experience with databases such as Snowflake and Teradata, as well as advanced knowledge in various data science and AI techniques. The successful candidate will play a pivotal role in driving data-driven decision-making and innovation within our organization.\n\nKey Responsibilities\nDesign, develop, and implement advanced machine learning models to solve complex business problems.\nApply AI techniques and generative AI models to enhance data analysis and predictive capabilities.\nUtilize Tableau and other visualization tools to create insightful and actionable dashboards for stakeholders.\nManage and optimize large datasets using Snowflake and Teradata databases.\nCollaborate with cross-functional teams to understand business needs and translate them into analytical solutions.\nStay updated with the latest advancements in data science, machine learning, and AI technologies.\nMentor and guide junior data scientists, fostering a culture of continuous learning and development.\nCommunicate complex analytical concepts and results to non-technical stakeholders effectively.\nKey Technologies &\n\nSkills:\nMachine Learning ModelsSupervised learning, unsupervised learning, reinforcement learning, deep learning, neural networks, decision trees, random forests, support vector machines (SVM), clustering algorithms, etc.\nAI TechniquesNatural language processing (NLP), computer vision, generative adversarial networks (GANs), transfer learning, etc.\nVisualization ToolsTableau, Power BI, Matplotlib, Seaborn, Plotly, etc.\nDatabasesSnowflake, Teradata, SQL, NoSQL databases.\nProgramming LanguagesPython (essential), R, SQL.\nPython LibrariesTensorFlow, PyTorch, scikit-learn, pandas, NumPy, Keras, SciPy, etc.\nData ProcessingETL processes, data warehousing, data lakes.\nCloud PlatformsAWS, Azure, Google Cloud Platform.\nMinimum Qualifications\nBachelor's or Master's degree in Computer Science, Statistics, Mathematics, Data Science, or a related field.\nMinimum of [X] years of experience as a Data Scientist or in a similar role.\nProven track record in developing and deploying machine learning models and AI solutions.\nStrong expertise in data visualization tools, particularly Tableau.\nExtensive experience with Snowflake and Teradata databases.\nExcellent problem-solving skills and the ability to work independently and collaboratively.\nExceptional communication skills with the ability to convey complex information clearly.\nPreferred Qualifications (Provide up to five (5) bullet points these can include soft skills)\nExcellent communication and collaboration skills to work effectively in cross-functional teams.\nAbility to translate business requirements into technical solutions.\nStrong problem-solving skills and the ability to work with complex datasets.\nExperience in statistical analysis and machine learning techniques.\nUnderstanding of business domains such as sales, financials, marketing, and telemetry.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['machine learning', 'artificial intelligence', 'sql', 'tableau', 'data visualization', 'snowflake', 'scipy', 'python', 'scikit-learn', 'data warehousing', 'numpy', 'pandas', 'tensorflow', 'data integration tools', 'matplotlib', 'pytorch', 'keras', 'machine learning algorithms', 'etl', 'nosql databases']",2025-06-13 05:44:23
Senior Data Engineer -Bangalore,Happiest Minds Technologies,6 - 10 years,Not Disclosed,['Bengaluru'],"Job Overview:\nThe primary purpose of this role is to translate business requirements and functional specifications into logical program designs and to deliver dashboards, schema, data pipelines, and software solutions. This includes developing, configuring, or modifying data components within various complex business and/or enterprise application solutions in various computing environments. You will partner closely with multiple Business partners, Product Owners, Data Strategy, Data Platform, Data Science and Machine Learning (MLOps) teams to drive innovative data products for end users. Additionally, you will help shape overall solution & data products, develop scalable solutions through best-in-class engineering practices.",,,,"['NoSQL', 'big data systems', 'Data Pipeline', 'MongoDB', 'SQL', 'Hive', 'GIT', 'Hadoop', 'Kafka', 'Agile', 'MQL', 'Ci/Cd']",2025-06-13 05:44:25
Technology Architect,Accenture,15 - 25 years,Not Disclosed,['Ahmedabad'],"Project Role :Technology Architect\n\n\n\n\n\nProject Role Description :Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\n\n\n\nMust have skills :Amazon Web Services (AWS)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Technology Architect, you will review and integrate all application requirements, including functional, security, integration, performance, quality, and operations requirements. You will also review and integrate the technical architecture requirements and provide input into final decisions regarding hardware, network products, system software, and security. In this role, you will contribute to the overall success of the project by ensuring the seamless integration of various components and systems. You will play a crucial role in shaping the technology architecture and ensuring its alignment with business goals and objectives.\nRoles & Responsibilities:- Expected to be a SME with deep knowledge and experience.- Should have Influencing and Advisory skills.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Collaborate with stakeholders to understand their requirements and translate them into technical solutions.- Design and develop technology architecture that meets business needs and aligns with industry best practices.- Evaluate and recommend technology solutions, tools, and frameworks to enhance system performance and scalability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Amazon Web Services (AWS).- Strong understanding of cloud computing concepts and architecture.- Experience in designing and implementing scalable and secure cloud solutions.- Knowledge of infrastructure as code and automation tools such as Terraform and Ansible.- Experience with containerization technologies such as Docker and Kubernetes.\nAdditional Information:- The candidate should have a minimum of 15 years of experience in Amazon Web Services (AWS).- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['kubernetes', 'docker', 'ansible', 'terraform', 'aws', 'c#', 'project management', 'web services', 'system software', 'technology architecture', 'hibernate', 'technology solutions', 'sql server', 'microservices', 'spring', 'spring boot', 'technical architecture', 'java', 'asp.net', 'j2ee', 'cloud computing']",2025-06-13 05:44:27
Technology Architect,Accenture,7 - 12 years,Not Disclosed,['Ahmedabad'],"Project Role :Technology Architect\n\n\n\n\n\nProject Role Description :Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\n\n\n\nMust have skills :Amazon Web Services (AWS)\n\n\n\n\nGood to have skills :Java Full Stack Development, Python (Programming Language), DevOpsMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Technology Architect, you will review and integrate all application requirements, including functional, security, integration, performance, quality, and operations requirements. You will also review and integrate the technical architecture requirements and provide input into final decisions regarding hardware, network products, system software, and security. In this role, you will play a crucial part in ensuring the smooth functioning of the technology infrastructure and contribute to the overall success of the organization.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Collaborate with stakeholders to understand their requirements and translate them into technical solutions.- Lead the design and development of technology solutions that align with business objectives.- Ensure the integration of various systems and applications to achieve seamless functionality.- Stay updated with the latest technology trends and provide recommendations for improvement.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Amazon Web Services (AWS).- Good To Have\n\n\n\n\nSkills:\nExperience with Java Full Stack Development, Python (Programming Language), DevOps.- Strong understanding of cloud computing principles and architecture.- Experience in designing and implementing scalable and secure cloud solutions using AWS services.- Knowledge of infrastructure as code and automation tools such as Terraform and Ansible.- Familiarity with containerization technologies like Docker and Kubernetes.- Ability to troubleshoot and resolve complex technical issues.- Excellent communication and collaboration skills.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Amazon Web Services (AWS).- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['kubernetes', 'docker', 'cloud computing', 'terraform', 'aws', 'css', 'c++', 'aws iam', 'web services', 'system software', 'bootstrap', 'ansible', 'sql', 'react.js', 'technical architecture', 'java', 'devops', 'linux', 'mysql', 'python', 'c', 'technology architecture', 'technology solutions', 'javascript', 'full stack']",2025-06-13 05:44:29
Technology Architect,Accenture,7 - 12 years,Not Disclosed,['Ahmedabad'],"Project Role :Technology Architect\n\n\n\n\n\nProject Role Description :Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\n\n\n\nMust have skills :Amazon Web Services (AWS)\n\n\n\n\nGood to have skills :Java Full Stack Development, Python (Programming Language), DevOpsMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Technology Architect, you will review and integrate all application requirements, including functional, security, integration, performance, quality, and operations requirements. You will also review and integrate the technical architecture requirements and provide input into final decisions regarding hardware, network products, system software, and security. In this role, you will play a crucial part in ensuring the smooth functioning of the technology infrastructure and contribute to the overall success of the organization.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Collaborate with stakeholders to understand their requirements and translate them into technical solutions.- Lead the design and development of technology solutions that align with business objectives.- Ensure the integration of various systems and applications to achieve seamless functionality.- Stay updated with the latest technology trends and provide recommendations for improvement.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Amazon Web Services (AWS).- Good To Have\n\n\n\n\nSkills:\nExperience with Java Full Stack Development, Python (Programming Language), DevOps.- Strong understanding of cloud computing principles and architecture.- Experience in designing and implementing scalable and secure cloud solutions using AWS services.- Knowledge of infrastructure as code and automation tools such as Terraform and Ansible.- Familiarity with containerization technologies like Docker and Kubernetes.- Ability to troubleshoot and resolve complex technical issues.- Excellent communication and collaboration skills.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Amazon Web Services (AWS).- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['kubernetes', 'docker', 'cloud computing', 'terraform', 'aws', 'css', 'c++', 'aws iam', 'web services', 'system software', 'bootstrap', 'ansible', 'sql', 'react.js', 'technical architecture', 'java', 'devops', 'linux', 'mysql', 'python', 'c', 'technology architecture', 'technology solutions', 'javascript', 'full stack']",2025-06-13 05:44:31
Technology Architect,Accenture,15 - 20 years,Not Disclosed,['Bhubaneswar'],"Project Role :Technology Architect\n\n\n\n\n\nProject Role Description :Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\n\n\n\nMust have skills :Amazon Web Services (AWS)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Technology Architect, you will review and integrate all application requirements, including functional, security, integration, performance, quality, and operations requirements. You will also review and integrate the technical architecture requirements and provide input into final decisions regarding hardware, network products, system software, and security.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead the design and implementation of technology solutions.- Provide technical guidance and expertise to project teams.- Ensure compliance with architectural standards and guidelines.- Conduct technology evaluations and recommend solutions.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Amazon Web Services (AWS).- Strong understanding of cloud computing principles.- Experience in designing scalable and secure AWS architectures.- Knowledge of infrastructure as code tools like Terraform or CloudFormation.- Hands-on experience with AWS services such as EC2, S3, RDS, and Lambda.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Amazon Web Services (AWS).- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['aws cloudformation', 'amazon ec2', 'lambda expressions', 'cloud computing', 'aws', 'web services', 'system software', 'amazon rds', 'technology architecture', 'hibernate', 'technology solutions', 'microservices', 'docker', 'spring', 'spring boot', 'technical architecture', 'java', 'j2ee', 'terraform']",2025-06-13 05:44:33
Technology Architect,Accenture,15 - 25 years,Not Disclosed,['Bhubaneswar'],"Project Role :Technology Architect\n\n\n\n\n\nProject Role Description :Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\n\n\n\nMust have skills :Amazon Web Services (AWS)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n15 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Technology Architect, you will review and integrate all application requirements, including functional, security, integration, performance, quality, and operations requirements. You will also review and integrate the technical architecture requirements and provide input into final decisions regarding hardware, network products, system software, and security. In this role, you will contribute to the overall success of the project by ensuring the seamless integration of various components and systems. You will play a crucial role in shaping the technology architecture and ensuring its alignment with business goals and objectives.\nRoles & Responsibilities:- Expected to be a SME with deep knowledge and experience.- Should have Influencing and Advisory skills.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Collaborate with stakeholders to understand their requirements and translate them into technical solutions.- Design and develop technology architecture that meets business needs and aligns with industry best practices.- Evaluate and recommend technology solutions, tools, and frameworks to enhance system performance and scalability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Amazon Web Services (AWS).- Strong understanding of cloud computing concepts and architecture.- Experience in designing and implementing scalable and secure cloud solutions.- Knowledge of infrastructure as code and automation tools such as Terraform and Ansible.- Experience with containerization technologies such as Docker and Kubernetes.\nAdditional Information:- The candidate should have a minimum of 15 years of experience in Amazon Web Services (AWS).- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['kubernetes', 'docker', 'ansible', 'terraform', 'aws', 'c#', 'project management', 'web services', 'system software', 'technology architecture', 'hibernate', 'technology solutions', 'sql server', 'microservices', 'spring', 'spring boot', 'technical architecture', 'java', 'asp.net', 'j2ee', 'cloud computing']",2025-06-13 05:44:35
CBS & Data Migration - M/Sr.M - Navi Mumbai,Wikilabs India,7 - 12 years,Not Disclosed,['Navi Mumbai'],"Job Title: Manager / Senior Manager CBS & Data Migration\n\nLocation: Navi Mumbai\n\nExperience: 7-12 years\n\nIndustry: Banking\n\nJob Responsibilities:\nCore Banking System (CBS) Implementation & Management:\nLead end-to-end implementation and maintenance of Core Banking Systems (CBS).\nWork closely with vendors, IT teams, and business units to ensure smooth CBS operations.\nProvide technical support, troubleshooting, and system optimization.\nEnsure compliance with banking regulatory standards and security protocols.\n\nData Migration & Management:\nPlan, execute, and oversee large-scale data migration projects for banking platforms.\nEnsure data accuracy, integrity, and security during migration.\nCollaborate with cross-functional teams to define data mapping, transformation, and validation strategies.\nConduct post-migration audits and performance tuning.\n\nTechnical & Project Coordination:\nWork on SQL, Oracle, and other database management systems to support migration.\nHandle data extraction, transformation, and loading (ETL) processes.\nCollaborate with software vendors for CBS and data migration improvements.\nMonitor project timelines, risks, and deliverables.\n\nKey Skills Required:\nCBS Implementation & Support (Finacle, TCS BaNCS, Flexcube, Temenos, etc.)\nBanking Data Migration & Transformation\nETL Processes & Data Mapping\nSQL / Oracle Database Management\nRegulatory Compliance & Risk Management\nProject Management & Vendor Coordination\n\nPreferred Qualifications:\nB.Tech / B.E. / MCA / M.Sc. (IT) or equivalent\nExperience in Banking, Fintech, or IT Services\n\n\nImmediate joiners or candidates with a notice period of 30 days preferred.",Industry Type: Banking,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['CBS Implementation', 'Data Migration', 'Flexcube', 'Finacle', 'Data Mapping', 'CBS Migration', 'Banking IT', 'Core Banking System Migration']",2025-06-13 05:44:37
Oracle BRM Data Migration Engineer,Techstar Group,5 - 10 years,Not Disclosed,"['Noida', 'Hyderabad', 'Bengaluru']","Work Location : Hyderabad, Bangalore, Noida, Pune\nQualifications and Skills :\n- Proven expertise in Oracle BRM (Mandatory skill) with a strong understanding of its architecture and modules to effectively manage data migration processes.\n\n- Hands-on experience in data migration activities, particularly with Oracle BRM, ensuring high efficiency and accuracy throughout migration projects.\n\n- Knowledge in SQL for querying and managing databases, crucial for data migration and integration tasks.\n\n- Strong knowledge of ETL tools and processes for efficient data extraction, transformation, and loading from various sources.\n\n- Ability to perform detailed data mapping, ensuring logical transformation and compatibility between source and target system data structures.\n\n- Experience in data cleansing techniques to ensure data integrity and consistency throughout the migration process.\n\n- Understanding of data quality principles and practices, essential to maintain high standards of data accuracy and dependability.\n\n- Proficiency in scripting for automation of data migration tasks, enhancing efficiency and reducing potential for errors.\n\n- Excellent analytical and problem-solving skills to identify and address data-related challenges and opportunities.\n\n- Handling the execution of the data migration and validations.\n\n- Handle the develop Migration strategy documents and techniques. Execute data integrity testing post migration.\n\n- Understanding BRM : Having a working knowledge of BRM data migration components, the BRM 12 schema, and the data model\n\n- Data migration strategy : Developing a migration strategy and implementation plan\n\n- Data loading : Being able to load data and integrate it with systems\n\n- Post-migration analysis : Performing post-migration analysis on events, invoices, open items, bills, and dunning\n\n- Data reconciliation : Developing scripts to reconcile migrated data\n\n- Working Knowledge of all the BRM Data migration components.\n\n- Must have hands-on in BRM to verify the sanity of the Data migration.\n\n- Advantage - Programming skills on Java technologies. Exp. in C/C++, Oracle 12c/19c, PL/SQL, PCM Java, BRM Webservice, Scripting language (perl/python)\n\nRoles and Responsibilities :\n\n- Analyze client data and formulating effective data migration plans tailored to Oracle BRM specifications.\n\n- Collaborate with cross-functional teams to gather and interpret data migration requirements accurately.\n\n- Develop and implement efficient data migration scripts and processes, ensuring minimal disruption to business operations.\n\n- Conduct thorough testing and validation of data migration outputs to guarantee data accuracy and conformity.\n\n- Monitor and troubleshoot migration activities to ensure seamless execution and rectify any issues promptly.\n\n- Document data migration processes, maps, and transformations for knowledge sharing and continuous improvement.\n\n- Liaise with stakeholders to present progress updates and discuss ongoing improvements to data migration practices.\n\n- Contribute to the development of data migration best practices and reusable frameworks within the organization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle BRM', 'Java', 'Data Quality', 'Oracle Apps', 'C++', 'PL/SQL', 'Data Migration', 'Data reconciliation', 'ETL', 'SQL']",2025-06-13 05:44:39
Senior Data Test Analyst,Exavalu,6 - 8 years,Not Disclosed,[],"We are seeking a detail-oriented and experienced Senior Data Test Analyst to join our team. The ideal candidate will have a strong background in testing large-scale data migration projects, along with hands-on experience in ETL testing, data quality validation, and test automation. The role requires designing and executing robust testing strategies to ensure data integrity, accuracy, and compliance.\nKey Responsibilities:\nExperience Range - 6 to 8 Years\nDevelop and execute test strategies for large-scale data migration using statistical sampling and verification techniques.\nDesign and implement test plans covering Data Migration, Obfuscation, Reconciliation, Data Quality, and Data Governance.\nConduct thorough testing of ETL processes using tools such as Ab Initio and Informatica.\nPerform report testing for both operational and regulatory reporting requirements.\nEnsure comprehensive test coverage through data profiling and validation techniques.\nCollaborate with data architects, business analysts, and developers to understand data requirements and transformation logic.\nDevelop and maintain automated test scripts for data validation and regression testing.\nPrepare detailed test plans, test cases, and defect reports for data migration and integration efforts.\nRequirements\nProven experience in data migration testing, including validation strategies and statistical sampling.\nStrong hands-on experience with ETL testing tools such as Ab Initio and/or Informatica.\nExpertise in data obfuscation, reconciliation, data quality, and governance testing.\nSolid understanding of regulatory and operational reporting testing.\nExperience in automation testing frameworks related to data validation.\nAbility to design and document test plans specifically tailored to data migration projects.\nFamiliarity with SQL for data querying and validation.\nExcellent analytical and problem-solving skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data migration', 'Test scripts', 'Testing tools', 'Analytical', 'Reconciliation', 'Regression testing', 'Data quality', 'Informatica', 'Test cases', 'SQL']",2025-06-13 05:44:40
Senior Databricks Data Engineer,"Creative Capsule, LLC",5 - 10 years,Not Disclosed,['Panaji'],"This position will primarily be responsible for designing, developing, and maintaining robust ETL/ELT pipelines for data ingestion, transformation, and storage. The role also involves designing and developing scalable data solutions.\nYou will work with a team responsible for ensuring the availability, reliability, and performance of data systems and requires a good understanding of infrastructure management, cost optimization, and performance tuning in Databricks environments. The candidate will design, develop, and maintain scalable data pipelines of Databricks on cloud platforms (Databricks, Azure, AWS, or GCP).\nThe person will also work with a collaborative team responsible for driving client performance by combining data-driven insights and strategic thinking to solve business challenges. The candidate should have strong organizational, critical thinking, and communication skills to interact effectively with stakeholders.\nResponsibilities:\nDesign, develop, and manage end-to-end data pipelines on Databricks using Spark, Delta Lake, and related technologies\nImplement and optimize ETL/ELT workflows for ingesting, transforming, and storing large volumes of structured and semi-structured data\nManage and monitor Databricks infrastructure, including cluster configuration, auto-scaling, job execution, and resource utilization\nIdentify and implement cost-saving strategies across Databricks workspaces through efficient pipeline design, job scheduling, and infrastructure tuning\nOrchestrate workflows using tools like Apache Airflow, Azure Data Factory, or similar orchestration frameworks\nCollaborate with data architects and platform engineers to ensure efficient data architecture and performance tuning\nMonitor data quality, integrity, and lineage across all pipelines and proactively troubleshoot data issues\nDevelop, maintain, and optimize data models and storage layers that support BI/reporting and analytics use cases\nPartner with business stakeholders to translate business needs into technical specifications and scalable data solutions\nMaintain comprehensive documentation of pipeline architecture, configurations, and operational best practices\nTechnical Qualifications:\nExperience in infrastructure management on Databricks: cluster setup, scaling, security roles, workspace configuration\nExperience in cost optimization techniques in Databricks (e.g., job cluster vs. all-purpose cluster usage, cost/performance tradeoffs)\nExperience with orchestration tools such as Apache Airflow, Azure Data Factory, or AWS Glue Workflows\nExperience with relational (e.g., PostgreSQL, SQL Server) and NoSQL (e.g., MongoDB) databases\nStrong proficiency in Apache Spark and Delta Lake within the Databricks ecosystem\nProficient in Python and SQL for data processing and pipeline development\nFamiliarity with data warehousing, data lakes, and distributed data processing frameworks\nUnderstanding of BI tools like Power BI, Tableau, or Looker is a plus\nPersonal Skills:\nStrong analytical skills: ability to read business requirements, analyze problems, and propose solutions\nAbility to identify alternatives and find optimal solutions\nAbility to follow through and ensure logical implementation\nQuick learner with the ability to adapt to new concepts and software\nAbility to work effectively in a team environment\nStrong time management skills, capable of handling multiple tasks and competing deadlines\nEffective written and verbal communication skills\nEducation and Work Experience:\nBackground in Computer Science, Information Technology, Data Science, or a related field preferred\nMinimum 5 years of experience in Data Engineering with at least 2 years of hands-on experience with Databricks (Azure, AWS, or GCP)\nCertification in Databricks, Azure Data Engineering, or any related data technology is an added advantage",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Infrastructure management', 'Postgresql', 'Scheduling', 'Data quality', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-13 05:44:42
"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon",One of the largest insurance providers.,5 - 10 years,Not Disclosed,['Gurugram'],"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon\n\nSummary: An excellent opportunity for someone having a minimum of five years of experience with expertise in building data pipelines. A person must have experience in Python, Pyspark and AWS.\n\nLocation- Gurgaon (Hybrid)\n\nYour Future Employer- One of the largest insurance providers.\n\nResponsibilities-\nTo design, develop, and maintain large-scale data pipelines that can handle large datasets from multiple sources.\nReal-time data replication and batch processing of data using distributed computing platforms like Spark, Kafka, etc.\nTo optimize the performance of data processing jobs and ensure system scalability and reliability.\nTo collaborate with DevOps teams to manage infrastructure, including cloud environments like AWS.\nTo collaborate with data scientists, analysts, and business stakeholders to develop tools and platforms that enable advanced analytics and reporting.\n\nRequirements-\nHands-on experience with AWS services such as S3, DMS, Lambda, EMR, Glue, Redshift, RDS (Postgres) Athena, Kinesics, etc.\nExpertise in data modeling and knowledge of modern file and table formats.\nProficiency in programming languages such as Python, PySpark, and SQL/PLSQL for implementing data pipelines and ETL processes.\nExperience data architecting or deploying Cloud/Virtualization solutions (Like Data Lake, EDW, Mart ) in the enterprise.\nCloud/hybrid cloud (preferably AWS) solution for data strategy for Data lake, BI and Analytics.\nWhat is in for you-\nA stimulating working environment with equal employment opportunities.\nGrowing of skills while working with industry leaders and top brands.\nA meritocratic culture with great career progression.\n\nReach us- If you feel that you are the right fit for the role please share your updated CV at randhawa.harmeen@crescendogroup.in\n\nDisclaimer- Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Pipeline', 'AWS', 'Data Ingestion', 'Data Engineering', 'Data Processing']",2025-06-13 05:44:44
"Sr. Manager, Data Governance Lead - Domain Specific",Horizon Therapeutics,12 - 15 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nABOUT AMGEN\nAmgen harnesses the best of biology and technology to fight the world s toughest diseases, and make people s lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what s known today.\nABOUT THE ROLE\nRole Description:\nThe Data Strategy and Governance Lead will operationalize the Enterprise Data Council vision across specific domains (Research, Clinical Trials, Commercial, etc.). He/She will coordinate activities at the tactical level, interpreting Enterprise Data Council direction and defining operational level impact deliverables and actions to build data foundations in specific domains . The Data Strategy and Governance Lead will partner with senior leadership and other Data Governance functional leads to align data initiatives with business goals. He/she will establish and enforce data governance policies and standards to provide high-quality data, easy to reuse and connect to accelerate AI innovative solutions to better serve patients .\nRoles & Responsibilities:\nResponsible for data governance and data management for a given domain of expertise (Research, Development, Supply Chain, etc.).\nManage a team of Data Governance Specialists and Data Stewards for a specific domain.\nResponsible for operationalizing the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nDrives cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles.\nProvides expert guidance on business process and system design to support data governance and data/information modelling objectives.\nMaintain documentation and act as an expert on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. for assigned domains.\nEnsure compliance with data privacy, security, and regulatory policies for the assigned domains\nPublish metrics to measure effectiveness and drive adoption of Data Governance policy and standards, that will be applied to mitigate identified risks across the data lifecycle (e.g., capture / production, aggregation / processing, reporting / consumption).\nEstablish enterprise level standards on the nomenclature, content, and structure of information (structured and unstructured data), metadata, glossaries, and taxonomies.\nJointly with Technology teams, business functions, and enterprise teams (e.g., MDM, Enterprise Data Architecture, Enterprise Data Fabric, etc.) define the specifications shaping the development and implementation of data foundations .\nFunctional Skills:\nMust-Have Skills:\nTechnical skills with in-depth knowledge of Pharma processes with preferred specialization in a domain (e.g., Research, Clinical, Commercial, Supply Chain, Finance, etc.).\nAware of industry trends and priorities and can apply to governance and policies.\nIn-depth knowledge and experience with data governance principles and technology; can design and implement Data Governance operating models to drive Amgen s transformation to be a data driven organization.\nIn-depth knowledge of data management, common data models, metadata management, data quality, reference & master data management, data stewardship, data protection, etc.\nExperience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nGood-to-Have Skills:\nExperience adopting industry standards in data products.\nExperience managing industry external data assets (e.g. Claims, EHR, etc.)\nAbility to successfully execute complex projects in a fast-paced environment and in managing multiple priorities effectively.\nAbility to manage projects or departmental budgets.\nExperience with modelling tools (e.g., Visio).\nBasic programming skills, experience in data visualization and data modeling tools.\nExperience working with agile development methodologies such as Scaled Agile.\nSoft Skills:\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management and customer focus.\nBasic Qualifications:\n12 to 15 years of Information Systems experience\n4 years of managerial experience directly managing people and leadership experience leading teams, projects, or programs.\nEQUAL OPPORTUNITY STATEMENT\nAmgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request and accommodation.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Business process', 'Change management', 'Claims', 'Data modeling', 'Pharma', 'Clinical trials', 'Data quality', 'Visio', 'Operations']",2025-06-13 05:44:45
Senior / Lead Data Engineer,Atlas,1 - 2 years,Not Disclosed,['Pune'],"1. Designing and developing data pipelines: Lead data engineers are responsible for designing and developing data pipelines that move data from various sources to storage and processing systems.\n2. Building and maintaining data infrastructure: Lead data engineers are responsible for building and maintaining data infrastructure, such as data warehouses, data lakes, and data marts.\n3. Ensuring data quality and integrity: Lead data engineers are responsible for ensuring data quality and integrity, by setting up data validation processes and implementing data quality checks.\n4. Managing data storage and retrieval: Lead data engineers are responsible for managing data storage and retrieval, by designing and implementing data storage systems, such as NoSQL databases or Hadoop clusters.\n5. Developing and maintaining data models: Lead data engineers are responsible for developing and maintaining data models, such as data dictionaries and entity-relationship diagrams, to ensure consistency in data architecture.\n6. Managing data security and privacy: Lead data engineers are responsible for managing data security and privacy, by implementing security measures, such as access controls and encryption, to protect sensitive data.\n7. Leading and managing a team: Lead data engineers may be responsible for leading and managing a team of data engineers, providing guidance and support for their work.",Industry Type: Industrial Equipment / Machinery,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent",['Senior Lead'],2025-06-13 05:44:47
Senior Data engineer,Wissen Technology,10 - 17 years,Not Disclosed,['Pune'],"Wissen Technology is Hiring fo r Senior Data engineer\n\nAbout Wissen Technology:\nWissen Technology is a globally recognized organization known for building solid technology teams, working with major financial institutions, and delivering high-quality solutions in IT services. With a strong presence in the financial industry, we provide cutting-edge solutions to address complex business challenges\n\nRole Overview : We are looking for a highly skilled and experienced Senior Data Engineer to join our growing data engineering team in Bangalore . In this role, you will be responsible for designing, developing, and maintaining scalable and efficient data pipelines and data architectures. You will collaborate closely with cross-functional teams to ensure data accessibility, reliability, and performance to support business intelligence, analytics, and data science initiatives\n\nExperience : 9-13 Years\nLocation: Bangalore\nKey Responsibilities\nDesign , build, and maintain scalable and reliable data pipelines for ingesting, transforming, and loading structured and unstructured data from diverse sources.\nDevelop optimized SQL queries and modular scripts for data manipulation and transformation.\nCreate and maintain data models and schemas to support advanced analytics, reporting, and operational processes.\nAutomate routine data engineering tasks and data quality checks using scripting (Python, Shell, etc.) and orchestration tools.\nCollaborate with engineering, product, and data science teams to understand data requirements and deliver high-quality solutions.\nAct as a liaison between engineering and business teams to translate business needs into technical solutions.\nImplement monitoring, alerting, and troubleshooting mechanisms for data pipelines and dashboards to ensure data integrity and availability.\nDefine and implement best practices for data validation, data governance, and compliance.\nManage test data and QA environments, supporting test data management processes.\nWork in an Agile environment and contribute to continuous improvement initiatives across the data engineering landscape.\n\n\nRequired Skills and Qualification\nBachelors or masters degree in computer science , Information Systems, or a related field.\n9+ years of professional experience in data engineering or a related field.\nStrong expertise in SQL development and performance tuning for both RDBMS and cloud-based databases.\nHands-on experience with cloud platforms, particularly AWS (e.g., S3, Glue, Lambda, Redshift, EMR).\nExperience with modern data warehouse technologies like Snowflake and Amazon Redshift .\nStrong background in data modeling , ETL/ELT architecture, and data warehousing concepts.\nProficiency in programming languages such as Python , Scala , or Java .\nExperience working with",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Telecom', 'Performance tuning', 'Manager Quality Assurance', 'Data management', 'Consulting', 'Manager Technology', 'Healthcare', 'Business intelligence', 'Monitoring', 'Python']",2025-06-13 05:44:49
"Walk-In Senior Data Engineer - DataStage, Azure & Power BI",Net Connect,6 - 10 years,5-11 Lacs P.A.,['Hyderabad( Madhapur )'],"Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\nBelow JD for your reference\n\nJob Description:\n\nWe are hiring an experienced Senior Data Engineer with strong expertise in IBM DataStage, , and . The ideal candidate will be responsible for end-to-end data integration, transformation, and reporting solutions that drive business decisions.",,,,"['Azure Data Factory', 'Datastage', 'Etl Datastage']",2025-06-13 05:44:51
Data Modelling Expert (with Avaloq experience),Luxoft,5 - 10 years,Not Disclosed,['Bengaluru'],"Project description\nWe are seeking a highly skilled Data Modelling Expert with deep experience in the Avaloq Core Banking platform to join our technology team. The ideal candidate will be responsible for designing, maintaining, and optimizing complex data models that support our banking products, client data, and regulatory reporting needs.\n\nThis role requires a blend of domain expertise in banking and wealth management, strong data architecture capabilities, and hands-on experience working with the Avaloq platform.\n\nResponsibilities\n\nDesign, implement, and maintain conceptual, logical, and physical data models within the Avaloq Core Banking system.\n\nCollaborate with business analysts, product owners, and Avaloq parameterisation teams to translate business requirements into robust data models.\n\nEnsure alignment of data models with Avaloq's object model and industry best practices.\n\nPerform data profiling, quality checks, and lineage tracing to support regulatory and compliance requirements (e.g., Basel III, MiFID II, ESG).\n\nSupport integration of Avaloq data with downstream systems (e.g., CRM, data warehouses, reporting platforms).\n\nProvide expert input on data governance, metadata management, and model documentation.\n\nContribute to change requests, upgrades, and data migration projects involving Avaloq.\n\nCollaborate with cross-functional teams to drive data consistency, reusability, and scalability.\n\nReview and validate existing data models, identify gaps or optimisation opportunities.\n\nEnsure data models meet performance, security, and privacy requirements.\n\nSkills\nMust have\n\nProven experience (5+ years) in data modelling or data architecture, preferably within financial services.\n\n3+ years of hands-on experience with Avaloq Core Banking Platform, especially its data structures and object model.\n\nStrong understanding of relational databases and data modelling tools (e.g., ER/Studio, ERwin, or similar).\n\nProficient in SQL and data manipulation in Avaloq environments.\n\nKnowledge of banking products, client lifecycle data, and regulatory data requirements.\n\nFamiliarity with data governance, data quality, and master data management concepts.\n\nExperience working in Agile or hybrid project delivery environments.\n\nNice to have\n\nExposure to Avaloq Scripting or parameterisation is a strong plus.\n\nExperience integrating Avaloq with data lakes, BI/reporting tools, or regulatory platforms.\n\nUnderstanding of data privacy regulations (GDPR, FINMA, etc.).\n\nCertification in Avaloq or relevant financial data management domains is advantageous.\n\nOther\n\nLanguages\n\nEnglishC1 Advanced\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['metadata management', 'sql', 'avaloq', 'data governance', 'wealth management', 'python', 'data analysis', 'data manipulation', 'data warehousing', 'data architecture', 'erwin', 'relational databases', 'financial services', 'data modeling', 'core banking', 'agile', 'etl', 'data profiling']",2025-06-13 05:44:53
nior Data Engineer,M360 Research,5 - 8 years,Not Disclosed,['Bengaluru'],"M3 Global Research, an M3 company, is seeking a Senior Data Engineer to join our data engineering team. This role will focus on building and maintaining robust data pipelines, working closely with stakeholders to ensure data solutions align with business objectives, and utilizing tools like Power BI for data visualization and reporting. The ideal candidate has strong analytical skills, a passion for data-driven decision-making, and excellent communication abilities to work effectively with stakeholders across the organization.\nEssential Duties and Responsibilities:\nDesign, develop, and maintain high-quality, secure data pipelines and processes to manage and transform data efficiently.\nLead the architecture and implementation of data models, schemas, and integrations that support business intelligence and reporting needs.\nCollaborate with cross-functional teams to understand data requirements and deliver optimal data solutions that align with business goals.\nMaintain and enhance data infrastructure, including data warehouses, lakes, and integration tools.\nProvide guidance on best practices for data management, security, and compliance.\nSupport Power BI and other visualization tools, ensuring consistent and reliable access to data insights.\nOversee the delivery of data initiatives, ensuring they meet project milestones, KPIs, and deadlines.\nEssential Job Functions:\nMaintain regular and punctual attendance.\nWork cooperatively and communicate effectively with team members and stakeholders.\nComply with all company policies and procedures.\nSupervisory Responsibility:\nYes\nOutcomes:\nDeliver high-quality, reliable data solutions.\nProvide stakeholders with clear and actionable insights through Power BI reports.\nEnsure data pipelines and ETL processes are optimized and running efficiently.\nFoster strong relationships with stakeholders to ensure their data needs are met.\nCompetencies:\nAttention to detail.\nAnalytical thinking and problem-solving skills.\nStrong communication and interpersonal skills to engage effectively with stakeholders.\nAbility to work in a fast-paced, agile environment.\nKnowledge and Skills:\nProficiency with data engineering tools and technologies (eg, SQL, Python, ETL tools).\nStrong experience with Power BI for data visualization and reporting.\nFamiliarity with cloud-based data platforms (eg, AWS, Azure, Google Cloud).\nExperience with data modeling, data warehousing, and designing scalable data architectures.\nStrong knowledge of database systems (eg, SQL Server, Oracle, PostgreSQL).\nExperience working in an Agile development environment.\nExcellent communication skills to work effectively with both technical and non-technical stakeholders.\nAbility to multi-task and manage multiple projects simultaneously.\nProblem-solving mindset with a desire to continuously improve data processes.\nMinimum Experience:\n5+ years of experience in data engineering or related fields.\n2+ years of experience with Power BI or similar data visualization tools.\nEducation and Training Required:\nbachelors degree in computer science, Data Science, or a related field, or equivalent experience",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'ISO', 'Analytical', 'Healthcare', 'Market research', 'Oracle', 'Business intelligence', 'Japanese', 'Recruitment', 'SQL']",2025-06-13 05:44:54
Mid-Level Data Engineer,BMW Techworks India,6 - 8 years,Not Disclosed,"['Chennai', 'Bengaluru']","What awaits you/ Job Profile\nProvide estimates for requirements, analyses and develop as per the requirement.\nDeveloping and maintaining data pipelines and ETL (Extract, Transform, Load) processes to extract data efficiently and reliably from various sources, transform it into a usable format, and load it into the appropriate data repositories.\nCreating and maintaining logical and physical data models that align with the organizations data architecture and business needs. This includes defining data schemas, tables, relationships, and indexing strategies for optimal data retrieval and analysis.\nCollaborating with cross-functional teams and stakeholders to ensure data security, privacy, and compliance with regulations.\nCollaborate with downstream application to understand their needs and build the data storage and optimize as per their need.\nWorking closely with other stakeholders and Business to understand data requirements and translate them into technical solutions.\nFamiliar with Agile methodologies and have prior experience working with Agile teams using Scrum/Kanban\nLead Technical discussions with customers to find the best possible solutions.\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks.\nOptimizing data pipelines to improve performance and cost, while ensuring a high quality of data within the data lake.\nMonitoring services and jobs for cost and performance, ensuring continual operations of data pipelines, and fixing of defects.\nConstantly looking for opportunities to optimize data pipelines to improve performance\nWhat should you bring along\nMust Have:\nHand on Expertise of 6- 8 years in AWS services like S3, Lambda, Glue, Athena, RDS, Step functions, SNS, SQS, API Gateway, Security, Access and Role permissions, Logging and monitoring Services.\nGood hand on knowledge on Python, Spark, Hive and Unix, AWS CLI\nPrior experience in working with streaming solution like Kafka\nPrior experience in implementing different file storage types like Delta-lake / Ice-berg.\nExcellent knowledge in Data modeling and Designing ETL pipeline.\nMust have strong knowledge in using different databases such as MySQL, Oracle and Writing complex queries.\nStrong experience working in a continuous integration and Deployment process.\nNice to Have:\nHand on experience in the Terraform, GIT, GIT Actions. CICD pipeline and Amazon Q.\nMust have technical skill\nPyspark, AWS ,SQL, Kafka, Glue, IAM. S3, Lambda, Step Function, Athena\nGood to have Technical skills\nTerraform, GIT, GIT Actions. CICD pipeline , AI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Data modeling', 'data security', 'MySQL', 'Agile', 'Scrum', 'Oracle', 'Monitoring', 'SQL', 'Python']",2025-06-13 05:44:56
Data Engineer - GCP,Happiest Minds Technologies,5 - 10 years,8-18 Lacs P.A.,"['Pune', 'Bengaluru']","Key Responsibilities:\nâ€¢ Data Pipeline Development: Designing, building, and maintaining robust data pipelines to move data from various sources (e.g., databases, external APIs, logs) to centralized data systems, such as data lakes or warehouses.\nâ€¢ Data Integration: Integrating data from multiple sources and ensuring it's processed in a consistent, usable format. This involves transforming, cleaning, and validating data to meet the needs of products, analysts and data scientists.\nâ€¢ Database Management: Creating, managing, and optimizing databases for storing large amounts of structured and unstructured data. Ensuring high availability, scalability, and security of data storage solutions.\nIdentifying and resolving issues related to the speed and efficiency of data systems. This could include optimizing queries, storage systems, and improving overall system architecture.\nâ€¢ : Automating routine tasks, such as data extraction, transformation, and loading (ETL), to ensure smooth data flows with minimal manual intervention.\nâ€¢ : Working closely with Work closely with product managers, UX/UI designers, and other stakeholders to understand data requirements and ensure data is in the right format for analysis and modeling.\nâ€¢ : Ensuring data integrity and compliance with data governance policies, including data quality standards, privacy regulations (e.g., GDPR), and security protocols.\n: Continuously monitoring data pipelines and databases for any disruptions or errors and troubleshooting any issues that arise to ensure continuous data flow.\nâ€¢ : Staying up to date with emerging data tools, technologies, and best practices in order to improve data systems and infrastructure.\nâ€¢ : Documenting data systems, pipeline processes, and data architectures, providing clear instructions for the team to follow, and ensuring that the architecture is understandable for stakeholders.",,,,"['Data Engineering', 'gcp', 'Python', 'SQL', 'Pyspark', 'Bigquery', 'Google Cloud Platforms']",2025-06-13 05:44:58
Digital Marketing Data Science Manager,Abinbev Gcc Services,6 - 10 years,Not Disclosed,['Bengaluru'],"AB InBev GCC was incorporated in 2014 as a strategic partner for Anheuser-Busch InBev. The center leverages the power of data and analytics to drive growth for critical business functions such as operations, finance, people, and technology. The teams are transforming Operations through Tech and Analytics.\nDo You Dream Big?\nWe Need You.\n\nJob Description",,,,"['NLP', 'Python', 'SQL', 'R', 'Power Bi', 'MFDL', 'Azure Cloud']",2025-06-13 05:45:00
Data Engineer,Axis Finance (AFL),7 - 11 years,Not Disclosed,"['Mumbai', 'Mumbai (All Areas)']","Key Responsibilities:\nShould have experience in below\nDesign, develop, and implement a Data Lake House architecture on AWS, ensuring scalability, flexibility, and performance.\nBuild ETL/ELT pipelines for ingesting, transforming, and processing structured and unstructured data.\nCollaborate with cross-functional teams to gather data requirements and deliver data solutions aligned with business needs.\nDevelop and manage data models, schemas, and data lakes for analytics, reporting, and BI purposes.\nImplement data governance practices, ensuring data quality, security, and compliance.\nPerform data integration between on-premise and cloud systems using AWS services.\nMonitor and troubleshoot data pipelines and infrastructure for reliability and scalability.\nSkills and Qualifications:\n7 + years of experience in data engineering, with a focus on cloud data platforms.\nStrong experience with AWS services: S3, Glue, Redshift, Athena, Lambda, IAM, RDS, and EC2.\nHands-on experience in building data lakes, data warehouses, and lake house architectures.\nShould have experience in ETL/ELT pipelines using tools like AWS Glue, Apache Spark, or similar.\nExpertise in SQL and Python or Java for data processing and transformations.\nFamiliarity with data modeling and schema design in cloud environments.\nUnderstanding of data security and governance practices, including IAM policies and data encryption.\nExperience with big data technologies (e.g., Hadoop, Spark) and data streaming services (e.g., Kinesis, Kafka).\nHave lending domain knowledge will be added advantage\nPreferred Skills:\nExperience with Databricks or similar platforms for data engineering.\nFamiliarity with DevOps practices for deploying data solutions on AWS (CI/CD pipelines).\nKnowledge of API integration and cloud data migration strategies.",Industry Type: NBFC,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['pipeline tools', 'lending domain', 'AWS', 'data models', 'spark', 'devops', 'databricks', 'date engineering platforms', 'hadoop', 'data lake house', 'API integration']",2025-06-13 05:45:02
Technical Architect,PwC India,10 - 15 years,Not Disclosed,"['Mumbai', 'Navi Mumbai', 'Gurugram']","Role Description\nWe are looking for a suitable candidate for the opening of Data/Technical Architect role for Data Management, preferably for one who has worked in Insurance or Banking and Financial Services domain and holds relevant experience of 10+ years. The candidate should be willing to take up the role of Senior Manager/Associate Director in an organization based on overall experience.\nLocation : Mumbai and Gurugram\nRelevant experience : 10+ years",,,,"['Data Architecture', 'Technical Architecture', 'Java', 'Bigquery', 'SCALA', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'Data Bricks', 'Python']",2025-06-13 05:45:04
Technology Architect,Accenture,8 - 11 years,Not Disclosed,['Kolkata'],"Project Role :Technology Architect\n\n\n\n\n\nProject Role Description :Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Cloud Data ArchitectureMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :BE or MCA\n\n\nSummary:As a Technology Architect, you will be responsible for reviewing and integrating all application requirements, including functional, security, integration, performance, quality, and operations requirements. Your typical day will involve reviewing and integrating technical architecture requirements, providing input into final decisions regarding hardware, network products, system software, and security, and utilizing Databricks Unified Data Analytics Platform to deliver impactful data-driven solutions.\nRoles & Responsibilities6 or more years of experience in implementing data ingestion pipelines from multiple sources and creating end to end data pipeline on Databricks platform. 2 or more years of experience using Python, PySpark or Scala. Experience in Databricks on cloud. Exp in any of AWS, Azure or GCPe, ETL, data engineering, data cleansing and insertion into a data warehouse\nMust have Skills like Databricks, Cloud Data Architecture, Python Programming Language, Data Engineering.\nProfessional AttributesExcellent writing, communication and presentation skills. Eagerness to learn and develop self on an ongoing basis. Excellent client facing and interpersonal skills.\n\nQualification\n\nBE or MCA",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data analytics', 'data architecture', 'data engineering', 'data bricks', 'hive', 'scala', 'system software', 'pyspark', 'microsoft azure', 'technology architecture', 'sql', 'pipeline', 'technical architecture', 'java', 'spark', 'kafka', 'data ingestion', 'sqoop', 'hadoop', 'aws', 'etl', 'big data']",2025-06-13 05:45:06
Security Architect,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Security Architect\n\n\n\n\n\nProject Role Description :Define the security architecture, ensuring that it meets the business requirements and performance goals.\n\n\n\nMust have skills :Security Platform Engineering\n\n\n\n\nGood to have skills :Java Enterprise Edition, Amazon Web Services (AWS), Infrastructure As Code (IaC)Minimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :BE or BTech Degree in Computer Science:As a Security Architect, you will be solving deep technical problems and building creative solutions in a dynamic environment working with knowledgeable and passionate SDEs. You are experienced building for the cloud designing for five 9s, globally distributed all active deployments, horizontal scalability, fault tolerance, and more. You are motivated by learning, evaluating, and deploying new technologies. Our services are deployed in an Amazon Web Services environment, and so you will be working hands on with many AWS components. You thrive in true agile, highly paced, production facing environment. You have a low tolerance for mediocrity. You love to write code and build extraordinary things.We are looking for coders, people who love to code, just like we do. You should be energetic, confident, and ready to contribute in many areas of the software development lifecycle. You may be involved in all the aspects from research, design, specs, coding, and bug fixing. Our team focus is on writing dependable code and getting high quality products and services to market as quick as possible.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Develop and implement security policies and procedures.- Conduct security assessments and audits.- Stay updated on the latest security trends and technologies.\nProfessional & Technical\n\n\n\n\nSkills:\n- Experience with CICD pipelines (i.e. Jenkins) with building and/or configuring pipelines for build and deployment of software.- Coding knowledge and experience with SQL- Experience of writing scripts in Python, PowerShell or Bash- Experience in building highly-available (HA) production-grade solutions in AWS.- CICV, CICD, Automation (Terraform a plus)- Experience designing/implementing high performance Web services using SOA/REST/Microservices- Experience in the design/build/maintenance/refactor of large scale low latency high performance systems- Ability to quickly learn and develop expertise in existing highly complex applications and architectures- Extensive knowledge with high volume distributed application development in cloud environment- Strong troubleshooting and debugging skills, particularly in both production and non-production environments.- Experience using Agile methodologies, TDD, Code review, clear and concise documentation- Strong analytic, problem solving, and troubleshooting skills- Uncommon ability and motivation to tackle problems and learn fast- Ability to perform at a high level within a technical team- Ability to work independently with minimal supervision- Excellent communication and relationship skills- Distributed teamwork\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Security Platform Engineering.- Minimum 3 years experience building AWS cloud native services using EC2, S3, ECS, SQS, API Gateway, Lambda, etc.- Minimum 5 years coding knowledge and experience with java and/or C++ and object oriented methodologies- Minimum 1 year experience with CICD pipelines (i.e. Jenkins) with building and/or configuring pipelines for build and deployment of software.- This position is based at our Pune office.- A BE or BTech Degree in Computer Science or related technical field or equivalent practical knowledge is required.\n\nQualification\n\nBE or BTech Degree in Computer Science",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['java', 'debugging', 'aws cloud native', 'troubleshooting', 'ci cd pipeline', 'c++', 'api gateway', 'soa', 'ci/cd', 'microservices', 'sql', 'ecs', 'aws cloud', 'oops', 'powershell', 'j2ee', 'jenkins', 'agile methodology', 'rest', 'python', 'amazon sqs', 'amazon ec2', 'lambda expressions', 'tdd', 'bash', 'terraform', 'aws']",2025-06-13 05:45:08
Technology Architect,Accenture,7 - 12 years,Not Disclosed,['Ahmedabad'],"Project Role :Technology Architect\n\n\n\n\n\nProject Role Description :Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\n\n\n\nMust have skills :Amazon Web Services (AWS)\n\n\n\n\nGood to have skills :Java Full Stack Development, Python (Programming Language), DevOpsMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Technology Architect, you will review and integrate all application requirements, including functional, security, integration, performance, quality, and operations requirements. You will also review and integrate the technical architecture requirements and provide input into final decisions regarding hardware, network products, system software, and security.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead architectural discussions and decisions- Develop and maintain architecture artifacts- Ensure alignment of technology solutions with business goals\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Amazon Web Services (AWS)- Good To Have\n\n\n\n\nSkills:\nExperience with Python (Programming Language)- Strong understanding of cloud computing principles- Knowledge of infrastructure as code- Experience in designing scalable and secure cloud solutions\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Amazon Web Services (AWS).- This position is based at our Ahmedabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'technical architecture', 'java', 'cloud computing', 'aws', 'css', 'c++', 'c', 'web services', 'system software', 'networking', 'bootstrap', 'technology architecture', 'ssl', 'technology solutions', 'javascript', 'sql', 'eclipse', 'notepad++', 'spring', 'react.js', 'design and coding', 'jenkins', 'mysql']",2025-06-13 05:45:09
Technology Architect,Accenture,15 - 20 years,Not Disclosed,['Indore'],"Project Role :Technology Architect\n\n\n\n\n\nProject Role Description :Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\n\n\n\nMust have skills :Amazon Web Services (AWS)\n\n\n\n\nGood to have skills :Java Full Stack DevelopmentMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years of fulltime education.Role:Technology Architect\n\nProject Role Description:Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\nMust have Skills :Amazon Web Services (AWS), SSINON SSI:Good to Have Skills :SSI:Java Full Stack Development NON SSI :Job :'',//field Key Responsibilities:1 Experience of designing multiple Cloud-native Application Architectures2 Experience of developing and deploying cloud-native application including serverless environment like Lambda 3 Optimize applications for AWS environment 4 Design, build and configure applications on AWS environment to meet business process and application requirements5 Understanding of security performance and cost optimizations for AWS6 Understanding to AWS Well-Architected best practices\nTechnical Experience:1 8/15 years of experience in the industry with at least 5 years and above in AWS 2 Strong development background with exposure to majority of services in AWS3 AWS Certified Developer professional and/or AWs specialty level certification DevOps /Security 4 Application development skills on AWS platform with either Java SDK, Python SDK, Reactjs5 Strong in coding using any of the programming languages like Python/Nodejs/Java/Net understanding of AWS architectures across containerization microservices and serverless on AWS 6 Preferred knowledge in cost explorer, budgeting and tagging in AWS 7 Experience with DevOps tools including AWS native DevOps tools like CodeDeploy,\nProfessional Attributes:a Ability to harvest solution and promote reusability across implementations b Self Motivated experts who can work under their own direction with right set of design thinking expertise c Proven interpersonal skills while contributing to team effort by accomplishing related results as needed\nEducational Qualification:15 years of fulltime education. Additional Info:1 Application developers skills on AWS platform with either Java SDK, Python SDK, Nodejs, ReactJS 2 AWS services Lambda, AWS Amplify, AWS App Runner, AWS CodePipeline, AWS Cloud nine, EBS, Faregate,Additional comment- Only Bangalore, No Location Flex and No Level Flex\n\nQualification\n\n15 years of fulltime education.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ssi', 'java', 'lambda expressions', 'aws cloud', 'aws', 'java sdk', 'python', 'aws iam', 'web services', 'java fullstack', 'aws codedeploy', 'technology architecture', 'amplify', 'budgeting', 'microservices', 'react.js', 'code deploy', 'node.js', 'technical architecture', 'devops', 'sdk', 'runner']",2025-06-13 05:45:11
Technology Architect,Accenture,7 - 12 years,Not Disclosed,['Ahmedabad'],"Project Role :Technology Architect\n\n\n\n\n\nProject Role Description :Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\n\n\n\nMust have skills :Amazon Web Services (AWS)\n\n\n\n\nGood to have skills :Java Full Stack Development, Python (Programming Language), DevOpsMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Technology Architect, you will review and integrate all application requirements, including functional, security, integration, performance, quality, and operations requirements. You will also review and integrate the technical architecture requirements and provide input into final decisions regarding hardware, network products, system software, and security.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead architectural discussions and drive decisions- Develop and maintain architecture artifacts- Conduct technology assessments and evaluations\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Amazon Web Services (AWS)- Good To Have\n\n\n\n\nSkills:\nExperience with Python (Programming Language)- Strong understanding of cloud computing principles- Experience in designing scalable and secure cloud solutions- Knowledge of infrastructure as code tools like Terraform\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Amazon Web Services (AWS)- This position is based at our Ahmedabad office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'java', 'terraform', 'cloud computing', 'aws', 'css', 'c++', 'c', 'web services', 'system software', 'networking', 'bootstrap', 'technology architecture', 'ssl', 'javascript', 'sql', 'eclipse', 'notepad++', 'spring', 'react.js', 'technical architecture', 'design and coding', 'jenkins', 'mysql']",2025-06-13 05:45:13
Solution Architect,Accenture,15 - 20 years,Not Disclosed,['Pune'],"Project Role :Solution Architect\n\n\n\n\n\nProject Role Description :Translate client requirements into differentiated, deliverable solutions using in-depth knowledge of a technology, function, or platform. Collaborate with the Sales Pursuit and Delivery Teams to develop a winnable and deliverable solution that underpins the client value proposition and business case.\n\n\n\nMust have skills :SAP for Utilities Billing\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. Your typical day will involve collaborating with teams to develop solutions and ensure applications align with business needs.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the team in implementing innovative solutions- Conduct regular team meetings to discuss progress and challenges- Stay updated on industry trends and technologies to enhance team performance\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP for Utilities Billing- Good To Have\n\n\n\n\nSkills:\nExperience with SAP ABAP Development for HANA- Strong understanding of SAP modules and integration- Experience in customizing SAP applications to meet specific business requirements- Knowledge of SAP data migration and data management processes- Ability to troubleshoot and resolve technical issues in SAP applications\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap data migration', 'sap', 'sap module', 'billing', 'sap abap', 'project management', 'lsmw', 'sap bods', 'data warehousing', 'business analysis', 'data migration', 'presales', 'sales', 'sql server', 'sql', 'sap s hana', 'bods', 'bapi', 'sap data services', 'sap mm', 'agile', 'sap hana', 'etl', 'business case']",2025-06-13 05:45:16
Technology Architect,Accenture,5 - 10 years,Not Disclosed,['Kolkata'],"Project Role :Technology Architect\n\n\n\n\n\nProject Role Description :Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\n\n\n\nMust have skills :Amazon Web Services (AWS)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :Should be a grad\n\n\nSummary:As a Technology Architect, you will be responsible for reviewing and integrating all application requirements, including functional, security, integration, performance, quality, and operations requirements. Your typical day will involve reviewing and integrating technical architecture requirements, providing input into final decisions regarding hardware, network products, system software, and security, and ensuring the successful delivery of AWS-based solutions.\nRoles & Responsibilities:- Lead the design and implementation of AWS-based solutions, ensuring alignment with business requirements and technical architecture standards.- Collaborate with cross-functional teams to review and integrate all application requirements, including functional, security, integration, performance, quality, and operations requirements.- Provide input into final decisions regarding hardware, network products, system software, and security, ensuring compliance with technical architecture requirements.- Ensure the successful delivery of AWS-based solutions, including monitoring and troubleshooting to identify and resolve technical issues.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nStrong experience in Amazon Web Services (AWS), including AWS architecture, AWS services, and AWS implementation best practices.- Good To Have\n\n\n\n\nSkills:\nExperience with other cloud platforms, such as Microsoft Azure or Google Cloud Platform.- Strong understanding of technical architecture requirements, including functional, security, integration, performance, quality, and operations requirements.- Experience with hardware, network products, system software, and security, including knowledge of industry standards and best practices.- Experience with monitoring and troubleshooting AWS-based solutions, including identifying and resolving technical issues.- Solid grasp of project management methodologies, including Agile and Waterfall, and experience leading cross-functional teams.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Amazon Web Services (AWS).- The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful AWS-based solutions.- This position is based at our Kolkata office.\n\nQualification\n\nShould be a grad",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information technology', 'project management process', 'technical architecture', 'gcp', 'aws', 'c#', 'project management', 'aws iam', 'web services', 'program management', 'system software', 'microsoft azure', 'technology architecture', 'vb', 'pmp', 'java', 'waterfall', 'troubleshooting', '.net', 'agile']",2025-06-13 05:45:18
Business Architect,Accenture,15 - 20 years,Not Disclosed,['Chennai'],"Project Role :Business Architect\n\n\n\n\n\nProject Role Description :Define opportunities to create tangible business value for the client by leading current state assessments and identifying high level customer requirements, defining the business solutions and structures needed to realize these opportunities, and developing business case to achieve the vision.\n\n\n\nMust have skills :Temenos Transact\n\n\n\n\nGood to have skills :Temenos Transact Development, Business AnalysisMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Architect, you will define opportunities to create tangible business value for the client by leading current state assessments and identifying high level customer requirements, defining the business solutions and structures needed to realize these opportunities, and developing business case to achieve the vision.\nRoles & Responsibilities:- Must be able to understand the requirements, functional solution, technical solutions and do development based on the requirements.- Must have worked in Transact implementation projects - Strong understanding of Retail, Corporate & Wealth banking process - Prior experience of writing Technical Specification Document, Integration Requirement Document, Integration Technical Design Document is must - Understand the Solution design Develop and deliver the development package as per the agreed timeline.- Able to work independently and design Architecture blueprint for the target solution.- Managed the offshore team members and delivered the quality work products from offshore to multiple client projects.\nProfessional & Technical\n\n\n\n\nSkills:\n- Should have good experience in managing and delivery Temenos Transact development projects.- Good understanding of Transact Core modules, CUSTOMER, AA Account, TPH, Temenos Data Event Streaming, Temenos APIs, Data migration etc.- Good understanding on Temenos Adapter Framework, Technical Stack, Integration touchpoints etc.- Should have good experience in complex & highly complex interface developments.- Should have good experience in IRIS, Integration Framework, Web-services, API Development, Data Event Streaming etc.- Candidate should possess excellent communication written oral and interpersonal skills- Experience working in Agile environment- Ability to learn new client applications- Candidate able to manage the team members by allocating tasks on daily basis- Candidate able to guide the team and get the output from the team members.- Monitor & report the development completion status.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Temenos Transact.- This position is based at our Chennai office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['retail', 'iris', 'technical design', 'temenos', 'agile', 'css', 'c++', 'web services', 'tafj', 'infobasic', 'jquery', 'spring', 'java', 'solution design', 'linux', 'html', 'c', 'business analysis', 'microsoft azure', 'javascript', 'temenos t24', 'design studio', 't24', 'jbase', 'business architecture', 'soap']",2025-06-13 05:45:19
Technology Architect,Accenture,8 - 13 years,Not Disclosed,['Coimbatore'],"Project Role :Technology Architect\n\n\n\n\n\nProject Role Description :Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Cloud Data ArchitectureMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : BE or MCA\n\n\nSummary:As a Technology Architect, you will be responsible for reviewing and integrating all application requirements, including functional, security, integration, performance, quality, and operations requirements. Your typical day will involve reviewing and integrating technical architecture requirements, providing input into final decisions regarding hardware, network products, system software, and security, and utilizing Databricks Unified Data Analytics Platform to deliver impactful data-driven solutions.\nRoles & ResponsibilitiesShould have a minimum of 8 years of experience in Databricks Unified Data Analytics Platform. Should have strong educational background in technology and information architectures, along with a proven track record of delivering impactful data-driven solutions. Strong requirement analysis and technical solutioning skill in Data and Analytics Client facing role in terms of running solution workshops, client visits, handled large RFP pursuits and managed multiple stakeholders.\nTechnical Experience10 or more years of experience in implementing data ingestion pipelines from multiple sources and creating end to end data pipeline on Databricks platform. 3 or more years of experience using Python, PySpark or Scala. Experience in Databricks on cloud. Exp in any of AWS, Azure or GCPe, ETL, data engineering, data cleansing and insertion into a data warehouse\nMust have Skills like Databricks, Cloud Data Architecture, Python Programming Language, Data Engineering.\nProfessional AttributesExcellent writing, communication and presentation skills. Eagerness to learn and develop self on an ongoing basis. Excellent client facing and interpersonal skills.\n\nQualification\n\nBE or MCA",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data analytics', 'data architecture', 'data engineering', 'data bricks', 'scala', 'system software', 'pyspark', 'microsoft azure', 'technology architecture', 'hibernate', 'sql', 'spring', 'technical architecture', 'java', 'information architecture', 'requirement analysis', 'aws', 'etl', 'rfp']",2025-06-13 05:45:21
Technology Architect,Accenture,5 - 10 years,Not Disclosed,['Coimbatore'],"Project Role :Technology Architect\n\n\n\n\n\nProject Role Description :Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Cloud Data ArchitectureMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :BE or MCA\n\n\nSummary:As a Technology Architect, you will review and integrate all application requirements, including functional, security, integration, performance, quality, and operations requirements. You will also review and integrate the technical architecture requirements and provide input into final decisions regarding hardware, network products, system software, and security.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the design and implementation of technology solutions.- Develop technical specifications and architecture designs.- Ensure compliance with architectural standards and guidelines.- Conduct technology evaluations and provide recommendations.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Cloud Data Architecture.- Strong understanding of data architecture principles.- Experience in designing and implementing data solutions.- Knowledge of cloud-based data technologies.- Ability to analyze complex technical requirements.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A BE or MCA is required.\n\nQualification\n\nBE or MCA",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'data architecture', 'data bricks', 'technical architecture', 'data architecture principles', 'architectural design', 'c#', 'system software', 'microsoft azure', 'technology architecture', 'hibernate', 'technology solutions', 'microservices', 'spring', 'java', 'j2ee', 'aws']",2025-06-13 05:45:23
Architect,Ford,17 - 20 years,Not Disclosed,['Chennai'],"We are seeking a highly motivated and experienced Architect for this leadership position reporting to the Director of Architecture, Ford Credit Europe. In this role, you will be a key individual contributor working with senior business and engineering partners to create the future designs of Ford Credit platforms whilst also helping to guide, steer and mentor other team members on their deliverables\nRequired Skills and Selection Criteria:\nMinimum of 17+ years overall IT experience in architecture or software engineering roles\nMinimum of 6+ years hands-on Enterprise or Solution Architecture experience\nBachelors degree in computer science, engineering, or a related field.\nGoogle Cloud Professional Cloud Architect certification or equivalent\nDeep technical proficiency in developing architecture designs for digital front end and back end foundational platforms, ideally React, Java Springboot, Apollo GraphQL, Kafka / PubSub, PostgreSQL, BigQuery technologies.\nExperience of migrating, designing and building applications in modern cloud hosting environments, specifically leveraging Google Cloud Platform services.\nKnowledge of enterprise frameworks and technologies. Strong in architecture design patterns, experience with secure interoperability standards and methods, architecture tools and process.\nSolid understanding of architectural principles and best practices, particularly in the context of lending and financial systems. Experience within the Banking and Financial sector, including knowledge of relevant regulatory and compliance requirements preferred.\nGood knowledge of strategic, new, and emerging technology trends, and the practical application of them within enterprise landscapes.\nExcellent communication skills, both written and verbal, with ability to communicate and collaborate on architectural proposals with diverse audiences (user groups, stakeholders and senior management).\nStrong problem-solving skills and the ability to adapt to changing business needs. Ability to work on multiple projects in a fast paced & dynamic environment.\nExperience with agile methodologies and modern development practices.\nTrack record of designing secure architectures, including knowledge of common security patterns, threats, and mitigation techniques relevant to digital platforms and financial applications.\nStrong understanding of data architecture principles, including data modeling, data governance, data integration patterns, and experience designing solutions that utilize various database technologies and data warehousing concepts, ideally with PostgreSQL and BigQuery experience.\nProvide Architecture support to European and Indian based engineering squads & product teams. Create and maintain comprehensive architectural documentation, including diagrams, specifications, and design patterns.\nContribute to the design and implementation of architectural frameworks for Ford Credit s European systems, ensuring alignment with the overall global architecture vision.\nDevelop and maintain architecture artefacts including roadmaps and current / future state designs following industry standard architecture models and best practices.\nSupport the European architecture team, developing talent of junior team members. Provide technical guidance and mentoring to architecture and engineering teams, ensuring adherence to architectural standards and best practices.\nProvide technical leadership and guidance to product teams. Collaborate with key business stakeholders, IT product teams, engineering management and external partners. Collaborate with cross-functional teams at a global level to deliver complex integrated solutions.\nIdentify and address performance bottlenecks, security vulnerabilities, and scalability issues within existing systems. Ensure all architectural solutions adhere to security best practices and regulatory compliance requirements.\nDesign transitional architectures to help product teams on their modernisation journeys and guide teams through required enterprise architecture processes ensuring alignment with standards.\nProvide architecture assessments on technical solutions and proposals, make recommendations that meet business needs and present in architectural governance forums.\nEvaluate new technologies and platforms, providing recommendations to the Director of Architecture on their potential application within Ford Credit.\nExperience in migrating legacy applications to Cloud platforms and business adoption of these platforms for Automotive Lending, Banking and Insurance\nDeep understanding of domain driven design and data mesh principles.\nHands-on experience designing, building, and deploying applications based on Java, Angular, React, PostGres, GraphQL, REST APIs, CI/CD pipelines, and Event based architecture\nLead a team of 2 to 4 Solution architects to utilize Google Cloud Platform & Services to modernize legacy applications.\nHands on experience in implementing API, Micro-services and application security\nStrong exposure working in an environment where Development processes - Agile, SAFe been used.\nShould have experience in BFS domain\nGood exposure to Google Cloud Platform\nPractical experience implementing cloud security best practices and different security tools and techniques like Identity and Access Management (IAM), Encryption, Network Security, etc.\nUnderstand complexity of business processes, current application landscape and create modernization roadmaps\nLead creation and management of Application integration models, GCP infrastructure provisioning and DevOps pipelines\nNice to Have\nMaster s degree in Computer science/engineering, Data science or related field.\nExperience in building high performance teams.\nAbility to negotiate & influence stakeholders for driving strategic solutions\nStrong leadership, communication, interpersonal, organizing, and problem-solving skills\nGood presentation skills with ability to communicate architectural proposals to diverse audiences (user groups, stakeholders, and senior management).\nExperience in Banking and Financial Regulatory Reporting space.\nAbility to work on multiple projects in a fast paced & dynamic environment.\nExposure to multiple, diverse technologies, platforms, and processing environments.",Industry Type: Automobile,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Front end', 'Data modeling', 'Access management', 'Postgresql', 'Agile', 'Network security', 'Application security', 'Automotive']",2025-06-13 05:45:25
Architect,Ford,8 - 13 years,Not Disclosed,['Chennai'],"We are seeking a highly motivated and experienced Architect for this leadership position reporting to the Director of Architecture, Ford Credit Europe. In this role, you will be a key individual contributor working with senior business and engineering partners to create the future designs of Ford Credit platforms whilst also helping to guide, steer and mentor other team members on their deliverables\nRequired Skills and Selection Criteria:\nMinimum of 17+ years overall IT experience in architecture or software engineering roles\nMinimum of 6+ years hands-on Enterprise or Solution Architecture experience\nBachelors degree in computer science, engineering, or a related field.\nGoogle Cloud Professional Cloud Architect certification or equivalent\nDeep technical proficiency in developing architecture designs for digital front end and back end foundational platforms, ideally React, Java Springboot, Apollo GraphQL, Kafka / PubSub, PostgreSQL, BigQuery technologies.\nExperience of migrating, designing and building applications in modern cloud hosting environments, specifically leveraging Google Cloud Platform services.\nKnowledge of enterprise frameworks and technologies. Strong in architecture design patterns, experience with secure interoperability standards and methods, architecture tools and process.\nSolid understanding of architectural principles and best practices, particularly in the context of lending and financial systems. Experience within the Banking and Financial sector, including knowledge of relevant regulatory and compliance requirements preferred.\nGood knowledge of strategic, new, and emerging technology trends, and the practical application of them within enterprise landscapes.\nExcellent communication skills, both written and verbal, with ability to communicate and collaborate on architectural proposals with diverse audiences (user groups, stakeholders and senior management).\nStrong problem-solving skills and the ability to adapt to changing business needs. Ability to work on multiple projects in a fast paced & dynamic environment.\nExperience with agile methodologies and modern development practices.\nTrack record of designing secure architectures, including knowledge of common security patterns, threats, and mitigation techniques relevant to digital platforms and financial applications.\nStrong understanding of data architecture principles, including data modeling, data governance, data integration patterns, and experience designing solutions that utilize various database technologies and data warehousing concepts, ideally with PostgreSQL and BigQuery experience.\nProvide Architecture support to European and Indian based engineering squads & product teams. Create and maintain comprehensive architectural documentation, including diagrams, specifications, and design patterns.\nContribute to the design and implementation of architectural frameworks for Ford Credit s European systems, ensuring alignment with the overall global architecture vision.\nDevelop and maintain architecture artefacts including roadmaps and current / future state designs following industry standard architecture models and best practices.\nSupport the European architecture team, developing talent of junior team members. Provide technical guidance and mentoring to architecture and engineering teams, ensuring adherence to architectural standards and best practices.\nProvide technical leadership and guidance to product teams. Collaborate with key business stakeholders, IT product teams, engineering management and external partners. Collaborate with cross-functional teams at a global level to deliver complex integrated solutions.\nIdentify and address performance bottlenecks, security vulnerabilities, and scalability issues within existing systems. Ensure all architectural solutions adhere to security best practices and regulatory compliance requirements.\nDesign transitional architectures to help product teams on their modernisation journeys and guide teams through required enterprise architecture processes ensuring alignment with standards.\nProvide architecture assessments on technical solutions and proposals, make recommendations that meet business needs and present in architectural governance forums.\nEvaluate new technologies and platforms, providing recommendations to the Director of Architecture on their potential application within Ford Credit.\nExperience in migrating legacy applications to Cloud platforms and business adoption of these platforms for Automotive Lending, Banking and Insurance\nDeep understanding of domain driven design and data mesh principles.\nHands-on experience designing, building, and deploying applications based on Java, Angular, React, PostGres, GraphQL, REST APIs, CI/CD pipelines, and Event based architecture\nLead a team of 2 to 4 Solution architects to utilize Google Cloud Platform & Services to modernize legacy applications.\nHands on experience in implementing API, Micro-services and application security\nStrong exposure working in an environment where Development processes - Agile, SAFe been used.\nShould have experience in BFS domain\nGood exposure to Google Cloud Platform\nPractical experience implementing cloud security best practices and different security tools and techniques like Identity and Access Management (IAM), Encryption, Network Security, etc.\nUnderstand complexity of business processes, current application landscape and create modernization roadmaps\nLead creation and management of Application integration models, GCP infrastructure provisioning and DevOps pipelines\nNice to Have\nMaster s degree in Computer science/engineering, Data science or related field.\nExperience in building high performance teams.\nAbility to negotiate & influence stakeholders for driving strategic solutions\nStrong leadership, communication, interpersonal, organizing, and problem-solving skills\nGood presentation skills with ability to communicate architectural proposals to diverse audiences (user groups, stakeholders, and senior management).\nExperience in Banking and Financial Regulatory Reporting space.\nAbility to work on multiple projects in a fast paced & dynamic environment.\nExposure to multiple, diverse technologies, platforms, and processing environments.",Industry Type: Auto Components,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Front end', 'Data modeling', 'Access management', 'Postgresql', 'Agile', 'Network security', 'Application security', 'Automotive']",2025-06-13 05:45:27
Data Migration Consultant - Gurgaon - F2F Interview,Echo It Solutions,3 - 8 years,10-15 Lacs P.A.,['Gurugram'],"Experienced SSIS Developer to design, develop, deploy, and maintain ETL solutions using SSIS. The candidate should have experience in data migration, data transformation, and integration workflows between multiple systems, including OCI.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Data Migration', 'OCI', 'SSIS', 'Oracle Cloud Infrastructure', 'ETL']",2025-06-13 05:45:28
Data Engineer For a US based IT Company based in Hyderabad,GLOBAL INSTITUTE FOR STAFFING & TRAINING...,5 - 10 years,16-20 Lacs P.A.,['Hyderabad( Kokapeta Village )'],"We are Hiring Data Engineer for a US based IT Company Based in Hyderabad. Candidates with minimum 5 Years of experience in Data Engineering can apply.\n\nThis job is for 1 year contract only\n\nJob Title: Data Engineer\nLocation: Hyderabad\nCTC: Upto 20 LPA\nExperience: 5+ Years\n\nJob Overview:\nWe are looking for a seasoned Senior Data Engineer with deep hands-on experience in Talend and IBM DataStage to join our growing enterprise data team. This role will focus on designing and optimizing complex data integration solutions that support enterprise-wide analytics, reporting, and compliance initiatives.\nIn this senior-level position, you will collaborate with data architects, analysts, and key stakeholders to facilitate large-scale data movement, enhance data quality, and uphold governance and security protocols.\n\nKey Responsibilities:\nDevelop, maintain, and enhance scalable ETL pipelines using Talend and IBM DataStage\nPartner with data architects and analysts to deliver efficient and reliable data integration solutions\nReview and optimize existing ETL workflows for performance, scalability, and reliability\nConsolidate data from multiple sourcesboth structured and unstructuredinto data lakes and enterprise platforms\nImplement rigorous data validation and quality assurance procedures to ensure data accuracy and integrity\nAdhere to best practices for ETL development, including source control and automated deployment\nMaintain clear and comprehensive documentation of data processes, mappings, and transformation rules\nSupport enterprise initiatives around data migration, modernization, and cloud transformation\nMentor junior engineers and participate in code reviews and team learning sessions\nRequired Qualifications:\nMinimum 5 years of experience in data engineering or ETL development\nProficient with Talend (Open Studio and/or Talend Cloud) and IBM DataStage\nStrong skills in SQL, data profiling, and performance tuning\nExperience handling large datasets and complex data workflows\nSolid understanding of data warehousing, data modeling, and data lake architecture\nFamiliarity with version control systems (e.g., Git) and CI/CD pipelines\nStrong analytical and troubleshooting skills\nEffective verbal and written communication, with strong documentation habits\n\nPreferred Qualifications:\nPrior experience in banking or financial services\nExposure to cloud platforms such as AWS, Azure, or Google Cloud\nKnowledge of data governance tools (e.g., Collibra, Alation)\nAwareness of data privacy regulations (e.g., GDPR, CCPA)\nExperience working in Agile/Scrum environments\n\nFor further assistance contact/whatsapp: 9354909518 or write to priya@gist.org.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Data Engineering', 'Kafka', 'Snowflake', 'Java', 'Spark', 'mongo', 'Ci/Cd', 'Data Pipeline', 'Agile', 'Scrum', 'Data Modeling', 'Talend', 'Oracle', 'AWS', 'Data Governance', 'Gdpr', 'azure', 'Python', 'Shell Scripting', 'Postgresql', 'Ibm Datastage', 'Workflow', 'Data Framework', 'GCPA', 'SQL', 'GIT', 'Amazon Redshift', 'GCP', 'Data Lake', 'Data Warehousing', 'ETL']",2025-06-13 05:45:30
Data Engineer,Mobio Solutions,3 - 5 years,Not Disclosed,['Ahmedabad'],"Job Overview:\nWe are looking for a skilled and experienced Data Engineer to join our team. The ideal candidate will have a strong background in Azure Data Factory, Databricks, Pyspark, Python , Azure SQL and other Azure cloud services, and will be responsible for building and managing scalable data pipelines, data lakes, and data warehouses . Experience with Azure Synapse Analytics, Microsoft Fabric or PowerBI will be considered a strong advantage.\nKey Responsibilities:\nDesign, develop, and manage robust and scalable ETL/ELT pipelines using Azure Data Factory and Databricks\nWork with PySpark and Python to transform and process large datasets\nBuild and maintain data lakes and data warehouses on Azure Cloud\nCollaborate with data architects, analysts, and stakeholders to gather and translate requirements into technical solutions\nEnsure data quality, consistency, and integrity across systems\nOptimize performance and cost of data pipelines and cloud infrastructure\nImplement best practices for security, governance, and monitoring of data pipelines\nMaintain and document data workflows and architecture\nRequired Skills & Qualifications:\n3-5 years of experience in Data Engineering\nStrong hands-on experience with:\nAzure Data Factory (ADF)\nAzure Databricks\nAzure SQL\nPySpark and Python\nAzure Storage (Blob, Data Lake Gen2)\nHands-on experience with data warehouse/Lakehouse/data lake architecture\nFamiliarity with Delta Lake, MLflow, and Unity Catalog is a plus\nGood understanding of SQL and performance tuning\nKnowledge of CI/CD in Azure for data pipelines\nExcellent problem-solving skills and ability to work independently\nPreferred Skills:\nExperience with Azure Synapse Analytics\nFamiliarity with Microsoft Fabric\nWorking knowledge of Power BI for data visualization and dashboarding\nExposure to DevOps and infrastructure as code (IaC) in Azure\nUnderstanding of data governance and security best practices\nDatabricks certification (e.g., Databricks Certified Data Engineer Associate/Professional)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Cloud', 'data governance', 'Data quality', 'data visualization', 'microsoft', 'Analytics', 'Monitoring', 'SQL', 'Python']",2025-06-13 05:45:32
Lead Data Engineer,anblicks,6 - 8 years,Not Disclosed,['Ahmedabad'],"Job Summary:\nWe are seeking a Senior Data Engineer with hands-on experience building scalable data pipelines using Microsoft Fabric. The role focuses on delivering ingestion, transformation, and enrichment workflows across medallion architecture.\nKey Responsibilities:\nDevelop and maintain data pipelines using Microsoft Fabric Data Factory and OneLake.\nDesign and build ingestion and transformation pipelines for structured and unstructured data.\nImplement frameworks for metadata tagging, version control, and batch tracking.\nEnsure security, quality, and compliance of data pipelines.\nContribute to CI/CD integration, observability, and documentation.\nCollaborate with data architects and analysts to meet business requirements.\nQualifications:\n6+ years of experience in data engineering; 2+ years working on Microsoft Fabric or Azure Data services.\nHands-on with tools like Azure Data Factory, Fabric, Databricks, or Synapse.\nStrong SQL and data processing skills (e.g., PySpark, Python).\nExperience with data cataloging, lineage, and governance frameworks.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['metadata', 'data services', 'Version control', 'Compliance', 'Architecture', 'Data processing', 'microsoft', 'SQL', 'Python']",2025-06-13 05:45:33
Data Engineer,Hinduja Tech,6 - 10 years,Not Disclosed,['Pune'],"Education: Bachelors or masters degree in computer science, Information Technology, Engineering, or a related field.Experience: 6-10 years\n8+ years of experience in data engineering or a related field.\nStrong hands-on experience with Azure Databricks, Spark, Python/Scala, CICD, Scripting for data processing.\nExperience working in multiple file formats like Parquet, Delta, and Iceberg.\nKnowledge of Kafka or similar streaming technologies for real-time data ingestion.",,,,"['Data Engineer', 'Azure Databricks', 'ETL', 'Pyspark', 'AWS', 'Python', 'SQL']",2025-06-13 05:45:35
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,['Pune'],"Job Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin']",2025-06-13 05:45:37
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,['Indore'],"Job Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin']",2025-06-13 05:45:38
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,[],"Job Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin']",2025-06-13 05:45:40
Data Modeler - SQL / Erwin,Leading Client,7 - 10 years,Not Disclosed,[],"Employment Type : Contract (Remote).\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin']",2025-06-13 05:45:41
Data Modeler - SQL / Erwin,Leading Client,7 - 10 years,Not Disclosed,[],"Employment Type : Contract (Remote).\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin']",2025-06-13 05:45:43
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,[],"Employment Type : Contract (Remote).\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin']",2025-06-13 05:45:44
Software Data Operations Engineer (BS+2),MAQ Software,2 - 5 years,Not Disclosed,['Noida'],"MAQ LLC d.b.a MAQ Software hasmultiple openings at Redmond, WA for:\nSoftware Data Operations Engineer (BS+2)\n\nResponsible for gathering & analyzing business requirements from customers. Implement,test and integrate software applications for use by customers. Develop &review cost effective data architecture to ensure appropriateness with currentindustry advances in data management, cloud & user experience. Automateuser test scenarios, debug & fix errors in cloud-based data infrastructure,reporting applications to meet customer needs. Must be able to traveltemporarily to client sites and or relocate throughout the United States.\n\nRequirements:Bachelors Degree or foreign equivalent in Computer Science, ComputerApplications, Computer Information Systems, Information Technology or relatedfield with two years of work experience in job offered, software engineer, systemsanalyst or related job.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Data Operations', 'software applications', 'data management', 'Data Operations', 'data architecture', 'data infrastructure']",2025-06-13 05:45:46
Product Manager - Data Management,Reuters,12 - 15 years,Not Disclosed,"['Mumbai', 'Hyderabad']","We are seeking an experienced Product Manager-Data Management to lead the development and adoption of our 3rd party data platforms, including D&B and other similar platforms. The successful candidate will be responsible for driving the integration and utilization of 3rd party data across marketing campaigns, improving data quality and accuracy, and expanding the use cases and applications for 3rd party data.\n  About the Role\nDevelop and execute a comprehensive strategy for 3rd party data platform adoption and expansion across the organization, with a focus on driving business outcomes and improving marketing effectiveness.\nCollaborate with marketing teams to integrate 3rd party data into their campaigns and workflows and provide training and support to ensure effective use of the data.\nDevelop and showcase compelling use cases that demonstrate the value of 3rd party data in improving marketing effectiveness and measure the success of these use cases through metrics such as adoption rate, data quality, and marketing ROI.\nDevelop and maintain a roadmap for 3rd party data platform adoption and expansion across the organization, with a focus on expanding use cases and applications for 3rd party data and developing new data-driven products and services.\nMonitor and measure the effectiveness of 3rd party data in driving business outcomes, and adjust the adoption strategy accordingly\nWork with cross-functional teams to ensure data quality and governance, and develop and maintain relationships with 3rd party data vendors to ensure seamless data integration and delivery.\nDrive the development of new data-driven products and services that leverage 3rd party data, and collaborate with stakeholders to prioritize and develop these products and services.\nShift Timings: 2 PM to 11 PM (IST).\nWork from office for 2 days in a week (Mandatory).\nAbout You\n12+ years of experience in data management, product management, or a related field.\nBachelors or Masters degree in Computer Science, Data Science, Information Technology, or a related field.\nExperience with data management tools such as data warehousing, ETL (Extract, Transform, Load), data governance, and data quality.\nUnderstanding of the Marketing domain and data platforms such as Treasure Data, Salesforce, Eloqua, 6Sense, Alteryx, Tableau and Snowflake within a MarTech stack.\nExperience with machine learning and AI frameworks (eg, TensorFlow, PyTorch).\nExpertise in SQL and Alteryx.\nExperience with data integration tools and technologies such as APIs, data pipelines, and data virtualization.\nExperience with data quality and validation tools and techniques such as data profiling, data cleansing, and data validation.\nStrong understanding of data modeling concepts, data architecture, and data governance.\nExcellent communication and collaboration skills.\nAbility to drive adoption and expansion of D&B data across the organization.\nCertifications in data management, data governance, or data science is nice to have.\nExperience with cloud-based data platforms (eg, AWS, GCP, Azure) nice to have.\nKnowledge of machine learning and AI concepts, including supervised and unsupervised learning, neural networks, and deep learning nice to have.\nWhat s in it For You\nHybrid Work Model: we've adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected.\nFlexibility & Work-Life Balance: Flex My Way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. This builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance.\nCareer Development and Growth: By fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow s challenges and deliver real-world solutions. Our Grow My Way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an AI-enabled future.\nIndustry Competitive Benefits: We offer comprehensive benefit plans to include flexible vacation, two company-wide Mental Health Days off, access to the Headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial we'llbeing.\nCulture: Globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. We live by our values: Obsess over our Customers, Compete to Win, Challenge (Y)our Thinking, Act Fast / Learn Fast, and Stronger Together.\nSocial Impact: Make an impact in your community with our Social Impact Institute. We offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and Environmental, Social, and Governance (ESG) initiatives.\nMaking a Real-World Impact: We are one of the few companies globally that helps its customers pursue justice, truth, and transparency. Together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.",Industry Type: Internet,Department: Product Management,"Employment Type: Full Time, Permanent","['Computer science', 'data cleansing', 'Data validation', 'Data management', 'Data modeling', 'Consulting', 'Data quality', 'Information technology', 'SQL', 'Salesforce']",2025-06-13 05:45:48
Lead Data Engineer,Moreyeahs,7 - 9 years,Not Disclosed,['Indore'],"We are seeking a highly skilled and experienced Lead Data Engineer (7+ years) to join our dynamic team. As a Lead Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure. You will be responsible for ensuring the efficient and reliable collection, storage, and transformation of large-scale data to support business intelligence, analytics, and data-driven decision-making.\n\nKey Responsibilities :\n\nData Architecture & Design :\n- Lead the design and implementation of robust data architectures that support data warehousing (DWH), data integration, and analytics platforms.\n- Develop and maintain ETL (Extract, Transform, Load) pipelines to ensure the efficient processing of large datasets.\n\nETL Development :\n- Design, develop, and optimize ETL processes using tools like Informatica Power Center, Intelligent Data Management Cloud (IDMC), or custom Python scripts.\n- Implement data transformation and cleansing processes to ensure data quality and consistency across the enterprise.\n\nData Warehouse Development :\n- Build and maintain scalable data warehouse solutions using Snowflake, Databricks, Redshift, or similar technologies.\n\n- Ensure efficient storage, retrieval, and processing of structured and semi-structured data.\n\nBig Data & Cloud Technologies :\n- Utilize AWS Glue and PySpark for large-scale data processing and transformation.\n- Implement and manage data pipelines using Apache Airflow for orchestration and scheduling.\n- Leverage cloud platforms (AWS, Azure, GCP) for data storage, processing, and analytics.\n\nData Management & Governance :\n- Establish and enforce data governance and security best practices.\n- Ensure data integrity, accuracy, and availability across all data platforms.\n- Implement monitoring and alerting systems to ensure data pipeline reliability.\n\nCollaboration & Leadership :\n\n- Work closely with data Stewards, analysts, and business stakeholders to understand data requirements and deliver solutions that meet business needs.\n- Mentor and guide junior data engineers, fostering a culture of continuous learning and development within the team.\n- Lead data-related projects from inception to delivery, ensuring alignment with business objectives and timelines.\n\nDatabase Management :\n- Design and manage relational databases (RDBMS) to support transactional and analytical workloads.\n- Optimize SQL queries for performance and scalability across various database platforms.\n\nRequired Skills & Qualifications :\nEducation: Bachelors or Masters degree in Computer Science, Information Systems, Engineering, or a related field.\n\nExperience :\n- Minimum of 7+ years of experience in data engineering, ETL, and data warehouse development.\n- Proven experience with ETL tools like Informatica Power Center or IDMC.\n- Strong proficiency in Python and PySpark for data processing.\n- Experience with cloud-based data platforms such as AWS Glue, Snowflake, Databricks, or Redshift.\n- Hands-on experience with SQL and RDBMS platforms (e.g., Oracle, MySQL, PostgreSQL).\n- Familiarity with data orchestration tools like Apache Airflow.\nTechnical Skills :\n- Advanced knowledge of data warehousing concepts and best practices.\n- Strong understanding of data modeling, schema design, and data governance.\n- Proficiency in designing and implementing scalable ETL pipelines.\n- Experience with cloud infrastructure (AWS, Azure, GCP) for data storage and processing.\n\nSoft Skills :\n- Excellent communication and collaboration skills.\n- Ability to lead and mentor a team of engineers.\n- Strong problem-solving and analytical thinking abilities.\n- Ability to manage multiple projects and prioritize tasks effectively.\n\nPreferred Qualifications :\n- Experience with machine learning workflows and data science tools.\n- Certification in AWS, Snowflake, Databricks, or relevant data engineering technologies.\n- Experience with Agile methodologies and DevOps practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data modeling', 'RDBMS', 'MySQL', 'Agile', 'Informatica', 'Oracle', 'Apache', 'Business intelligence', 'Analytics', 'Python']",2025-06-13 05:45:50
Power BI - Data Analyst,Hakkda,3 - 6 years,Not Disclosed,['Jaipur'],"ABOUT HAKKODA\n\nHakkoda, an IBM Company, is a modern data consultancy that empowers data driven organizations to realize the full value of the Snowflake Data Cloud. We provide consulting and managed services in data architecture, data engineering, analytics and data science. We are renowned for bringing our clients deep expertise, being easy to work with, and being an amazing place to work! We are looking for curious and creative individuals who want to be part of a fast-paced, dynamic environment, where everyone s input and efforts are valued. We hire outstanding individuals and give them the opportunity to thrive in a collaborative atmosphere that values learning, growth, and hard work. Our team is distributed across North America, Latin America, India and Europe. If you have the desire to be a part of an exciting, challenging, and rapidly-growing Snowflake consulting services company, and if you are passionate about making a difference in this world, we would love to talk to you!.\n\nWe are looking for a skilled and motivated Data Analyst / Data Engineer to join our growing data team in Jaipur. The ideal candidate should have hands-on experience with SQL, Python, Power BI , and familiarity with Snowflake is a strong advantage. You will play a key role in building data pipelines, delivering analytical insights, and enabling data-driven decision-making across the organization.\nRole Description:\nDevelop and manage robust data pipelines and workflows for data integration, transformation, and loading.\nDesign, build, and maintain interactive Power BI dashboards and reports based on business needs.\nOptimize existing Power BI reports for performance, usability, and scalability .\nWrite and optimize complex SQL queries for data analysis and reporting.\nUse Python for data manipulation, automation, and advanced analytics where applicable.\nCollaborate with business stakeholders to understand requirements and deliver actionable insights .\nEnsure high data quality, integrity, and governance across all reporting and analytics layers.\nWork closely with data engineers, analysts, and business teams to deliver scalable data solutions .\nLeverage cloud data platforms like Snowflake for data warehousing and analytics (good to have).\nQualifications\n3-6 years of professional experience in data analysis or data engineering.\nBachelor s degree in computer science , Engineering, Data Science, Information Technology , or a related field.\nStrong proficiency in SQL with the ability to write complex queries and perform data modeling.\nHands-on experience with Power BI for data visualization and business intelligence reporting.\nProgramming knowledge in Python for data processing and analysis.\nGood understanding of ETL/ELT , data warehousing concepts, and cloud-based data ecosystems.\nExcellent problem-solving skills , attention to detail, and analytical thinking.\nStrong communication and interpersonal skills to work effectively with cross-functional teams .\nPreferred / Good to Have\nExperience working with large datasets and cloud platforms like Snowflake, Redshift, or BigQuery.\nFamiliarity with workflow orchestration tools (e.g., Airflow) and version control systems (e.g., Git).\nPower BI Certification (e.g., PL-300: Microsoft Power BI Data Analyst).\nExposure to Agile methodologies and end-to-end BI project life cycles.\nBenefits:\n\n- Health Insurance\n- Paid leave\n- Technical training and certifications\n- Robust learning and development opportunities\n- Incentive\n- Toastmasters\n- Food Program\n- Fitness Program\n- Referral Bonus Program\n\nHakkoda is committed to fostering diversity, equity, and inclusion within our teams. A diverse workforce enhances our ability to serve clients and enriches our culture. We encourage candidates of all races, genders, sexual orientations, abilities, and experiences to apply, creating a workplace where everyone can succeed and thrive.\n\nReady to take your career to the next level? Apply today and join a team that s shaping the future!!\n\nHakkoda is an IBM subsidiary which has been acquired by IBM and will be integrated in the IBM organization. Hakkoda will be the hiring entity. By Proceeding with this application, you understand that Hakkoda will share your personal information with other IBM subsidiaries involved in your recruitment process, wherever these are located. More information on how IBM protects your personal information, including the safeguards in case of cross-border data transfer, are available here.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Data modeling', 'Consulting', 'Agile', 'Workflow', 'microsoft', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-13 05:45:52
Lead Data Engineer (Immediate joiner),Decision Point,4 - 9 years,15-30 Lacs P.A.,"['Gurugram', 'Chennai']","Role & responsibilities\nâ€¢ Assume ownership of Data Engineering projects from inception to completion.\nImplement fully operational Unified Data Platform solutions in production environments using technologies like Databricks, Snowflake, Azure Synapse etc.\nShowcase proficiency in Data Modelling and Data Architecture\nUtilize modern data transformation tools such as DBT (Data Build Tool) to streamline and automate data pipelines (nice to have).",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Synapse', 'Python', 'Etl Pipelines', 'Airflow', 'Bigquery', 'Advance Sql', 'Azure Cloud', 'GCP', 'Data Modeling', 'Data Architecture', 'AWS']",2025-06-13 05:45:53
Data Engineer,Konrad Group,3 - 7 years,15-30 Lacs P.A.,['Gurugram( Sector 42 Gurgaon )'],"Who We Are\n\nKonrad is a next generation digital consultancy. We are dedicated to solving complex business problems for our global clients with creative and forward-thinking solutions. Our employees enjoy a culture built on innovation and a commitment to creating best-in-class digital products in use by hundreds of millions of consumers around the world. We hire exceptionally smart, analytical, and hard working people who are lifelong learners.\nAbout The Role\nAs a Data Engineer youll be tasked with designing, building, and maintaining scalable data platforms and pipelines. Your deep knowledge of data platforms such as Azure Fabric, Databricks, and Snowflake will be essential as you collaborate closely with data analysts, scientists, and other engineers to ensure reliable, secure, and efficient data solutions.\n\nWhat Youll Do\n\nDesign, build, and manage robust data pipelines and data architectures.\nImplement solutions leveraging platforms such as Azure Fabric, Databricks, and Snowflake.\nOptimize data workflows, ensuring reliability, scalability, and performance.\nCollaborate with internal stakeholders to understand data needs and deliver tailored solutions.\nEnsure data security and compliance with industry standards and best practices.\nPerform data modelling, data extraction, transformation, and loading (ETL/ELT).\nIdentify and recommend innovative solutions to enhance data quality and analytics capabilities.\n\nQualifications\n\nBachelors degree or higher in Computer Science, Data Engineering, Information Technology, or a related field.\nAt least 3 years of professional experience as a Data Engineer or similar role.\nProficiency in data platforms such as Azure Fabric, Databricks, and Snowflake.\nHands-on experience with data pipeline tools, cloud services, and storage solutions.\nStrong programming skills in SQL, Python, or related languages.\nExperience with big data technologies and concepts (Spark, Hadoop, Kafka).\nExcellent analytical, troubleshooting, and problem-solving skills.\nAbility to effectively communicate technical concepts clearly to non-technical stakeholders.\nAdvanced English\n\nNice to have\n\nCertifications related to Azure Data Engineering, Databricks, or Snowflake.\nFamiliarity with DevOps practices and CI/CD pipelines.\n\nPerks and Benefits\n\nComprehensive Health & Wellness Benefits Package \nSocials, Outings & Retreats\nCulture of Learning & Development\nFlexible Working Hours\nWork from Home Flexibility\nService Recognition Programs\n\nKonrad is committed to maintaining a diverse work environment and is proud to be an equal opportunity employer. All qualified applicants, regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status will receive consideration for employment. If you have any accessibility requirements or concerns regarding the hiring process or employment with us, please notify us so we can provide suitable accommodation.\nWhile we sincerely appreciate all applications, only those candidates selected for an interview will be contacted.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Azure Data Factory', 'Azure Databricks', 'Spark', 'Fabric', 'Python']",2025-06-13 05:45:55
Full Stack Data Scientist,Vimo Getinsured,2 - 7 years,Not Disclosed,['Gurugram( Sector 61 Gurgaon )'],"About the Role\nAs a Data Science Engineer, you will need strong technical skills in data modeling, machine learning, data engineering, and software development. You will have the ability to conduct literature reviews and critically evaluate research papers to identify applicable techniques. Additionally, you should be able to design and implement efficient and scalable data processing pipelines, perform exploratory data analysis, and collaborate with other teams to integrate data science models into production systems. Passion for conversational AI and a desire to solve some of the most complex problems in the Natural Language Processing space are essential. You will work on highly scalable, stable, and automated deployments, aiming for high performance. Taking on the challenge of building and scaling a truly remarkable AI platform to impact the lives of millions of customers will be part of your responsibilities. Working in a challenging yet enjoyable environment, where learning new things is the norm, you should think of solutions beyond boundaries. You should also drive outcomes with full ownership, deeply believe in customer obsession, and thrive in a fast-paced environment of learning and innovation.\nYou will work in a challenging, consumer-facing problem space, where you can make an immediate impact. You will get to work with the latest technologies, learn to use new tools and get the opportunity to have your say in the final product. Youll work alongside a great team in an open, collaborative environment. We are part of Vimo, a well-funded, stable mid-size company with excellent salaries, medical/dental/vision coverage, and perks. Vimo is an Equal Opportunity Employer.",,,,"['python', 'Langchain', 'Neural Networks', 'LLM', 'Linux', 'Data Structures', 'Natural Language Processing', 'Jupyter Notebook', 'Machine Learning', 'Deep Learning', 'Numpy', 'Data Science', 'pandas', 'Nltk', 'Langgraph', 'Transformers', 'BERT', 'langsmith']",2025-06-13 05:45:57
Gcp Architect,InfoCepts,12 - 17 years,Not Disclosed,['Bengaluru'],Key Result Areas:\nArchitect modern DA solutions using best of breed cloud services specifically from GCP aligned to client needs and drive implementation for successful delivery\nDemonstrate expertise for client success through delivery support and thought leadership cloud data architecture with focus on GCP data analytics services.\nContribute to business growth through presales support for GCP based solutions,,,,"['Automation', 'Software development methodologies', 'GCP', 'MIS', 'HIPAA', 'Agile', 'Apache', 'SQL', 'Python', 'Data architecture']",2025-06-13 05:45:58
Technical Architect,SRSInfoway,8 - 13 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Job Title: Enterprise Architect Architecture Office\nExperience: 8-10 Years\nEmployment Type: Full-Time\n\nJob Summary:\nWe are looking for a seasoned Enterprise/Technical Architect to join the Architecture Office and lead end-to-end architecture design and governance for enterprise systems. This role requires a strong grasp of modern architectural frameworks, deep technical expertise, and the ability to align technology with strategic business objectives.\n\nKey Responsibilities:\nDesign, review, and guide the architecture of enterprise-grade applications and systems.\nDevelop and maintain architectural standards, principles, and reference models.\nCollaborate with business, engineering, and infrastructure teams to ensure architectural alignment.\nEvaluate and recommend tools, technologies, and frameworks aligned with enterprise needs.\nLead architectural governance reviews and ensure compliance with best practices.\nCreate high-level solution designs, integration patterns, and data flow models.\nMentor development teams on architecture principles and implementation.\nRequired Skills & Experience:\n810 years of overall IT experience, with 3â€“5 years in architectural roles.\nExpertise in designing scalable, secure, and high-performance applications.\nDeep experience with microservices, cloud platforms (AWS/Azure/GCP), API management, and containerization (Docker/Kubernetes).\nStrong understanding of enterprise integration patterns, data architecture, and DevOps pipelines.\nHands-on knowledge of Java/.NET, architectural modeling tools (e.g., ArchiMate, Sparx EA), and documentation frameworks.\nExperience with architectural frameworks like TOGAF, Zachman, or similar is preferred.",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Containerization', 'cloud platforms', 'microservices']",2025-06-13 05:46:00
Solution Architect,Hitachi Vantara,10 - 20 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Job Title: Senior Technical Consultant / Solution Architect Storage & Content Technologies\n\nJob Summary:\n\nWe are looking for a client-facing Technical Consultant or Solution Architect with strong leadership and problem-solving skills. You will act as a key advisor to both internal and customer technical teams, providing expert guidance on system architecture, implementation techniques, troubleshooting, and solution development. This role requires active engagement in architectural planning, change management, and team coordination, while also contributing to client growth opportunities through technical insight and solution proposals.",,,,"['Storage', 'Infrastructure', 'Presales', 'Solutioning']",2025-06-13 05:46:02
Lead/Solution Architect: Java Full Stack,Omega Healthcare,10 - 15 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role : Tech Lead\nEducational Qualification : ME / BE / MCA / MSc\nExperience Required : 10+ Years\nShifts : Day shift\nResponsibilities:\n10+ years of expertise working in Core Java,J2EE, Design Patterns, Spring, Spring Boot, Micro service architecture\nExpertise in Web services; SOAP and REST\nExperience with at least one UI technology like Angular, React JS, OJET\nGood knowledge in Postgres/ MySQL, Advanced PL SQL with JSON data management\nBuild tools like Ant, Maven and Gradle\nExperience in Azure Repos/SVN/ Git source control tool is must\nSolid understanding of Software development life cycle and OOPS concept\nWorking with APIs/Integrations is must\nExperience working with JDK 17 + is preferred\nExcellent understanding of architectural principles involved in SaaS and multi-tenant platforms\nStrong in development tools like Eclipse with SDS, IntelliJ, Git, Cradle, Sonar, Jenkins, Jira/ADO/ Artifactory\nORM technologies like Hibernate\nExperience with Kubernetes and Dockers is preferred\nExperience of messaging systems and data pipelines such as RabbitMQ and Kafka is preferred\nExperience using other API Management solutions like Apigee\nWell versed with scalability, automation, resiliency, high availability and user experience\nExperience in cloud computing application implementations on AWS is preferred\nStrong background in creating secure cloud architectures for customer facing applications that are enterprise grade and highly scalable is strongly preferred\nExperience in Agile, DevOps culture is a plus\nShould have managed a team of SSE/SE and lead at least 2 projects\nGood communication skills",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'cloud integration', 'solution architecture', 'Java Architecture', 'data architecture', 'Angular', 'application development']",2025-06-13 05:46:03
SAP MDG Architect,Akana Services,10 - 15 years,Not Disclosed,[],"Key Responsibilities:\n*Good understanding of SAP MDG technical framework includes BADI, BAPI/RFC/FM, Workflows, BRF+, Enterprise Services, IDoc, Floorplan Manager, WebDynPro, Fiori, and MDG API framework, Workflows, AMDP, CDS, BRF+, BOPF.\n*Should have knowledge working with REST API, SOAP API, ODATA. SAP AIF Application Interface Framework, Inapp extensibility.\n*Knowledge of SAP data dictionary tables, views, relationships, and corresponding data architecture for ECC and S/4 HANA for various SAP master.\n*Implementation experience of SAP MDG in key domains such as Customer, Supplier, Material, Business Partners.\nRole Requirements and Qualifications:\n*10+ years of SAP experience, with at least 5 years focused on SAP MDG architecture and implementation.\n*Proven experience with multiple end-to-end SAP MDG implementations.\n*Deep understanding of SAP MDG Central Governance, Consolidation, and Mass Processing.\n*Expertise in MDG data modeling, BRF+ rules, workflows, FPM, and UI configuration.\n*Strong integration knowledge using SOA services, IDocs, and MDG Data Replication Framework.\n*Experience with SAP S/4HANA, SAP Fiori, and MDG on S/4HANA.\n*Solid understanding of data governance frameworks and master data management principles.\n*Ability to assess current master data landscapes and propose future-state architecture.\n*Excellent communication, leadership, and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['SAP Fiori', 'Sap Mdg', 'SAP S/4HANA']",2025-06-13 05:46:05
Teamcenter Architect,We are hiring for well knowned MNC,8 - 13 years,20-35 Lacs P.A.,['Bengaluru'],"10+ years of experience in Teamcenter PLM implementations, with at least 3+ years as a Technical Architect.\nLead end-to-end Teamcenter PLM technical architecture and deployment strategy.\nDefine and implement Teamcenter solutions including data modeling, workflow customization, and BMIDE configurations.\nCollaborate with business stakeholders to gather requirements and propose scalable Teamcenter solutions.\nDrive architectural decisions, performance tuning, and capacity planning.\nOversee Teamcenter installations, upgrades, environment setup, clustering, and disaster recovery planning.\nSupport Teamcenter integrations with CAD (NX, SolidWorks, CATIA), ERP, and other enterprise systems.\nConduct code reviews and provide governance for customizations, extensions, and data migration strategies.\nDeep expertise in:\nTeamcenter architecture, ITK, RAC, SOA, and Active Workspace.\nBMIDE, Workflows, Custom Handlers, ACLs, and Stylesheets.\nDeployment on multi-tier architecture and cloud platforms (optional but preferred).",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Temporary/Contractual","['PLM', 'ITK', 'Architect', 'Bmide', 'SOA', 'RAC', 'Awc 1', 'Architecting']",2025-06-13 05:46:07
S&C Global Network - AI - Responsible AI - Sr Analyst,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Entity:- Accenture Strategy & Consulting\n\n\n\nTeam:- Strategy & Consulting Global Network\n\n\n\nPractice:- Responsible AI COE\n\n\n\nTitle:- Responsible AI Specialist/ Sr. Analyst\n\n\n\nJob location:- Bangalore/Gurgaon/Pune/ Hyderabad/Chennai/Mumbai\n\nThe rapid development of AI is creating new opportunities to improve the lives of people around the world, from business to healthcare to education. As a result, it is also raising new questions about the best way to build fairness, interpretability, privacy and security into these systems.\n\nThe Data and AI revolution is changing everything. Its everywhere transforming how we work and play. Join Accenture and help transform leading companies and communities around the world.\n\nAccenture is driving these exciting changes and bringing them to life across 40 industries in more than 120 countries. The sheer scale of our capabilities and client engagements and the way we collaborate with the ecosystem, operate, and deliver value provides an unparalleled opportunity to grow and advance.\n\nAccentures S&C Global Network Data & AI team covers the range of skills, from Strategy, Data Science, Data Architecture, AI Engineering and Visual Insights. When combined with our broader Strategy and Consulting practice, we bring a unique ability to drive end to end business change through the application of Data and AI.\n\nAt the forefront of the industry, youll help make our Responsible AI vision a reality for clients looking to better serve their customers and operate always-on enterprises. Were not just focused on increasing revenues our technologies and innovations are making millions of lives easier and more comfortable. But above all, were doing this responsibly and inclusively to make sure AI technology is used equitably and in a way that is both ethically and technically sound.\n\nJoin us and become an integral part of our global Responsible AI team with the credibility, expertise and insight clients depend on. There will never be a typical day at Accenture, but thats why people love it here. You will be working with famous brands and household names no worrying about how to explain what you do to your family again!\n\nWe are looking for experienced and motivated individuals who will be a part of the\n\n\n\nResponsible AI\n\n\n\nCentre of Excellence (COE) within Accenture to join our multi-disciplinary Responsible AI team. We are committed to help people build AI products/solutions/services in a responsible and trustworthy manner.\n\n\n\nThe ideal candidate should have a strong client-facing consulting background in data science/analytics with an ability to pick up new technologies very quickly. He/she will be passionate about understanding the impact of AI systems on people and society and will have a track record in using tools to undertake assessments such as\n\n\n\nFairness/Bias, Explainability, Model Validation and Robustness, to assess model behaviour through a Responsibility lens.\n\nBeing a part of Accenture will help you grow both professionally and personally as you help shape our thinking and approaches to Responsible AI, working alongside world-class academics, industry leaders and practitioners. Responsible AI is a key strategic priority for Accenture and were looking for the very best in the field to help us meet our ambitious goals in this space.\n\n\n\nResponsibilities:\n\nAs a client-facing in Responsible AI, you will consult with Accentures clients on how todesign & develop reliable, effective user-centered AI systems in adherence to general best practices for software systems, together with practices that address responsibility considerations unique to AI & machine learning. You will also be expected to contribute to research on how AI systems can be designed holistically with fairness, interpretability, privacy, security, safety, and robustness built in by design.\n\nAs part of our team, you will:\nBe a subject matter expert on technical aspects of Responsible AI & Data\nCollaborate with colleagues to develop best practices, frameworks, tools for scalable implementation of Responsible AI in enterprises\nConduct research on Responsible AI policies, principles, issues, risk identification, risk remediation, regulatory requirements, latest trends etc.\nBring a strong conceptual understanding of Responsible AI, principles & tools with experience of using these tools in close collaboration with internal and external stakeholders/clients\nEvaluate and implement technical best practices and tools for fairness, explainability, transparency, accountability, and other relevant aspects of Responsible AI\nDevelop a clear understanding of clients business issues to adopt the best approach to implement the Responsible AI Framework\nEstablish a consistent and collaborative presence by partnering with clients to understand the wider business goals, objectives & competitive constraints\nProvide thought leadership by publishing in public forums/conferences/blogs on Responsible AI products, research or developments\nLeading diverse and well-qualified RAI team.\n\n\nQualification\n\n\n\nSkillset :\n\n\n\n\nEducation:- PhD / Masters / Bachelors degree in Statistics / Economics / Mathematics /Computer Science / Physics or related disciplines from Premier Colleges in India or Abroad. Specialization in Data Science.\n\n\n\nMust Have\n3 10 years of Hands-on Data science experience in solving real life complex business problems\nMinimum 1 years experience in enhancing AI systems to meet Responsible AI principles - Explainability, Fairness, Accountability, etc.\nPassionate about understanding the impact of AI systems on people and society\nHands-on experience of using techniques such as data bias testing (e.g. for under-represented groups, proxy variables, recall bias, skew etc), Explainability (e.g. SHAP values, LIME), sensitivity testing, repeatability and similar to understand model limitations.\nDemonstrated experience in writing reports that summarize analysis / assessments into simple and concise actionable points\nStrong conceptual knowledge and practical experience in the Development, Validation, and Deployment of ML/AL models such as:\nSupervised Learning - regression, classification techniques\nUnsupervised Learning clustering techniques\nRecommender Systems\nReinforcement Learning\nDeep Learning Sequence models (RNN, GRU, LSTM etc.), CNN, GAN etc\nEconometric models\nExploratory Data analysis, Hypothesis testing etc.\nComfortable in ingestion of technical whitepapers, legal policies, government regulations etc. in relation into Responsible AI and work with Academic partners to convert them into practice\nAbility to learn and develop new methods, strategies and frameworks to proactively identify potential loopholes.\nComfortable with ambiguity, believe in first principles and have the skill to transform broad ideas into action plans\nExcellent written and verbal communication skills with ability to clearly communicate ideas and results to both technical and non-technical business audience, such as senior leaders\nGood time management skills to manage day-to-day work progress and ensure timely and high-quality deliverables\nSelf-motivated with ability to work independently across multiple projects and set priorities and Strong analytical bent of mind.\n\n\n\n\nGood to have\nCloud Certifications (Azure / AWS / GCP)\nKnowledge of AWS SageMaker Clarify / Azure Responsible ML and Fairlearn SDK / GCP AI Explanations\nExperience in Chatbot Analytics, Web Crawling\nExperience in MLOps tools like MLflow or Kubeflow\nKnowledge of cybersecurity, vulnerability assessment, risk remediation etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'chatbot', 'network analysis', 'aws', 'web crawling', 'switching', 'eigrp', 'network engineering', 'load balancing', 'networking', 'bgp', 'network data', 'ccnp', 'f5', 'routing', 'vlan', 'mpls', 'cisco', 'network security', 'network administration', 'ospf', 'sdwan', 'firewall', 'cisco routers', 'ccna']",2025-06-13 05:46:09
Lead Architect,Bot Consulting,6 - 11 years,Not Disclosed,['Jaipur'],"We are looking for people experienced with data architecture, design, and development of database mapping and migration processes. This person will have direct experience optimizing new and current databases, data pipelines and implementing advanced capabilities while ensuring data integrity and security.\nIdeal candidates will have strong communication skills and the ability to guide clients and project team members, acting as a key point of contact for direction and expertise.\nRoles & Responsibilities:\nDesign, develop, and optimize database architectures and data pipelines.\nEnsure data integrity and security across all databases and data pipelines.\nLead and guide clients and project team members, acting as a key point of contact for direction and expertise.\nCollaborate with cross-functional teams to understand business requirements and translate them into technical solutions.\nManage and support large-scale technology programs, ensuring they meet business objectives and compliance requirements.\nDevelop and implement migration, dev/ops, and ETL/ELT ingestion pipelines using tools such as DataStage, Informatica, and Matillion.\nUtilize project management skills to work effectively within Scrum and Agile Development methods.\nCreate and leverage metrics to develop actionable and measurable insights, influencing business decisions.\nQualifications & Skills\n7+ years of proven work experience in data warehousing, business intelligence (BI), and analytics.\n3+ years of experience as a Data Architect.\n3+ years of experience working on Cloud platforms (AWS, Azure, GCP).\nBachelors Degree (BA/BS) in Computer Science, Information Systems, Mathematics, MIS, or a related field.\nStrong understanding of migration processes, dev/ops, and ETL/ELT ingestion pipelines.\nProficient in tools such as DataStage, Informatica, and Matillion.\nExcellent project management skills and experience with Scrum and Agile Development methods.\nAbility to develop actionable and measurable insights and create metrics to influence business decisions.\nPrevious consulting experience managing and supporting large-scale technology programs.\nNice to Have\n6 to 12 months of experience working with Snowflake.\nUnderstanding of Snowflake design patterns and migration architectures.\nKnowledge of Snowflake roles, user security, and capabilities like Snowpipe.\nProficiency in SQL scripting.\nCloud experience on AWS (Azure and GCP are also beneficial).\nPython scripting skills.\nSigns You May Be a Great Fit\nImpact: Play a pivotal role in shaping a rapidly growing venture studio.\nCulture: Thrive in a collaborative, innovative environment that values creativity and ownership.\nGrowth: Access professional development opportunities and mentorship.\nBenefits: Competitive salary, health/we'llness packages, and flexible work options.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['IT services', 'Computer science', 'MIS', 'Datastage', 'Consulting', 'Scrum', 'Informatica', 'Business intelligence', 'Analytics', 'Python']",2025-06-13 05:46:10
Principal Architect,Softbiz Solutions,12 - 16 years,Not Disclosed,['Hyderabad'],"Job Title: Principal Architect\nJob Location: Hyd/Mohali/Kochi\nMode of Work: Hybrid\n\nAbout the Role\nWere looking for a seasoned Principal Architect who thrives at the intersection of client innovation and organizational growth. Youll design transformational enterprise solutions for our clients while simultaneously driving Softobizs technical evolution, ensuring that every client engagement strengthens our capabilities and every internal innovation enhances our client value.\nThis role demands someone who can seamlessly move between architecting solutions for enterprise clients and shaping the technology roadmap that keeps Softobiz at the industry forefront. Your expertise will directly influence both our clients competitive advantage and our own market position.\nWhat Youll Do\nSolution Architecture & Innovation Design and deliver enterprise-grade solutions that solve complex business challenges while continuously identifying opportunities to enhance Softobizs service offerings and technical capabilities. Lead architectural discussions with client executives and internal leadership teams, ensuring that every solution we build advances both client success and our organizational learning.\nTechnical Strategy & Roadmap Execution Develop and execute comprehensive technology strategies that serve dual purposes - solving immediate client needs while building reusable capabilities that strengthen our competitive position. Lead the implementation of Softobizs technical roadmap by driving adoption of new technologies, establishing development standards, and ensuring successful delivery of strategic initiatives. Evaluate emerging technologies through the lens of both client value and internal advancement, then champion their adoption across the organization.\nClient Engagement & Internal Leadership Serve as the primary technical advisor for enterprise clients while mentoring internal teams on advanced architectural patterns and emerging technologies. Your client-facing experience will directly inform our internal training programs, standards development, and capability building initiatives.\nThought Leadership & Market Intelligence Build Softobizs reputation as a technology innovator by translating insights from client engagements into thought leadership content, internal best practices, and new service offerings. Your deep understanding of enterprise challenges will guide both our marketing strategy and our research and development priorities.\nRequired Expertise\nEnterprise Architecture Experience 12+ years designing large-scale enterprise solutions with demonstrated success in client-facing roles. You should have a track record of leading complex transformations that deliver measurable business value while building lasting client relationships that drive organizational growth.\nCloud & Infrastructure Mastery Deep expertise across major cloud platforms with experience designing hybrid and multi-cloud strategies that balance client requirements with platform optimization. Your cloud architecture decisions should reflect both immediate solution needs and long-term scalability considerations that benefit future engagements.\nDistributed Systems & Integration Extensive experience with microservices architectures, distributed systems design, and enterprise integration patterns. Youll need to solve complex integration challenges for clients while establishing reusable patterns and frameworks that accelerate future project delivery.\nData & AI Integration Proven ability to architect modern data platforms and integrate AI/ML capabilities into enterprise applications. Your experience should span both traditional data warehouse modernization and cutting-edge AI implementations, with an understanding of how these capabilities can be packaged into repeatable service offerings.\nTechnology Leadership & Execution Experience leading technical teams and driving successful implementation of architectural standards in both client and internal contexts. You should be comfortable presenting to C-level executives while also leading change management initiatives, establishing development processes, and ensuring successful adoption of new technologies across teams within our organization.\nThe Impact Youll Create\nYour architectural decisions will directly influence enterprise client outcomes while simultaneously building Softobizs reputation as the premier technology partner for enterprise transformation. Every solution you design becomes a case study that enhances our market position, and every internal process you improve strengthens our ability to deliver exceptional client value.\nYoull work with cutting-edge technologies on high-stakes projects while helping shape the future direction of both client enterprises and Softobiz itself. Your expertise will drive revenue growth through exceptional client delivery and operational excellence through continuous organizational improvement.\nWhat Were Looking For\nStrategic Thinking - Ability to see beyond immediate project requirements to identify opportunities for long-term value creation, both for clients and for Softobiz.\nTechnical Excellence - Deep expertise across modern technology stacks with a proven ability to make architectural decisions that scale and evolve with changing business needs.\nClient Focus - Experience building trust with enterprise stakeholders and translating complex technical concepts into clear business value propositions.\nInnovation Mindset - Passion for emerging technologies coupled with the judgment to evaluate their practical application in enterprise contexts.\nExecution Excellence - Proven track record of not just designing strategies but successfully implementing them, with experience leading cross-functional teams through complex technology transformations and organizational change initiatives.\nQualifications\nBachelors degree in Computer Science or related field; advanced degree preferred\n12+ years of enterprise architecture experience with at least 5 years in client-facing roles\nDeep expertise in cloud platforms, distributed systems, microservices, and enterprise integration\nExperience with data architecture, AI/ML integration, and modern development practices\nStrong communication skills with experience presenting to executive audiences\nIndustry certifications in relevant technology platforms\nWhy Join Softobiz\nYoull have the opportunity to work with industry-leading clients while helping build the technology organization that will define the next generation of enterprise solutions. Your expertise will directly impact both immediate client success and long-term organizational growth, creating a unique career experience that combines the excitement of cutting-edge client work with the satisfaction of building something lasting.\nWe offer competitive compensation, comprehensive benefits, flexible work arrangements, and significant opportunities for professional growth and industry recognition.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Change management', 'Architecture', 'Enterprise integration', 'Enterprise applications', 'Enterprise architecture', 'Project delivery', 'Distribution system', 'Data architecture']",2025-06-13 05:46:12
Software Development Manager,Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"As part of the AWS Solutions organization, we have a vision to provide business applications, leveraging Amazon s unique experience and expertise, that are used by millions of companies worldwide to manage day-to-day operations. We will accomplish this by accelerating our customers businesses through delivery of intuitive and differentiated technology solutions that solve enduring business challenges. We blend vision with curiosity and Amazon s real-world experience to build opinionated, turnkey solutions. Where customers prefer to buy over build, we become their trusted partner with solutions that are no-brainers to buy and easy to use.\n\nJust Walk Out builds technology enables a new kind of stores with no lines and no checkout you just grab and go! Customers simply use the Amazon Go app to enter the store, take what they want from our selection of fresh, delicious meals and grocery essentials, and go!\n\nOur checkout-free shopping experience is made possible by our Just Walk Out Technology, which automatically detects when products are taken from or returned to the shelves and keeps track of them in a virtual cart. When you re done shopping, you can just leave the store. Shortly after, we ll charge your Amazon account and send you a receipt. Our Just Walk Out Technology uses a variety of technologies including vision, sensor fusion, and advanced machine learning. Innovation is part of our DNA! Our goal is to be Earths most customer centric company and we are just getting started. We need people who want to join an ambitious program that continues to push the state of the art in vision, machine learning, distributed systems and hardware.\n\nAs a Software Development Manager, you will own delivery of cross functional initiatives to build new features and improve Just Walk Out experience. You will manage a team of 6-10 software development engineers to build and enhance the shopping experience in Just Walk out stores.\n\n\nWe are looking for an individual that thrives by creating high-bar engineering culture. You ll be chartered with building software that deliver critical capabilities to our AWS Just Walk out business enabling bar raising customer experience, embed operational knowledge and encode best practices into everything we do.\nWe have a team culture that encourages innovation and we expect team members and management alike to take high degree of ownership for their program vision and execution of ideas. Beyond a strong engineering, data storage/modeling, visualization, and front-end background, a successful candidate will have experience successfully leading teams, developing people, and building a scalable infrastructure. We are looking for someone that is a self-starter, someone that approaches complex business questions with data and curiosity, and a person that dives below the surface to identify the root cause and ""so what"" rather than just superficial trends\n\nAbout the team\nDiverse Experiences\nAmazon values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn t followed a traditional path, or includes alternative experiences, don t let it stop you from applying.\n\nWhy AWS\nAmazon Web Services (AWS) is the world s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating that s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses.\nWork/Life Balance\nWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there s nothing we can t achieve.\nInclusive Team Culture\nAWS values curiosity and connection. Our employee-led and company-sponsored affinity groups promote inclusion and empower our people to take pride in what makes us unique. Our inclusion events foster stronger, more collaborative teams. Our continual innovation is fueled by the bold ideas, fresh perspectives, and passionate voices our teams bring to everything we do.\n\nMentorship and Career Growth\nWe re continuously raising our performance bar as we strive to become Earth s Best Employer. That s why you ll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional.\n\nThe Retail Platform Technology team in AWS Just Walk out, builds the platform and architecture that offers retail capabilities to the customers to run their Just walk out stores. This spans across various domains such as Order Management, in-store pricing intelligence, shopper receipts and refunds, and the merchant experience capabilities that allow merchants to seamlessly integrate with Amazon to power their stores with Just Walk out technology. 2+ years of engineering team management experience\n7+ years of engineering experience\n8+ years of leading the definition and development of multi tier web services experience\nKnowledge of engineering practices and patterns for the full software/hardware/networks development life cycle, including coding standards, code reviews, source control management, build processes, testing, certification, and livesite operations\nExperience partnering with product or program management teams Experience in communicating with users, other technical teams, and senior leadership to collect requirements, describe software product features, technical designs, and product strategy\nExperience in recruiting, hiring, mentoring/coaching and managing teams of Software Engineers to improve their skills, and make them more effective, product software engineers\nExperience designing or architecting (design patterns, reliability and scaling) of new and existing systems",,,,"['Order management', 'Cloud computing', 'Team management', 'Front end', 'Coding', 'Machine learning', 'Technology solutions', 'Business applications', 'Distribution system', 'Operations']",2025-06-13 05:46:14
Solution Architect,Kanerika Software,10 - 20 years,Not Disclosed,"['Indore', 'Hyderabad', 'Ahmedabad']","Job Summary:\n\nAs a Solution Architect, you will collaborate with our sales, presales and COE teams to provide technical expertise and support throughout the new business acquisition process. You will play a crucial role in understanding customer requirements, presenting our solutions, and demonstrating the value of our products.\nYou thrive in high-pressure environments, maintaining a positive outlook and understanding that career growth is a journey that requires making strategic choices. You possess good communication skills, both written and verbal, enabling you to convey complex technical concepts clearly and effectively. You are a team player, customer-focused, self-motivated, responsible individual who can work under pressure with a positive attitude. You must have experience in managing and handling RFPs/ RFIs, client demos and presentations, and converting opportunities into winning bids. You possess a strong work ethic, positive attitude, and enthusiasm to embrace new challenges. You can multi-task and prioritize (good time management skills), willing to display and learn. You should be able to work independently with less or no supervision. You should be process-oriented, have a methodical approach and demonstrate a quality-first approach.",,,,"['Azure Data Factory', 'Microsoft Fabric', 'Data Analytics', 'Azure Cloud']",2025-06-13 05:46:16
App Automation Eng Senior Analyst,Accenture,5 - 8 years,Not Disclosed,['Kolkata'],"Skill required: Tech for Operations - Microsoft Azure Cloud Services\n\n\n\n\nDesignation: App Automation Eng Senior Analyst\n\n\n\n\nQualifications:Any Graduation/12th/PUC/HSC\n\n\n\n\nYears of Experience:5 to 8 years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the world s largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com.In our Service Supply Chain offering, we leverage a combination of proprietary technology and client systems to develop, execute, and deliver BPaaS (business process as a service) or Managed Service solutions across the service lifecycle:Plan, Deliver, and Recover. In this role, you will partner with business development and act as a Business Subject Matter Expert (SME) to help build resilient solutions that will enhance our clients supply chains and customer experience.The Senior Azure Data factory (ADF) Support Engineer Il will be a critical member of our Enterprise Applications Team, responsible for designing, supporting & maintaining robust data solutions. The ideal candidate is proficient in ADF, SQL and has extensive experience in troubleshooting Azure Data factory environments, conducting code reviews, and bug fixing. This role requires a strategic thinker who can collaborate with cross-functional teams to drive our data strategy and ensure the optimal performance of our data systems.\n\n\n\n\nWhat are we looking for\nBachelor s or Master s degree in Computer Science, Information Technology, or a related field. Proven experience (5+ years) as a Azure Data Factory Support Engineer Il Expertise in ADF with a deep understanding of its data-related libraries. Strong experience in Azure cloud services, including troubleshooting and optimizing cloud-based environments. Proficient in SQL and experience with SQL database design. Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy. Experience with ADF pipelines. Excellent problem-solving and troubleshooting skills. Experience in code review and debugging in a collaborative project setting. Excellent verbal and written communication skills. Ability to work in a fast-paced, team-oriented environment. Strong understanding of the business and a passion for the mission of Service Supply Chain Hands on with Jira, Devops ticketing, ServiceNow is good to have\n\n\n\nRoles and Responsibilities: Innovate. Collaborate. Build. Create. Solve ADF & associated systems Ensure systems meet business requirements and industry practices. Integrate new data management technologies and software engineering tools into existing structures. Recommend ways to improve data reliability, efficiency, and quality. Use large data sets to address business issues. Use data to discover tasks that can be automated. Fix bugs to ensure robust and sustainable codebase. Collaborate closely with the relevant teams to diagnose and resolve issues in data processing systems, ensuring minimal downtime and optimal performance. Analyze and comprehend existing ADF data pipelines, systems, and processes to identify and troubleshoot issues effectively. Develop, test, and implement code changes to fix bugs and improve the efficiency and reliability of data pipelines. Review and validate change requests from stakeholders, ensuring they align with system capabilities and business objectives. Implement robust monitoring solutions to proactively detect and address issues in ADF data pipelines and related infrastructure. Coordinate with data architects and other team members to ensure that changes are in line with the overall architecture and data strategy. Document all changes, bug fixes, and updates meticulously, maintaining clear and comprehensive records for future reference and compliance. Provide technical guidance and support to other team members, promoting a culture of continuous learning and improvement. Stay updated with the latest technologies and practices in ADF to continuously improve the support and maintenance of data systems. Flexible Work Hours to include US Time Zones Flexible working hours however this position may require you to work a rotational On-Call schedule, evenings, weekends, and holiday shifts when need arises Participate in the Demand Management and Change Management processes. Work in partnership with internal business, external 3rd party technical teams and functional teams as a technology partner in communicating and coordinating delivery of technology services from Technology For Operations (TfO)\n\nQualification\n\nAny Graduation,12th/PUC/HSC",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure cloud services', 'supply chain', 'sql', 'oracle adf', 'troubleshooting', 'microsoft azure services', 'css', 'demand management', 'software testing', 'microsoft azure', 'azure cloud', 'change management', 'javascript', 'jquery', 'servicenow', 'ticketing', 'devops', 'debugging', '.net', 'html', 'jira']",2025-06-13 05:46:18
Senior Software Engineer - Adobe Experience Platform ( AEP ),Wells Fargo,4 - 7 years,Not Disclosed,['Hyderabad'],"About this role:\nWells Fargo is seeking a senior Software Engineer - Adobe Experience Platform (AEP)\nIn this role, you will:\nLead moderately complex initiatives and deliverables within technical domain environments\nContribute to large scale planning of strategies\nDesign, code, test, debug, and document for projects and programs associated with technology domain, including upgrades and deployments",,,,"['Software Engineering', 'Data Science', 'data model', 'data analysis', 'data modeling', 'configuration', 'APIs', 'AEP', 'CDP']",2025-06-13 05:46:20
Senior Principal NetSuite Technical Consultant,Oracle,6 - 10 years,Not Disclosed,['Hyderabad'],"An experienced consulting professional who has a broad understanding of solutions, industry best practices, multiple business processes or technology designs within a product/technology family. Operates independently to provide quality work products to an engagement. Performs varied and complex duties and tasks that need independent judgment, in order to implement Oracle products and technology to meet customer needs. Applies Oracle methodology, company procedures, and leading practices.\nTypical Workload:\nEach day can be very different. It is a great job for people looking to step out of normal routines. We have a broad range of responsibilities. The typical workload breaks down into the rough percentages:\n30% scripting, QA, creating and executing test plans.\n30% is customer facing, walking though use cases, reviewing test plans, and sometimes just brought in to solve problems.\n15% attend internal meetings including knowledge transfers, mentoring sessions for less experienced resources, and other strategic initiatives\n10% integration consulting with customer and third-party systems\n10% executive updates, documentation, and overall project management.\n5% data migration\nResponsibilities include:\nTrack and report project progress to appropriate parties using NetSuite and Jira\nAssist in defining custom scripts on NetSuite s SuiteCloud platform\nCollaborate with other NetSuite consultants to validate business/technical requirements through interview and analysis\nProduce system design documents and participate in technical walkthroughs\nLead technical work streams, design scripts, or validate scripts, coordinate with other developers, Quality Assurance (QA) and deployment activities\nConduct Code Reviews\nConduct user acceptance testing for complex solutions\nAssist in development, QA, and deployment processes as necessary to meet project requirements\nAbility to work in a global team environment\nMentor less experienced consultants\nPreferred Qualifications/Skills include:\n6+ years of NetSuite or other ERP / CRM Solutions. NetSuite highly preferred.\nA degree in mathematics, computer science or engineering\nNetSuite SuiteCloud Development/Design/Testing/Code Review experience, including 3rd party Integration\nExperience leading technical work streams including other developers and including global delivery teams.\nExposure to system architecture, object-oriented design, web frameworks and patterns, experience strongly preferred\nAbility to author detailed documents capturing workflow processes, use cases, exception handling, and test cases\nConsulting role experience\nSoftware development lifecycle (SDLC) methodology knowledge and use\nSoftware development (JavaScript preferred)\nProficiency in error resolution, error handling and debugging.\nExperience with IDEs (WebStorm preferred), source control systems (GIT preferred), unit-testing tools and defect management tools\nExperience with XML/XSL and Web Services (SOAP, WSDL, REST, JSON)\nExperience developing web applications using JSP/Servlets, DHTML, and JavaScript\nExperience with Jira\nStrong interpersonal and communication skills\nTravel:\nModest to moderate, as appropriate\nAt Oracle, we don t just value differences we celebrate them. We re committed to creating a workplace where all kinds of people work together. We believe innovation starts with diversity and inclusion.\nDetailed Description and Job Requirements\nA consulting position operating independently with some assistance and guidance to provide quality work products to a project team or customer that comply with Oracle methodologies and practices. Performs standard duties and tasks with some variation to implement Oracle products and technology to meet customer specifications.\n\nStandard assignments are accomplished without assistance by exercising independent judgment, within defined policies and processes, to deliver functional and technical solutions on moderately complex customer engagements.\n\n6-10+ years of overall experience in relevant functional or technical roles. Undergraduate degree or equivalent experience. Product or technical expertise relevant to practice focus. Ability to communicate effectively and build rapport with team members and clients. Ability to travel as needed.\n\nOperates independently to provide quality work products to an engagement. Performs varied and complex duties and tasks that need independent judgment, in order to implement Oracle products and technology to meet customer needs. Applies Oracle methodology, company procedures, and leading practices. Demonstrates expertise to deliver functional and technical solutions on moderately complex customer engagements. May lead the solution design and implementation aspects of engagement(s) ensuring high quality, integrated software solutions within constraints of time and budget. May act as the team lead on projects, providing coaching, guidance and feedback to develop skills of team members. Effectively consults with management of customer organizations. Participates in business development activities. Develops and leads detailed solutions for moderately complex projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ERP', 'Manager Quality Assurance', 'XML', 'Project management', 'Javascript', 'HTML', 'Oracle', 'SDLC', 'CRM', 'DHTML']",2025-06-13 05:46:22
IN Senior Associate ETL Testing GDC- OC Application Technology,PwC Service Delivery Center,3 - 5 years,Not Disclosed,['Kolkata'],"Not Applicable\nSpecialism\nMicrosoft\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in software and product innovation focus on developing cuttingedge software solutions and driving product innovation to meet the evolving needs of clients. These individuals combine technical experience with creative thinking to deliver innovative software products and solutions.\n\nIn testing and quality assurance at PwC, you will focus on the process of evaluating a system or software application to identify any defects, errors, or gaps in its functionality. Working in this area, you will execute various test cases and scenarios to validate that the system meets the specified requirements and performs as expected.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\ns\nExperience in testing data validation scenarios and Data ingestion, pipelines, and transformation processes (e.g. ETL).\nAn understanding of Data warehousing concepts and data engineering tools and how they can be used strategically.\nStrong SQL skills to profile, compare, and validate data transformation for complex data migration processes.\nHandson experience of creating test artifacts like Test Plan, Test cases, Test summary report, etc.\nDemonstrated experience of working with test case management tools like ALM, ADO, JIRA, etc.\nThorough knowledge of STLC and experience of handson Defect Management lifecycle.\nUnderstanding of information governance principles and how they could apply in a testing capacity.\nExperience in report testing & API testing is desired, but not mandatory.\nTendency to proactively learn new technology and ability to work independently in highvisibility, highperformance projects with low supervision.\nManaging and analyzing existing processes to identify improvement opportunities or scope of automation.\nBuilding and leveraging client relationships as well as highlevel verbal and written communication skills.\nMandatory skill sets\nETL Testing\nPreferred skill sets\nData Warehousing concepts\nYears of experience required\n4+\nEducation qualification\nB.Tech/B.E./MCA\nEducation\nDegrees/Field of Study required Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nETL (Extract Transform Load) Testing\nData Warehousing (DW)\nNo",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data migration', 'Data validation', 'Manager Quality Assurance', 'ETL testing', 'Test planning', 'Test cases', 'microsoft', 'Data warehousing', 'SQL']",2025-06-13 05:46:24
"Loan IQ, Lending / Senior Consultant Specialist",Hsbc,3 - 7 years,Not Disclosed,['Pune'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of a Senior consultant specialist\nIn this role, you will:\nResponsible for providing expertise as a business / system analyst in commercial lending financing, accounting front-office trading transformation as well as play a key role in the regular change management process and also support the test management and planning activities.\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nOperational Knowledge of Loan IQ\nKnowledge of Complex Lending businesses ideally with exposure to project/export finance structures\nElicit business requirements from business users (including accounting, operations, risk, treasury) and subject matter experts\nAbility to gather, model and document clear business and functional requirements / specifications\nVery good working knowledge of Loan IQ configuration\nDefine and implement accounting mapping for commercial lending activities\nWrite functional specifications and work with development on implementing required enhancement/changes.\nCreation of test plans and test scripts\nAbility to present and discuss with the business and IT users any impact resulting from the project\nManage scope and requirements throughout the project lifecycle\nProvide overview and training for end-users\nConfiguring the application to satisfy business requirements\nHas prior experience with Data Migration or Integration projects\nStrong analytical skills and the ability to merge multiple existing workflows into one, standard flow\nProven experience in delivering quality specifications that are well understood by both the business and development/implementation team\nProven experience in an IT Development environment with in-depth specialisation in Loan IQ and the ability to make it work for complex loans\nThe successful candidate will also meet the following requirements:\nMust have good written and verbal communication skills and experience of communicating complex ideas to management and key stakeholders as well as to project team members\nStrong interpersonal skills with the ability to deal with difficult and challenging situations. Ability to communicate with and manage both IT professionals and business units.\nTeam player with collaborate attitude, able to maintain high level of discipline within the given SDLC process\nAbility to manage multiple priorities, commitments, and projects.\nExperience working in collaboration with teams from different areas of organization\nEfficient time management skills to handle challenging workload.\nProactive on taking leadership when needed, self-motivated, dynamic and result oriented.\nYou ll achieve more when you join HSBC.\n.",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Loans', 'Front office', 'Change management', 'Data migration', 'Test scripts', 'Test management', 'Operations', 'Financial services', 'SDLC', 'commercial lending']",2025-06-13 05:46:26
Sr. Principal Software Engineer,Morningstar,12 - 18 years,Not Disclosed,"['Thane', 'Navi Mumbai', 'Mumbai (All Areas)']","Position Title: Senior Principal Software Engineer\nThe Area: Morningstar Data for equities provides comprehensive coverage of global stock markets from as early as 1975. We are continuously broadening our coverage and creating new products to help clients prepare for regulatory changes and other industry shifts. Our data features proprietary statistics and is developed using stringent quality screens to ensure accuracy. From APIs to data feeds, our solutions are delivered quickly to help institutions meet a broad range of functions. The Role: In this role, you will collaborate with technology manager, scrum Master, functional experts, and developers to build technology solutions for Morningstar`s Equity applications. The team is looking for forward-thinking problem solvers who thrive in a fastpaced environment. The Lead is responsible in mentoring the team members, provide guidance and opportunities for the team members to expand their capabilities and skills. They will have to coordinate and work with the members in a global team. We are looking for an individual that can apply discipline, create solid software products.\n\nResponsibilities:\nâ€¢ Design & develop web and enterprise solutions to be flexible, scalable & extensible.\nâ€¢ Improve complex data flow, data structures and db design to move to next platform.\nâ€¢ Enforce good agile practices like test driven development, Continuous Integration.\nâ€¢ Hands-on development will be an integral part of the responsibilities.\nâ€¢ Develop areas of continuous and automated deployment.\nâ€¢ Introduce and follow good development practices, innovative frameworks and technology solutions that help business move faster.\nâ€¢ Follow best practices like estimation, planning, reporting and improvement brought to processes in every day work.\nâ€¢ Analyses and reviews system requirements. Uses requirement and other design documents to gain overall understanding of the functionality of the new or enhanced application.\nâ€¢ Participate actively in the design, architecture and build phases, to aim at producing high quality deliverables.\nâ€¢ Provide recommendations on product and development environment improvements.\n\nRequirements:\nThese are the most important skills, qualities, etc. that weâ€™d like for this role.\nâ€¢ Minimum 12 Years of experience\nâ€¢ Bachelor of Science in Computer Science, Engineering, or equivalent.\nâ€¢ At least 2+ years as a software Lead/Architect\nâ€¢ Demonstrated familiarity with AI-powered assistants (e.g., GitHub Copilot, ChatGPT) for code generation, debugging, and/or other technical tasks.\nâ€¢ Hands-on in Java 8, Adv Java, Spring Framework\nâ€¢ Strong knowledge and hands-on on micro-services based architecture.\nâ€¢ Very Strong knowledge of databases and hands on MS SQL/MySQL/PostgreSQL or NoSQL DB DynamoDB/MongoDB.\nâ€¢ Experience with building REST based APIs.\nâ€¢ Experience in analysis, design, coding and implementation of large-scale, n-tier Java based platforms.\nâ€¢ Knowledge of any JavaScript framework like Vue, Angular JS (version >2)/ NodeJS etc.\nâ€¢ Be aware of activity in the open source world. Contributing back to open source is a big plus.\nâ€¢ Distributed computing, with experience in cloud computing (Amazon Web Services platform and associated technologies)\nâ€¢ Experience on agile practices\nâ€¢ Experience with modern development practices in areas of Product design, Requirement Analysis, Test Driven Development, Automation & Unit Testing, in a product development environment.\nâ€¢ Excellent listening, written and verbal communication skills. Good to Have: â€¢ Machine Learning knowledge.\nâ€¢ Exposure to Capital Market domain preferred (Indexes, Equity etc.) Morningstar is an equal opportunity employer",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java Fullstack', 'Java', 'Vue.Js', 'Java Spring Boot', 'Spring Boot', 'React.Js', 'AWS', 'Angular']",2025-06-13 05:46:28
S&C Global Network - AI - CDP - Analyst,Accenture,8 - 10 years,Not Disclosed,['Gurugram'],"Job Title -\n\n\n\nS&C Global Network - AI - RTCDP - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nBengaluru\n\n\n\nMust-have skills:Web Analytics Tools\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\nTechnical\n\n\n\n\nSkills:\n\nAny CDP platforms experience e.g., Lytics CDP platform developer, or/and\nSegment CDP platform developer, or/and\nAdobe Experience Platform (Real time CDP) developer, or/and\nCustom CDP developer on any cloud\nGA4/GA360, or/and Adobe Analytics\nGoogle Tag Manager, and/or Adobe Launch, and/or any Tag Manager Tool\nGoogle Ads, DV360, Campaign Manager, Facebook Ads Manager, The Trading desk etc.\nDeep Cloud experiecne (GCP, AWS, Azure)\nAdvance level Python, SQL, Shell Scripting experience\nData Migration, DevOps, MLOps, Terraform Script programmer\n\n\n\nSoft\n\n\n\n\nSkills:\n\nStrong problem solving skills\nGood team player\nAttention to details\nGood communication skills\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\nWHATS IN IT FOR YOU\n\nAs part of our Analytics practice, you will join a worldwide network of over 20k+ smart and driven colleagues experienced in leading AI/ML/Statistical tools, methods and applications. From data to analytics and insights to actions, our forward-thinking consultants provide analytically-informed, issue-based insights at scale to help our clients improve outcomes and achieve high performance.\n\n\n\nWhat you would do in this role\n\nA Consultant/Manager for Customer Data Platforms serves as the day-to-day marketing technology point of contact and helps our clients get value out of their investment into a Customer Data Platform (CDP) by developing a strategic roadmap focused on personalized activation. You will be working with a multidisciplinary team of Solution Architects, Data Engineers, Data Scientists, and Digital Marketers.\n\n\n\nKey Duties and Responsibilities:\nBe a platform expert in one or more leading CDP solutions. Developer level expertise on Lytics, Segment, Adobe Experience Platform, Amperity, Tealium, Treasure Data etc. Including custom build CDPs\nDeep developer level expertise for real time even tracking for web analytics e.g., Google Tag Manager, Adobe Launch etc.\nProvide deep domain expertise in our clients business and broad knowledge of digital marketing together with a Marketing Strategist industry\nDeep expert level knowledge of GA360/GA4, Adobe Analytics, Google Ads, DV360, Campaign Manager, Facebook Ads Manager, The Trading desk etc.\nAssess and audit the current state of a clients marketing technology stack (MarTech) including data infrastructure, ad platforms and data security policies together with a solutions architect.\nConduct stakeholder interviews and gather business requirements\nTranslate business requirements into BRDs, CDP customer analytics use cases, structure technical solution\nPrioritize CDP use cases together with the client.\nCreate a strategic CDP roadmap focused on data driven marketing activation.\nWork with the Solution Architect to strategize, architect, and document a scalable CDP implementation, tailored to the clients needs.\nProvide hands-on support and platform training for our clients.\nData processing, data engineer and data schema/models expertise for CDPs to work on data models, unification logic etc.\nWork with Business Analysts, Data Architects, Technical Architects, DBAs to achieve project objectives - delivery dates, quality objectives etc.\nBusiness intelligence expertise for insights, actionable recommendations.\nProject management expertise for sprint planning\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:\n\n\n\n8 to 10 Years\n\n\n\n\nEducational Qualification:\n\n\n\nB.Com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['digital marketing', 'display video', 'google ads', 'campaign management', 'facebook ads manager', 'trading', 'python', 'adobe analytics', 'adobe', 'microsoft azure', 'sql', 'gcp', 'web analytics', 'devops', 'segmentation', 'web analytics tools', 'shell scripting', 'network analysis', 'aws', 'seo']",2025-06-13 05:46:30
"Engineer, Staff/Manager",Qualcomm,11 - 16 years,Not Disclosed,['Bengaluru'],"Synechron is seeking a knowledgeable and proactive Data Modeler to guide the design and development of data structures that support our clients' business objectives. In this role, you will collaborate with cross-functional teams to translate business requirements into scalable and efficient data models, ensuring data accuracy, consistency, and integrity. You will contribute to creating sustainable and compliant data architectures that leverage emerging technologies such as cloud, IoT, mobile, and blockchain. Your work will be instrumental in enabling data-driven decision-making and operational excellence across projects.Software Required\n\nSkills:\nStrong understanding of data modeling concepts, methodologies, and tools Experience with data modeling for diverse technology platforms including cloud, mobile, IoT, and blockchain Familiarity with database management systems (e.g., relational, NoSQL) Knowledge of SDLC and Agile development practices Proficiency in modeling tools such as ERwin, PowerDesigner, or similar Preferred Skills:\nExperience with data integration tools and ETL processes Knowledge of data governance and compliance standards Familiarity with cloud platforms (AWS, Azure, GCP) and how they impact data architectureOverall Responsibilities Collaborate with business analysts, data engineers, and stakeholders to understand data requirements and translate them into robust data models Design logical and physical data models optimized for performance, scalability, and maintainability Develop and maintain documentation for data structures, including data dictionaries and metadata Conduct reviews of data models and code to ensure adherence to quality standards and best practices Assist in designing data security and privacy measures in alignment with organizational policies Stay informed about emerging data modeling trends and incorporate best practices into project delivery Support data migration, integration, and transformation activities as needed Provide technical guidance and mentorship related to data modeling standardsTechnical Skills (By Category) Data Modeling & Data Management: EssentialLogical/physical data modeling, ER diagrams, data dictionaries PreferredDimensional modeling, data warehousing, master data management Programming Languages: PreferredSQL (expertise in writing complex queries) OptionalPython, R for data analysis and scripting Databases & Data Storage Technologies: EssentialRelational databases (e.g., Oracle, SQL Server, MySQL) PreferredNoSQL (e.g., MongoDB, Cassandra), cloud-native data stores Cloud Technologies: PreferredBasic understanding of cloud data solutions (AWS, Azure, GCP) Frameworks & Libraries: Not typically required, but familiarity with data integration frameworks is advantageous Development Tools & Methodologies: EssentialData modeling tools (ERwin, PowerDesigner), version control (Git), Agile/Scrum workflows Security & Compliance: Knowledge of data security best practices, regulatory standards like GDPR, HIPAAExperience Minimum of 8+ years of direct experience in data modeling, data architecture, or related roles Proven experience designing data models for complex systems across multiple platforms (cloud, mobile, IoT, blockchain) Experience working in Agile environments using tools like JIRA, Confluence, Git Preference for candidates with experience supporting data governance and data quality initiativesNoteEquivalent demonstrated experience in relevant projects or certifications can qualify candidates.Day-to-Day Activities Participate in daily stand-ups and project planning sessions Collaborate with cross-functional teams to understand and analyze business requirements Create, review, and refine data models and associated documentation Develop data schemas, dictionaries, and standards to ensure consistency Support data migration, integration, and performance tuning activities Conduct peer reviews and provide feedback on data models and solutions Keep current with the latest industry developments in data architecture and modeling Troubleshoot and resolve data-related technical issuesQualifications Bachelors or Masters degree in Computer Science, Data Science, Information Technology, or related fields Demonstrated experience with data modeling tools and techniques in diverse technological environments Certifications related to data modeling, data management, or cloud platforms (preferred)Professional Competencies Strong analytical and critical thinking skills to develop optimal data solutions Effective communication skills for translating technical concepts to non-technical stakeholders Ability to work independently and in collaborative team environments Skilled problem solver able to handle complex data challenges Adaptability to rapidly evolving technologies and project requirements Excellent time management and prioritization skills to deliver quality outputs consistently",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data modeling', 'modeling tools', 'relational databases', 'scrum', 'agile', 'confluence', 'hipaa', 'data warehousing', 'data architecture', 'erwin', 'sql', 'git', 'gcp', 'mysql', 'etl', 'mongodb', 'jira', 'python', 'oracle', 'microsoft azure', 'sql server', 'nosql', 'gdpr', 'cassandra', 'aws', 'data integration', 'sdlc']",2025-06-13 05:46:32
Application Developer-Oracle Cloud Middleware,IBM,4 - 8 years,Not Disclosed,['Bengaluru'],"As a Software Developer you'll participate in many aspects of the software development lifecycle, such as design, code implementation, testing, and support. You will create software that enables your clients' hybrid-cloud and AI journeys. Your primary responsibilities includeComprehensive Feature Development and Issue ResolutionWorking on the end to end feature development and solving challenges faced in the implementation. Stakeholder Collaboration and Issue ResolutionCollaborate with key stakeholders, internal and external, to understand the problems, issues with the product and features and solve the issues as per SLAs defined. Continuous Learning and Technology IntegrationBeing eager to learn new technologies and implementing the same in feature development.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nYouâ€™ll have access to all the technical and management training courses you need to become the expert you want to be.\nYouâ€™ll learn directly from expert developers in the field; our team leads love to mentor\nYou have the opportunity to work in many different areas to figure out what really excites you.\nShould have minimum 3 or more years of relevant experience in ODI(Oracle Database Integrator) 12c Development and Implementation.\nShould have hands on experience in complex data migration between heterogeneous large complex databases (Oracle database is must)\n\n\nPreferred technical and professional experience\nYouâ€™ll have access to all the technical and management training courses you need to become the expert you want to be.\nShould have minimum 3 or more years of relevant experience in ODI (Oracle Database Integrator) 12c Development and Implementation.\nShould have good knowledge of integrating with Web Services, XML(Extensible Markup Language) and other API(Application Programming Interface) to transfer the data - from source and target, in addition to database",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['integrator', '12c', 'oracle', 'odi', 'oracle database', 'web services', 'data migration', 'jasper reports', 'sql', 'plsql', 'xml', 'html', 'wms', 'middleware', 'python', 'c', 'software testing', 'jda', 'flash animation', 'red prairie', 'warehouse management system', 'moca', 'groovy', 'hybrid cloud', 'aws', 'unix']",2025-06-13 05:46:34
Package Specialist-SAP HANA Basis,IBM,7 - 10 years,Not Disclosed,['Bengaluru'],"SLT System Implementation and ConfigurationLead the implementation and configuration of the SAP SLT system, including system setup, installation, and configuration of data replication processes. Define and maintain system connections and communication channels with source systems.\nData ReplicationDesign and configure data replication processes using SLT technology to ensure real-time and accurate data replication from various source systems, such as SAP ERP, non-SAP databases, or external systems, to the SAP S/4HANA environment.\nData Mapping and TransformationCollaborate with stakeholders to understand data requirements and perform data mapping and transformation activities. Define rules and logic for data transformation to align source system data with the target SAP S/4HANA structure and requirements.\nData Quality and IntegrityMonitor and ensure the quality and integrity of replicated data. Perform data validation and error handling to identify and resolve data replication issues, data inconsistencies, or data conflicts.\nSystem Monitoring and Performance OptimizationMonitor the SLT system and data replication processes for performance, throughput, and system availability. Identify bottlenecks or areas for improvement and implement enhancements to optimize data replication performance\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nQualificationsBachelorâ€™s degree in engineering, Computer Science, Information Systems, or a related field. A masterâ€™s degree is a plus.\nAbout 7-10 years of experience as a SAP SLT Consultant, working with SAP S/4HANA or SAP ERP systems.\nStrong knowledge and hands-on experience with SAP SLT technology, including system setup, configuration, and monitoring.\nProficient in SAP SLT replication setup, including data modeling, transformation, and data mapping.\nExperience with data replication from various source systems, such as SAP ERP, non-SAP databases, or external systems. Knowledge of data integration techniques, data validation, error handling, and data quality management\n\n\nPreferred technical and professional experience\nFamiliarity with SAP S/4HANA data structures, data models, and data migration processes. Project management experience, with the ability to manage tasks, prioritize activities, and deliver high-quality results within deadlines.\nStrong analytical and problem-solving skills, with the ability to troubleshoot data replication issues and implement effective solutions. Excellent communication and interpersonal skills, with the ability to collaborate with cross-functional teams and stakeholders.\nProject management experience, with the ability to manage tasks, prioritize activities, and deliver high-quality results within deadlines. SAP SLT certification or relevant SAP certifications are highly desirable",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['project management', 'sap', 'sap s hana', 'sap tm', 'slt', 'sap upgrade', 'data validation', 'sap erp', 'data quality management', 'sql', 'netweaver', 'tableau', 'data modeling', 'sap basis', 'system monitoring', 'data structures', 'sap hana', 'sap basis administration', 'data integration']",2025-06-13 05:46:36
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Workday Data Mapping & Conversions\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :Mandatory to have Workday Related certification15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for overseeing the application development process and ensuring successful implementation.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the team in developing innovative solutions- Ensure timely delivery of projects- Mentor junior team members\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Workday Data Mapping & Conversions- Strong understanding of data mapping and conversion processes- Experience in configuring and customizing applications- Knowledge of integration tools and techniques- Hands-on experience with data migration strategies\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Workday Data Mapping & Conversions- This position is based at our Bengaluru office- A mandatory Workday Related certification is required\n\nQualification\n\nMandatory to have Workday Related certification15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['workday', 'data mapping', 'data migration', 'application development', 'sql', 'c#', 'oracle', 'data analysis', 'rice components', 'oracle apps technical', 'data warehousing', 'business analysis', 'sql server', 'plsql', 'data modeling', 'xml publisher reports', 'etl', 'unix', 'oracle apps']",2025-06-13 05:46:38
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP Global Trade Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :Graduate\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your day will involve overseeing project progress, coordinating with teams, and ensuring successful application development.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead application design and development- Coordinate with stakeholders to gather requirements- Ensure timely delivery of projects\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Global Trade Services- Strong understanding of SAP modules integration- Experience in SAP implementation projects- Knowledge of SAP data migration tools- Hands-on experience in SAP system configuration\nAdditional Information:- The candidate should have a minimum of 5 years of experience in SAP Global Trade Services- This position is based at our Bengaluru office- A Graduate degree is required\n\nQualification\n\nGraduate",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'application design', 'sap module', 'application development', 'sap global trade services', 'c#', 'customs handling', 'sap logistics execution', 'seeburger', 'gts', 'sap sd', 'sql server', 'spl', 'sql', 'sap ewm', 'sap le', 'sap scm', 'wm', 'supply chain management', 'sap warehouse management']",2025-06-13 05:46:40
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Palantir Foundry\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\nProject Role :Lead Data Engineer\n\nProject Role Description :Design, build and enhance applications to meet business process and requirements in Palantir foundry.Work experience :Minimum 6 years\nMust have Skills :Palantir Foundry , PySpark, TypeScript (for customizing Workshop Forms & UI)Good to Have Skills :Experience in Pyspark, python and SQLKnowledge on Big Data tools & TechnologiesOrganizational and project management experience.Job & Key Responsibilities :Responsible for designing , developing, testing, and supporting data pipelines and applications on Palantir foundry.Configure and customize Workshop to design and implement workflows and ontologies.Configure and customize Workshop applications, including designing Forms, Workflows, and Ontology-based interactions.Write TypeScript to create dynamic and interactive Forms in Workshop for user-driven data entry and validation.Collaborate with data engineers and stakeholders to ensure successful deployment and operation of Palantir foundry applications.Work with stakeholders including the product owner, data, and design teams to assist with data-related technical issues and understand the requirements and design the data pipeline.Work independently, troubleshoot issues and optimize performance.Communicate design processes, ideas, and solutions clearly and effectively to team and client. Assist junior team members in improving efficiency and productivity.\nTechnical Experience :Proficiency in PySpark, Python and Sql with demonstrable ability to write & optimize SQL and spark jobs.Hands on experience on Palantir foundry related services like Data Connection, Code repository, Contour , Data lineage & Health checks.Good to have working experience with workshop , ontology , slate.Hands-on experience in data engineering and building data pipelines (Code/No Code) for ELT/ETL data migration, data refinement and data quality checks on Palantir Foundry.Experience in TypeScript to create and customize Forms in Workshop, including form validation, user interactions, and data binding with Ontology.Experience in ingesting data from different external source systems using data connections and sync.Good Knowledge on Spark Architecture and hands on experience on performance tuning & code optimization.Proficient in managing both structured and unstructured data, with expertise in handling various file formats such as CSV, JSON, Parquet, and ORC.Experience in developing and managing scalable architecture & managing large data sets.Good understanding of data loading mechanism and adeptly implement strategies for capturing CDC.Nice to have test driven development and CI/CD workflows.Experience in version control software such as Git and working with major hosting services (e. g. Azure DevOps, GitHub, Bitbucket, Gitlab).Implementing code best practices involves adhering to guidelines that enhance code readability, maintainability, and overall quality.\nEducational Qualification:15 years of full-term education\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'sql', 'spark', 'typescript', 'python', 'hive', 'continuous integration', 'palantir', 'csv', 'data migration', 'bitbucket', 'azure devops', 'parquet', 'git', 'java', 'devops', 'json', 'hadoop', 'big data', 'orc', 'github', 'software testing', 'performance tuning', 'data quality', 'gitlab', 'sqoop', 'sql knowledge']",2025-06-13 05:46:42
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Bhubaneswar'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Microsoft Azure Data Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your day will involve overseeing the application development process and ensuring seamless communication within the team and stakeholders.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Expected to provide solutions to problems that apply across multiple teams- Lead the application development process- Ensure effective communication within the team and stakeholders- Provide guidance and mentorship to team members\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Azure Data Services- Strong understanding of cloud computing principles- Experience in designing and implementing scalable applications- Knowledge of DevOps practices and tools- Hands-on experience with Azure data storage solutions\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Microsoft Azure Data Services- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data services', 'microsoft azure', 'application development', 'devops', 'cloud computing', 'sap bi', 'oracle', 'sap bods', 'data warehousing', 'data migration', 'sap bo', 'sql server', 'sql', 'plsql', 'bods', 'sap bw', 'sap lumira', 'business objects', 'sap data services', 'sap hana', 'etl', 'sap abap', 'unix']",2025-06-13 05:46:44
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Microsoft Azure Data Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your day will involve overseeing the application development process and ensuring seamless communication within the team and stakeholders.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Expected to provide solutions to problems that apply across multiple teams- Lead the application development process effectively- Ensure timely delivery of high-quality applications- Provide guidance and mentorship to team members\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Azure Data Services- Strong understanding of cloud computing principles- Experience in designing and implementing scalable applications- Knowledge of DevOps practices and tools- Hands-on experience with Azure services such as Azure SQL Database and Azure Functions\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Microsoft Azure Data Services- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data services', 'microsoft azure', 'sql azure', 'devops', 'cloud computing', 'sap bi', 'oracle', 'sap bods', 'data warehousing', 'data migration', 'sap bo', 'sql server', 'sql', 'application development', 'plsql', 'bods', 'sap bw', 'sap lumira', 'business objects', 'sap data services', 'sap hana', 'etl']",2025-06-13 05:46:46
Tech Delivery Subject Matter Expert,Accenture,12 - 15 years,Not Disclosed,['Bengaluru'],"Project Role :Tech Delivery Subject Matter Expert\n\n\n\n\n\nProject Role Description :Drive innovative practices into delivery, bring depth of expertise to a delivery engagement. Sought out as experts, enhance Accentures marketplace reputation. Bring emerging ideas to life by shaping Accenture and client strategy. Use deep technical expertise, business acumen and fluid communication skills, work directly with a client in a trusted advisor relationship to gather requirements to analyze, design and/or implement technology best practice business changes.\n\n\n\nMust have skills :SAP Flexible Real Estate Management (RE-FX)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Tech Delivery Subject Matter Expert, you will drive innovative practices into delivery and bring depth of expertise to various engagements. Your typical day will involve collaborating with clients to understand their needs, analyzing requirements, and implementing technology best practices. You will be sought out for your expertise, enhancing the reputation of the organization in the marketplace. By shaping strategies and bringing emerging ideas to life, you will work closely with clients in a trusted advisor capacity, ensuring that their business changes are effectively designed and executed. We are seeking an experienced SAP REFX Consultant with a strong background in lease accounting (IFRS16) and SAP S/4HANA implementation.Responsibilities- Configure and integrate the REFX module with other SAP modules to ensure compliance with IFRS16 requirements.- Lead or support the transition from ECC to SAP S/4HANA, focusing on REFX-related data and structures.- Collaborate with cross-functional teams to gather requirements, analyze gaps, and provide solutions.- Develop functional documentation, configuration documents, and training materials.- Provide ongoing support, troubleshooting, and enhancements in the REFX module.:- 6+ years of experience in the SAP REFX module.- Strong knowledge and hands-on configuration of SAP REFX lease accounting (IFRS16).- Proficiency in data migration strategies and tools for REFX.- Experience in integration with SAP FI and CO modules.- Solid understanding of real estate master data, contract management, and valuation.- SAP Certification in S/4HANA Finance is an added advantage.- Excellent communication and documentation skills.\nQualifications- Bachelor's/Master's degree in Finance, Accounting, Information Technology, or related field.\nAdditional Information:- The candidate should have minimum 12 years of experience in SAP Flexible Real Estate Management (RE-FX).- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['master data', 'real estate', 'documentation', 'data migration', 'valuation', 'contract management', 'sap', 'information technology', 'sap fi', 'sap flexible real estate management', 'accounting', 'general ledger', 'sap s hana', 'lease accounting', 'sap refx', 'troubleshooting', 'finance']",2025-06-13 05:46:48
S&CGN - Tech Strategy & Advisory - SAP FICO - Manager,Accenture,8 - 10 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nS&CGN - Tech Strategy & Advisory - SAP S/4 MDG - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nBengaluru, BDC7A\n\n\n\nMust-have skills:Data Architecture\n\n\n\n\nGood to have skills:Knowledge of emerging technologies, cloud computing, and cybersecurity best practices.\n\n\n\nJob\n\n\nSummary:\nStrong functional understanding and hands-on experience of SAP MDM / MDG backed up with implementation projects and aligned with SAP MM, SD, FICO, PP processes\nResponsible for process design, configuration, assist with testing, gather requirements and ultimately setup a full functional development for MDG-S, MDG-C and MDG-MM, test, and production environment to deliver MDG objects and integration solutions\nAbility to work on customized SAP environment and integrated non-SAP interfaces.\nAbility to understand customer demands, challenge requirements in terms of business value and effort / complexity & translate them into solutions.\nAdept in developing, delivering and supporting analytic solutions based on business requirements.\nHaving understanding of Analytical data modelling, knowledge on data models - attribute, analytical and calculation will be appreciated.\nGood Knowledge of SAP Services Business Processes knowledge and understanding of DataWarehouseCloud experience\n\nExcellent problem-solving skills & deep understanding of processes from business & functional perspective.\nExcellent Verbal & Written Communication Skills and Proficiency in MS Office applications\nAbility to work with clients & teams from multiple geographies\n\n\n\n\n\nRoles & Responsibilities:\n\nDevelop and execute technology transformation strategies, oversee implementation projects, and optimize digital capabilities for business efficiency.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:\n\n\n\n8-10Years\n\n\n\n\nEducational Qualification:\n\n\n\nAny Degree",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['sap', 'data architecture', 'mdg', 'sap mdm', 'sap mm', 'fico', 'webdynpro', 'cyber security', 'software testing', 'adobe forms', 'webdynpro abap', 'sap s hana', 'sd', 'odata', 'idocs', 'sap fiori', 'sap fico', 'alv reports', 'smartforms', 'cloud computing', 'sap abap', 'sap hana', 'sap workflow', 'oo abap']",2025-06-13 05:46:50
Software Development Engineer,Accenture,15 - 20 years,Not Disclosed,['Coimbatore'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :SAP Data Migration\n\n\n\n\nGood to have skills :SAP HANA CloudMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Software Development Engineer, you will analyze, design, code, and test multiple components of application code across one or more clients. You will also perform maintenance, enhancements, and/or development work.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead and mentor junior team members.- Conduct code reviews and ensure code quality.- Stay updated with the latest technologies and trends.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Data Migration.- Good To Have\n\n\n\n\nSkills:\nExperience with SAP HANA Cloud.- Strong understanding of data migration processes.- Experience in analyzing, designing, and testing software components.- Knowledge of SAP systems and architecture.- Ability to troubleshoot and resolve technical issues efficiently.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in SAP Data Migration.- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data migration', 'sap hana', 'sap data migration', 'sap', 'software development', 'lsmw', 'sap sd', 'data warehousing', 'sql', 'sap s hana', 'idocs', 'java', 'bapi', 'sap data services', 'sap mm', 'code review', 'etl', 'idoc', 'cransoft', 'sap bods', 'sql server', 'data quality', 'bods', 'sap abap', 'data integration']",2025-06-13 05:46:52
S&C Global Network - AI - Responsible AI - Specialist,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Entity:- Accenture Strategy & Consulting\n\n\n\nTeam:- Strategy & Consulting Global Network\n\n\n\nPractice:- Responsible AI COE\n\n\n\nTitle:- Responsible AI Specialist/ Sr. Analyst\n\n\n\nJob location:- Bangalore/Gurgaon/Pune/ Hyderabad/Chennai/Mumbai\n\nThe rapid development of AI is creating new opportunities to improve the lives of people around the world, from business to healthcare to education. As a result, it is also raising new questions about the best way to build fairness, interpretability, privacy and security into these systems.\n\nThe Data and AI revolution is changing everything. Its everywhere transforming how we work and play. Join Accenture and help transform leading companies and communities around the world.\n\nAccenture is driving these exciting changes and bringing them to life across 40 industries in more than 120 countries. The sheer scale of our capabilities and client engagements and the way we collaborate with the ecosystem, operate, and deliver value provides an unparalleled opportunity to grow and advance.\n\nAccentures S&C Global Network Data & AI team covers the range of skills, from Strategy, Data Science, Data Architecture, AI Engineering and Visual Insights. When combined with our broader Strategy and Consulting practice, we bring a unique ability to drive end to end business change through the application of Data and AI.\n\nAt the forefront of the industry, youll help make our Responsible AI vision a reality for clients looking to better serve their customers and operate always-on enterprises. Were not just focused on increasing revenues our technologies and innovations are making millions of lives easier and more comfortable. But above all, were doing this responsibly and inclusively to make sure AI technology is used equitably and in a way that is both ethically and technically sound.\n\nJoin us and become an integral part of our global Responsible AI team with the credibility, expertise and insight clients depend on. There will never be a typical day at Accenture, but thats why people love it here. You will be working with famous brands and household names no worrying about how to explain what you do to your family again!\n\nWe are looking for experienced and motivated individuals who will be a part of the\n\n\n\nResponsible AI\n\n\n\nCentre of Excellence (COE) within Accenture to join our multi-disciplinary Responsible AI team. We are committed to help people build AI products/solutions/services in a responsible and trustworthy manner.\n\n\n\nThe ideal candidate should have a strong client-facing consulting background in data science/analytics with an ability to pick up new technologies very quickly. He/she will be passionate about understanding the impact of AI systems on people and society and will have a track record in using tools to undertake assessments such as\n\n\n\nFairness/Bias, Explainability, Model Validation and Robustness, to assess model behaviour through a Responsibility lens.\n\nBeing a part of Accenture will help you grow both professionally and personally as you help shape our thinking and approaches to Responsible AI, working alongside world-class academics, industry leaders and practitioners. Responsible AI is a key strategic priority for Accenture and were looking for the very best in the field to help us meet our ambitious goals in this space.Qualification\n\n\n\nResponsibilities:\n\nAs a client-facing in Responsible AI, you will consult with Accentures clients on how todesign & develop reliable, effective user-centered AI systems in adherence to general best practices for software systems, together with practices that address responsibility considerations unique to AI & machine learning. You will also be expected to contribute to research on how AI systems can be designed holistically with fairness, interpretability, privacy, security, safety, and robustness built in by design.\n\nAs part of our team, you will:\nBe a subject matter expert on technical aspects of Responsible AI & Data\nCollaborate with colleagues to develop best practices, frameworks, tools for scalable implementation of Responsible AI in enterprises\nConduct research on Responsible AI policies, principles, issues, risk identification, risk remediation, regulatory requirements, latest trends etc.\nBring a strong conceptual understanding of Responsible AI, principles & tools with experience of using these tools in close collaboration with internal and external stakeholders/clients\nEvaluate and implement technical best practices and tools for fairness, explainability, transparency, accountability, and other relevant aspects of Responsible AI\nDevelop a clear understanding of clients business issues to adopt the best approach to implement the Responsible AI Framework\nEstablish a consistent and collaborative presence by partnering with clients to understand the wider business goals, objectives & competitive constraints\nProvide thought leadership by publishing in public forums/conferences/blogs on Responsible AI products, research or developments\nLeading diverse and well-qualified RAI team.\n\n\n\n\nSkillset :\n\n\n\n\nEducation:- PhD / Masters / Bachelors degree in Statistics / Economics / Mathematics /Computer Science / Physics or related disciplines from Premier Colleges in India or Abroad. Specialization in Data Science.\n\n\n\nMust Have\n3 10 years of Hands-on Data science experience in solving real life complex business problems\nMinimum 1 years experience in enhancing AI systems to meet Responsible AI principles - Explainability, Fairness, Accountability, etc.\nPassionate about understanding the impact of AI systems on people and society\nHands-on experience of using techniques such as data bias testing (e.g. for under-represented groups, proxy variables, recall bias, skew etc), Explainability (e.g. SHAP values, LIME), sensitivity testing, repeatability and similar to understand model limitations.\nDemonstrated experience in writing reports that summarize analysis / assessments into simple and concise actionable points\nStrong conceptual knowledge and practical experience in the Development, Validation, and Deployment of ML/AL models such as:\nSupervised Learning - regression, classification techniques\nUnsupervised Learning clustering techniques\nRecommender Systems\nReinforcement Learning\nDeep Learning Sequence models (RNN, GRU, LSTM etc.), CNN, GAN etc\nEconometric models\nExploratory Data analysis, Hypothesis testing etc.\nComfortable in ingestion of technical whitepapers, legal policies, government regulations etc. in relation into Responsible AI and work with Academic partners to convert them into practice\nAbility to learn and develop new methods, strategies and frameworks to proactively identify potential loopholes.\nComfortable with ambiguity, believe in first principles and have the skill to transform broad ideas into action plans\nExcellent written and verbal communication skills with ability to clearly communicate ideas and results to both technical and non-technical business audience, such as senior leaders\nGood time management skills to manage day-to-day work progress and ensure timely and high-quality deliverables\nSelf-motivated with ability to work independently across multiple projects and set priorities and Strong analytical bent of mind.\n\n\n\n\nGood to have\nCloud Certifications (Azure / AWS / GCP)\nKnowledge of AWS SageMaker Clarify / Azure Responsible ML and Fairlearn SDK / GCP AI Explanations\nExperience in Chatbot Analytics, Web Crawling\nExperience in MLOps tools like MLflow or Kubeflow\nKnowledge of cybersecurity, vulnerability assessment, risk remediation etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['network data', 'data science', 'chatbot', 'aws', 'web crawling', 'switching', 'eigrp', 'load balancing', 'networking', 'bgp', 'ccnp', 'f5', 'routing', 'vlan', 'mpls', 'network security', 'ccnp routing', 'network administration', 'ospf', 'stp', 'sdwan', 'firewall', 'cisco routers', 'hsrp', 'ccna']",2025-06-13 05:46:54
Software Development Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :SAP FI CO Finance\n\n\n\n\nGood to have skills :NA\n\n\n\n\nEducational Qualification :15 years of full time education\n\n\nSummary:As a Software Development Engineer, you will engage in a variety of tasks that involve analyzing, designing, coding, and testing multiple components of application code across various clients. Your typical day will include collaborating with team members to perform maintenance and enhancements, as well as developing new features to improve application functionality and user experience. You will also be responsible for troubleshooting issues and ensuring that the application meets the required standards and specifications, contributing to the overall success of the projects you are involved in.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge.- Continuously evaluate and improve development processes to increase efficiency.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP FI CO Finance.- Strong understanding of financial accounting principles and practices.- Experience with integration of SAP modules and data migration.- Familiarity with SAP reporting tools and financial analysis.- Ability to troubleshoot and resolve issues within SAP environments.\nAdditional Information:- The candidate should have minimum 5 years of experience in SAP FI CO Finance.- This position is based at our Bengaluru office.- A 15 years of full time education is required.\n\nQualification\n\n15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['financial analysis', 'sap', 'accounting', 'sap fico', 'financial accounting', 'python', 'software development', 'oracle', 'data warehousing', 'power bi', 'data migration', 'sql server', 'sql', 'plsql', 'tableau', 'java', 'etl tool', 'troubleshooting', 'etl', 'reporting tools', 'informatica', 'unix']",2025-06-13 05:46:56
Software Development Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :SAP FI CO Finance\n\n\n\n\nGood to have skills :NA\n\n\n\n\nEducational Qualification :15 years of full time education\n\n\nSummary:As a Software Development Engineer, you will engage in a dynamic work environment where you will analyze, design, code, and test various components of application code across multiple clients. Your day will involve collaborating with team members to ensure the successful implementation of enhancements and maintenance tasks, while also focusing on the development of new features to meet client needs. You will be responsible for troubleshooting issues and providing solutions, ensuring that the application functions optimally and meets the required standards of quality and performance. Your role will also include documenting your work and participating in team discussions to share insights and best practices, contributing to a culture of continuous improvement and innovation.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to gather requirements and translate them into technical specifications.- Conduct thorough testing and debugging of application components to ensure high-quality deliverables.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP FI CO Finance.- Strong understanding of financial processes and accounting principles.- Experience with integration of SAP modules and data migration.- Familiarity with SAP reporting tools and financial analysis.- Ability to troubleshoot and resolve issues related to SAP applications.\nAdditional Information:- The candidate should have minimum 3 years of experience in SAP FI CO Finance.- This position is based at our Bengaluru office.- A 15 years of full time education is required.\n\nQualification\n\n15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['financial analysis', 'sap', 'software development', 'accounting', 'sap fico', 'c#', 'python', 'c++', 'oracle', 'data warehousing', 'data migration', 'javascript', 'sql server', 'sql', 'plsql', 'java', 'etl tool', 'debugging', 'troubleshooting', 'technical specifications', 'html', 'mysql', 'etl', 'reporting tools']",2025-06-13 05:46:58
Software Development Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :Spring Boot, Amazon Web Services (AWS)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Software Development Engineer, you will analyze, design, code, and test multiple components of application code across one or more clients. You will perform maintenance, enhancements, and/or development work. Your typical day will involve analyzing requirements, designing software solutions, writing code, and conducting testing to ensure the quality of the application. You will collaborate with cross-functional teams, contribute to key decisions, and provide solutions to problems that apply across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Expected to provide solutions to problems that apply across multiple teams- Conduct code reviews and provide feedback to team members- Identify and resolve technical issues- Participate in architectural discussions and propose improvements\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Spring Boot- Strong understanding of software development principles and best practices- Experience with Java programming language- Knowledge of RESTful APIs and microservices architecture- Familiarity with database systems such as MySQL or PostgreSQL\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Spring Boot- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['microservices', 'java', 'software development', 'spring boot', 'aws', 'css', 'c++', 'web services', 'hibernate', 'jquery', 'sql', 'spring', 'git', 'postgresql', 'oops', 'j2ee', 'json', 'mysql', 'code review', 'html', 'software engineering', 'data structures', 'c#', 'rest', 'python', 'c', 'javascript', 'sql server', '.net']",2025-06-13 05:47:00
Application Developer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP Flexible Real Estate Management (RE-FX)\n\n\n\n\nGood to have skills :SAP CO Management AccountingMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. Your typical day will involve collaborating with teams to develop solutions that align with business needs and requirements.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the team in implementing innovative solutions- Conduct regular team meetings to ensure alignment and progress- Mentor junior team members to enhance their skills\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Flexible Real Estate Management (RE-FX)- Good To Have\n\n\n\n\nSkills:\nExperience with SAP CO Management Accounting- Strong understanding of SAP modules and integration- Experience in customizing and configuring SAP RE-FX modules- Knowledge of SAP data migration and integration with other systems\nAdditional Information:- The candidate should have a minimum of 5 years of experience in SAP Flexible Real Estate Management (RE-FX)- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'sap module', 'sap flexible real estate management', 'application development', 'sap co management', 'rest', 'css', 'python', 'c', 'web services', 'progress 4gl', 'javascript', 'sql server', 'performance appraisal', 'sql', 'plsql', 'employee orientation', 'java', 'recruitment', 'html']",2025-06-13 05:47:02
Application Developer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP Data Migration\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. A typical day involves collaborating with various teams to understand their needs, developing solutions that align with business objectives, and ensuring that applications are optimized for performance and usability. You will also engage in problem-solving activities, providing support and guidance to your team members while continuously seeking opportunities for improvement and innovation in application development.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and ensure timely delivery of application development milestones.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Data Migration.- Strong understanding of data migration strategies and methodologies.- Experience with data mapping and transformation processes.- Familiarity with SAP modules and their integration points.- Ability to troubleshoot and resolve data migration issues effectively.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in SAP Data Migration.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap data migration', 'sap', 'data migration', 'data mapping', 'application development', 'lsmw', 'oracle', 'sap bods', 'data warehousing', 'sql server', 'sql', 'plsql', 'sap s hana', 'data quality', 'bods', 'bapi', 'sap data services', 'sap mm', 'sap hana', 'etl', 'sap abap', 'data integration']",2025-06-13 05:47:04
Application Developer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP Sales and Distribution (SD)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. You will collaborate with teams to ensure successful project delivery and implementation.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead and mentor junior professionals- Conduct regular knowledge sharing sessions within the team- Stay updated on the latest industry trends and technologies\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Sales and Distribution (SD)- Strong understanding of SAP modules integration- Experience in SAP implementation projects- Knowledge of SAP data migration tools- Hands-on experience in SAP customization and configuration\nAdditional Information:- The candidate should have a minimum of 5 years of experience in SAP Sales and Distribution (SD)- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'sap module', 'distribution', 'sap presales', 'application development', 'web services', 'sap sd', 'business development', 'presales', 'triggers', 'sap sales and distribution', 'javascript', 'apex', 'sql', 'salesforce', 'sap s hana', 'sd', 'java', 'project delivery', 'html', 'sap hana', 'sap abap']",2025-06-13 05:47:06
Application Developer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP Sales and Distribution (SD)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. Your typical day will involve collaborating with teams to develop solutions that align with business needs and requirements.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead the design and development of SAP Sales and Distribution (SD) applications.- Implement best practices for application configuration and customization.- Provide technical guidance and support to team members.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Sales and Distribution (SD).- Strong understanding of SAP SD modules and functionalities.- Experience in SAP SD configuration and customization.- Knowledge of integration with other SAP modules.- Experience in SAP SD data migration and data management.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in SAP Sales and Distribution (SD).- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap sd', 'distribution', 'sap presales', 'sd module', 'sap', 'data management', 'web services', 'logistics', 'business development', 'data migration', 'apex', 'sql', 'salesforce', 'sap s hana', 'sd', 'java', 'sap mm', 'html', 'sap hana', 'erp', 'presales', 'javascript', 'application development', 'mm module', 'sap abap']",2025-06-13 05:47:08
Application Developer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :ASP.NET MVC\n\n\n\n\nGood to have skills :Amazon Web Services (AWS)Minimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will engage in the design, construction, and configuration of applications tailored to fulfill specific business processes and application requirements. Your typical day will involve collaborating with team members to understand project needs, developing innovative solutions, and ensuring that applications are optimized for performance and usability. You will also participate in testing and debugging processes to deliver high-quality applications that meet user expectations and business goals.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist in the documentation of application specifications and user guides.- Engage in continuous learning to stay updated with the latest technologies and best practices.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in ASP.NET MVC.- Good To Have\n\n\n\n\nSkills:\nExperience with Amazon Web Services (AWS).- Strong understanding of web application development principles.- Experience with front-end technologies such as HTML, CSS, and JavaScript.- Familiarity with database management systems and SQL.\nAdditional Information:- The candidate should have minimum 3 years of experience in ASP.NET MVC.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['html', 'net mvc', 'javascript', 'asp', 'web application development', 'css', 'web services', 'jquery', 'sql', 'database management', 'java', 'asp.net', 'debugging', 'web api', 'mvc', 'c#', 'entity framework', 'sql server', 'application development', 'asp.net core mvc', 'angular', 'linq', '.net', 'aws', 'angularjs']",2025-06-13 05:47:10
Software Development Lead,Accenture,12 - 15 years,Not Disclosed,['Bengaluru'],"Project Role :Software Development Lead\n\n\n\n\n\nProject Role Description :Develop and configure software systems either end-to-end or for a specific stage of product lifecycle. Apply knowledge of technologies, applications, methodologies, processes and tools to support a client, project or entity.\n\n\n\nMust have skills :CRM Architecture\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Software Development Lead, you will be responsible for developing and configuring software systems, either end-to-end or for specific stages of the product lifecycle. Your typical day will involve collaborating with various teams to ensure that the software meets client requirements, applying your knowledge of technologies and methodologies to support project goals, and overseeing the implementation of solutions that enhance system performance and user experience. You will engage in problem-solving activities, ensuring that the software development process aligns with best practices and industry standards, while also mentoring junior team members to foster their growth and development in the field.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and ensure timely delivery of milestones.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in CRM Architecture.- Strong understanding of software development methodologies.- Experience with system integration and data migration.- Ability to design scalable and efficient software solutions.- Familiarity with cloud-based technologies and services.\nAdditional Information:- The candidate should have minimum 12 years of experience in CRM Architecture.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['software development', 'javascript', 'system integration', 'software development methodologies', 'crm', 'c#', 'project management', 'python', 'web services', 'business analysis', 'microsoft azure', 'jquery', 'sql server', 'java', 'asp.net', 'scrum', 'agile', 'aws', 'agile methodology']",2025-06-13 05:47:11
Application Developer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP Profitability & Performance Mgt PaPM\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years of Education\n\n\nSummary:As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve collaborating with team members to develop innovative solutions and enhance application performance.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to analyze business requirements and translate them into technical solutions.- Develop and implement software solutions to enhance application performance.- Conduct code reviews and provide technical guidance to junior team members.- Stay updated on industry trends and best practices to continuously improve application development processes.- Assist in troubleshooting and resolving technical issues to ensure smooth application operation.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Profitability & Performance Mgt PaPM.- Strong understanding of SAP modules and integration with other systems.- Experience in developing and customizing SAP applications.- Knowledge of SAP Fiori and UI5 development.- Hands-on experience in SAP data migration and system upgrades.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in SAP Profitability & Performance Mgt PaPM.- This position is based at our Bengaluru office.- A 15 years of Education is required.\n\nQualification\n\n15 years of Education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'sap module', 'application development', 'ui', 'sap fiori', 'css', 'accounts payable', 'python', 'project management', 'software development', 'oracle', 'business analysis', 'accounting', 'javascript', 'jquery', 'sql', 'java', 'sap application', 'troubleshooting', 'code review', 'html']",2025-06-13 05:47:14
Quality Engineer (Tester),Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Quality Engineer (Tester)\n\n\n\n\n\nProject Role Description :Enables full stack solutions through multi-disciplinary team planning and ecosystem integration to accelerate delivery and drive quality across the application lifecycle. Performs continuous testing for security, API, and regression suite. Creates automation strategy, automated scripts and supports data and environment configuration. Participates in code reviews, monitors, and reports defects to support continuous improvement activities for the end-to-end testing process.\n\n\n\nMust have skills :Data Warehouse ETL Testing\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Quality Engineer (Tester), you will enable full stack solutions through multi-disciplinary team planning and ecosystem integration to accelerate delivery and drive quality across the application lifecycle. Your typical day will involve performing continuous testing for security, API, and regression suite. You will create automation strategy, automated scripts, and support data and environment configuration. Additionally, you will participate in code reviews, monitor, and report defects to support continuous improvement activities for the end-to-end testing process.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Conduct thorough testing of data warehouse ETL processes.- Develop and execute test cases, test plans, and test scripts.- Identify and document defects, issues, and risks.- Collaborate with cross-functional teams to ensure quality standards are met.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Warehouse ETL Testing.- Strong understanding of SQL and database concepts.- Experience with ETL tools such as Informatica or Talend.- Knowledge of data warehousing concepts and methodologies.- Experience in testing data integration, data migration, and data transformation processes.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Data Warehouse ETL Testing.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data warehousing', 'data warehouse etl testing', 'sql', 'database creation', 'etl', 'software testing', 'test case execution', 'talend', 'test cases', 'data migration', 'etl testing', 'quality engineering', 'quality testing', 'test planning', 'informatica', 'data integration']",2025-06-13 05:47:15
Application Developer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Syniti ADM for SAP\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :syniti\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. You will collaborate with teams to ensure successful project delivery and meet client expectations.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the design and development of software applications.- Conduct code reviews and provide technical guidance to team members.- Participate in project planning and provide input on technical feasibility and implementation.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Syniti ADM for SAP.- Strong understanding of data migration and data quality management.- Experience in SAP data migration projects.- Knowledge of SAP data models and structures.- Hands-on experience with SAP data extraction and transformation.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Syniti ADM for SAP.- This position is based at our Bengaluru office.- A syniti education is required.\n\nQualification\n\nsyniti",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap data migration', 'sap', 'data migration', 'data quality management', 'adm', 'python', 'data analytics', 'data analysis', 'data warehousing', 'business intelligence', 'application development', 'sql', 'plsql', 'data extraction', 'tableau', 'project delivery', 'code review', 'etl', 'sap hana']",2025-06-13 05:47:17
Application Developer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Syniti ADM for SAP\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :SYNITI\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. You will collaborate with teams to ensure seamless integration and functionality.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead and mentor junior professionals- Conduct regular knowledge sharing sessions- Stay updated on industry trends and best practices\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Syniti ADM for SAP- Strong understanding of data migration processes- Experience in SAP data management and governance- Knowledge of SAP data models and structures- Hands-on experience in SAP data quality management\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Syniti ADM for SAP- This position is based at our Bengaluru office- A SYNITI education is required\n\nQualification\n\nSYNITI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'data management', 'data migration', 'adm', 'data quality management', 'python', 'data analysis', 'data analytics', 'data warehousing', 'business analysis', 'machine learning', 'business intelligence', 'application development', 'sql', 'plsql', 'tableau', 'data modeling', 'data visualization']",2025-06-13 05:47:19
Murex Datamart Reporting Support Engineer,Synechron,5 - 10 years,Not Disclosed,"['Pune', 'Bengaluru', 'Hinjewadi']","Job Summary\nSynechron is seeking a dedicated Murex Datamart Reporting Support Engineer at the mid-level to support and optimize the reporting functions within our financial systems infrastructure. This role focuses on providing effective tier 2 support for Datamart and reporting modules, resolving incidents, and ensuring the accuracy and availability of reports such as P&L, Market Valuation, Accounting, and Risk. The successful candidate will bring solid technical expertise, analytical skills, and communication abilities to collaborate with technical and business stakeholders, supporting operational excellence and continuous improvement.\nThis position plays a crucial role in maintaining reliable reporting outputs, resolving data issues efficiently, and supporting strategic reporting initiatives aligned with business needs.\nSoftware Requirements\nRequired Skills:\nProficiency with Murex (version 3.1 or above) focusing on Datamart and reporting modules\nStrong SQL skills for data querying, analysis, and solving data-related issues\nShell scripting (Bash/sh) for automation and troubleshooting tasks\nExperience supporting report generation (P&L, MV, Risk, Accounting)\nFamiliarity with incident management tools such as ServiceNow or JIRA\nPreferred Skills:\nExperience with reporting tools such as PowerBI, Tableau, or QlikView\nKnowledge of data warehousing and data architecture concepts\nBasic scripting in Python or Perl for automation tasks\nOverall Responsibilities\nSupport and maintain Murex Datamart and reporting modules, ensuring system stability and report integrity\nRespond to and resolve L2 support tickets related to report discrepancies, data issues, and system errors\nCollaborate with business users to understand reporting requirements, diagnose issues, and implement solutions\nPerform data analysis and troubleshooting using SQL queries to identify root causes of problems\nAssist in system upgrades, patching, and configuration changes impacting reporting environments\nAutomate routine report validation tasks and improve existing processes to enhance efficiency\nDocument problem resolutions, configurations, and procedures for team knowledge sharing\nSupport incident escalations, communicate effectively with stakeholders, and prioritize support activities\nParticipate in continuous improvement initiatives to enhance report accuracy, timeliness, and system performance\nStrategic objectives:\nEnsure high availability and accuracy of critical reports\nReduce report incidents and data inconsistencies\nAutomate manual processes to improve operational efficiency\nPerformance outcomes:\nConsistent high-quality report availability\nRapid incident resolution with minimal business disruption\nClear documentation and proactive stakeholder communication\nTechnical Skills (By Category)\nReporting & Data Analysis (Essential):\nExperience supporting Murex Datamart, especially related to P&L, MV, Risk, and Accounting reports\nSQL mastery for data extraction, validation, and issue diagnosis\nKnowledge of report configuration and static data management\nScripting & Automation (Essential):\nShell scripting (Bash/sh) for automating data checks, batch processes, and troubleshooting\nExperience in automating routine report validation and data reconciliation\nData Management & Architecture (Essential):\nUnderstanding of relational databases, data flow, and data warehousing concepts\nExperience with data definitions, static data, and interface setup\nSupport & Incident Management (Essential):\nHands-on use of ServiceNow, JIRA, or equivalent service management tools\nAdditional Skills (Preferred):\nBasic knowledge of cloud deployment environments\nFamiliarity with additional scripting languages like Python or Perl\nExperience Requirements\n5+ years of production support experience supporting Murex Datamart and reporting modules\nProven experience in resolving report and data-related issues efficiently\nSupport experience in a support or operational role in financial services (trading, risk, or accounting)\nExperience working in structured support environments with incident escalation and resolution\nAlternative pathways:\nCandidates demonstrating strong support skills, extensive scripting experience, and deep understanding of financial reporting can be considered irrespective of exact years if their expertise is aligned\nDay-to-Day Activities\nMonitor system dashboards, reports, and alerts for performance issues or data discrepancies\nTroubleshoot and resolve report failures and data anomalies using SQL and scripts\nEngage with business users to clarify reporting needs and resolve issues\nSupport system upgrades, patches, and configurations affecting reporting modules\nAutomate manual validation routines to improve reliability and efficiency\nDocument resolutions, configurations, and operational procedures\nCollaborate with technical teams, support units, and stakeholders for incident resolution\nParticipate in shift handovers, incident reviews, and ongoing process improvements\nQualifications\nBachelors degree in Computer Science, Finance, Data Management, or a related discipline\n5+ years supporting Murex Datamart and Reporting modules in a production environment\nStrong SQL and shell scripting expertise\nKnowledge of financial reporting processes such as P&L, MV, Risk, and Accounting\nExperience supporting high-pressure environments, managing incidents, and problem resolution\nWillingness to work in shifts, including nights, weekends, or holidays as needed\nProfessional Competencies\nCritical thinking and analytical skills to troubleshoot complex issues\nEffective communication skills for liaising with technical teams and business stakeholders\nCollaboration skills to support cross-team coordination and problem-solving\nAbility to work independently, prioritize workloads, and manage multiple issues efficiently\nAdaptability and willingness to learn new tools and processes\nFocus on continuous enhancement of operational procedures and system stability",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Murex Datamart Reporting', 'Murex Datamart', 'Data Management', 'shell scripting', 'Data Architecture', 'SQL']",2025-06-13 05:47:21
Application Developer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Oracle HCM Cloud Core HR\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. A typical day involves collaborating with various teams to understand their needs, developing solutions that align with business objectives, and ensuring that applications are optimized for performance and usability. You will also engage in problem-solving activities, providing support and guidance to your team members while continuously seeking opportunities for improvement in application functionality and user experience.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and ensure alignment with business goals.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Oracle HCM Cloud Core HR.- Good To Have\n\n\n\n\nSkills:\nExperience with Oracle HCM Cloud Payroll, Oracle HCM Cloud Talent Management.- Strong understanding of application development methodologies.- Experience with integration of Oracle HCM Cloud with other enterprise applications.- Familiarity with data migration processes and tools.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in Oracle HCM Cloud Core HR.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['oracle core hr', 'oracle hcm cloud', 'development methodologies', 'application development', 'payroll', 'c#', 'css', 'oracle', 'jsp', 'enterprise applications', 'ado.net', 'data migration', 'hibernate', 'jquery', 'sql server', 'sql', 'microservices', 'talent management', 'spring', 'hcm', 'java', 'asp.net', 'html']",2025-06-13 05:47:23
Application Developer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Salesforce Omnistudio Platform\n\n\n\n\nGood to have skills :Salesforce DevelopmentMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :Regular 15year full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements in Pune. You will be responsible for developing innovative solutions to enhance business processes and meet application needs.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the development and implementation of new applications- Conduct code reviews and provide technical guidance to team members- Stay updated on industry trends and best practices to enhance application development\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Salesforce Omnistudio Platform, Salesforce Development- Strong understanding of Salesforce architecture and customization- Experience in developing and implementing Salesforce solutions- Knowledge of Salesforce integration and data migration- Hands-on experience with Salesforce Lightning components\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Salesforce Omnistudio Platform- This position is based at our Pune office- A Regular 15-year full-time education is required\n\nQualification\n\nRegular 15year full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['salesforce lightning', 'application development', 'sales force development', 'salesforce', 'salesforce integration', 'visualforce', 'rest', 'css', 'web services', 'data migration', 'triggers', 'javascript', 'jquery', 'apex', 'sql', 'java', 'leadership development', 'json', 'code review', 'html', 'mysql']",2025-06-13 05:47:25
Application Designer,Accenture,3 - 8 years,Not Disclosed,['Nagpur'],"Project Role :Application Designer\n\n\n\n\n\nProject Role Description :Assist in defining requirements and designing applications to meet business process and application requirements.\n\n\n\nMust have skills :ServiceNow Software Asset Management (SAM)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Designer, you will assist in defining requirements and designing applications to meet business process and application requirements. You will play a crucial role in ensuring the successful implementation of ServiceNow Software Asset Management (SAM) in our organization.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with stakeholders to gather and analyze requirements for application design.- Design and develop applications using ServiceNow Software Asset Management (SAM) to meet business process requirements.- Ensure the applications are scalable, efficient, and adhere to best practices.- Perform code reviews and provide feedback to improve application design and quality.- Troubleshoot and resolve issues related to application design and functionality.- Stay updated with the latest industry trends and technologies related to ServiceNow Software Asset Management (SAM).\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in ServiceNow Software Asset Management (SAM) and Strong understanding of ServiceNow Custom Application Creation ServiceNow Human Resource Service Management (HR)ServiceNow IT Operations ManagementServiceNow IT Service Management- Strong understanding of IT asset management principles and best practices.- Experience in designing and developing applications using ServiceNow platform.- Knowledge of ServiceNow modules and functionalities related to Software Asset Management (SAM).- Experience with data migration and integration in ServiceNow platform.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in ServiceNow Software Asset Management (SAM).- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['it asset management', 'service management', 'software asset management', 'servicenow', 'it operations', 'tds', 'project management', 'software development', 'import', 'application design', 'vat', 'excise', 'gst', 'logistics', 'application development', 'java', 'service tax', 'income tax', 'export', 'dgft']",2025-06-13 05:47:27
Application Developer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Oracle Utilities Meter Data Management (MDM)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :Minimum 15 years of full-time education Degree in Engineering will be a plus\n\n\nSummaryAs a MDM Conversion Designer/Developer he/she should Participate in various phases of the project Design, Build, Test and Deploy. Ability to understand and develop complex integrations with on-premises, and cloud-based applications. Quickly resolve defects and provide root cause analysis, implementation support Work with vendor support team to resolve issues Work with different stakeholders like customer, Project manager, Architects, Testers.\nRoles & Responsibilities-Design and implement data conversion strategies for MDM initiatives.-Data Migration Experience - Data conversion, mapping of tables, Oracle data model knowledge, experience of Oracle db. CCB high level knowhow is desirable but not mandatory -Analyse and map source data to target MDM structures.-Develop ETL processes to extract, transform, and load data into the MDM system.-Collaborate with business analysts and stakeholders to gather requirements and validate data quality.-Create and maintain documentation for data mapping, transformation rules, and conversion processes.-Perform data profiling and cleansing to ensure accuracy and consistency.-Test and validate data migration processes to ensure successful implementation.-Troubleshoot and resolve data-related issues post-conversion.-Stay updated on MDM best practices and technologies to continuously improve processes.\nProfessional & Technical\n\n\n\n\nSkills:\n-Excellent communication, problem solving and interpersonal skills.-Strong analytical capabilities to solve complicated issues arising during design and testing phases. ----Experience in all fuel or multi fuel will be a plus. -Must be a team player.\nAdditional Information-6 to 10 years of Experience with at least 2 to 3 projects implementation experience in Oracle Utilities Application Framework based CCB, C2M, MDM in design and implementation of end-to-end solution strategy. -Knowledge and experience in Groovy Language. Experience in Agile methodology-Minimum 15 years of full-time education Degree in Engineering will be a plus\n\nQualification\n\nMinimum 15 years of full-time education Degree in Engineering will be a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['oracle', 'meter data management', 'oracle dba', 'mdm', 'agile methodology', 'data conversion', 'project management', 'css', 'software testing', 'fuel', 'data mapping', 'data migration', 'root cause analysis', 'javascript', 'application development', 'sql', 'groovy', 'oracle database', 'html', 'agile', 'etl']",2025-06-13 05:47:29
Application Lead,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various stakeholders to gather requirements, overseeing the development process, and ensuring that the applications meet the specified needs. You will also engage in problem-solving discussions with your team, providing guidance and support to ensure successful project outcomes. Additionally, you will monitor project progress, address any challenges that arise, and facilitate communication among team members to maintain alignment and efficiency throughout the project lifecycle.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Facilitate knowledge sharing sessions to enhance team capabilities.- Mentor junior team members to foster their professional growth.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data integration and ETL processes.- Experience with cloud computing platforms and services.- Familiarity with data governance and compliance standards.- Ability to work with large datasets and perform data analysis.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'data governance', 'etl', 'data integration', 'etl process', 'sql development', 'python', 'oracle', 'datastage', 'data warehousing', 'data architecture', 'business intelligence', 'sql server', 'sql', 'plsql', 'unix shell scripting', 'data modeling', 'ssrs', 'ssis', 'informatica', 'unix']",2025-06-13 05:47:31
Application Developer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Workday Data Mapping & Conversions\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :Mandatory to have Workday Related certification15 years full time education\n\n\nSummary:As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. You will collaborate with teams to ensure successful project delivery and implementation.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead and mentor junior professionals in the team.- Ensure timely delivery of project milestones.- Conduct regular team meetings to discuss progress and challenges.- Stay updated on industry trends and best practices.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Workday Data Mapping & Conversions.- Strong understanding of data integration and transformation processes.- Experience with ETL tools and data migration strategies.- Knowledge of Workday HCM and Financials modules.- Hands-on experience in developing Workday reports and dashboards.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Workday Data Mapping & Conversions.- This position is based at our Bengaluru office.- A mandatory Workday Related certification is required.\n\nQualification\n\nMandatory to have Workday Related certification15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['workday', 'data mapping', 'hcm', 'data integration', 'process transformation', 'workday integration', 'project management', 'business analysis', 'data migration', 'dashboards', 'application development', 'sql', 'plsql', 'workday hcm', 'xml', 'project delivery', 'xslt', 'etl', 'eib', 'core connector']",2025-06-13 05:47:33
Application Developer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP Plant Maintenance (PM)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will engage in the design, construction, and configuration of applications tailored to fulfill specific business processes and application requirements. Your typical day will involve collaborating with team members to understand project needs, developing innovative solutions, and ensuring that applications function seamlessly to support organizational goals. You will also participate in testing and troubleshooting to enhance application performance and user experience, contributing to the overall success of the projects you are involved in.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist in the documentation of application processes and workflows.- Engage in continuous learning to stay updated with the latest technologies and methodologies.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Plant Maintenance (PM).- Strong understanding of application development methodologies.- Experience with system integration and data migration processes.- Familiarity with troubleshooting and debugging techniques.- Ability to work collaboratively in a team-oriented environment.\nAdditional Information:- The candidate should have minimum 3 years of experience in SAP Plant Maintenance (PM).- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['development methodologies', 'application development', 'debugging', 'troubleshooting', 'sap plant maintenance', 'c#', 'css', 'python', 'sap', 'c', 'jsp', 'ado.net', 'dbms', 'data migration', 'hibernate', 'jquery', 'sql server', 'microservices', 'plsql', 'spring', 'system integration', 'java', 'asp.net', 'html']",2025-06-13 05:47:34
Application Developer,Accenture,15 - 20 years,Not Disclosed,['Coimbatore'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Salesforce Technical Architecture\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n18 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 yrs of regular education\n\n\nSummary:As a Salesforce Technical Architect, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with the Salesforce platform, collaborating with cross-functional teams, and ensuring the successful delivery of Salesforce solutions.\nRoles & Responsibilities:- Lead the design and development of Salesforce solutions, ensuring alignment with business requirements and best practices.- Collaborate with cross-functional teams, including business analysts, developers, and project managers, to ensure successful delivery of Salesforce solutions.- Provide technical guidance and mentorship to team members, ensuring adherence to Salesforce best practices and standards.- Stay updated with the latest advancements in Salesforce technology, integrating innovative approaches for sustained competitive advantage.- Ensure the successful delivery of Salesforce solutions, including managing project timelines, budgets, and resources.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExpertise in Salesforce Technical Architecture.- Good To Have\n\n\n\n\nSkills:\nExperience with Salesforce Lightning, Apex, and Visualforce.- Strong understanding of Salesforce best practices and standards.- Experience with Salesforce integrations and data migration.- Experience with Salesforce Communities and Marketing Cloud.- Solid grasp of web technologies, including HTML, CSS, and JavaScript.\nAdditional Information:- The candidate should have a minimum of 18 years of experience in Salesforce Technical Architecture.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful Salesforce solutions.- This position is based at our Bengaluru office.\n\nQualification\n\n15 yrs of regular education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['visualforce', 'apex', 'salesforce', 'technical architecture', 'web technologies', 'c#', 'rest', 'css', 'web services', 'salesforce lightning', 'data migration', 'javascript', 'sql server', 'application development', 'sql', 'microservices', 'spring', 'java', 'computer science', 'asp.net', 'j2ee', 'json', 'html']",2025-06-13 05:47:36
Application Developer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP CPI for Data Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. Your day will involve collaborating with teams to create innovative solutions and ensure seamless application functionality.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the team in implementing cutting-edge technologies- Drive continuous improvement initiatives within the team\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP CPI for Data Services- Strong understanding of data integration and transformation processes- Experience in developing and implementing data migration strategies- Hands-on experience with SAP Cloud Platform Integration tools- Knowledge of SAP ERP systems integration- Good To Have\n\n\n\n\nSkills:\nExperience with SAP Cloud Platform Integration for process integration\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in SAP CPI for Data Services- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data services', 'sap erp', 'sap cpi', 'data integration', 'process transformation', 'sap upgrade', 'erp', 'sap', 'oracle', 'sap netweaver', 'data migration', 'application development', 'sql', 'sap s hana', 'spring', 'java', 'sap fiori', 'sap basis', 'sapui5', 'sap hana', 'sap abap', 'sap cloud']",2025-06-13 05:47:38
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :ASP.NET MVC\n\n\n\n\nGood to have skills :Amazon Web Services (AWS)Minimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various teams to ensure project milestones are met, facilitating discussions to address challenges, and guiding your team in implementing effective solutions. You will also engage in strategic planning sessions to align project goals with organizational objectives, ensuring that all stakeholders are informed and involved in the development process. Your role will be pivotal in driving innovation and efficiency within the application development lifecycle, fostering a collaborative environment that encourages creativity and problem-solving.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and implement necessary adjustments to meet deadlines.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in ASP.NET MVC.- Good To Have\n\n\n\n\nSkills:\nExperience with Amazon Web Services (AWS).- Strong understanding of web application architecture and design patterns.- Experience with front-end technologies such as HTML, CSS, and JavaScript.- Familiarity with database management systems and SQL.- Ability to troubleshoot and optimize application performance.Must -AWS:Lambda, DynamoDB, CloudWatch, ...-GitHubMust/Nice -NodeJSNice -React-Azure DevOps-NewRelic knowledge-Fintech knowledge\nAdditional Information:- The candidate should have minimum 5 years of experience in ASP.NET MVC.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['dynamo db', 'lambda expressions', 'design patterns', 'aws', 'amazon cloudwatch', 'css', 'web services', 'web application architecture', 'new relic', 'hibernate', 'net mvc', 'javascript', 'azure devops', 'sql', 'spring', 'react.js', 'asp.net core mvc', 'database management', 'java', 'asp.net', 'html', 'mvc', 'asp']",2025-06-13 05:47:40
Business Analyst,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Microsoft Dynamics 365 ERP Technical\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. You will assess the current state, identify customer requirements, and define the future state and/or business solution. Research, gather, and synthesize information to contribute to key decisions and solutions.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead requirements gathering sessions with stakeholders.- Develop functional specifications and system design documents.- Conduct gap analysis and recommend solutions.- Facilitate user acceptance testing and provide post-implementation support.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Dynamics 365 ERP Technical.- Strong understanding of system integration and data migration.- Experience in configuring and customizing Dynamics 365 ERP modules.- Knowledge of SQL and database management.- Hands-on experience in troubleshooting and resolving technical issues.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Microsoft Dynamics 365 ERP Technical.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['erp', 'microsoft dynamics', 'data migration', 'sql', 'system integration', 'design documents', 'project management', 'gap analysis', 'system design', 'business analysis', 'user stories', 'database management', 'brd', 'user acceptance testing', 'troubleshooting', 'scrum', 'agile']",2025-06-13 05:47:42
Analyst - SAP-PC-PS,KPMG India,7 - 9 years,Not Disclosed,['Mumbai'],"KPMG entities in India are professional services firm(s). These Indian member firms are affiliated with KPMG International Limited. KPMG was established in India in August 1993. Our professionals leverage the global network of firms and are conversant with local laws, regulations, markets, and competition. KPMG has offices across India in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, Vadodara, and Vijayawada.\nKPMG entities in India offer services to national and international clients in India across sectors. We strive to provide rapid, performance-based, industry-focused, and technology-enabled services, which reflect a shared knowledge of global and local industries and our experience of the Indian business environment.\nJob Responsibilities:\nShould experience in SAP S/4HANA Public Cloud Professional Services expert with 7+ years of total experience, including 4+ years in Professional Services areas.\nKey Responsibilities:\nEvent-Based Revenue Recognition (EBRR)\nProfessional Services Configuration\nBusiness Requirement Documentation\nERP Professional Services Applications\nShould have worked in Customer Project Management Integration points for triggering Sales Orders Header to header Mapping and work items to Sales Order Mapping for Fixed Time, Periodic Services, Time & Expenses, Usage Basis Project types.\nEducation :\nBachelor s degree or higher in Information Technology, Business, Engineering, or a related field\nBE/BTech/MBA/MCA Full-Time Education\nSAP Certification\nEOE KI :\nRoles & Responsibilities:\nExtensive team leadership in Professional Services module configuration and implementation.\nIn-depth knowledge of SAP S/4HANA functional consulting, especially in cross-module integration, customer/internal projects, and project scenarios.\nExpertise in EBRR, multiple contract handling, billing solutions, and cross-company data migration.\nExperience in designing and optimizing end-to-end SAP Professional Services processes.\nWorking experience with SAP S/4HANA Public Cloud, Ariba, Concur, and SAP Analytics Cloud.\nAgile-based delivery method expertise and project cost management.\nStrong problem-solving, decision-making, interpersonal, and communication skills.\nSelf-motivated, quick learner, and ability to work under aggressive timelines.\nChange management, governance, continuous development, and analytical thinking.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ERP', 'Change management', 'SAP', 'Project management', 'Consulting', 'Billing', 'Agile', 'professional services', 'Information technology', 'Analytics']",2025-06-13 05:47:44
Analyst - SAP-PC-MM,KPMG India,2 - 7 years,Not Disclosed,['Mumbai'],"KPMG entities in India are professional services firm(s). These Indian member firms are affiliated with KPMG International Limited. KPMG was established in India in August 1993. Our professionals leverage the global network of firms and are conversant with local laws, regulations, markets, and competition. KPMG has offices across India in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, Vadodara, and Vijayawada.\nKPMG entities in India offer services to national and international clients in India across sectors. We strive to provide rapid, performance-based, industry-focused, and technology-enabled services, which reflect a shared knowledge of global and local industries and our experience of the Indian business environment.\nJob Responsibilities:\nShould experience in SAP S/4HANA Public Cloud Professional Services expert with2 years of total experience,\nKey Responsibilities:\nEvent-Based Revenue Recognition (EBRR)\nProfessional Services Configuration\nBusiness Requirement Documentation\nERP Professional Services Applications\nShould have worked in Customer Project Management Integration points for triggering Sales Orders Header to header Mapping and work items to Sales Order Mapping for Fixed Time, Periodic Services, Time & Expenses, Usage Basis Project types.\nEducation :\nBachelor s degree or higher in Information Technology, Business, Engineering, or a related field\nBE/BTech/MBA/MCA Full-Time Education\nSAP Certification\nEOE KI :\nRoles & Responsibilities:\nExtensive team leadership in Professional Services module configuration and implementation.\nIn-depth knowledge of SAP S/4HANA functional consulting, especially in cross-module integration, customer/internal projects, and project scenarios.\nExpertise in EBRR, multiple contract handling, billing solutions, and cross-company data migration.\nExperience in designing and optimizing end-to-end SAP Professional Services processes.\nWorking experience with SAP S/4HANA Public Cloud, Ariba, Concur, and SAP Analytics Cloud.\nAgile-based delivery method expertise and project cost management.\nStrong problem-solving, decision-making, interpersonal, and communication skills.\nSelf-motivated, quick learner, and ability to work under aggressive timelines.\nChange management, governance, continuous development, and analytical thinking.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ERP', 'Change management', 'Data migration', 'SAP', 'Project management', 'Consulting', 'Billing', 'Agile', 'Information technology', 'Analytics']",2025-06-13 05:47:46
IN_Senior Associate_Pyspark _D&A_Advisory_Bangalore,PwC Service Delivery Center,3 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\n.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decisionmaking for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\nWhy PWC\nHandson experience in Pyspark, preferably more than 3 years familiarity with RDD level programming as well as coding using Spark DataFrames API.\nAbility to develop and maintain data pipelines and ETLs using Python and Pyspark.\nShould have good knowledge of Python and Spark concepts like Driver vs Worker, actions and transforms, data partitioning and bucketing etc.\nExperience in data management activities and data lifecycle management for example activities related to integrating source databases to a data lake via full batch or incremental data updates.\nAbility to design, implement, and optimize Spark jobs for performance and scalability.\nPerform data analysis and troubleshooting to ensure data quality and reliability.\nExperience in cloud technologies esp. object storage for data lakes is a must.\nExperience in Spark Streaming, GraphX is a plus.\nMandatory skill sets\nPyspark\nPreferred skill sets\nPyspark\nYears of experience required\n5+\nEducation qualification\nBE/BTech/MBA/MCA\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Master of Business Administration, Bachelor of Technology\nDegrees/Field of Study preferred\nRequired Skills\nPySpark\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis {+ 12 more}\nNo",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Data management', 'Process improvement', 'Analytical', 'Trend analysis', 'Data collection', 'Data quality', 'DBMS', 'Business intelligence', 'Data architecture']",2025-06-13 05:47:48
Analyst-Qlik Sense Developer,AstraZeneca India Pvt. Ltd,5 - 6 years,Not Disclosed,['Bengaluru'],"Job Title: Analyst-Qlik Sense Developer\nGlobal Career Level: C3\nIntroduction to role:\nAre you ready to transform raw data into actionable insights? As a Qlik Sense Data Reporting and Analytics Developer, youll be at the forefront of driving informed decision-making and strategic initiatives across Alexion. Your expertise in designing, developing, and maintaining data reporting solutions will empower our organization to make data-driven decisions. If you have a strong background in data analysis, proficiency in visualization tools, and excellent communication skills, this is the role for you!\nAccountabilities:\nSupport the Alexion team with field force reporting by designing, developing, validating, and maintaining Qlik Sense dashboards and supporting model tiers for various business units and indications.\nUnderstand business objectives, data sources, and key performance indicators (KPIs) to design effective solutions.\nDesign and implement data models in QlikSense, including ETL processes.\nWrite and optimize Qlik scripting language using SQL to transform raw data into actionable insights by creating QVDs; transforming source data into dimensions/factors for dashboards.\nIntegrate data from multiple sources, ensuring accuracy, consistency, and optimal performance.\nDevelop interactive dashboards, reports, and visualizations using Qlik Sense.\nIdentify and address performance bottlenecks in Qlik applications; optimize data models, load scripts, and front-end visualizations for fast user experiences.\nConduct thorough testing of Qlik applications to validate data accuracy, functionality, and usability.\nCollaborate with QA testers and business users to resolve issues promptly.\nDesign intuitive user interfaces that facilitate data exploration, analysis, and insight generation.\nWork closely with cross-functional teams to align Qlik development efforts with organizational goals.\nCommunicate project status, challenges, and recommendations to stakeholders clearly.\nInstill a culture of continuous improvement, testing, and deployment of new capabilities for the business.\nEssential Skills/Experience:\nAdvanced understanding/experience with SQL, Snowflake, and Veeva CRM.\nAbility to create new rules and adjust existing rules.\nExpertise in Qlik scripting language + data modelling concepts, including related skills in:\n- Data warehouse\n- Data architecture\n- Data visualization (inclusive of Vizlib extensions)\n- Section access (security)\n- N-printing for sending reports.\nRecent project experience with Qlik + experience with other BI tools\nDesirable Skills/Experience:\nBackground in computer science, information systems, or related field.\n5-6 years of experience in developing reporting and visualization applications.\nExperience in web-development (JavaScript and CSS)\nExcellent analytical and problem-solving skills, with keen attention to detail.\nAbility to work independently and collaboratively in a dynamic environment.\nStrong communication and interpersonal skills.\nAt AstraZenecas Alexion division, youll find an environment where work isnt ordinary. Our closeness to patients brings us closer to our work and each other. With a rapidly expanding portfolio, youll enjoy the entrepreneurial spirit of a leading biotech combined with the security of a global pharma. Here, your career is not just a path but a journey to making a difference where it truly counts. Youll be empowered with tailored development programs designed for skill enhancement and fostering a deep understanding of our patients journeys. Join us to innovate and grow in a culture that celebrates diversity, innovation, and connection.\nReady to make an impact? Apply now and be part of our journey!\n11-Jun-2025\n14-Jun-2025\nAlexion is proud to be an Equal Employment Opportunity and Affirmative Action employer. We are committed to fostering a culture of belonging where every single person can belong because of their uniqueness. The Company will not make decisions about employment, training, compensation, promotion, and other terms and conditions of employment based on race, color, religion, creed or lack thereof, sex, sexual orientation, age, ancestry, national origin, ethnicity, citizenship status, marital status, pregnancy, (including childbirth, breastfeeding, or related medical conditions), parental status (including adoption or surrogacy), military status, protected veteran status, disability, medical condition, gender identity or expression, genetic information, mental illness or other characteristics protected by law. Alexion provides reasonable accommodations to meet the needs of candidates and employees. To begin an interactive dialogue with Alexion regarding an accommodation, please contact accommodations@Alexion.com . Alexion participates in E-Verify.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Printing', 'Data analysis', 'Front end', 'Pharma', 'Web development', 'Javascript', 'Analytics', 'SQL', 'CRM']",2025-06-13 05:47:50
Software Engineer III,JPMorgan Chase Bank,0 - 5 years,Not Disclosed,['Bengaluru'],"As a Software Engineer III at JPMorgan Chase within the AI/ML Data platform team, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm s business objectives.\n\nJob responsibilities\n\n\n\nDesign, develop, and maintain automated test scripts specifically for API and UI testing. This involves creating robust and reusable test scripts that ensure the functionality and performance of software applications.\n\nConduct testing using Python frameworks such as Pytest . This includes writing and executing test cases, as well as leveraging the features of these frameworks to enhance test coverage and efficiency.\n\nImplement UI test automation using tools like Playwright or Selenium, and perform API testing with Python. This requires a deep understanding of these tools to effectively automate the testing process and validate the user interface and backend services.\n\nCollaborate closely with development and product teams to identify test requirements and strategies.\n\nAnalyze test results to identify issues, bugs, or performance bottlenecks. Work on resolutions by collaborating with developers to fix defects and improve the quality of the software.\n\nEnsure that automated tests are seamlessly integrated into the Continuous Integration/Continuous Deployment (CI/CD) pipeline.\n\nAdds to team culture of diversity, equity, inclusion, and respect\n\n\n\nRequired qualifications, capabilities, and skills\n\n\nFormal training or certification on software engineering concepts and 3+ years applied experience\n\nHands on programming skills in Python, demonstrating the ability to write clean, efficient, and maintainable code, with a deep understanding of Pythons syntax, libraries, and frameworks, and experience in developing software applications using Python\n\nExperience with test automation tools and libraries for both API and UI testing, including the ability to design, develop, and maintain automated test scripts, and familiarity with tools such as Selenium, Postman, or similar frameworks that facilitate automated testing processes.\n\nHands-on experience with Amazon Web Services (AWS), particularly with services such as S3 and Lambda, including the ability to deploy, manage, and optimize cloud-based applications and services, leveraging AWSs capabilities to enhance application performance and scalability.\n\nStrong communication skills and teamwork abilities to collaborate effectively with cross-functional teams to achieve common goals.\n\n\nPreferred qualifications, capabilities, and skills\n\n\nFamiliarity with CI/CD pipelines and tools like Jenkins, Docker, Kubernetes\n\nFamiliar with cloud tools like Kubernetes, EKS, AWS Glue, ECS\n\nFamiliar with building performance tests using Locust",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Backend', 'Test case execution', 'Test scripts', 'UI testing', 'Cloud', 'Agile', 'Selenium', 'Software Engineer III', 'Application software', 'Python']",2025-06-13 05:47:52
Immediate Joiner / Java Developer,Service based Top B2B MNC in IT Services...,5 - 10 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nBuild high performant, highly scalable, complex and\ndistributed systems end to end\nDevelop simple solutions to address complex\nproblems.\nGive to a future-ready, high quality, and performant\ncode base.\nBring technical solutions to the leadership team,\nfeedback on solutions recommended, new product\nideas with the team through design review, pair\nprogramming, code review and tech talk.\nAbility to share technical solutions and product\nideas with the broader team through design review,\ncode review, proof-of-concepts and show and tell\nParticipate in brainstorming sessions and give ideas\nto our technology, algorithms and products\nImplement new features in a highly collaborative\nenvironment with product managers, UI/UX guides,\nand software and hardware engineers.\nMinimum Qualifications\nBachelor's degree (or above) in\nengineering/computer science with an overall work\nexperience of 5-8 years in in Java/Kotlin\nDevelopment\nPossess advanced knowledge of object-oriented\ndesign and development and data architectures.\nConfirmed ability to communicate with different\nlevels in the organization, influence others and\nbuilding consensus, developing teamwork across\nteams, and seek problems/remove obstacles in a\ntimely manner\n\nBroad Information Technology experience,\nincluding understanding of tools, processes and\nstandard methodologies of project execution\nAdept in coordinating and participating in all\nactivities (analysis, scoping, and design,\ncoding/code reviews, test case reviews, defect\nmanagement, implementation planning /execution\nand support, leading resources and timelines)\nExperience working with and applying Design\npatterns to tackle problems.\nStrong CS fundamentals in algorithms and data\nstructures\nExtensive technical experience and development\nexpertise in Core Java, Koltin, Kotlin Co-routines,\nKafka, Vertx, Nodejs , Java Script, JQuery and\nAJAX, Asynchronous programming .\nExperience doing Object Oriented Analysis and\nDesign, using Domain Driven Design, and Design\nPatterns\nExperience with tools like Maven, Jenkins, Git\nExtensive technical experience and development\nexpertise in data-driven applications utilizing\nsignificant relational database engines/NO SQL\nDatabases as\npart of the overall application architecture\n(experience with any or all of the following helpful:\nMysql Oracle, SQL Server, MongoDB, Cassandra)\nExcellent debugging and testing skills\nAbility to perform performance and scalability\nanalysis as needed.\nGood knowledge of technology and product trends,\nincluding knowledge of what is happening in Open\n\nSource and in other parts of the software\ndevelopment industry\nAbility to work with large-scale distributed systems\nExcellent in problem-solving and multitasking skills\nAbility to work in fast paced environment with good\npartnership capabilities.\nImplement solutions focusing on reuse and industry\nstandards at a program, enterprise or operational\nscope.\nStrong understanding of system performance and\nscaling\nAbility to work in an agile and collaborative setup\nwithin an engineering team.\nPossess excellent communication, sharp analytical\nabilities with validated design skills, able to think\ncritically of the current system in terms of growth\nand stability\nProven ability to mentor other software developers\nto maintain architectural vision and software quality\nStrong desire to build, sense of ownership, urgency,\nand drive\nVerify stability, interoperability, portability, security\nand scalability of java system architecture.\n\nDesired Qualifications\nValidated experience of strong desire to work in\nproduct development\nBe highly flexible and adaptable and demonstrate\npassion for platform development\nExcellent partnership, written and verbal\ncommunication skills\n\nExpertise in delivering high-quality, innovative\napplication\nFamiliar with AWS or other Cloud environment\nConsistent track record of innovation and thought\nleadership.\nObsession with quality and customer experience -\nAttention to detail coupled with ability to think\nabstractly\nFamiliarity with running large scale web services;\nunderstanding of systems internals and networking\nare a plus\nConsistent track record of developing large\nenterprise grade software and delivering high\nquality products/releases on time\nAnalyze performance characteristics to identify\nbottlenecks, failure points, and security holes in\nlarge scale systems. Passionate about performance\nand scalability\nSelf-Starter with the ability to remain flexible and\nlearn new technologies quickly\nCan do attitude and flexibility in pursuing various\nassignments to make the company successful\nConsistent track record of innovation and thought\nleadership.\nObsession with quality and customer experience -\nAttention to detail coupled with ability to think\nabstractly\nThe ability to take convert raw requirements into\ngood design while exploring technical feasibility\ntradeoffs\nProven ability to achieve stretch goals in a highly\ninnovative and fast paced environment\n\nEvaluate current or emerging technologies to\nconsider monetary factors of java program.\n\n\nMandatory skills - Core Java, AWS, , Kafka, OOPS, Maven/Jenkins and No-SQL",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Core Java', 'Kafka', 'AWS', 'Jenkins', 'Maven', 'NoSQL', 'Manves', 'OOPS']",2025-06-13 05:47:53
Java Developer(Immediate Joiner's within 10 Days),Service based Top B2B MNC in IT Services...,6 - 9 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nBuild high performant, highly scalable, complex and\ndistributed systems end to end\nDevelop simple solutions to address complex\nproblems.\nGive to a future-ready, high quality, and performant\ncode base.\nBring technical solutions to the leadership team,\nfeedback on solutions recommended, new product\nideas with the team through design review, pair\nprogramming, code review and tech talk.\nAbility to share technical solutions and product\nideas with the broader team through design review,\ncode review, proof-of-concepts and show and tell\nParticipate in brainstorming sessions and give ideas\nto our technology, algorithms and products\nImplement new features in a highly collaborative\nenvironment with product managers, UI/UX guides,\nand software and hardware engineers.\nMinimum Qualifications\nBachelor's degree (or above) in\nengineering/computer science with an overall work\nexperience of 5-8 years in in Java/Kotlin\nDevelopment\nPossess advanced knowledge of object-oriented\ndesign and development and data architectures.\nConfirmed ability to communicate with different\nlevels in the organization, influence others and\nbuilding consensus, developing teamwork across\nteams, and seek problems/remove obstacles in a\ntimely manner\n\nBroad Information Technology experience,\nincluding understanding of tools, processes and\nstandard methodologies of project execution\nAdept in coordinating and participating in all\nactivities (analysis, scoping, and design,\ncoding/code reviews, test case reviews, defect\nmanagement, implementation planning /execution\nand support, leading resources and timelines)\nExperience working with and applying Design\npatterns to tackle problems.\nStrong CS fundamentals in algorithms and data\nstructures\nExtensive technical experience and development\nexpertise in Core Java, Koltin, Kotlin Co-routines,\nKafka, Vertx, Nodejs , Java Script, JQuery and\nAJAX, Asynchronous programming .\nExperience doing Object Oriented Analysis and\nDesign, using Domain Driven Design, and Design\nPatterns\nExperience with tools like Maven, Jenkins, Git\nExtensive technical experience and development\nexpertise in data-driven applications utilizing\nsignificant relational database engines/NO SQL\nDatabases as\npart of the overall application architecture\n(experience with any or all of the following helpful:\nMysql Oracle, SQL Server, MongoDB, Cassandra)\nExcellent debugging and testing skills\nAbility to perform performance and scalability\nanalysis as needed.\nGood knowledge of technology and product trends,\nincluding knowledge of what is happening in Open\n\nSource and in other parts of the software\ndevelopment industry\nAbility to work with large-scale distributed systems\nExcellent in problem-solving and multitasking skills\nAbility to work in fast paced environment with good\npartnership capabilities.\nImplement solutions focusing on reuse and industry\nstandards at a program, enterprise or operational\nscope.\nStrong understanding of system performance and\nscaling\nAbility to work in an agile and collaborative setup\nwithin an engineering team.\nPossess excellent communication, sharp analytical\nabilities with validated design skills, able to think\ncritically of the current system in terms of growth\nand stability\nProven ability to mentor other software developers\nto maintain architectural vision and software quality\nStrong desire to build, sense of ownership, urgency,\nand drive\nVerify stability, interoperability, portability, security\nand scalability of java system architecture.\n\nDesired Qualifications\nValidated experience of strong desire to work in\nproduct development\nBe highly flexible and adaptable and demonstrate\npassion for platform development\nExcellent partnership, written and verbal\ncommunication skills\n\nExpertise in delivering high-quality, innovative\napplication\nFamiliar with AWS or other Cloud environment\nConsistent track record of innovation and thought\nleadership.\nObsession with quality and customer experience -\nAttention to detail coupled with ability to think\nabstractly\nFamiliarity with running large scale web services;\nunderstanding of systems internals and networking\nare a plus\nConsistent track record of developing large\nenterprise grade software and delivering high\nquality products/releases on time\nAnalyze performance characteristics to identify\nbottlenecks, failure points, and security holes in\nlarge scale systems. Passionate about performance\nand scalability\nSelf-Starter with the ability to remain flexible and\nlearn new technologies quickly\nCan do attitude and flexibility in pursuing various\nassignments to make the company successful\nConsistent track record of innovation and thought\nleadership.\nObsession with quality and customer experience -\nAttention to detail coupled with ability to think\nabstractly\nThe ability to take convert raw requirements into\ngood design while exploring technical feasibility\ntradeoffs\nProven ability to achieve stretch goals in a highly\ninnovative and fast paced environment\n\nEvaluate current or emerging technologies to\nconsider monetary factors of java program.\n\n\nMandatory skills - Core Java, AWS, , Kafka, OOPS, Maven/Jenkins and No-SQL",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Core Java', 'Kafka', 'AWS', 'Jenkins', 'Maven', 'NoSQL', 'Manves', 'OOPS']",2025-06-13 05:47:55
Solution Architecture Specialist,Accenture,7 - 11 years,Not Disclosed,['Kolkata'],"Skill required: Tech for Operations - Tech Solution Architecture\n\n\n\n\nDesignation: Solution Architecture Specialist\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:7 to 11 years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nIn our Service Supply Chain offering, we leverage a combination of proprietary technology and client systems to develop, execute, and deliver BPaaS (business process as a service) or Managed Service solutions across the service lifecycle:Plan, Deliver, and Recover. Join our dynamic Service Supply Chain (SSC) team and be at the forefront of helping world class organizations unlock their full potential. Imagine a career where your innovative work makes a real impact, and every day brings new challenges and opportunities for growth. We re on the lookout for passionate, talented individuals ready to make a difference. . If you re eager to shape the future and drive success, this is your chancejoin us now and lets build something extraordinary together!The Technical Solution Architect I is responsible for evaluating an organizations business needs and determining how IT can support those needs leveraging software like Azure, and Salesforce. Aligning IT strategies with business goals has become paramount, and a solutions architect can help determine, develop, and improve technical solutions in support of business goals. The Technical Solution Architect I also bridge communication between IT and business operations to ensure everyone is aligned in developing and implementing technical solutions for business problems. The process requires regular feedback, adjustments, and problem solving in order to properly design and implement potential solutions. To be successful as a Technical Solution Architect I, you should have excellent technical, analytical, and project management skills.\n\n\n\n\nWhat are we looking for\nMinimum of 5 years of IT experienceMinimum of 1 year of experience in solution architectureMinimum of 1 year of Enterprise-scale project delivery experienceMicrosoft Azure Cloud ServicesMicrosoft Azure Data FactoryMicrosoft Azure DatabricksMicrosoft Azure DevOpsWritten and verbal communicationAbility to establish strong client relationshipProblem-solving skillsStrong analytical skillsExpert knowledge of Azure Cloud ServicesExperience with Azure Data platforms (Logic apps, Service bus, Databricks, Data Factory, Azure integration services)CI/CD, version-controlling experience using Azure DevopsPython ProgrammingKnowledge of both traditional and modern data architecture and processing concepts, including relational databases, data warehousing, and business analytics. (e.g., NoSQL, SQL Server, Oracle, Hadoop, Spark, Knime). Good understanding of security processes, best practices, standards & issues involved in multi-tier cloud or hybrid applications. Proficiency in both high-level and low-level designing to build an architect using customization or configuration on Salesforce Service cloud, Field Service lightening, APEX, Visual Force, Lightening, Community. Expertise in designing and building real time/batch integrations between Salesforce and other systems. Design Apex and Lightning framework including Lightning Pattern, Error logging framework etc.\n\n\n\nRoles and Responsibilities: Meet with clients to understanding their needs (lead architect assessment meetings),and determining gaps between those needs and technical functionality. Communicate with key stakeholder, across different stages of the Software Development Life Cycle. Work on creating the high-level design and lead architectural decisions Interact with clients to create end-to-end specifications for Azure & Salesforce cloud solutions Provide clarification and answer any question regarding the solution architecture Lead the development of custom enterprise solutions Responsible for application architecture, ensuring high performance, scalability, and availability for those applications Responsible for overall data architect, modeling, and related standards enforced throughout the enterprise ecosystem including data, master data, and metadata, processes, governance, and change control Unify the data architecture used within all applications and identifying appropriate systems of record, reference, and management Share engagement experience with the internal audiences and enrich collective IP. Conduct architecture workshops and other enablement sessions.\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'project management', 'microsoft azure', 'ci/cd', 'apex', 'oracle', 'datafactory', 'data warehousing', 'azure data factory', 'sql server', 'azure cloud', 'nosql', 'data bricks', 'salesforce', 'software development life cycle', 'azure automation', 'spark', 'hadoop']",2025-06-13 05:47:57
Cloud Migration Engineer,Accenture,15 - 25 years,Not Disclosed,['Chennai'],"Project Role :Cloud Migration Engineer\n\n\n\n\n\nProject Role Description :Provides assessment of existing solutions and infrastructure to migrate to the cloud. Plan, deliver, and implement application and data migration with scalable, high-performance solutions using private and public cloud technologies driving next-generation business outcomes.\n\n\n\nMust have skills :Salesforce Service Cloud\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n15 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Cloud Migration Engineer, you will provide a comprehensive assessment of existing solutions and infrastructure to facilitate their migration to the cloud. Your typical day will involve planning, delivering, and implementing application and data migration strategies that leverage both private and public cloud technologies. You will focus on creating scalable and high-performance solutions that drive next-generation business outcomes, ensuring that the migration process aligns with organizational goals and enhances operational efficiency.\nRoles & Responsibilities:- Expected to be a Subject Matter Expert with deep knowledge and experience.- Should have influencing and advisory skills.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate workshops and training sessions to enhance team understanding of cloud migration processes.- Develop and maintain documentation related to migration strategies and best practices.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Salesforce Service Cloud.- Strong understanding of cloud architecture and migration strategies.- Experience with data integration and transformation tools.- Familiarity with security best practices in cloud environments.- Ability to analyze and optimize cloud performance metrics.\nAdditional Information:- The candidate should have minimum 15 years of experience in Salesforce Service Cloud.- This position is based at our Chennai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud migration', 'apex', 'salesforce', 'cloud architecture', 'salesforce service cloud', 'visualforce', 'sfdc', 'salesforce lightning', 'triggers', 'salesforce cpq', 'sales force development', 'salesforce crm', 'salesforce sales cloud', 'data loader', 'salesforce administration', 'aws']",2025-06-13 05:47:59
Senior Software Engineer - React Developer,Mastercard,6 - 9 years,Not Disclosed,['Pune'],"Real-Time Payments (RTP) offer immediate transfer and detailed messaging options for A2A (Account to Account) payments and are critical on Mastercard s journey for Multi-Rail Payments. Real Time Payments are categorized as Critical National Infrastructure for respective countries and requires high benchmark of resilience, availability, quality, and scalability. Within Real Time Paymeents, Data Feed Manager provides access to transaction, settlement, and reference data information from the Instant Payment System (IPS) in the form of data extracts, all year round.\n\nAll About You\n6-9 years of overall career experience into Technology or a similar role\nExtensive experience in UI technologies and practices using JavaScript, Typescript, React\nexperience in working with fast paced, modern architecture using Java, Spring Boot, Oracle RDBMS, SQL, Flyway.\nexperience in designing and implementing data models, data migration, and data streams.\nexperience working in cross-functional and large projects with globally dispersed engineering teams.\nthe ability to write secure code and be familiar with secure coding standards (eg, OWASP, CWE, SEI CERT) and the detection and remediation of security vulnerabilities.\nthe skills in building applications using opensource frameworks to achieve reuse and reduce development times (eg, Spring Boot, React, others).\nthe ability to configure rules and build automation for code with vulnerability scanning and software composition analysis using standard tools (eg, SonarQube, Checkmarx, JFrog XRay, Blackduck, others)\nthe skills to conduct various performance tests (eg, load, spike, breakpoint, endurance) in order to assess if the design and implementation meets the non-functional requirements.\nproven history of designing enterprise-grade applications utilizing Domain Driven Design principles, microservices, event driven systems, 12 factor principles for cloud native applications, REST and GRPC API design, CAP Theorem, etc\na good understanding of distributed systems design and implementation.\nthe ability to perform debugging and troubleshooting to analyze core, heap, thread dumps and remove coding errors.\nthe proficiency in Software Development Best Practices (TDD/BDD, Unit test, Continuous Integration and Delivery)\nthe skills to orchestrate release workflows and pipelines and apply standardized pipelines via APIs to achieve CI and CD using industry-standard tools (eg, Jenkins, AWS/Azure pipelines, XL Release, Ansible, others)\npractitioner of automation/configuration management, deployment strategies (blue/green, canary, A/B testing, feature flags).\nexperience and comprehensive understanding of software/application observability (logging, tracing, metrics, etc), and health and liveliness features, which enable improved R&A and operational monitoring.\nvery good analytical and problem solving skills.\nthe desire to stay abreast of advances in software engineering practices, technologies, and tooling.\nvery good inter-personal skills and ability to work in a collaborative environment\neffective communication and interpersonal skills, with an ability to express design ideas to a development team and senior management.\n\nNice to have:\nExperience in building and maintaining critical national infrastructure.\nExperience working in payments industry.\nGood understanding of private and public cloud eco systems.\nWe currently operate a hybrid working pattern which allows colleagues to gain a work life balance with the requriement to work from the office and home for part of the week.\nFlexible to working patterns that enable colaboration and engagement with other teams/individuals that are located in differnet global time zones, as required.\n\nThe Real Time Payments team is looking for a Senior Software Engineer with proven experience in a squad, including coaching and mentoring.\n\nA self starter who is able to take understand and take control of code without much need for knowledge transfer/training.\nSelf sufficient in delivery and be able to progress on work areas with minimal supervision\nown complex problems having dependency across services and facilitate cross-functional team interactions to drive resolution.\nbe responsible for the analysis, design, development and delivery of software solutions.\nwrite code to build and enhance applications/services and promote code-reviews, secure code, whilst adhering to standards and best practices resulting in the deliver high-quality artifacts through to production.\ndefine, design, and develop procedures and solutions at a service level to meet the business requirements/enhancements.\nautomate and simplify all aspects of software delivery and development actively evangelizing the need to automate and simplify where needed.\ndrive seamless integration across all connected services to meet end-user expectations.\npromote a blameless postmortem culture to identify root causes of incidents and implement learnings.\nadvocate for engineering principles outside of current organization/platform.\nbuild relationships and effective partnerships across organizations.\nconduct technical interviews for hiring engineering staff and raising the performance bar.\narchitect monitoring, logging, and alerting to provide end-to-end observability.\n\nCorporate Security Responsibility\nEvery person working for, or on behalf of, Mastercard is responsible for information security. All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and therefore, it is expected that the successful candidate for this position must:\nabide by Mastercard s security policies and practices;\nensure the confidentiality and integrity of the information being accessed;\nreport any suspected information security violation or breach, and\ncomplete all periodic mandatory security trainings in accordance with Mastercard s guidelines.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data migration', 'Coding', 'Information security', 'Configuration management', 'Debugging', 'Javascript', 'Troubleshooting', 'IPS', 'Monitoring', 'SQL']",2025-06-13 05:48:01
Senior Software Engineer - React Developer,Dynamic Yield,6 - 9 years,Not Disclosed,['Pune'],"Our Purpose\nTitle and Summary\nSenior Software Engineer - React Developer\nWho is Mastercard?\nMastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships, and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.\nOur decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.\n\nOverview\nReal-Time Payments (RTP) offer immediate transfer and detailed messaging options for A2A (Account to Account) payments and are critical on Mastercard s journey for Multi-Rail Payments. Real Time Payments are categorized as Critical National Infrastructure for respective countries and requires high benchmark of resilience, availability, quality, and scalability. Within Real Time Paymeents, Data Feed Manager provides access to transaction, settlement, and reference data information from the Instant Payment System (IPS) in the form of data extracts, all year round.\n\nAll About You\n6-9 years of overall career experience into Technology or a similar role\nExtensive experience in UI technologies and practices using JavaScript, Typescript, React\nexperience in working with fast paced, modern architecture using Java, Spring Boot, Oracle RDBMS, SQL, Flyway.\nexperience in designing and implementing data models, data migration, and data streams.\nexperience working in cross-functional and large projects with globally dispersed engineering teams.\nthe ability to write secure code and be familiar with secure coding standards (e.g., OWASP, CWE, SEI CERT) and the detection and remediation of security vulnerabilities.\nthe skills in building applications using opensource frameworks to achieve reuse and reduce development times (e.g., Spring Boot, React, others).\nthe ability to configure rules and build automation for code with vulnerability scanning and software composition analysis using standard tools (e.g., SonarQube, Checkmarx, JFrog XRay, Blackduck, others)\nthe skills to conduct various performance tests (e.g., load, spike, breakpoint, endurance) in order to assess if the design and implementation meets the non-functional requirements.\nproven history of designing enterprise-grade applications utilizing Domain Driven Design principles, microservices, event driven systems, 12 factor principles for cloud native applications, REST and GRPC API design, CAP Theorem, etc.\na good understanding of distributed systems design and implementation.\nthe ability to perform debugging and troubleshooting to analyze core, heap, thread dumps and remove coding errors.\nthe proficiency in Software Development Best Practices (TDD/BDD, Unit test, Continuous Integration and Delivery)\nthe skills to orchestrate release workflows and pipelines and apply standardized pipelines via APIs to achieve CI and CD using industry-standard tools (e.g., Jenkins, AWS/Azure pipelines, XL Release, Ansible, others)\npractitioner of automation/configuration management, deployment strategies (blue/green, canary, A/B testing, feature flags).\nexperience and comprehensive understanding of software/application observability (logging, tracing, metrics, etc), and health and liveliness features, which enable improved R&A and operational monitoring.\nvery good analytical and problem solving skills.\nthe desire to stay abreast of advances in software engineering practices, technologies, and tooling.\nvery good inter-personal skills and ability to work in a collaborative environment\neffective communication and interpersonal skills, with an ability to express design ideas to a development team and senior management.\n\nNice to have:\nExperience in building and maintaining critical national infrastructure.\nExperience working in payments industry.\nGood understanding of private and public cloud eco systems.\nWe currently operate a hybrid working pattern which allows colleagues to gain a work life balance with the requriement to work from the office and home for part of the week.\nFlexible to working patterns that enable colaboration and engagement with other teams/individuals that are located in differnet global time zones, as required.\n\nThe Real Time Payments team is looking for a Senior Software Engineer with proven experience in a squad, including coaching and mentoring.\n\nA self starter who is able to take understand and take control of code without much need for knowledge transfer/training.\nSelf sufficient in delivery and be able to progress on work areas with minimal supervision\nown complex problems having dependency across services and facilitate cross-functional team interactions to drive resolution.\nbe responsible for the analysis, design, development and delivery of software solutions.\nwrite code to build and enhance applications/services and promote code-reviews, secure code, whilst adhering to standards and best practices resulting in the deliver high-quality artifacts through to production.\ndefine, design, and develop procedures and solutions at a service level to meet the business requirements/enhancements.\nautomate and simplify all aspects of software delivery and development actively evangelizing the need to automate and simplify where needed.\ndrive seamless integration across all connected services to meet end-user expectations.\npromote a blameless postmortem culture to identify root causes of incidents and implement learnings.\nadvocate for engineering principles outside of current organization/platform.\nbuild relationships and effective partnerships across organizations.\nconduct technical interviews for hiring engineering staff and raising the performance bar.\narchitect monitoring, logging, and alerting to provide end-to-end observability.\n\nCorporate Security Responsibility\nEvery person working for, or on behalf of, Mastercard is responsible for information security. All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and therefore, it is expected that the successful candidate for this position must:\nabide by Mastercard s security policies and practices;\nensure the confidentiality and integrity of the information being accessed;\nreport any suspected information security violation or breach, and\ncomplete all periodic mandatory security trainings in accordance with Mastercard s guidelines.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data migration', 'Coding', 'Information security', 'Configuration management', 'Debugging', 'Javascript', 'Troubleshooting', 'IPS', 'Monitoring', 'SQL']",2025-06-13 05:48:02
Functional Analyst Finance and Controlling Product Ownership,Atlas Copco,3 - 7 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Your Role Context and Mission Within the Group IT Services organization of Atlas Copco and within the SAP Competence Center, the act! organization ( Atlas Copco Together ) ensures that all customer centers for the business areas Compressor Technique, Power Technique and Industrial Tools in the different countries can rely on a legally compliant, performant, stable and well supported business application. act! is used on a global scale covering all time zones. act! secures efficient Customer Center end to end processes governed by business councils. Together with these business councils, act! steers the digital roadmap of the CC s and makes sure to realize this roadmap according to the right quality and cost. act! enables efficient and effective integrations to support reorganizations, mergers & acquisitions roadmaps of the business areas in a standardized group solution act! is part of the SAP Competence center and provides global implementation experience within the Group The SAP ECC on Hana platform (EHP 8 with simple finance add on) is the core platform to enable this standardization, using the SD, MM, PS, PP, CS and FICO as the core SAP modules. It also includes SAP GTS for trade compliance purposes and SAP BRIM to support our rental business , SAP B/4 Hana (for reporting) and other platforms like Esker, MS Power Platform (workflows), Montova (electronic communication). The platform is also integrated with different group and business area specific applications using SAP PO, SAP BTP, Azure and KAFKA as integration platforms. The role Within act!, we are looking for a SAP Finance and Controlling analyst with several years of proven SAP FICO experience, with a good functional (finance, controlling, and integration processes towards logistics, production and service), technical, data and reporting affinity and background . You will be part of a cross functional team responsible to deliver end to end consistent processes and solutions for the involved process area in line with business council requests. The team is as much as possible self-reliant, organized with agile or project principles depending on the scope and timeline. The act functional analyst provides the necessary knowledge and leadership to guide and assist our internal and external stakeholders in the involved knowledge area, and to drive and perform the delivery cycle of the assigned change requests. You can manage the delivery of your assigned scope as a project manager, taking up responsibility for in time, in full and in budget delivery. Bigger projects might be assign upon proven seniority. You re a member of a hive/project, reporting to your appointed manager, and steered by the responsible product owner, solution architect and scrum master. You will be part of a community with peer solution architects, functional and technical analysts where you will share best practices in alignment with the overall act! strategy. To succeed, you will need What we can expect from you Roles & Responsibilities Analyze and Understand Business and technical finance and controlling requirements: you will be responsible for understanding the business processes of the organization and support the identification of areas where the act! reporting systems can be utilized to build reports. This involves gathering requirements from stakeholders, conducting workshops, and finetuning business requirements and create based on this solution designs and builds up to handover to support. Full Lifecycle Implementation and Support: The act! SAP Finance and Controlling analyst will be responsible for end-to-end delivery starting from requirement gathering, analysis, solution design, developing solutions according to platform and architecture guidelines, testing, providing support in User acceptance testing and hyper care. Additionally, you will be an escalation point of contact to provide support to end-users, troubleshoot system issues. Documentation and Training: you will be responsible for documenting the technical specifications, testing results and End-to-end solution documentation. You will also be responsible for creating and delivering input for training materials for end-users and do the handover to the involved support organizations. Continuous Improvement: you will be responsible for continuously improving the SAP reporting system by identifying opportunities to streamline your solution, introduce housekeeping activities to improve system performance and functionality, and providing recommendations for future enhancements. Competences Candidate should have proven experience in a business/customer facing role. Candidate should have experience in writing Functional Specifications independently and should have worked on Custom Objects build from Scratch to Deployments Should have experience in at least 2-3 end to end Implementation including transactional Data migration and reconciliation You have experience with solution and can effectively communicate ideas and recommendations orally and in writing, while considering the viewpoint of others. A customer centric individual who understands customers needs and seeks to fulfill or exceed expectations by advising and providing adequate options and solutions. A team player and natural diplomat who interacts and unites team members, customers, all stakeholders. A methodic and structured achiever, who can plan, organize, prioritize, assess, adapt and deliver the promise. A resilient person who can cope with change in an ever faster moving digital landscape and who can spread this attitude of self-sufficiency. An innovative ""there is always a better way"" person with a positive, flexible, and responsive mindset who embraces a quality and can do attitude and promotes digital transformation. Open-minded with a global mindset, curious to understand and learn new perspectives. A person who complies with our DNA => Commitment - Interaction - Innovation. Technology Experience 5+ Years of IS/IT, reporting and related processes background. General knowledge of SAP ERP Platforms. Knowledge in SAP FICO: General finance modules (AR/AP/NewGL, AA, Banking, ..) and Controlling (COPA, COPC, ) Experience with PowerBI and other reporting solutions are a plus Experience with S/4HANA is a plus Ability to work in a structured way and effectively communicate with employees at all levels. Project Management experience is a plus, specifically the Agile methodology. The ability to motivate people. Good spoken and written communication skills with colleagues at all levels. Showing a responsible attitude. The ability to plan and prioritize your own work and having good coordination with the rest of the team, including close cooperation with offshore resources and the support team. Comfortable working under pressure to tight deadlines. Education You hold a master s degree in IT, Business economics or equivalent experience. You have excellent organizational and planning skills and strong analytical abilities. You have a positive attitude and good presentation skills to communicate complex technical information into understandable business language. You are familiar with the principles of Agile methodology. You communicate effectively in English and are prepared to make short trips abroad when needed. In return, we offer you What you can expect from us A challenging process and SAP centric landscape with room for innovation A friendly, family-like atmosphere Plenty of opportunities to grow and develop A culture known for respectful interaction, ethical behavior and integrity An organization that uses diversity as a driver of performance Potential to see your ideas realized and to make an impact New challenges and new things to learn every day Ability for work from home (flexibility can be offered depending on the assigned tasks)",,,,"['IT services', 'Data migration', 'SAP ERP', 'Project management', 'Analytical', 'GTS', 'Scrum', 'Agile methodology', 'User acceptance testing', 'Logistics']",2025-06-13 05:48:04
Programmer/Analyst 5,Lam Research,10 - 17 years,Not Disclosed,['Bengaluru'],"The Group You ll Be A Part Of\nLAM HR-Applications team is looking for a passionate, engaging Sr HR Applications Architect to join our growing team. This role will perform Technology evaluation, Identification, Solution Design, Execute the design for entire stack of HR-Applications echo-system and perform Production Support.\nThe Global Information Systems Group is dedicated to the success of Lam through providing best-in-class and innovative information system solutions and services. Together, we support users globally with data, information, and systems to achieve their business objectives.\nThe Impact You ll Make\nDesigns, develops, modifies, debugs and evaluates programs for functional areas, including but not limited to finance, human resources, manufacturing and marketing. Analyzes existing programs or formulates logic for new systems, devises logic procedures, prepares flowcharting, performs coding and tests/debugs programs. Develops conversion and system implementation plans. Prepares and obtains approval of system and programming documentation. Recommends changes in development, maintenance and system standards. Trains users in conversion and implementation of system. May be internal or external, client-focused, working in conjunction with Professional Services and outsourcing functions. May include company-wide, web-enabled solutions.\nWhat You ll Do\nLead design and implementation of the HR systems of the organization across HR technologies\nInterface with business stakeholders, assess feasibility of the requirements and guide the Technology Leads and Implementation teams to align the solution development\nFront-run the transformation and migration initiative in HR Applications COE ensuring a scalable solution to accommodate future enhancements and adoption to all BU s of Lam\nExplore new technologies and practices, be a part of the core team building an HR COE and define the standards and best practices\nAct as a SPOC/L3 for the current product support related activities and the\nHR-Echo System\nCross- training teams on knowledge transfer across business functions\nWho We re Looking For\nExcellent grasp of HR systems (both SAAS & On-Premises) technical and functional\nProven experience leading System Transformation, Integrations, Data Migrations, Implementations, Assessments and Process re-engineering.\n14+ years of experience as an HR Applications Architect\nVery strong communication and collaboration skills\nFlexible to travel and work hours.\nPreferred Qualifications",Industry Type: Electronic Components / Semiconductors,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Architect', 'Production support', 'Coding', 'Flex', 'Manager Technology', 'Process re-engineering', 'HR', 'Outsourcing', 'Product support', 'Team building']",2025-06-13 05:48:06
Application Database Administrator-NoSQL Cloud Databases,IBM,3 - 7 years,Not Disclosed,['Mumbai'],"As Consultant, you are responsible to develop design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new mobile solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nConfigure Datastax Cassandra as per requirement of the project solution\nDesign the database system specific to Cassandra in consultation with the data modelers, data architects and etl specialists as well as the microservices/ functional specialists. Thereby produce an effective database system in Cassandra according to the solution & client's needs and specifications.\nInterface with functional & data teams to ensure the integrations with other functional and data systems are working correctly and as designed. Participate in responsible or supporting roles in different tests or UAT that involve the DataStax Cassandra database.\nThe role will also need to ensure that the Cassandra database is performing and error free. This will involve troubleshooting errors and performance issues and resolution of the same as well as plan for further database improvement.\nEnsure the database documentation & operation manual is up to date and usable\n\n\nPreferred technical and professional experience\nHas expertise, experience and deep knowledge in the configuration, design, troubleshooting of NoSQL server software and related products on Cloud, specifically DataStax Cassandra.\nHas knowledge/ experience in other NoSQl/ Cloud database.\nInstalls, configures and upgrades RDBMS or NoSQL server software and related products on Cloud",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['rdbms', 'datastax cassandra', 'nosql', 'cassandra', 'troubleshooting', 'cloud server', 'python', 'database mirroring', 'sql server dba', 'data architecture', 'database administration', 'sql server', 'microservices', 'high availability', 'solution design', 'log shipping', 'agile', 'etl']",2025-06-13 05:48:08
S&C Global Network - AI - CDP - Marketing Analytics - Consultant,Accenture,8 - 10 years,Not Disclosed,['Gurugram'],"Job Title -\n\n\n\nS&C Global Network - AI - CDP - Marketing Analytics - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nGurugram, DDC1A, NonSTPI\n\n\n\nMust-have skills:Data Analytics\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\n\nAs part of our Analytics practice, you will join a worldwide network of over 20k+ smart and driven colleagues experienced in leading AI/ML/Statistical tools, methods and applications. From data to analytics and insights to actions, our forward-thinking consultants provide analytically-informed, issue-based insights at scale to help our clients improve outcomes and achieve high performance.\n\n\n\nWhat you would do in this role\n\nA Consultant/Manager for Customer Data Platforms serves as the day-to-day marketing technology point of contact and helps our clients get value out of their investment into a Customer Data Platform (CDP) by developing a strategic roadmap focused on personalized activation. You will be working with a multidisciplinary team of Solution Architects, Data Engineers, Data Scientists, and Digital Marketers.\n\n\n\nKey Duties and Responsibilities:\nBe a platform expert in one or more leading CDP solutions. Developer level expertise on Lytics, Segment, Adobe Experience Platform, Amperity, Tealium, Treasure Data etc. Including custom build CDPs\nDeep developer level expertise for real time even tracking for web analytics e.g., Google Tag Manager, Adobe Launch etc.\nProvide deep domain expertise in our clients business and broad knowledge of digital marketing together with a Marketing Strategist industry\nDeep expert level knowledge of GA360/GA4, Adobe Analytics, Google Ads, DV360, Campaign Manager, Facebook Ads Manager, The Trading desk etc.\nAssess and audit the current state of a clients marketing technology stack (MarTech) including data infrastructure, ad platforms and data security policies together with a solutions architect.\nConduct stakeholder interviews and gather business requirements\nTranslate business requirements into BRDs, CDP customer analytics use cases, structure technical solution\nPrioritize CDP use cases together with the client.\nCreate a strategic CDP roadmap focused on data driven marketing activation.\nWork with the Solution Architect to strategize, architect, and document a scalable CDP implementation, tailored to the clients needs.\nProvide hands-on support and platform training for our clients.\nData processing, data engineer and data schema/models expertise for CDPs to work on data models, unification logic etc.\nWork with Business Analysts, Data Architects, Technical Architects, DBAs to achieve project objectives - delivery dates, quality objectives etc.\nBusiness intelligence expertise for insights, actionable recommendations.\nProject management expertise for sprint planning\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nExperience with A/B testing tools is a plus.\nMust have programming experience in PySpark, Python, Shell Scripts.\nRDBMS, TSQL, NoSQL experience is must.\nManage large volumes of structured and unstructured data, extract & clean data to make it amenable for analysis.\nExperience in deployment and operationalizing the code is an added advantage.\nExperience with source control systems such as Git, Bitbucket, and Jenkins build and continuous integration tools.\nProficient in Excel, MS word, PowerPoint, etc\n\n\n\nTechnical\n\n\n\n\nSkills:\n\nAny CDP platforms experience e.g., Lytics CDP platform developer, or/and\nSegment CDP platform developer, or/and\nAdobe Experience Platform (Real time CDP) developer, or/and\nCustom CDP developer on any cloud\nGA4/GA360, or/and Adobe Analytics\nGoogle Tag Manager, and/or Adobe Launch, and/or any Tag Manager Tool\nGoogle Ads, DV360, Campaign Manager, Facebook Ads Manager, The Trading desk etc.\nDeep Cloud experiecne (GCP, AWS, Azure)\nAdvance level Python, SQL, Shell Scripting experience\nData Migration, DevOps, MLOps, Terraform Script programmer\n\n\n\nSoft\n\n\n\n\nSkills:\n\nStrong problem solving skills\nGood team player\nAttention to details\nGood communication skills\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:\n\n\n\n8-10Years\n\n\n\n\nEducational Qualification:\n\n\n\nAny Degree",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['digital marketing', 'python', 'data analytics', 'pyspark', 'campaign management', 'trading', 'adobe analytics', 'rdbms', 'adobe', 'microsoft azure', 'display video', 't-sql', 'google ads', 'nosql', 'sql', 'git', 'gcp', 'devops', 'jenkins', 'segmentation', 'facebook ads manager', 'shell scripting', 'aws']",2025-06-13 05:48:10
Application Support Engineer,Accenture,5 - 10 years,Not Disclosed,['Hyderabad'],"Project Role :Application Support Engineer\n\n\n\n\n\nProject Role Description :Act as software detectives, provide a dynamic service identifying and solving issues within multiple components of critical business systems.\n\n\n\nMust have skills :SAP Master Data Governance MDG Tool\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :Any Degree\n\n\nSummary:As an Application Developer for Packaged Application Development, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using SAP Master Data Governance MDG Tool. Your typical day will involve collaborating with cross-functional teams, analyzing business requirements, and developing solutions to meet those requirements.\nRoles & Responsibilities:- Design, build, and configure applications using SAP Master Data Governance MDG Tool to meet business process and application requirements.- Collaborate with cross-functional teams to analyze business requirements and develop solutions to meet those requirements.- Develop and maintain technical documentation related to the application development process.- Provide technical support and troubleshooting for applications developed using SAP Master Data Governance MDG Tool.\nProfessional & Technical\n\n\n\n\nSkills:\n-SAP MDG projects following industry best practices involving Data Modelling, Process Modelling, UI Modelling, Business Validation Rules Modelling, Derivations, and Data Replication Framework (DRF).Expert level skill & hands-on experience in interface development including IDOC, Web services and Custom proxy interfaces.Knowledge of SAP data dictionary tables, views, relationships, and corresponding data architecture for ECC and S/4 HANA for various SAP master and transactional data entities including excellent functional knowledge for core master data objects like a customer, vendor, and material.Hands-on experience in configuring customer, vendor, and product/material master data in MDG including data harmonization involving de-duplication, mass changes, and data replication involving Key/Value mapping, SOA Web services, ALE/Idoc.-Hands-on experience in handling custom data model extensions including the UI -Hands-on experience in web service enhancements -Good to have experience in CMP process enhancements based on the custom requirements.Experience:Have a minimum of 7.5 years in SAP Master Data Governance (MDG) with at least 2 full cycle implementations.Implementation experience of SAP MDG in key domains such as Customer, Supplier, Material, Business Partners.Hands-on experience with SAP Fiori.Experience in SAP MDG mass processing, consolidation, central governanceExcellent understanding of Data Migration concepts and strategiesExperience in data migration using SAP and non-SAP solutions\n\nQualification\n\nAny Degree",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['integrated planning', 'data migration', 'mdg', 'sap master data governance', 'sap mdg', 'sap', 'application development', 'sap s hana', 'technical support', 'ui', 'data modeling', 'sap fiori', 'application support', 'soap web services', 'troubleshooting', 'technical documentation', 'sap abap']",2025-06-13 05:48:12
Application Developer,Accenture,7 - 12 years,Not Disclosed,['Bhubaneswar'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Stibo Product Master Data Management\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will be involved in designing, building, and configuring applications to meet business process and application requirements. Your typical day will revolve around creating solutions that align with business needs and application specifications.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the team in implementing innovative solutions- Conduct regular team meetings to ensure project progress- Stay updated on industry trends and technologies\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Stibo Product Master Data Management- Strong understanding of data modeling and data architecture- Experience in data integration and data migration- Hands-on experience in application development and customization- Knowledge of data governance and data quality management\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Stibo Product Master Data Management- This position is based at our Bhubaneswar office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'data architecture', 'master data management', 'data modeling', 'data integration', 'python', 'data analysis', 'data analytics', 'data migration', 'data quality management', 'application development', 'sql', 'tableau', 'data governance', 'data visualization']",2025-06-13 05:48:14
Application Developer-Oracle Cloud Middleware,IBM,4 - 8 years,Not Disclosed,['Kolkata'],"As a Software Developer you'll participate in many aspects of the software development lifecycle, such as design, code implementation, testing, and support. You will create software that enables your clients' hybrid-cloud and AI journeys.\n\nYour primary responsibilities include\nComprehensive Feature Development and Issue ResolutionWorking on the end to end feature development and solving challenges faced in the implementation.\nStakeholder Collaboration and Issue ResolutionCollaborate with key stakeholders, internal and external, to understand the problems, issues with the product and features and solve the issues as per SLAs defined.\nContinuous Learning and Technology IntegrationBeing eager to learn new technologies and implementing the same in feature development\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nShould have minimum 3 or more years of relevant experience in ODI(Oracle Database Integrator) 12c Development and Implementation.\nShould have good knowledge of integrating with Web Services, XML(Extensible Markup Language) and other API(Application Programming Interface) to transfer the data - from source and target, in addition to database.\nShould have hands on experience in complex data migration between heterogeneous large complex databases (Oracle database is must\n\n\nPreferred technical and professional experience\nExposure in risks management and resolving issues that affect release scope.\nAbility to maintain quality and bring potential solutions to the table",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['web services', '12c', 'oracle', 'odi', 'oracle database', 'integrator', 'data migration', 'jasper reports', 'sql', 'plsql', 'xml', 'html', 'wms', 'middleware', 'python', 'c', 'software testing', 'jda', 'flash animation', 'red prairie', 'warehouse management system', 'moca', 'groovy', 'hybrid cloud', 'aws', 'unix']",2025-06-13 05:48:16
Package Consultant-Oracle Business Analytics,IBM,2 - 7 years,Not Disclosed,['Kolkata'],"Should have minimum 3 or more years of relevant experience in FDI\nShould have good knowledge of integrating with Web Services, XML (Extensible Markup Language) and other API (Application Programming Interface) to transfer the data from source and target, in addition to database.\nShould have hands on experience in complex data migration between heterogeneous large complex databases (Oracle database is must)\nBeing eager to learn new technologies and implementing the same in feature development\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nShould have minimum 3 or more years of relevant experience in FDI\nShould have good knowledge of integrating with Web Services, XML (Extensible Markup Language) and other API (Application Programming Interface) to transfer the data from source and target, in addition to database.\nShould have hands on experience in complex data migration between heterogeneous large complex databases (Oracle database is must)\nBeing eager to learn new technologies and implementing the same in feature development\n\n\nPreferred technical and professional experience\nExposure in risks management and resolving issues that affect release scope.\nAbility to maintain quality and bring potential solutions to the table",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['oracle', 'web services', 'fdi', 'oracle database', 'xml', 'companies act', 'due diligence', 'fema matters', 'company law', 'business analytics', 'sebi', 'corporate law', 'mergers', 'sql', 'drafting', 'listing agreement', 'odi', 'secretarial activities', 'rbi', 'corporate governance', 'fema', 'stock exchange']",2025-06-13 05:48:18
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Microsoft Dynamics 365 ERP Technical\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for overseeing the entire application development process and ensuring its successful implementation. Your role will involve collaborating with cross-functional teams, managing the project, and providing technical expertise to drive the application development forward.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the effort to design, build, and configure applications.- Act as the primary point of contact for the application development project.- Collaborate with cross-functional teams to ensure successful implementation.- Manage the project and ensure timely delivery.- Provide technical expertise and guidance to the team.- Identify and resolve any issues or challenges that arise during the development process.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Dynamics 365 ERP Technical.- Good To Have\n\n\n\n\nSkills:\nExperience with other ERP systems such as SAP or Oracle.- Strong understanding of the Microsoft Dynamics 365 ERP platform.- Experience in designing and implementing customizations and extensions.- Knowledge of integration capabilities and best practices.- Familiarity with data migration and data integration processes.- Ability to troubleshoot and resolve technical issues.- Excellent problem-solving and analytical skills.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Microsoft Dynamics 365 ERP Technical.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data migration', 'erp', 'sap', 'oracle', 'microsoft dynamics', 'ms dynamics crm', 'sql', 'plsql', 'salesforce', 'microsoft dynamics ax', 'java', 'asp.net', 'ssrs', 'html', 'c#', 'project management', 'asp.net mvc', 'microsoft azure', 'vb', 'sql server', 'javascript', 'application development', 'ssis', 'aws', 'unix']",2025-06-13 05:48:20
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Microsoft Dynamics CRM Technical\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your day will involve overseeing the application development process and ensuring seamless communication within the team and stakeholders.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the application development process- Ensure effective communication within the team and stakeholders\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Dynamics CRM Technical- Strong understanding of CRM systems- Experience in customizing and configuring Microsoft Dynamics CRM- Knowledge of CRM development best practices- Hands-on experience in CRM integration and data migration\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Microsoft Dynamics CRM Technical- This position is based at our Ahmedabad office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['crm systems', 'ms dynamics crm', 'application development', 'microsoft dynamics crm technical', 'crm', 'c#', 'oracle', 'web services', 'plugins', 'microsoft dynamics', 'data migration', 'javascript', 'sql server', 'sql', 'plsql', 'jscript', 'salesforce', 'cobol', 'asp.net', 'ssrs', 'wcf', 'ssis', 'unix']",2025-06-13 05:48:22
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Syniti ADM for SAP\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your day will involve overseeing the application development process and ensuring successful project delivery.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the application development process- Ensure successful project delivery- Mentor junior team members\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Syniti ADM for SAP- Good To Have\n\n\n\n\nSkills:\nExperience with SAP HANA DB Administration, SAP BusinessObjects Data Services, SAP Legacy System Migration Workbench- Strong understanding of SAP data migration tools- Experience in SAP application development- Knowledge of SAP data management best practices\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Syniti ADM for SAP- This position is based at our Pune office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap data migration', 'sap', 'adm', 'sap application', 'sap hana', 'data management', 'legacy', 'oracle', 'database administration', 'sap business objects data services', 'sql server', 'sql', 'plsql', 'java', 'oracle dba', 'db administration', 'linux', 'project delivery', 'hana db', 'mysql', 'ssis', 'unix']",2025-06-13 05:48:24
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP Master Data Governance MDG Tool\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for overseeing the application development process and ensuring successful project delivery.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the application development process- Ensure successful project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Master Data Governance MDG Tool- Strong understanding of data governance principles- Experience in configuring and customizing SAP MDG Tool- Knowledge of SAP data models and structures- Hands-on experience in data migration and data quality management- ABAP hands on mandatory.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in SAP Master Data Governance MDG Tool- This position is based at our Hyderabad office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data migration', 'mdg', 'sap master data governance', 'data governance', 'abap', 'webdynpro', 'sap', 'data quality management', 'webdynpro abap', 'application development', 'sql', 'odata', 'idocs', 'data modeling', 'sap mdm', 'project delivery', 'sap abap', 'sap hana', 'sap workflow', 'oo abap', 'sap mdg']",2025-06-13 05:48:26
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Bhubaneswar'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various teams to ensure that application development aligns with business objectives, overseeing project timelines, and facilitating communication among stakeholders to drive project success. You will also engage in problem-solving activities, ensuring that the applications meet the required standards and specifications while fostering a collaborative environment for your team members.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge.- Continuously assess and improve application performance and user experience.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data integration and ETL processes.- Experience with cloud computing platforms and services.- Familiarity with data governance and compliance standards.- Ability to work with large datasets and perform data analysis.\nAdditional Information:- The candidate should have minimum 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bhubaneswar office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'data governance', 'etl', 'data integration', 'etl process', 'sql development', 'python', 'oracle', 'datastage', 'data warehousing', 'data architecture', 'sql server', 'application development', 'sql', 'plsql', 'unix shell scripting', 'data modeling', 'ssrs', 'ssis', 'informatica', 'unix']",2025-06-13 05:48:28
Application Lead,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Spring Boot, Amazon Web Services (AWS), Microservices and Light Weight Architecture, Java Standard Edition\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your day will involve overseeing the application development process and collaborating with team members to ensure project success.\nRoles & Responsibilities:- Should be strong in core java - Must be very strong in APIs - Strong understanding of Spring Data JPA, Spring Security - Moderate knowledge of CI/CD pipelines using AWS/PCF/Openshift - Strong debugging skills -Working in Agile methodology 8 Familiar with different workflows of version control like git, bit bucket\nProfessional & Technical\n\n\n\n\nSkills:\n1. Minimum 5 years of experience in application development 2. Minimum 3 years experience developing RESTful APIs in Spring Boot, micro services 3. Experience in code coverage and fixing sonar issues is a must 4. Experience is monitoring tools like NewRelic or Dynatrace is desirable5. Collaborate with other Software Engineers, having domain expertise to build the right solution that business needs 6. Willing to learn new technologies and adaptability to change in technology/business functionality 7 Good Communication Skills 8 Ready to work in shifts -12 PM to 10 PM\nAdditional Information:- The candidate should have a minimum of 8 years of experience in Spring Boot.- This position is based at our Hyderabad office.- A 15 years full-time education is required.-\nEducational Qualification:Graduation, Any technical degree\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['spring data jpa', 'microservices', 'spring boot', 'java', 'spring security', 'continuous integration', 'rest', 'code coverage', 'new relic', 'ci/cd', 'sonar', 'java standard edition', 'application development', 'pcf', 'fix', 'light weight architecture', 'debugging', 'api', 'aws', 'ci cd pipeline']",2025-06-13 05:48:29
Application Lead,Accenture,3 - 8 years,Not Disclosed,['Coimbatore'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Spring Boot, Amazon Web Services (AWS), Java Standard Edition, Microservices and Light Weight Architecture\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your day will involve overseeing the application development process and collaborating with team members to ensure project success.\nRoles & Responsibilities:- Should be strong in core java - Must be very strong in APIs - Strong understanding of Spring Data JPA, Spring Security - Moderate knowledge of CI/CD pipelines using AWS/PCF/Openshift - Strong debugging skills -Working in Agile methodology 8 Familiar with different workflows of version control like git, bit bucket\nProfessional & Technical\n\n\n\n\nSkills:\n1.Minimum 5 years of experience in application development 2.Minimum 3 years experience developing RESTful APIs in Spring Boot, micro services 3.Experience in code coverage and fixing sonar issues is a must 4.Experience is monitoring tools like NewRelic or Dynatrace is desirable5.Collaborate with other Software Engineers, having domain expertise to build the right solution that business needs 6.Willing to learn new technologies and adaptability to change in technology/business functionality 7 Good Communication Skills 8 Ready to work in shifts -12 PM to 10 PM\nAdditional Information:- The candidate should have a minimum of 8 years of experience in Spring Boot.- This position is based at our Hyderabad office.- A 15 years full-time education is required.-\nEducational Qualification:Graduation, Any technical degree\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['spring data jpa', 'microservices', 'spring boot', 'java', 'spring security', 'continuous integration', 'rest', 'code coverage', 'new relic', 'ci/cd', 'sonar', 'java standard edition', 'application development', 'pcf', 'fix', 'light weight architecture', 'debugging', 'api', 'aws', 'ci cd pipeline']",2025-06-13 05:48:31
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Pune'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP FI S/4HANA Accounting, SAP FI CO Finance\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various teams to ensure that application development aligns with business objectives, overseeing project timelines, and facilitating communication among stakeholders to drive project success. You will also engage in problem-solving activities, ensuring that the applications meet the required standards and specifications while fostering a collaborative environment for your team.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge.- Continuously assess and improve team processes to increase efficiency.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP FI S/4HANA Accounting, SAP FI CO Finance.- Good To Have\n\n\n\n\nSkills:\nExperience with SAP integration tools.- Strong understanding of financial reporting and compliance requirements.- Experience in application lifecycle management and project management methodologies.- Familiarity with data migration strategies and tools.\nAdditional Information:- The candidate should have minimum 5 years of experience in SAP FI S/4HANA Accounting.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap fi', 'accounting', 'sap fico', 'financial reporting', 'sap integration', 'c#', 'project management', 'sap', 'sap sd', 'sap implementation', 'sap support', 'data migration', 'vb', 'application lifecycle management', 'sql server', 'application development', 'sql', 'asp.net', 'sap mm', 'sap hana']",2025-06-13 05:48:34
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Pune'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP FI S/4HANA Accounting, SAP FI CO Finance\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various teams to ensure that application development aligns with business objectives, overseeing project timelines, and facilitating communication among stakeholders to drive successful outcomes. You will also engage in problem-solving activities, providing guidance and support to your team while ensuring adherence to best practices in application development.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior professionals to enhance their skills and knowledge.- Continuously assess and improve team processes to increase efficiency.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP FI S/4HANA Accounting, SAP FI CO Finance.- Good To Have\n\n\n\n\nSkills:\nExperience with SAP integration tools.- Strong understanding of financial accounting principles and practices.- Experience in application lifecycle management and project management methodologies.- Familiarity with data migration strategies and tools.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in SAP FI S/4HANA Accounting.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap fi', 'accounting', 'sap fico', 'financial accounting', 'sap integration', 'c#', 'project management', 'sap', 'sap sd', 'sap implementation', 'sap support', 'data migration', 'vb', 'application lifecycle management', 'sql server', 'application development', 'sap pi', 'asp.net', 'sap mm', 'sap hana']",2025-06-13 05:48:36
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Gurugram'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP BusinessObjects Data Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:Looking for BODS Admin to perform Admin activities.As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your day will involve overseeing the application development process and ensuring successful project delivery.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the application development process effectively- Ensure timely project delivery- Provide guidance and mentorship to team members\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP BusinessObjects Data Services- Strong understanding of data integration and ETL processes- Experience in designing and implementing data migration strategies- Knowledge of data quality management and data governance practices\nAdditional Information:- The candidate should have a minimum of 5 years of experience in SAP BusinessObjects Data Services- This position is based at our Indore office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data services', 'sap business objects data services', 'etl', 'data integration', 'etl process', 'sap', 'data warehousing', 'data migration', 'sap bo', 'application development', 'sql', 'plsql', 'etl tool', 'business objects', 'data governance', 'project delivery', 'sap hana', 'sap businessobjects']",2025-06-13 05:48:38
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Navi Mumbai'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP Data Migration\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various teams to ensure that application requirements are met, overseeing the development process, and providing guidance to team members. You will also engage in problem-solving activities, ensuring that the applications are aligned with business objectives and user needs, while maintaining a focus on quality and efficiency throughout the project lifecycle.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and ensure timely delivery of milestones.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Data Migration.- Strong understanding of data mapping and transformation processes.- Experience with data validation and quality assurance techniques.- Familiarity with SAP modules and integration points.- Ability to troubleshoot and resolve data migration issues effectively.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in SAP Data Migration.- This position is based in Mumbai.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap data migration', 'sap', 'data mapping', 'sql', 'process transformation', 'project management', 'lsmw', 'oracle', 'program management', 'data warehousing', 'sap bods', 'business analysis', 'data migration', 'sql server', 'plsql', 'sap data services', 'sap mm', 'etl', 'sap hana', 'sap abap']",2025-06-13 05:48:40
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Navi Mumbai'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP Master Data Migration\n\n\n\n\nGood to have skills :SAP BusinessObjects Data ServicesMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various teams to ensure that application requirements are met, overseeing the development process, and providing guidance to team members. You will also engage in problem-solving activities, ensuring that the applications are aligned with business objectives and user needs, while maintaining a focus on quality and efficiency throughout the project lifecycle.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing and training sessions to enhance team capabilities.- Monitor project progress and ensure timely delivery of milestones.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Master Data Migration.- Good To Have\n\n\n\n\nSkills:\nExperience with SAP BusinessObjects Data Services.- Strong understanding of data migration processes and best practices.- Experience with data quality assessment and cleansing techniques.- Familiarity with application design and configuration methodologies.\nAdditional Information:- The candidate should have minimum 5 years of experience in SAP Master Data Migration.- This position is based at our Mumbai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap data migration', 'sap', 'data migration', 'sap business objects data services', 'sap businessobjects', 'oracle', 'application design', 'data warehousing', 'sap bods', 'sap bo', 'sql', 'plsql', 'data quality', 'bods', 'etl tool', 'business objects', 'etl', 'sap hana', 'web intelligence', 'informatica']",2025-06-13 05:48:41
Application Lead,Accenture,3 - 8 years,Not Disclosed,['Indore'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Microsoft Dynamics CRM Technical\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for overseeing the entire application development process and ensuring its successful implementation. This role requires strong leadership skills and technical expertise in Microsoft Dynamics CRM Technical.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Lead the design, development, and implementation of Microsoft Dynamics CRM applications.- Act as the primary point of contact for all application-related inquiries and issues.- Collaborate with cross-functional teams to gather requirements and define project scope.- Provide technical guidance and mentorship to junior team members.- Ensure the successful delivery of projects within the specified timeline and budget.- Identify and mitigate risks and issues throughout the application development lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Dynamics CRM Technical.- Good To Have\n\n\n\n\nSkills:\nExperience with other CRM platforms.- Strong understanding of Microsoft Dynamics CRM architecture and customization capabilities.- Experience in designing and implementing complex CRM solutions.- Knowledge of CRM integration with other systems and applications.- Familiarity with CRM best practices and industry standards.- Excellent problem-solving and troubleshooting skills.- Strong communication and interpersonal skills.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Microsoft Dynamics CRM Technical.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ms dynamics crm', 'microsoft dynamics', 'troubleshooting', 'microsoft dynamics crm technical', 'crm', 'c#', 'asp.net mvc', 'web services', 'plugins', 'data migration', 'vb', 'sql server', 'javascript', 'sql', 'jscript', 'salesforce', 'sql server reporting services', 'ssrs', 'asp.net', 'wcf', 'ssis']",2025-06-13 05:48:43
Software Development Engineer,Accenture,15 - 20 years,Not Disclosed,['Gurugram'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :SAP Plant Maintenance (PM)\n\n\n\n\nGood to have skills :NA\n\n\n\n\nEducational Qualification :15 years of full time education\n\n\nSummary:As a Software Development Engineer, you will engage in a dynamic work environment where you will analyze, design, code, and test various components of application code across multiple clients. Your day will involve collaborating with team members to perform maintenance and enhancements, ensuring that the applications meet the evolving needs of users while adhering to best practices in software development.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and ensure timely delivery of milestones.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Plant Maintenance (PM).- Strong understanding of application development methodologies.- Experience with system integration and data migration processes.- Familiarity with troubleshooting and debugging techniques.- Ability to work with cross-functional teams to deliver comprehensive solutions.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in SAP Plant Maintenance (PM).- This position is based at our Gurugram office.- A 15 years of full time education is required.\n\nQualification\n\n15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['development methodologies', 'application development', 'debugging', 'troubleshooting', 'sap plant maintenance', 'c#', 'css', 'sap', 'software development', 'ado.net', 'data migration', 'hibernate', 'jquery', 'sql server', 'microservices', 'spring', 'system integration', 'java', 'asp.net', 'oops', '.net', 'html']",2025-06-13 05:48:45
Software Development Engineer,Accenture,15 - 20 years,Not Disclosed,['Nagpur'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :SAP HCM Payroll\n\n\n\n\nGood to have skills :NA\n\n\n\n\nEducational Qualification :15 years of full time education\n\n\nSummary:As a Software Development Engineer, you will engage in a variety of tasks that involve analyzing, designing, coding, and testing multiple components of application code across various clients. Your typical day will include collaborating with team members to ensure the successful execution of projects, addressing any issues that arise, and contributing to the overall improvement of application performance and functionality. You will also be involved in maintenance and enhancement activities, ensuring that the applications meet the evolving needs of users and stakeholders.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and ensure timely delivery of milestones.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP HCM Payroll.- Strong understanding of payroll processing and compliance regulations.- Experience with SAP HCM modules and integration with other systems.- Ability to troubleshoot and resolve issues related to payroll processing.- Familiarity with data migration and system upgrades.\nAdditional Information:- The candidate should have minimum 5 years of experience in SAP HCM Payroll.- This position is based at our Nagpur office.- A 15 years of full time education is required.\n\nQualification\n\n15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['software development', 'payroll processing', 'regulations', 'sap hcm', 'payroll', 'project management', 'operations management', 'python', 'training and development', 'team management', 'training', 'data migration', 'sql', 'hcm', 'java', 'service delivery', 'stakeholder management', 'sla management']",2025-06-13 05:48:47
Software Development Engineer,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :SAP HCM Payroll\n\n\n\n\nGood to have skills :NA\n\n\n\n\nEducational Qualification :15 years of full time education\n\n\nSummary:As a Software Development Engineer, you will engage in a variety of tasks that involve analyzing, designing, coding, and testing multiple components of application code across various clients. Your typical day will include collaborating with team members to ensure the successful execution of projects, addressing maintenance and enhancement requests, and contributing to the overall development work that supports client needs. You will be involved in problem-solving and innovation, ensuring that the applications meet the highest standards of quality and performance.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and ensure timely delivery of milestones.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP HCM Payroll.- Strong understanding of payroll processing and compliance regulations.- Experience with SAP HCM modules and integration with other systems.- Ability to troubleshoot and resolve issues related to payroll processing.- Familiarity with data migration and system upgrades.\nAdditional Information:- The candidate should have minimum 5 years of experience in SAP HCM Payroll.- This position is based at our Hyderabad office.- A 15 years of full time education is required.\n\nQualification\n\n15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['software development', 'payroll processing', 'regulations', 'sap hcm', 'payroll', 'project management', 'operations management', 'python', 'training and development', 'team management', 'training', 'data migration', 'sql', 'hcm', 'java', 'service delivery', 'stakeholder management', 'sla management']",2025-06-13 05:48:49
S&C Global Network - AI - Responsible AI - Specialist,Accenture,3 - 8 years,Not Disclosed,['Gurugram'],"Entity:- Accenture Strategy & Consulting\n\n\n\nTeam:- Strategy & Consulting Global Network\n\n\n\nPractice:- Responsible AI COE\n\n\n\nTitle:- Responsible AI Specialist/ Sr. Analyst\n\n\n\nJob location:- Bangalore/Gurgaon/Pune/ Hyderabad/Chennai/Mumbai\n\nThe rapid development of AI is creating new opportunities to improve the lives of people around the world, from business to healthcare to education. As a result, it is also raising new questions about the best way to build fairness, interpretability, privacy and security into these systems.\n\nThe Data and AI revolution is changing everything. Its everywhere transforming how we work and play. Join Accenture and help transform leading companies and communities around the world.\n\nAccenture is driving these exciting changes and bringing them to life across 40 industries in more than 120 countries. The sheer scale of our capabilities and client engagements and the way we collaborate with the ecosystem, operate, and deliver value provides an unparalleled opportunity to grow and advance.\n\nAccentures S&C Global Network Data & AI team covers the range of skills, from Strategy, Data Science, Data Architecture, AI Engineering and Visual Insights. When combined with our broader Strategy and Consulting practice, we bring a unique ability to drive end to end business change through the application of Data and AI.\n\nAt the forefront of the industry, youll help make our Responsible AI vision a reality for clients looking to better serve their customers and operate always-on enterprises. Were not just focused on increasing revenues our technologies and innovations are making millions of lives easier and more comfortable. But above all, were doing this responsibly and inclusively to make sure AI technology is used equitably and in a way that is both ethically and technically sound.\n\nJoin us and become an integral part of our global Responsible AI team with the credibility, expertise and insight clients depend on. There will never be a typical day at Accenture, but thats why people love it here. You will be working with famous brands and household names no worrying about how to explain what you do to your family again!\n\nWe are looking for experienced and motivated individuals who will be a part of the\n\n\n\nResponsible AI\n\n\n\nCentre of Excellence (COE) within Accenture to join our multi-disciplinary Responsible AI team. We are committed to help people build AI products/solutions/services in a responsible and trustworthy manner.\n\n\n\nThe ideal candidate should have a strong client-facing consulting background in data science/analytics with an ability to pick up new technologies very quickly. He/she will be passionate about understanding the impact of AI systems on people and society and will have a track record in using tools to undertake assessments such as\n\n\n\nFairness/Bias, Explainability, Model Validation and Robustness, to assess model behaviour through a Responsibility lens.\n\nBeing a part of Accenture will help you grow both professionally and personally as you help shape our thinking and approaches to Responsible AI, working alongside world-class academics, industry leaders and practitioners. Responsible AI is a key strategic priority for Accenture and were looking for the very best in the field to help us meet our ambitious goals in this space.Qualification\n\n\n\nResponsibilities:\n\nAs a client-facing in Responsible AI, you will consult with Accentures clients on how todesign & develop reliable, effective user-centered AI systems in adherence to general best practices for software systems, together with practices that address responsibility considerations unique to AI & machine learning. You will also be expected to contribute to research on how AI systems can be designed holistically with fairness, interpretability, privacy, security, safety, and robustness built in by design.\n\nAs part of our team, you will:\nBe a subject matter expert on technical aspects of Responsible AI & Data\nCollaborate with colleagues to develop best practices, frameworks, tools for scalable implementation of Responsible AI in enterprises\nConduct research on Responsible AI policies, principles, issues, risk identification, risk remediation, regulatory requirements, latest trends etc.\nBring a strong conceptual understanding of Responsible AI, principles & tools with experience of using these tools in close collaboration with internal and external stakeholders/clients\nEvaluate and implement technical best practices and tools for fairness, explainability, transparency, accountability, and other relevant aspects of Responsible AI\nDevelop a clear understanding of clients business issues to adopt the best approach to implement the Responsible AI Framework\nEstablish a consistent and collaborative presence by partnering with clients to understand the wider business goals, objectives & competitive constraints\nProvide thought leadership by publishing in public forums/conferences/blogs on Responsible AI products, research or developments\nLeading diverse and well-qualified RAI team.\n\n\n\n\nSkillset :\n\n\n\n\nEducation:- PhD / Masters / Bachelors degree in Statistics / Economics / Mathematics /Computer Science / Physics or related disciplines from Premier Colleges in India or Abroad. Specialization in Data Science.\n\n\n\nMust Have\n3 10 years of Hands-on Data science experience in solving real life complex business problems\nMinimum 1 years experience in enhancing AI systems to meet Responsible AI principles - Explainability, Fairness, Accountability, etc.\nPassionate about understanding the impact of AI systems on people and society\nHands-on experience of using techniques such as data bias testing (e.g. for under-represented groups, proxy variables, recall bias, skew etc), Explainability (e.g. SHAP values, LIME), sensitivity testing, repeatability and similar to understand model limitations.\nDemonstrated experience in writing reports that summarize analysis / assessments into simple and concise actionable points\nStrong conceptual knowledge and practical experience in the Development, Validation, and Deployment of ML/AL models such as:\nSupervised Learning - regression, classification techniques\nUnsupervised Learning clustering techniques\nRecommender Systems\nReinforcement Learning\nDeep Learning Sequence models (RNN, GRU, LSTM etc.), CNN, GAN etc\nEconometric models\nExploratory Data analysis, Hypothesis testing etc.\nComfortable in ingestion of technical whitepapers, legal policies, government regulations etc. in relation into Responsible AI and work with Academic partners to convert them into practice\nAbility to learn and develop new methods, strategies and frameworks to proactively identify potential loopholes.\nComfortable with ambiguity, believe in first principles and have the skill to transform broad ideas into action plans\nExcellent written and verbal communication skills with ability to clearly communicate ideas and results to both technical and non-technical business audience, such as senior leaders\nGood time management skills to manage day-to-day work progress and ensure timely and high-quality deliverables\nSelf-motivated with ability to work independently across multiple projects and set priorities and Strong analytical bent of mind.\n\n\n\n\nGood to have\nCloud Certifications (Azure / AWS / GCP)\nKnowledge of AWS SageMaker Clarify / Azure Responsible ML and Fairlearn SDK / GCP AI Explanations\nExperience in Chatbot Analytics, Web Crawling\nExperience in MLOps tools like MLflow or Kubeflow\nKnowledge of cybersecurity, vulnerability assessment, risk remediation etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['network data', 'data science', 'chatbot', 'aws', 'web crawling', 'switching', 'eigrp', 'load balancing', 'networking', 'bgp', 'ccnp', 'f5', 'routing', 'vlan', 'mpls', 'network security', 'ccnp routing', 'network administration', 'ospf', 'stp', 'sdwan', 'firewall', 'cisco routers', 'hsrp', 'ccna']",2025-06-13 05:48:51
Software Development Engineer,Accenture,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :Syniti ADM for SAP\n\n\n\n\nGood to have skills :SAP BusinessObjects Data Services, SAP Legacy System Migration Workbench LSMW, SAP Data & Development, AgriBusinessMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Software Development Engineer, you will analyze, design, code, and test multiple components of application code across one or more clients. You will perform maintenance, enhancements, and/or development work. Your typical day will involve analyzing requirements, designing software solutions, writing code, and conducting testing to ensure the quality of the application. You will collaborate with team members and actively participate in discussions to provide solutions to work-related problems. Your role will require you to work independently and become a subject matter expert in Syniti ADM for SAP, contributing to the success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Design and develop software solutions based on client requirements.- Perform code reviews and provide feedback to team members.- Collaborate with cross-functional teams to ensure successful project delivery.- Troubleshoot and debug application issues to ensure smooth operation.- Stay updated with the latest industry trends and technologies.- Create and maintain technical documentation for reference and knowledge sharing.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Syniti ADM for SAP.- Good To Have\n\n\n\n\nSkills:\nExperience with SAP BusinessObjects Data Services, SAP Legacy System Migration Workbench LSMW, SAP Data & Development.- Strong understanding of data migration concepts and methodologies.- Experience in analyzing and mapping data from legacy systems to SAP.- Knowledge of SAP modules and their integration with Syniti ADM.- Familiarity with SAP data structures and data models.- Ability to write efficient and optimized code in Syniti ADM for SAP.- Experience in performance tuning and optimization of Syniti ADM applications.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Syniti ADM for SAP.- This position is based at our Mumbai office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'lsmw', 'data migration', 'adm', 'data structures', 'c#', 'legacy', 'software development', 'performance tuning', 'system migration', 'sap business objects data services', 'sql server', 'sql', 'java', 'project delivery', 'troubleshooting', '.net', 'code review', 'technical documentation', 'etl']",2025-06-13 05:48:53
Software Development Engineer,Accenture,3 - 8 years,Not Disclosed,['Gurugram'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :SAP FI CO Finance\n\n\n\n\nGood to have skills :NA\n\n\n\n\nEducational Qualification :15 years of full time education\n\n\nSummary:As a Software Development Engineer, you will engage in a dynamic work environment where you will analyze, design, code, and test various components of application code across multiple clients. Your day will involve collaborating with team members to ensure the successful implementation of enhancements and maintenance tasks, while also contributing to the development of new features that meet client needs. You will be responsible for troubleshooting issues and providing solutions, ensuring that the application remains robust and efficient.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to gather requirements and translate them into technical specifications.- Conduct thorough testing and debugging of application components to ensure high-quality deliverables.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP FI CO Finance.- Strong understanding of financial processes and accounting principles.- Experience with integration of SAP modules and data migration.- Familiarity with SAP reporting tools and financial analysis.- Ability to troubleshoot and resolve issues related to SAP applications.\nAdditional Information:- The candidate should have minimum 3 years of experience in SAP FI CO Finance.- This position is based at our Gurugram office.- A 15 years of full time education is required.\n\nQualification\n\n15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['financial analysis', 'sap', 'software development', 'accounting', 'sap fico', 'c#', 'python', 'c++', 'oracle', 'data warehousing', 'data migration', 'javascript', 'sql server', 'sql', 'plsql', 'java', 'etl tool', 'debugging', 'troubleshooting', 'technical specifications', 'html', 'mysql', 'etl', 'reporting tools']",2025-06-13 05:48:55
Software Development Engineer,Accenture,15 - 20 years,Not Disclosed,['Navi Mumbai'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :SAP Sales and Distribution (SD)\n\n\n\n\nGood to have skills :NA\n\n\n\n\nEducational Qualification :15 years of full time education\n\n\nSummary:As a Software Development Engineer, you will engage in a dynamic work environment where you will analyze, design, code, and test various components of application code across multiple clients. Your day will involve collaborating with team members to perform maintenance and enhancements, ensuring that the applications meet the evolving needs of users and stakeholders. You will also be responsible for troubleshooting issues and implementing solutions that enhance the overall functionality and performance of the applications.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and ensure alignment with organizational goals.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Sales and Distribution (SD).- Strong understanding of business processes related to sales and distribution.- Experience with SAP modules integration and configuration.- Ability to troubleshoot and resolve issues within the SAP SD module.- Familiarity with data migration and system upgrade processes.\nAdditional Information:- The candidate should have minimum 5 years of experience in SAP Sales and Distribution (SD).- This position is based at our Mumbai office.- A 15 years of full time education is required.\n\nQualification\n\n15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'software development', 'sap sd', 'distribution', 'sap presales', 'project management', 'team management', 'business analysis', 'business development', 'training', 'process improvement', 'data migration', 'sales', 'strategic planning', 'service delivery', 'troubleshooting', 'sap hana']",2025-06-13 05:48:56
Software Development Engineer,Accenture,3 - 8 years,Not Disclosed,['Kolkata'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :SAP FI CO Finance\n\n\n\n\nGood to have skills :NA\n\n\n\n\nEducational Qualification :15 years of full time education\n\n\nSummary:As a Software Development Engineer, you will engage in a dynamic work environment where you will analyze, design, code, and test various components of application code across multiple clients. Your day will involve collaborating with team members to ensure the successful implementation of enhancements and maintenance tasks, while also contributing to the development of new features that meet client needs. You will be responsible for troubleshooting issues and providing solutions, ensuring that the application remains robust and efficient.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to gather requirements and translate them into technical specifications.- Conduct thorough testing and debugging of application components to ensure high-quality deliverables.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP FI CO Finance.- Strong understanding of financial processes and accounting principles.- Experience with integration of SAP modules and data migration.- Familiarity with SAP reporting tools and financial analysis.- Ability to troubleshoot and resolve issues within SAP environments.\nAdditional Information:- The candidate should have minimum 3 years of experience in SAP FI CO Finance.- This position is based at our Kolkata office.- A 15 years of full time education is required.\n\nQualification\n\n15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['financial analysis', 'sap', 'software development', 'accounting', 'sap fico', 'c#', 'python', 'c++', 'oracle', 'data warehousing', 'data migration', 'javascript', 'sql server', 'sql', 'plsql', 'java', 'etl tool', 'debugging', 'troubleshooting', 'technical specifications', 'html', 'mysql', 'etl', 'reporting tools']",2025-06-13 05:48:58
Software Development Engineer,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :SAP HCM Payroll\n\n\n\n\nGood to have skills :NA\n\n\n\n\nEducational Qualification :15 years of full time education\n\n\nSummary:As a Software Development Engineer, you will engage in a variety of tasks that involve analyzing, designing, coding, and testing multiple components of application code across various clients. Your typical day will include collaborating with team members to perform maintenance and enhancements, as well as developing new features to improve application functionality. You will also be responsible for troubleshooting issues and ensuring that the application meets the required standards of quality and performance. Your role will require you to stay updated with the latest technologies and methodologies to effectively contribute to the team's success.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Mentor junior professionals to foster their growth and development.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP HCM Payroll.- Good To Have\n\n\n\n\nSkills:\nExperience with SAP SuccessFactors.- Strong understanding of payroll processing and compliance regulations.- Experience in integrating SAP HCM with other enterprise applications.- Familiarity with data migration and system configuration in SAP HCM.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in SAP HCM Payroll.- This position is based at our Hyderabad office.- A 15 years of full time education is required.\n\nQualification\n\n15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap success factor', 'payroll processing', 'regulations', 'sap hcm', 'payroll', 'c#', 'rest', 'project management', 'software development', 'enterprise applications', 'data migration', 'hibernate', 'javascript', 'sql server', 'sql', 'spring', 'hcm', 'java', 'j2ee', 'troubleshooting', 'html', 'mysql', 'agile']",2025-06-13 05:49:00
Software Development Engineer,Accenture,5 - 10 years,Not Disclosed,['Hyderabad'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :Cloud Data Architecture\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Solutions Architect - Lead, you will analyze, design, code, and test multiple components of application code. You will perform maintenance, enhancements, and/or development work, contributing to the overall success of the projects.\nRoles & Responsibilities:Design and develop the overall architecture of our digital data platform using AWS services.Create and maintain cloud infrastructure designs and architectural diagrams. Collaborate with stakeholders to understand business requirements and translate them into scalable AWS-based solutions. Evaluate and recommend AWS technologies, services, and tools for the platform. Ensure the scalability, performance, security, and cost-effectiveness of the AWS-based platform. Lead and mentor the technical team in implementing architectural decisions and AWS best practices. Develop and maintain architectural documentation and standards for AWS implementations. Stay current with emerging AWS technologies, services, and industry trends. Optimize existing AWS infrastructure for performance and cost. Implement and manage disaster recovery and business continuity plans.\nProfessional & Technical\n\n\n\n\nSkills:\nMinimum 8 years of experience in IT architecture, with at least 5 years in a solutions architect role. Strong knowledge of AWS platform and services (e.g., EC2, S3, RDS, Lambda, API Gateway, VPC, IAM). Experience with big data technologies and data warehousing solutions on AWS (e.g., Redshift, EMR, Athena).Experience in Infrastructure as Code (e.g., CloudFormation, Terraform). Exposure to Continuous Integration/Continuous Deployment (CI/CD) pipelines. Experience in Containerization technologies (e.g., Docker, Kubernetes).Proficiency in multiple programming languages and frameworks. AWS Certified Solutions Architect - Professional certification required.\nAdditional Information:The candidate should have a minimum of 5 years of experience in solutions architect role.This position is based at our Hyderabad office.A 15 years full time education is required (Bachelor of Engineering in Electronics/Computer Science, or any related stream).\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'it architecture', 'aws stack', 'data architecture', 'aws', 'kubernetes', 'software development', 'api gateway', 'amazon redshift', 'big data technologies', 'data warehousing', 'ci/cd', 'emr', 'docker', 'amazon ec2', 'lambda expressions', 'iam', 'computer science', 'athena', 'api']",2025-06-13 05:49:02
Software Development Engineer,Accenture,3 - 8 years,Not Disclosed,['Gurugram'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :SAP FI CO Finance\n\n\n\n\nGood to have skills :NA\n\n\n\n\nEducational Qualification :15 years of full time education\n\n\nSummary:As a Software Development Engineer, you will engage in a dynamic work environment where you will analyze, design, code, and test various components of application code across multiple clients. Your day will involve collaborating with team members to ensure the successful implementation of enhancements and maintenance tasks, while also contributing to the development of new features that meet client needs. You will be responsible for troubleshooting issues and providing solutions, ensuring that the application remains robust and efficient.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist in the documentation of application processes and updates.- Collaborate with cross-functional teams to gather requirements and provide technical insights.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP FI CO Finance.- Strong understanding of financial accounting principles and practices.- Experience with integration of SAP modules and data migration.- Familiarity with reporting tools and financial analysis techniques.- Ability to troubleshoot and resolve application issues effectively.\nAdditional Information:- The candidate should have minimum 3 years of experience in SAP FI CO Finance.- This position is based at our Gurugram office.- A 15 years of full time education is required.\n\nQualification\n\n15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['financial analysis', 'accounting', 'data migration', 'sap fico', 'financial accounting', 'python', 'software development', 'oracle', 'sap module', 'data warehousing', 'javascript', 'sql server', 'sql', 'plsql', 'tableau', 'java', 'etl tool', 'troubleshooting', 'html', 'etl', 'reporting tools', 'informatica']",2025-06-13 05:49:04
Application Developer,Accenture,5 - 10 years,Not Disclosed,['Noida'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP Data Services Development\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :SAP Data Services\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. Your day will involve collaborating with teams to develop solutions and ensure applications align with business needs.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead and mentor junior professionals- Conduct regular team meetings to discuss progress and challenges- Stay updated on industry trends and technologies to enhance team performance\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Data Services Development- Strong understanding of ETL processes and data integration- Experience in designing and implementing data migration strategies- Knowledge of SAP Data Services administration and configuration- Hands-on experience in troubleshooting and resolving technical issues\nAdditional Information:- The candidate should have a minimum of 5 years of experience in SAP Data Services Development- This position is based at our Noida office- A SAP Data Services education is required\n\nQualification\n\nSAP Data Services",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap data services', 'troubleshooting', 'etl', 'data integration', 'etl process', 'python', 'sap', 'oracle', 'datastage', 'data warehousing', 'data migration', 'sql server', 'application development', 'sql', 'plsql', 'tableau', 'unix shell scripting', 'data modeling', 'ssis', 'informatica', 'unix']",2025-06-13 05:49:06
Software Development Engineer,Accenture,15 - 20 years,Not Disclosed,['Nagpur'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :Oracle CC&B Technical Architecture\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Software Development Engineer, you will analyze, design, code, and test multiple components of application code across one or more clients. You will also perform maintenance, enhancements, and/or development work in a dynamic environment.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead technical discussions and provide guidance to team members.- Conduct code reviews and ensure coding standards are met.- Identify areas for process improvement and implement solutions.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Oracle CC&B Technical Architecture.- Strong understanding of database design and optimization.- Experience in system integration and data migration.- Hands-on experience in performance tuning and troubleshooting.- Knowledge of software development lifecycle methodologies.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Oracle CC&B Technical Architecture.- This position is based at our Nagpur office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cc', 'oracle', 'performance tuning', 'database design', 'technical architecture', 'c#', 'rest', 'css', 'software development', 'data migration', 'javascript', 'jquery', 'sql', 'microservices', 'spring', 'system integration', 'java', 'asp.net', 'j2ee', 'troubleshooting', 'code review', 'html', 'mysql']",2025-06-13 05:49:08
Application Support Engineer,Accenture,15 - 20 years,Not Disclosed,['Indore'],"Project Role :Application Support Engineer\n\n\n\n\n\nProject Role Description :Act as software detectives, provide a dynamic service identifying and solving issues within multiple components of critical business systems.\n\n\n\nMust have skills :Environment Health and Safety\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time educationKey Responsibilities :1-Required EHS Expert with good expertise and work experience in Intelex tool 2-Ability to support the design and implementation of EHS technology solutions for client using Intelex 3-Developing and supporting enterprise deployments of Intelex 4-Solving design and configuration challenges to meet client requirements, training, and guiding client teams on how to best configure Intelex and support to user post-deployment.5-Design, deploy and support custom developed applications in Intelex\nTechnical Experience :1-At least five years of experience implementing Intelex Experience leading EHS software implementations, including design workshops, development, testing, data migration and post-go-live support 2-Configuration expertise in Intelex with experience in common Intelex applications such as EHS Incident Management, Audit Management, Inspections/BBS, Action Plans, Compliance Assurance, Permit Management, Management of Change and Document Control 3-A deep understanding of security settings and structure Experience4-Knowledge of programming languages including Java Script, HTML, CSS and .NET framework5-Experience with Business Intelligence (BI) platforms, such as Power BI 6-Experience in Agile project management in a across functional team\nProfessional Attributes :1-Good Communication skills 2-Should be comfortable interfacing with a client daily.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['css', 'business intelligence', 'javascript', 'html', 'agile', 'oracle', 'power bi', 'sql', 'plsql', 'production support', 'software implementation', 'technical support', 'java', 'incident management', 'application support', 'linux', 'software support', 'desktop support', '.net', 'troubleshooting', 'unix']",2025-06-13 05:49:10
Application Developer,Accenture,15 - 20 years,Not Disclosed,['Navi Mumbai'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP BusinessObjects Data Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. A typical day involves collaborating with various teams to understand their needs, developing solutions that align with business objectives, and ensuring that applications are optimized for performance and usability. You will also engage in problem-solving activities, providing support and enhancements to existing applications while maintaining a focus on quality and efficiency in your work.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and ensure timely delivery of application features.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP BusinessObjects Data Services.- Strong understanding of data integration and transformation processes.- Experience with ETL (Extract, Transform, Load) methodologies.- Familiarity with database management systems and SQL.- Ability to troubleshoot and resolve application issues effectively.-Atleast 1 end to end Data migration to S/4 HANA Using SAP BODS\nAdditional Information:- The candidate should have minimum 7.5 years of experience in SAP BusinessObjects Data Services.- This position is based in Mumbai.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap business objects data services', 'sql', 'etl', 'data integration', 'process transformation', 'data services', 'sap bi', 'webi', 'sap', 'sap bods', 'data migration', 'sap bo', 'application development', 'sap s hana', 'database management', 'business objects', 'sap hana', 'sap businessobjects']",2025-06-13 05:49:12
Software Development Engineer,Accenture,3 - 8 years,Not Disclosed,['Noida'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :Microsoft Dynamics Business Central Functional\n\n\n\n\nGood to have skills :NA\n\n\n\n\nEducational Qualification :15 years of full time education\n\n\nSummary:As a Software Development Engineer, you will analyze, design, code, and test multiple components of application code across one or more clients. You will perform maintenance, enhancements, and/or development work, contributing to the overall success of the projects.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Collaborate with team members to analyze, design, and develop software solutions.- Conduct code reviews and provide feedback to ensure code quality.- Troubleshoot and debug software applications to resolve issues.- Document software specifications and technical designs.- Stay updated with the latest technologies and trends in software development.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Dynamics Business Central Functional.- Strong understanding of business processes and workflows in Microsoft Dynamics Business Central.- Experience in configuring and customizing Microsoft Dynamics Business Central modules.- Knowledge of integration with other systems and data migration in Microsoft Dynamics Business Central.- Good To Have\n\n\n\n\nSkills:\nExperience with Microsoft Dynamics 365 Finance and Operations.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Microsoft Dynamics Business Central Functional.- This position is based at our Noida office.- A 15 years of full-time education is required.\n\nQualification\n\n15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['business central', 'software development', 'microsoft dynamics', 'technical design', 'code review', 'c#', 'python', 'microsoft dynamics navision', 'ms dynamics crm', 'data migration', 'navision', 'javascript', 'sql server', 'sql', 'java', 'microsoft dynamics nav', 'ssrs', 'troubleshooting', '.net', 'html']",2025-06-13 05:49:14
Application Developer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP S/4HANA Group Reporting\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve collaborating with team members to develop innovative solutions and enhance application functionalities.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Develop and implement software solutions to meet business requirements.- Collaborate with cross-functional teams to enhance application functionalities.- Conduct code reviews and provide technical guidance to team members.- Troubleshoot and resolve application issues in a timely manner.- Stay updated on industry trends and best practices to improve application development processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP S/4HANA Group Reporting.- Strong understanding of SAP S/4HANA architecture and functionalities.- Experience in designing and implementing SAP solutions.- Knowledge of SAP Fiori and SAP Cloud Platform.- Hands-on experience in SAP data migration and integration.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in SAP S/4HANA Group Reporting.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'application development', 'sap s hana', 'sap fiori', 'sap cloud', 'sap upgrade', 'oracle', 'software testing', 'regression testing', 'automation testing', 'manual testing', 'smoke testing', 'sql', 'java', 'selenium', 'sap basis', 'troubleshooting', 'code review', 'sap hana', 'sap abap']",2025-06-13 05:49:16
Application Developer,Accenture,15 - 20 years,Not Disclosed,['Chennai'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP FI S/4HANA Central Finance\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. A typical day involves collaborating with various teams to understand their needs, developing solutions that align with business objectives, and ensuring that applications are optimized for performance and usability. You will also engage in problem-solving activities, providing support and enhancements to existing applications while staying updated with the latest technologies and methodologies in application development.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and ensure timely delivery of application features.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP FI S/4HANA Central Finance.- Strong understanding of application development methodologies.- Experience with integration of financial processes within SAP environments.- Familiarity with data migration strategies and tools.- Ability to troubleshoot and resolve application issues effectively.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in SAP FI S/4HANA Central Finance.- This position is based at our Chennai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap fi', 'central finance', 'sap', 'development methodologies', 'application development', 'css', 'jsp', 'ado.net', 'dbms', 'data migration', 'hibernate', 'jquery', 'microservices', 'plsql', 'spring', 'java', 'asp.net', 'sap fico', 'html', 'c#', 'python', 'project management', 'c', 'sql server', 'microsoft windows']",2025-06-13 05:49:18
Application Developer,Accenture,15 - 20 years,Not Disclosed,['Pune'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Ab Initio\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. A typical day involves collaborating with various teams to understand their needs, developing innovative solutions, and ensuring that applications are aligned with business objectives. You will engage in problem-solving activities, participate in team meetings, and contribute to the overall success of projects by leveraging your expertise in application development.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge.- Continuously evaluate and improve application performance and user experience.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Ab Initio.- Strong understanding of data integration and ETL processes.- Experience with data warehousing concepts and methodologies.- Familiarity with SQL and database management systems.- Ability to troubleshoot and resolve application issues efficiently.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in Ab Initio.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ab initio', 'sql', 'etl', 'data integration', 'etl process', 'python', 'oracle', 'datastage', 'data warehousing', 'data architecture', 'business intelligence', 'sql server', 'application development', 'plsql', 'tableau', 'unix shell scripting', 'data modeling', 'hadoop', 'ssis', 'informatica', 'unix']",2025-06-13 05:49:20
Application Developer,Accenture,7 - 12 years,Not Disclosed,['Kolkata'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Workday Data Mapping & Conversions\n\n\n\n\nGood to have skills :Workday Core HCMMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. Your typical day will involve collaborating with team members to develop and implement solutions for various business needs.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead data mapping and conversion projects- Develop and maintain data integration solutions- Conduct data analysis and provide insights for decision-making\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Workday Data Mapping & Conversions- Strong understanding of Workday Core HCM- Experience with data migration and integration- Knowledge of ETL processes and tools- Hands-on experience with Workday Studio- Familiarity with Workday reporting tools\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Workday Data Mapping & Conversions- This position is based at our Kolkata office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['workday functional', 'workday', 'data mapping', 'core hcm', 'workday core hcm', 'rest', 'python', 'data analysis', 'oracle', 'web services', 'data migration', 'sql server', 'application development', 'sql', 'plsql', 'hcm', 'java', 'xml', 'xslt', 'etl', 'data integration', 'reporting tools', 'eib', 'core connector']",2025-06-13 05:49:22
Application Developer,Accenture,15 - 20 years,Not Disclosed,['Chennai'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Amazon Web Services (AWS)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:We are looking for an experienced AWS Cloud Specialist to help design, build, and maintain cloud infrastructure that supports robust and secure connectivity between AWS and Salesforce platforms. The ideal candidate will have strong practical knowledge of core AWS services, including EC2 (VMs), S3, IAM, CloudWatch, and VPC, as well as experience with advanced networking solutions like Direct Connect and NLB. This role is critical to ensuring smooth, secure integration and performance between cloud applications and Salesforce.\nRoles & Responsibilities:- Design and manage foundational AWS infrastructure components including:EC2 instances (VMs)S3 bucketsIAM roles, users, and policiesCloudWatch, CloudTrail for monitoring and auditing- Architect and implement secure AWS networking using VPC, subnets, route tables, NAT Gateways, and Network Load Balancers (NLBs).- Establish and maintain Salesforce-to-AWS connectivity using Direct Connect, VPNs, and Salesforce Private Connect.- Support Salesforce API integrations and ensure network performance for connected services.- Automate infrastructure provisioning and management using Terraform or CloudFormation.- Monitor system health, diagnose issues, and troubleshoot performance bottlenecks across AWS and Salesforce touchpoints.- Maintain clear documentation for architecture, configurations, procedures, and change management.\nProfessional & Technical\n\n\n\n\nSkills:\n- Proficiency with core AWS services, including:EC2 (virtual machines)S3 (storage)IAM (identity & access management)CloudWatch, CloudTrail- Hands-on experience with:VPC design, subnets, security groups, route tablesDirect Connect, VPN, and hybrid network setupNetwork Load Balancer (NLB) and target group configuration- Understanding of Salesforce integration methods such as Private Connect and REST APIs- Familiarity with cloud networking and security best practices- Experience with Infrastructure-as-Code tools like Terraform or CloudFormation- Strong problem-solving, collaboration, and documentation skills- AWS Certifications (e.g., Solutions Architect Associate, Advanced Networking Specialty)- Exposure to API Gateway, Lambda, and serverless integration- Knowledge of Salesforce authentication protocols (OAuth2, SAML)- Scripting experience (e.g., Python, Bash) for automation tasks- Understanding of compliance standards (e.g., GDPR, HIPAA) and cloud cost optimization strategies\nAdditional Information:- The candidate should have minimum 5 years of experience in Amazon Web Services (AWS).- This position is based at our Chennai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'aws core services', 'aws cloudformation', 'lambda expressions', 'aws', 'rest', 'api gateway', 'web services', 'api integration', 'hipaa', 'networking', 'identity access management', 'gdpr', 'salesforce', 'iam', 'saml', 'terraform', 'bash', 'api', 'oauth', 'amazon cloudwatch', 'cloud trail']",2025-06-13 05:49:23
Application Developer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Spring Boot, Amazon Web Services (AWS), Microservices and Light Weight Architecture, Java Standard Edition\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your day will involve overseeing the application development process and collaborating with team members to ensure project success.\nRoles & Responsibilities:- Should be strong in core java - Must be very strong in APIs - Strong understanding of Spring Data JPA, Spring Security - Moderate knowledge of CI/CD pipelines using AWS/PCF/Openshift - Strong debugging skills -Working in Agile methodology 8 Familiar with different workflows of version control like git, bit bucket\nProfessional & Technical\n\n\n\n\nSkills:\n1.Minimum 5 years of experience in application development 2.Minimum 3 years experience developing RESTful APIs in Spring Boot, micro services 3.Experience in code coverage and fixing sonar issues is a must 4.Experience is monitoring tools like NewRelic or Dynatrace is desirable5.Collaborate with other Software Engineers, having domain expertise to build the right solution that business needs 6.Willing to learn new technologies and adaptability to change in technology/business functionality 7 Good Communication Skills 8 Ready to work in shifts -12 PM to 10 PM\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Spring Boot.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['spring data jpa', 'microservices', 'spring boot', 'java', 'spring security', 'continuous integration', 'rest', 'code coverage', 'new relic', 'ci/cd', 'sonar', 'java standard edition', 'application development', 'pcf', 'fix', 'light weight architecture', 'debugging', 'api', 'aws', 'ci cd pipeline']",2025-06-13 05:49:25
Application Developer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Spring Boot, Amazon Web Services (AWS), Microservices and Light Weight Architecture, Java Standard Edition\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your day will involve overseeing the application development process and collaborating with team members to ensure project success.\nRoles & Responsibilities:- Should be strong in core java - Must be very strong in APIs - Strong understanding of Spring Data JPA, Spring Security - Moderate knowledge of CI/CD pipelines using AWS/PCF/Openshift - Strong debugging skills -Working in Agile methodology 8 Familiar with different workflows of version control like git, bit bucket\nProfessional & Technical\n\n\n\n\nSkills:\n1.Minimum 5 years of experience in application development 2.Minimum 3 years experience developing RESTful APIs in Spring Boot, micro services 3.Experience in code coverage and fixing sonar issues is a must 4.Experience is monitoring tools like NewRelic or Dynatrace is desirable5.Collaborate with other Software Engineers, having domain expertise to build the right solution that business needs 6.Willing to learn new technologies and adaptability to change in technology/business functionality 7 Good Communication Skills 8 Ready to work in shifts -12 PM to 10 PM\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Spring Boot.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['spring data jpa', 'microservices', 'spring boot', 'java', 'spring security', 'continuous integration', 'rest', 'code coverage', 'new relic', 'ci/cd', 'sonar', 'java standard edition', 'application development', 'pcf', 'fix', 'light weight architecture', 'debugging', 'api', 'aws', 'ci cd pipeline']",2025-06-13 05:49:27
Application Developer,Accenture,15 - 20 years,Not Disclosed,['Bhubaneswar'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP Master Data Governance MDG Tool\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. Your typical day will involve collaborating with teams to develop solutions that align with business needs and requirements.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Expected to provide solutions to problems that apply across multiple teams- Lead and mentor junior team members- Conduct regular knowledge sharing sessions within the team\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Master Data Governance MDG Tool- Strong understanding of data modeling and data governance principles- Experience in configuring and customizing SAP MDG applications- Knowledge of SAP MDG data models and workflows- Hands-on experience in data migration and data quality management\nAdditional Information:- The candidate should have a minimum of 12 years of experience in SAP Master Data Governance MDG Tool- This position is based at our Pune office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['mdg', 'sap master data governance', 'data modeling', 'data governance', 'sap mdg', 'webdynpro', 'python', 'sap', 'oracle', 'data migration', 'data quality management', 'webdynpro abap', 'sql server', 'application development', 'sql', 'odata', 'idocs', 'java', 'sap abap', 'sap hana', 'sap workflow', 'oo abap']",2025-06-13 05:49:29
Application Developer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP FI S/4HANA Central Finance\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will engage in the design, construction, and configuration of applications tailored to fulfill specific business processes and application requirements. Your typical day will involve collaborating with cross-functional teams to gather requirements, developing innovative solutions, and ensuring that applications are optimized for performance and usability. You will also participate in testing and debugging processes to ensure the highest quality of deliverables, while continuously seeking opportunities for improvement and efficiency in application development.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist in the documentation of application specifications and user guides.- Engage in continuous learning to stay updated with the latest technologies and best practices.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP FI S/4HANA Central Finance.- Strong understanding of application development methodologies.- Experience with integration of financial processes within SAP environments.- Ability to troubleshoot and resolve application issues effectively.- Familiarity with data migration and transformation processes.\nAdditional Information:- The candidate should have minimum 3 years of experience in SAP FI S/4HANA Central Finance.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap fi', 'central finance', 'sap', 'development methodologies', 'application development', 'css', 'jsp', 'ado.net', 'dbms', 'data migration', 'hibernate', 'jquery', 'microservices', 'plsql', 'spring', 'java', 'asp.net', 'debugging', 'html', 'c#', 'python', 'project management', 'c', 'sql server', 'microsoft windows']",2025-06-13 05:49:31
Application Developer,Accenture,15 - 20 years,Not Disclosed,['Chennai'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP Basis Administration\n\n\n\n\nGood to have skills :Amazon Web Services (AWS), SAP HANA DB AdministrationMinimum\n\n\n\n2 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will engage in the design, construction, and configuration of applications tailored to fulfill specific business processes and application requirements. Your typical day will involve collaborating with team members to understand project needs, developing innovative solutions, and ensuring that applications are optimized for performance and usability. You will also participate in testing and troubleshooting to ensure that the applications function as intended, contributing to the overall success of the projects you are involved in.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist in the documentation of application processes and workflows.- Engage in continuous learning to stay updated with the latest technologies and best practices.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Basis Administration.- Good To Have\n\n\n\n\nSkills:\nExperience with Amazon Web Services (AWS), SAP HANA DB Administration.- Strong understanding of system configuration and performance tuning.- Experience with application lifecycle management and deployment processes.- Familiarity with database management and optimization techniques.\nAdditional Information:- The candidate should have minimum 2 years of experience in SAP Basis Administration.- This position is based at our Chennai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['performance tuning', 'system configuration', 'hana db', 'sap basis administration', 'sap hana', 'sap', 'web services', 'application lifecycle management', 'application development', 'sql', 'database management', 'oracle dba', 'db administration', 'troubleshooting', 'aws']",2025-06-13 05:49:33
Application Lead,Accenture,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP Sales and Distribution (SD)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for ensuring the successful delivery of projects and providing technical guidance to the team. Your typical day will involve collaborating with stakeholders, analyzing requirements, designing solutions, and overseeing the development and implementation of applications.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Lead the design, development, and implementation of applications.- Collaborate with stakeholders to gather and analyze requirements.- Provide technical guidance and mentorship to the team.- Ensure the successful delivery of projects within the defined timelines.- Perform code reviews and ensure adherence to coding standards.- Troubleshoot and resolve technical issues and bugs.- Stay updated with the latest industry trends and technologies.- Identify areas for process improvement and implement best practices.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Sales and Distribution (SD).- Strong understanding of software development lifecycle and methodologies.- Experience in designing and developing applications using SAP SD.- Knowledge of integration with other SAP modules such as MM and FI.- Hands-on experience in configuring and customizing SAP SD modules.- Experience in data migration and system integration.- Good To Have\n\n\n\n\nSkills:\nExperience with SAP ABAP programming.- Familiarity with SAP S/4HANA and Fiori applications.- Knowledge of SAP SD pricing and billing processes.- Experience in performance tuning and optimization.- Excellent problem-solving and analytical skills.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in SAP Sales and Distribution (SD).- This position is based at our Mumbai office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'distribution', 'sap presales', 'sap s hana', 'software development life cycle', 'oracle', 'performance tuning', 'sap sd', 'data migration', 'sales', 'application development', 'sql', 'plsql', 'system integration', 'java', 'solution design', 'sap fiori', 'troubleshooting', 'sap abap', 'abap']",2025-06-13 05:49:35
Full Stack Engineer,Accenture,15 - 20 years,Not Disclosed,['Nagpur'],"Project Role :Full Stack Engineer\n\n\n\n\n\nProject Role Description :Responsible for developing and/or engineering the end-to-end features of a system, from user experience to backend code. Use development skills to deliver innovative solutions that help our clients improve the services they provide. Leverage new technologies that can be applied to solve challenging business problems with a cloud first and agile mindset.\n\n\n\nMust have skills :Data Modeling Techniques and Methodologies\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Full Stack Engineer, you will be responsible for developing and engineering the end-to-end features of a system, from user experience to backend code. A typical day involves collaborating with cross-functional teams to design, implement, and optimize innovative solutions that enhance client services. You will leverage new technologies and methodologies to address complex business challenges while maintaining a cloud-first and agile approach. Your role will require you to engage in problem-solving and decision-making processes that drive project success and improve overall system performance.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing and mentoring within the team to enhance overall skill levels.- Continuously evaluate and improve development processes to increase efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Modeling Techniques and Methodologies.- Strong understanding of software development life cycle and agile methodologies.- Experience with cloud technologies and services.- Proficient in programming languages such as Java, JavaScript, or Python.- Familiarity with database management systems and data architecture.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in Data Modeling Techniques and Methodologies.- This position is based at our Nagpur office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'software development life cycle', 'java', 'data modeling', 'agile methodology', 'rest', 'css', 'data architecture', 'hibernate', 'cloud technologies', 'ajax', 'javascript', 'jquery', 'spring', 'react.js', 'spring boot', 'angular', 'node.js', 'full stack', 'html', 'mysql', 'angularjs', 'mongodb']",2025-06-13 05:49:37
Application Designer,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Application Designer\n\n\n\n\n\nProject Role Description :Assist in defining requirements and designing applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP FI S/4HANA Central Finance\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Designer, you will assist in defining requirements and designing applications to meet business process and application requirements. Your typical day involves collaborating with stakeholders to understand business needs and translating them into functional design solutions.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead design discussions and provide innovative solutions- Conduct regular reviews to ensure project alignment- Mentor junior team members for skill development\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP FI S/4HANA Central Finance- Strong understanding of financial processes and integration with SAP systems- Experience in designing and implementing SAP solutions- Knowledge of SAP S/4HANA Finance modules- Hands-on experience in SAP Central Finance implementation- Expertise in SAP data migration and integration\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in SAP FI S/4HANA Central Finance- This position is based at our Hyderabad office- A 15 years full time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap data migration', 'sap fi', 'sap module', 'sap s hana', 'central finance', 'sap fico implementation', 'sap', 'lsmw', 'application design', 'fscm', 'sap bods', 'data migration', 'sap co', 'copa', 'sap finance', 'sap fiori', 'sap data services', 'sap fico', 'sap mm', 'sap controlling', 'sap hana', 'sap abap']",2025-06-13 05:49:38
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Chennai'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP Master Data Governance MDG Tool\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. A typical day involves collaborating with various teams to ensure project milestones are met, addressing any challenges that arise, and providing guidance to team members to foster a productive work environment. You will also engage in strategic discussions to align project goals with organizational objectives, ensuring that all stakeholders are informed and involved in the decision-making process.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and implement necessary adjustments to meet deadlines.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Master Data Governance MDG Tool.- Strong understanding of data governance principles and practices.- Experience with application design and configuration.- Ability to lead cross-functional teams effectively.- Familiarity with project management methodologies.\nAdditional Information:- The candidate should have minimum 5 years of experience in SAP Master Data Governance MDG Tool.- This position is based in Chennai.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'application design', 'mdg', 'sap master data governance', 'data governance', 'webdynpro', 'sap sd', 'sap implementation', 'data migration', 'webdynpro abap', 'master data management', 'odata', 'idocs', 'data modeling', 'sap mdm', 'mdm', 'sap abap', 'sap hana', 'sap workflow', 'oo abap', 'sap mdg']",2025-06-13 05:49:40
Application Developer,Accenture,15 - 20 years,Not Disclosed,['Navi Mumbai'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n18 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will be involved in designing, building, and configuring applications to meet business process and application requirements. You will play a crucial role in ensuring the smooth functioning of applications.\nRoles & Responsibilities:- Expected to be a SME with deep knowledge and experience.- Should have influencing and Advisory skills.- Engage with multiple teams and responsible for team decisions.- Expected to provide solutions to problems that apply across multiple teams.- Provide solutions to business area problems.- Lead and mentor junior professionals in the team.- Collaborate with stakeholders to gather requirements.- Conduct code reviews and ensure best practices are followed.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data architecture principles.- Experience in designing and implementing data solutions.- Knowledge of cloud platforms like AWS or Azure.- Hands-on experience with ETL processes.- Familiarity with data modeling and database design.\nAdditional Information:- The candidate should have a minimum of 18 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Mumbai office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'microsoft azure', 'data architecture principles', 'data modeling', 'aws', 'c#', 'rest', 'css', 'web services', 'javascript', 'jquery', 'application development', 'sql', 'microservices', 'database design', 'spring', 'spring boot', 'java', 'asp.net', 'j2ee', 'code review', 'html', 'mysql', 'etl']",2025-06-13 05:49:42
Application Lead,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP Master Data Governance MDG Tool\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your day will involve overseeing the application development process and ensuring successful project delivery.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the team in implementing best practices for application development- Ensure timely delivery of projects- Mentor junior team members for their professional growth\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Master Data Governance MDG Tool- Strong understanding of data governance principles- Experience in configuring and customizing SAP MDG Tool- Knowledge of SAP data models and structures- Hands-on experience in data migration and data quality management\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in SAP Master Data Governance MDG Tool- This position is based at our Pune office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'mdg', 'sap master data governance', 'data governance', 'sap mdg', 'webdynpro', 'data migration', 'data quality management', 'webdynpro abap', 'application development', 'sql', 'odata', 'idocs', 'data modeling', 'sap mdm', 'project delivery', 'sap abap', 'sap hana', 'sap workflow', 'oo abap']",2025-06-13 05:49:44
Application Lead,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Microsoft Dynamics CRM Technical\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :Any Btech degree\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute to key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact for the project- Manage the team and ensure successful project delivery- Collaborate with multiple teams to make key decisions- Provide solutions to problems for the immediate team and across multiple teams\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Dynamics CRM Technical- Good To Have\n\n\n\n\nSkills:\nExperience with Microsoft Azure- Strong understanding of Microsoft Dynamics CRM Technical- Experience in customizing and extending Microsoft Dynamics CRM- Hands-on experience in developing plugins, workflows, and custom integrations- Knowledge of Microsoft Dynamics CRM security model- Experience in data migration and integration with Microsoft Dynamics CRM- Solid grasp of Microsoft Dynamics CRM customization and configuration\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Microsoft Dynamics CRM Technical- This position is based at our Hyderabad office- A Btech degree is required\n\nQualification\n\nAny Btech degree",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ms dynamics crm', 'microsoft dynamics', 'microsoft azure', 'microsoft dynamics crm technical', 'crm', 'c#', 'team management', 'ado.net', 'data migration', 'sql server', 'javascript', 'sql', 'sql server integration services', 'ssrs', 'asp.net', 'project delivery', 'ssis']",2025-06-13 05:49:46
Application Developer,Accenture,7 - 12 years,Not Disclosed,['Gurugram'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP FI Asset Accounting\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. Your day will involve collaborating with teams to create innovative solutions and contribute to key decisions.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the team in implementing SAP FI Asset Accounting solutions- Analyze and optimize asset accounting processes- Provide technical guidance and support to team members\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP FI Asset Accounting- Strong understanding of financial accounting principles- Experience in configuring and customizing SAP FI-AA modules- Knowledge of integration with other SAP modules- Experience in data migration and system upgrades\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in SAP FI Asset Accounting- This position is based at our Gurugram office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'sap fi', 'application development', 'financial accounting', 'asset accounting', 'accounts receivable', 'general ledger accounting', 'accounts payable', 'oracle', 'data migration', 'accounting', 'general ledger', 'javascript', 'sql', 'plsql', 'java', 'bank reconciliation', 'html', 'finance']",2025-06-13 05:49:48
Application Designer,Accenture,15 - 20 years,Not Disclosed,['Navi Mumbai'],"Project Role :Application Designer\n\n\n\n\n\nProject Role Description :Assist in defining requirements and designing applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP MM Materials Management\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Designer, you will assist in defining requirements and designing applications to meet business process and application requirements. Your typical day will involve collaborating with various stakeholders to gather insights, analyzing business needs, and translating them into functional specifications. You will also engage in design discussions, ensuring that the applications align with the overall business strategy and user expectations, while continuously seeking opportunities for improvement and innovation in application design.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and ensure alignment with business objectives.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP MM Materials Management.- Strong understanding of supply chain processes and inventory management.- Experience with SAP integration and data migration.- Familiarity with application design methodologies and best practices.- Ability to analyze and optimize business processes.\nAdditional Information:- The candidate should have minimum 5 years of experience in SAP MM Materials Management.- This position is based in Mumbai.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['supply chain', 'sap mm materials management', 'sap integration', 'sap mm', 'inventory management', 'project management', 'sap project management', 'software testing', 'application design', 'asset management', 'data migration', 'sap testing', 'java', 'sap wm', 'capm', 'sap hana', 'sap mdg']",2025-06-13 05:49:50
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP EWM\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various teams to ensure that application requirements are met, overseeing the development process, and providing guidance to team members. You will also engage in problem-solving activities, ensuring that the applications are aligned with business objectives and user needs, while maintaining a focus on quality and efficiency throughout the project lifecycle.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and ensure timely delivery of milestones.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP EWM.- Good To Have\n\n\n\n\nSkills:\nExperience with SAP S/4HANA.- Strong understanding of supply chain management processes.- Experience in application integration and data migration.- Familiarity with agile methodologies and project management tools.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in SAP EWM.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'sap ewm', 'sap s hana', 'supply chain management', 'agile methodology', 'warehouse management', 'sap sd', 'sap implementation', 'data migration', 'sql', 'plsql', 'sap wm', 'sap fiori', 'sap logistics', 'sap fico', 'sap mm', 'sap warehouse management', 'agile', 'sap abap', 'sap hana', 'oo abap']",2025-06-13 05:49:53
Application Lead,Accenture,12 - 15 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Ab Initio\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various teams to ensure that project goals are met, facilitating discussions to address challenges, and guiding the team in implementing effective solutions. You will also engage in strategic planning and decision-making processes, ensuring that the applications developed align with organizational objectives and meet user needs. Your role will require you to balance technical expertise with leadership skills, fostering a collaborative environment that encourages innovation and efficiency.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate training and development opportunities for team members to enhance their skills.- Monitor project progress and ensure timely delivery of milestones.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Ab Initio.- Strong understanding of data integration and ETL processes.- Experience with performance tuning and optimization of data processing workflows.- Familiarity with data governance and data quality best practices.- Ability to troubleshoot and resolve technical issues in a timely manner.\nAdditional Information:- The candidate should have minimum 12 years of experience in Ab Initio.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ab initio', 'performance tuning', 'etl', 'data integration', 'etl process', 'sql development', 'python', 'oracle', 'data warehousing', 'data architecture', 'sql server', 'sql', 'plsql', 'data quality', 'unix shell scripting', 'data modeling', 'ssrs', 'data governance', 'ssis', 'informatica', 'unix']",2025-06-13 05:49:55
Application Designer,Accenture,7 - 12 years,Not Disclosed,['Mumbai'],"Project Role :Application Designer\n\n\n\n\n\nProject Role Description :Assist in defining requirements and designing applications to meet business process and application requirements.\n\n\n\nMust have skills :Workday Learning\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :Mandatory to have Workday Primary skill Related certification15 years full time education\n\n\nSummary:As an Application Designer, you will assist in defining requirements and designing applications to meet business process and application requirements. Your typical day will involve collaborating with stakeholders to understand their needs and translating them into functional design solutions.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead design discussions and provide innovative solutions- Conduct regular code reviews and ensure best practices are followed\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Workday Learning, Mandatory to have Workday Primary skill Related certification- Strong understanding of application design principles- Experience in translating business requirements into technical solutions- Knowledge of integration techniques and data migration strategies\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Workday Learning- This position is based at our Mumbai office- A mandatory Workday Primary skill Related certification is required\n\nQualification\n\nMandatory to have Workday Primary skill Related certification15 years full time education",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['workday', 'application design', 'spring', 'java', 'design principles', 'c#', 'rest', 'c++', 'python', 'software development', 'hibernate', 'javascript', 'sql server', 'microservices', 'cobol', 'design patterns', 'oops', 'multithreading', 'data structures', 'mes', 'mvc', 'sap hana', 'jira']",2025-06-13 05:49:57
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Spring Boot, Microservices and Light Weight Architecture, Java Standard Edition, Amazon Web Services (AWS)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for overseeing the entire application development process and ensuring its successful implementation. This role requires strong leadership skills and expertise in application development.\nRoles & Responsibilities:- Design, develop, and maintain high-quality applications using Spring Boot.- Collaborate with cross-functional teams to identify and prioritize application requirements.- Develop and maintain Microservices and Light Weight Architecture.- Integrate MongoDB with Spring Boot applications for efficient data storage and retrieval.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must Have\n\n\n\n\nSkills:\nProficiency in Java Standard Edition, Microservices and Light Weight Architecture, Amazon Web Services (AWS) and MongoDB- Resource should be good at Coding. Please conduct Coding Test.- Strong understanding of Spring Boot and its various components.- Experience with RESTful web services and API development.- Experience with database design and development.- Experience with version control systems such as Git.- Experience with agile development methodologies such as Scrum or Kanban.\nAdditional Information:- The candidate should have a minimum of 7 years of experience in Spring Boot.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering high-quality applications.- This position is based at our Hyderabad office.\nCoding test is mandatory for every resource\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['microservices', 'mongodb', 'application development', 'spring boot', 'aws', 'web services', 'jsp', 'hibernate', 'jquery', 'java standard edition', 'database design', 'spring', 'git', 'java', 'kanban', 'j2ee', 'html', 'jpa', 'api', 'rest', 'javascript', 'light weight architecture', 'struts', 'servlets', 'scrum', 'agile']",2025-06-13 05:49:59
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Spring Boot, Amazon Web Services (AWS), Oracle Procedural Language Extensions to SQL (PLSQL)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your day will involve overseeing the application development process and ensuring seamless communication within the team and stakeholders.\nRoles & Responsibilities:- Design, develop, and maintain high-quality applications using Spring Boot.- Collaborate with cross-functional teams to identify and prioritize application requirements.- Develop and maintain Microservices and Light Weight Architecture.- Integrate MongoDB with Spring Boot applications for efficient data storage and retrieval.\nProfessional & Technical\n\n\n\n\nSkills:\nMust To Have\n\n\n\n\nSkills:\nProficiency in Spring Boot, Amazon Web Services (AWS), Oracle Procedural Language Extensions to SQL (PLSQL)- Resource should be good at Coding. Please conduct Coding Test.- Strong understanding of Spring Boot and its various components.- Experience with RESTful web services and API development.- Experience with database design and development.- Experience with version control systems such as Git.- Experience with agile development methodologies such as Scrum or Kanban.- Knowledge of database technologies such as MySQL, PostgreSQL, or MongoDB.- Good to have AWS AppSync, Lambda experience.- Ready to work in shifts - 12 PM to 10 PM\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Spring Boot.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'spring boot', 'lambda expressions', 'aws', 'oracle procedural language', 'rest', 'oracle', 'web services', 'plsql', 'microservices', 'application development', 'database design', 'git', 'postgresql', 'kanban', 'oracle procedural language extensions to sql', 'scrum', 'mysql', 'agile', 'api', 'mongodb']",2025-06-13 05:50:01
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP Master Data Governance MDG Tool\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various teams to ensure that application requirements are met, overseeing the development process, and providing guidance to team members. You will also engage in problem-solving activities, ensuring that the applications are aligned with business objectives and user needs. Your role will require you to facilitate communication between stakeholders and the development team, ensuring that all parties are informed and engaged throughout the project lifecycle.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate training and knowledge sharing sessions to enhance team capabilities.- Monitor project progress and ensure timely delivery of milestones.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Master Data Governance MDG Tool.- Strong understanding of data governance principles and practices.- Experience with application design and development methodologies.- Ability to analyze and optimize application performance.- Familiarity with integration processes and tools related to SAP.\nAdditional Information:- The candidate should have minimum 5 years of experience in SAP Master Data Governance MDG Tool.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'application design', 'mdg', 'sap master data governance', 'data governance', 'webdynpro', 'sap sd', 'sap implementation', 'data migration', 'webdynpro abap', 'master data management', 'odata', 'idocs', 'data modeling', 'sap mdm', 'mdm', 'sap abap', 'sap hana', 'sap workflow', 'oo abap', 'sap mdg']",2025-06-13 05:50:03
Engagement Delivery Director (Salesforce),Salesforce,17 - 20 years,Not Disclosed,['Mumbai'],"we're Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing we'll and doing good - you've come to the right place.\n  Responsibilities:\nPlan and manage the technical delivery of multiple programs within an account using Agile techniques and frameworks\nLead and manage delivery team/s to successfully deliver all key milestones and product outcomes within scope, on time, on budget and to expected standards\nFully accountable for the delivery of work assignments on time and to expectations in terms of quality, deliverables and outcomes\nTake responsibility for delivering high quality customer focused services\nVery good understanding of engagement model ( T&M and Fixed Price) and plan end to end execution effectively by keeping scope and milestone on check\nImplement effective stakeholder engagement and communications strategy for all stages of projects\nPrepare scope and business cases for more ambiguous or complex projects including cost and resource impacts\nAnticipate and assess the impact of changes in scope and effectively manage the change request process\nManage transitions between project stages and ensure that changes are consistent with organisational and customer goals\nWorking closely with Engagement Management team on Initial scoping and Change Management\nReport and escalate issues such as variances and manage delivery by exception to ensure issues are understood and actions to resolve identified\nSupport the delivery of all governance materials, artefacts and meetings to ensure products are delivered and maintained in a transparent fashion and stored and maintained as per Organisational standards\nGuide, support, coach, provide direction, upskill team members and maintain a cohesive culture within the project team\nManage effective implementation of resource planning, on-boarding and transitioning of resources.\nEncourage a culture of Inclusiveness and diversity\nKeep abreast of new salesforce products and trends within the industry\nWork collaboratively with cross-functional teams such as MuleSoft, SI partner teams, UI/UX teams, tech PODs to contribute to achieving business outcomes\nParticipate in meetings and discussions to continuously improve delivery\nRequired Skills/Experience\nDegree or equivalent relevant experience required. Experience will be evaluated based on the core competencies for the role (eg extracurricular management roles, work experience, etc)\n17+ years experience, predominantly managing SaaS based projects. CRM and Salesforce knowledge\nGood to have: Very good understanding of Salesforce implementation involving custom development, integration and data migration\nProject management experience (Agile preferred)\nManage and develop stakeholder relationships by effective governance model and by including steering committees, through effective communications, documentation, negotiation and issues management to ensure delivery of products and the achievement of outcomes and benefits",Industry Type: Internet,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Stakeholder Engagement', 'Change management', 'Data migration', 'Focus', 'Project management', 'Engagement management', 'Agile', 'Resource planning', 'CRM', 'Salesforce']",2025-06-13 05:50:04
IN_Associate _D365 CRM Technical_MS Dynamics _Advisory_Hyderabad,PwC Service Delivery Center,2 - 4 years,Not Disclosed,['Hyderabad'],"Management Level\nAssociate\n& Summary\n\n\nThose in Microsoft Dynamics ERP at PwC will specialise in analysing client needs, implementing ERP software solutions, and offering training and support for seamless integration and utilisation of Microsoft ERP applications. This will enable clients to optimise operational efficiency and achieve their strategic objectives.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\nMicrosoft Dynamics 365 Online Application (Sales or Service) modules\nExperience in customization and extension of Dynamics 365 CRM client side customization using java scripts, web resources etc..., server side customization using C# & .Net assemblies\nJava scripting frameworks like Node.js, Angular.js, React.jsJQuery , CSS, HTML 5, AJAX\nDeveloping REST APIsIntegration experience with external systems through REST API s . Both with and without middleware.\nServer administration and Deployment for MSCRM application.\nWorking knowledge of source control tools like GIT, VSS etc.\nWorking with Power apps\nData migration experience either through a tool or through MS Dynamics feature.\nExperience with Microsoft Azure services (Application servers, Database Services, Service Bus)\nMandatory skill sets\nDynamics 365 CRM Technical\nPreferred skill sets\nNode.js, Angular.js, React.jsJQuery , CSS, HTML 5, AJAX\nYear of experience required\n24 Years\nEducational Qualification\nBE/BTech\nEducation\nDegrees/Field of Study required Bachelor of Technology, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nMicrosoft Windows 365\nNode.js\nNo",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SIDE', 'html5', 'Consulting', 'Javascript', 'Microsoft Dynamics', 'Business applications', 'Operations', 'Middleware', 'CRM', 'Ajax']",2025-06-13 05:50:06
"Manager, SAP Basis",Merck Sharp & Dohme (MSD),5 - 9 years,Not Disclosed,['Hyderabad'],"Manager SAP Basis\nThe Opportunity\nBased in Hyderabad, join a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nBe part of an organisation driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products.\nDrive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the worlds greatest health threats.\nOur Technology Centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our company s IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\nA focused group of leaders in each Tech Center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\nRole Overview\nAs a SAP Basis resource, the candidate will work with a globally diverse set of teams that includes Systems, Applications, and Products (SAP) Basis, Security, Advanced Business Application Programming (ABAP), SAP functional team members, Infrastructure team and other IT process partners providing support for existing and new initiatives.\nThe candidate will work closely with and advise the SAP Technical Architect on architectural topics and new applications / technologies to be introduced.\nThe candidate will lead some cross-functional projects, relied upon to answer complex questions, and assists with program-wide initiatives and it is expected that the candidate can lead technical initiatives and be hands-on.\nWhat will you do in this role:\nWork closely with and manage the planning / execution and delivery of activities to be performed by our service provider.\nEstablish SAP specific technical standards, technical strategy and best practices to support implementation teams by working closely with them.\nSupport product mapping exercise based on detailed analysis of requirements and produce SAP specific solution specifications.\nProvide direction to Enterprise Resource Planning (ERP) Center of Excellence teams for solution realization and candidate should be able to invite self to discussions that are relevant from a technical point of view.\nLead project teams by working in partnership with Project Managers. Review infrastructure requirements, system landscape design & Security integrations for required SAP modules.\nEvaluate SAP products offered by SAP and/or 3rd party providers, as part of ongoing and future product strategy.\nAble to work within and lead technical teams on issues resolution and performance tuning.\nInstalling, managing, patching and upgrading SAP applications. SAP client and system refreshes.\nUnderstanding of DB administration along with DB replication and HA/DR processes.\nPerform SAP system monitoring and schedule jobs.\nProvide SAP product specific guidance in integration and data architecture areas.\nWhat should you have:\nBachelors degree in Information Technology, Computer Science or any Technology stream.\n4+ years of experience implementing SAP projects with hands on Basis experience on the following: SAP S/4HANA or ECC, BW, PI, Portals, Solution Manager, System upgrades, system/client refreshes, Disaster Recovery, High Availability\nReview infrastructure requirements, system landscape design & Security integrations for required SAP modules.\nEvaluate SAP products offered by SAP and/or 3rd party providers, as part of ongoing and future product strategy.\nAble to work within and lead technical teams on issues resolution and performance tuning.\nInstalling, managing, patching and upgrading SAP applications.\nSAP client and system refreshes.\nUnderstanding of DB administration along with DB replication and HA/DR processes.\nWork closely with SAP, vendors, peer groups to ensure we are staying current with ERP strategy and technology.\nPerform SAP system monitoring and schedule jobs.\nSAP Basis experience working on SAP S/4HANA deployments on Cloud platforms (example: AWS, GCP or Azure).\nOur technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation.\nWho we are:\nWhat we look for:\n#HYDIT2025\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nHybrid\nShift:\nValid Driving License:\nHazardous Material(s):\nRequired Skills:\nEmerging Technologies, Hiring Management, Management Process, Methods and Tools, Process Management, Program Implementation, Requirements Management, SAP HCM, Software Development, Software Development Life Cycle (SDLC), Solution Architecture, Strategic Planning, System Designs, Technical Advice\nPreferred Skills:\nJob Posting End Date:\n07/9/2025\n*A job posting is effective until 11:59:59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'CVS', 'Application programming', 'Disaster recovery', 'SAP Basis', 'Healthcare', 'Information technology', 'ABAP', 'SDLC']",2025-06-13 05:50:08
Sr Software Engineer (Full Stack),Johnson Controls,4 - 7 years,Not Disclosed,['Pune'],"Job Title-Senior Software Engineer (AI Engineering)\nPosting Title- Senior Software Engineer (Full Stack - AI Data Engineering)\nJob Code/Job Profile/Job Level- 172\nPreferred Locations-\nIndia (Pune)\nIntroduction\nThe future is being built today and Johnson Controls is making that future more productive, more secure and more sustainable. We are harnessing the power of cloud, AI/ML and Data analytics, the Internet of Things (IoT), and user design thinking to deliver on the promise of intelligent buildings and smart cities that connect communities in ways that make people s lives and the world better.",,,,"['Computer science', 'Object oriented design', 'Machine learning', 'Javascript', 'Engineering Manager', 'HTML', 'Scrum', 'Open source', 'Monitoring', 'Python']",2025-06-13 05:50:10
Senior Sql Database Developer For Apptad _ Hyderabad,Apptad,8 - 13 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Job Description Summary\n\nSeeking an experienced and driven Senior Database Developer with a strong focus on SQL Server and advanced knowledge of Object-Oriented Programming (OOPs) concepts. This role requires a seasoned professional with 710 yearsâ€™ experience, adept at designing and maintaining complex database systems. The ideal candidate will also have the flexibility and willingness to work with backend languages such as Java or .NET as business needs arise.",,,,"['Sql Database Development', 'SQL Database', 'T-SQL', 'Performance Tuning']",2025-06-13 05:50:11
Vice President Senior KDB/Q Developer,Barclays,3 - 13 years,Not Disclosed,['Pune'],"Join us as a Vice President Senior Senior KDB/Q Developer at Barclays. To build and maintain the systems that collect, store, process, and analyse data, such as data pipelines, data warehouses and data lakes to ensure that all data is accurate, accessible, and secure. The ideal candidate is responsible and accountable for building and maintenance of data architectures pipelines that enable the transfer and processing of durable, complete and consistent data, design and implementation of data warehouse and data lakes that manage the appropriate data volumes and velocity and adhere to the required security measures. The candidate is also expected for the development of processing and analysis algorithms fit for the intended data complexity and volumes in collaboration with data scientist to build and deploy machine learning models. To contribute or set strategy, drive requirements and make recommendations for change.\n\nTo be successful as a Vice President Senior KDB/Q Developer, you should have experience with:\n\nExcellent Q/Kdb+ programming skills on Linux.\nDevelopment experience in front office / electronic trading systems\nStrong knowledge of agile and SDLC processes.\nExperience with Maven / Java / Git / Team City / Jira / Confluence.\n\nSome other highly valued skills may include:\n\nStrong academic record with a numerate degree (2:1 or higher) - e.g., computer science, maths, physics, engineering.\nFull-lifecycle development on at least 1 large commercial system, with ideal candidate focusing on significant development of large distributed Kdb+ systems.\nYou may be assessed on key critical skills relevant for success in role, such as risk and controls, change and transformation, business acumen, strategic thinking and digital and technology, as well as job-specific technical skill.\n\nThis role is for Pune location.\nPurpose of the role\nTo build and maintain the systems that collect, store, process, and analyse data, such as data pipelines, data warehouses and data lakes to ensure that all data is accurate, accessible, and secure.\nAccountabilities\nBuild and maintenance of data architectures pipelines that enable the transfer and processing of durable, complete and consistent data.\nDesign and implementation of data warehoused and data lakes that manage the appropriate data volumes and velocity and adhere to the required security measures.\nDevelopment of processing and analysis algorithms fit for the intended data complexity and volumes.\nCollaboration with data scientist to build and deploy machine learning models.\nVice President Expectations\nTo contribute or set strategy, drive requirements and make recommendations for change. Plan resources, budgets, and policies; manage and maintain policies/ processes; deliver continuous improvements and escalate breaches of policies/procedures..\nIf managing a team, they define jobs and responsibilities, planning for the department s future needs and operations, counselling employees on performance and contributing to employee pay decisions/changes. They may also lead a number of specialists to influence the operations of a department, in alignment with strategic as well as tactical priorities, while balancing short and long term goals and ensuring that budgets and schedules meet corporate requirements..\nIf the position has leadership responsibilities, People Leaders are expected to demonstrate a clear set of leadership behaviours to create an environment for colleagues to thrive and deliver to a consistently excellent standard. The four LEAD behaviours are: L - Listen and be authentic, E - Energise and inspire, A - Align across the enterprise, D - Develop others..\nOR for an individual contributor, they will be a subject matter expert within own discipline and will guide technical direction. They will lead collaborative, multi-year assignments and guide team members through structured assignments, identify the need for the inclusion of other areas of specialisation to complete assignments. They will train, guide and coach less experienced specialists and provide information affecting long term profits, organisational risks and strategic decisions..\nAdvise key stakeholders, including functional leadership teams and senior management on functional and cross functional areas of impact and alignment.\nManage and mitigate risks through assessment, in support of the control and governance agenda.\nDemonstrate leadership and accountability for managing risk and strengthening controls in relation to the work your team does.\nDemonstrate comprehensive understanding of the organisation functions to contribute to achieving the goals of the business.\nCollaborate with other areas of work, for business aligned support areas to keep up to speed with business activity and the business strategies.\nCreate solutions based on sophisticated analytical thought comparing and selecting complex alternatives. In-depth analysis with interpretative thinking will be required to define problems and develop innovative solutions.\nAdopt and include the outcomes of extensive research in problem solving processes.\nSeek out, build and maintain trusting relationships and partnerships with internal and external stakeholders in order to accomplish key business objectives, using influencing and negotiating skills to achieve outcomes.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Maven', 'Front office', 'electronic trading', 'Analytical', 'Machine learning', 'Vice President', 'Agile', 'Counselling', 'SDLC']",2025-06-13 05:50:13
EVSE - Sr. Software Backend Engineer,Enphase Energy,2 - 4 years,Not Disclosed,['Bengaluru'],"Description\nEnphase Energy is a global energy technology company and leading provider of solar, battery, and electric vehicle charging products. Founded in 2006, Enphase transformed the solar industry with our revolutionary microinverter technology, which turns sunlight into a safe, reliable, resilient, and scalable source of energy to power our lives. Today, the Enphase Energy System helps people make, use, save, and sell their own power. Enphase is also one of the fastest growing and innovative clean energy companies in the world, with approximately 68 million products installed across more than 145 countries.\nWe are building teams that are designing, developing, and manufacturing next-generation energy technologies and our work environment is fast-paced, fun and full of exciting new projects.\nIf you are passionate about advancing a more sustainable future, this is the perfect time to join Enphase!\nAbout the role\nAt Enphase, we think big. We re on a mission to bring solar energy to the next level, one where it s ready to meet the energy demands of an entire globe. As we work towards our vision for a solar-powered planet, we need visionary and talented people to join our team as Senior Back-End engineers.\nThe Back-End engineer will develop, maintain, architect expand cloud microservices for the EV (Electric Vehicle) Business team. Codebase uses Java, Spring Boot, Mongo, REST APIs, MySQL. Applications are dockized and hosted in AWS using a plethora of AWS services.\nWhat you will be doing\nProgramming in Java + Spring Boot\nREST API with JSON, XML etc. for data transfer\nMultiple database proficiency including SQL and NoSQL (Cassandra, MongoDB)\nAbility to develop both internal facing and external facing APIs using JWT and OAuth2.0\nFamiliar with HA/DR, scalability, performance, code optimizations\nExperience with working with highly performance and throughput systems\nAbility to define, track and deliver items to one s own schedule.\nGood organizational skills and the ability to work on more than one project at a time.\nExceptional attention to detail and good communication skills\nWho you are and what you bring\nB.E/B.Tech in Computer Science from top tier college and >70% marks\nMore than 4 years of overall Back-End development experience\nExperience with SQL + NoSQL (Preferably MongoDB)\nExperience with Amazon Web Services, JIRA, Confluence, GIT, Bitbucket etc.\nAbility to work independently and as part of a project team.\nStrong organizational skills, proactive, and accountable\nExcellent critical thinking and analytical problem-solving skills\nAbility to establish priorities and proceed with objectives without supervision.\nAbility to communicate effectively and accurately.\nclear concise written project status update throughout the project lifecycle\nHighly skilled at facilitating and documenting requirements\nExcellent facilitation, collaboration, and presentation skills\nComfort with ambiguity, frequent change, or unpredictability\nGood Practice of writing clean and scalable code\nExposure or knowledge in Renewable Tech companies\nGood understanding of cloud technologies, such as Docker, Kubernetes, EKS, Kafka, AWS Kinesis etc.\nKnowledge of NoSQL Database systems like MongoDB or CouchDB, including Graph Databases\nAbility to work in a fast-paced environment.\nExposure or knowledge in Renewable Tech companies",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['kubernetes', 'confluence', 'facilitation', 'bitbucket', 'eks', 'scalability', 'sql', 'docker', 'aws kinesis', 'java', 'git', 'computer science', 'xml', 'json', 'mysql', 'mongodb', 'programming', 'communication skills', 'jira', 'rest', 'presentation skills', 'nosql', 'spring boot', 'cassandra', 'kafka', 'aws', 'couchdb']",2025-06-13 05:50:15
Sr. Technical Consultant,Environmental Resources Management (ERM),5 - 8 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Bengaluru']","ERM is seeking Senior Technical Consultants with deep expertise in systems integration, data migration, middleware technologies and advanced configuration on leading EHSS platforms like Nabsic. The ideal candidate will bring both technical depth and solution-oriented consulting experience , enabling clients to achieve seamless inter-system operability and digital transformation goals in their Environment, Health, Safety and Sustainability (EHSS) landscape.\nYou will work in an environment that encourages innovation, cross-functional collaboration and excellence in technical delivery to consistently deliver solutions that are robust, scalable and meet complex business requirements.\n1.1.1 Responsibilities:\nAct as the technical lead for EHSS system integrations and configuration initiatives, supporting client needs across a wide range of technical scenarios.\nLead and manage integration design, development and deployment , enabling systems to communicate via REST/SOAP APIs, middleware, file-based connectors , or other integration methods.\nDesign and execute data migration strategies , including ETL , bulk data operations, validation and reconciliation across legacy and new systems.\nSupport advanced Nabsic configurations including Forms, Rules, Scripts, Approval Processes and Data Models.\nManage technical discussions with client IT teams, vendors and implementation partners , ensuring alignment and interoperability across system architectures.\nAnalyze and troubleshoot integration errors, performance bottlenecks and deployment issues using appropriate monitoring and logging tools.\nParticipate in technical workshops, UAT, solution design reviews and provide subject matter expertise throughout the implementation lifecycle.\nCreate and maintain detailed solution design documents, integration architecture diagrams and configuration specs .\nSupport EHSS reporting requirements through data extraction, transformation and interfacing with reporting systems (e.g., Power BI, Tableau).\nCollaborate across time zones with functional consultants, developers and SMEs to deliver high-quality technical solutions on-time and within budget.\nDrive adherence to SLAs and quality standards during project delivery and ongoing support.\nMaintain technical documentation and ensure knowledge transition to support teams and client IT.\n1.1.2 Requirements:\nBachelor s Degree in Computer Science, Information Technology, Engineering or related technical discipline.\n5 to 8 years of relevant technical experience in system implementation, integration and support within EHSS domains.\nPlatform Expertise: Enablon, Nabsic (Forms, Rules, Scripts, Workflows), SAP RE-FX\nSystem Integration Middleware: Strong hands on experience with REST/SOAP APIs, MuleSoft, Azure Logic Apps, Dell Boomi\nData Migration ETL: Source mapping, transformation, validation, reconciliation\nDev Tools Monitoring: Postman, Swagger, Git, JIRA, ServiceNow\nBackend Scripting: JavaScript, Python, SQL, JSON, XML\nFrontend Development: Vue.js, React.js, jQuery, HTML5, CSS3, Bootstrap\nSecurity Auth: OAuth2, SAML, API Keys\nProject Management: Agile methodologies, cross-functional collaboration\nSQL Server Oracle: Advanced database development, performance tuning, and integration\n.NET Framework: Extensive experience with C#, .NET 2.0-8.0, Windows Forms, and Windows Services\nReporting Analytics: Power BI, Tableau\nExposure to EHSS platforms such as Enablon, Sphera, Cority, Intelex, SAP, Workiva, Salesforce or Benchmark Gensuite is a plus.\nStrong written and verbal communication skills to interact effectively with clients, vendors and internal teams.\nAbility to work independently and manage priorities in a dynamic, fast-paced environment.\nWillingness to travel as needed for client engagements.\n1.1.3 Relevant Information:\nIndustry: Sustainability Consulting Services\nFunctional Area: Technical Delivery System Integration\nRole: Senior Technical Consultant - Integrations Middleware\nCareer Level: CL2 / CL3\nNumber of Vacancies: One\nLocation: Bengaluru, India\n1.1.4 Education:\nBE/B.Tech/MCA - Preferred in Computer Science, Information Technology, or related technical stream.",Industry Type: Management Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Performance tuning', 'SAP', 'jQuery', 'XML', 'Project management', 'Javascript', 'Agile', 'Oracle', 'SQL', 'Python']",2025-06-13 05:50:17
"Applications Engineering, Sr Engineer",Black Duck Software,1 - 6 years,Not Disclosed,['Bengaluru'],"Applications Engineering, Sr Engineer\nAt Black Duck Software, Inc.,\n\nBlack Duck Software, Inc. is the trusted standard for companies that have a zero-tolerance policy for software failures. Our Software Security and Quality business is all about building secure software faster. That starts with our static analysis, software composition analysis, and dynamic analysis. So, our customers can build security and quality into the DNA of their code at any stage of the software development lifecycle and across the supply chain. All while minimizing risks and maximizing speed of application development.",,,,"['C++', 'Tomcat', 'Linux', 'MySQL', 'Agile', 'Application development', 'Open source', 'Technical support', 'SQL', 'Python']",2025-06-13 05:50:19
Senior Consultant Specialist,British universal bank and financial ser...,8 - 12 years,Not Disclosed,['Pune'],"Hands on experience with LOAN IQ and Operational Knowledge of Loan IQ\nKnowledge of Complex Lending businesses ideally with exposure to project/export finance structures\nElicit business requirements from business users (including accounting, operations, risk, treasury) and subject matter experts\nAbility to gather, model and document clear business and functional requirements / specifications\nVery good working knowledge of Loan IQ configuration\nDefine and implement accounting mapping for commercial lending activities\nWrite functional specifications and work with development on implementing required enhancement/changes.\nCreation of test plans and test scripts\nAbility to present and discuss with the business and IT users any impact resulting from the project\nManage scope and requirements throughout the project lifecycle\nProvide overview and training for end-users\nConfiguring the application to satisfy business requirements\nHas prior experience with Data Migration or Integration projects\nStrong analytical skills and the ability to merge multiple existing workflows into one, standard flow\nProven experience in delivering quality specifications that are well understood by both the business and development/implementation team\nProven experience in an IT Development environment with in-depth specialisation in Loan IQ and the ability to make it work for complex loans\nMust have good written and verbal communication skills and experience of communicating complex ideas to management and key stakeholders as well as to project team members\nStrong interpersonal skills with the ability to deal with difficult and challenging situations. Ability to communicate with and manage both IT professionals and business units.\nTeam player with collaborate attitude, able to maintain high level of discipline within the given SDLC process\nAbility to manage multiple priorities, commitments, and projects.\nExperience working in collaboration with teams from different areas of organization\nEfficient time management skills to handle challenging workload.\nProactive on taking leadership when needed, self-motivated, dynamic and result oriented.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Loan IQ', 'INTERFACE', 'Lending Operations', 'LENDING', 'Integration', 'Functional Design']",2025-06-13 05:50:20
Senior Database Developer,client of Techwise Digital,7 - 11 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Key Responsibilities:\nDesign, develop, and optimize complex SQL Server databases (2016/2019 or higher).\nCreate and maintain advanced stored procedures, functions, views, and triggers.\nDrive performance tuning, indexing, and database optimization strategies.\nCollaborate with backend developers (.NET/Java) to integrate database logic using solid OOPs principles.\nParticipate in system design, code reviews, and architecture discussions.\nTroubleshoot and enhance high-volume, transactional systems.\nEnsure best practices around database security, backups, and high availability.\n\nSkills & Qualifications:\n\n7-10 years of experience in SQL Server development.\nStrong T-SQL, query optimization, and data modeling expertise.\nSound understanding of database normalization/denormalization and indexing.\nHands-on with OOPs and experience working with Java or .NET (C#, ASP.NET).\nExposure to ETL processes (SSIS or similar) and data migration strategies.\nFamiliarity with source control tools (Git, TFS) and Agile/SDLC practices.\nExcellent analytical and troubleshooting skills.\n\nIf you're ready to make an impact in a fast-paced BFSI environment, apply now or reach out via DM for more details. Let's connect!",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Part Time, Temporary/Contractual","['T-SQL', 'SQL Server', 'Database Development', 'SSIS']",2025-06-13 05:50:22
Senior Etl Informatica Developer,VBeyond,6 - 8 years,19-25 Lacs P.A.,"['Noida', 'Chennai', 'Bengaluru']","We are seeking a highly skilled and experienced Senior ETL & Reporting QA Analyst to join our dynamic team. The ideal candidate will bring strong expertise in ETL and Report Testing, with a solid command of SQL, and hands-on experience in Informatica, as well as BI Reporting tools. A strong understanding of the Insurance domain is crucial to this role. This position will be instrumental in ensuring the accuracy, reliability, and performance of our data pipelines and reporting solutions.\n\nKey Responsibilities:\nDesign, develop, and execute detailed test plans and test cases for ETL processes, data migration, and data warehousing solutions.\nPerform data validation and data reconciliation using complex SQL queries across various source and target systems.\nValidate Informatica ETL workflows and mappings to ensure accurate data transformation and loading.\nConduct end-to-end report testing and dashboard validations using Cognos (preferred), or comparable BI tools such as Tableau or Power BI.\nCollaborate with cross-functional teams including Business Analysts, Developers, and Data Engineers to understand business requirements and transform them into comprehensive test strategies.\nIdentify, log, and track defects to closure using test management tools and actively participate in defect triage meetings.\nMaintain and enhance test automation scripts and frameworks where applicable.\nEnsure data integrity, consistency, and compliance across reporting environments, particularly in the insurance domain context.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Informatica', 'ETL', 'Power Bi', 'Insurance Domain', 'Tableau', 'SQL']",2025-06-13 05:50:24
Sr. Consultant - Architecture,Hakkda,6 - 10 years,Not Disclosed,['Jaipur'],"ABOUT HAKKODA\n\nHakkoda, an IBM Company, is a modern data consultancy that empowers data driven organizations to realize the full value of the Snowflake Data Cloud. We provide consulting and managed services in data architecture, data engineering, analytics and data science. We are renowned for bringing our clients deep expertise, being easy to work with, and being an amazing place to work! We are looking for curious and creative individuals who want to be part of a fast-paced, dynamic environment, where everyone s input and efforts are valued. We hire outstanding individuals and give them the opportunity to thrive in a collaborative atmosphere that values learning, growth, and hard work. Our team is distributed across North America, Latin America, India and Europe. If you have the desire to be a part of an exciting, challenging, and rapidly-growing Snowflake consulting services company, and if you are passionate about making a difference in this world, we would love to talk to you!.\n\nWe are looking for people experienced with data architecture, design and development of database mapping and migration processes. This person will have direct experience optimizing new and current databases, data pipelines and implementing advanced capabilities while ensuring data integrity and security. Ideal candidates will have strong communication skills and the ability to guide clients and project team members. Acting as a key point of contact for direction and expertise.\nKey Responsibilities\nDesign, develop, and optimize database architectures and data pipelines.\nEnsure data integrity and security across all databases and data pipelines.\nLead and guide clients and project team members, acting as a key point of contact for direction and expertise.\nCollaborate with cross-functional teams to understand business requirements and translate them into technical solutions.\nManage and support large-scale technology programs, ensuring they meet business objectives and compliance requirements.\nDevelop and implement migration, dev/ops, and ETL/ELT ingestion pipelines using tools such as DataStage, Informatica, and Matillion.\nUtilize project management skills to work effectively within Scrum and Agile Development methods.\nCreate and leverage metrics to develop actionable and measurable insights, influencing business decisions.\nQualifications\n7+ years of proven work experience in data warehousing, business intelligence (BI), and analytics.\n3+ years of experience as a Data Architect.\n3+ years of experience working on Cloud platforms (AWS, Azure, GCP).\nBachelors Degree (BA/BS) in Computer Science, Information Systems, Mathematics, MIS, or a related field.\nStrong understanding of migration processes, dev/ops, and ETL/ELT ingestion pipelines.\nProficient in tools such as DataStage, Informatica, and Matillion.\nExcellent project management skills and experience with Scrum and Agile Development methods.\nAbility to develop actionable and measurable insights and create metrics to influence business decisions.\nPrevious consulting experience managing and supporting large-scale technology programs.\nNice to Have\n6-12 months of experience working with Snowflake.\nUnderstanding of Snowflake design patterns and migration architectures.\nKnowledge of Snowflake roles, user security, and capabilities like Snowpipe.\nProficiency in SQL scripting.\nCloud experience on AWS (Azure and GCP are also beneficial)\nPython scripting skills.\nBenefits:\n\n- Health Insurance\n- Paid leave\n- Technical training and certifications\n- Robust learning and development opportunities\n- Incentive\n- Toastmasters\n- Food Program\n- Fitness Program\n- Referral Bonus Program\n\nHakkoda is committed to fostering diversity, equity, and inclusion within our teams. A diverse workforce enhances our ability to serve clients and enriches our culture. We encourage candidates of all races, genders, sexual orientations, abilities, and experiences to apply, creating a workplace where everyone can succeed and thrive.\n\nReady to take your career to the next level? Apply today and join a team that s shaping the future!!\n\nHakkoda is an IBM subsidiary which has been acquired by IBM and will be integrated in the IBM organization. Hakkoda will be the hiring entity. By Proceeding with this application, you understand that Hakkoda will share your personal information with other IBM subsidiaries involved in your recruitment process, wherever these are located. More information on how IBM protects your personal information, including the safeguards in case of cross-border data transfer, are available here.",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Managed services', 'MIS', 'Datastage', 'Consulting', 'Scrum', 'Informatica', 'Business intelligence', 'Analytics', 'Python']",2025-06-13 05:50:25
Oracle Fusion Technical Consultant,HTC Global Services,2 - 7 years,Not Disclosed,['Hyderabad'],"Seeking a highly skilled Oracle Fusion Technical Consultant with 2 years of expertise in data migration and integrations using Oracle Integration Cloud, reports development. Candidate must have a sound understanding of Oracle Technical Development using OIC, PaaS and BIP.\nRequirements:\nStrong functional experience and expertise in Oracle Cloud ERP modules\nExperienced in providing best practices system level recommendations, guidance and knowledge transfer throughout the project lifecycle",,,,"['Procurement', 'ERP', 'Data migration', 'PAAS', 'Cloud', 'Oracle Fusion', 'UAT', 'Oracle', 'User acceptance testing', 'Testing']",2025-06-13 05:50:27
SOC Analyst L3,Rackspace Technology,3 - 4 years,Not Disclosed,['Gurugram'],"About Rackspace Cyber Defence\n\nRackspace Cyber Defence is our next generation cyber defence and security operations capability that builds on 20+ years of securing customer environments to deliver proactive, risk-based, threat-informed and intelligence driven security services.\nOur purpose is to enable our customers to defend against the evolving threat landscape across on-premises, private cloud, public cloud and multi-cloud workloads.\nOur goal is to go beyond traditional security controls to deliver cloud-native, DevOps-centric and fully integrated 24x7x365 cyber defence capabilities that deliver a , , , approach to detecting and responding to threats.",,,,"['Coding', 'Information security', 'SOC', 'Javascript', 'Network security', 'Workflow', 'microsoft', 'cisco', 'Information technology', 'Python']",2025-06-13 05:50:29
Head-SAP PP & QM,Vardhman,12 - 16 years,Not Disclosed,['Ludhiana'],"Role & responsibilities\nPosition Overview: This role will be central to driving smooth SAP S/4 HANA implementation & support at Vardhman Textiles. The ideal candidate will bring deep Manufacturing process expertise (preferable in Textiles Or other complex manufacturing), SAP configuration expertise, strong integration with other modules and proven leadership in delivering end-to-end SAP programs that meet both technical and business objectives. Post SAP implementation, the person would be part of SAP Centre of Excellence (CoE) in PP & QM and would ensure smooth adoption and problem resolution in PP & QM across Vardhman Group.\n\na) Module Leadership: Act as the functional lead for SAP PP and QM from the customer (Vardhman Textiles) side, owning solution design and process alignment.\nb) Partner Governance: Manage the SAP implementation partner to ensure high-value delivery and accountability.\nc) Business Collaboration: Engage with cross-functional business teams to define, validate, and prioritize SAP requirements and track its delivery to satisfaction.\nd) Implementation Delivery: Lead end-to-end project executionfrom Business Blueprint (BBP), Realization, Testing (UAT), to Go-live and stabilization.\ne) SAP CoE Contribution: Operate as a key member of the SAP Center of Excellence, delivering effective solutions for incidents, enhancements, and change requests.\nf) Contribute in Digital Transformation in areas of Operations\n\nKey Skills & Competencies\n\nDeep SAP PP Functional Knowledge MRP, production orders, BOMs, routings, work centers, production versions, and shop floor control.\nSAP QM Expertise Inspection planning, MICs, notifications, skip logic, batch classification, and compliance management.\nTechnical Customization Proficient in coordinating technical deliveries with development team (user exits, BAdIs, SmartForms, RICEFW specs, and enhancements.)\nMaster Data & Tools Experience with Data Management/ Migration for clean and accurate data migration and setup.\nManufacturing Domain Alignment Strong understanding of textile-specific processes (dyeing, finishing, quality specs like GSM, shrinkage).\nStakeholder Engagement Effective at managing business expectations, driving consensus, and ensuring user adoption through training and support.\n\nEducation & Experience\nBachelors degree in Engineering or Technology; post-graduate qualification (MBA, M.Tech) preferred.\nHands-on SAP experience, with significant expertise in SAP PP and QM modules, including full-cycle implementations.\nDeep domain experience in manufacturing operations; prior work in the textile industry is a strong plus.\nProven experience working with SAP ECC and/or S/4HANA environments.\n\nLocation: Ludhiana, Delhi NCR\nReporting to: IT Head",Industry Type: Textile & Apparel (Yarn & Fabric),Department: IT & Information Security,"Employment Type: Full Time, Permanent","['SAP ECC', 'PP Module', 'End To End Implementation', 'Sap Hana', 'SAP Implementation', 'consultant', 'manufacturing industry', 'Green Field Projects', 'QM Module', 'Sap Configuration', 'textile industry', 'Sap Integration']",2025-06-13 05:50:31
"Senior Netsuite Consultant opportunity with Redaptive Inc, Pune.",Redaptive Inc.,7 - 12 years,Not Disclosed,['Pune'],"Responsibilities:\nNetSuite Modules & Customization:\nExcellent understanding and Hands-on experience in NetSuite modules like Order-to-Cash, Procure-to-Pay, Record-to-Report, Inventory Management, Fixed Assets Management, Revenue Recognition, Lease Management, Billing Schedules, Advanced Procurement, Advanced Financials, Intercompany management, Multi-Book Accounting and Taxation.\nMonitor and administer account management, roles, user access, profile creation, security administration.",,,,"['Integration', 'Order-to-Cash', 'Data Architecture', 'Netsuite Implementations', 'Stakeholder Management', 'Fixed Assets Management', 'Automation', 'Inventory Management', 'Procure-to-Pay', 'Record-to-Report', 'Revenue Recognition']",2025-06-13 05:50:33
Sr. Databricks Developer,Newscape Consulting,7 - 9 years,Not Disclosed,['Pune( Baner )'],"We are looking for a highly skilled Senior Databricks Developer to join our data engineering team. You will be responsible for building scalable and efficient data pipelines using Databricks, Apache Spark, Delta Lake, and cloud-native services (Azure/AWS/GCP). You will work closely with data architects, data scientists, and business stakeholders to deliver high-performance, production-grade solutions.\nKey Responsibilities :\n- Design, build, and maintain scalable and efficient data pipelines on Databricks using PySpark, Spark SQL, and optionally Scala.\n- Work with Databricks components including Workspace, Jobs, DLT (Delta Live Tables), Repos, and Unity Catalog.\n- Implement and optimize Delta Lake solutions aligned with Lakehouse and Medallion architecture best practices.\n- Collaborate with data architects, engineers, and business teams to understand requirements and deliver production-grade solutions.\n- Integrate CI/CD pipelines using tools such as Azure DevOps, GitHub Actions, or similar for Databricks deployments.\n- Ensure data quality, consistency, governance, and security by using tools like Unity Catalog or Azure Purview.\n- Use orchestration tools such as Apache Airflow, Azure Data Factory, or Databricks Workflows to schedule and monitor pipelines.\n- Apply strong SQL skills and data warehousing concepts in data modeling and transformation logic.\n- Communicate effectively with technical and non-technical stakeholders to translate business requirements into technical solutions.\nRequired Skills and Qualifications :\n- Hands-on experience in data engineering, with specifically in Databricks.\n- Deep expertise in Databricks Workspace, Jobs, DLT, Repos, and Unity Catalog.\n- Strong programming skills in PySpark, Spark SQL; Scala experience is a plus.\n- Proficient in working with one or more cloud platforms : Azure, AWS, or GCP.\n- Experience with Delta Lake, Lakehouse architecture, and medallion architecture patterns.\n- Proficient in building CI/CD pipelines for Databricks using DevOps tools.\n- Familiarity with orchestration and ETL/ELT tools such as Airflow, ADF, or Databricks Workflows.\n- Strong understanding of data governance, metadata management, and lineage tracking.\n- Excellent analytical, communication, and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'ETL', 'Delta Lake', 'Azure Data Lake', 'Apache', 'Data Bricks']",2025-06-13 05:50:35
Senior Software Engineer,Arjun Global,7 - 12 years,Not Disclosed,['Ahmedabad'],"Position: Senior Software Engineer\nLocation: Ahmedabad\nJob Type: Full Time\n\nRoles & Responsibilities:\nSupport, maintain, and enhance existing and new product functionality for trading software in a real-time, multi-threaded, multi-tier server architecture environment to create high and low level design for concurrent high throughput, low latency software architecture\nProvide software development plans that meet future needs of clients and markets\nEvolve the new software platform and architecture by introducing new components and integrating them with existing ones\nPerform memory, cpu and resource management\nAnalyse stack traces, memory profiles and production incident reports from support teams\nPropose fixes, enhancements to existing trading systems\nAdhere to release and sprint planning with the Quality Assurance Group and Project Management.\nWork on a team building new solutions based on requirements and features\nAttend and participate in daily scrum meetings\n\nTechnical Skills Required :\nBachelors or Masters degree in Computer Science or Computer Engineering or Management Information Systems\nProficiency in JavaScript with a minimum of five years of direct experience in team-based development\nExperience with multi-threaded browser\nFamiliarity of server development and Amazon Web Services (AWS) cloud technologies\nStrong object oriented code design understanding\nSOLID design patterns\nFamiliarity with agile/iterative development methodologies\nExperience developing on large scale systems\nSolid debugging and performance tuning skills\n\n\nCompetitive Benefits Offered By Our Client:\n\nRelocation Support: Our client offers an additional relocation allowance to assist with moving expenses.\nComprehensive Health Benefits: Including medical, dental, and vision coverage.\nFlexible Work Schedule: Hybrid work model with an expectation of just 2 days on-site per week.\nGenerous Paid Time Off (PTO): 21 days per year, with the ability to roll over 1 day into the following year. Additionally, 1 day per year is allocated for volunteering, 2 training days per year for uninterrupted professional development, and 1 extra PTO day during milestone years.\nPaid Holidays & Early Dismissals: A robust paid holiday schedule with early dismissal on select days, plus generous parental leave for all genders, including adoptive parents.\nTech Resources: A rent-to-own program offering employees a company-provided Mac/PC laptop and/or mobile phone of their choice, along with a tech accessories budget for monitors, headphones, keyboards, and other office equipment.\nHealth & Wellness Subsidies: Contributions toward gym memberships and health/wellness initiatives to support your well-being.\nMilestone Anniversary Bonuses: Special bonuses to celebrate key career milestones.\nInclusive & Collaborative Culture: A forward-thinking, culture-based organisation that values diversity and inclusion and fosters collaborative teams.",Industry Type: Financial Services (Broking),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Javascript', 'AWS', 'OOPS', 'Low Latency']",2025-06-13 05:50:36
Senior Workday Integration Developer,Luxoft,5 - 10 years,Not Disclosed,['Gurugram'],"Project description\nThe role of Tech Business Analyst focuses on supporting the implementation and delivery of Workday Integrations across HR, Payroll, and business systems. The position collaborates closely with downstream teams, functional consultants, developers, and testers to enable seamless integration with internal and third-party platforms such as ADA, SAP, Benevity, Clarity, Aspect, and ControliQ. Agile methodologies are followed to ensure scalable, compliant, and business-aligned integration solutions.\n\nResponsibilities\n\nAct as the Tech Business Analyst in the end-to-end implementation of Workday integrations.\n\nGather integration requirements from downstream systems and document detailed design specifications.\n\nSupport all project phases, including design, build, testing (SIT), PRR, and deployment.\n\nDocument field mappings, transformation logic, user stories, and cost trackers.\n\nCreate and update process models using enterprise modeling tools.\n\nFacilitate alignment between business and technical teams to ensure smooth delivery.\n\nConduct business analysis such as scenario, data, gap, and change impact analysis.\n\nDevelop process flows, data models, business rules, and reporting structures as part of solution design.\n\nMaintain internal documentation repositories (e.g., Confluence) and manage stakeholder updates.\n\nCollaborate with cross-functional teams, including integration, development, QA, and business stakeholders.\n\nSupport vendor coordination, testing, cutover, and go-live planning.\n\nApply agile practices to ensure high-quality, user-centric outcomes.\n\nSkills\nMust have\n\n5+ years of experience as a Senior Workday Integration Developer/ Consultant with proven expertise in Workday integrations, configuration, and system optimization.\n\nStrong experience in Workday Integrations, including design and delivery.\n\nProven integration experience with: oCustomer Master ADA [MEID] oBenevity, Clarity, ODI oAspect & ControliQ (Bidirectional Time & Attendance interfaces) oODB and GPR\n\nUK\n\nProficient in data mapping, transformation logic, and integration architecture.\n\nExperience with Workday security frameworks, authentication protocols (OAuth, SAML), and data governance.\n\nAbility to translate business requirements into scalable technical solutions.\n\nExcellent analytical, documentation, and process modeling skills.\n\nStrong workshop facilitation and stakeholder engagement capabilities.\n\nProficiency working within agile delivery environments.\n\nNice to have\n\nWorkday Certified Integration Developer with exposure to large-scale implementations.\n\nExperience with reporting and data migration in cloud-based HR platforms.\n\nWorking knowledge of enterprise modeling tools and data lake environments.\n\nAdvanced Excel and experience with data analytics and validation.\n\nFamiliarity with compliance, regulatory reporting, and change management practices.\n\nStrong communication and stakeholder management across technical and non-technical teams.\n\nDemonstrated ability to work independently, manage risks, and adapt in fast-paced environments.\n\nExperience collaborating with external vendors and integration partners.\n\nOther\n\nLanguages\n\nEnglishB2 Upper Intermediate\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['documentation', 'data mapping', 'stakeholder management', 'stakeholder engagement', 'agile', 'workday integration', 'rest', 'project management', 'workday', 'web services', 'soa', 'integration architecture', 'microservices', 'sql', 'java', 'advanced excel', 'xml', 'mule esb', 'xslt', 'process modeling']",2025-06-13 05:50:38
Sr. Salesforce Developer,Nextracker,8 - 13 years,Not Disclosed,['Hyderabad'],"This position will be divided between enhancing the current Salesforce platform that includes CRM, CPQ, and Service Cloud leveraging out of the box features and custom development\nThe Senior Salesforce CPQ Developer will be involved in the full development life-cycle from technical design to development, testing, and deployment and engage and facilitate discussions with other Salesforce experts\nSuccessful candidates will be technical professionals, experienced in web application development\nPerform hands-on technical Salesforce development and implementations, with a focus on delivering functional solutions on the Salesforce.com platform\nWork with senior members of the team to analyze business requirements, translating those requirements in to customized solutions using the Salesforce platform\nImplement Salesforce solutions that adhere to platform best practices, and perform peer code reviews\nTypical Salesforce implementations include custom platform development (Apex, Visualforce, Lightning Components), integrations with back office systems (often through the use of middle-ware tools) and complex data migrations;\nProvide development support, from design through testing and deployment, often working with other members of the team\nImplement, deploy and document projects that leverage the Salesforce.com toolset\nPost-delivery: work with client teams in supporting the live application and perform hand-off and knowledge transfer activities, positioning our clients for long term success\nWork in a fast pace environment with team of developers\nTechnicall skill Requirements\nHave 8+ years of experience in Salesforce development, including Apex, Visualforce, and Lightning Web Components.\nAtleast 5+ years of experience in Implementing, Customizing & Enhancing Salesforce CPQ\nFamiliarity with front-end web technologies, such as HTML, CSS, and JavaScript.\nHave deep expertise with Lightning Flows, Apex Triggers, and SOQL/SOSL\nExtensive knowledge of Salesforce configuration, customization, and security\nHave experience with development tools such as Salesforce CLI, Workbench, and Git\nKnowledge of Salesforce integration patterns and technologies, including REST and SOAP APIs.\nHave excellent verbal and written communication skills\nDeep understanding of Salesforce CPQ capabilities, with a keen insight into best practices and industry standards.\nKnowledge of cloud-based computing principles and practices\nHere are a few of our preferred experiences\nHave Salesforce Platform Developer I and II or other Developer Certifications\nNice to have Salesforce Certified CPQ Specialist certification\nExposure to Mulesoft, Netsuite",Industry Type: Power,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Front end', 'GIT', 'Javascript', 'Back office', 'HTML', 'Software solutions', 'Monitoring', 'Analysis services', 'CRM', 'Salesforce']",2025-06-13 05:50:40
Senior Engineer,The TJX Companies Inc,5 - 10 years,Not Disclosed,['Hyderabad'],"TJX Companies\nAt TJX Companies, every day brings new opportunities for growth, exploration, and achievement. You ll be part of our vibrant team that embraces diversity, fosters collaboration, and prioritizes your development. Whether you re working in our four global Home Offices, Distribution Centers or Retail Stores TJ Maxx, Marshalls, Homegoods, Homesense, Sierra, Winners, and TK Maxx, you ll find abundant opportunities to learn, thrive, and make an impact. Come join our TJX family a Fortune 100 company and the world s leading off-price retailer.\nJob Description:\nAbout TJX:\nTJX is a Fortune 100 company that operates off-price retailers of apparel and home fashions. TJX India - Hyderabad is the IT home office in the global technology organization of off-price apparel and home fashion retailer TJX, established to deliver innovative solutions that help transform operations globally. At TJX, we strive to build a workplace where our Associates contributions are welcomed and are embedded in our purpose to provide excellent value to our customers every day. At TJX India, we take a long-term view of your career. We have a high-performance culture that rewards Associates with career growth opportunities, preferred assignments, and upward career advancement. We take well-being very seriously and are committed to offering a great work-life balance for all our Associates.\nWhat you will discover:\nInclusive culture and career growth opportunities\nGlobal IT Organization which collaborates across U.S., Canada, Europe and Australia\nChallenging, collaborative, and team-based environment\nWhat you will do:\nEnterprise Data & Analytics thrives on strong relationships with our business partners and working diligently to address their needs which support TJX India growth and operational stability. On this tightly knit and fast-paced solution delivery team you will be constantly challenged to stretch and think outside the box.\nYou will have a real opportunity to be a part of TJX s transformation to a data driven culture, with the autonomy to work with the business to unlock game changing insights that drive business value. We have modernized our technology stack to focus on the cloud and top tier tools. We are looking for someone who embraces the use of technology to build, manage, and govern data.\nAs a Senior Engineer at TJX India, you will work with a team that partners with the EDA Data Foundation team and delivers products for one or more customer analytics product teams.\nKey Responsibilities:\nAbility to understand the functional and technical design architecture implemented in Datalake on Snowflake\nAbility to lead team of ETL developers and guide them on business / Functional requirement and help in technical area\nExtract, transform and load from various sources system to snowflake using a combination of Talend, Azure Data factory, Snowflake, Data Lake Analytics, Data ingestion to one or more Azure services\nCollaborate effectively within Data Technology teams, Business Information teams to design and build optimized data flows from source to Data visualization\nDevelop and deploy performance optimization methodologies\nStrong in drafting functional and technical documentation\nSupport the Solutioning team from architectural design , testing and implementation\nDesign and architect end-to-end data engineering pipelines, ensuring optimal performance, reliability, and scalability\nDevelop and maintain data integration processes, including ETL/ELT workflows and data ingestion pipelines\nEvaluate and recommend emerging technologies and tools to enhance the data architecture and engineering capabilities\nProvide technical leadership and guidance to junior team members, fostering a culture of collaboration and innovation\nEnsure compliance with data governance and security policies, including data privacy regulations\nBe a part of data modernization projects providing direction on matters of overall design and technical direction, acts as the primary driver toward establishing guidelines and approaches\nExcellent communication and collaboration skills, with the ability to effectively bridge the gap between technical and non-technical audiences\nWhat You will Need (Minimum Qualifications):\nAt least 5 years in-depth, data engineering experience and execution of ETL (Talend) data pipelines, scripting and SQL queries\nA minimum of 2 years of experience in building enterprise level solution on Azure cloud environment with Azure Function, Azure Databricks, Azure Event Hub, Azure Event Grid, Azure data lake, Azure Data factory\nA minimum of 1 year of experience on hands-on coding in python and/or pyspark on Azure Databricks environment\n3 years of experience working in Agile and Scrum frameworks\nExtensive knowledge of Data warehousing and ability to understand star schema\nWork experience in relational database and hands-on on writing SQL queries, stored procedure extensively\nHighly proficient in Data analysis - analysing SQL, Python scripts, ETL/ELT transformation scripts\nImplement CI/CD pipelines for application deployment using Azure DevOps\nPreferred Qualifications:\nExperience in Talend (4+ years) for implementing large scale Data Engineering projects\nExperience in Data Engineering and DevOps knowledge\nExperience in programming languages like SQL, Python / Pyspark, Shell Scripting and Scala.\nExperience in Azure Functions, Azure Data Factory, Logic Apps and Spark platforms\nExperience in Datastage and other ETL tool is a plus\nBachelor s degree in computer science, Engineering, Mathematics, a technical field, or equivalent practical experience.\nCome Discover Different at TJX India. From opportunity and teamwork to growth, we think you ll find that it s so much more than a job. When you re a part of our global TJX family, you have the full support of a diverse, close-knit group of people dedicated to finding great deals and fantastic style. Best of all? They have a lot of fun doing it.\nWe care about our culture, but we also prioritize the tangible stuff (Competitive salaries: check. Solid benefits: check. Plenty of room for advancement: of course). It s our way of empowering you to make your career here.\nWe consider all applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, gender identity and expression, marital or military status. We also provide reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act and applicable state and local law.\nIn addition to our open door policy and supportive work environment, we also strive to provide a competitive salary and benefits package. TJX considers all applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, gender identity and expression, marital or military status, or based on any individuals status in any group or class protected by applicable federal, state, or local law. TJX also provides reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act and applicable state and local law.\nAddress:\nSalarpuria Sattva Knowledge City, Inorbit Road\nLocation:\nAPAC Home Office Hyderabad IN",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Coding', 'Datastage', 'Shell scripting', 'Agile', 'Scrum', 'Analytics', 'SQL', 'Python']",2025-06-13 05:50:42
Senior Dot Net Manager,Arieotech Solutions,14 - 17 years,Not Disclosed,['Pune'],"Experience required 10-15\nPosition Fulltime\nMode Hybrid .\nNeed guy from Pune as location of work is Pashan Pune .\nNeed utmost technical person . Role would be 40 percent managerial 60 percent technical .\nRoles and responsibility :\nCollaborate with internal teams to produce software design an architecture\nWrite clean, scalable code using .NET programming languages (.net core and framework)\nPrepare and maintain code for various .Net applications and resolve any defects in systems.\nRevise, update, refactor, and debug code Improve existing software\nDevelop documentation throughout the software development\nMonitor everyday activities of the system and Serve as an expert on applications and provide technical support.\nPreference\nExperience 8 + yrs\nExcellent communication skills\nAbility for critical thinking & creativity\nHaving a systematic and logical approach to problem-solving, team working skills\nProvide expert advice to project teams on the use of integration technology, data architecture, modelling, and system architecture including integration best practices.\nCommunicate project status to various levels of management.\nManage an Integration/Architecture Roadmap and project backlog in partnership with the R&D leadership team, prioritize initiatives in line with business goals, and drive design and deployment of integration solutions that enable scalability, high availability, and re-use.\nAlso needs hands experience for .NET/Java Language\nAI/ Azure Open AI knowledge is a big plus\nRequirement\nGood expertise in the MS entity framework/Dapper\nProven experience as a .NET Developer\nFamiliarity with the .NET framework, SQL Server & design/architectural patterns Model-View-Controller (MVC))\nFamiliarity with working of asp dot net core application\nKnowledge of at least one of the .NET languages (e.g. C# ..)\nFamiliarity with architecture styles/APIs (REST, RPC)\nExperience with alerting mechanisms for API s in case of any failures.\nUnderstanding of Agile methodologies\nGood troubleshooting and communication skills\nExperience with concurrent development source control (Git)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System architecture', 'Software design', 'GIT', 'Agile', 'Entity framework', 'MVC', 'Troubleshooting', 'Technical support', 'SQL', 'Data architecture']",2025-06-13 05:50:44
Sr. Salesforce Support Engineer/Lead_Mumbai,Krios,4 - 9 years,Not Disclosed,"['Mumbai Suburban', 'Mumbai (All Areas)( kandivali )']","Dear Candidate,\n\nPresently we are hiring Sr. Salesforce Suppport Engineer/Lead in our Organisation "" Krios Info Solutions""\nAbout Krios\nKrios offers Technology Consulting, managed services, and software development services in Microsoft technologies, Salesforce, SAP and Analytics. 30+ happy customers across diverse industry verticals is a testimony of our client centricity. Krios today is a team of 300+ Kriosians, driven by a mission of building globally competitive technology solutions for our clients across diverse industry verticals in India and overseas.\nWebsite: www.kriosispl.com\n\nRoles and Responsibilities\nProvide technical support to customers on Salesforce platform, ensuring timely resolution of issues and high levels of customer satisfaction.\nTroubleshoot complex salesforce queries, including data migration, configuration, and customization.\nCollaborate with cross-functional teams to resolve business requirements and implement solutions using Salesforce tools such as Apex, LWCs, and Flows.\nDevelop expertise in various aspects of Salesforce ecosystem, including Lightning Web Components (LWC), Visualforce Pages, and Aura components.\nParticipate in knowledge sharing sessions to document troubleshooting steps and best practices for future reference.\nDesired Candidate Profile\n4-9 years of experience in Salesforce Support role with a strong understanding of Salesforce architecture and its applications.\nProficiency in writing SOQL queries for querying Salesforce objects; ability to write efficient queries for large datasets.\nStrong hands-on experience working with Salesforces Lead object; familiarity with lead management processes is essential.\nExperience working on multiple projects simultaneously; excellent time management skills required.\nInterested candidate please APPLY or send your profile with me on sheetal.madkar@kriosispl.com for further interview process\n\nThanks & Regards,\nSheetal Madkar\nTeam Lead - IT Recruitment\nConnect - https://www.linkedin.com/in/sheetal-madkar-5139b28b/\n_________________________________\nKrios Info Solutions Pvt. Ltd.\n701-702, Wakad Business Bay Behind Tip Top International Hotel, Wakad, Pune, Maharashtra - 411057.\nIndia | USA | Netherlands | Australia\nWebsite: www.kriosispl.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Salesforce Support', 'Salesforces Lead', 'Soql Query', 'Sr. Salesforce Support']",2025-06-13 05:50:46
Sr. RoR Developer_vConstruct_subsidiary of US based company,Vconstruct,4 - 9 years,Not Disclosed,['Noida'],"VueOps is on a mission to transform the building management industry. We are looking for a Senior Software Engineer with experience in Ruby-on-Rails to join our team. This is an incredible opportunity to be a key contributor to the product development team for a Silicon Valley startup. You will be working in a fast-paced environment, working with your fellow engineers, product managers and other business stakeholders contributing to the key features on our product roadmap.\n\nKey Responsibilities",,,,"['REST API', 'Ror', 'Ruby Rails', 'OOPS', 'Rails', 'react', 'Ruby', 'Angular']",2025-06-13 05:50:47
Informatica Developer â€“ IDMC Specialist,Mobile Programming,6 - 11 years,Not Disclosed,"['Kochi', 'Bengaluru', 'Thiruvananthapuram']","Candidate Skill:Technical Skills Informatica IDMC, CDI, CAI, ETL, ELT, SQL, Oracle, SQL Server, Data Quality, Cloud Integration, Operational Dashboards\nJob Description:We are looking for an experienced Informatica Developer with a specialization in Informatica Intelligent Data Management Cloud (IDMC). The ideal candidate should have hands-on experience with Cloud Data Integration (CDI) and Cloud Application Integration (CAI) and be skilled in building, deploying, and optimizing cloud-based ETL/ELT solutions.\nKey Responsibilities:Design, develop, and maintain data pipelines and integrations using Informatica IDMCWork on Cloud Data Integration (CDI) and Cloud Application Integration (CAI) modulesBuild and optimize ETL/ELT mappings, workflows, and data quality rules in a cloud setupDeploy and monitor data jobs using IDMCs operational dashboards and alerting toolsCollaborate with data architects and business analysts to understand data integration requirementsWrite and optimize SQL queries for data processingWork with RDBMS systems such as Oracle or SQL ServerTroubleshoot and resolve integration issues efficientlyEnsure performance tuning and high availability of data solutions\nRequired Skills:Strong hands-on experience with Informatica IDMCProficiency in CDI, CAI, and cloud-based data workflowsSolid understanding of ETL/ELT processes, data quality, and data integration best practicesExpertise in SQL and working with Oracle/SQL ServerStrong analytical and problem-solving skillsExcellent communication and interpersonal abilitiesTechnical Key Skills: Informatica IDMC, CDI, CAI, ETL, ELT, SQL, Oracle, SQL Server, Data Quality, Cloud Integration, Operational Dashboards",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Informatica', 'Operational Dashboards', 'Cloud Integration', 'Data Quality', 'Informatica IDMC', 'CDI', 'CAI', 'SQL Server', 'ETL', 'ELT', 'Oracle', 'SQL']",2025-06-13 05:50:49
Snowflake Developer Role,Relanto Global,4 - 9 years,6-16 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Dear Candidate,\n\nWe are hiring for Snowflake Developers for Bangalore location with shorter notice\n\nBelow is the JD for your reference:\nBachelors or Masters degree in Computer Science, Information Technology, Data Science, or a related field.\n4+ years of experience in data architecture, data engineering, or a related field.\nExtensive experience with Snowflake, including designing and implementing Snowflake-based solutions.\nMust be strong in SQL\nProven track record of contributing to data projects and working in complex environments.\nFamiliarity with cloud platforms (e.g., AWS, GCP) and their data services.\nSnowflake certification (e.g., SnowPro Core, SnowPro Advanced) is a plus.\n\nRegards,\nTAG - Team",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'SQL', 'PLSQL']",2025-06-13 05:50:51
Informatica/ETL PowerCenter Developer,Saama Technologies,7 - 12 years,Not Disclosed,"['Pune', 'Chennai', 'Coimbatore']","Role Description: The Informatica/ETL PowerCenter Developer would need to have at least 7+ years of experience.\n\nResponsibilities and Qualifications:  \nParticipates in ETL Design of new or changing mappings and workflows with the team and prepares technical specifications.\nCreates ETL Mappings, Mapplets, Workflows, Worklets using Informatica PowerCenter 10.x and prepare corresponding documentation.",,,,"['Etl Informatica', 'PLSQL', 'Informatica Powercenter', 'SQL']",2025-06-13 05:50:53
Oracle DB Consultant,Mobile Programming,5 - 8 years,Not Disclosed,['Bengaluru'],"We are seeking an experienced Oracle DB Consultant to join our team in Bengaluru. As an Oracle Database Consultant, you will play a critical role in managing, optimizing, and maintaining the Oracle database environment, ensuring high availability, performance, and security. You will work closely with development and infrastructure teams to deliver efficient database solutions that support business goals and objectives.Key Responsibilities:\n- Administer and manage Oracle database systems, including installation, configuration, and maintenance.\n- Perform database tuning, optimization, and performance enhancement activities to ensure high availability and performance.\n- Develop and implement backup and recovery strategies for Oracle databases.\n- Work closely with development teams to design and optimize database schemas, queries, and procedures.\n- Monitor database health, performance, and security, ensuring compliance with best practices.\n- Troubleshoot and resolve database-related issues in a timely manner.\n- Ensure database security through access control, encryption, and other security measures.\n- Collaborate with cross-functional teams to implement data migration, integration, and transformation strategies.\n- Provide technical support and training for team members and end-users as needed.\n- Keep up to date with the latest developments in Oracle database technologies and recommend improvements.Required Skills and Qualifications:\n- 5 to 8 years of experience in Oracle Database Administration.\n- Hands-on experience with Oracle 12c, 19c, and Oracle RAC.\n- Strong knowledge of SQL, PL/SQL, and Oracle performance tuning.\n- Experience with backup and recovery solutions (RMAN, Data Guard, etc.).\n- Understanding of Oracle data replication and high availability solutions.\n- Familiarity with Oracle Cloud offerings is a plus.\n- Ability to perform database migrations, upgrades, and patching.\n- Strong troubleshooting skills and ability to optimize complex database queries.\n- Ability to collaborate effectively with cross-functional teams to meet business requirements.\n- Excellent communication skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle DB', 'Data Guard', 'Database Performance Tuning', 'Backup and Recovery', 'Oracle Database', 'Data Replication', 'Oracle 12c', 'Oracle GoldenGate', 'RMAN', 'SQL', 'PL/SQL', 'Oracle Cloud', 'Oracle RAC', 'Linux/Unix']",2025-06-13 05:50:54
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Bengaluru'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:50:56
SAP FICO Consultant - S/4HANA,Forward Eye Technologies,7 - 12 years,Not Disclosed,['Bengaluru'],"We are looking for an experienced SAP FICO Consultant with S/4HANA expertise to join our team. The ideal candidate will have a deep understanding of financial and controlling processes and hands-on experience in implementing or supporting SAP S/4HANA Finance solutions.\n\nKey Responsibilities :\n\n- Configure and implement SAP FICO modules (GL, AP, AR, AA, CO-CCA, CO-IO, COPA, Product Costing, ML).\n\n- Lead and support SAP S/4HANA Finance implementation and migration projects.\n\n- Analyze business requirements and translate them into functional specifications.\n\n- Work with cross-functional teams for integrations with MM, SD, PP, and other SAP modules.\n\n- Perform unit testing, integration testing, and support UAT with the business.\n\n- Provide post-go-live support and troubleshooting for FICO modules.\n\n- Prepare functional documentation and user training manuals.\n\n- Involve in data migration, cutover planning, and month-end/year-end activities.\n\n- Strong understanding of New GL, Universal Journal (ACDOCA), and Central Finance concepts.\n\n- Experience in CO-PA (Account-based and Costing-based) and Margin Analysis in S/4HANA.\n\nMust-Have Skills :\n\n- 6+ years of hands-on experience in SAP FICO with at least 12 full lifecycle implementations in S/4HANA.\n\n- Experience in Universal Journal, Fiori apps for finance, and S/4HANA Finance innovations.\n\n- Proficiency in Cost Center Accounting (CCA), Internal Orders (IO), Profit Center Accounting (PCA), Product Costing,\nand COPA.\n\n- Exposure to integration with SD/MM/PP modules.\n\n- Experience with taxation setup, bank interface, electronic bank statement, and payment configurations.\n\n- Good knowledge of business processes in finance and controlling.\n\nGood to Have :\n\n- Experience with Central Finance and Group Reporting.\n\n- Familiarity with SAP Activate methodology.\n\n- Basic knowledge of ABAP debugging and Fiori configuration.\n\n- Experience with interfaces like Concur, BlackLine, Vertex, etc.\n\n- Knowledge of Agile/Scrum project management.\n\nEducation & Certifications :\n\n- SAP FICO and/or S/4HANA Finance Certification is a plus.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP FICO', 'S/4 HANA', 'SAP MM', 'SAP Integration', 'SAP COPA', 'SAP GL', 'SAP Implementation', 'SAP Support']",2025-06-13 05:50:58
Oracle Finance Functional Consultant,Calfus Technologies India,3 - 7 years,Not Disclosed,"['Pune', 'Bengaluru']","As a Finance Functional Consultant, you would be responsible for designing and implementing Oracle Financials Cloud solutions, leading process design with clients, handling integrations, managing testing phases, and overseeing accurate data migration.\n\nWhat you'll Do:\n\nDesign and implement Oracle Financials Cloud solutions that meet business needs with focus on industry best practices\nLead the process design along with client business team and prepare process flows and solution design documents\nFunctional design integrations with third party systems\nPrepare test scenarios and anchor the testing phases of the project\nOversee data migration activities ensuring high data integrity and accuracy\nOn your first day, we'll expect you to have\n\nMust have carried at least 2 implementations on Oracle Fusion Financials\nWorking experience in any 2 of the financial modules: AP, AR, GL, FA, CM, Expenses, Advanced Collections\nExperience in implementing Leases /Assets will be a plus\nExcellent problem-solving skills and ability to troubleshoot complex business problems\nOracle certification in these modules will be an added advantage\nStrong communication and interpersonal skills and ability to work collaboratively with client stakeholders and internal technical team\nExperience and willingness to work in an onsite-offshore model\nQualification: CA/ICWA/CS, MBA in Finance",Industry Type: Software Product,Department: Consulting,"Employment Type: Full Time, Permanent","['Process design', 'ERP', 'Data migration', 'Enterprise applications', 'Finance', 'Test scenarios', 'Wellness', 'data integrity', 'Oracle financials', 'Oracle']",2025-06-13 05:50:59
SAP MM ETL Consultant,Sourceright Technologies,10 - 15 years,Not Disclosed,"['Chennai', 'Bengaluru']","Stakeholder Collaboration & Business Engagement\nParticipate and engage in Data Quality implementation discussions with business.\nParticipate in architecture discussions around the Data Quality framework.\nCollaborate with business users from Supply Chain, Manufacturing, Quality etc to capture Business rules.\nProvide business requirements to BODS / LSMW team to enable data loads.\nReview and track exceptions for data quality rules and incorporate logic in the tool.\nIdentify and investigate continuous improvement opportunities in the data standards, data quality rules, and data maintenance processes for each master data object by working with Business and other MDM Teams as required.\nEnsure to capture sign off from key stakeholders at critical stages of the project.\nData Quality Rules & Standards Definition.\nDefine and document data quality rules based on business requirements, functional specifications, and compliance needs.\nDerive technical definitions for agreed business rules / Data Quality rules to support tool development.\nSupport Rule development in DQ tool.\nReview and validate rule output and inform business.\nData Profiling, Cleansing & Monitoring\nCreate and maintain data quality dashboards and reports to track improvements and errors.\nReview data profiling results for material master data and identify opportunities for data cleansing.\nSupport the design and execution of the Data Quality Framework.\nPerform root cause analysis and recommend process changes and system controls.\nPerform pre-validation and post-validation as part of Data cleansing execution.\nMaterial Master Data Management (SAP)\nManage and maintain Material Master data in SAP where BODs cannot be used.\nEnsure data accuracy, consistency, and completeness across all material master records.\nDrive discussions on obsolete records identification and define the deactivation criteria.\nData Migration, Integration & Tool Support\nWork closely with IT teams to support data migration, cleansing, and validation activities during SAP projects or enhancements.\nPerform process analysis and translate business issues and requirements into actionable plans to improve the Master Data Process from a tool and programming perspective.\nRecommend system and process enhancements to improve data quality and governance practices.\nSkills required\n10+ years experience in SAP Material Master module along with knowledge on SAP MM T-codes and fields\nHands-on experience in creating, maintaining, and validating material master data in SAP\nFamiliarity with industry-specific material types, units of measure, valuation classes, and procurement types\nKnowledge of material lifecycle processes (creation, change, extension, obsolescence, deletion)\nStrong skills in data quality rule definition, documentation, and enforcement\nExperience with data profiling, cleansing, and standardization techniques\nAbility to perform root cause analysis of data issues and recommend remediation\nWorking knowledge of SAP BODS (BusinessObjects Data Services) for ETL, transformation, and data quality operations\nProficient in Excel and comfortable with SQL or reporting tools.\nStrong knowledge of SOX compliance, internal controls, and audit procedures.\nAbility to provide functional input for BODS developers - mapping rules, logic, validation\nHands-on experience with SAP LSMW - recording, field mapping, conversion rules, batch input methods\nUnderstanding of data load processes and best practices for data migration in SAP\nStrong skills in data validation techniques - pre-load and post-load checks, reconciliation, exception reporting\nAbility to interpret and analyze SAP data using SE16N, SQVI, and custom reports\nAbility to document data standards, data definitions, and quality rules\nTechnical skills in exploring, analysing, profiling, manipulating data sets through Excel, Power BI, SAP IS, Tableau etc",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Procurement', 'Supply chain', 'Data migration', 'SAP MM', 'Reconciliation', 'Data quality', 'Monitoring', 'Reporting tools', 'SQL', 'Auditing']",2025-06-13 05:51:01
Functional Consultant - Supplier,Sourceright Technologies,10 - 18 years,Not Disclosed,"['Chennai', 'Bengaluru']","Stakeholder Collaboration & Business Engagement\nParticipate and engage in Data Quality implementation discussions with business and technical product owners in procurement.\nParticipate in architecture discussions around the Data Quality framework.\nCollaborate with business users from procurement, P2P finance, supply chain etc to capture Business rules.\nProvide business requirements to BODS / LSMW team to enable data loads.\nReview and track exceptions for data quality rules and incorporate logic in the tool.\nIdentify and investigate continuous improvement opportunities in the data standards, data quality rules, and data maintenance processes for each master data object by working with Business and other MDM Teams as required.\nEnsure to capture sign off from key stakeholders at critical stages of the project.\nData Quality Rules & Standards Definition.\nDefine and document data quality rules based on business requirements, functional specifications, and compliance needs.\nDerive technical definitions for agreed business rules / Data Quality rules to support tool development.\nSupport Rule development in DQ tool.\nReview and validate rule output and inform business.\nData Profiling, Cleansing & Monitoring\nCreate and maintain data quality dashboards and reports to track improvements and errors.\nReview data profiling results for supplier master data and identify opportunities for data cleansing.\nSupport the design and execution of the Data Quality Framework.\nPerform root cause analysis and recommend process changes and system controls.\nPerform pre-validation and post-validation as part of Data cleansing execution.\nSupplier/vendor Master Data Management (SAP)\nManage and maintain supplier Master data in SAP where BODs cannot be used.\nEnsure data accuracy, consistency, and completeness across all supplier master records.\nDrive discussions on obsolete records identification and define the deactivation criteria.\nData Migration, Integration & Tool Support\nWork closely with IT teams to support data migration, cleansing, and validation activities during SAP projects or enhancements.\nPerform process analysis and translate business issues and requirements into actionable plans to improve the Master Data Process from a tool and programming perspective.\nRecommend system and process enhancements to improve data quality and governance practices.\nSkills required\n10+ years experience in SAP vendor Master objects along with knowledge on SAP T-codes and fields\nHands-on experience in creating, maintaining, and validating supplier/vendor master data in SAP\nGood understanding of purchase order process, invoice process, payment process etc within P2P.\nFamiliarity with industry-specific rules in procurement, P2P finance, various vendor types.\nKnowledge of supplier/vendor lifecycle processes (creation, change, extension, obsolescence, deletion)\nStrong skills in data quality rule definition, documentation, and enforcement\nExperience with data profiling, cleansing, and standardization techniques\nAbility to perform root cause analysis of data issues and recommend remediation\nWorking knowledge of SAP BODS (BusinessObjects Data Services) for ETL, transformation, and data quality operations\nAbility to provide functional input for BODS developers - mapping rules, logic, validation\nHands-on experience with SAP LSMW - recording, field mapping, conversion rules, batch input methods\nUnderstanding of data load processes and best practices for data migration in SAP\nStrong skills in data validation techniques - pre-load and post-load checks, reconciliation, exception reporting\nAbility to interpret and analyze SAP data using SE16N, SQVI, and custom reports\nAbility to document data standards, data definitions, and quality rules\nTechnical skills in exploring, analysing, profiling, manipulating data sets through Excel, Power BI, SAP IS, Tableau etc\nGood understanding of Ariba, SAP ECC, SAP MDG.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Procurement', 'Supply chain', 'data cleansing', 'Data migration', 'Data validation', 'Reconciliation', 'business rules', 'Data quality', 'Monitoring', 'Ariba']",2025-06-13 05:51:03
MSD 365 CRM Peofessional,Diverse Lynx,5 - 8 years,Not Disclosed,['Bengaluru'],"JD for MSD 365 CRM. Key Responsibilities\nCustomization Configuration: Develop and customize Dynamics 365 CRM applications using tools like Power Apps, Power Automate, and Power Virtual Agents.\nPlugin Development: Create and maintain custom plugins using C# to implement complex business logic.\nJavaScript Development: Implement client-side logic using JavaScript to enhance user interface and experience.\nIntegration: Integrate Dynamics 365 CRM with external systems using RESTful APIs, Azure Logic Apps, and other integration tools.\nData Management: Manage data migration and integration tasks, ensuring data integrity and consistency.\nTesting Debugging: Conduct unit testing and debugging to ensure the quality and reliability of custom solutions.\nDocumentation: Prepare and maintain technical documentation for custom solutions and configurations.\nSupport Maintenance: Provide ongoing support and maintenance for existing CRM solutions, troubleshooting issues as they arise.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data migration', 'Data management', 'Debugging', 'Javascript', 'data integrity', 'Maintenance Manager', 'Unit testing', 'Troubleshooting', 'CRM', 'Technical documentation']",2025-06-13 05:51:05
SAP FICO - BP - ECSSAP,Diverse Lynx,12 - 17 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Skill-SAP FICO - BP - ECSSAP\nExperiance- 12 Years and 8 relevent\nJob Location- Hyderabad,Bangalore, Pune\n\nDetailed JD (Roles and Responsibilities)-\nHaving 8+ years of experience in SAP FICO.\nHaving implementation in SAP ECC and production support for SAP S/4 HANA Finance. Roll-out and SAP support experience.\nGood communication and presentation skills with a customer-oriented attitude.\nInvolved in gathering analyzing the requirements and enhancement activities.\nEnterprise structure design and configuration: Setting up enterprise structure and assigning relationships between the organizational units of FI/CO, SD, and MM\nConfiguration in all sub-areas of FI modules including FI-G/L, A/P, A/R, FI-AA\nFixed Assets module: Setting up depreciation charts and Areas, Asset classification, defining GL accounts for FA integration with FI, performing depreciation run and Migration of Fixed Assets across Company Codes.\nWorking knowledge on S4 Hana finance 1909\nGeneral workflow in Business Process\nGood knowledge on Cost center profit center and Internal orders master data.\nGeneral Ledger Accounting and related Processes.\nExtensive experience in data migration from Legacy system to SAP for Upgrade / New Implementation / Integration using LSMW/LTMC.\nConfigured and managed the Vendor and Customer Master data.\nInvolved in FICO testing on various sub modules like Accounts Payables, Account Receivables, Asset Accounting, internal orders, and cost centers.\nDetailed knowledge on correspondence like Dunning, SOA, and Invoices.\nVery good working knowledge on APP payment processing and Dunning.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Business process', 'Data migration', 'Depreciation', 'Payment processing', 'SAP FICO', 'SOA', 'Production support', 'Fixed assets', 'Workflow', 'General ledger accounting']",2025-06-13 05:51:06
Tech Lead : MERN,2070health,2 - 3 years,Not Disclosed,['Bengaluru'],"*Please note this job is not for 2070 Health.*\nAbout Wysa\nWysa is the worlds most advanced AI-based digital companion for behavioral health. We are the global tech leader in the mental health space.\nWe are trusted by employers, payors and healthcare providers and government agencies because we are able to provide a scalable and low cost solution.\nBeing an AI-based solution, Wysa overcomes the stigma and privacy concerns that often restrict people seeking help for their mental wellbeing.\nWysas Mission\nWysa is on a mission to help 50 million people with their mental wellbeing by the next decade. We are working with our partners all over the globe to bring high-quality digital mental wellbeing solutions to the ones in need.\nCurrently we have reached 6.5 million people and have saved 450 lives.\nOur Tech Landscape From 10000ft above\nThe Wysa AI Digital companion is our core product, around which we have a huge suite of tools, products, and programs. We use the MERN stack heavily, and python for our ML and NLP and are constantly iterating on our use of generative AI across business and end user products as well as internal devX. We also use a plethora of other technologies based on the use case, e.g. s3-athena-glue-quicksight for our business insights dashboards. Exclusively on the cloud, our servers are primarily on AWS, fronted by cloudflare.\nA few numbers about scale in our various user segments\nB2C - A Total userbase of 6Million+, spread across 100+ countries\nB2B and Enterprise Clients - 70+ clients globally\nB2G (Business to govt.), where Wysa is part of govt. initiatives or is offered as a service to Govt. employees - this is currently active in India, UK, and Singapore)\n\nWe build for data residency when we work with governments and organisations in regions like India, EU, and US, UK, while at the same time fulfilling our SLAs and promise of 99.9% uptime worldwide! We take pride in\n1. Privacy and security by design\n2. Heavily optimising our cloud and infra costs\n3. Constantly innovating and leading the industry in using GenAI safely\n4. Constantly evolving the way we reach our users, including building for voice, outreach over whatsapp etc.\n\n\nQualifications\nHas a total of 6+ Years of experience with at least 2-3 years in leadership role, leading a team of 4+ developers\nYou have a knack for solving problems involving providing the service to users at scale while ensuring 1",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'Career development', 'Networking', 'Database design', 'Cloud', 'Healthcare', 'microsoft', 'Analytics', 'Python', 'Data architecture']",2025-06-13 05:51:08
Oracle Cloud HCM OTL Functional Consultant,Diverse Lynx,4 - 10 years,Not Disclosed,['Bengaluru'],"Role : Oracle Cloud HCM OTL Functional Consultant\n\nThe ideal candidate will be responsible for Taking hand over of OTL module and to provide BAU Support.\nCandidate is expected to be able to configure, optimize OTL module to streamline processes, ensure compliance, and enhance the employee experience.\n\nRelevant experience\n7 to 10 Years overall HCM Functional and\n3 4 years in OTL module\n\nPrimary Skills\nOracle Cloud HCM Functional Skills Core HR, OTL Module\n\nExperience\nMust have worked on configurations/ set-ups, Ticket based support for Oracle Time and Labor (OTL)\nStrong understanding of HR processes\nDefine time entry\nGenerate Data Dictionary Time Attributes-processing,\nand device processing configurations, including entry field and layouts,\ntime categories and consumers, validation and calculation rules,\ngroups and profiles.\nSet up time entry interfaces and approval workflows.\nIntegrate OTL with other HCM modules.\nExperience with data migration, system integrations, and report development.\nOTL Implementation experience will be an added advantage.\n\nQualifications:\nBachelors degree in human resources, Information Technology, or a related field.\nExcellent problem-solving, analytical, and communication skills.\nOracle HCM Cloud certification is a plus.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Data dictionary', 'Data migration', 'HR processes', 'HCM', 'Analytical', 'OTL', 'Cloud', 'HR', 'Oracle', 'Information technology']",2025-06-13 05:51:10
SAP RAR professionals,Clifyx Technology,7 - 15 years,Not Disclosed,['Bengaluru'],"Number of Openings*\n1\nECMS RQ# *\n529196\nDuration of contract*\n6 months\nTotal Years of Experience*\n8+ Years\nRelevant Years of Experience\n7-15 years of work experience (certification is a preference)\nDetailed JD *(Roles and Responsibilities)\nMin. 5 years of SAP RAR revenue accounting and reporting experience specializing in design,\nConfiguration and RAR integration with FI, SD and or BRIM\nThree full life cycle implementation experience with SAP RAR\nRevenue Accounting and Reporting and Revenue recognition Rev Rec implementations.\nExperience with RAR data migration both with OCM Optimized Contract Management and CCM Classical Contract Management\nDetail understanding of order to cash process, SD FICO modules integration BRIM process, IFRS 16 COPA and Revenue Recognition process\nKnowledge of BRF is highly desired\nExperience in solution design and configuration of business processes within RAR SD and FI CO\nHas demonstrated project leadership skills by coordinating cross functional teams and focused on delivering detail designs to align with industry best practices\nPrior experience in high tech or software industries with good understanding of accounting concepts\nRequire strong verbal and written communication skills including business and functional requirements management level presentations and proposals presentation\nExperience in S4 HANA is preferred\nCandidates with bachelors in finance CPA CPA equivalent preferred.\nSAP Revenue Accounting and Reporting functionalities including revenue recognition contract management, and revenue reporting. Experience of configuring and customising SAP RAR to align with specific business requirements and compliance with revenue recognition standards.\nBuilding reports and analytics within SAP RAR to track revenue compliance, and other key financial indicators.\nAnalyse and optimize SAP RAR for performance, particularly in large volume transaction environments.\nKnowledge of international financial reporting standards IFRS 15 and American standards ASC 606 for revenue recognition and how they are implemented in SAP RAR.\nUnderstanding of SAP S4HANA Finance particularly in areas related to revenue and cost accounting.\nIntegrating SAP RAR with other SAP modules like SAP SD SAP FI CO and external systems. Using integration technologies and protocols such as IDocs BAPIs and web services.\nSkills in data migration strategies and tools for SAP RAR, including understanding of data structures and requirements specific to revenue accounting.\nExperience with a variety of delivery methodologies including agile and tools such as Service Now and JIRA.\nGood knowledge on SAP RAR Customized Report and Enhancement Requests.\nMandatory skills*\nSAP RAR\nDesired skills*\nSAP RAR\nClient Name (for internal purpose only)*\nHPE\nDomain*\nHi Tech",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Data migration', 'SAP', 'Financial reporting', 'Billing', 'Agile', 'Data structures', 'JIRA', 'IFRS', 'Analytics', 'FICO']",2025-06-13 05:51:12
SAP EHS professionals,Clifyx Technology,10 - 15 years,Not Disclosed,['Bengaluru'],"Number of Openings\n1\nECMS Request no\n528681\nTotal Yrs. of Experience*\n10 +Yrs.\nRelevant Yrs. of experience*\n8 + Yrs.\nJob Description\nConsultants with implementation experience in SAP Environmental Management and SAP Risk Assessment with atleast two end to end implementation cycle. Candidate should be able to design, implement and test SAP Environmental Management and Risk Assessment Modules. Perform data migration and be eligible to configure, conceptualize and test processes on Emission Management, Compliance Requirements, Permit Management, Risk Assessment and Manage EHS Risk by Identifying, evaluating and monitoring risks\nMandatory skill\nSAP EHS\nDesired skills*\nSAP EHS\nDomain*\nSAP EHS\nVendor billing rate*\nPrecise Work Location\nOffshore\nBG Check\nPost Onboarding\nDelivery Anchor for screening, interviews and feedback*\nIs there any working in shifts from standard Daylight (to avoid confusions post onboarding) *\nNormal Shift",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Environment management', 'Data migration', 'Compliance', 'SAP EHS', 'Risk assessment', 'Billing', 'Conceptualization', 'Vendor', 'Management', 'Monitoring']",2025-06-13 05:51:14
SAP MDG professionals,Clifyx Technology,8 - 10 years,Not Disclosed,['Bengaluru'],"Number of Openings*\n1\nECMS Request no in sourcing stage *\n526936\nDuration of contract*\n5-6 months\nTotal Yrs. of Experience*\n8-10 Years\nRelevant Yrs. of experience* (Total and Relevant cannot be the same which results in sourcing irrelevant talents)\n8 years\nDetailed JD *(Roles and Responsibilities)\nShould have good master data management skills including developing and working with data migration tools.\nCertification in SAP would be an added advantage.\nSAP Master Data Management configuration and customization\nData modelling: Understand types of master data and could design data models that would reflect org structures.\nData quality management: Data cleansing, validation, and standardization\nData modelling: Well versed with Process Modelling, Workflow, BRF+ and related Badi enhancement in MDG.\nIntegration: Integrating of SAP Master Data Management with other sub systems\nLife cycle management of master data, Data quality management and Data Consolidation.\nIntegration between SAP and Non-SAP systems and Workflow management.\nIntegration with 3rd Party system\nHaving ABAP development and coding experience with custom logics user exit and AMDP and Badi enhancement.\nAt least 1 End to end S4 or ECC implementation/ Rollout experience. Experience in SAP Data Migration techniques/tools like LTMC/ LSMW/ BAPI / Batch Recording / BODS / SDI/EDI/ Win-shuttle.\nWell-versed with Data Migration objects which include transactional as well as master data objects like Customers, Vendors, Materials, pricing condition, BOM, Business Partner, Hierarchy Maintenance, Open AR, open AP, GL balances, open SOs, Open POs, etc.\nMust have good understanding of the master data integration and impact among SD, MM, TPM (CRM) and FI functional areas.\nMust have good understanding on SAP S/4 data integration with SAP ARIBA SLP and Salesforce.\nMust have knowledge of SAP Master data tables and ability to navigate SAP. Ability to work with functional team members to organize data loads and validation.\nWell-versed with all data migration activities from an implementation/rollout perspective\nExperience in handling critical incidents and escalations.\nStrong knowledge on ITIL processes specially on Problem, Change and Incident Management.\nFlexible to work for 24*7 team and ensure the coverage.\nExcellent verbal and written communication skills.\nCommitment to continuous learning and staying updated on the latest Tech knowledge and best practices.\nMandatory skills*\nAt least 1 End to end S4 or ECC implementation/ Rollout experience. Experience in SAP Data Migration techniques/tools like LTMC/ LSMW/ BAPI / Batch Recording / BODS / SDI/EDI/ Win-shuttle.\nWell-versed with Data Migration objects which include transactional as well as master data objects like Customers, Vendors, Materials, pricing condition, BOM, Business Partner, Hierarchy Maintenance, Open AR, open AP, GL balances, open SOs, Open POs, etc.\nDesired skills*\nhould have good master data management skills including developing and working with data migration tools.\nCertification in SAP would be an added advantage.\nSAP Master Data Management configuration and customization\nData modelling: Understand types of master data and could design data models that would reflect org structures.\nData quality management: Data cleansing, validation, and standardization\nData modelling: Well versed with Process Modelling, Workflow, BRF+ and related Badi enhancement in MDG.\nIntegration: Integrating of SAP Master Data Management with other sub systems\nLife cycle management of master data, Data quality management and Data Consolidation.\nIntegration between SAP and Non-SAP systems and Workflow management.\nIntegration with 3rd Party system\nHaving ABAP development and coding experience with custom logics user exit and AMDP and Badi enhancement.\nDomain*\nRetail\nApprox. vendor billing rate excluding service tax* (Currency should be in relevance to the candidate work location)\n10000 INR/day\nDelivery Anchor for screening, interviews and feedback*\nPrecise Work Location* (E.g. Bangalore Infosys SEZ or STP)\nBangalore/Pune/Hyderabad STP\nBGCheck (Pre onboarding Or Post onboarding)\nAny client prerequisite BGV Agency*\nPost Onboarding\nIs there any working in shifts from standard Daylight (to avoid confusions post onboarding) *\nWill be confirmed",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data migration', 'SAP', 'Master data management', 'Coding', 'Billing', 'EDI', 'ABAP', 'STP', 'CRM', 'Salesforce']",2025-06-13 05:51:15
Oracle Cloud Technical Support Consultant,BAJAJ FINSERVE,6 - 9 years,Not Disclosed,['Bengaluru'],"POSITION : Oracle Cloud Technical Support Consultant (OIC)\nExperience : 6+  Yrs\nDuration : 6 Months can be extended (post that C2H)\nLocation : Bangalore  (WFO but on case to case basis hybrid), if the candidate is too good they are okay for permanent remote as well.\nShift: US Shift (6PM to 3AM)( Cab facility is available)\nMandatory skills: OIC, VBCS, BIP/OTBI, PCS/OPA\nJob Description:\nResponsibilities:\nProvide day-to-day support and quality assurance for a variety of customers\nYouâ€™ll be responsible for assisting our clients based in the United States during their business hours. This position requires flexibility, excellent communication skills, and the ability to work effectively across US time zones and manage the significant time difference (ranging from 9.5 to 12.5 hours) between India and specific US time zone.\nDesign, develop and support integrations in OIC to Oracle ERP Cloud including making REST and SOAP calls, FBDI File Uploads.\nDesign, develop and support integrations in OIC to Oracle ERP Cloud including extracting Oracle ERP Cloud data using BI Publisher reports, analysis and OTBI Reports.\nDesign and develop customizations using Visual Builder, ADF and Process Builder in OIC to Oracle ERP Cloud\nProvide hands-on technical and development support for implemented Oracle ERP Cloud modules\nProvide hands-on technical and development support for implemented integrations in OIC to Oracle ERP Cloud.\nProvide hands-on technical and development support for implemented customizations using Visual Builder and Process Builder in OIC to Oracle ERP Cloud\nGather and document business requirements on IT incidents and change work.\nDocument and manage technical specifications and software packages \nAssist in defining and optimizing simple yet effective business processes and drive change within the organization through negotiation and consensus-building \nHelp ensure that ERP initiatives follow the proper planning, scheduling and management processes \nManage on-time changes delivery and business expectations and ensure internal customer satisfaction \nProvide hands on analysis, design, testing, implementation and post implementation support utilizing prescribed software design lifecycle techniques and system documentation techniques (AIMS/OUM)\nLiaise directly with clients to ensure all requests for change are properly designed, assessed, prioritized, and managed through to completion.\nAssist with transitioning clients into support post Cloud implementation projects and from competitors.\nIdentifying persistent problems and work with key stakeholders to address the root causes.\nSupport all aspects of Cloud Quarterly Releases from Impact Assessment through testing and defect support \nCreating test plans and coordinating testing with the different stakeholders\nQualifications:\nA minimum of 7 yearsâ€™ experience as a technical support analyst or similar role.\nOracle ERP Cloud and Oracle Integration Cloud experience.\nOracle ERP and HCM integration delivery using Oracle Integration Cloud\nExperience of system interfaces (predominantly on a PaaS Cloud)\nProvide expert-level technical knowledge in Oracle PaaS products, including OIC, VBCS, PCS/OPA, ATP, and others.\nExperience building integration in OIC using REST/SOAP Services.\nExperience building integrations in OIC, building and Uploading FBDI Files to Oracle ERP Cloud\nKnowledge of Oracle Interface tables in financial and procurement modules.\nHands-On Experience of XSLT\nHands-On Experience on data migration/integration methods i.e. SOAP and Rest Web Services, FBDI and ADF DI\nHands-On Experience on reporting tools such as OTBI, BI Publisher\nHands-On development of packages and functions using SQL/PLSQL and exposing them as REST using ORDS.\nGood Knowledge on building custom ESS jobs\nHands-on with development & unit testing of integration components & web services (SOAP/REST) using OIC\nHands-on with development & unit testing of VBCS components\nHands-on with development & unit testing of ADF components using Jdeveloper\nTechnical requirements: OIC, FBDI files, OTBI Reports, SQL, PL/SQL, Oracle BI Publisher, VBCS, PCS/OPA, ATP & Cloud knowledge\nMust have good experience translating business requirements and design into technical solutions \nITIL process Knowledge\nAbility to research, learn, troubleshoot and support complex system customisations\nWillingness to operate and progress in areas that are outside of previous experience\nAbility to multi-task and prioritise across concurrent workload may be required.\nExcellent written and verbal communication\nExperienced user of defect tracking systems, including the extraction of key data for weekly reporting and KPI tracking.\nDesirable Skills: \nHands-On Experience on reporting tools such as Smart View and FRS reports\nTechnical requirements: BPM workflows, Application Composer, OBIEE/OBIA/OAC\nOracle Cloud Technical Certification\n\n\nExcellent Communication Skills\n\nNotice Period : Immediate Joiners ( Who can join in the month of June )\n\nInterested Candidate Share Resume at dipti.bhaisare@in.experis.com",Industry Type: Insurance,Department: Other,"Employment Type: Part Time, Temporary/Contractual","['PCS', 'Vbcs', 'Otbi Reports', 'Oic', 'Bip', 'Opa']",2025-06-13 05:51:17
Database Developer,Aavas Financiers,3 - 7 years,Not Disclosed,['Bengaluru'],"Responsibilities:\nDesign, develop, and implement scalable data strategies aligned with business goals, including data modelling, migration and warehousing.\nIntegrate and correlate data from different business platforms/departments.\nAnalysing, planning, and defining data architecture framework for all the data warehouse components, ensuring a flexible architecture to support the companys current and future growth.",,,,"['SQL', 'Oracle RAC', 'Redshift', 'Python']",2025-06-13 05:51:19
Oracle Database Administrator Lead,Happiest Minds Technologies,8 - 12 years,12-19 Lacs P.A.,['Bengaluru( Electronics City Phase 1 )'],"JD for Oracle DBA:\nProvisioning and configuring scalable relational database management systems in the cloud. Migrating and converting data from on-premises relational database management systems to database systems in the Cloud. Setting up database monitoring, backups and restores, database refreshes (e.g., cloning, export/import), and database encryption to ensure compliance to DPEP standards Performing database performance monitoring and executing performance analyses of whole databases and specific queries. Developing scripts and stored procedures. Installing and configuring data replication tools. Using WDAT's Oracle Golden Gate technology (or equivalent Data Migration tools) to replicate and distribute data. Building, testing and tuning data migration pipelines",,,,"['Golden Gate', 'Oracle DBA', 'Oracle Golden Gate']",2025-06-13 05:51:20
Business Analyst - Finance and Accounting,Bahwan CyberTek,6 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Business Analyst - Finance and Accounting\nLocation: Bengaluru, Pune, Hyderabad, Indore\nRole: Business Analyst - Finance and Accounting\nJob Summary:\nWe are seeking a highly skilled Business Analyst with expertise in Finance and Accounting to support SAP ERP implementations. The ideal candidate will have experience in developing BRD, FRD, and RTM while ensuring seamless ERP deployment and process optimization.\nKey Responsibilities:\nGather, analyse, and document business requirements for SAP Finance & Accounting modules.\nPrepare BRD (Business Requirement Document) capturing business objectives, process flows, and key system needs.\nDevelop FRD (Functional Requirement Document) detailing SAP functionalities, configurations, and system behavior.\nMaintain RTM (Requirements Traceability Matrix) to track requirements from inception to implementation, ensuring alignment.\nConduct gap analysis and recommend SAP solutions for process improvements.\nWork closely with SAP consultants, developers, and finance teams to ensure optimal system configurations.\nFacilitate requirements gathering workshops with stakeholders and document findings.\nAssist in data migration, reconciliation, and financial reporting enhancements within SAP.\nLead UAT (User Acceptance Testing) sessions for financial workflows and ERP validation.\nProvide training and support for SAP Finance module users and stakeholders.\nEnsure regulatory compliance, financial reporting standards, and internal controls.\nCollaborate on post-go-live support, change management, and continuous system improvements.\nRequired Skills & Qualifications:\nBachelor's or Master's degree in Finance, Accounting, Business Administration, or related field.\n5-8+ years of experience as a Business Analyst, focusing on Finance & Accounting ERP implementations.\nProven expertise in SAP FI/CO, S/4HANA, and ERP system deployment.\nStrong understanding of BRD, FRD, and RTM methodologies for ERP projects.\nExperience with business process modeling (BPM) and workflow automation.\nProficiency in SAP data migration, integration, and testing strategies.\nFamiliarity with SQL, Power BI, Tableau, and financial analytics tools.\nKnowledge of Agile, Scrum, and Lean methodologies in ERP implementations.\nStrong problem-solving, analytical, and stakeholder management skills.\nExcellent documentation, presentation, and communication skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analyst - Finance and Accounting', 'Accounting', 'Finance', 'SAP ERP Implementation', 'SAP FI/CO', 'SAP ERP', 'SAP solutions', 'power BI', 'RTM', 'Tableau', 'SQL', 'Agile Methodology', 'S/4HANA', 'ERP Implementation', 'BRD', 'SAP data migration', 'Agile', 'FRD', 'Scrum', 'SAP Implementation', 'Sap Integration']",2025-06-13 05:51:22
QAD ERP Functional consultant,Pi Square Technologies,8 - 13 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Hi ,\n\nHope all doing well!!\n\nHere I am sharing job opportunity for the position of QAD ERP Functional consultant for one of my client.If any one interested please share cvs at pravallika.chitikela@pisquaretech.com\n\nPlease find below detailed JD:\nQualifications:\nBachelors degree in Information Technology, Business Administration, Supply Chain Management, or related field.\n8+ years of experience as an ERP Functional Consultant, with a strong focus on manufacturing industry processes.\n5+ years of hands-on experience with QAD ERP systems.\nIn-depth knowledge of manufacturing processes such as production, inventory management, procurement, and supply chain operations.\nStrong understanding of ERP modules such as Production, Materials Management, Inventory, Procurement, and Finance.\nProven ability to implement ERP systems end-to-end, including requirement gathering, configuration, testing, deployment, and post-go-live support.\nExperience in data migration from legacy systems to ERP solutions.\nExcellent problem-solving, communication, and interpersonal skills.\nAbility to manage multiple projects and prioritize tasks effectively.\nExperience in project management methodologies (Agile, Waterfall) is a plus.\nAbility to work independently and in a team environment.\nPreferred Skills:\nExpertise in QAD ERP\nKnowledge of other manufacturing ERPs such as SAP, Oracle, or Infor is an advantage.\nExperience in ERP integration with other systems such as MES, PLM, and CRM.\nCertification in QAD or relevant ERP systems is a plus.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['QAD', 'ERP Functional', 'ERP Integration']",2025-06-13 05:51:24
Etl Lead,Innominds Software,10 - 20 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","table {mso-displayed-decimal-separator:""\\.""; mso-displayed-thousand-separator:""\\,"";} tr {mso-height-source:auto;} col {mso-width-source:auto;} td {padding-top:1px; padding-right:1px; padding-left:1px; mso-ignore:padding; color:black; font-size:11.0pt; font-weight:400; font-style:normal; text-decoration:none; font-family:""Aptos Narrow"", sans-serif; mso-font-charset:0; text-align:general; vertical-align:bottom; border:none; white-space:nowrap; mso-rotate:0;} .xl73 {border:.5pt solid black; white-space:normal;} 10+ years of experience in Quality Assurance, focusing on ETL testing, data validation, and data migration projects.\n* Proven experience creating detailed test cases, test plans, and test scripts.\n* Hands-on experience with ETL tools like Talend (preferred), Informatica PowerCenter, or DataStage.\n* Proficiency in SQL for complex query writing and optimization for data validation and testing.\n* Experience with cloud data migration projects, specifically working with databases like Snowflake.\n* Strong understanding of semi-structured data formats like JSON and XML, with hands-on testing experience.\n* Proven ability to lead QA efforts, manage teams, and coordinate with on-shore and off-shore teams effectively.\n* Strong analytical and troubleshooting skills for resolving data quality and testing challenges.\nPreferred Skills:\n* Experience with automated testing tools and frameworks, particularly for ETL processes.\n* Knowledge of data governance and data quality best practices.\n* Familiarity with AWS or other cloud-based ecosystems.\n* ISTQB or equivalent certification in software testing.",,,,"['ETL', 'ETL Tool', 'ETL Testing', 'Etl Informatica', 'Informatica', 'SQL']",2025-06-13 05:51:26
salesforce,Espire Infolabs,7 - 12 years,Not Disclosed,['Gurugram'],"Role - Salesforce\nLocation - Hybrid - Gurgaon / Remote\nExperience - 7+ years\nJob Summary:\nWe are seeking highly skilled Salesforce Lead Developers with strong hands-on experience in Service Cloud, Einstein for Service for Service, and Agent Console (Agent Workspace).\nThe ideal candidates will lead technical design, development, integration, and enhancements for customer service initiatives, working closely with business and technical stakeholders.\nKey Responsibilities:\nLead the design, development, and implementation of Salesforce Service Cloud solutions including Case Management, Omnichannel, Knowledge Base, and CTI integration.\nArchitect and develop advanced solutions leveraging Salesforce Einstein Bots, Einstein Case Classification, Einstein Article Recommendations, and Next Best Action features.\nCustomize and enhance Agent Console/Agent Workspace to optimize the agent experience (screen flows, macros, quick actions, guided setups).\nBuild and maintain Lightning Components (LWC), Apex Classes, Triggers, Flows, and Process Builders.\nIntegrate Salesforce with external systems using REST/SOAP APIs, Mulesoft (if applicable).\nConduct code reviews, enforce best practices, and guide junior developers.\nCollaborate with Product Owners, Architects, Admins, and QA to deliver scalable solutions.\nParticipate in technical workshops, solution design discussions, and sprint planning ceremonies.\nCreate technical documentation, deployment scripts, and maintain version control standards.\nEnsure solutions align with Salesforce governance, security, and architecture best practices.\nTechnical Skills Required:\nExtensive experience with Salesforce Service Cloud setup, configuration, and optimization.\nStrong expertise in Einstein for Service (Bots, Article Recommendation, Case Classification).\nDeep knowledge of Agent Console/Agent Workspace customization.\nProficiency in Apex (Classes, Triggers, Batch, Schedulers), Lightning Web Components (LWC), and Visualforce.\nHands-on experience in Salesforce Flows, Omnichannel Setup, Macros, Quick Text.\nExperience integrating Salesforce with third-party systems via REST/SOAP APIs.\nUnderstanding of Salesforce DX, Git based version control, and DevOps pipelines (Gearset, Copado preferred).\nKnowledge of Data Migration using Data Loader, Workbench, or APIs.\nGood understanding of Salesforce Security (Profiles, Roles, Permission Sets, Shield).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Einstein', 'Salesforce', 'Lightning', 'LWC', 'Apex']",2025-06-13 05:51:27
MDM Developer,NetApp,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nParticipates in reviewing, analyzing, and modifying client/server applications and systems.\nJob Requirements\nDevelop and maintain integrations between CDM and other systems, such as CRM, order management, and other boundary systems.\nCustomize and extend MDM functionality using Oracle tools, such as Oracle Integration Cloud, Oracle Application Composer, Oracle Visual Builder.",,,,"['data management', 'web services', 'unit testing', 'groovy scripting', 'data migration', 'tools', 'oracle fusion', 'master data management', 'sql', 'plsql', 'cloud', 'operations', 'java', 'spark', 'oracle erp', 'visual', 'etl', 'pubsub', 'rest', 'python', 'oracle', 'software testing', 'application', 'mdm', 'integration', 'oracle sql', 'data integration']",2025-06-13 05:51:29
Team center developer,tekskill,4 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Responsibilities\nDevelop and customize solutions in Siemens Teamcenter based on business requirements.\nImplement BMIDE configurations including data model changes, business objects, and properties.\nDesign and develop ITK and SOA-based server-side customizations.\nWork on Active Workspace Client (AWC) customizations including stylesheets, XRTs, and client extensions.\nTroubleshoot and resolve issues related to Teamcenter performance, customization, and user support.\nParticipate in data migration activities using tools like Import/Export (IPEC), BMIDE XML, or custom scripts.\nSupport Teamcenter integrations with CAD tools (NX, CATIA, etc.) and other enterprise applications (SAP, CRM).\nDevelop and maintain workflows, handlers, and process templates.\nCollaborate with functional teams to understand requirements and deliver scalable PLM solutions.\nProvide technical documentation, code reviews, and mentoring for junior team members if needed.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ITK', 'team center', 'SOA', 'Bmide']",2025-06-13 05:51:31
Marketing Analytics - TL / Assistant Manager,Leading Client,3 - 8 years,Not Disclosed,['Bengaluru'],Candidate Expectations\n3--12 years of work experience\nFlexible to work in shift as per client requirement\nCertification(s) Preferred:\nAdobe Experience Platform\nAdobe Real-Time Customer Data Platform\nAdobe Customer Journey Analytics\nJob Responsibilities\nUtilizing previous experience in CDPs (Customer data platforms) such as Adobe Experience Platform (RTCDP) or similar products;\nHaving an enhanced understanding of customer profile segmentation and experience in 360 degree view of customers in CDP for further analytical processing and decision making;\nShowcasing proven track record of managing successful CDP implementation/management and delivering capabilities that drive business growth;\nDemonstrating work experience in data architecture and data modeling; preferably with experience working in CDP CRM paid media social and offline data;\nEnsuring an enhanced understanding of data-driven marketing analytics and relevance / usage of real-time event data and associated attributes;\nSetting strategic direction and driving execution through collaboration with a cross-functional team;\nDemonstrating familiarity with CRM and Marketing Automation platforms (i.e. Salesforce Sales Cloud Salesforce Marketing Cloud etc.);\nHaving extensive hands on expertise in implementing/administering Adobe Experience Cloud Products and marketing automation platforms and technologies;\nEnsuring an enhanced understanding of identity resolution components and the enabling technology i.e. profile merge rules identity graph identity service provider etc.;\nWorking with audience-based digital marketing strategy either at an agency or brand;\nDriving project management through a full lifecycle including the ability to prioritize sequence execute and deliver projects on time and on budget;\nTranslating business requirements and objectives into segmentation strategies and audience building logic;\nOwning Adobe AEP and driving proactive platform ownership that is directly aligned to the day-to-day delivery of marketing tactics and closely connected to the other digital marketing teams to develop capabilities and manage technology roadmap;\nHaving oversight of the CDP technology solution (e.g. oversee steady state support teams interact with business owner plan for break / fix and other enhancements / maintenance);\nDriving how customer and prospect information is unified across sales marketing and service channels;\nCollaborating with Insights team to refine and optimize measurement process;\nImplementing and refining Adobe Products and marketing automation system;\nCollaborating with Marketing leadership to evolve the use of data within the marketing function allowing us to stay ahead responding to clients in real-time and limiting redundancy.\nContact Person : - Subhikshaa\nContact Number : - 9840114687,Industry Type: BPM / BPO,Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['Adobe Experience Platform', 'Adobe Customer Journey Analytics', 'Salesforce Sales Cloud', 'Sales Accreditation', 'Paid Media', 'Adobe Real-Time Customer Data Platform', 'Salesforce Marketing Cloud', 'Adobe Experience Cloud', 'Marketing Automation platforms', 'CRM']",2025-06-13 05:52:07
Workday Integration Consultant,Ekloud Data Labs,5 - 8 years,Not Disclosed,"['Pune', 'Ahmedabad', 'Bengaluru']","Job Title: Workday Integration Consultant\nLocation: Remote\nUAN: Mandatory\nExperience Required: 5+ Years in Workday Implementation & Configuration\nKey Responsibilities:\nDesign, develop, and maintain integrations between Workday HCM and external/internal systems using EIB, Workday Studio, Core Connectors, Document Transformation, and Cloud Connect for Benefits.\nConfigure and support Workday HCM modules, business processes, and reporting frameworks.\nBuild and troubleshoot complex integrations, ensuring seamless data flow and system interoperability.\nPerform integration testing, data validation, and post-migration support for critical HCM implementations.\nLeverage Web Services (SOAP/REST) and APIs for system communication and automation.\nSupport Workday-related data migration activities, including mapping, transformation, and loading of PII-sensitive information.\nCollaborate with cross-functional teams during M&A (mergers & acquisitions) for integration and transition support.\nDevelop technical and functional documentation and participate in project deliverables and status updates.\nIdentify integration risks early and provide mitigation strategies.\nContinuously monitor and optimize existing integrations for performance and accuracy.\nRequired Skills:\nMinimum 5 years of hands-on experience in Workday integration and implementation.\nProficiency in EIB, Studio, Cloud Connect, PECI, PICOF, CCW, and Document Transformation.\nStrong understanding of Workday business objects, custom objects, and data model.\nExpertise in Web Services (WSDL, SOAP, REST) and XML/XSLT development and testing.\nExperience with data transformation, migration, and integration testing.\nFamiliarity with data privacy and security protocols (e.g., PII handling).\nExcellent communication, documentation, and stakeholder coordination skills.\nPreferred Qualifications:\nWorkday Integration Certifications highly preferred.\nExperience with Workday to non-Workday HCM migrations.\nM&A experience in HCM platforms is a strong advantage.\nExposure to Workday reporting and analytics tools is a plus.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Workday Integration', 'Workday HCM', 'Document Transformation', 'data model', 'PICOF', 'custom objects', 'WSDL', 'XML/XSLT', 'Studio', 'SOAP', 'security protocols', 'REST', 'Workday business objects', 'integration testing', 'CCW', 'migration', 'data privacy', 'Cloud Connect', 'data transformation', 'PECI', 'EIB']",2025-06-13 05:52:09
Project Delivery Lead,Exavalu,5 - 10 years,Not Disclosed,[],"We are seeking a Project Delivery Lead with strong experience in delivering data and analytics projects. The ideal candidate will have more than 10 years of overall experience with a solid understanding of the end-to-end data value chain, with specific expertise in data modelling, enterprise data warehouse (EDW) concepts, and data mart design. Experience managing technical teams and delivering projects in cloud and hybrid environments is essential.\n  Key Responsibilities:\nLead end-to-end delivery of data and analytics initiatives, ensuring alignment with business goals and timelines.\nWork closely with data architects, engineers, and analysts to design and deliver scalable data solution\nDrive the development and implementation of entire data journey from source to consumption layer(BI)\nLead and Coordinate data migration and modernization activities, including movement to cloud platform\nInterface with business stakeholders to understand reporting and analytics needs, and translate them into technical requirements\nRequired Skills & Experience:\nStrong understanding of data modelling principles, star/snowflake schemas, and data warehouse architecture\nHands-on experience managing projects involving EDW, data marts, and data integration pipelines\n5+ years of technical project management experience in the data and analytics space\nFamiliarity with data migration methodologies and cloud platforms (AWS preferred)\nExperience with Modern BI tools such as Tableau/Power BI and/or legacy BI tools such as SAP BusinessObjects (BO) will be preferred\nStrong communication and coordination skills to work across technical and business teams\nPreferred Qualifications:\nExperience with AWS data services (eg, Redshift, S3, Glue, Athena)\nUnderstanding of Insurance domain, particularly Property & Casualty (P&C)\nExposure to Agile or hybrid delivery models; PMP or Scrum certification is a plus",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Delivery Lead', 'PMP', 'Data migration', 'SAP', 'Data modeling', 'Project management', 'Agile', 'Scrum', 'Project delivery', 'Analytics']",2025-06-13 05:52:11
NEO4j / Neptune Developer,Mobile Programming,3 - 5 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced NEO4j / Neptune Developer to join our team in Hyderabad\nIn this role, you will be responsible for designing, implementing, and optimizing graph-based solutions using NEO4j or Amazon Neptune databases\nYou will collaborate with cross-functional teams to integrate and deploy graph technologies that solve complex business problems\nThis is a fantastic opportunity for someone who thrives in a dynamic environment and is excited about leveraging graph databases to create innovative solutions\nKey Responsibilities:\nDesign, develop, and maintain graph database models using NEO4j or Amazon Neptune\nDevelop and implement graph query languages like Cypher (for NEO4j) or SPARQL for efficient data retrieval\nOptimize graph database performance for large-scale data and high-volume queries\nCollaborate with teams to identify business requirements and design graph-based data models\nIntegrate graph database solutions with existing systems and applications\nTroubleshoot and resolve performance issues or bugs within the graph database solutions\nContribute to the continuous improvement of the development process, tools, and techniques\nProvide support for data migration and integration of graph technologies with other enterprise systems\nWrite high-quality, clean, and maintainable code, ensuring best practices are followed\nRequired Skills and Qualifications:\n3-5 years of experience in developing with NEO4j or Amazon Neptune\nStrong knowledge of graph database modeling, relationships, and graph theory\nProficiency in Cypher query language (for NEO4j) and SPARQL (for Amazon Neptune)\nHands-on experience with graph analytics and performance tuning\nExperience integrating graph databases with other systems and services\nFamiliarity with NoSQL databases and distributed data architectures\nUnderstanding of cloud-based graph database solutions (eg, AWS Neptune)\nAbility to work in an Agile development environment\nStrong troubleshooting and problem-solving skills\nExcellent written and verbal communication skills\nTechnical Skills:\nNEO4j | Amazon Neptune | Cypher | SPARQL | Graph Database Modeling | NoSQL | Graph Analytics | Python | Java | AWS | Data Migration | ETL | Cloud Solutions | Agile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['NEO4j', 'Java', 'NoSQL', 'Data Migration', 'Agile', 'ETL', 'AWS', 'Graph Analytics', 'Python', 'Cloud Solutions']",2025-06-13 05:52:13
ETL Developer - ODI,KPI Partners,3 - 4 years,Not Disclosed,['Pune'],"About Us:\nKPI Partners is a leading provider of data analytics and business intelligence solutions. We are committed to helping organizations excel through effective data management. Our innovative team focuses on delivering impactful business insights, and we are looking for talented individuals to join us on this journey.\n\nJob Summary:\nWe are seeking an experienced ETL Developer with expertise in Oracle Data Integrator (ODI) to join our dynamic team. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes to extract data from multiple sources and transform it into a format suitable for analysis. You will work closely with business analysts, data architects, and other stakeholders to ensure the successful implementation of data integration solutions.\n\n\n- Design and implement ETL processes using Oracle Data Integrator (ODI) to support data warehousing and business intelligence initiatives.\n- Collaborate with business stakeholders to gather requirements and translate them into technical specifications.\n- Develop, test, and optimize ETL workflows, mappings, and packages to ensure efficient data loading and processing.\n- Perform data quality checks and validations to ensure the accuracy and reliability of transformed data.\n- Monitor and troubleshoot ETL processes to resolve issues and ensure timely delivery of data.\n- Document ETL processes, technical specifications, and any relevant workflows.\n- Stay up-to-date with industry best practices and technology trends related to ETL and data integration.\n\n\n- Bachelor s degree in Computer Science, Information Technology, or a related field.\n- Proven experience as an ETL Developer with a focus on Oracle Data Integrator (ODI).\n- Strong understanding of ETL concepts, data warehousing, and data modeling.\n- Proficiency in SQL and experience with database systems such as Oracle, SQL Server, or others.\n- Familiarity with data integration tools and techniques, including data profiling, cleansing, and transformation.\n- Experience in performance tuning and optimization of ETL processes.\n- Excellent analytical and problem-solving skills.\n- Strong communication and teamwork abilities, with a commitment to delivering high-quality results.\n\n\n- Competitive salary and benefits package.\n- Opportunities for professional growth and career advancement.\n- A collaborative and innovative work environment.\n- The chance to work on exciting projects with leading organizations across various industries.\n\nKPI Partners is an equal-opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.",,,,"['Computer science', 'data cleansing', 'Performance tuning', 'Data management', 'Data modeling', 'Analytical', 'Oracle data integrator', 'Data quality', 'Business intelligence', 'Information technology']",2025-06-13 05:52:15
MDM Developer(IBM Infosphere),RWAVE Software Technologies,6 - 8 years,1-1.5 Lacs P.A.,['Hyderabad'],"Role: Sr. MDM Developer\nWork Location: Hyderabad (Hybrid)\nExperience: 6+ years\nContract duration: 12 months\nJob Description :\nIBM Infosphere MDM Developer with a strong background in Java development. This role is responsible for designing, developing, and maintaining Master Data Management solutions using IBM MDM (Standard or Advanced Edition), and integrating them with enterprise systems through custom Java components and APIs.\nResponsibilities:\nDesign and implement MDM solutions using IBM Infosphere MDM (v11.x or higher).\nDevelop custom business rules, extensions, and services using Java and IBM MDM APIs.\nConfigure and enhance MDM services like party/consumer/member model, suspect duplicate processing, match & merge, etc.\nDevelop integration components using REST/SOAP web services and message queues (e.g., MQ).\nCollaborate with data architects, business analysts, and downstream system teams to define data flows and mappings.\nCreate and maintain MDM batch jobs, data stewardship workflows, and data quality processes.\nTroubleshoot and optimize performance of MDM transactions and database operations.\nParticipate in code reviews, CI/CD pipeline implementation, and DevOps practices.\nProvide documentation, knowledge transfer, and support for production deployments.\nRequired Skills & Qualifications:\n4+ years of experience with IBM Infosphere MDM (Standard/Advanced Edition).\n2+ years of hands-on experience with Java/J2EE development.\nStrong understanding of MDM data models: Party, Product, Account, etc.\nExperience with MDM Workbench, MDM Web Services, and Suspect Duplicate Processing (SDP).\nProficient in RESTful API development and integration using Java frameworks (Spring Boot preferred).\nFamiliarity with DB2, Oracle, or other relational databases.\nExperience with data quality tools, data governance, and data stewardship processes.\nExperience with Agile methodology and version control tools like Git.\n\n\nShare Your CV at:\nCareers@rwavesoftech.com",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Temporary/Contractual","['Java', 'RDBMS', 'Ibm Infosphere', 'Spring Boot', 'Ibm Mdm', 'GIT', 'Agile', 'Workbench', 'SDP']",2025-06-13 05:52:16
SSIS Technical Lead,Sopra Steria,6 - 10 years,Not Disclosed,['Noida'],"Role: SSIS Developer\nSkillset: SSIS, ETL (Extract Transform Load), TSQL\nExperience: 6-8 years\nLocation: Noida and Chennai\nJob Description:\nAt least 3 years of IT experience in Data Migration preferable SAP Data Migration. Extensive experience in ETL process.",,,,"['data management', 'sap sd', 'project', 'data migration', 'data mapping', 't-sql', 'sap s hana', 'data cleansing', 'ms office products', 'etl tool', 'waterfall', 'sap finance', 'sap mm', 'etl', 'powerpoint', 'data analysis', 'sap', 'sap ecc', 'sql server', 'excel', 's', 'visio', 'migration', 'agile', 'ssis', 'sap abap', 'word', 'etl process', 'ms office']",2025-06-13 05:52:19
Power BI Developer,Luxoft,5 - 10 years,Not Disclosed,['Pune'],"Project description\nYou will be working in a global team that manages and performs a global technical control.\nYou'll be joining Assets Management team which is looking after asset management data foundation and operates a set of in-house developed tooling.\nAs an IT engineer you'll play an important role in ensuring the development methodology is followed, and lead technical design discussions with the architects. Our culture centers around partnership with our businesses, transparency, accountability and empowerment, and passion for the future.\n\nResponsibilities\n\nDesign, develop and maintain/support Power BI workflows to take data from multiple sources to make it ready for analytics and reporting.\n\nOptimize existing workflows to ensure performance, scalability and reliability.\n\nSupport the automation of manual processes to improve operational efficiency.\n\nDocument workflows, processes, and best practices for knowledge sharing.\n\nProvide training and mentorship to other team members on Alteryx development.\n\nCollaborate with other members of the team to deliver data solutions for the program.\n\nSkills\nMust have\n\nProficiency in Power BI Desktop, Power BI Service (5+ yrs of experience)\n\nExperience with creating interactive dashboards, custom visuals, and reports.\n\nData Modeling:\n\nStrong understanding of data modeling concepts, including relationships, calculated columns, measures, and hierarchies.\n\nExpertise in using DAX (Data Analysis Expressions) for complex calculations.\n\nSQL and Database Management:\n\nProficiency in SQL to extract, manipulate, and analyze data from databases.\n\nKnowledge of database design and querying.\n\nETL (Extract, Transform, Load) Tools:\n\nExperience with data transformation and cleaning using tools like Power Query, SSIS, or other ETL tools.\n\nNice to have\n\nData Architecture & EngineeringDesign and implement efficient and scalable data warehousing solutions using Azure Databricks and Microsoft Fabric.\n\nBusiness Intelligence & Data VisualizationCreate insightful Power BI dashboards to help drive business decisions.\n\nOther\n\nLanguages\n\nEnglishC1 Advanced\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['power bi', 'power bi desktop', 'sql', 'data modeling', 'dax', 'azure databricks', 'data analysis', 'bi', 'data warehousing', 'data architecture', 'business intelligence', 'dashboards', 'alteryx', 'power query', 'database design', 'etl tool', 'power bi dashboards', 'data transformation', 'ssis', 'etl']",2025-06-13 05:52:21
Starburst Engineer,Luxoft,0 - 4 years,Not Disclosed,['Pune'],"Project description\nYou will be working in a global team that manages and performs a global technical control.\nYou'll be joining Assets Management team which is looking after asset management data foundation and operates a set of in-house developed tooling.\nAs an IT engineer you'll play an important role in ensuring the development methodology is followed, and lead technical design discussions with the architects. Our culture centers around partnership with our businesses, transparency, accountability and empowerment, and passion for the future.\n\nResponsibilities\n\nDesign, develop, and maintain scalable data solutions using Starburst.\n\nCollaborate with cross-functional teams to integrate Starburst with existing data sources and tools.\n\nOptimize query performance and ensure data security and compliance.\n\nImplement monitoring and alerting systems for data platform health.\n\nStay updated with the latest developments in data engineering and analytics.\n\nSkills\nMust have\n\nBachelor's degree or Masters in a related technical field; or equivalent related professional experience.\n\nPrior experience as a Software Engineer applying new engineering principles to improve existing systems including leading complex, well defined projects.\n\nStrong knowledge of Big-Data Languages including:\n\nSQL\n\nHive\n\nSpark/Pyspark\n\nPresto\n\nPython\n\nStrong knowledge of Big-Data Platforms, such as:o The Apache Hadoop ecosystemo AWS EMRo Qubole or Trino/Starburst\n\nGood knowledge and experience in cloud platforms such as AWS, GCP, or Azure.\n\nContinuous learner with the ability to apply previous experience and knowledge to quickly master new technologies.\n\nDemonstrates the ability to select among technology available to implement and solve for need.\n\nAble to understand and design moderately complex systems.\n\nUnderstanding of testing and monitoring tools.\n\nAbility to test, debug, fix issues within established SLAs.\n\nExperience with data visualization tools (e.g., Tableau, Power BI).\n\nUnderstanding of data governance and compliance standards.\n\nNice to have\n\nData Architecture & EngineeringDesign and implement efficient and scalable data warehousing solutions using Azure Databricks and Microsoft Fabric.\n\nBusiness Intelligence & Data VisualizationCreate insightful Power BI dashboards to help drive business decisions.\n\nOther\n\nLanguages\n\nEnglishC1 Advanced\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'python', 'pyspark', 'sql', 'spark', 'azure databricks', 'microsoft azure', 'data warehousing', 'power bi', 'data architecture', 'business intelligence', 'data engineering', 'dashboards', 'tableau', 'apache', 'gcp', 'debugging', 'hadoop', 'data visualization', 'aws', 'engineering design', 'presto']",2025-06-13 05:52:23
Databricks Developer,Luxoft,5 - 10 years,Not Disclosed,['Pune'],"Project description\nYou will be working in a global team that manages and performs a global technical control.\nYou'll be joining Assets Management team which is looking after asset management data foundation and operates a set of in-house developed tooling.\nAs an IT engineer you'll play an important role in ensuring the development methodology is followed, and lead technical design discussions with the architects. Our culture centers around partnership with our businesses, transparency, accountability and empowerment, and passion for the future.\n\nResponsibilities\n\nDesign, build, and manage data pipelines using Azure Data Integration Services (Azure DataBricks, ADF, Azure Functions.)\n\nCollaborate closely with the security team to develop robust data solutions that support our security initiatives.\n\nImplement, monitor, and optimize data processes, ensuring adherence to security and data governance best practices.\n\nTroubleshoot and resolve data-related issues, ensuring data quality and accessibility.\n\nDevelop strategies for data acquisitions and integration of the new data into our existing architecture.\n\nDocument procedures and workflows associated with data pipelines, contributing to best practices.\n\nShare knowledge about latest Azure Data Integration Services trends and techniques.\n\nImplement and manage CI/CD pipelines to automate data and UI testcases and integrate testing with development pipelines.\n\nImplement and manage CI/CD pipelines to automate development and integrate test pipelines.\n\nConduct regular reviews of the system, identify possible security risks, and implement preventive measures.\n\nSkills\nMust have\n\nExcellent command of English\n\nBachelor's or Master's degree in Computer Science, Information Technology, or related field.\n\n5+ years of experience in data integration and pipeline development using Azure Data Integration Services including Azure Data Factory and Azure Databricks.\n\nHands-on with Python and Spark\n\nStrong understanding of security principles in the context of data integration.\n\nProven experience with SQL and other data query languages.\n\nAbility to write, debug, and optimize data transformations and datasets.\n\nExtensive experience in designing and implementing ETL solutions using Azure Databricks, Azure Data Factory or similar technologies.\n\nFamiliar with automated testing frameworks using Squash\n\nNice to have\n\nData Architecture & EngineeringDesign and implement efficient and scalable data warehousing solutions using Azure Databricks and Microsoft Fabric.\n\nBusiness Intelligence & Data VisualizationCreate insightful Power BI dashboards to help drive business decisions.\n\nOther\n\nLanguages\n\nEnglishC1 Advanced\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['security compliance', 'azure data factory', 'pipeline', 'data integration', 'etl process', 'azure databricks', 'continuous integration', 'python', 'microsoft azure', 'power bi', 'data warehousing', 'data architecture', 'warehouse', 'business intelligence', 'sql', 'spark', 'oracle adf', 'etl']",2025-06-13 05:52:24
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Delhi'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:52:26
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Dehradun'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:52:28
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Bhopal'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:52:30
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Chandigarh'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:52:32
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Varanasi'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:52:34
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Patna'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:52:35
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Gwalior'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:52:37
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Pune'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:52:39
PostgreSQL Database Developer,Maimsd Technology,2 - 7 years,Not Disclosed,['Bhopal'],"Our dynamic and growing company is actively seeking an experienced PostgreSQL Database Developer to join our team. As a PostgreSQL Database Developer, you will play a crucial role in designing, implementing, and maintaining our database systems. The ideal candidate should have a strong background in database development, performance optimization, and data modeling.\n\nJob Responsibilities :\n\n- Responsible to design, implement, and maintain database schemas in PostgreSQL and perform data modeling to ensure efficiency, reliability, and scalability.\n\n- Responsible to optimize and tuneSQL queries for improved performance , and also identify and resolve performance bottlenecks in database systems.\n\n- Responsible to manage data migration and integration processes between different systems and ensure data consistency and integrity during the migration process.\n\n- Responsible to develop and maintain stored procedures, functions, and triggers to support application requirements and Implement business logic within the database layer.\n\n- Responsible to Implement and maintain database security policies and manage user roles, permissions, and access control within the database.\n\n- Responsible to implement and oversee database backup and recovery processes and also ensure data availability and reliability.\n\n- Responsible to collaborate with cross-functional teams, including application developers, system administrators, and business analysts, to understand database requirements.\n\n- Responsible to create and maintain documentation related to database design, processes, and best practices.\n\nRequirements\n\nQualifications :\n\n- Bachelors degree in Computer Science, Information Technology, or a related field.\n\n- Proven experience as a Database Developer with a focus on PostgreSQL.\n\n- In-depthknowledge of database design principles, normalization, and data modeling.\n\n- Strong proficiency in writing and optimizing SQL queries.\n\n- Experience with performance tuning and query optimization techniques.\n\n- Familiarity with database security best practices and access control.\n\n- Hands-on experience with data migration, integration, and ETL processes.\n\n- Proficiency in scripting languages (e.g., Python, Bash) for automation tasks.\n\n- Knowledge of backup and recovery processes.\n\n- Excellent communication and collaboration skills.\n\n- Ability to work independently and as part of a team.\n\nPreferred Skills :\n- Experience with PostgreSQL replication and clustering.\n\n- Familiarity with NoSQL databases.\n\n- Knowledge of cloud database solutions (e.g., AWS RDS, Azure Database for PostgreSQL).\n\n- Understanding of DevOps practices and tools",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PostgreSQL', 'AWS RDS', 'Database Performance Tuning', 'Bash Scripting', 'MySQL', 'Azure Database', 'ETL', 'Python']",2025-06-13 05:52:40
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Vijayawada'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang Engineering', 'Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:52:42
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Indore'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang Engineering', 'Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:52:44
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Gurugram'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang Engineering', 'Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:52:46
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Mysuru'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang Engineering', 'Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:52:47
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Jabalpur'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang Engineering', 'Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:52:49
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Mumbai'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang Engineering', 'Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:52:51
SAP QM Functional Consultant - Implementation & Integration,Leading Client,5 - 7 years,Not Disclosed,['Indore'],"Essential Job Functions :\n\n- 5+ years of experience in SAP QM.\n\n- Worked on ECC to S4 Hana migrations / Implementation experience.\n\n- Strong functional skills in QM/PP with at least 1-2 full life cycles.\n\n- Should have done At least one implementation on S/4 HANA.\n\n- Should know basic integration with PP/ MM/ FICO modules.\n\n- Should have handled Implementations, Upgrades, Rollouts, and Systems Integration.\n\n- Should be able to work independently and work with a cross-functional and client-facing team team within an on/offshore model.\n\n- Knowledge of shopfloor quality is an added advantage.\n\n- Proven ability to work creatively and analytically in a problem-solving environment.\n\n- Must be a very good team player with good interpersonal and communication skills.\n\n- SAP Certification and exposure to SAP S/4 HANA will be an added advantage.\n\n- Excellent analytical and problem-solving skills.\n\n- Excellent verbal and written communication skills, and can communicate clearly and concisely.\n\n- Thorough knowledge of traceability reports, COA, QM tables, shop floor QM integration is necessary.\n\n- Configuration and master data deep knowledge.\n\n- Ability to articulate the analysis and design, and build.\n\n- Master data expertise.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP', 'SAP ECC', 'SAP QM', 'S/4 HANA', 'SAP FICO', 'Data Migration', 'SAP PP', 'SAP MM', 'SAP Integration', 'SAP Implementation', 'Functional Consultant', 'SAP Support']",2025-06-13 05:52:53
SAP QM Functional Consultant - Implementation & Integration,Leading Client,5 - 7 years,Not Disclosed,['Mumbai'],"Essential Job Functions :\n\n- 5+ years of experience in SAP QM.\n\n- Worked on ECC to S4 Hana migrations / Implementation experience.\n\n- Strong functional skills in QM/PP with at least 1-2 full life cycles.\n\n- Should have done At least one implementation on S/4 HANA.\n\n- Should know basic integration with PP/ MM/ FICO modules.\n\n- Should have handled Implementations, Upgrades, Rollouts, and Systems Integration.\n\n- Should be able to work independently and work with a cross-functional and client-facing team team within an on/offshore model.\n\n- Knowledge of shopfloor quality is an added advantage.\n\n- Proven ability to work creatively and analytically in a problem-solving environment.\n\n- Must be a very good team player with good interpersonal and communication skills.\n\n- SAP Certification and exposure to SAP S/4 HANA will be an added advantage.\n\n- Excellent analytical and problem-solving skills.\n\n- Excellent verbal and written communication skills, and can communicate clearly and concisely.\n\n- Thorough knowledge of traceability reports, COA, QM tables, shop floor QM integration is necessary.\n\n- Configuration and master data deep knowledge.\n\n- Ability to articulate the analysis and design, and build.\n\n- Master data expertise.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP', 'SAP ECC', 'SAP QM', 'S/4 HANA', 'SAP FICO', 'Data Migration', 'SAP PP', 'SAP MM', 'SAP Integration', 'SAP Implementation', 'Functional Consultant', 'SAP Support']",2025-06-13 05:52:54
SAP QM Functional Consultant - Implementation & Integration,Leading Client,5 - 7 years,Not Disclosed,['Pune'],"Essential Job Functions :\n\n- 5+ years of experience in SAP QM.\n\n- Worked on ECC to S4 Hana migrations / Implementation experience.\n\n- Strong functional skills in QM/PP with at least 1-2 full life cycles.\n\n- Should have done At least one implementation on S/4 HANA.\n\n- Should know basic integration with PP/ MM/ FICO modules.\n\n- Should have handled Implementations, Upgrades, Rollouts, and Systems Integration.\n\n- Should be able to work independently and work with a cross-functional and client-facing team team within an on/offshore model.\n\n- Knowledge of shopfloor quality is an added advantage.\n\n- Proven ability to work creatively and analytically in a problem-solving environment.\n\n- Must be a very good team player with good interpersonal and communication skills.\n\n- SAP Certification and exposure to SAP S/4 HANA will be an added advantage.\n\n- Excellent analytical and problem-solving skills.\n\n- Excellent verbal and written communication skills, and can communicate clearly and concisely.\n\n- Thorough knowledge of traceability reports, COA, QM tables, shop floor QM integration is necessary.\n\n- Configuration and master data deep knowledge.\n\n- Ability to articulate the analysis and design, and build.\n\n- Master data expertise.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP', 'SAP ECC', 'SAP QM', 'S/4 HANA', 'SAP FICO', 'Data Migration', 'SAP PP', 'SAP MM', 'SAP Integration', 'SAP Implementation', 'Functional Consultant', 'SAP Support']",2025-06-13 05:52:56
Microsoft Dynamics NAV and D365 Business Central Consultant,Gemsource It Consulting,7 - 12 years,14-24 Lacs P.A.,['PAN India'],"We are seeking a seasoned Microsoft Dynamics NAV / D365 Business Central Consultant with a strong background in Supply Chain Management (Trade and Logistics) and Finance modules to join our implementation team. The ideal candidate will have proven expertise in end-to-end SCM processes, financial operations, and experience managing full lifecycle implementations. You will work closely with business stakeholders, gather requirements, conduct fit-gap analyses, and deliver tailored solutions to optimize business processes.\nKey Responsibilities:\nLead the implementation of Microsoft Dynamics NAV and D365 Business Central focusing on Supply Chain Management and Finance modules.\nAnalyze business requirements and conduct fit-gap analysis to recommend best practices and solutions aligned with Microsoft Dynamics capabilities.\nConfigure and customize SCM processes including Order to Cash, Procure to Pay, Inventory, Warehouse Management, Inventory Costing, and Inventory Valuation.\nConfigure and support Finance modules including General Ledger, Accounts Payable, Accounts Receivable, Fixed Assets, and Financial Reporting.\nPerform inventory master data setup, inventory transfers, returns, and lifecycle management activities.\nLead requirement gathering sessions, prepare detailed Functional Design Documents (FDD), and create end-to-end business scenarios for SIT.\nConduct Conference Room Pilots (CRP) to demonstrate system functionalities to business users and stakeholders.\nSupport business users through User Acceptance Testing (UAT) and provide guidance to ensure smooth adoption.\nManage data migration activities from legacy systems into Dynamics NAV / D365 Business Central.\nWork effectively in an onsite/offshore delivery model, collaborating across time zones with global teams.\nEnsure compliance with localization requirements and coordinate with legal/regulatory teams.\nPrepare and maintain project plans, status reports, and other implementation documentation.\nCommunicate effectively across all organizational levels, facilitating discussions and managing stakeholder expectations.\nQualifications:\nBachelors degree in Business, IT, Finance, or related field.\nMinimum 7 years hands-on experience in Microsoft Dynamics NAV / D365 Business Central implementations with focus on Supply Chain Management and Finance modules.\nMinimum 2 years experience working on Sales and Purchase modules within Dynamics environments.\nStrong knowledge of supply chain lifecycle including Inventory, Sales, Purchase, and Warehouse Management.\nDemonstrated experience in data migration, fit-gap analysis, and preparing functional documentation.\nExcellent communication and presentation skills with ability to lead client workshops and training sessions.\nExperience working with US-based customers or teams is preferred.\nStrong organizational skills and ability to manage multiple priorities within complex projects.\nKnowledge of localization and regulatory compliance within Microsoft Dynamics solutions.\nPreferred Skills:\nMicrosoft Dynamics certifications related to NAV and D365 Business Central.\nExperience with Agile/Scrum methodologies in ERP implementations.\nKnowledge of Power Platform and integration with Dynamics 365.\nFamiliarity with financial accounting principles and inventory costing methods.",Industry Type: Recruitment / Staffing,Department: Consulting,"Employment Type: Full Time, Permanent","['Microsoft Dynamics NAV', 'D365 Supply Chain Management', 'Onsite Offshore Model', 'ERP', 'Business Central', 'Microsoft Dynamics', 'ERP Navision', 'Dynamics Nav', 'ERP Implementation', 'Supply Chain Logistics', 'Order To Cash', 'D365 Functional', 'Microsoft Dynamics Navision', 'Supply Chain Management']",2025-06-13 05:52:58
SAP FICO Consultant - S/4HANA,Forward Eye Technologies,7 - 12 years,Not Disclosed,['Jaipur'],"We are looking for an experienced SAP FICO Consultant with S/4HANA expertise to join our team. The ideal candidate will have a deep understanding of financial and controlling processes and hands-on experience in implementing or supporting SAP S/4HANA Finance solutions.\n\nKey Responsibilities :\n\n- Configure and implement SAP FICO modules (GL, AP, AR, AA, CO-CCA, CO-IO, COPA, Product Costing, ML).\n\n- Lead and support SAP S/4HANA Finance implementation and migration projects.\n\n- Analyze business requirements and translate them into functional specifications.\n\n- Work with cross-functional teams for integrations with MM, SD, PP, and other SAP modules.\n\n- Perform unit testing, integration testing, and support UAT with the business.\n\n- Provide post-go-live support and troubleshooting for FICO modules.\n\n- Prepare functional documentation and user training manuals.\n\n- Involve in data migration, cutover planning, and month-end/year-end activities.\n\n- Strong understanding of New GL, Universal Journal (ACDOCA), and Central Finance concepts.\n\n- Experience in CO-PA (Account-based and Costing-based) and Margin Analysis in S/4HANA.\n\nMust-Have Skills :\n\n- 6+ years of hands-on experience in SAP FICO with at least 12 full lifecycle implementations in S/4HANA.\n\n- Experience in Universal Journal, Fiori apps for finance, and S/4HANA Finance innovations.\n\n- Proficiency in Cost Center Accounting (CCA), Internal Orders (IO), Profit Center Accounting (PCA), Product Costing,\nand COPA.\n\n- Exposure to integration with SD/MM/PP modules.\n\n- Experience with taxation setup, bank interface, electronic bank statement, and payment configurations.\n\n- Good knowledge of business processes in finance and controlling.\n\nGood to Have :\n\n- Experience with Central Finance and Group Reporting.\n\n- Familiarity with SAP Activate methodology.\n\n- Basic knowledge of ABAP debugging and Fiori configuration.\n\n- Experience with interfaces like Concur, BlackLine, Vertex, etc.\n\n- Knowledge of Agile/Scrum project management.\n\nEducation & Certifications :\n\n- SAP FICO and/or S/4HANA Finance Certification is a plus.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP FICO', 'S/4 HANA', 'SAP MM', 'SAP Integration', 'SAP COPA', 'SAP GL', 'SAP Implementation', 'SAP Support']",2025-06-13 05:52:59
SAP FICO Consultant - S/4HANA,Forward Eye Technologies,7 - 12 years,Not Disclosed,['Lucknow'],"We are looking for an experienced SAP FICO Consultant with S/4HANA expertise to join our team. The ideal candidate will have a deep understanding of financial and controlling processes and hands-on experience in implementing or supporting SAP S/4HANA Finance solutions.\n\nKey Responsibilities :\n\n- Configure and implement SAP FICO modules (GL, AP, AR, AA, CO-CCA, CO-IO, COPA, Product Costing, ML).\n\n- Lead and support SAP S/4HANA Finance implementation and migration projects.\n\n- Analyze business requirements and translate them into functional specifications.\n\n- Work with cross-functional teams for integrations with MM, SD, PP, and other SAP modules.\n\n- Perform unit testing, integration testing, and support UAT with the business.\n\n- Provide post-go-live support and troubleshooting for FICO modules.\n\n- Prepare functional documentation and user training manuals.\n\n- Involve in data migration, cutover planning, and month-end/year-end activities.\n\n- Strong understanding of New GL, Universal Journal (ACDOCA), and Central Finance concepts.\n\n- Experience in CO-PA (Account-based and Costing-based) and Margin Analysis in S/4HANA.\n\nMust-Have Skills :\n\n- 6+ years of hands-on experience in SAP FICO with at least 12 full lifecycle implementations in S/4HANA.\n\n- Experience in Universal Journal, Fiori apps for finance, and S/4HANA Finance innovations.\n\n- Proficiency in Cost Center Accounting (CCA), Internal Orders (IO), Profit Center Accounting (PCA), Product Costing,\nand COPA.\n\n- Exposure to integration with SD/MM/PP modules.\n\n- Experience with taxation setup, bank interface, electronic bank statement, and payment configurations.\n\n- Good knowledge of business processes in finance and controlling.\nGood to Have :\n- Experience with Central Finance and Group Reporting.\n\n- Familiarity with SAP Activate methodology.\n\n- Basic knowledge of ABAP debugging and Fiori configuration.\n\n- Experience with interfaces like Concur, BlackLine, Vertex, etc.\n\n- Knowledge of Agile/Scrum project management.\n\nEducation & Certifications :\n\n- SAP FICO and/or S/4HANA Finance Certification is a plus.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP', 'SAP FICO', 'S/4 HANA', 'SAP MM', 'SAP Integration', 'SAP COPA', 'SAP GL', 'SAP Implementation', 'SAP Support']",2025-06-13 05:53:01
SAP FICO Consultant - S/4HANA,Forward Eye Technologies,7 - 12 years,Not Disclosed,['Chennai'],"We are looking for an experienced SAP FICO Consultant with S/4HANA expertise to join our team. The ideal candidate will have a deep understanding of financial and controlling processes and hands-on experience in implementing or supporting SAP S/4HANA Finance solutions.\n\nKey Responsibilities :\n\n- Configure and implement SAP FICO modules (GL, AP, AR, AA, CO-CCA, CO-IO, COPA, Product Costing, ML).\n\n- Lead and support SAP S/4HANA Finance implementation and migration projects.\n\n- Analyze business requirements and translate them into functional specifications.\n\n- Work with cross-functional teams for integrations with MM, SD, PP, and other SAP modules.\n\n- Perform unit testing, integration testing, and support UAT with the business.\n\n- Provide post-go-live support and troubleshooting for FICO modules.\n\n- Prepare functional documentation and user training manuals.\n\n- Involve in data migration, cutover planning, and month-end/year-end activities.\n\n- Strong understanding of New GL, Universal Journal (ACDOCA), and Central Finance concepts.\n\n- Experience in CO-PA (Account-based and Costing-based) and Margin Analysis in S/4HANA.\n\nMust-Have Skills :\n\n- 6+ years of hands-on experience in SAP FICO with at least 12 full lifecycle implementations in S/4HANA.\n\n- Experience in Universal Journal, Fiori apps for finance, and S/4HANA Finance innovations.\n\n- Proficiency in Cost Center Accounting (CCA), Internal Orders (IO), Profit Center Accounting (PCA), Product Costing,\nand COPA.\n\n- Exposure to integration with SD/MM/PP modules.\n\n- Experience with taxation setup, bank interface, electronic bank statement, and payment configurations.\n\n- Good knowledge of business processes in finance and controlling.\nGood to Have :\n- Experience with Central Finance and Group Reporting.\n\n- Familiarity with SAP Activate methodology.\n\n- Basic knowledge of ABAP debugging and Fiori configuration.\n\n- Experience with interfaces like Concur, BlackLine, Vertex, etc.\n\n- Knowledge of Agile/Scrum project management.\n\nEducation & Certifications :\n\n- SAP FICO and/or S/4HANA Finance Certification is a plus.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP', 'SAP FICO', 'S/4 HANA', 'SAP MM', 'SAP Integration', 'SAP COPA', 'SAP GL', 'SAP Implementation', 'SAP Support']",2025-06-13 05:53:03
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Visakhapatnam'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'GCP', 'PostgreSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS', 'Restful services development']",2025-06-13 05:53:04
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Mangaluru'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'GCP', 'PostgreSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS', 'Restful services development']",2025-06-13 05:53:06
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Ranchi'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'GCP', 'PostgreSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS', 'Restful services development']",2025-06-13 05:53:08
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Coimbatore'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'GCP', 'PostgreSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS', 'Restful services development']",2025-06-13 05:53:10
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Nagpur'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'GCP', 'PostgreSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS', 'Restful services development']",2025-06-13 05:53:11
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Udaipur'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'GCP', 'PostgreSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS', 'Restful services development']",2025-06-13 05:53:13
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Kochi'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'GCP', 'PostgreSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS', 'Restful services development']",2025-06-13 05:53:15
SAP QM Functional Consultant - Implementation & Integration,Leading Client,5 - 7 years,Not Disclosed,['Ahmedabad'],"Essential Job Functions :\n\n- 5+ years of experience in SAP QM.\n\n- Worked on ECC to S4 Hana migrations / Implementation experience.\n\n- Strong functional skills in QM/PP with at least 1-2 full life cycles.\n\n- Should have done At least one implementation on S/4 HANA.\n\n- Should know basic integration with PP/ MM/ FICO modules.\n\n- Should have handled Implementations, Upgrades, Rollouts, and Systems Integration.\n\n- Should be able to work independently and work with a cross-functional and client-facing team team within an on/offshore model.\n\n- Knowledge of shopfloor quality is an added advantage.\n\n- Proven ability to work creatively and analytically in a problem-solving environment.\n\n- Must be a very good team player with good interpersonal and communication skills.\n\n- SAP Certification and exposure to SAP S/4 HANA will be an added advantage.\n\n- Excellent analytical and problem-solving skills.\n\n- Excellent verbal and written communication skills, and can communicate clearly and concisely.\n\n- Thorough knowledge of traceability reports, COA, QM tables, shop floor QM integration is necessary.\n\n- Configuration and master data deep knowledge.\n\n- Ability to articulate the analysis and design, and build.\n\n- Master data expertise.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP QM', 'SAP ECC', 'S/4 HANA', 'SAP FICO', 'Data Migration', 'SAP PP', 'SAP MM', 'SAP Integration', 'SAP Implementation', 'SAP Support']",2025-06-13 05:53:17
SAP QM Functional Consultant - Implementation & Integration,Leading Client,5 - 7 years,Not Disclosed,['Nagpur'],"Essential Job Functions :\n\n- 5+ years of experience in SAP QM.\n\n- Worked on ECC to S4 Hana migrations / Implementation experience.\n\n- Strong functional skills in QM/PP with at least 1-2 full life cycles.\n\n- Should have done At least one implementation on S/4 HANA.\n\n- Should know basic integration with PP/ MM/ FICO modules.\n\n- Should have handled Implementations, Upgrades, Rollouts, and Systems Integration.\n\n- Should be able to work independently and work with a cross-functional and client-facing team team within an on/offshore model.\n\n- Knowledge of shopfloor quality is an added advantage.\n\n- Proven ability to work creatively and analytically in a problem-solving environment.\n\n- Must be a very good team player with good interpersonal and communication skills.\n\n- SAP Certification and exposure to SAP S/4 HANA will be an added advantage.\n\n- Excellent analytical and problem-solving skills.\n\n- Excellent verbal and written communication skills, and can communicate clearly and concisely.\n\n- Thorough knowledge of traceability reports, COA, QM tables, shop floor QM integration is necessary.\n\n- Configuration and master data deep knowledge.\n\n- Ability to articulate the analysis and design, and build.\n\n- Master data expertise.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP QM', 'SAP ECC', 'S/4 HANA', 'SAP FICO', 'Data Migration', 'SAP PP', 'SAP MM', 'SAP Integration', 'SAP Implementation', 'SAP Support']",2025-06-13 05:53:18
SAP QM Functional Consultant - Implementation & Integration,Leading Client,5 - 7 years,Not Disclosed,['Delhi / NCR'],"Essential Job Functions :\n\n- 5+ years of experience in SAP QM.\n\n- Worked on ECC to S4 Hana migrations / Implementation experience.\n\n- Strong functional skills in QM/PP with at least 1-2 full life cycles.\n\n- Should have done At least one implementation on S/4 HANA.\n\n- Should know basic integration with PP/ MM/ FICO modules.\n\n- Should have handled Implementations, Upgrades, Rollouts, and Systems Integration.\n\n- Should be able to work independently and work with a cross-functional and client-facing team team within an on/offshore model.\n\n- Knowledge of shopfloor quality is an added advantage.\n\n- Proven ability to work creatively and analytically in a problem-solving environment.\n\n- Must be a very good team player with good interpersonal and communication skills.\n\n- SAP Certification and exposure to SAP S/4 HANA will be an added advantage.\n\n- Excellent analytical and problem-solving skills.\n\n- Excellent verbal and written communication skills, and can communicate clearly and concisely.\n\n- Thorough knowledge of traceability reports, COA, QM tables, shop floor QM integration is necessary.\n\n- Configuration and master data deep knowledge.\n\n- Ability to articulate the analysis and design, and build.\n\n- Master data expertise.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP QM', 'SAP ECC', 'S/4 HANA', 'SAP FICO', 'Data Migration', 'SAP PP', 'SAP MM', 'SAP Integration', 'SAP Implementation', 'SAP Support']",2025-06-13 05:53:20
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Allahabad'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:53:22
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Bhubaneswar'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:53:23
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Madurai'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:53:25
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Aurangabad'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:53:27
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Agra'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:53:29
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Ludhiana'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:53:30
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Vadodara'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:53:32
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Jaipur'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:53:34
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Jamshedpur'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:53:35
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Chennai'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:53:37
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Noida'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:53:39
Golang Engineer - Backend Technologies,Tinvio Digital Services,2 - 6 years,Not Disclosed,['Faridabad'],"Responsibilities :\n\n- Hands-on development in Golang to deliver trustworthy and smooth functionalities to our users\n\n- Monitor, debug, and fix issues in production at high velocity based on user impact\n\n- Maintain good code coverage for all new development, with well-written and testable code\n\n- Write and maintain clean documentation for software services\n\n- Integrate software components into a fully functional software system\n\n- Comply with project plans with a sharp focus on delivery timelines\n\nRequirement :\n\n- Bachelor's degree in computer science, information technology, or a similar field\n\n- Must have 3+ years of experience in developing highly scalable, performant web applications\n\n- Strong problem-solving skills and experience in application debugging\n\n- Hands-on experience of Restful services development using Golang\n\n- Hands-on working experience with database; SQL (PostgreSQL / MySQL) | NoSQL (Redis/ MongoDB/Cassandra)\n\n- Working experience of message streaming/queuing systems like Apache Kafka, RabbitMQ, SQS, IBM MQ\n\n- Cloud experience with Amazon Web Services (AWS) and Google Cloud Computing (GCP)\n\n- Experience with Serverless Architectures (AWS/GCP) would be a plus\n\n- Hands-on experience with API / Echo framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'NoSQL', 'MySQL', 'Cloud', 'RESTful Webservices', 'Kafka', 'Backend Architecture', 'AWS']",2025-06-13 05:53:41
Lead Software Engineer,Global Payments,8 - 10 years,Not Disclosed,['Noida'],"Collaborates with clients and other functional areas SMEs in the design of IT Roadmaps items to illustrate architectural complexities and interactions of information systems.\nAnalyzes, refines and documents the business requirements of the client.\nAnalyzes existing systems to detect critical deficiencies and recommend solutions for improvement.\nPlans and designs information systems and implements updates within scope of established guidelines and objectives.\nResearches new technological advances to assess current practices for compliance with systems requirements. Recommends solutions to address current system needs, process improvements and controls. . Makes recommendations for future information system needs.\nProvides technical architecture and support across applications and guidance to other functional areas to define software/hardware requirements and in planning and delivering infrastructure.\nAnalyzes infrastructure and capacity planning.\nEmploys a thorough knowledge of required procedures, methodologies and/or application standards, including Payment Card Industry (PCI) and security related compliance to write or modify software programs to include analysis, writing specifications and code, program installation and documentation for use with multiple application/user database systems.\nMaintains information systems by configuring software and hardware, tracking errors and data movement, and troubleshooting.\nCollaborate with engineers across the Core and Team to create technical designs, develop, test and solve complex problems that drive the solution from initial concept to production.\nContribute to our Automated build, deploy and test processes for each solution.\nWork in an iterative manner that fits we'll with the development practices and pace within the team, with focus on a fail fast approach.\nDemo your work for colleagues and members of the business team.\nConduct research on new and interesting technologies that help to progress our products and platforms.\nCreate mechanisms/architectures that enable rapid recovery, repair and cleanup of existing solutions with good understanding of fault tolerance and failure domains.\nIdentify opportunities to deliver self service capability for the most common infrastructure and application management tasks.\nCreate automated tests that easily plug into our automated code pipeline.\nProvide deep and detailed levels of monitoring across all levels of the application.\nAttend sessions, seminars and be an evangelist for the latest technology.\nLead , help mentor other engineers and technical analysts.\nPlan sprints within your project team to keep yourself and the team moving forward.\nMinimum Qualifications\nMCA, B. Tech. or B.E. (four year college degree) or equivalent.\nTypically minimum of 8 years - Professional Experience In Coding, Designing, Developing And Analyzing Data. Typically has an advanced knowledge and use of one or more back end languages / technologies and a moderate understanding of the other corresponding end language / technology from the following but not limited to; two or more modern programming languages used in the enterprise, experience working with various APIs, external Services, experience with both relational and NoSQL Databases.\nPreferred Qualifications\nB.Tech / Masters Degree ( Regular)\nWhat Are Our Desired Skills and Capabilities?\nSupervision - Determines methods and procedures on new assignments and may coordinate activities of other personnel (Team Lead).\nExperience of working on SOA Architecture, Microservices Architecture, Event drives and serverless architectures.\nGood Knowledge of JAVA/JEE Design Patterns, Enterprise Integration Design Patterns ,SOA Design Patterns, MicroServices Design Patterns.\nExperience of working on RestFull services, SOAP WebServices, gRPC , Async & streaming technologies.\nExperience of working on Java 1.8 +, Spring 4.x +, Spring Boot, Spring data, SpringREST, Spring MVC, Spring-integration (ie no EJB :), Tomcat 8.5.x (embedded version), JUnit + Spring-test, application stack\nExperience of working on ORM / Persistence frameworks or technologies like Hibernate , MyBatis, iBatis\nExperience on designing and developing Fault Tolerant , HA systems\nGood hands on experience on AWS stack and services like S3,EC2, KMS, EKS, MSK, Lambda, Iam, RDS, Dynamo,Cloudwatch\nGood hands on experience on Cloud Native projects like Prometheus, Grafana, Argo, Harbour, Helm, Istio, K8S etc\nGood experience of working on Agile development model and Automation Test driven development (TDD) methodologies.\nGood experience of using container technology to build out an automated platform architecture that allows for seamless deployment between on-premise and external cloud environments\nGood experience of leveraging open technology such as Docker, Kubernetes, Terraform, Bash, Javascript, Python, Git, Jenkins, Linux, HAProxy, AWS Cloud, ELK, Java, Kafka, MongoDB, Zookeeper, and AWS Amazon Web Service (EC2 Container Service, Cloud Formation, Elastic Load Balancer, Auto scaling Group).\nGood experience of Integrating systems using a wide variety of protocols like REST, SOAP, MQ, TCP/IP, JSON and others\nGood experience of designing and building automated code deployment systems that simplify development work and make our work more consistent and predictable.\nExhibit a deep understanding of server virtualization, networking and storage ensuring that the solution scales and performs with high availability and uptime.\nSoft Skills :\nIs Adaptable, Result oriented, portrays a positive attitude, Flexible & Multi Task orientated.\nIs able to accept guidance and is a good listener.\nHas Good oral and written communication skills\nHas ability to understand business needs and translate them into technology solutions.\nHas strong research and problem resolution skills\nIs a strong Team Player, with good time management, interpersonal & presentation skills.\nHas strong customer focus & understands external and internal customer expectations\nIs able to articulate Technical solutions in language understood by business users.\nHas a go getter attitude to handle challenging development tasks.\nCan drive Change and has a good Innovation track record.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hibernate', 'Automation', 'Tomcat', 'Web services', 'Linux', 'Networking', 'SOA', 'Javascript', 'EJB', 'Python']",2025-06-13 05:53:43
Engineer III - Software Engineering,World Courier,8 - 13 years,Not Disclosed,['Pune'],"Designs, implements, unit tests, documents and deploys applications and APIs based on functional requirements\nDesigns and develops database applications using SQL Server Integration Services, TSQL and Stored Procedures\nConsults with the business to determine logical design for new business solutions according to existing data architecture\nPerforms code reviews, analyze execution plans, and re-factor inefficient code\nProvides technical support & guidance to Software Engineers I/II in development of desired software/applications\nFollows data standards, resolves data issues, completes unit testing and completes system documentation for ETL processes\nAssists Managers with project development, including documentation of features, recording of progress, and creation of the testing plan\nCollaborates with business analysts, source system experts, and other team members to determine data extraction and transformation requirements\nCollaborates with IT operations and testing organizations to ensure timely releases of projects and database environments are sustainable\nSupports IT staff and business users in the development, usage and troubleshooting of database-driven applications\nCreates detailed designs, performs analyses, creates prototypes, and documents completed design\nActs as a senior member of the team; represents the organization on project teams and may perform technical project leadership roles while fostering teamwork and collaboration within and across work groups\nIntermediates knowledge on client requests, features, best practices, project scopes, and budgets\nContributes to the growth of the company by advancing personal skills, working with the development team to improve the codebase, and seeking opportunities to improve the company s tools and processes\nDesigns, develops, and automates scalable data engineering solutions by leveraging cloud infrastructure\nExtends or migrate existing data pipelines to new cloud environment\nCrafts data models of program processes and data entities and develops technical design & documentation of solutions\nParticipates in optimization of data asset performance\nTransfers knowledge of data access/consumption mechanisms to business stakeholders, data visualization specialists and/or data scientists\nDevelops solutions and recommendations for improving data integrity issues\nWhat your background should look like (minimum qualifications)\nBachelors degree or related experience and 8+ years of development experience\nRequires some training in fields such as business administration, accountancy, sales, marketing, computer sciences, or similar vocations, generally obtained through completion of a four-year bachelors degree program, technical vocational training, or equivalent combination of experience and education\n1. 8+ years of .NET, Web API, JSON, ASP.NET, MVC, and C# required.\n2. 8+ years of Angular, Typescript, JavaScript, Node.js, HTML 5, CSS, and Bootstrap required.\n3. Experience in developing REST API using C# and .NET frameworks.\n4. Experience in writing SQL queries is required.\n5. Experience in Visual Studio, Git, Azure DevOps, TFS, NuGet, and Visual Studio Code required.\n6. Experience developing client-side test code using Jasmine and Karma is preferred.\n7. Experience in React JS is preferred.\n8. Thoroughly understand the Systems Development Life Cycle (SDLC) process and agile methodologies.\n9. Familiarity with the Company s products and resources, and prior healthcare industry knowledge preferred.\n10. Strong analytical problem-solving and conceptual skills\n11. Ability to communicate effectively both orally and in writing.\n12. Good interpersonal skills\n13. Ability to prioritize workload and consistently meet deadlines.\n14. Ability to use good judgment in conveying project status and problem escalation.\nExperience & Educational Requirements:\nbachelors Degree in Computer Science, Information Technology or any other related discipline or equivalent related experience. 4+ years of directly-related or relevant experience, preferably in software designing and development.\n\nPreferred Certifications:\nAndroid Development Certification\nMicrosoft Asp.Net Certification\nMicrosoft Certified Engineer\nApplication / Infrastructure / Enterprise Architect Training and Certification, eg TOGAF\nCertified Scrum Master\nSAFe Agile Certification\nDevOps Certifications like AWS Certified DevOps Engineer\n\nBehavioral Skills:\nCritical Thinking\nDetail Oriented\nImpact and Influencing\nInterpersonal Communication\nMultitasking\nProblem Solving\nTime Management\n\nTechnical Skills:\nAPI Design\nCloud Computing Methodologies\nIntegration Testing & Validation\nProgramming/Coding\nDatabase Management\nSoftware Development Life Cycle (SDLC)\nTechnical Documentation\nWeb Application Infrastructure\nWeb Development Frameworks\n\nTools Knowledge:\nCloud Computing Tools like AWS, Azure, Google cloud\nContainer Management and Orchestration Tools\nBig Data Frameworks like Hadoop\nJava Frameworks like JDBC, Spring, ORM Solutions, JPA, JEE, JMS, Gradle, Object Oriented Design\nMicrosoft Office Suite\nNoSQL Database Platforms like MongoDB, BigTable, Redis, RavenDB Cassandra, HBase, Neo4j, and CouchDB\nProgramming Languages like JavaScript, HTML/CSS, Python, SQL\nOperating Systems & Servers like Windows, Linux, Citrix, IBM, Oracle, SQL",Industry Type: Medical Devices & Equipment,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['JMS', 'Linux', 'Web development', 'JDBC', 'HTML', 'Windows', 'Troubleshooting', 'Technical support', 'SDLC', 'Python']",2025-06-13 05:53:45
Engineer - mBOM,NKT Project And Automation,5 - 10 years,Not Disclosed,['Chennai'],"As a Engineer mBOM ,you will be responsible for creating and managing manufacturing bills of materials (M-BOM) for cables and accessories, ensuring alignment with project needs and production workflows.\n  Key Responsibilities:\nCreate M-BOMs in Windchill and SAP based on cable design and production flow\nReview and iterate BOMs with input from stakeholders\nSupport packing BOM creation for accessories\nAssist in legacy cable data migration into Windchill\nEnsure adherence to PLM best practices\nContribute to continuous improvement initiatives related to BOM processes\n  Your Qualifications and Experience\nwe're seeking a structured and detail-oriented professional with a passion for engineering and process optimization.\nB.E. in Mechanical or Electrical Engineering\n5+ years of experience in M-BOM creation using Windchill in a relevant industry\nProficiency in Windchill (PDM & MPM link), SAP, and Microsoft Office\nStrong communication skills and a collaborative mindset\nA self-driven, analytical, and results-oriented approach",Industry Type: Consumer Electronics & Appliances,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['PLM', 'Accessories', 'Process optimization', 'Data migration', 'SAP', 'Renewable energy', 'Analytical', 'Packaging', 'Cable laying', 'Continuous improvement']",2025-06-13 05:53:47
Junior Engineer,Cornerstone India,3 months duration,Unpaid,['Pune'],"Posted on: 6/10/2025 - Application Deadline: - Were looking for a This role is\nWere looking for an Junior Engineer\nWe are seeking a highly motivated Junior Engineer to join our dynamic team. As an intern with 0- 6 months of experience, you will work on cutting-edge\ntechnologies including .NET Core, microservices, REST APIs, React, and SQL databases. If you have a strong foundation in C#, and React and a keen\ninterest in cloud computing (preferably AWS) and AI, we want to hear from you!\nIn this role, you will\nDevelop, test, and deploy applications using .NET Core, C#, and React. Write clean, scalable, and efficient code.\nDesign and implement microservices-based architectures and RESTful APIs to support scalable and robust applications.\nCreate responsive and interactive user interfaces using React.\nCollaborate with UX/UI designers to deliver a seamless front-end experience.\nWork with SQL/ No SQL databases to design, query, and optimize data storage solutions.\nLeverage basic knowledge of AWS to integrate and deploy cloud-based services.\nStay current with emerging technologies and industry trends, with a particular interest in AI.\nParticipate in code reviews and contribute ideas to improve overall development practices.\nWork closely with cross-functional teams including development, QA, and operations to ensure successful project delivery.\nCommunicate effectively to understand project requirements and provide timely updates.\nParticipate in agile activities like sprint planning, and technical design reviews; provide input as appropriate.\nParticipate in key architectural decisions and design considerations.\nTroubleshoot complex production issues and provide detailed RCA.\nYou ve Got What It Takes If You Have\nBachelor s or master s degree in Computer Science or a related field with an enthusiastic mindset of Want to Learn a lot .\n0-6 months of experience with active hands-on development experience in C#, .Net Core, and/ or React.\nExposure to developing Microservices, RESTful services, or other SOA development experience (preferably AWS).\nknowledge ORM like Entity Framework, NHibernate, or similar.\nStrong in OOPs and Good to have exposure to design principles like SOLID, KISS.\nKnowledge of working on projects with public cloud providers like Amazon Web Services is a plus.\nKnowledge of Advanced front-end development frameworks and platforms, React knowledge is a plus\nKnowledge of relational databases such as Microsoft SQL Server/My SQL. Exposure to other non-relational DBs like DynamoDB is a plus!\nKnowledge of Scrum or other Agile development methodologies\nExcellent analytical, quantitative, and problem-solving abilities.\nConversant in algorithms, software design patterns, and their best usage.\nGood team player with the ability to perform in a fast-paced work environment.\nStrong interpersonal, written, and oral communication skills.\nPassion for continuous process and technology learning and improvement.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Cloud computing', 'RCA', 'Software design', 'Front end', 'SOA', 'Analytical', 'Entity framework', 'Scrum', 'Project delivery']",2025-06-13 05:53:49
Staff Software Engineer,Fourkites,8 - 10 years,Not Disclosed,['Chennai'],"At FourKites we have the opportunity to tackle complex challenges with real-world impacts. Whether it s medical supplies from Cardinal Health or groceries for Walmart, the FourKites platform helps customers operate global supply chains that are efficient, agile and sustainable.\nJoin a team of curious problem solvers that celebrates differences, leads with empathy and values inclusivity\n.\nStaff Engineer JD:\nAs a Staff Software Engineer, you will get an opportunity to work on features end to end (backend frontend) using the latest technologies such as RoR, Java, GoLang, Angular, React, Redis, PostgreSQL. You will develop products that can change the logistics landscape and will be used by some of the biggest corporations in the world. You will develop integrations with our strategic partners to help expand our ecosystem. You will work closely with our US team and customers to develop features that help shape the logistics and supply chain industry.\nWho you are:\nBachelor s degree in Computer Science Engineering or related field from a reputed institution.\nMinimum of 8 years of experience in Software Engineering and Web application development.\nGood understanding of software design, Microservices architecture, object-oriented principles, and design patterns.\nExperience with Design and development of highest quality software/services using RoR/Golang/Java.\nGood knowledge of RESTful APIs and microservices architecture\nStrong understanding of Java, Spring Framework, and object-oriented programming principles\nExperience in one of Azure, Amazon Web Services or other cloud services.\nExperience with databases such as MySQL, PostgreSQL, or MongoDB\nFamiliarity with front-end technologies such as HTML, CSS, and JavaScript is a plus\nStrong knowledge of Git (branches, submodules, rebasing) and other Agile tools such as JIRA Confluence.\nAgile SDLC experience\nExcellent oral and written communication skills\nWhat you ll be doing:\nDesign, architect, implement, test, profile, release, and optimize highest quality software/services using RoR/Golang/Java.\nPartner with product manages to analyse product requirements and plan engineering execution\nDocument HLD/LLD for easy knowledge sharing and future scaling\nPerform design and code reviews\nImplement code with very high coverage of unit tests and component tests\nCross-training peers and mentoring teammates\nPossess expert knowledge in performance, security, scalability, architecture, and best practices\nFunctionally decompose complex problems into simple, straight-forward solutions\nCollaborate with UX designers to develop responsive user interface components\nWorking knowledge of SQL based (any RDBMS) and NOSQL data stores (any one) with the ability to write intermediate level SQL\nExperience in building Web application backends using Java Spring Boot or similar\nExperience with frontend libraries/frameworks such as React/Angular is a plus.\nEducation Qualification: Graduate from B.E/ B.Tech / MCA / M.Tech Background.\nWho we are:\nFourKites , the leader in AI-driven supply chain transformation for global enterprises and pioneer of real-time visibility, turns supply chain data into automated action. FourKites Intelligent Control Tower breaks down enterprise silos by creating a real-time digital twin of orders, shipments, inventory and assets. This comprehensive view, combined with AI-powered digital workers, enables companies to prevent disruptions, automate routine tasks, and optimize performance across As the leader in AI-driven supply chain transformation, FourKites pioneered the Intelligent Control Tower powered by the world s largest real-time visibility network. Our platform creates comprehensive digital twins of your supply chain with AI-powered digital workers to automate resolution, improve collaboration and drive outcomes across all stakeholders. Unlike traditional control towers, we enable true real-time execution and intelligent fulfillment, transforming both your supply and customer\nBenefits\nMedical benefits start on first day of employment\n36 PTO days( Sick, Casual and Earned) , 5 recharge days, 2 volunteer days\nHome Office setups and Technology reimbursement\nLifestyle Family benefits\nAnnual Swags/ Festive Swags\nOngoing learning development opportunities ( Professional development program, Toast Master club etc.)",Industry Type: Courier / Logistics,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Software design', 'RDBMS', 'Postgresql', 'MySQL', 'Javascript', 'Agile', 'HTML', 'SDLC', 'SQL']",2025-06-13 05:53:50
Engagement Delivery Director (Salesforce),Tableau Software,17 - 22 years,Not Disclosed,['Mumbai'],"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.\nJob Category\nCustomer Success\nJob Details\nAbout Salesforce\n.\nResponsibilities:\nPlan and manage the technical delivery of multiple programs within an account using Agile techniques and frameworks\nLead and manage delivery team/s to successfully deliver all key milestones and product outcomes within scope, on time, on budget and to expected standards\nFully accountable for the delivery of work assignments on time and to expectations in terms of quality, deliverables and outcomes\nTake responsibility for delivering high quality customer focused services\nVery good understanding of engagement model ( TM and Fixed Price) and plan end to end execution effectively by keeping scope and milestone on check\nImplement effective stakeholder engagement and communications strategy for all stages of projects\nPrepare scope and business cases for more ambiguous or complex projects including cost and resource impacts\nAnticipate and assess the impact of changes in scope and effectively manage the change request process\nManage transitions between project stages and ensure that changes are consistent with organisational and customer goals\nWorking closely with Engagement Management team on Initial scoping and Change Management\nReport and escalate issues such as variances and manage delivery by exception to ensure issues are understood and actions to resolve identified\nSupport the delivery of all governance materials, artefacts and meetings to ensure products are delivered and maintained in a transparent fashion and stored and maintained as per Organisational standards\nGuide, support, coach, provide direction, upskill team members and maintain a cohesive culture within the project team\nManage effective implementation of resource planning, on-boarding and transitioning of resources.\nEncourage a culture of Inclusiveness and diversity\nKeep abreast of new salesforce products and trends within the industry\nWork collaboratively with cross-functional teams such as MuleSoft, SI partner teams, UI/UX teams, tech PODs to contribute to achieving business outcomes\nParticipate in meetings and discussions to continuously improve delivery\n\nRequired Skills/Experience\nDegree or equivalent relevant experience required. Experience will be evaluated based on the core competencies for the role (e.g. extracurricular management roles, work experience, etc.)\n17+ years experience, predominantly managing SaaS based projects. CRM and Salesforce knowledge\nGood to have: Very good understanding of Salesforce implementation involving custom development, integration and data migration\nProject management experience (Agile preferred)\nManage and develop stakeholder relationships by effective governance model and by including steering committees, through effective communications, documentation, negotiation and issues management to ensure delivery of products and the achievement of outcomes and benefits\nSalesforce is an equal opportunity employer and maintains a policy of non-discrimination with all employees and applicants for employment. What does that mean exactlyIt means that at Salesforce, we believe in equality for all. And we believe we can lead the path to equality in part by creating a workplace that s inclusive, and free from discrimination.",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Stakeholder Engagement', 'Change management', 'Data migration', 'Focus', 'Project management', 'Engagement management', 'Agile', 'Resource planning', 'CRM', 'Salesforce']",2025-06-13 05:53:52
SAP BTP - ABAP Lead,Sutherland Global Services Inc,12 - 17 years,Not Disclosed,['Hyderabad'],"We seek a highly experienced and strategic SAP Offshore SAP ABAP lead, to lead and oversee the implementation and optimization of RISE with SAP S/4HANA solutions for our clients who are into downstream oil and gas operations. This role seeks an experienced RISE with SAP S/4HANA ABAP lead, for development, customization, and optimization of SAP S/4HANA applications using ABAP to meet complex business requirements. Lead ABAP development in RISE with SAP S/4HANA by designing, developing, and optimizing custom applications and reports in cloud-based environments.\nKey Responsibilities: Solution Architecture and Design:",,,,"['Procurement', 'Performance tuning', 'Data migration', 'Coding', 'SAP Basis', 'Customer service', 'EDI', 'Information technology', 'ABAP', 'Analytics']",2025-06-13 05:53:54
Developer 3 (C++),HYLAND,5 - 11 years,Not Disclosed,['Kolkata'],"Overview\nHyland Software is widely known as a great company to work for and a great company to do business with. Being a leader in providing software solution for managing content, processes, and cases for organizations across the globe we enabled more than 20,000 organizations to digitalize their workplaces and transform their operations.\nCurrently we are looking for the position of Developer 3 (C++)\n\nThe Developer is responsible for the overall performance of the product through applying principles of software engineering to the design development maintenance testing and evaluation of the software. The Developer ensures timely delivery of high quality software within the release timelines and guidelines.\nWhat you will be doing\nWork within our Acuo Core VNA team as a Senior Software Developer, focusing onmoving the product forward through refactoring and modernization, including the following:\nVNA functionality and workflow optimizations for cloud deployments.\nImplementation of RESTful web services.\nArchitect new components. Re-architect existing components as needed.\nEnhancing the supportability, security, usability, and performance of the\nsoftware.\nWhat will make you successful\nMicrosoft C++ Development (in-depth knowledge and experience required): Including\nmemory management, concurrency and multi-threading.\nMicrosoft .NET Development (required)\nHealthcare Enterprise Workflow, Components and Standards (preferred, nice-to-have)\nExperience working with healthcare enterprise workflow components,\nstandards, or integration initiatives. For example, familiarity with PACS/VNA\nsystems, similar platforms, or integration standards/profiles such as IHE,\nXDS, and HL7.\nDICOM Standard (preferred, nice-to-have)\nKnowledge and experience working with the DICOM Standard including one\nor more of the following: multi-frame image encoding, pixel data and tag data\nstructures, compression techniques (e.g. JPEG, JPEG 2000), transmission\nand workflows involving DICOM images, DICOM toolkit development using\nC++ libraries.\nWeb services\\RESTful service implementation and design (required)\nUnit testing development (required)\nAmazon Web Services or cloud technologies (strong knowledge)\nSQL Server and/or PostgreSQL (strong knowledge)\nFamiliarity with large capacity storage systems and Cloud storage environments (strong\nknowledge)\nCloud development, for example AWS and/or Azure (nice to have)\nHyland s Offering\n\nWe re proud of our culture and take employee engagement seriously. By listening to employees feedback, we re able to provide meaningful benefits and programs to our workforce.\nLearning & Development - development budget (used for certifications, conferences etc..), tuition assistance program, 4,000+ self-paced online courses, instructor-led webinars, mentorship programs, structured on-boarding experience full of trainings, dedicated Learning & Development department supporting our employees.\nR&D focus cutting edge technologies, constant modernization efforts, dynamic and innovative environment, dedicated R&D Education Services department to help you grow.\nWork-life balance culture flexible work environment and working hours (we are working in task-based system!), possibility to work from home, we value trust, and we believe efficiency does not depend on your actual location, however we would like to spend time together in the office!\nWell-being - private medical healthcare, life insurance, gym reimbursement, psychologist & dietician consultation, wellness manager care, constant wellbeing programs\nCommunity Engagement Volunteer time off (12h/year), Hylanders for Hylanders relief found, Mission fit giving, Dolars-for-doers matching gift programs.\nDiversity & Inclusion employee resource groups, inclusion benefits and policies\nNiceties & Events snacks and beverages, employee referral program, birthday, baby gifts, constant incentives, and employee programs\nIf you would like to join the company where honesty, integrity and fairness lie in the bottom of values, where people are truly passionate about technology and dedicated to their work connect with us!\n\nWe are committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, age, physical or mental disability, veteran or military status, genetic information, sexual orientation, marital status, gender identity or any other legally recognized protected basis under federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for employment, verify identity and maintain employment statistics on applicants.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['C++', 'Dicom', 'Memory management', 'Postgresql', 'Data structures', 'Workflow', 'Healthcare', 'Unit testing', 'microsoft', 'SQL']",2025-06-13 05:53:56
Dynamic CRM Developer,Diverse Lynx,4 - 6 years,Not Disclosed,['Hyderabad'],"Diverse Lynx is looking for Dynamic CRM Developer to join our dynamic team and embark on a rewarding career journey\nA CRM (Customer Relationship Management) Developer is a professional who specializes in designing, developing, and implementing CRM systems and applications\nThey work closely with stakeholders, such as business analysts and sales teams, to understand their requirements and create customized CRM solutions to meet their needs\nHere are some key responsibilities of a CRM Developer:CRM System Development: The CRM Developer is responsible for developing CRM systems and applications using programming languages and development frameworks\nThey utilize CRM platforms, such as Salesforce, Microsoft Dynamics 365, or custom-built solutions, to create robust and scalable CRM systems\nCustomization and Configuration: The CRM Developer customizes and configures CRM systems to align with business processes and requirements\nThey create and modify CRM entities, fields, workflows, and business rules to capture and manage customer data effectively\nIntegration: The CRM Developer integrates CRM systems with other enterprise systems, such as ERP (Enterprise Resource Planning) systems, marketing automation tools, or customer support platforms\nThey design and implement data integration, API integrations, or middleware solutions to ensure seamless data flow and synchronization across systems\nData Management: The CRM Developer manages and maintains CRM databases, ensuring data integrity and accuracy\nThey design data models, create data migration strategies, and implement data validation rules to maintain high-quality customer data within the CRM system\nCustom Development: The CRM Developer creates custom functionalities and features within the CRM system to address specific business requirements\nThey develop custom modules, plugins, or extensions using programming languages, such as JavaScript, C#, or Apex, to enhance the CRM system's capabilities",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c#', 'erp', 'customization', 'configuration', 'plugins', 'microsoft dynamics', 'business rules', 'data migration', 'javascript', 'apex', 'salesforce', 'automation', 'automation tools', 'system development', 'migration', 'integration', 'api', 'data integration', 'programming', 'crm', 'middleware']",2025-06-13 05:53:58
SAP SD Professional,Diverse Lynx,4 - 10 years,Not Disclosed,['Hyderabad'],"1. Payroll and Location: If selected, you will be working on the payroll of our organization, Diverse Lynx India Pvt. Ltd., and stationed at our clients office located in Hyderabad. (WFO is Mandatory)\n2. Interview Process: Once your profile has been shortlisted by our technical panel, we will promptly arrange a face-to-face interview/Virtual Interview for you. Rest assured that we will keep you informed every step of the way. Kindly help us with the interview slot date timing so that we can line up the project team for technical evaluation.\n3. Selection Confirmation: We understand how important it is to receive timely updates during the hiring process. Therefore, we are committed to providing confirmation of your selection on the same day as your interview.\nThis is an excellent opportunity for professionals like yourself who are seeking growth and development in the SAP field. Must have Implementation/Support experience with minimum 4 Years of total experience.\n\nTo book your interview slots; please help with the below details and you can connect with our Head directly for any further information support. Experience- 4 Years-10 Years.\n-Full Name (As per PAN Card)\n-PAN Card No:\n-Date of Birth- DD/MM/YEAR/-\n-Total Experience -\n-Relevant Experience in the given Skill set -\n-Highest Qualification and Passing Year with Date 10th Onwards: -\n-Current Location -\n-Preferred Location -\n-Current Company (Parent Company)\n-Contact No. -\n-Mail Id -\n-Notice Period -\n-Reason for Change -\n-Current CTC\n-Expected CTC\n\nBest Regards\n\nManish Prasad\nAccount Manager(Human Resource-Recruitment)\nDiverse Lynx India Pvt. Ltd.\nEmail ID:- manish.prasad@diverselynx.in\nURL: http://www.diverselynx.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Payroll', 'Data migration', 'SAP', 'SAP SD', 'cyber security', 'SOC', 'Ab Initio', 'HTTP', 'Recruitment', 'Auditing']",2025-06-13 05:53:59
SAP ABAP Professional,Diverse Lynx,4 - 10 years,Not Disclosed,['Hyderabad'],"1. Payroll and Location: If selected, you will be working on the payroll of our organization, Diverse Lynx India Pvt. Ltd., and stationed at our clients office located in Hyderabad. (WFO is Mandatory)\n2. Interview Process: Once your profile has been shortlisted by our technical panel, we will promptly arrange a face-to-face interview/Virtual Interview for you. Rest assured that we will keep you informed every step of the way. Kindly help us with the interview slot date timing so that we can line up the project team for technical evaluation.\n3. Selection Confirmation: We understand how important it is to receive timely updates during the hiring process. Therefore, we are committed to providing confirmation of your selection on the same day as your interview.\nThis is an excellent opportunity for professionals like yourself who are seeking growth and development in the SAP field. Must have Implementation/Support experience with minimum 4 Years of total experience.\n\nTo book your interview slots; please help with the below details and you can connect with our Head directly for any further information support. Experience- 4 Years-10 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Payroll', 'Data migration', 'SAP', 'cyber security', 'SAP ABAP', 'SOC', 'Ab Initio', 'HTTP', 'Recruitment', 'Auditing']",2025-06-13 05:54:01
L3 Support/ Infra Cloud Engineer/VM Ware,Tech Vedika Software,8 - 9 years,Not Disclosed,['Hyderabad'],"Job Description\nLinux: A strong understanding of the Linux operating system, which companies frequently use for cloud development. Cloud engineers should understand the architecture, maintenance, and administration of Linux servers.\nNetworking proficiency: Clear understanding of virtual networks and general network management functions.\nKnowledge of DevOps: DevOps is a popular framework for cloud engineering, so having a hands-on understanding of DevOps practices can be valuable to employers. Amazon Web Services (AWS) OR Azure DevOps in particular is a sought-after skill by cloud providers.\nContainerization tools knowledge: Proficiency in containerization tools and understanding of Docker and Kubernetes.\nVirtualization skills: Insightful knowledge to deploy and run application software on virtual machines.\n\n\nQualifications\nPrefer to have at least 8+ years of experience and with the following strong skills on\nVMware, Servers, Nutanix, ESXI\nAWS/Azure,Linux\nDocker/ Docker Files/ Dev Container\nKube",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['VMware', 'Web services', 'Linux', 'Networking', 'GCP', 'Cloud', 'Application software', 'Virtualization', 'AWS', 'Analytics']",2025-06-13 05:54:03
Functional Consultant,Synoptek,2 - 5 years,Not Disclosed,['Ahmedabad'],"Synoptek\nWe think globally, act locally. As a Managed Services Provider, Synoptek provides world-class strategic IT leadership and hyper-efficient IT operational support, enabling our global client-base to grow and transform their businesses. We are excited to have experienced continuous growth and in keeping with that momentum we are seeking to add talent to our team. When you partner with Synoptek, you engage with an ever-growing, ever-evolving IT organization that provides a high-caliber team, results growth, and clarity.\nFunctional Consultant",,,,"['Supply chain', 'Business process', 'Computer science', 'Data migration', 'operational support', 'Managed services', 'Project management', 'Process improvement', 'Consulting', 'Gap analysis']",2025-06-13 05:54:05
"Lead Functional Consultant, SCM & Manufacturing",Synoptek,6 - 10 years,Not Disclosed,['Ahmedabad'],"This is an amazing opportunity to work within one of the fastest growing Managed Services Providers. We are a company with a heart and soul dedicated to the ongoing success and growth of our employees and continued business success of the customers we support. We foster a fun and connected environment with employee benefits extending beyond general compensation and into company sponsored events and an invested culture of learning.\nThe Lead Functional Consultant, SCM & Manufacturing, is responsible for leading the end-to-end project lifecycle, from project conception to successful implementation, specifically focusing on Supply Chain Management (SCM) and Manufacturing solutions. This role requires a deep understanding of Microsoft Dynamics AX/ F&SCM, strong leadership skills, and the ability to coordinate and manage both functional teams and project scope effectively.",,,,"['Data migration', 'Supply chain management', 'Managed services', 'Process improvement', 'Consulting', 'Warehouse management', 'microsoft', 'SCM', 'Information technology', 'Software implementation']",2025-06-13 05:54:07
Software Engineer I,Crego,2 - 7 years,Not Disclosed,['Gurugram'],"Join our AI team to develop cutting-edge machine learning models that power our lending decision engine and fraud detection systems\nExperience: 2 years of experience required\nSkills:\nPython\nMySQL\nAWS\nDjango/Flask\nRESTful APIs\nResponsibilities:\nDesign, develop, and maintain scalable and efficient backend services in a microservices architecture\nImplement and optimize backend solutions for containerized applications using ECS (Elastic Container Service)\nCollaborate with cross-functional teams to define and implement API specifications and integration points\nWork with Python (Django & Flask) to build robust and efficient server-side applications\nConduct performance analysis and optimization to ensure high availability and responsiveness of backend systems\nParticipate in agile development processes, including sprint planning, code reviews, and daily stand-ups\nTroubleshoot and debug issues on time, ensuring the stability of production systems\nDevelop comprehensive API and code documentation for internal and external stakeholders\nCollaborate with front-end developers to integrate user-facing elements using server-side logic\nImplement and maintain messaging systems for efficient communication between microservices\nRequirements:\nBachelors degree in Computer Science, Engineering, or a related field\nProven experience as a Backend Developer with a focus on microservices architecture\nStrong proficiency in Python (Django & Flask)\nExperience with ECS and containerized applications\nProficient in version control systems, particularly Git\nKnowledge of performance optimization techniques and best practices\nFamiliarity with agile development methodologies\nStrong troubleshooting and debugging skills\nExperience in API design, implementation, and documentation\nSolid understanding of *RESTful APIs *and messaging systems\nExcellent communication and collaboration skills\nAbility to work in a fast-paced and dynamic environment\nQualifications:\nBachelors degree in Computer Science\nExperience with FinTech is a big plusCertification in AWS (Amazon Web Services) or relevant cloud platforms is a plus\nFamiliarity with additional programming languages and frameworks\nExperience with DevOps practices and CI/CD pipelines is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Backend', 'Front end', 'Version control', 'GIT', 'Django', 'MySQL', 'Machine learning', 'Performance optimization', 'Python']",2025-06-13 05:54:08
Oracle Release Automation Engineer,Aria Systems,2 - 4 years,Not Disclosed,['Chennai'],"Job Description\nTitle: Oracle Release Automation Engineer\nDepartment: Production Operations\nLocation: Chennai, India\nAbout Aria Systems\nAria provides a cloud-based monetization platform for subscription- and usage-based businesses. Aria removes billing bottlenecks and allows companies to rapidly introduce and evolve their offerings, bundles, and promotions.\nInnovative companies, like VMWare, EWS, Mindbody, Allstate, Comcast, Intel, and Telstra depend on Aria for agility to accelerate their time to revenue, maximize customer value, and ultimately grow their business.\nWe are looking for talented, passionate people with strong track records and relevant expertise to help us achieve our goals. We are a fast-moving startup that offers a dynamic working environment with a collaborative culture, where you and your peers can learn from each other, and where you can make visible contributions that benefit our world-class customers and partners.\nDepartment Overview\nAria s Operations team consists of Systems, Application, and Database Administrators, DevOps, and Security & Compliance. The team works remotely with a primarily office out of Broomall, PA, and individuals working out of Chennai India. This team operates in a 24x7 environment, supporting Aria production and development systems.\nPosition Summary\nWe are currently looking for an Oracle Release Automation Engineer to join our Production Operations/Release Operations team with a home office in Broomall, PA. This position reports directly to the Sr. Director of Release Operations.\nResponsibilities\nWork with development and QA to release the latest code into the client facing environments.\nExecute security code scans against the raw code prior to release.\nDevelop automated release software to handle the many different code bases used within the Aria application.\nRe-engineer the existing release code moving it into a modern technology stack.\nContinue to enhance our release process through automation were possible\nTake on tasks assigned by Ops leadership that will cross other functional areas\nAddress any assigned security related findings to ensure Aria stays compliant\nSupport development as required to handle any problems that may experience with managing their code repository\nRequired Qualifications\nA successful candidate will possess at least 2-4 years of experience in an environment that requires 24x7x365 support.\n1-3 years working as an Oracle DBA contributing to support of mission critical databases.\nProficient in SQL and PL/SQL including procedures, packages, queries and triggers.\nWorking knowledge of the Linux operating system.\nStrong experience with Amazon Web Services (AWS).\nStrong coding skills using Bash, Perl, Python, or Ruby.\nStrong experience with shell scripting.\nKnowledge and use of Open-Source automated tools including Jenkins, Console and Ansible.\nAbility to reverse engineer existing solutions and develop new innovative processes.\nExcellent communication, troubleshooting and problem-solving skills.\nAbility to multitask and learn new technologies.\nEnergetic learner and strong team player.\nPreferred Qualifications\nCandidates with experience in an environment that requires continual security and compliance audits.\nAgile experience.\nStrong scripting skills.\nDeveloper experience with PL/SQL.\nExperience with source/version control management, artifact management & tracking tools (Atlassian tools preferred).\nProduction experience with Oracle Enterprise & Edition-Based Redefinition.\nPHP experience.\nExperience with Kubernetes in the enterprise.\nAria Culture\nAria Systems foster a flexible, rewarding, and close-knit work environment while encouraging innovation and self-direction. As a startup we expect all our employees to share our strong sense of urgency, ability to learn and master new technologies quickly, and willingness to adapt to rapidly changing priorities. In return Aria offers competitive salaries and benefits, attractive stock options, and a flexible, informal work environment.\nApply for this position directly on our website at .\nAria Systems is an AA/EEO employer that actively pursues and hires a diverse workforce.\nPlease, no phone calls. Principals only; recruiters please do not respond to this ad.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['VMware', 'Automation', 'Linux', 'Shell scripting', 'PHP', 'Oracle DBA', 'Perl', 'Oracle', 'Troubleshooting', 'Python']",2025-06-13 05:54:10
Project Manager,RIA Advisory,7 - 10 years,Not Disclosed,['Pune'],"Key Responsibilities:\n\n1. Product Development Management\nDefine and manage the product roadmap, ensuring alignment with business goals.\nWork closely with developers, designers, and QA to track sprints, releases, and feature delivery.\nMonitor project budgets and resource allocation, ensuring efficiency.\nIdentify and mitigate risks, dependencies, and bottlenecks proactively.\nEnsure proper documentation of feature requests, change logs, and technical decisions.\n\n2. Product Implementation & Deployment\nCollaborate with clients and internal teams to ensure smooth implementation of the product.\nTrack and resolve implementation challenges, including infrastructure, data migration, and integration issues.\nDefine and oversee go-live strategies, user acceptance testing (UAT), and training.\nAddress customer pain points, bugs, and issues, ensuring timely resolution.\nLead post-implementation reviews to gather feedback and identify areas for improvement.\n\n3. Tracking & Reporting\nMaintain a centralized tracking system for all ongoing projects, sprint statuses, and implementation progress.\nLead regular review meetings to discuss issues, blockers, and progress updates with cross-functional teams.\nOversee root cause analysis (RCA) for critical issues and ensure corrective actions.\nProvide periodic status reports to leadership on key metrics, timelines, and budget adherence.\n\nKey Skills & Competencies:\n\nStrong knowledge of Agile & Scrum methodologies (PMP, CSM)\nExperience in managing end-to-end software product lifecycles.\nAbility to track and manage multiple projects, resources, and deadlines effectively.\nStrong communication skills to coordinate between product, tech, and implementation teams.\nHands-on experience with JIRA, Confluence, or other project tracking tools.\n\nPreferred Qualifications:\n\n7+ years of experience in project management roles within product development or software implementation.\nExperience in SaaS, enterprise software, or tech product companies is a plus.",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Product Development Management', 'Sprint Planning', 'Resource Planning', 'Product Roadmap', 'JIRA', 'SDLC Life Cycle', 'Technical Project Management', 'Software Project Management', 'Agile Methodology', 'RCA', 'PMP', 'Agile', 'Scrum', 'Budgeting', 'IT Project Management', 'Stakeholder Management']",2025-06-13 05:54:12
Project Manager,Accelya,8 - 13 years,Not Disclosed,['Mumbai (All Areas)( Vikhroli West )'],"Specialist Program Management\nVikhroli, Mumbai\n\nRole purpose\nAs a Specialist Program Management, you will be responsible for the successful initiation, planning, design, execution, monitoring, controlling and closure of a project. You will work closely with management to make sure that the scope and direction of each project is on schedule, as well as other departments for support.\n\nDuties & Responsibilities:\nPlanning, Estimation, Scheduling all implementation activities such as data migration, parallel run, packaging, installation, UAT, user training etc\nEnsure successful product deployment and implementation\nHandle Implementation projects for 1- 2 customers simultaneously\nContract Management\nProject Planning (Resources, Activities, Infrastructure, Travel and Dependencies)\nRMP (Risk Mgt Plan)- ( Product, Resources, Contractual deliveries, RFP Planning ,ensuring agreed points are getting delivered, Solutioning for Gaps)\nProject Organisation Management (People Management)\nBudget Management ( Cost/Efforts/Schedule)\nCSAT (Customer Satisfaction )\nEnsure Implementation site ( customer site) for readiness\nExecute the Customer data migration successfully\nHand over of all the artifacts /learning to the support manager while project enters the support phase\nEnsure data center site readiness for hosted implementation\nAttend Steering committee meetings to update the stakeholder\nDevelop talent and enhance team capabilities by maintaining high team morale and productivity\n\nKnowledge, Experience & Skills:\nExperience with Project Management Tools MS Project\nExperience working in Agile Methodology\nStrong technical background, with understanding or hands-on experience in software development process\nExcellent client-facing and internal communication skills\n\nWhat do we offer?\nOpen culture and challenging opportunity to satisfy intellectual needs\nWork-life balance\nExcellent, dynamic and multicultural environment\n\nAbout Accelya\nAccelya is a leading global technology and service provider to the air transport industry delivering innovative change at speed and scale.\nThe companys market-leading passenger, cargo, and industry platforms support airline retailing from offer to settlement, both above and below the wing.\nOver 250 airline customers count on Accelya, with operations spread across nine countries and employing over 2,000 professionals worldwide.\nFor more than 40 years, Accelya has been the industryâ€™s partner for change, simplifying airline financial and commercial processes and empowering the air transport community to take better control of the future. Whether partnering with IATA on industry-wide initiatives or enabling digital transformation to simplify airline processes, Accelya drives the airline industry forward and proudly puts control back in the hands of airlines so they can move further, faster.\nFor more information, please visit www.accelya.com and https://accelya.turtl.co/story/accelya-corporatebrochure/page/1.\n\nInterested candidates can share profiles at anushka.nibre@accelya.com with below mentioned details:\n\nCurrent CTC\nExpected CTC\nNotice Period\nSummary of your experience",Industry Type: Software Product,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Project Risk', 'PMP certified', 'Project Scope', 'Project Monitoring', 'Project Tracking', 'Budgeting', 'Project Planning', 'Prince2', 'Project Scheduling', 'Project Life Cycle']",2025-06-13 05:54:14
Implementation Manager (TMS),Fleetx,3 - 5 years,9-15 Lacs P.A.,['Gurugram'],"Role Overview:\nWere seeking a highly driven and detail-oriented Implementation Manager to lead end-to-end onboarding and deployment of our Transport Management System for enterprise clients. You will act as the bridge between our product, tech, and client teams, ensuring smooth, timely, and value-driven implementations.\n\nKey Responsibilities:",,,,"['Transport Management System', 'Communication Skills', 'Client Onboarding', 'SaaS Implementation', 'Client Relationship Management', 'Enterprise Clients', 'Problem Solving', 'Client Engagement', 'Post-implementation Support', 'Stakeholder Management']",2025-06-13 05:54:16
Tally Developer,Woodapple Software Solutions,3 - 8 years,3-7 Lacs P.A.,['Dombivli'],"Role & responsibilities\n\nDesign and implement custom modules, reports, and functions in Tally using TDL to cater to specific business requirements, such as custom vouchers, invoice layouts, ledger enhancements, and approval workflows.\nIntegrate Tally with external platforms (e.g., Excel, SQL Server, web APIs) to automate data import/export, reduce manual entry, and improve real-time data synchronization with CRMs, ERPs, or e-commerce systems.\nDesign and automate invoice, delivery challan, credit note, and other templates to match business branding and compliance needs, using TDL scripting and print configuration.\nPerform smooth data migration from legacy systems or between Tally versions (e.g., ERP 9 to Prime) with data integrity. Ensure compatibility with the latest Tally updates and patches.\nProvide prompt assistance to users for any functional or technical issues in Tally. Conduct root cause analysis for recurring problems and develop permanent solutions.\nCreate customized reports such as MIS, sales, purchase, inventory, P&L, balance sheets, and tax summaries (GST, TDS) tailored to department and management needs.\nHandle Tally license activation/renewal, manage data backups, user roles, password policies, and ensure secure access and data integrity within the system.\nCollaborate with finance, operations, and inventory teams to understand their processes and develop Tally-based solutions that simplify and automate tasks.\nEnsure that all Tally configurations support compliance with statutory requirements such as GST, TDS, audit trails, and other local financial regulations.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Integration', 'Debugging', 'Tdl', 'Tally', 'Tally Software']",2025-06-13 05:54:17
Database Developer,Blueaspire Technologies,4 - 7 years,10-12 Lacs P.A.,['Gurugram'],"Job Summary:\nWe are seeking a Database Developer with 4-5 years of hands-on experience in MS-SQL and MySQL. The ideal candidate will be responsible for designing, developing, and maintaining efficient and secure database systems to support business applications and data processes.\n\nRole & responsibilities\nDatabase Design & Development\nDesign and develop scalable and high-performance MS-SQL databases and schemas.\nCreate and maintain stored procedures, triggers, views, and user-defined functions.\nCollaborate with application developers to implement database solutions that meet business requirements.\nQuery Optimization\nWrite, review, and optimize complex SQL queries for efficient data access and manipulation.\nAnalyze execution plans and improve query performance through indexing and refactoring.\nPerformance Tuning\nMonitor and enhance database performance to ensure availability, responsiveness, and scalability.\nIdentify and resolve performance bottlenecks through proactive tuning.\nDocumentation & Reporting\nMaintain clear and comprehensive documentation for database designs, queries, and processes.\nDevelop and support database reports as required by the business teams.\nSecurity & Compliance\nImplement and manage database security, including roles, permissions, and user access controls.\nEnsure compliance with internal policies and external data protection regulations.\nContinuous Improvement\nStay updated with the latest trends, features, and best practices in database development.\nRecommend and support improvements in database architecture and processes.\n\n\n\nPreferred candidate profile\nUp to 4 years of experience in MS-SQL Server and MySQL database development.\nStrong SQL skills, including experience with writing stored procedures, triggers, and functions.\nExperience with performance tuning, query optimization, and indexing.\nFamiliarity with database security and backup strategies.\nStrong analytical and problem-solving skills.\nPreferred Qualifications:\nExposure to cloud database platforms like Azure SQL or AWS RDS.\nExperience in data migration or transformation projects.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Query Optimization', 'MySQL', 'SQL Queries', 'Performance Tuning', 'MS SQL server', 'Stored Procedures', 'Triggers']",2025-06-13 05:54:19
Technical Business Analyst,IT Company,6 - 8 years,Not Disclosed,"['Pune', 'Mumbai (All Areas)']","Technical Business Analysts\n\nLocation: Pune/Mumbai\n\n\nBusiness analysts mainly with data and cloud experience specially around data mapping and data migration. 6-8 years of experience will be good.\n- Experience on Data mapping, Data migration Projects (Azure Projects). Domain not mandatory\n- Experience in mapping source to destination systems, data migration , data lake\n- Good to have experience on Finance Systems\n- No code writing",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business analyst', 'Data Mapping', 'SQL', 'Data Migration']",2025-06-13 05:54:21
Golang Developer,Pratiti Technologies,3 - 8 years,Not Disclosed,[],"â€¢ 4+ years in software development, with a focus on data-intensive applications, cloud solutions, and scalable data architectures.\nâ€¢ Development experience in Go Programming Language (GoLang).\nâ€¢ Amazon AWS experience (EC2, S3, SQS, SNS, Kinesis, ELB, Lambda).\nâ€¢ Experience implementing and Using APIs and understanding of HTTP and REST architecture.\nâ€¢ Experience implementing microservices and delivering to market.\nâ€¢ Experience with both relational & NoSQL databases, writing Stored Procedures, Functions etc.\nâ€¢ Experience with NoSQL databases like DynamoDB/DocumentDB/MongoDB.\nâ€¢ Experience working in a CI/CD environment, and related tools/pipelines/processes.\nâ€¢ Experience with terraform and infrastructure as code (IaC).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'IAM', 'Okta', 'AWS', 'Oidc']",2025-06-13 05:54:22
"Workday implementation, configuration",Ekloud Data Labs,5 - 10 years,"50,000-1.25 Lacs P.A.",[],"Key Requirements\n1. Workday Experience: 5+ years of experience in Workday implementation, configuration, and support.\n2. Technical Skills: Cloud Connect, EIB, Core Connectors, Document Transformation, Workday Studio, Web Services, APIs (WSDL, SOAP, REST).\n3. HCM Expertise: General HCM, Business Process Framework, Reporting, and integrations.\n4. Integration Experience: Building and supporting integrations between Workday HCM and other applications.\n\nNice to Have\n1. M&A Experience: Merger and Acquisition experience.\n2. Workday Certifications: Workday Integration Certifications.\n\nResponsibilities\n1. Design and Develop Integrations: Between Workday HCM and other applications.\n2. Solve Complex Business Problems: Integrating Workday with external applications.\n3. Data Migration: Experience in data migration, including Workday to other HCM ERP systems.\n\nSoft Skills\n1. Strong Communication: Written and verbal communication skills.\n2. Collaboration: Ability to work with cross-functional teams.\n3. Risk Management: Timely identification and escalation of risks.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Workday technical', 'Core Connectors', 'Cloud Connect', 'EIB']",2025-06-13 05:54:24
Sap Ewm Consultant,Cemetrix,6 - 9 years,Not Disclosed,['Indore'],"SAP EWM\nCompany: Yash Technology\nLocation: Indore\nType of employee: 6years (Long term contract to hire)\nMode of work: Hybrid\nNotice period: Immediate or 20 days\n\nInterested candidates kindly share your profiles to padma.ashwitha@gmail.com\n\nJD:\n\nDefine to-be business processes together with the business KU and team\nHelping KU to understand SAP process to get right input for end to end business process\nDesign to-be flow diagrams\nIdentify change impacts and help with GAP approval process\nHPALM Support for the creation test scripts & execution of test sets for BAT* and Validation Tests\nCollect, Draft and approve all business requirements BP** and Risk assessment under HPALM***\nEnsuring right master data for the right warehouse master data and organization setup\nHelp reviewing training documents and prepare demo if required\nReview of the solution provided by integration partner together with business during sprint delivery\nExternal collaboration with 3PL partners for the integration into SAP (External Warehouse, AGV etc.)\nCross team collaboration for integration topics like PP, O2C, 3PL and with other SAP teams\nTesting support and execution in HPALM\nHelp preparing Data migration to respective Data Owners\nCutover preparation for Warehouse master data\n\n\nRegards,\nP. Ashwitha",Industry Type: Recruitment / Staffing,Department: Other,"Employment Type: Full Time, Permanent","['Sap Ewm', 'Sap Le', 'business process logi', 'PP', 'Gap Analysis', 'o2c', 'Process Mapping', 'business acceptance test', 'SAP Functional Consultancy', 'Sprint Review', 'Test Scripts', 'Risk Assessments', '3Pl', 'Data Migration', 'Master Data Management', 'Flow Chart', 'Hp Alm', 'Cutover Management']",2025-06-13 05:54:26
DBA PL/SQL Developer,Softeon,3 - 6 years,Not Disclosed,['Chennai'],"Job Summary:\n\nWe are seeking a skilled and experienced DBA PL/SQL Developer to join our IT team. The ideal candidate will have a strong understanding of Oracle Database Architecture and be proficient in PL/SQL programming. This role involves the design, development, implementation, and maintenance of database solutions to support enterprise applications, ensuring high performance, data integrity, and reliability.\n\nRole & responsibilities:\nDesign and implement database structures including tables, indexes, views, and constraints.\nDevelop and maintain PL/SQL code including procedures, functions, triggers, and packages.\nWrite, debug, and optimize complex SQL and PL/SQL queries for data manipulation and reporting.\nManage Oracle database architecture, including schema design, storage management, and space utilization.\nImplement and manage database backup, recovery, and disaster recovery procedures.\nMonitor and tune database performance for optimal efficiency.\nCollaborate with application developers and project managers to support application requirements.\nEnsure database security, data integrity, and access control compliance.\nExecute data migration and transformation jobs using scripts and tools.\nApply database best practices, standards, and version control processes.\nParticipate in database upgrades, patching, and maintenance activities.\nRequired Skills and Qualifications:\n\nBachelor's degree in Computer Science, Information Technology, or related field.\n3 6 years of hands-on experience with Oracle Database and PL/SQL development.\nIn-depth understanding of Oracle database architecture and administration.\nStrong experience in database design, normalization, and data modeling.\nProven expertise in SQL query optimization and PL/SQL performance tuning.\nKnowledge of backup and recovery strategies using RMAN or equivalent.\nExperience in database security and access control implementation.\nFamiliarity with version control tools (e.g., Git) and agile development methodologies.\nAbility to manage multiple priorities and communicate effectively with stakeholders.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Pl / Sql Development', 'Sql Database Development', 'Oracle SQL', 'Architecture', 'Oracle Database']",2025-06-13 05:54:28
Navision Functional Consultant,KMI Business Technologies,3 - 7 years,3.5-8.5 Lacs P.A.,['Mumbai( Nariman Point )'],"Hi\n\nGreeting from KMI Business technologies!!!!\n\nWe are seeking an experienced Navision Functional Consultant to lead the implementation, configuration, and support of Microsoft Dynamics NAV/Business Central. The ideal candidate will collaborate with business stakeholders to gather requirements, map financial processed, configure modules, and provide user training.\n\nGreetings from KMI Business Technologies Pvt LTD. !!!!!\n\nJob Title: Navision Functional Consultant\n\nJob Description: We are seeking an experienced Navision Functional Consultant to lead the implementation, configuration, and support of Microsoft Dynamics NAV/Business Central. The ideal candidate will collaborate with business stakeholders to gather requirements, map financial processes, configure modules, and provide user training.\n\nKey Responsibilities:\nAnalyze business needs and map them to Navision functionalities.\nConfigure finance, sales, and inventory modules.\nLead data migration and system testing.\nProvide end-user training and support.\nCollaborate with technical teams for customizations.\n\n\nQualifications:\n3+ years of experience in Navision/Dynamics NAV/Business Central.\nStrong accounting and financial process knowledge.\nExcellent problem-solving and communication skills.\n\n\nInterested Candidates please share your profile on the below details:\nE-Mail : Rudhrika.bhatt@kmi.co.in\n\nTotal Experience:\nRelevant Experience:\nExp.in Business Central/ Navision:\nExperience in Implementing and functional support for Accounting Module:\nExperience in Super User/User training:\nExperience in Functional Rale:\nCurrent Location:\nCurrent CTC:\nExpected CTC:\nNotice Period:\nReason for Change:\nThanks & Regards,\nRudhrika Bhatt\nAssitant Manager HR\n1008 Dalamal House, Nariman Point,\nMumbai 400 021, India\nE-Mail : Rudhrika.bhatt@kmi.co.in",Industry Type: Telecom / ISP,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Business Central', 'Navision', 'User Training', 'Functional Support', 'Implementation And Maintenance', 'NAV', 'Super User', 'ERP Implementation', 'Microsoft Dynamics NAV', 'ERP Functional']",2025-06-13 05:54:30
Software Engineer Staff,Fourkites,9 - 14 years,30-45 Lacs P.A.,[],"At FourKites we have the opportunity to tackle complex challenges with real-world impacts. Whether its medical supplies from Cardinal Health or groceries for Walmart, the FourKites platform helps customers operate global supply chains that are efficient, agile and sustainable.\nJoin a team of curious problem solvers that celebrates differences, leads with empathy and values inclusivity.\nAs a Staff Software Engineer, you will get an opportunity to work on features end to end (backend & frontend) using the latest technologies such as RoR, Java, GoLang, Angular, React, Redis, PostgreSQL. You will develop products that can change the logistics landscape and will be used by some of the biggest corporations in the world. You will develop integrations with our strategic partners to help expand our ecosystem. You will work closely with our US team and customers to develop features that help shape the logistics and supply chain industry.\nWho you are:\nBachelorâ€™s degree in Computer Science & Engineering or related field from a reputed institution.\nMinimum of 8 years of experience in Software Engineering and Web application development.\nGood understanding of software design, Microservices architecture, object-oriented principles, and design patterns.\nExperience with Design and development of highest quality software/services using RoR/Golang/Java.\nGood knowledge of RESTful APIs and microservices architecture\nStrong understanding of Java, Spring Framework, and object-oriented programming principles\nExperience in one of Azure, Amazon Web Services or other cloud services.\nExperience with databases such as MySQL, PostgreSQL, or MongoDB\nFamiliarity with front-end technologies such as HTML, CSS, and JavaScript is a plus\nStrong knowledge of Git (branches, submodules, rebasing) and other Agile tools such as JIRA & Confluence.\nAgile SDLC experience\nExcellent oral and written communication skills\nWhat youâ€™ll be doing:\nDesign, architect, implement, test, profile, release, and optimize highest quality software/services using RoR/Golang/Java.\nPartner with product manages to analyse product requirements and plan engineering execution\nDocument HLD/LLD for easy knowledge sharing and future scaling\nPerform design and code reviews\nImplement code with very high coverage of unit tests and component tests\nCross-training peers and mentoring teammates\nPossess expert knowledge in performance, security, scalability, architecture, and best practices\nFunctionally decompose complex problems into simple, straight-forward solutions\nCollaborate with UX designers to develop responsive user interface components\nWorking knowledge of SQL based (any RDBMS) and NOSQL data stores (any one) with the ability to write intermediate level SQL\nExperience in building Web application backends using Java Spring Boot or similar\nExperience with frontend libraries/frameworks such as React/Angular is a plus.\nEducation Qualification: Graduate from B.E/ B.Tech / MCA / M.Tech Background.\nWho we are:\nFourKitesÂ®, the leader in AI-driven supply chain transformation for global enterprises and pioneer of real-time visibility, turns supply chain data into automated action. FourKitesâ€™ Intelligent Control Towerâ„¢ breaks down enterprise silos by creating a real-time digital twin of orders, shipments, inventory and assets. This comprehensive view, combined with AI-powered digital workers, enables companies to prevent disruptions, automate routine tasks, and optimize performance across As the leader in AI-driven supply chain transformation, FourKites pioneered the Intelligent Control Towerâ„¢ powered by the worldâ€™s largest real-time visibility network. Our platform creates comprehensive digital twins of your supply chain with AI-powered digital workers to automate resolution, improve collaboration and drive outcomes across all stakeholders. Unlike traditional control towers, we enable true real-time execution and intelligent fulfillment, transforming both your supply and customer\nBenefits\nMedical benefits start on first day of employment\n36 PTO days( Sick, Casual and Earned) , 5 recharge days, 2 volunteer days\nHome Office setups and Technology reimbursement\nLifestyle & Family benefits\nAnnual Swags/ Festive Swags\nOngoing learning & development opportunities ( Professional development program, Toast Master club etc.)",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Golang', 'Casandra', 'SQL Database', 'Ci/Cd', 'Elastic Search', 'Python']",2025-06-13 05:54:32
OIC Developer,Estuate Software,5 - 10 years,Not Disclosed,['Hyderabad'],"in a\nJob Title:\nOracle Integration Cloud (OIC) Developer OTC & Finance Modules\nLocation:\nHyderabad, India (Hybrid)\n3 days onsite, 2 days remote per week\nJob Type:\nFull-Time | Permanent\nExperience Required:\n58 years of relevant experience\nJob Summary:\nWe are hiring a skilled Oracle Integration Cloud (OIC) Developer to join our ERP team in Hyderabad. This role requires hands-on experience with Oracle Fusion integrations, particularly across the Order to Cash (OTC) lifecycle and Finance modules (GL, AR, AP). You will design, develop, and maintain integration solutions that connect Oracle Fusion Cloud with external and internal systems, helping streamline business operations.\nKey Responsibilities:\nDesign, develop, and deploy integration solutions using Oracle Integration Cloud (OIC).\nBuild integrations across Order to Cash (O2C) modules including Order Management, AR, and Shipping.\nWork on financial integrations with modules such as General Ledger (GL), Accounts Payable (AP), and Accounts Receivable (AR).\nUse tools like FBDI, BI Publisher, and ESS Jobs for data migration and reporting.\nDevelop reusable mappings and templates, handle error logging, and support issue resolution in production.\nCollaborate with business analysts and functional consultants to translate business requirements into technical solutions.\nParticipate in SIT, UAT, and Go-Live activities.\nRequired Skills:\n3+ years of experience with Oracle Integration Cloud (OIC).\nStrong understanding of Oracle Fusion ERP, especially in OTC and Financials.\nProven experience with SOAP/REST Web Services, XSLT, and data transformation.\nHands-on experience with FBDI, BI Publisher, and ERP Adapter configurations.\nKnowledge of cloud-to-cloud and cloud-to-on-premise integration patterns.\nExperience with debugging and optimizing integration flows.\nNice to Have:\nOracle certifications in OIC or Fusion ERP.\nExperience with tools like JIRA, Git, and Agile methodologies.\nPrior experience working in a shared services or global delivery model.\nEducational Qualification:\nBachelor's degree in Computer Science, Engineering, or a related field.\nPerks & Benefits:\nCompetitive salary package\nHybrid work environment (3 days in-office)\nHealth insurance and wellness benefits\nOpportunities for learning and Oracle certification support\nExposure to global ERP transformation projects Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Order To Cash', 'Oracle Integration Cloud', 'O2C', 'Oic']",2025-06-13 05:54:34
Salesforce Developer,Leading Client,5 - 10 years,Not Disclosed,['Hyderabad'],"Name of Projects in which the skills were used (add rows if necessary)\nNo: of months worked in each Project of work done using the skill (Mandatory/ optional)\nApex Customization - M\nSalesforce Configuration - M\nLWC - M\nJavascript/Jquery - M\nSlack - O\nMulesoft - O\nDesign, develop, and deploy customized Salesforce solutions using Apex, Visualforce, and Lightning components Collaborate with stakeholders to gather requirements and translate them into technical designs Create and maintain workflows, process builders, triggers, and validation rules Perform data migration using Data Loader or third-party tools Integrate Salesforce with external systems via REST/SOAP APIs Ensure unit testing, deployment through CI/CD tools, and post-deployment support Maintain platform security and ensure adherence to Salesforce best practices",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Salesforce', 'Lightning Web Components', 'Slack', 'JavaScript', 'Jquery', 'Salesforce Lightning', 'Mulesoft']",2025-06-13 05:54:35
Database Administrator,Prudent Globaltech Solutions,7 - 12 years,Not Disclosed,['Hyderabad'],"DBA\nCreate and maintain optimal data pipeline architecture.\nAssemble large, complex data sets that meet functional/non-functional business requirements.\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. using Python/open source technologies.",,,,"['Database Administration', 'Postgresql', 'Cloud', 'ETL', 'OLTP']",2025-06-13 05:54:37
Java Full Stack Developer,Prudent Globaltech Solutions,7 - 12 years,Not Disclosed,['Hyderabad'],"We are looking for a senior java developer to play a key role in the development and support of our company's application. This includes designing, developing, consulting, and mentoring developers/business partners regarding real estate application voucher/deal/invoice implementation. As a member of our development team, your communication and analytical skills are essential to this role.\nJOB DESCRIPTION\nOverall 10+ of experience, 7+ years programming as a software developer\n2-3 years of hands-on with SQL",,,,"['Ci/Cd', 'Java Fullstack', 'Microservices', 'Angular', 'Java', 'Spring Boot', 'azure']",2025-06-13 05:54:38
Salesforce Developer,Prudent Globaltech Solutions,7 - 12 years,Not Disclosed,['Hyderabad'],"Requirement\n6+ years of technical expertise on Salesforce.com, using Force.com Apex, Visual Force, JavaScript, HTML5, CSS, Ant, and the use of these tools with Salesforce.com\nAt least 1.5 years of experience in Salesforce LWC (Lightning Web Components)\nHands-on experience with customization (Apex Triggers, Classes, Visual Force & Flows)\nStrong with configuration, customization, programming with Apex APIs, Apex Triggers, Flows, Workflows with actions, Approval Processes, Reports & Dashboards\nExperience in org/data migration projects from legacy systems to Salesforce\nExpertise in data migration tools and techniques\nFamiliarity with Salesforce.com best practices, support mechanisms, procedures and limitations are required\nPlans, designs, develops and maintains multiple complex salesforce implementations\nCollaborate across multiple scrum teams; Follow development, testing, and deployment practices that are leading edge on the Salesforce platform\nAnalyzes problems, conducts root cause analysis, helps in the resolution of problems using defined problem management procedure, and helps in application support and maintenance of customer applications\nExperience with at least full life cycle implementations on Salesforce.com with 500+ user licenses\nStrong knowledge of Agile Software development methodologies, design, and architectural patterns\nWork closely with QA to review test plans and test cases and ensure the quality of deliverables from the team\nExperience with MuleSoft is preferred\nExcellent written and verbal communication skills\nCertifications:\nSalesforce Platform Developer II (PDII) certification is required",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Apex Programming', 'Lwc', 'Soql', 'Sosl', 'Salesforce Lightning', 'Salesforce Integration']",2025-06-13 05:54:40
DevOps Engineer,Easemytrip,1 - 6 years,Not Disclosed,"['Noida', 'Gurugram']","About the Role:\nAs a DevOps Engineer at EaseMyTrip.com, you will be pivotal in optimizing and maintaining our IT infrastructure and deployment processes. Your role involves managing cloud environments, implementing automation, and ensuring seamless deployment of applications across various platforms. You will collaborate closely with development teams to enhance system reliability, security, and efficiency, supporting our mission to provide exceptional travel experiences through robust technological solutions. This position is critical for maintaining high operational standards and driving continuous innovation.\n\nRole & responsibilities:\nCloud Computing Mastery: Expert in managing Amazon Web services (AWS) environments, with skills in GCP and Azure for comprehensive cloud solutions and automation.\nWindows Server Expertise: Profound knowledge of configuring and maintaining Windows Server systems and Internet Information Services (IIS).\nDeployment of .NET Applications: Experienced in deploying diverse .NET applications such as ASP.Net, MVC, Web API, and WCF using Jenkins.\nProficiency in Version Control: Skilled in utilizing GitLab or GitHub for effective version control and collaboration.\nLinux Server Management: Capable of administering Linux servers with a focus on security and performance optimizations.\nScripting and Automation: Ability to write and maintain scripts for automation of routine tasks to improve efficiency and reliability.\nMonitoring and Optimization: Implement monitoring tools to ensure high availability and performance of applications and infrastructure.\nSecurity Best Practices: Knowledge of security protocols and best practices to safeguard systems and data.\nContinuous Integration/Continuous Deployment (CI/CD): Develop and maintain CI/CD pipelines to streamline software updates and deployments.\nCollaboration and Support: Work closely with development teams to troubleshoot deployment issues and enhance the overall operational efficiency.\n\nPreferred candidate profile:\nMigration Project Leadership: Experienced in leading significant migration projects from planning through to execution.\nDatabase Expertise: Strong foundation in both SQL and NoSQL database technologies.\nExperience with Diverse Tech Stacks: Managed projects involving various technologies, including 2-tier, 3-tier, and microservices architectures.\nProficiency in Automation Tools: Hands-on experience with automation and deployment tools such as Jenkins, Bamboo, and Code Deploy.\nAdvanced Code Management: Highly skilled in managing code revisions and maintaining code integrity across multiple platforms.\nStrategic DevOps Experience: Proven track record in developing and implementing DevOps strategies at an enterprise level.\nConfiguration Management Skills: Proficient in using tools like Ansible, Chef, or Puppet for configuration management.\nTechnology Versatility: Experience working with a range of programming languages and frameworks, including .NET, MVC, LAMP, Python, and NodeJS.\nProblem Solving and Innovation: Ability to solve complex technical issues and innovate new solutions to enhance system reliability and performance.\nEffective Communication: Strong communication skills to collaborate with cross-functional teams and articulate technical challenges and solutions clearly.",Industry Type: Travel & Tourism,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Code Deploy', 'Aws Cloud', 'Ci/Cd', 'Devops', 'Devops Automation', 'Apm Tools', 'Cloud Automation Devops', 'Devops Jenkins', 'Aws Codedeploy', 'Aws Api Gateway', 'Vpc', 'Load Balancing', 'Route3', 'Code Pipeline', 'Auto Scaling', 'Pipeline', 'Elastic Cache', 'Aws Devops', 'Aws Lambda', 'Build', 'New Relic', 'Azure Devops', 'Cloudfront']",2025-06-13 05:54:42
Sap Abap Technical Consultant,Varun Info Tech Consulting,2 - 3 years,6-9 Lacs P.A.,['Hyderabad( Madhapur )'],"Roles and Responsibilities\nIntegrate SAP with Salesforce using OData Rest APIs.\nDesign, develop, test, and deploy SAP ABAP applications using OOPS concepts.\nExpert knowledge in LSMW and LTMC for data migration.\n\nImplement & Support Adobe forms.\n\nFamiliarity with Fiori App developments and enhancements.\nDesired Candidate Profile\n2 to 3 years of experience in SAP ABAP development, Preferred on HANA platform.\nBachelor's degree in Computers or Information Technology (B. Tech/B.E.).\nStrong understanding of Object-Oriented Programming (OOP) principles and design patterns.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Odata', 'LSMW', 'Sap Abap Hana', 'Sap Fiori', 'OO ABAP', 'Adobe Forms', 'Cds Views', 'LTMC']",2025-06-13 05:54:44
Salesforce Developer,Varun Info Tech Consulting,2 - 4 years,3-6 Lacs P.A.,['Hyderabad( Madhapur )'],"Roles and Responsibilities\nDesign, develop, test, deploy, and maintain Salesforce solutions to meet business requirements.\nCollaborate with cross-functional teams to gather requirements and deliver high-quality solutions.\nTroubleshoot issues related to Salesforce platform, including data migration, integration, and performance optimization.\nDevelop custom components using Apex programming language and LWC (Lightning Web Components).\nEnsure adherence to coding standards, best practices, and company policies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Salesforce Sales Cloud', 'Salesforce Service Cloud', 'Salesforce Certification', 'Sales Force Development', 'Salesforce Integration']",2025-06-13 05:54:45
