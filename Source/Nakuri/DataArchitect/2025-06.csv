job_title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Data Architect,Acesoft,6 - 10 years,19-22.5 Lacs P.A.,['Bengaluru'],"Hi all,\nWe are hiring fore the Data Architecture\nExperience: 6 - 9 years\nLocation: Bangalore\nNotice Period: Immediate - 15 Days\nSkills:\nData Architecture\nAzure Data Factory\nAzure Data Bricks\nAzure Cloud\nArchitecture\n\nIf you are interested drop your resume at mojesh.p@acesoftlabs.com\nCall: 9701971793",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Azure Data Factory', 'Architecture', 'Azure Databricks', 'Data Architecture', 'Azure Synapse', 'Data Modeling']",2025-06-12 06:05:55
Data Architect,CGI,8 - 12 years,15-30 Lacs P.A.,['Hyderabad'],"Job Title: Data Architect / Data Modeler\nExperience Level: 8+ Years\nLocation: Hyderabad\nJob Summary\nWe are seeking a highly experienced Data Architect to join our growing Data & Analytics team. This role demands a strategic thinker and technical expert who can design and build robust, scalable, and efficient data solutions. You will play a critical role in architecting end-to-end data pipelines, designing optimized data models, and delivering business-centric data infrastructure using cutting-edge technologies such as Python, PySpark, SQL, Snowflake, and/or Databricks.\nThe ideal candidate will have a deep understanding of data engineering best practices and a proven track record of enabling data-driven decision-making through innovative and scalable data solutions.\nKey Responsibilities\nArchitect & Design Scalable Data Pipelines\nLead the design and implementation of high-performance, scalable, and maintainable data pipelines that support batch and real-time processing.\nData Modeling & Data Architecture\nDesign and implement optimized data models and database schemas to support analytics, reporting, and machine learning use cases.\nCloud Data Platforms\nDevelop and manage modern cloud-based data architectures using platforms like Snowflake or Databricks, ensuring performance, security, and cost-efficiency.\nData Integration & ETL Development\nBuild robust ETL/ELT workflows to ingest, transform, and provision data from a variety of internal and external sources.\nCollaboration with Stakeholders\nWork closely with data analysts, data scientists, product managers, and business leaders to translate business requirements into technical specifications and data solutions.\nData Quality & Governance\nImplement and advocate for best practices in data quality, security, compliance, lineage, and governance.\nPerformance Optimization\nOptimize data storage and query performance using advanced SQL, partitioning, indexing, caching strategies, and compute resource tuning.\nMentorship & Best Practices\nProvide mentorship to junior engineers, establish coding standards, and contribute to the growth and maturity of the data engineering practice.\nRequired Qualifications\nBachelors or Master’s degree in Computer Science, Engineering, Data Science, or a related field.\n8+ years of experience in data engineering or related roles.\nStrong expertise in Python and PySpark for data processing and transformation.\nProficient in advanced SQL with a deep understanding of query optimization and performance tuning.\nHands-on experience with Snowflake and/or Databricks in a production environment.\nExperience in designing and implementing data warehouses and data lakes.\nSolid understanding of distributed computing frameworks, big data ecosystems, and modern data architecture patterns.\nExperience with CI/CD, version control systems (e.g., Git), and workflow orchestration tools (e.g., Airflow, dbt, etc.).\nStrong communication skills with the ability to clearly articulate technical concepts to non-technical stakeholders.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'sql', 'snowflake', 'Data Architecture']",2025-06-12 06:05:57
Data Architect,Ford,14 - 17 years,Not Disclosed,['Chennai'],"We are looking for Data Solution Architect to join FC India IT Architecture team. In this role, you will define analytics solutions and guide engineering teams to implement big data solutions on the cloud. Work involves migrating data from legacy on-prem warehouses to Google cloud data platform. This role will provide architecture assistance to data engineering teams in India, with key responsibility of supporting applications globally. This role will also drive business adoption of the new platform and sunset of legacy platforms.\nGoogle Professional Solution Architect certification.\n8+ years of relevant work experience in analytics application and data architecture, with deep understanding of cloud hosting concepts and implementations.\n5+ years experience in Data and Solution Architecture in analytics space. Solid knowledge of cloud data architecture, data modelling principles, and expertise in Data Modeling tools.\nExperience in migrating legacy analytics applications to Cloud platform and business adoption of these platforms to build insights and dashboards through deep knowledge of traditional and cloud Data Lake, Warehouse and Mart concepts.\nGood understanding of domain driven design and data mesh principles.\nExperience with designing, building, and deploying ML models to solve business challenges using Python/BQML/Vertex AI on GCP.\nKnowledge of enterprise frameworks and technologies. Strong in architecture design patterns, experience with secure interoperability standards and methods, architecture tolls and process.\nDeep understanding of traditional and cloud data warehouse environment, with hands on programming experience building data pipelines on cloud in a highly distributed and fault-tolerant manner. Experience using Dataflow, pub/sub, Kafka, Cloud run, cloud functions, Bigquery, Dataform, Dataplex , etc.\nStrong understanding on DevOps principles and practices, including continuous integration and deployment (CI/CD), automated testing & deployment pipelines.\nGood understanding of cloud security best practices and be familiar with different security tools and techniques like Identity and Access Management (IAM), Encryption, Network Security, etc. Strong understanding of microservices architecture.\nNice to Have\nBachelor s degree in Computer science/engineering, Data science or related field.\nStrong leadership, communication, interpersonal, organizing, and problem-solving skills\nGood presentation skills with ability to communicate architectural proposals to diverse audiences (user groups, stakeholders, and senior management).\nExperience in Banking and Financial Regulatory Reporting space.\nAbility to work on multiple projects in a fast paced & dynamic environment.\nExposure to multiple, diverse technologies, platforms, and processing environments.\nUtilize Google Cloud Platform & Data Services to modernize legacy applications.\nUnderstand technical business requirements and define architecture solutions that align to Ford Motor & Credit Companies Patterns and Standards.\nCollaborate and work with global architecture teams to define analytics cloud platform strategy and build Cloud analytics solutions within enterprise data factory.\nProvide Architecture leadership in design & delivery of new Unified data platform on GCP.\nUnderstand complex data structures in analytics space as well as interfacing application systems. Develop and maintain conceptual, logical & physical data models. Design and guide Product teams on Subject Areas and Data Marts to deliver integrated data solutions.\nProvide architectural guidance for optimal solutions considering regional Regulatory needs.\nProvide architecture assessments on technical solutions and make recommendations that meet business needs and align with architectural governance and standard.\nGuide teams through the enterprise architecture processes and advise teams on cloud-based design, development, and data mesh architecture.\nProvide advisory and technical consulting across all initiatives including PoCs, product evaluations and recommendations, security, architecture assessments, integration considerations, etc.\nLeverage cloud AI/ML Platforms to deliver business and technical requirements.",Industry Type: Auto Components,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Data modeling', 'Access management', 'Enterprise architecture', 'Consulting', 'Network security', 'Data structures', 'Analytics', 'Python']",2025-06-12 06:06:00
Data Architect,Wissen Infotech,7 - 10 years,12-22 Lacs P.A.,['Bengaluru'],"Be the digital functional owner of their scope: understand the technical solution, describe it to business stakeholders, and suggest evolutions to meet new business needs - Work closely with stakeholders to understand business objectives, gather requirements, and translate them into comprehensive functional specifications. - Collaborate with software development teams to communicate business requirements, validate technical feasibility, and ensure alignment with project goals - Conduct thorough analysis of end-to-end processes and data flows, identifying areas for improvement and proposing innovative solutions - Document stories, use cases, and process flows to ensure clear communication between business and technical teams - Facilitate workshops, meetings, and training sessions to promote a shared understanding of business analysis methodologies and foster a culture of continuous improvement - Partner with software development leaders to ensure that business requirements are clearly communicated, validated, and integrated into development initiatives - Champion a customer-centric approach to business analysis, seeking to understand and address the needs of end-users and stakeholders - Foster a collaborative and growth-oriented team environment, promoting knowledge sharing and skill development - Troubleshoot and resolve integration issues, ensuring minimal disruption to business operations - Participate in testing and validation activities to ensure solutions meet business requirements and quality standards Internal - Participate in architecture review board meetings and make strategic recommendations for cloud architecture Qualifications - 4+ years experience working in IT product, analyst, functional or architecture roles - Bachelor’s degree in business administration, Information Technology, or related field (technical background required) - Comprehensive knowledge and hands-on experience with MDM environments - Strong analytical and problem-solving skills - Excellent communication and collaboration skills - Ability to manage multiple projects simultaneously and prioritize tasks effectively - Strong understanding of business analysis methodologies, tools, and best practices within a software development context - A desire for continuous learning and stay updated with emerging technologies - Experience in Agile methodologies and leading business analysis efforts in Agile environments - Understanding of service business processes is a plus - Experience with Informatica Cloud and AWS environments is a plus - Certification in Business Analysis (e.g., CBAP) is a plus Skills - Due to the nature of this position sitting on a global team, fluent English communication skills (written & spoken) are required - Technical awareness to understand and validate development proposals against functional requirements - Experience with BI tools such as Power BI, Tableau, Quicksight - Ability to see the consumer perspective and act as their advocate - Strong interpersonal skills, with ability to communicate and convince at various levels of the organization, and in a multicultural environment - Ability to effectively multi-task and manage priorities - Strong analytical and synthesis skills - Initiative to uncover and solve problems proactively - Ability to understand complex software development environments Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power BI', 'Tableau', 'Quicksight']",2025-06-12 06:06:02
Data Architect,Ford,8 - 13 years,Not Disclosed,['Chennai'],"We are looking for Data Solution Architect to join FC India IT Architecture team. In this role, you will define analytics solutions and guide engineering teams to implement big data solutions on the cloud. Work involves migrating data from legacy on-prem warehouses to Google cloud data platform. This role will provide architecture assistance to data engineering teams in India, with key responsibility of supporting applications globally. This role will also drive business adoption of the new platform and sunset of legacy platforms.\nGoogle Professional Solution Architect certification.\n8+ years of relevant work experience in analytics application and data architecture, with deep understanding of cloud hosting concepts and implementations.\n5+ years experience in Data and Solution Architecture in analytics space. Solid knowledge of cloud data architecture, data modelling principles, and expertise in Data Modeling tools.\nExperience in migrating legacy analytics applications to Cloud platform and business adoption of these platforms to build insights and dashboards through deep knowledge of traditional and cloud Data Lake, Warehouse and Mart concepts.\nGood understanding of domain driven design and data mesh principles.\nExperience with designing, building, and deploying ML models to solve business challenges using Python/BQML/Vertex AI on GCP.\nKnowledge of enterprise frameworks and technologies. Strong in architecture design patterns, experience with secure interoperability standards and methods, architecture tolls and process.\nDeep understanding of traditional and cloud data warehouse environment, with hands on programming experience building data pipelines on cloud in a highly distributed and fault-tolerant manner. Experience using Dataflow, pub/sub, Kafka, Cloud run, cloud functions, Bigquery, Dataform, Dataplex , etc.\nStrong understanding on DevOps principles and practices, including continuous integration and deployment (CI/CD), automated testing & deployment pipelines.\nGood understanding of cloud security best practices and be familiar with different security tools and techniques like Identity and Access Management (IAM), Encryption, Network Security, etc. Strong understanding of microservices architecture.\nNice to Have\nBachelor s degree in Computer science/engineering, Data science or related field.\nStrong leadership, communication, interpersonal, organizing, and problem-solving skills\nGood presentation skills with ability to communicate architectural proposals to diverse audiences (user groups, stakeholders, and senior management).\nExperience in Banking and Financial Regulatory Reporting space.\nAbility to work on multiple projects in a fast paced & dynamic environment.\nExposure to multiple, diverse technologies, platforms, and processing environments.\nUtilize Google Cloud Platform & Data Services to modernize legacy applications.\nUnderstand technical business requirements and define architecture solutions that align to Ford Motor & Credit Companies Patterns and Standards.\nCollaborate and work with global architecture teams to define analytics cloud platform strategy and build Cloud analytics solutions within enterprise data factory.\nProvide Architecture leadership in design & delivery of new Unified data platform on GCP.\nUnderstand complex data structures in analytics space as well as interfacing application systems. Develop and maintain conceptual, logical & physical data models. Design and guide Product teams on Subject Areas and Data Marts to deliver integrated data solutions.\nProvide architectural guidance for optimal solutions considering regional Regulatory needs.\nProvide architecture assessments on technical solutions and make recommendations that meet business needs and align with architectural governance and standard.\nGuide teams through the enterprise architecture processes and advise teams on cloud-based design, development, and data mesh architecture.\nProvide advisory and technical consulting across all initiatives including PoCs, product evaluations and recommendations, security, architecture assessments, integration considerations, etc.\nLeverage cloud AI/ML Platforms to deliver business and technical requirements.",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Data modeling', 'Access management', 'Enterprise architecture', 'Consulting', 'Network security', 'Data structures', 'Analytics', 'Python']",2025-06-12 06:06:04
Data Architect,Opus Technologies,10 - 16 years,35-50 Lacs P.A.,['Pune'],"Role & responsibilities\nDefine and evolve data engineering & analytics offerings aligned with payments domain needs (e.g., transaction analytics, fraud detection, customer insights).\nLead reference architecture creation for data modernization, real-time analytics, and cloud-native data platforms (e.g., Azure Synapse, GCP BigQuery).\nBuild reusable components, PoCs, and accelerators for ingestion, transformation, data quality, and governance.\nSupport pre-sales engagements with solution design, estimation, and client workshops.\nGuide delivery teams on data platform implementation, optimization, and security.\nMentor and upskill talent pool through learning paths and certifications.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Spark', 'Python', 'Azure Data Factory']",2025-06-12 06:06:06
Data Architect,"NTT DATA, Inc.",3 - 7 years,Not Disclosed,['Bengaluru'],"Additional Career Level Description\n\n\nKnowledge and application\nApplies advanced wide-ranging experience and in-depth professional knowledge to develop and resolve complex models and procedures in creative way .\nDirects the application of existing principles and guides development of new policies and ideas.\nDetermines own methods and procedures on new assignments .\n\n\n\nProblem solving\nUnderstands and works on complex issues where analysis of situation or data requires an in-depth evaluation of variable factors, solutions may need to be devised from limited informatio n.\nExercises judgment in selecting methods, evaluating, adapting of complex techniques and evaluation criteria for obtaining results.\n\n\n\nInteraction\nFrequently advises key people outside own area of expertise on complex matters, using persuasion in delivering messages.\n\n\n\nImpact\nDevelops and manages operational initiatives to deliver tactical results and achieve medium-term goals.\n\n\n\nAccountability\nMay be accountable through team for delivery of tactical business targets .\nWork is reviewed upon completion and is consistent with departmental objectives.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Architecture', 'data modeling', 'Data Architect', 'artificial intelligence', 'sql']",2025-06-12 06:06:09
Data Architect / Engagement Lead,Ignitho,7 - 10 years,Not Disclosed,['Chennai( Sholinganallur )'],"Job Title: Data Architect / Engagement Lead\nLocation: Chennai\nReports To: CEO\n\nAbout the Company:\nIgnitho Inc. is a leading AI and data engineering company with a global presence, including US, UK, India, and Costa Rica offices.\nVisit our website to learn more about our work and culture: www.ignitho.com.\nIgnitho is a portfolio company of Nuivio Ventures Inc., a venture builder dedicated to developing Enterprise AI product companies across various domains, including AI, Data Engineering, and IoT.\nLearn more about Nuivio at: www.nuivio.com.\n\nJob Summary:\nAs the Data Architect and Engagement Lead, you will define the data architecture strategy and lead client engagements, ensuring alignment between data solutions and business goals. This dual role blends technical leadership with client-facing responsibilities.\n\nKey Responsibilities:\nDesign scalable data architectures, including storage, processing, and integration layers.\nLead technical discovery and requirements gathering sessions with clients.\nProvide architectural oversight for data and AI solutions.\nAct as a liaison between technical teams and business stakeholders.\nDefine data governance, security, and compliance standards.\n\nRequired Qualifications:\nBachelors or Masters in computer science, Information Systems, or similar.\n7+ years of experience in data architecture, with client-facing experience.\nDeep knowledge of data modelling, cloud data platforms (Snowflake / BigQuery/ Redshift / Azure), and orchestration tools.\nExcellent communication, stakeholder management, and technical leadership skills.\nFamiliarity with AI/ML systems and their data requirements is a strong plus.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Data Modeling', 'Azure Cloud', 'Bigquery', 'Redshift Aws', 'Artificial Intelligence', 'Snowflake', 'Machine Learning']",2025-06-12 06:06:11
Data Architect,"NTT DATA, Inc.",8 - 13 years,Not Disclosed,['Chennai'],"Req ID: 324664\n\nWe are currently seeking a Data Architect to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDevelop and articulate long-term strategic goals for data architecture vision and establish data standards for enterprise systems.\n\nUtilize various cloud technologies, including Azure, AWS, GCP, and data platforms like Databricks and Snowflake.\n\nConceptualize and create an end-to-end vision outlining the seamless flow of data through successive stages.\n\nInstitute processes for governing the identification, collection, and utilization of corporate metadata, ensuring accuracy and validity.\n\nImplement methods and procedures for tracking data quality, completeness, redundancy, compliance, and continuous improvement.\n\nEvaluate and determine governance, stewardship, and frameworks for effective data management across the enterprise.\n\nDevelop comprehensive strategies and plans for data capacity planning, data security, life cycle data management, scalability, backup, disaster recovery, business continuity, and archiving.\n\nIdentify potential areas for policy and procedure enhancements, initiating changes where required for optimal data management.\n\nFormulate and maintain data models and establish policies and procedures for functional design.\n\nOffer technical recommendations to senior managers and technical staff in the development and implementation of databases and documentation.\n\nStay informed about upgrades and emerging database technologies through continuous research.\n\nCollaborate with project managers and business leaders on all projects involving enterprise data.\n\nDocument the data architecture and environment to ensure a current and accurate understanding of the overall data landscape.\n\nDesign and implement data solutions tailored to meet customer needs and specific use cases.\n\nProvide thought leadership by recommending the most suitable technologies and solutions for various use cases, spanning from the application layer to infrastructure.\n\nBasic Qualifications:\n\n8+ years of hands-on experience with various database technologies\n\n6+ years of experience with Cloud-based systems and Enterprise Data Architecture, driving end-to end technology solutions.\n\nExperience with Azure, Databricks, Snowflake\n\nKnowledgeable on concepts of GenAI\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nPossess certifications in AWS, Azure, and GCP to complement extensive hands-on experience.\n\nDemonstrated expertise with certifications in Snowflake.\n\nValuable ""Big 4"" Management Consulting experience or exposure to multiple industries.\n\nUndergraduate or graduate degree preferred.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['enterprise information architecture', 'microsoft azure', 'gcp', 'database creation', 'aws', 'snowflake', 'data life cycle management', 'metadata', 'data management', 'data security', 'data warehousing', 'data architecture', 'sql', 'data bricks', 'data quality', 'database implementation']",2025-06-12 06:06:14
Data Architect,Coforge,11 - 16 years,Not Disclosed,"['Noida', 'Greater Noida', 'Delhi / NCR']","-Data Architect Department:\nData & Analytics The Data Architect having more than 14 years of experience and should play a pivotal role in designing, developing, and governing scalable data architectures to support enterprise-wide data integration, analytics, and reporting.\nThis role will focus on creating unified data models, optimizing data pipelines, and ensuring compliance with regulatory standards (GDPR) using cloud-based platforms.\nThe ideal candidate is a strategic thinker with deep expertise in data modeling, cloud data platforms, and governance.",,,,"['Data Migration', 'Data Warehousing', 'Data Modeling', 'Informatica', 'SSIS', 'ETL Tool']",2025-06-12 06:06:16
AWS Data Architect (Standard),Infogain,12 - 14 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nDesign and implement scalable, reliable, and high-performance data architectures to support business\nneeds.\nDevelop and maintain real-time data streaming solutions using Kafka and other streaming\ntechnologies.\nUtilize AWS cloud services to build and manage data infrastructure, ensuring security, performance,\nand cost optimization.\nCreate efficient and optimized data models for structured and unstructured datasets.\nDevelop, optimize, and maintain SQL queries for data processing, analysis, and reporting.\nWork with cross-functional teams to define data requirements and implement solutions that align with\nbusiness goals.\nImplement ETL/ELT pipelines using Python and other relevant tools.\nEnsure data quality, consistency, and governance across the organization.\nTroubleshoot and resolve issues related to data pipelines and infrastructure.\nRequired Skills and Qualifications:\nExperience in Data Engineering and Architecture.\nProficiency in Python for data processing and automation.\nStrong expertise in AWS (S3, Redshift, Glue, Lambda, EMR, etc.) for cloud-based data solutions.\nHands-on experience with Kafka for real-time data streaming.\nDeep understanding of data modeling principles for transactional and analytical workloads.\nStrong knowledge of SQL for querying and performance optimization.\nExperience in building and maintaining ETL/ELT pipelines.\nFamiliarity with big data technologies like Spark, Hadoop, or Snowflake is a plus.\nStrong problem-solving skills and ability to work in a fast-paced environment.\nExcellent communication and stakeholder management skills\nEXPERIENCE\n12-14 Years\nSKILLS\nPrimary Skill: Data Engineering\nSub Skill(s): Data Engineering\nAdditional Skill(s): Kafka, Python, Data Modeling, ETL, Data Architecture, SQL, Redshift, Pyspark",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data modeling', 'Analytical', 'Data processing', 'Data quality', 'Stakeholder management', 'AWS', 'SQL', 'Python', 'Data architecture']",2025-06-12 06:06:19
Data Architect,.,7 - 12 years,20-35 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Job Description\nWe are seeking a highly skilled Azure Data Engineer with strong expertise in Data Architecture, PySpark/Python, Azure Databricks, and data streaming solutions. The ideal candidate will have hands-on experience in designing and implementing large-scale data pipelines, along with solid knowledge of data governance and data modeling.\nKey Responsibilities\nDesign, develop, and optimize PySpark/Python-based data streaming jobs on Azure Databricks.\nBuild scalable and efficient data pipelines for batch and real-time processing.\nImplement data governance policies, ensuring data quality, security, and compliance.\nDevelop and maintain data models (dimensional, relational, NoSQL) to support analytics and reporting.\nCollaborate with cross-functional teams (data scientists, analysts, and business stakeholders) to deliver data solutions.\nTroubleshoot performance bottlenecks and optimize Spark jobs for efficiency.\nEnsure best practices in CI/CD, automation, and monitoring of data workflows.\nMentor junior engineers and lead technical discussions (for senior/managerial roles).\nMandatory Skills & Experience\n5+ years of relevant experience as a Data Engineer/Analyst/Architect (8+ years for Manager/Lead positions).\nExpert-level proficiency in PySpark/Python and Azure Databricks (must have worked on real production projects).\nStrong experience in building and optimizing streaming data pipelines (Kafka, Event Hubs, Delta Lake, etc.).\n4+ years of hands-on experience in data governance & data modeling (ER, star schema, data vault, etc.).\nIn-depth knowledge of Azure Data Factory, Synapse, ADLS, and SQL/NoSQL databases.\nExperience with Delta Lake, Databricks Workflows, and performance tuning.\nFamiliarity with data security, metadata management, and lineage tracking.\nExcellent communication skills (must be able to articulate technical concepts clearly).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure Databricks', 'Data Modeling', 'Data Governance', 'Python', 'ETL']",2025-06-12 06:06:21
Data Architect- Snowflake & DBT,InfoCepts,3 - 5 years,Not Disclosed,['Chennai'],"InfoCepts is looking for Data Architect- Snowflake & DBT to join our dynamic team and embark on a rewarding career journey\nDesign and Development: Create and implement data warehouse solutions using Snowflake, including data modeling, schema design, and ETL (Extract, Transform, Load) processes\nPerformance Optimization: Optimize queries, performance-tune databases, and ensure efficient use of Snowflake resources for faster data retrieval and processing\nData Integration: Integrate data from various sources, ensuring compatibility, consistency, and accuracy\nSecurity and Compliance: Implement security measures and ensure compliance with data governance and regulatory requirements, including access control and data encryption\nMonitoring and Maintenance: Monitor system performance, troubleshoot issues, and perform routine maintenance tasks to ensure system health and reliability\nCollaboration: Collaborate with other teams, such as data engineers, analysts, and business stakeholders, to understand requirements and deliver effective data solutions\nSkills and Qualifications:Snowflake Expertise: In-depth knowledge and hands-on experience working with Snowflake's architecture, features, and functionalities\nSQL and Database Skills: Proficiency in SQL querying and database management, with a strong understanding of relational databases and data warehousing concepts\nData Modeling: Experience in designing and implementing effective data models for optimal performance and scalability\nETL Tools and Processes: Familiarity with ETL tools and processes to extract, transform, and load data into Snowflake\nPerformance Tuning: Ability to identify and resolve performance bottlenecks, optimize queries, and improve overall system performance\nData Security and Compliance: Understanding of data security best practices, encryption methods, and compliance standards (such as GDPR, HIPAA, etc)\nProblem-Solving and Troubleshooting: Strong analytical and problem-solving skills to diagnose and resolve issues within the Snowflake environment\nCommunication and Collaboration: Good communication skills to interact with cross-functional teams and effectively translate business requirements into technical solutions\nScripting and Automation: Knowledge of scripting languages (like Python) and experience in automating processes within Snowflake",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'python', 'hipaa', 'data security', 'data warehousing', 'data architecture', 'sql querying', 'relational databases', 'sql', 'gdpr', 'database design', 'database management', 'data modeling', 'etl tool', 'data governance', 'data warehousing concepts', 'etl', 'communication skills']",2025-06-12 06:06:24
Starburst Data Engineer/ Architect,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Starburst Data Engineer/Architect \nExpertise in Starburst and policy management like Ranger or equivalent.\nIn-depth knowledge of data modelling principles and techniques, including relational and dimensional.\nExcellent problem solving skills and the ability to troubleshoot and debug complex data related issues.\nStrong awareness of data tools and platforms like: Starburst, Snowflakes, Databricks and programming languages like SQL.\nIn-depth knowledge of data management principles, methodologies, and best practices with excellent analytical, problem-solving and decision making skills.\nDevelop, implement and maintain database systems using SQL.\nWrite complex SQL queries for integration with applications.\nDevelop and maintain data models (Conceptual, physical and logical) to meet organisational needs.\n\n\n ? \n\nDo\n\n1. Managing the technical scope of the project in line with the requirements at all stages\n\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\n\nb. Develop record management process and policies\n\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\n\nd. Providing sales data, proposals, data insights and account reviews to the client base\n\ne. Identify areas to increase efficiency and automation of processes\n\nf. Set up and maintain automated data processes\n\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\n\nh. Produce and track key performance indicators\n\n\n\n2. Analyze the data sets and provide adequate information\n\na. Liaise with internal and external clients to fully understand data content\n\nb. Design and carry out surveys and analyze survey data as per the customer requirement\n\nc. Analyze and interpret complex data sets relating to customer??s business and prepare reports for internal and external audiences using business analytics reporting tools\n\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\n\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\n\nf. Develop predictive models and share insights with the clients as per their requirement\n\n ? \n\nDeliver\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nStartburst.\n\nExperience5-8 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'sql', 'data bricks', 'data modeling', 'policy management', 'hive', 'snowflake', 'python', 'data mining', 'data warehousing', 'power bi', 'dbms', 'data architecture', 'sql server', 'plsql', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'etl', 'ssis', 'data integration', 'informatica']",2025-06-12 06:06:27
GCP Data Architect (Standard),Infogain,12 - 14 years,Not Disclosed,['Bengaluru'],"Position Summary Experienced Senior Data Engineer utilizing Big Data & Gogle Cloud technologies to develop large scale, on-cloud data processing pipelines and data warehouses. What you ll do Consult customers across the world on their data engineering needs around Adobes Customer Data Platform. Support pre-sales discsusions around complex and large scale cloud, data engineering solutions. Design custom solutions on cloud integrating Adobes solutions in scalable and performant manner. Deliver complex, large scale, enterprise grade on-clould data engineer and integration solutions in hand-on manner. Good to have Experience of consulting India customers. Multi-cloud expertise preferable AWS and GCP\nEXPERIENCE\n12-14 Years\nSKILLS\nPrimary Skill: Data Engineering\nSub Skill(s): Data Engineering\nAdditional Skill(s): Python, BigQuery",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SUB', 'GCP', 'Data Architect', 'Consulting', 'Cloud', 'Presales', 'Data processing', 'Adobe', 'big data', 'Python']",2025-06-12 06:06:30
Data Architect - AWS,Happiest Minds Technologies,10 - 15 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Roles and responsibilities\nWork closely with the Product Owners and stake holders to design the Technical Architecture for data platform to meet the requirements of the proposed solution.\nWork with the leadership to set the standards for software engineering practices within the machine learning engineering team and support across other disciplines\nPlay an active role in leading team meetings and workshops with clients.\nChoose and use the right analytical libraries, programming languages, and frameworks for each task.",,,,"['SQL', 'data architect', 'Python', 'Pyspark', 'Apache Airflow', 'GLUE', 'Kinesis', 'Amazon Redshift', 'Data Architecture Principles', 'Data Modeling', 'Data Warehousing', 'Athena', 'Lambda', 'AWS']",2025-06-12 06:06:32
Data Architect (Data Bricks),Diacto Technologies Pvt Ltd,5 - 9 years,Not Disclosed,['Pune( Baner )'],"Job Overview:\nDiacto is seeking an experienced and highly skilled Data Architect to lead the design and development of scalable and efficient data solutions. The ideal candidate will have strong expertise in Azure Databricks, Snowflake (with DBT, GitHub, Airflow), and Google BigQuery. This is a full-time, on-site role based out of our Baner, Pune office.\n\nQualifications:\nB.E./B.Tech in Computer Science, IT, or related discipline\nMCS/MCA or equivalent preferred\n\nKey Responsibilities:\nDesign, build, and optimize robust data architecture frameworks for large-scale enterprise solutions\nArchitect and manage cloud-based data platforms using Azure Databricks, Snowflake, and BigQuery\nDefine and implement best practices for data modeling, integration, governance, and security\nCollaborate with engineering and analytics teams to ensure data solutions meet business needs\nLead development using tools such as DBT, Airflow, and GitHub for orchestration and version control\nTroubleshoot data issues and ensure system performance, reliability, and scalability\nGuide and mentor junior data engineers and developers\n\nExperience and Skills Required:\n5 to12 years of experience in data architecture, engineering, or analytics roles\nHands-on expertise in Databricks, especially Azure Databricks\nProficient in Snowflake, with working knowledge of DBT, Airflow, and GitHub\nExperience with Google BigQuery and cloud-native data processing workflows\nStrong knowledge of modern data architecture, data lakes, warehousing, and ETL pipelines\nExcellent problem-solving, communication, and analytical skills\n\nNice to Have:\nCertifications in Azure, Snowflake, or GCP\nExperience with containerization (Docker/Kubernetes)\nExposure to real-time data streaming and event-driven architecture\n\nWhy Join Diacto Technologies?\nCollaborate with experienced data professionals and work on high-impact projects\nExposure to a variety of industries and enterprise data ecosystems\nCompetitive compensation, learning opportunities, and an innovation-driven culture\nWork from our collaborative office space in Baner, Pune\nHow to Apply:\nOption 1 (Preferred)\n\nCopy and paste the following link on your browser and submit your application for the automated interview process: -\n\nhttps://app.candidhr.ai/app/candidate/gAAAAABoRrTQoMsfqaoNwTxsE_qwWYcpcRyYJk7NzSUmO3LKb6rM-8FcU58CUPYQKc65n66feHor-TGdCEfyouj0NmKdgYcNbA==/\n\nOption 2\n\n1. Please visit our website's career section at https://www.diacto.com/careers/\n2. Scroll down to the ""Who are we looking for?"" section\n3. Find the listing for "" Data Architect (Data Bricks)"" and\n4. Proceed with the virtual interview by clicking on ""Apply Now.""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Azure Databricks', 'Airflow', 'Etl Pipelines', 'Github', 'google BigQuery', 'DBT', 'Data Security', 'Data Modeling', 'Elt', 'Data Governance']",2025-06-12 06:06:35
Data Architect,Quincy Compressor,6 - 12 years,Not Disclosed,['Pune'],"As a Data Architect, you'll design and optimize data architecture to ensure data is accurate, secure, and accessible. you'll collaborate across teams to shape the data strategy, implement governance, and promote best practices enabling the business to gain insights, innovate, and make data-driven decisions at scale.\n  Your responsibilites\nResponsible for defining the enterprise data architecture which streamlines, standardises, and enhances accessibility of organisational data.\nElicits data requirements from senior Business stakeholders and the broader IS function, translating their needs into conceptual, logical, and physical data models.\nOversees the effective integration of data from various sources, ensuring data quality and consistency.\nMonitors and optimises data performance, collaborating with Data Integration and Product teams to deliver changes that improve data performance.\nSupports the Business, Data Integration Platforms team and wider IS management to define a data governance framework that sets out how data will be governed, accessed, and secured across the organisation; supports the operation of the data governance model as a subject matter advisor.\nProvides advisory to Data Platform teams in defining the Data Platform architecture, providing advisory on metadata, data integration, business intelligence, and data storage needs.\nSupports the Data Integration Platforms team, and other senior IS stakeholders to define a data vision and strategy, setting out how the organisation will exploit its data for maximum Business value.\nBuilds and maintains a repository of data architecture artefacts (eg, data dictionary).\nWhat we're Looking For\nProven track record in defining enterprise data architectures, data models, and database/data warehouse solutions.\nEvidenced ability to advise on the use of key data platform architectural components (eg, Azure Lakehouse, Data Bricks, etc) to deliver and optimise the enterprise data architecture.\nExperience in data integration technologies, real-time data ingestion, and API-based integrations.\nExperience in SQL and other database management systems.\nStrong problem-solving skills for interpreting complex data requirements and translating them into feasible data architecture solutions and models.\nExperience in supporting the definition of an enterprise data vision and strategy, advising on implications and/or uplifts required to the enterprise data architecture.\nExperience designing and establishing data governance models and data management practices, ensuring data is correct and secure whilst still being accessible, in line with regulations and wider organisational policies.\nAble to present complex data-related initiatives and issues to senior non-data conversant audiences.\nProven experience working with AI and Machine Learning models preferred, but not essential.\nWhat We Can Offer You\nWe support your growth within the role, department, and across the company through internal opportunities.\nWe offer a hybrid working model, allowing you to combine remote work with the opportunity to connect with your team in modern, welcoming office spaces.\nWe encourage continuous learning with access to online platforms (eg, LinkedIn Learning), language courses, soft skills training, and various we'llbeing initiatives, including workshops and webinars.\nJoin a diverse and inclusive work environment where your ideas are valued and your contributions make a difference.",Industry Type: Industrial Equipment / Machinery,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['metadata', 'Architecture', 'Data management', 'Data Architect', 'data governance', 'Data quality', 'Business intelligence', 'Information technology', 'SQL', 'Data architecture']",2025-06-12 06:06:37
Data Architect - Supply Chain,Exxon Mobil Corporation,3 - 8 years,Not Disclosed,['Bengaluru'],"About us\nWe invite you to bring your ideas to ExxonMobil to help create sustainable solutions that improve quality of life and meet society s evolving needs. Learn more about our What and our Why and how we can work together .\nExxonMobil s affiliates in India\nExxonMobil s affiliates have offices in India in Bengaluru, Mumbai and the National Capital Region.\nExxonMobil s affiliates in India supporting the Product Solutions business engage in the marketing, sales and distribution of performance as well as specialty products across chemicals and lubricants businesses. The India planning teams are also embedded with global business units for business planning and analytics.",,,,"['ERP', 'SAP', 'Networking', 'Data management', 'Data modeling', 'XML', 'Agile', 'JSON', 'SQL', 'Python']",2025-06-12 06:06:39
Data Architect Data Architect,Mufg Pension & Market Services,7 - 10 years,Not Disclosed,['Mumbai'],"Overview\nThe Data Architect will play a key role in defining and overseeing the enterprise data model, ensuring alignment with MUFG s strategic objectives. The Data Architect will be responsible for setting standards and frameworks for data classification and ensuring data flows across applications and data provenance is fully understood. These frameworks will empower the organisation to harness its data effectively for reporting, machine learning (ML), artificial intelligence (AI), auditing, and other business-critical initiatives. Working within the Enterprise Architecture practice, you will collaborate across teams globally.\n\nKey Accountabilities and main responsibilities\nStrategic Focus\nDefine and maintain the enterprise data model at an abstract level, ensuring alignment with MUFG s strategic goals and business outcomes.\nCollaborate with the Enterprise Architecture practice to shape data frameworks that align with the organisations overarching IT strategy.\nDevelop standards for data classification and data provenance to support global initiatives such as reporting, ML, AI, and auditing.\nProvide thought leadership on emerging trends, technologies, and best practices for enterprise data architecture.\nInfluence and build consensus among stakeholders across EUROPE/UK & ANZ regions to drive the adoption of strategic data practices.\nOperational Management\nEstablish a common data model, data dictionary and data business rules that represents the CM business domain\nMap and document data flows across applications to ensure consistency, efficiency, and security in data management.\nOversee adherence to data standards, facilitating governance and ensuring compliance with regional regulations (e.g., GDPR).\nMonitor and ensure traceability of data sources and transformations through robust data provenance frameworks.\nPrepare and maintain comprehensive documentation for the enterprise data model, data flows, classifications, and governance standards.\nCollaborate with teams globally to address specific operational challenges in data architecture and maintain alignment across regions.\nGovernance & Risk\nPlay an active role in technology governance as part of the Architecture Governance Board (AGB).\nThe above list of key accountabilities is not an exhaustive list and may change from time-to-time based on business needs.\n\nExperience & Personal Attributes\nProven experience (7-10 years) as a Data Architect or in a similar strategic role, ideally within financial services or a global organisation.\nDeep understanding of abstract data modelling, data flows across complex application landscapes, and enterprise data standards.\nExpertise in data governance, security, and compliance frameworks, with a strong knowledge of GDPR and regional standards.\nFamiliarity with tools and methodologies that enable reporting, ML, AI, and auditing across an enterprise.\nExceptional communication skills to engage with technical and non-technical stakeholders across EMEA and APAC regions.\nA strategic, collaborative mindset with the ability to influence and build consensus across diverse teams.\nBachelor s degree in Computer Science, Information Systems, or a related field (preferred).\nEnthusiastic attitude, discipline and approach.\nCan demonstrate independent working. Ability to work under pressure with a view to attaining monthly targets.\n\nOverview\nThe Data Architect will play a key role in defining and overseeing the enterprise data model, ensuring alignment with MUFG s strategic objectives. The Data Architect will be responsible for setting standards and frameworks for data classification and ensuring data flows across applications and data provenance is fully understood. These frameworks will empower the organisation to harness its data effectively for reporting, machine learning (ML), artificial intelligence (AI), auditing, and other business-critical initiatives. Working within the Enterprise Architecture practice, you will collaborate across teams globally.\n\nKey Accountabilities and main responsibilities\nStrategic Focus\nDefine and maintain the enterprise data model at an abstract level, ensuring alignment with MUFG s strategic goals and business outcomes.\nCollaborate with the Enterprise Architecture practice to shape data frameworks that align with the organisations overarching IT strategy.\nDevelop standards for data classification and data provenance to support global initiatives such as reporting, ML, AI, and auditing.\nProvide thought leadership on emerging trends, technologies, and best practices for enterprise data architecture.\nInfluence and build consensus among stakeholders across EUROPE/UK & ANZ regions to drive the adoption of strategic data practices.\nOperational Management\nEstablish a common data model, data dictionary and data business rules that represents the CM business domain\nMap and document data flows across applications to ensure consistency, efficiency, and security in data management.\nOversee adherence to data standards, facilitating governance and ensuring compliance with regional regulations (e.g., GDPR).\nMonitor and ensure traceability of data sources and transformations through robust data provenance frameworks.\nPrepare and maintain comprehensive documentation for the enterprise data model, data flows, classifications, and governance standards.\nCollaborate with teams globally to address specific operational challenges in data architecture and maintain alignment across regions.\nGovernance & Risk\nPlay an active role in technology governance as part of the Architecture Governance Board (AGB).\nThe above list of key accountabilities is not an exhaustive list and may change from time-to-time based on business needs.\n\nExperience & Personal Attributes\nProven experience (7-10 years) as a Data Architect or in a similar strategic role, ideally within financial services or a global organisation.\nDeep understanding of abstract data modelling, data flows across complex application landscapes, and enterprise data standards.\nExpertise in data governance, security, and compliance frameworks, with a strong knowledge of GDPR and regional standards.\nFamiliarity with tools and methodologies that enable reporting, ML, AI, and auditing across an enterprise.\nExceptional communication skills to engage with technical and non-technical stakeholders across EMEA and APAC regions.\nA strategic, collaborative mindset with the ability to influence and build consensus across diverse teams.\nBachelor s degree in Computer Science, Information Systems, or a related field (preferred).\nEnthusiastic attitude, discipline and approach.\nCan demonstrate independent working. Ability to work under pressure with a view to attaining monthly targets.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data dictionary', 'Data management', 'Enterprise architecture', 'IT strategy', 'Artificial Intelligence', 'Machine learning', 'business rules', 'Financial services', 'Data architecture']",2025-06-12 06:06:41
Immediate Joiner- Data Engineer,Healthedge,1 - 4 years,Not Disclosed,['Bengaluru'],"Data Engineer\nYou will be working with agile cross functional software development teams developing cutting age software to solve a significant problem in the Provider Data Management space. This hire will have experience building large scale complex data systems involving multiple cross functional data sets and teams. The ideal candidate will be excited about working on new product development, is comfortable pushing the envelope and challenging the status quo, sets high standards for him/herself and the team, and works well with ambiguity.\nWhat you will do:\nBuild data pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements.\nWork closely with data architect, SMEs and other technology partners to develop & execute data architecture and product roadmap.\nBuild analytical tools to utilize the data pipeline, providing actionable insight into key business performance including operational efficiency and business metrics.\nWork with stakeholders including the leadership, product, customer teams to support their data infrastructure needs while assisting with data-related technical issues.\nAct as a subject matter expert to other team members for technical guidance, solution design and best practices within the customer organization.\nKeep current on big data and data visualization technology trends, evaluate, work on proof-of-concept and make recommendations on cloud technologies.\nWhat you bring:\n2+ years of data engineering experience working in partnership with large data sets (preferably terabyte scale)\nExperience in building data pipelines using any of the ETL tools such as Glue, ADF, Notebooks, Stored Procedures, SQL/Python constructs or similar.\nDeep experience working with industry standard RDBMS such Postgres, SQL Server, Oracle, MySQL etc. and any of the analytical cloud databases such as Big Query, Redshift, Snowflake or similar\nAdvanced SQL expertise and solid programming experience with Python and/or Spark\nExperience working with orchestration tools such as Airflow and building complex dependency workflows.\nExperience, developing and implementing Data Warehouse or Data Lake Architectures, OLAP technologies, data modeling with star/snowflake-schemas to enable analytics & reporting.\nGreat problem-solving capabilities, troubleshooting data issues and experience in stabilizing big data systems.\nExcellent communication and presentation skills as youll be regularly interacting with stakeholders and engineering leadership.\nBachelors or master's in quantitative disciplines such as Computer Science, Computer Engineering, Analytics, Mathematics, Statistics, Information Systems, or other scientific fields.\nBonus points:\nHands-on deep experience with cloud data migration, and experience working with analytic platforms like Fabric, Databricks on the cloud.\nCertification in one of the cloud platforms (AWS/GCP/Azure)\nExperience or demonstrated understanding with real-time data streaming tools like Kafka, Kinesis or any similar tools.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Pyspark', 'Cloud', 'Python']",2025-06-12 06:06:43
Data Modelling,Tech Mahindra,7 - 12 years,Not Disclosed,['Chennai'],"Role: Data Modelling\nFulltime (Work from office Monday to Friday)\nLocation: Chennai\n\n\nMinimum Qualifications:\nBachelors Degree or experience in Engineering,\n5+ years of experience in data architecture or a related field, with a strong understanding of cloud-based data solutions\nProficiency in designing and implementing Medallion Architecture with bronze, silver, and gold layers\nExperienced curating and Erwin modeling data into Star Schemas\nStrong background in semantic modeling and creating meaningful, user-friendly data sets\nExperience with AWS, Synapse, and Power BI.\nWork visa sponsorship is not available for this position",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Warehouse', 'Data Modeling', 'ERwin', 'Azure Cloud', 'GCP', 'AWS']",2025-06-12 06:06:46
Senior Data Manager/ Lead,Codeforce 360,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description:\nWe are looking for a highly experienced and dynamic Senior Data Manager / Lead to oversee a team of Data Engineers and Data Scientists. This role demands a strong background in data platforms such as Snowflake and proficiency in Python, combined with excellent people management and project leadership skills. While hands-on experience in the technologies is beneficial, the primary focus of this role is on team leadership, strategic planning, and project delivery .\n\nJob Title : Senior Data Manager / Lead\nLocation: Hyderabad (Work From Office)\nShift Timing: 10AM-7PM\nKey Responsibilities:\nLead, mentor, and manage a team of Data Engineers and Data Scientists.\nOversee the design and implementation of data pipelines and analytics solutions using Snowflake and Python.\nCollaborate with cross-functional teams (product, business, engineering) to align data solutions with business goals.\nEnsure timely delivery of projects, with high quality and performance.\nConduct performance reviews, training plans, and support career development for the team.\nSet priorities, allocate resources, and manage workloads within the data team.\nDrive adoption of best practices in data management, governance, and documentation.\nEvaluate new tools and technologies relevant to data engineering and data science.\n\nRequired Skills & Qualifications:\n6+ years of experience in data-related roles, with at least 23 years in a leadership or management position.\nStrong understanding of Snowflake architecture, performance tuning, data sharing, security, etc.\nSolid knowledge of Python for data engineering or data science tasks.\nExperience in leading data migration, ETL/ELT, and analytics projects.\nAbility to translate business requirements into technical solutions.\nExcellent leadership, communication, and stakeholder management skills.\nExposure to tools like Databricks, Dataiku, Airflow, or similar platforms is a plus.\nBachelors or Master’s degree in Computer Science, Engineering, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Bricks', 'Python', 'Airflow', 'Data Migration', 'Dataiku', 'Data Warehousing', 'ETL', 'ELT', 'SQL']",2025-06-12 06:06:48
Data Engineer,Aqilea Softech,5 - 9 years,13-20 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Job Title: Data Engineer\nCompany : Aqilea India(Client : H&M India)\nEmployment Type: Full Time\nLocation: Bangalore(Hybrid)\nExperience: 4.5 to 9 years\nClient : H&M India\n\nAt H&M, we welcome you to be yourself and feel like you truly belong. Help us reimagine the future of an entire industry by making everyone look, feel, and do good. We take pride in our history of making fashion accessible to everyone and led by our values we strive to build a more welcoming, inclusive, and sustainable industry. We are privileged to have more than 120,000 colleagues, in over 75 countries across the world. Thats 120 000 individuals with unique experiences, skills, and passions. At H&M, we believe everyone can make an impact, we believe in giving people responsibility and a strong sense of ownership. Our business is your business, and when you grow, we grow.\nWebsite : https://career.hm.com/\n\nWe are seeking a skilled and forward-thinking Data Engineer to join our Emerging Tech team. This role is designed for someone passionate about working with cutting-edge technologies such as AI, machine learning, IoT, and big data to turn complex data sets into actionable insights.\nAs the Data Engineer in Emerging Tech, you will be responsible for designing, implementing, and optimizing data architectures and processes that support the integration of next-generation technologies. Your role will involve working with large-scale datasets, building predictive models, and utilizing emerging tools to enable data-driven decision-making across the business. You ll collaborate with technical and business teams to uncover insights, streamline data pipelines, and ensure the best use of advanced analytics technologies.\n\nKey Responsibilities:\nDesign and build scalable data architectures and pipelines that support machine learning, analytics, and IoT initiatives.\nDevelop and optimize data models and algorithms to process and analyse large-scale, complex data sets.\nImplement data governance, security, and compliance measures to ensure high-quality\nCollaborate with cross-functional teams (engineering, product, and business) to translate business requirements into data-driven solutions.\nEvaluate, integrate, and optimize new data technologies to enhance analytics capabilities and drive business outcomes.\nApply statistical methods, machine learning models, and data visualization techniques to deliver actionable insights.\nEstablish best practices for data management, including data quality, consistency, and scalability.\nConduct analysis to identify trends, patterns, and correlations within data to support strategic business initiatives.\nStay updated on the latest trends and innovations in data technologies and emerging data management practices.\n\nSkills Required :\nBachelors or masters degree in data science, Computer Science, Engineering, Statistics, or a related field.\n4.5-9 years of experience in data engineering, data science, or a similar analytical role, with a focus on emerging technologies.\nProficiency with big data frameworks (e.g., Hadoop, Spark, Kafka) and experience with modern cloud platforms (AWS, Azure, or GCP).\nSolid skills in Python, SQL, and optionally R, along with experience using machine learning libraries such as Scikit-learn, TensorFlow, or PyTorch.\nExperience with data visualization tools (e.g., Tableau or Power BI or D3.js) to communicate insights effectively.\nFamiliarity with IoT and edge computing data architectures is a plus.\nUnderstanding of data governance, compliance, and privacy standards.\nAbility to work with both structured and unstructured data.\nExcellent problem-solving, communication, and collaboration skills, with the ability to work in a fast-paced, cross-functional team environment.\nA passion for emerging technologies and a continuous desire to learn and innovate.\nInterested Candidates can share your Resumes to mail id karthik.prakadish@aqilea.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Powerbi', 'Hadoop', 'Kafka', 'Tableau', 'Azure', 'GCP', 'Data Engineer', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-12 06:06:50
"Walk-In Senior Data Engineer - DataStage, Azure & Power BI",Net Connect,6 - 10 years,5-11 Lacs P.A.,['Hyderabad( Madhapur )'],"Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\nBelow JD for your reference\n\nJob Description:\n\nWe are hiring an experienced Senior Data Engineer with strong expertise in IBM DataStage, , and . The ideal candidate will be responsible for end-to-end data integration, transformation, and reporting solutions that drive business decisions.",,,,"['Azure Data Factory', 'Datastage', 'Etl Datastage']",2025-06-12 06:06:52
Data Architecture,Top B2B MNC in Management Consulting Dom...,5 - 8 years,Not Disclosed,['Bengaluru'],"About the Company\nGreetings from Teamware Solutions a division of Quantum Leap Consulting Pvt. Ltd\n\nAbout the Role\nWe are hiring a Data Architecture\n\nLocation: Bangalore\nWork Model: Hybrid\nExperience: 5-9 Years\nNotice Period: Immediate to 15 Days\n\nJob Description:\nData Architecture, Data Governance, Data Modeling\n\nAdditional Information:\nMandatory Skills: Data Architecture, Data Governance, Data Modeling\nNice to have skills Certification in Data Engineering\nInterview Mode Virtual Interview\nminimum 5 yrs relevant experience and maximum 9 yrs for this requirement. Someone with more experience in building PySpark data streaming jobs on Azure Databricks\nwho have done real projects, have expertise, and hands-on experience also\nAlso, Data governance and data modeling experience with a minimum of 4 years is mandatory\nCommunication should be excellent\n\n\nPlease let me know if you are interested in this position and send me your resumes to netra.s@twsol.com",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Architecture', 'Data Modeling', 'Data Governance', 'Data Engineering']",2025-06-12 06:06:55
"Sr. Data Engineer, R&D Data Catalyst Team",Amgen Inc,7 - 9 years,Not Disclosed,['Hyderabad'],"The R&D Data Catalyst Team is responsible for buildingData Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nThe Data Engineer will be responsible for the end-to-end development of an enterprise analytics and data mastering solution leveraging Databricks and Power BI. This role requiresexpertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that research cohort-building and advanced research pipeline.The ideal candidate will have experience creating and surfacing large unifiedrepositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\nYou will collaborate closely with stakeholders, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a strong background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nRoles & Responsibilities:\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with stakeholders to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\nBasic Qualifications and Experience:\nMasters degree with 1 to 3years of experience in Data Engineering OR\nBachelors degree with 4 to 5 years of experience in Data Engineering\nDiploma and 7 to 9 years of experience in Data Engineering.\nFunctional Skills:\nMust-Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nStrong communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood-to-Have Skills:\nExperience in developing differentiated and deliverable solutions\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'ETL processes', 'DAX', 'Business Objects', 'data warehouse design', 'ETL', 'PowerBI Models', 'AWS', 'Power Query']",2025-06-12 06:06:57
Sr Data Engineering Manager,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a Senior Data Engineering Manager with a strong background in Regulatory or Integrated Product Teams within the Biotech or Pharmaceutical domain. This role will lead the end-to-end data strategy and execution for regulatory product submissions, lifecycle management, and compliance reporting, ensuring timely and accurate delivery of regulatory data assets across global markets.You will be embedded in a cross-functional Regulatory Integrated Product Team (IPT) and serve as the data and technology lead, driving integration between scientific, regulatory, and engineering functions to support submission-ready data and regulatory intelligence solutions.\nRoles & Responsibilities:\nFunctional Skills:\nLead the engineering strategy and implementation for end-to-end regulatory operations, including data ingestion, transformation, integration, and delivery across regulatory systems.\nServe as the data engineering SME in the Integrated Product Team (IPT) to support regulatory submissions, agency interactions, and lifecycle updates.\nCollaborate with global regulatory affairs, clinical, CMC, quality, safety, and IT teams to gather submission data requirements and translate them into data engineering solutions.\nManage and oversee the development of data pipelines, data models, and metadata frameworks that support submission data standards (e.g., eCTD, IDMP, SPL, xEVMPD).\nEnable integration and reporting across regulatory information management systems (RIMS), EDMS, clinical trial systems, and lab data platforms.\nImplement data governance, lineage, validation, and audit trails for regulatory data workflows, ensuring GxP and regulatory compliance.\nGuide the development of automation solutions, dashboards, and analytics that improve visibility into submission timelines, data quality, and regulatory KPIs.\nEnsure interoperability between regulatory data platforms and enterprise data lakes or lakehouses for cross-functional reporting and insights.\nCollaborate with IT, data governance, and enterprise architecture teams to ensure alignment with overall data strategy and compliance frameworks.\nDrive innovation by evaluating emerging technologies in data engineering, graph data, knowledge management, and AI for regulatory intelligence.\nLead, mentor, and coach a small team of data engineers and analysts, fostering a culture of excellence, innovation, and delivery.\nDrive Agile and Scaled Agile (SAFe) methodologies, managing sprint backlogs, prioritization, and iterative improvements to enhance team velocity and project delivery.\nStay up-to-date with emerging data technologies, industry trends, and best practices, ensuring the organization leverages the latest innovations in data engineering and architecture.\nMust-Have Skills:\n812 years of experience in data engineering or data architecture, with 3+ years in a senior or managerial capacity, preferably within the biotech or pharmaceutical industry.\nProven experience supporting regulatory functions, including submissions, tracking, and reporting for FDA, EMA, and other global authorities.\nExperience with ETL/ELT tools, data pipelines, and cloud-based data platforms (e.g., Databricks, AWS, Azure, or GCP).\nFamiliarity with regulatory standards and data models such as eCTD, IDMP, HL7, CDISC, and xEVMPD.\nDeep understanding of GxP data compliance, audit requirements, and regulatory submission processes.\nExperience with tools like Power BI, Tableau, or Qlik for regulatory dashboarding and visualization is a plus.\nStrong project management, stakeholder communication, and leadership skills, especially in matrixed, cross-functional environments.\nAbility to translate technical capabilities into regulatory and business outcomes.Prepare team members for stakeholder discussions by helping assess data costs, access requirements, dependencies, and availability for business scenarios.\nGood-to-Have Skills:\nPrior experience working on integrated product teams or regulatory transformation programs.\nKnowledge of Regulatory Information Management Systems (RIMS), Veeva Vault RIM, or Master Data Management (MDM) in regulated environments.\nFamiliarity with Agile/SAFe methodologies and DevOps/DataOps best practices.\nEducation and Professional Certifications\n12 to 15 years of experience in Computer Science, IT or related field\nScaled Agile SAFe certification preferred\nProject Management certifications preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'engineering strategy', 'DevOps', 'Project Management', 'DataOps', 'Agile', 'data strategy']",2025-06-12 06:06:59
Lead AWS Glue Data Engineer,Allegis Group,8 - 13 years,Not Disclosed,[],"Lead AWS Glue Data Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\n\nWe are seeking a skilled Lead AWS Data Engineer with 8+ years of strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena.\nImplement data migration and transformation processes using AWS DMS and Glue.\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\n\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDS', 'Glue', 'DMS', 'Lambda', 'Redshift', 'Athena']",2025-06-12 06:07:01
Data Engineer,Dun & Bradstreet,5 - 9 years,Not Disclosed,['Hyderabad'],"Key Responsibilities:\n1. Design, build, and deploy new data pipelines within our Big Data Eco-Systems using Streamsets/Talend/Informatica BDM etc. Document new/existing pipelines, Datasets.\n2. Design ETL/ELT data pipelines using StreamSets, Informatica or any other ETL processing engine. Familiarity with Data Pipelines, Data Lakes and modern Data Warehousing practices (virtual data warehouse, push down analytics etc.)\n3. Expert level programming skills on Python\n4. Expert level programming skills on Spark\n5. Cloud Based Infrastructure: GCP\n6. Experience with one of the ETL Informatica, StreamSets in creation of complex parallel loads, Cluster Batch Execution and dependency creation using Jobs/Topologies/Workflows etc.,\n7. Experience in SQL and conversion of SQL stored procedures into Informatica/StreamSets, Strong exposure working with web service origins/targets/processors/executors, XML/JSON Sources and Restful APIs.\n8. Strong exposure working with relation databases DB2, Oracle & SQL Server including complex SQL constructs and DDL generation.\n9. Exposure to Apache Airflow for scheduling jobs\n10. Strong knowledge of Big data Architecture (HDFS), Cluster installation, configuration, monitoring, cluster security, cluster resources management, maintenance, and performance tuning\n11. Create POCs to enable new workloads and technical capabilities on the Platform.\n12. Work with the platform and infrastructure engineers to implement these capabilities in production.\n13. Manage workloads and enable workload optimization including managing resource allocation and scheduling across multiple tenants to fulfill SLAs.\n14. Participate in planning activities, Data Science and perform activities to increase platform skills\n\nKey Requirements:\n1. Minimum 6 years of experience in ETL/ELT Technologies, preferably StreamSets/Informatica/Talend etc.,\n2. Minimum of 6 years hands-on experience with Big Data technologies e.g. Hadoop, Spark, Hive.\n3. Minimum 3+ years of experience on Spark\n4. Minimum 3 years of experience in Cloud environments, preferably GCP\n5. Minimum of 2 years working in a Big Data service delivery (or equivalent) roles focusing on the following disciplines:\n6. Any experience with NoSQL and Graph databases\n7. Informatica or StreamSets Data integration (ETL/ELT)\n8. Exposure to role and attribute based access controls\n9. Hands on experience with managing solutions deployed in the Cloud, preferably on GCP\n10. Experience working in a Global company, working in a DevOps model is a plus",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'GCP', 'Data engineer', 'Spark', 'ETL']",2025-06-12 06:07:03
"Data Engineering : Sr Software Engineer, Tech Lead & Sr Tech Lead",Reflion Tech,7 - 12 years,22.5-37.5 Lacs P.A.,"['Mumbai( Ghansoli )', 'Navi Mumbai', 'Mumbai (All Areas)']","Hiring: Data Engineering Senior Software Engineer / Tech Lead / Senior Tech Lead\n\n- Hybrid (3 Days from office) | Shift: 2 PM 11 PM IST\n- Experience: 5 to 12+ years (based on role & grade)\n\nOpen Grades/Roles:\nSenior Software Engineer: 58 Years\nTech Lead: 7–10 Years\nSenior Tech Lead: 10–12+ Years\n\nJob Description – Data Engineering Team\n\nCore Responsibilities (Common to All Levels):\n\nDesign, build and optimize ETL/ELT pipelines using tools like Pentaho, Talend, or similar\nWork on traditional databases (PostgreSQL, MSSQL, Oracle) and MPP/modern systems (Vertica, Redshift, BigQuery, MongoDB)\nCollaborate cross-functionally with BI, Finance, Sales, and Marketing teams to define data needs\nParticipate in data modeling (ER/DW/Star schema), data quality checks, and data integration\nImplement solutions involving messaging systems (Kafka), REST APIs, and scheduler tools (Airflow, Autosys, Control-M)\nEnsure code versioning and documentation standards are followed (Git/Bitbucket)\n\nAdditional Responsibilities by Grade\n\nSenior Software Engineer (5–8 Yrs):\nFocus on hands-on development of ETL pipelines, data models, and data inventory\nAssist in architecture discussions and POCs\nGood to have: Tableau/Cognos, Python/Perl scripting, GCP exposure\n\nTech Lead (7–10 Yrs):\nLead mid-sized data projects and small teams\nDecide on ETL strategy (Push Down/Push Up) and performance tuning\nStrong working knowledge of orchestration tools, resource management, and agile delivery\n\nSenior Tech Lead (10–12+ Yrs):\nDrive data architecture, infrastructure decisions, and internal framework enhancements\nOversee large-scale data ingestion, profiling, and reconciliation across systems\nMentoring junior leads and owning stakeholder delivery end-to-end\nAdvantageous: Experience with AdTech/Marketing data, Hadoop ecosystem (Hive, Spark, Sqoop)\n\n- Must-Have Skills (All Levels):\n\nETL Tools: Pentaho / Talend / SSIS / Informatica\nDatabases: PostgreSQL, Oracle, MSSQL, Vertica / Redshift / BigQuery\nOrchestration: Airflow / Autosys / Control-M / JAMS\nModeling: Dimensional Modeling, ER Diagrams\nScripting: Python or Perl (Preferred)\nAgile Environment, Git-based Version Control\nStrong Communication and Documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SQL', 'ETL', 'Orchestration', 'Postgresql', 'Peri', 'Informatica', 'ETL Tool', 'SSIS', 'Elt', 'Modeling', 'MongoDB', 'Data Architecture', 'Talend', 'Pentaho', 'Python']",2025-06-12 06:07:06
Senior Data Engineer,Jeavio,5 - 10 years,Not Disclosed,[],"We are seeking an experienced Senior Data Engineer to join our team. The ideal candidate will have a strong background in data engineering and AWS infrastructure, with hands-on experience in building and maintaining data pipelines and the necessary infrastructure components. The role will involve using a mix of data engineering tools and AWS services to design, build, and optimize data architecture.\n\nKey Responsibilities:\nDesign, develop, and maintain data pipelines using Airflow and AWS services.\nImplement and manage data warehousing solutions with Databricks and PostgreSQL.\nAutomate tasks using GIT / Jenkins.\nDevelop and optimize ETL processes, leveraging AWS services like S3, Lambda, AppFlow, and DMS.\nCreate and maintain visual dashboards and reports using Looker.\nCollaborate with cross-functional teams to ensure smooth integration of infrastructure components.\nEnsure the scalability, reliability, and performance of data platforms.\nWork with Jenkins for infrastructure automation.\n\nTechnical and functional areas of expertise:\nWorking as a senior individual contributor on a data intensive project\nStrong experience in building high performance, resilient & secure data processing pipelines preferably using Python based stack.\nExtensive experience in building data intensive applications with a deep understanding of querying and modeling with relational databases preferably on time-series data.\nIntermediate proficiency in AWS services (S3, Airflow)\nProficiency in Python and PySpark\nProficiency with ThoughtSpot or Databricks.\nIntermediate proficiency in database scripting (SQL)\nBasic experience with Jenkins for task automation\n\nNice to Have :\nIntermediate proficiency in data analytics tools (Power BI / Tableau / Looker / ThoughSpot)\nExperience working with AWS Lambda, Glue, AppFlow, and other AWS transfer services.\nExposure to PySpark and data automation tools like Jenkins or CircleCI.\nFamiliarity with Terraform for infrastructure-as-code.\nExperience in data quality testing to ensure the accuracy and reliability of data pipelines.\nProven experience working directly with U.S. client stakeholders.\nAbility to work independently and take the lead on tasks.\n\nEducation and experience:\nBachelors or masters in computer science or related fields.\n5+ years of experience\n\nStack/Skills needed:\nDatabricks\nPostgreSQL\nPython & Pyspark\nAWS Stack\nPower BI / Tableau / Looker / ThoughSpot\nFamiliarity with GIT and/or CI/CD tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'AWS', 'Data Bricks', 'Python', 'Etl Pipelines', 'Airflow', 'Database Scripting', 'Postgresql', 'Looker', 'SQL']",2025-06-12 06:07:08
Senior Data Engineer,The Main Stage Productions,4 - 6 years,Not Disclosed,['Bengaluru'],"Design and implement cloud-native data architectures on AWS, including data lakes, data warehouses, and streaming pipelines using services like S3, Glue, Redshift, Athena, EMR, Lake Formation, and Kinesis.\nDevelop and orchestrate ETL/ELT pipelines\n\nRequired Candidate profile\nParticipate in pre-sales and consulting activities such as:\nEngaging with clients to gather requirements and propose AWS-based data engineering solutions.\nSupporting RFPs/RFIs, technical proposals",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS Glue', 'GitHub Actions', 'PySpark', 'Scala', 'CodePipeline', 'Step Functions', 'data engineering']",2025-06-12 06:07:10
Senior Data Engineer : 7+ Years,Jayam Solutions Pvt Ltd - CMMI Level III Company,5 - 9 years,Not Disclosed,['Hyderabad( Madhapur )'],"Job Description:\nPosition: Sr.Data Engineer\nExperience: Minimum 7 years\nLocation: Hyderabad\nJob Summary:\n\nWhat Youll Do\n\nDesign and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.\nParticipate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.\nIndependently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.\nMonitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.\nMaintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.\nAcquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.\nEngage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.\nDrive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.\n\nWhat Youll Bring\n\nBachelors degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.\n7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.\nExtensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.\nCloud - AWS\nApache Flink/Spark , Redis\nDatabase platform- Databricks.\nProficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.\nExpertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.\nExperience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).\nAbility to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.\nStrong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.\nExceptional attention to detail and professionalism in report and dashboard creation.\nExcellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.\nOutstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.\n\nPlease feel free to contact us: 9440806850\nEmail ID : careers@jayamsolutions.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Flink', 'Redis', 'Spark', 'Python', 'SCALA', 'Ci/Cd', 'Devops', 'AWS']",2025-06-12 06:07:12
Data Migration Consultant,Excellerate Global Solutions,6 - 10 years,8-18 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Data migration SAP ABAP , S4 HANA , FSCM ,FICO",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Temporary/Contractual","['Sap Data Migration', 'SAP FICO', 'SAP ABAP', 'FSCM', 'Sap Hana', 'S4 Hana Finance']",2025-06-12 06:07:14
Senior Data Engineer,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nOR\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Git', 'PySpark', 'CI/CD', 'Databricks', 'ETL', 'NOSQL', 'AWS', 'data integration', 'SQL', 'Apache Spark', 'Python']",2025-06-12 06:07:16
Senior Data Engineer,Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data engineering', 'performance tuning', 'data security', 'data processing', 'Hadoop', 'Apache Spark', 'SQL', 'CI/CD', 'troubleshooting', 'big data', 'aws', 'ETL', 'Python']",2025-06-12 06:07:18
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nRole Description:\nWe are seeking a Senior Data Engineer with expertise in Graph Data technologies to join our data engineering team and contribute to the development of scalable, high-performance data pipelines and advanced data models that power next-generation applications and analytics. This role combines core data engineering skills with specialized knowledge in graph data structures, graph databases, and relationship-centric data modeling, enabling the organization to leverage connected data for deep insights, pattern detection, and advanced analytics use cases. The ideal candidate will have a strong background in data architecture, big data processing, and Graph technologies and will work closely with data scientists, analysts, architects, and business stakeholders to design and deliver graph-based data engineering solutions.\nRoles & Responsibilities:\nDesign, build, and maintain robust data pipelines using Databricks (Spark, Delta Lake, PySpark) for complex graph data processing workflows.\nOwn the implementation of graph-based data models, capturing complex relationships and hierarchies across domains.\nBuild and optimize Graph Databases such as Stardog, Neo4j, Marklogic or similar to support query performance, scalability, and reliability.\nImplement graph query logic using SPARQL, Cypher, Gremlin, or GSQL, depending on platform requirements.\nCollaborate with data architects to integrate graph data with existing data lakes, warehouses, and lakehouse architectures.\nWork closely with data scientists and analysts to enable graph analytics, link analysis, recommendation systems, and fraud detection use cases.\nDevelop metadata-driven pipelines and lineage tracking for graph and relational data processing.\nEnsure data quality, governance, and security standards are met across all graph data initiatives.\nMentor junior engineers and contribute to data engineering best practices, especially around graph-centric patterns and technologies.\nStay up to date with the latest developments in graph technology, graph ML, and network analytics.\nWhat we expect of you\nMust-Have Skills:\nHands-on experience in Databricks, including PySpark, Delta Lake, and notebook-based development.\nHands-on experience with graph database platforms such as Stardog, Neo4j, Marklogic etc.\nStrong understanding of graph theory, graph modeling, and traversal algorithms\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies with strong problem-solving and analytical skills\nExcellent collaboration and communication skills, with experience working with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SPARQL', 'Maven', 'PySpark', 'GSQL', 'Subversion', 'AWS services', 'Stardog', 'Cypher', 'SAFe', 'Jenkins', 'DevOps', 'Git', 'Neo4j', 'Delta Lake', 'Graph Databases', 'Spark', 'Marklogic', 'Gremlin']",2025-06-12 06:07:21
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Maven', 'SparkSQL Apache Spark', 'PySpark', 'Subversion', 'OLAP', 'Scaled Agile methodologies', 'SQL', 'Scaled Agile Framework', 'Jenkins', 'NOSQL database', 'Git', 'Databricks', 'Data Fabric', 'Data Mesh', 'AWS', 'Python']",2025-06-12 06:07:23
Data Engineer- MS Fabric,InfoCepts,5 - 9 years,Not Disclosed,['India'],"Position: Data Engineer – MS Fabric\n  Purpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\n",,,,"['components', 'data', 'scala', 'delta', 'pyspark', 'data warehousing', 'rules', 'azure data factory', 'sql', 'parquet', 'analytics', 'sql azure', 'spark', 'oracle adf', 'data pipeline architecture', 'etl', 'python', 'azure synapse', 'microsoft azure', 'power bi', 'data bricks', 'data quality', 'system', 't', 'fabric', 'data integration', 'etl process']",2025-06-12 06:07:26
Technical Architect,PwC India,10 - 15 years,Not Disclosed,"['Mumbai', 'Navi Mumbai', 'Gurugram']","Role Description\nWe are looking for a suitable candidate for the opening of Data/Technical Architect role for Data Management, preferably for one who has worked in Insurance or Banking and Financial Services domain and holds relevant experience of 10+ years. The candidate should be willing to take up the role of Senior Manager/Associate Director in an organization based on overall experience.\nLocation : Mumbai and Gurugram\nRelevant experience : 10+ years",,,,"['Data Architecture', 'Technical Architecture', 'Java', 'Bigquery', 'SCALA', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'Data Bricks', 'Python']",2025-06-12 06:07:29
Data Engineering Lead,Yotta Techports,10 - 15 years,30-35 Lacs P.A.,['Hyderabad'],"Responsibilities:\nLead and manage an offshore team of data engineers, providing strategic guidance, mentorship, and support to ensure the successful delivery of projects and the development of team members.\nCollaborate closely with onshore stakeholders to understand project requirements, allocate resources efficiently, and ensure alignment with client expectations and project timelines.\nDrive the technical design, implementation, and optimization of data pipelines, ETL processes, and data warehouses, ensuring scalability, performance, and reliability.\nDefine and enforce engineering best practices, coding standards, and data quality standards to maintain high-quality deliverables and mitigate project risks.\nStay abreast of emerging technologies and industry trends in data engineering, and provide recommendations for tooling, process improvements, and skill development.\nAssume a data architect role as needed, leading the design and implementation of data architecture solutions, data modeling, and optimization strategies.\nDemonstrate proficiency in AWS services such as:\nExpertise in cloud data services, including AWS services like Amazon Redshift, Amazon EMR, and AWS Glue, to design and implement scalable data solutions.\nExperience with cloud infrastructure services such as AWS EC2, AWS S3, to optimize data processing and storage.\nKnowledge of cloud security best practices, IAM roles, and encryption mechanisms to ensure data privacy and compliance.\nProficiency in managing or implementing cloud data warehouse solutions, including data modeling, schema design, performance tuning, and optimization techniques.\nDemonstrate proficiency in modern data platforms such as Snowflake and Databricks, including:\nDeep understanding of Snowflake's architecture, capabilities, and best practices for designing and implementing data warehouse solutions.\nHands-on experience with Databricks for data engineering, data processing, and machine learning tasks, leveraging Spark clusters for scalable data processing.\nAbility to optimize Snowflake and Databricks configurations for performance, scalability, and cost-effectiveness.\nManage the offshore team's performance, including resource allocation, performance evaluations, and professional development, to maximize team productivity and morale.\n\nQualifications:\nBachelor's degree in Computer Science, Engineering, or a related field; advanced degree preferred.\n10+ years of experience in data engineering, with a proven track record of leadership and technical expertise in managing complex data projects.\nProficiency in programming languages such as Python, Java, or Scala, as well as expertise in SQL and relational databases (e.g., PostgreSQL, MySQL).\nStrong understanding of distributed computing, cloud technologies (e.g., AWS), and big data frameworks (e.g., Hadoop, Spark).\nExperience with data architecture design, data modeling, and optimization techniques.\nExcellent communication, collaboration, and leadership skills, with the ability to effectively manage remote teams and engage with onshore stakeholders.\nProven ability to adapt to evolving project requirements and effectively prioritize tasks in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team Handling', 'Snowflake', 'Data Services', 'Cloud Infrastructure', 'Data Bricks']",2025-06-12 06:07:31
Data Engineer - R&D Data Catalyst Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role, you will be responsible for the end-to-end development of an enterprise analytics and data mastering solution using Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and impactful enterprise solutions that research cohort-building and advanced research pipeline. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be extraordinarily skilled with data analysis and profiling.\nYou will collaborate closely with key customers, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a good background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with key customers to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The R&D Data Catalyst Team is responsible for building Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nBasic Qualifications:\nMasters degree and 1 to 3 years of Data Engineering experience OR\nBachelors degree and 3 to 5 years of Data Engineering experience OR\nDiploma and 7 to 9 years of Data Engineering experience\nMust Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3 years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood to Have Skills:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nThe highest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, remote teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data management', 'Power BI', 'data governance', 'data warehousing', 'Databricks', 'ETL', 'AWS']",2025-06-12 06:07:33
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nAs part of the cybersecurity organization, the Data Engineer is responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nCreate data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, Gitlab, LucidChart,etc.\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data engineering', 'data security', 'Agile', 'cloud data platforms', 'Databricks', 'data governance frameworks', 'ETL', 'AWS', 'SQL', 'Python']",2025-06-12 06:07:36
Data Engineer,Xenonstack,2 - 5 years,Not Disclosed,['Mohali( Phase 8B Mohali )'],"At XenonStack, We committed to become the Most Value Driven Cloud Native, Platform Engineering and Decision Driven Analytics Company. Our Consulting Services and Solutions towards the Neural Company and its Key Drivers.\nXenonStacks DataOps team is looking for a Data Engineer who will be responsible for employing techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field.\nYou should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. The ideal candidate should be highly skilled in all aspects of Python, Java/Scala, SQL and analytical skills.\nJob Responsibilities:\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with co-workers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analysed quickly by BI & AI Teams.\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant co-workers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nRequirements:\nTechnical Requirements\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch, Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores.\nProfessional Attributes:\nExcellent communication skills & Attention to detail.\nAnalytical mind and problem-solving Aptitude with Strong Organizational skills & Visual Thinking.\nBenefits:\nDiscover the benefits of joining our team:\nDynamic and purposeful work culture in a people-oriented organization contributing to multi-million-dollar projects with guaranteed job security.\nOpen, authentic, and transparent communication fostering a warm work environment.\nRegular constructive feedback and exposure to diverse technologies.\nRecognition and rewards for exceptional performance achievements.\nAccess to certification courses & Skill Sessions to develop continually and refine your skills.\nAdditional allowances for team members assigned to specific projects.\nSpecial skill allowances to acknowledge and compensate for unique expertise.\nComprehensive medical insurance policy for your health and well-being.\nTo Learn more about the company -\nWebsite - http://www.xenonstack.com/",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Spark', 'ETL', 'Python', 'SQL', 'Java', 'Data Processing', 'Machine Learning']",2025-06-12 06:07:38
Project Manager - Data Migration,TALWORX,9 - 14 years,22.5-35 Lacs P.A.,['Mumbai (All Areas)'],"Role & responsibilities\nDefine project scope, objectives and deliverables in collaboration with IT leads and business sponsors\nBuild & manage detailed migration plan. Coordinate with the internal client PMO function for project onboarding, configuration and tracking on approved PM tools\nCoordinate with cross functional teams & vendors\nOversee data mapping, transformation, validation and testing activities\nManage vendor coordination for tools & services supporting the migration\nEnsure compliance with data governance, security policies and regulatory requirements\nIdentify & manage migration related risks including data loss, downtime & performance issues\nEnsure adherence to QA/UAT protocols and change management process\nSupport audit readiness and documentation\nDefine, Setup and run governance forums\nCommunicate progress, risks, decisions to executive leadership and stakeholders\nDrive validation, optimization and improvement opportunities post migration to enhance data performance and usability\nMentor and guide technical teams throughout the migration journey\nLead and drive change management & knowledge transfer\n\nPreferred candidate profile\n8+ years of experience\nProven expertise delivering atleast one enterprise level end to end data warehouse management program\nSolid understanding of relational database systems, data structures and SQL\nStrong understanding of data modelling, ETL tools/processes, performance tuning and migration planning best practices\nExperience with cloud based database platforms is a plus\nStrong project management skills\nStrong knowledge of SDLC and project management methodologies (Agile, waterfall, Hybrid)\n\nPreferred Skills:\nBFSI or Life Sciences industry exposure\nExperience with data reconciliation and validation frameworks\nStrong interpersonal and communication skills\nAbility to lead cross functional, geographically distributed teams\nFamiliarity with Oracle DMS or Sybase",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['data warehouse', 'Data Migration', 'Oracle', 'SyBase', 'SQL', 'Waterfall', 'Database Management', 'Agile', 'oracle DMS', 'SDLC']",2025-06-12 06:07:41
Assoc. Data Engineer - R&D Precision Medicine Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nThe R&D Precision Medicine team is responsible for Data Standardization, Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with access to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These data include clinical data, omics, and images. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\n\nThe Data Engineer will be responsible for full stack development of enterprise analytics and data mastering solutions leveraging Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that support research cohort-building and advanced AI pipelines. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\n\nYou will collaborate closely with partners, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a solid background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\n\nRoles & Responsibilities\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data management tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with partners to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is someone with these qualifications.\n\nBasic Qualifications:\nMasters degree with 1 to 3 years of experience in Data Engineering OR\nBachelors degree with 1 to 3 years of experience in Data Engineering\nMust-Have\n\nSkills:\nMinimum of 1 year of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 1 year of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nExperience using cloud platforms (AWS), data lakes, and data warehouses.\nWorking knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling and data anlysis\nGood-to-Have\n\nSkills:\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft CertifiedData Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data lakes', 'data pipelines', 'ETL processes', 'AWS', 'data warehouses', 'BI solutions']",2025-06-12 06:07:43
OAC ODI Architect (Senior Oracle Analytics Consultant),Mastek,10 - 15 years,15-30 Lacs P.A.,"['Ahmedabad', 'Chennai']","We are looking for OAC ODI Architect to be based in Ahemdabad or Chennai\nMinimum Architect exp 4 Yrs\nOracle Analytics Consultant (OAC, ODI, FDI) Tech Architect\nLocation: [Specify Location or Remote] Chennai Ahmedabad\nExpected DOJ June\nEmployment Type: Full-time\nExperience Level: 10 - 15+ Years\nJob Summary:\nWe are seeking an experienced and results-driven Senior Oracle Analytics Consultant with over 10 years of hands-on experience in Oracle Analytics Cloud (OAC), Oracle Data Integrator (ODI), and Fusion Data Intelligence (FDI). The ideal candidate will have a deep understanding of enterprise data architecture, data integration best practices, and cloud-based analytics solutions. This role involves working closely with cross-functional teams to design, implement, and support advanced analytics and data integration solutions that drive business value.\nKey Responsibilities:\nLead the design, development, and deployment of analytics solutions using Oracle Analytics Cloud (OAC).\nArchitect and implement data integration pipelines using Oracle Data Integrator (ODI) for on-prem and cloud data sources.\nCollaborate with business and IT stakeholders to design and deploy Fusion Data Intelligence (FDI) based dashboards and KPIs.\nOptimize performance of OAC dashboards and reports, including data modeling and visualization best practices.\nDevelop and manage data models, RPDs, and semantic layers within OAC.\nBuild and maintain ETL mappings, packages, and workflows in ODI.\nIntegrate Oracle Fusion Applications with OAC and FDI for near-real-time reporting.\nDrive data governance and quality initiatives across analytics platforms.\nTroubleshoot technical issues and provide solutions in a timely manner.\nMentor junior developers and provide technical leadership on complex projects.\nQualifications:\nBachelors or Masters degree in Computer Science, Information Systems, or related field.\n10+ years of relevant experience with strong focus on:\nOracle Analytics Cloud (OAC) - Must\nOracle Data Integrator (ODI) - Must\nFusion Data Intelligence (FDI) Good to Have\nExpertise in Oracle Fusion ERP/HCM data models and subject areas.\nExperience integrating multiple data sources, including on-premise and cloud systems.\nStrong understanding of SQL, PL/SQL, and performance tuning.\nFamiliarity with data lake architecture, data warehousing, and ELT/ETL design patterns.\nProven experience working in Agile/DevOps environments.\nExcellent communication, analytical thinking, and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oac', 'ODI', 'Odi Architecture', 'FDI']",2025-06-12 06:07:45
Data Engineer,Trantor,5 - 10 years,Not Disclosed,[],"We are looking for a skilled and motivated Data Engineer with deep expertise in GCP,\nBigQuery, Apache Airflow to join our data platform team. The ideal candidate should have hands-on experience building scalable data pipelines, automating workflows, migrating large-scale datasets, and optimizing distributed systems. The candidate should have experience with building Web APIs using Python. This role will play a key part in designing and maintaining robust data engineering solutions across cloud and on-prem environments.\nKey Responsibilities\nBigQuery & Cloud Data Pipelines:\nDesign and implement scalable ETL pipelines for ingesting large-scale datasets.\nBuild solutions for efficient querying of tables in BigQuery.\nAutomated scheduled data ingestion using Google Cloud services and scheduled\nApache Airflow DAGs",,,,"['Airflow', 'Etl Pipelines', 'GCP', 'Bigquery', 'Python', 'SFTP', 'ETL', 'SQL']",2025-06-12 06:07:48
Job opening For Data Warehouse + ADF + ETL,bct,3 - 6 years,Not Disclosed,['Pune'],"Greetings of the Day !!!\n\nWe have job opening for Data Warehouse + ADF + ETL with one of our Client .If you are interested for this role , kindly share update resume along with below details in this email id : shaswati.m@bct-consulting.com\n\nJob Description:\nSenior Data Engineer\nAs a Senior Data Engineer, you will support the European World Area using the Windows & Azure suite of Analytics & Data platforms. The focus of the role is on the technical aspects and implementation of data gathering, integration and database design.\nWe look forward to seeing your application!\nIn This Role, Your Responsibilities Will Be:\nData Ingestion and Integration: Collaborate with Product Owners and analysts to understand data requirements & design, develop, and maintain data pipelines for ingesting, transforming, and integrating data from various sources into Azure Data Services.\nMigration of existing ETL packages: Migrate existing SSIS packages to Synapse pipelines\nData Modelling: Assist in designing and implementing data models, data warehouses, and databases in Azure Synapse Analytics, Azure Data Lake Storage, and other Azure services.\nData Transformation: Develop ETL (Extract, Transform, Load) processes using SQL Server Integration Services (SSIS), Azure Synapse Pipelines, or other relevant tools to prepare data for analysis and reporting.\nData Quality and Governance: Implement data quality checks and data governance practices to ensure the accuracy, consistency, and security of data assets.\nMonitoring and Optimization: Monitor and optimize data pipelines and workflows for performance, scalability, and cost efficiency.\nDocumentation: Maintain comprehensive documentation of processes, including data lineage, data dictionaries, and pipeline schedules.\nCollaboration: Work closely with cross-functional teams, including data analysts, data scientists, and business stakeholders, to understand their data needs and deliver solutions accordingly.\nAzure Services: Stay updated on Azure data services and best practices to recommend and implement improvements in our data architecture and processes\nFor This Role, You Will Need:\n3-5 years of experience in Data Warehousing with On-Premises or Cloud technologies\nStrong practical experience of Synapse pipelines / ADF.\nStrong practical experience of developing ETL packages using SSIS.\nStrong practical experience with T-SQL or any variant from other RDBMS.\nGraduate degree educated in computer science or a relevant subject.\nStrong analytical and problem-solving skills.\nStrong communication skills in dealing with internal customers from a range of functional areas.\nWillingness to work flexible working hours according to project requirements.\nTechnical documentation skills.\nFluent in English.\nPreferred Qualifications that Set You Apart:\nOracle PL/SQL.\nExperience in working on Azure Services like Azure Synapse Analytics, Azure Data Lake.\nWorking experience with Azure DevOps paired with knowledge of Agile and/or Scrum methods of delivery.\nLanguages: French, Italian, or Spanish would be an advantage.\nAgile certification.\nThanks,\nShaswati",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ADF', 'ETL', 'SSIS', 'Data ware house']",2025-06-12 06:07:50
Manager Data Engineer – Research Data and Analytics,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will create and develop data lake solutions for scientific data that drive business decisions for Research. You will build scalable and high-performance data engineering solutions for large scientific datasets and collaborate with Research collaborators. You will also provide technical leadership to junior team members. The ideal candidate possesses experience in the pharmaceutical or biotech industry, demonstrates deep technical skills, is proficient with big data technologies, and has a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nLead, manage, and mentor a high-performing team of data engineers\nDesign, develop, and implement data pipelines, ETL processes, and data integration solutions\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks\nDevelop and maintain data models for biopharma scientific data, data dictionaries, and other documentation to ensure data accuracy and consistency\nOptimize large datasets for query performance\nCollaborate with global multi-functional teams including research scientists to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nCollaborate with Data Architects, Business SMEs, Software Engineers and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve data-related challenges\nAdhere to best practices for coding, testing, and designing reusable code/component\nExplore new tools and technologies that will help to improve ETL platform performance\nParticipate in sprint planning meetings and provide estimations on technical implementation\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nDoctorate Degree OR\nMasters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications:\n3+ years of experience in implementing and supporting biopharma scientific research data analytics (software platforms)\n\n\nFunctional Skills:\nMust-Have Skills:\nProficiency in SQL and Python for data engineering, test automation frameworks (pytest), and scripting tasks\nHands on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing\nExcellent problem-solving skills and the ability to work with large, complex datasets\nAble to engage with business collaborators and mentor team to develop data pipelines and data models\n\n\nGood-to-Have Skills:\nA passion for tackling complex challenges in drug discovery with technology and data\nGood understanding of data modeling, data warehousing, and data integration concepts\nGood experience using RDBMS (e.g. Oracle, MySQL, SQL server, PostgreSQL)\nKnowledge of cloud data platforms (AWS preferred)\nExperience with data visualization tools (e.g. Dash, Plotly, Spotfire)\nExperience with diagramming and collaboration tools such as Miro, Lucidchart or similar tools for process mapping and brainstorming\nExperience writing and maintaining technical documentation in Confluence\nUnderstanding of data governance frameworks, tools, and best practices\n\n\nProfessional Certifications:\nDatabricks Certified Data Engineer Professional preferred\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Spotfire', 'PySpark', 'PostgreSQL', 'Plotly', 'SparkSQL', 'SQL server', 'SQL', 'process mapping', 'Dash', 'MySQL', 'ETL', 'Oracle', 'data governance frameworks', 'Python']",2025-06-12 06:07:52
Data Engineer,Databeat,3 - 7 years,Not Disclosed,['Hyderabad( Rai Durg )'],"Experience Required: 3+ years\n\nTechnical knowledge: AWS, Python, SQL, S3, EC2, Glue, Athena, Lambda, DynamoDB, RedShift, Step Functions, Cloud Formation, CI/CD Pipelines, Github, EMR, RDS,AWS Lake Formation, GitLab, Jenkins and AWS CodePipeline.\n\n\n\nRole Summary: As a Senior Data Engineer,with over 3 years of expertise in Python, PySpark, SQL to design, develop and optimize complex data pipelines, support data modeling, and contribute to the architecture that supports big data processing and analytics to cutting-edge cloud solutions that drive business growth. You will lead the design and implementation of scalable, high-performance data solutions on AWS and mentor junior team members.This role demands a deep understanding of AWS services, big data tools, and complex architectures to support large-scale data processing and advanced analytics.\nKey Responsibilities:\nDesign and develop robust, scalable data pipelines using AWS services, Python, PySpark, and SQL that integrate seamlessly with the broader data and product ecosystem.\nLead the migration of legacy data warehouses and data marts to AWS cloud-based data lake and data warehouse solutions.\nOptimize data processing and storage for performance and cost.\nImplement data security and compliance best practices, in collaboration with the IT security team.\nBuild flexible and scalable systems to handle the growing demands of real-time analytics and big data processing.\nWork closely with data scientists and analysts to support their data needs and assist in building complex queries and data analysis pipelines.\nCollaborate with cross-functional teams to understand their data needs and translate them into technical requirements.\nContinuously evaluate new technologies and AWS services to enhance data capabilities and performance.\nCreate and maintain comprehensive documentation of data pipelines, architectures, and workflows.\nParticipate in code reviews and ensure that all solutions are aligned to pre-defined architectural specifications.\nPresent findings to executive leadership and recommend data-driven strategies for business growth.\nCommunicate effectively with different levels of management to gather use cases/requirements and provide designs that cater to those stakeholders.\nHandle clients in multiple industries at the same time, balancing their unique needs.\nProvide mentoring and guidance to junior data engineers and team members.\n\n\n\nRequirements:\n3+ years of experience in a data engineering role with a strong focus on AWS, Python, PySpark, Hive, and SQL.\nProven experience in designing and delivering large-scale data warehousing and data processing solutions.\nLead the design and implementation of complex, scalable data pipelines using AWS services such as S3, EC2, EMR, RDS, Redshift, Glue, Lambda, Athena, and AWS Lake Formation.\nBachelor's or Masters degree in Computer Science, Engineering, or a related technical field.\nDeep knowledge of big data technologies and ETL tools, such as Apache Spark, PySpark, Hadoop, Kafka, and Spark Streaming.\nImplement data architecture patterns, including event-driven pipelines, Lambda architectures, and data lakes.\nIncorporate modern tools like Databricks, Airflow, and Terraform for orchestration and infrastructure as code.\nImplement CI/CD using GitLab, Jenkins, and AWS CodePipeline.\nEnsure data security, governance, and compliance by leveraging tools such as IAM, KMS, and AWS CloudTrail.\nMentor junior engineers, fostering a culture of continuous learning and improvement.\nExcellent problem-solving and analytical skills, with a strategic mindset.\nStrong communication and leadership skills, with the ability to influence stakeholders at all levels.\nAbility to work independently as well as part of a team in a fast-paced environment.\nAdvanced data visualization skills and the ability to present complex data in a clear and concise manner.\nExcellent communication skills, both written and verbal, to collaborate effectively across teams and levels.\n\nPreferred Skills:\nExperience with Databricks, Snowflake, and machine learning pipelines.\nExposure to real-time data streaming technologies and architectures.\nFamiliarity with containerization and serverless computing (Docker, Kubernetes, AWS Lambda).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'SQL', 'Data Pipeline', 'Python', 'Amazon Ec2', 'Data Engineering', 'Data Bricks', 'Aws Lambda', 'Amazon Redshift', 'Azure Cloud', 'Data Lake', 'Data Modeling', 'Athena']",2025-06-12 06:07:54
Snowflake Developer with Azure Data Factory,Net Connect,6 - 10 years,6-11 Lacs P.A.,['Hyderabad'],Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\n\nBelow JD for your reference\n\nJob Description:,,,,"['Azure Data Factory', 'Snowflake', 'SQL']",2025-06-12 06:07:56
Data Engineer,Nemetschek,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\n5+ years in software development, with a focus on data-intensive applications, cloud solutions, and scalable data architectures.\nDevelopment experience in GoLang for building scalable and efficient data applications.\nExperience with Snowflake, Redshift, or similar data platforms including architecture, data modeling, performance optimization, and integrations.\nExperience designing and building data lakes and data warehouses, ensuring data integrity, scalability, and performance.\nProficient in developing and managing ETL pipelines, using modern tools and techniques to transform, load, and integrate data efficiently.\nExperience with high-volume event streams (such as Kafka, Kinesis) and near real-time data processing solutions for fast and accurate reporting.\nHands-on experience with Terraform for automating infrastructure deployment and configuration management in cloud environments.\nExperience with containerization technologies (Docker, Kubernetes) and orchestration.\nSolid grasp of database fundamentals (SQL, NoSQL, data modeling, performance tuning)\nExperience with CI/CD pipelines and automation tools for testing, deployment, and continuous improvement.\nExperience working in AWS cloud environments, specifically with big data solutions and serverless architectures\nAbility to mentor and guide junior engineers, fostering a culture of learning and innovation\nStrong communication skills to articulate technical concepts clearly to non-technical stakeholders.\nWHAT WE OFFER\nA young, dynamic, and innovation-oriented environment\nA wide variety of projects within different industries\nA very open and informal culture where knowledge sharing, and employee development are key.\nRoom for personal initiative, development, and growth\nRealistic career opportunities\nCompetitive package and fringe benefits.\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Golang', 'Snowflake', 'Javascript', 'ETL', 'AWS']",2025-06-12 06:07:59
Solution Architect - Java,Epam Systems,10 - 13 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","We are looking for a Solution Architect - Java to join our team.\nYour expertise will shape the design of scalable software solutions. You will leverage your extensive experience in solution architecture and microservice architecture styles to drive innovation. If you are ready to take your career to the next level, we encourage you to apply.\n\nResponsibilities\nDesign secure, reliable, high availability, scalable solutions for the program\nDefine, plan, and support execution of the technology strategy for one or more products\nCollaborate closely with the global solution architecture and engineering team to define principles and best practices\nEngage with wider architecture and technology teams to ensure alignment with technical strategies and policies\nSupport development teams and work with stakeholders, promoting agile development\nCreate a culture of technical excellence and continuous improvement\nResearch, create, and evaluate technical solution alternatives for business needs using current and upcoming technologies\nDrive overall software implementation using expertise in microservices-based architectures for the fintech industry\nPartner with senior technical and product leaders to deliver on designs\nCollaborate with development teams, operations, and product owners\nProvide technical leadership and mentorship to development teams\nRepresent as the primary architect and technical advocate in program discussions\nRequirements\nExperience in product engineering with over 15 years in designing scalable software solutions\nBackground in computer science fundamentals, web applications, and microservices-based software architecture\nExperience with high transaction volume financial systems operating at global scale\nKnowledge of web technologies including HTML5, CSS, and JavaScript, along with front-end frameworks like AngularJS and ReactJS\nProficiency in designing and building back-end microservices using Java and Spring frameworks\nUnderstanding of storage technologies such as PostgreSQL and SQL Server for large-scale applications\nFamiliarity with cloud-native technologies and best practices, including Amazon Web Services and Microsoft Azure\nCapability to work effectively in an Agile environment focused on continuous improvement\nDesire to collaborate and provide mentorship to technology teams\nHands-on experience in building prototypes to solve complex business problems\nEnglish proficiency at a professional level",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['css', 'technical leadership', 'web services', 'web application', 'microservices', 'sql', 'spring', 'react.js', 'java', 'postgresql', 'computer science', 'gcp', 'design', 'software solutions', 'end', 'html', 'architecture', 'microsoft azure', 'javascript', 'sql server', 'high availability', 'framework', 'web technologies', 'agile', 'aws', 'angularjs']",2025-06-12 06:08:02
Data Migration Expert - Talend,Maddisoft Solutions,10 - 20 years,Not Disclosed,['Hyderabad'],"Job Title: Data Migration Expert - Talend\nLocation: Hyderabad, India\n\nJob Description:\nMinimum of 9+ years of experience in data migration in Talend projects\nHandsons experience 5+ mandatory knowledge of Talend/SQL tools(Basic knowledge of SAP Routing and SAP Production order tables Individually is an add on)\nProficiency in data migration tools and methodologies, SAP ECC AND S/4HANA Migration Cockpit.\nHands-on experience Data Replication, Data Quality, Data Workbench, Talend or similar ETL tools.\nFamiliarity with Talend (ETL) is plus.\nStrong understanding of data modeling concepts, data mapping techniques, and data transformation rules.\nExcellent SQL skills for data extraction, manipulation, and analysis.\nExperience with SAP all Cross functional modules such as Finance (FI), Controlling (CO), Material Management (MM), Sales and Distribution (SD), or Production Planning (PP).\nStrong analytical, problem-solving, and troubleshooting skills.\nExcellent communication, presentation, and interpersonal skills.\nAbility to work independently and as part of a team in a fast-paced, dynamic environment.\nSAP certification in Data Migration or related field is a plus.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['SAP ECC', 'Data Migration', 'Talend']",2025-06-12 06:08:04
Azure Data Bricks (4-15 Yrs) - Bangalore,Happiest Minds Technologies,4 - 9 years,Not Disclosed,['Bengaluru'],"Hi,\n\nGreetings from Happiest Minds Technologies\n\nCurrently we are hiring for below positions and looking for immediate joiners.\n1. Azure Databricks Bangalore 5 to 10 Yrs - Bangalore\nAs a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools. Proficiency in Python PySpark, Pandas, NumPy, SciPy, Spark SQL, DataFrames, RDDs, Delta Lake, Databricks Notebooks, and MLflow is required, along with hands-on experience in Azure Data Lake, Blob Storage, and Synapse Analytics.",,,,"['Pyspark', 'Azure', 'Data Bricks', 'sql', 'ETL']",2025-06-12 06:08:06
SAP DATA Steward,Maddisoft Solutions,10 - 20 years,Not Disclosed,['Hyderabad'],"Job Title: SAP DATA STEWARD (CONTRACT)\nLocation: HYDERABAD, INDIA\n\nAs the SAP Data Steward is responsible for creating, maintaining, and deactivating master data and data attributes in SAP with focus on data migration. The Data Stewards has an essential role in establishing in monitoring existing and new data against and ensuring timely and high-quality creation of new data in the system.\nBachelors Degree or Associates degree with additional 9+ years of work experience required or an equivalent combination of education and experience.\nRequires SAP functional knowledge on SAP Routings with Migration Perspective.\nShould have complete knowledge on SAP Routings tables. Need to have basic knowledge on linking between the tables and joins needed for getting the extract template ready as per Client's Format.\nExcellent attention to detail, exceptional interest in creating order and consistency required.\n10+ years of experience in data management and/or data governance activities and responsibilities.\nExperience working with SAP ECC required.\nDemonstrated expert-level experience and capability with MS Excel required.\nHigh degree of initiative and ownership, as well as a proven history of delivering results while working with several different departments in a fast-paced environment required.\nExperience creating and running business reports and data queries is preferred.\nConfident user of Microsoft Office (Word, Excel, Outlook, PowerPoint, Teams).\nExperience working with teams across multiple functions.\nAbility to multi-task and work under tight timelines required.\nExcellent communication skills both verbal and written.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['SAP ECC', 'Data Stewardship', 'Data Management', 'Data Governance', 'SAP Routing']",2025-06-12 06:08:09
Mobile B2C BSS OSS Solution Architect Service fulfilment,Prodapt Solutions,5 - 10 years,Not Disclosed,['Bengaluru'],"Overview\n\n*Need Telecom Solution Architect/Digital Solution Designer with telecom domain experience.\n*Should have handson in Lead to Cash journey - Processes and Systems for Cellular Mobile service for Retail customers\n*Should have solid understanding on different Service Fulfilment journey metrics.\n*Should have solid understanding of different BSS/OSS applications that support Lead to Cash journey.\n*Should have strong understanding of different Channels, Product Catalog, Order Orchestration, Provisioning & Activation\n*Should have conducted workshops to gather requirements from stakeholders, presenting different solution options to architects.\n*Should have expertise in:\n**Designing multi-channel Enterprise solutions among BSS domains CRM, Order Management, BPM, OmniChannel implementations, TMForum Open APIs , Catalog, Intelligence platforms etc\n**Knowledge on Event/Data driven architecture, micro service framework and API integration\n**Experience in designing enterprise systems in BSS\n\nResponsibilities\n\nMandatory to have excellent understanding of Telecom Domain, and experience of Consulting / Solution Design / Solution Architecture for Telecom Solutions (Products and Services development).\nAbility to analyze Telco processes, simplify them and develop solutions in alignment with architecture principles.\nExperience in working with business and application teams to collect requirements and deliver solutions.\nExperience in BSS/OSS is must, at least 5+ Years.\nUnderstands Open API or TMF API specification or API based integration with partners\nDefine L2/L3 data model using reference data architecture like SID\nDeveloping micro services-based architecture\nTMF - open digital architecture or similar framework to develop channels architecture\nExperience of using modelling tools for architecture development\nAs a Telecom Domain expert, you will be required to seek out the best ways to improve business processes and effectiveness through technology, strategy, analytic solutions.\nBrings in strong knowledge in the telecom domain, and having working experience across Business and Technology Roles, across B2B and B2C businesses.\nExperience with Business Analysis, Business Process Re-engineering, E2E Solution Design / Component or System Design / Solution Architecture, in Telecom OSS / BSS domain is required.\n\nGood to have working experience on Digital Transformation, OSS/BSS Cloud Implementations, Network Transformation programs.\nUnderstanding of Technology Landscape such as E2E OSS & BSS, Microservices based Architecture, RPA, Service Assurance, Billing, and Invoicing for B2C business.\nShould have a business focus, and an equal understanding of IT/Technology enablement, so as to be able to align the business goals/objectives to the IT delivery.\nGood experience in data modelling and integration patterns.\nHas good understanding of frameworks relevant for Telecommunications domain (for e.g. eTOM, SID, TAM, ITIL, ODA)\nAble to understand and translate business requirements, and working with product / technical and operations teams to help design and build new Products and Services\nAble to conduct workshops and design thinking sessions, to understand IT Process / Technology / Architecture and existing OSS/BSS implementations.\nAble to deliver intelligent business insights through translation of reports and analytics.\nUnderstanding of technologies such as 4G/5G/Metro Ethernet/ATM/MPLS/SONET/SDH etc. and their mapping to OSS systems is an added advantage.\nConduct proof-of-concept (POCs) for requirements.\nGood communication and Presentation skills.\n\nGood to have\nUnderstanding of autonomous networks and closed loop automation\nKnowledge of Architecture design methodologies and Industry reference frameworks – TOGAF etc. – with a clear demonstration of the deliverables created as a result of applying the framework\nKnowledge of 5G & IoT\nExperience in any of the COTS products\nExperience in Network optimization techniques\nUnderstanding of intelligent applying across OSS land scape\nExperience of working on network technologies\nKnowledge of DevOps and open-source tools\nExperience in any one of the programming languages - Java or Python\nExperience in any one of the DB technologies - Oracle, MySQL, Postgres, Cassandra\nWorking with development teams and product managers to build software solutions/web applications\n\n\nBachelor's degree (in any field) is mandatory.\nMSc/BE /Masters with specialization in IT/Computer Science is desired.\nAtleast 5 years of work experience.\n\nAdaptability and experience working in multi-channel delivery projects is preferred.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['product catalog', 'bss', 'open source', 'telecom', 'provisioning', 'python', 'etom', 'oracle', 'togaf', 'business analysis', 'solution delivery', 'data architecture', 'tmf', 'microservices', 'java', 'postgresql', 'solution design', 'cassandra', 'devops', 'open api', 'e2e', 'mysql', 'api', 'digital transformation']",2025-06-12 06:08:12
Mobile B2C BSS OSS Solution Architect Service Assurance,Prodapt Solutions,5 - 10 years,Not Disclosed,['Bengaluru'],"Overview\n\n*Need Telecom Solution Architect/Digital Solution Designer with telecom domain experience.\n*Should have hands-on in Service Assurance journey - Processes and Systems for Cellular Mobile service for Retail customers\n*Should have solid understanding on different Service Assurance journey metrics.\n*Should have solid understanding of different BSS OSS applications that support Service Assurance Journey\n*Should have solid understanding of Billing & Rating\n*Should have conducted workshops to gather requirements from stakeholders, presenting different solution options to architects.\n*Should have expertise in:\n**Designing multi-channel Enterprise solutions among BSS/OSS domains with in-depth knowledge in Network monitoring system, Fault Orchestration/Incident Management system, Test & Diagnostics system, Billing & Rating system, Provisioning & Activation system, Service Inventory, CRM, Field Engineering Management system, OmniChannel implementations, Network performance management system, TMForum Open APIs, Intelligence platforms etc\n**Knowledge on Event/Data driven architecture, micro service framework and API integration\n**Experience in designing enterprise systems in BSS/OSS\n\nResponsibilities\n\nMandatory to have excellent understanding of Telecom Domain, and experience of Consulting / Solution Design / Solution Architecture for Telecom Solutions (Products and Services development).\nAbility to analyze Telco processes, simplify them and develop solutions in alignment with architecture principles.\nExperience in working with business and application teams to collect requirements and deliver solutions.\nExperience in BSS/OSS is must, at least 5+ Years.\nUnderstands Open API or TMF API specification or API based integration with partners\nDefine L2/L3 data model using reference data architecture like SID\nDeveloping micro services-based architecture\nTMF - open digital architecture or similar framework to develop channels architecture\nExperience of using modelling tools for architecture development\nAs a Telecom Domain expert, you will be required to seek out the best ways to improve business processes and effectiveness through technology, strategy, analytic solutions.\nBrings in strong knowledge in the telecom domain, and having working experience across Business and Technology Roles, across B2B and B2C businesses.\nExperience with Business Analysis, Business Process Re-engineering, E2E Solution Design / Component or System Design / Solution Architecture, in Telecom OSS / BSS domain is required.\n\nGood to have working experience on Digital Transformation, OSS/BSS Cloud Implementations, Network Transformation programs.\nUnderstanding of Technology Landscape such as E2E OSS & BSS, Microservices based Architecture, RPA, Service Assurance, Billing, and Invoicing for B2C business.\nShould have a business focus, and an equal understanding of IT/Technology enablement, so as to be able to align the business goals/objectives to the IT delivery.\nGood experience in data modelling and integration patterns.\nHas good understanding of frameworks relevant for Telecommunications domain (for e.g. eTOM, SID, TAM, ITIL, ODA)\nAble to understand and translate business requirements, and working with product / technical and operations teams to help design and build new Products and Services\nAble to conduct workshops and design thinking sessions, to understand IT Process / Technology / Architecture and existing OSS/BSS implementations.\nAble to deliver intelligent business insights through translation of reports and analytics.\nUnderstanding of technologies such as 4G/5G/Metro Ethernet/ATM/MPLS/SONET/SDH etc. and their mapping to OSS systems is an added advantage.\nConduct proof-of-concept (POCs) for requirements.\nGood communication and Presentation skills.\n\nGood to have\nUnderstanding of autonomous networks and closed loop automation\nKnowledge of Architecture design methodologies and Industry reference frameworks – TOGAF etc. – with a clear demonstration of the deliverables created as a result of applying the framework\nKnowledge of 5G & IoT\nExperience in any of the COTS products\nExperience in Network optimization techniques\nUnderstanding of intelligent applying across OSS land scape\nExperience of working on network technologies\nKnowledge of DevOps and open-source tools\nExperience in any one of the programming languages - Java or Python\nExperience in any one of the DB technologies - Oracle, MySQL, Postgres, Cassandra\nWorking with development teams and product managers to build software solutions/web applications\n\n\nBachelor's degree (in any field) is mandatory.\nMSc/BE /Masters with specialization in IT/Computer Science is desired.\nAtleast 5 years of work experience.\n\nAdaptability and experience working in multi-channel delivery projects is preferred.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['bss', 'service assurance', 'software testing', 'billing', 'telecom', 'python', 'engineering management', 'field engineering', 'performance management system', 'open source', 'network monitoring', 'network performance', 'java', 'postgresql', 'incident management', 'provisioning', 'mysql', 'crm']",2025-06-12 06:08:14
IT Auditor,KPMG Assurance and Consulting Services LLP,2 - 7 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nPerform testing of IT Application Controls, IPE, and Interface Controls through code reviews, IT General Controls review covering areas such as Change Management, Access Management, Backup Management, Incident and Problem Management, SDLC, Data Migration, Batch Job scheduling/monitoring and Business Continuity and Disaster Recovery\nRisk Based IT Internal Audit for Financial Services Entities\nIT SOX 404 Controls Testing, Quality Assurance\nInternal Financial Controls related to IT General Controls as part of Financial Statements Audits\nBusiness Systems Controls / IT Application Controls\nIT Risk & Control Self-Assessment\nAuditing Emerging Technologies such as Cloud Security, Intelligent Automation, RPA, IoT etc.\nWorking knowledge of programming languages(C/C++/Java/SQL)\n\nPreferred candidate profile\nA Bachelor's degree in engineering and approximately 2-7 years of related work experience; or a masters or MBA degree in business, computer science, information systems, engineering\nExpertise in code review skills (e.g., Java, C++, C, SQL, Oracle)\nExperience in performing IT audits of banking/financial sector applications\nGood to have knowledge of other IT regulations, standards and benchmarks used by the IT industry (e.g., NIST, PCI-DSS, ITIL, OWASP, SOX, COBIT, SSAE18/ISAE 3402 etc.)",Industry Type: Management Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['IT Audit', 'ITGC', 'Code Review', 'ITAC']",2025-06-12 06:08:17
ETL Developer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\nResponsibilities:\nDesign and implement the data modeling, data ingestion and data processing for various datasets\nDesign, develop and maintain ETL Framework for various new data source\nDevelop data ingestion using AWS Glue/ EMR, data pipeline using PySpark, Python and Databricks.\nBuild orchestration workflow using Airflow & databricks Job workflow\nDevelop and execute adhoc data ingestion to support business analytics.\nProactively interact with vendors for any questions and report the status accordingly\nExplore and evaluate the tools/service to support business requirement\nAbility to learn to create a data-driven culture and impactful data strategies.\nAptitude towards learning new technologies and solving complex problem.\nQualifications:\nMinimum of bachelors degree. Preferably in Computer Science, Information system, Information technology.\nMinimum 5 years of experience on cloud platforms such as AWS, Azure, GCP.\nMinimum 5 year of experience in Amazon Web Services like VPC, S3, EC2, Redshift, RDS, EMR, Athena, IAM, Glue, DMS, Data pipeline & API, Lambda, etc.\nMinimum of 5 years of experience in ETL and data engineering using Python, AWS Glue, AWS EMR /PySpark and Airflow for orchestration.\nMinimum 2 years of experience in Databricks including unity catalog, data engineering Job workflow orchestration and dashboard generation based on business requirements\nMinimum 5 years of experience in SQL, Python, and source control such as Bitbucket, CICD for code deployment.\nExperience in PostgreSQL, SQL Server, MySQL & Oracle databases.\nExperience in MPP such as AWS Redshift, AWS EMR, Databricks SQL warehouse & compute cluster.\nExperience in distributed programming with Python, Unix Scripting, MPP, RDBMS databases for data integration\nExperience building distributed high-performance systems using Spark/PySpark, AWS Glue and developing applications for loading/streaming data into Databricks SQL warehouse & Redshift.\nExperience in Agile methodology\nProven skills to write technical specifications for data extraction and good quality code.\nExperience with big data processing techniques using Sqoop, Spark, hive is additional plus\nExperience in data visualization tools including PowerBI, Tableau.\nNice to have experience in UI using Python Flask framework anglular\n\n\nMandatory Skills: Python for Insights. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'data bricks', 'aws glue', 'amazon ec2', 'python', 'spark', 'glue', 'amazon redshift', 'cloud platforms', 'aws', 'data engineering', 'sql']",2025-06-12 06:08:19
Snowflake Developer,internal,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","We are seeking a skilled Snowflake Developer to design, develop, and manage scalable data solutions using the Snowflake cloud data platform. The ideal candidate will have deep experience in data warehousing, SQL development, ETL processes, and cloud data architecture. Understanding of Control M and Tableau will be an added advantage. 1. Snowflake (Cloud Data Warehouse):1. Good understanding of Snowflake ECO system2. Good experience on Data modeling and Dimensional Modeling and techniques and will be able to drive the Technical discussions with IT & Business and Architects / Data Modelers3. Need to guide the team and provide the technical solutions4. Need to prepare the technical solution and architectures as part of project requirements5. Virtual Warehouse (Compute) - Good Understanding of Warehouse creation & manage6. Data Modeling & Storage - Strong knowledge on LDM/PDM design7. Data Loading/Unloading and Data Sharing- Should have good knowledge8. SnowSQL (CLI)- Expertise and excellent understanding of Snowflake Internals and Integration9. Strong hands on experience on SNOWSQL queries and Stored procedures and performance tuning techniques10. Good knowledge on SNOWSQL Scripts preparation the data validation and Audits11. SnowPipe Good knowledge of Snow pipe implementation12. Expertise and excellent understanding of S3 - Internal data copy/movement13. Good knowledge on Security & Readers and Consumers accounts14. Good knowledge and hands on experience on Query performance tuning implementation techniques2. SQL Knowledge:1. Advance SQL knowledge and hands on experience on complex queries writing using with Analytical functions2. Strong knowledge on stored procedures3. Troubleshooting, problem solving and performance tuning of SQL queries accessing data warehouse",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Part Time, Temporary/Contractual","['Snowflake', 'S3 integration', 'SQL']",2025-06-12 06:08:22
Senior Software Engineer - Adobe Experience Platform ( AEP ),Wells Fargo,4 - 7 years,Not Disclosed,['Hyderabad'],"About this role:\nWells Fargo is seeking a senior Software Engineer - Adobe Experience Platform (AEP)\nIn this role, you will:\nLead moderately complex initiatives and deliverables within technical domain environments\nContribute to large scale planning of strategies\nDesign, code, test, debug, and document for projects and programs associated with technology domain, including upgrades and deployments",,,,"['Software Engineering', 'Data Science', 'data model', 'data analysis', 'data modeling', 'configuration', 'APIs', 'AEP', 'CDP']",2025-06-12 06:08:25
Workday Consultant,Tata Consultancy Services,4 - 9 years,4-8.5 Lacs P.A.,['Bengaluru'],"Dear Candidate,\nGreetings from TATA Consultancy Services!!\nThank you for expressing your interest in exploring a career possibility with the TCS Family.\nHiring For:- Workday\nLocation: Brigade Bhuwalka, Bangalore\nExperience: 4 to 12 years\nExperience in the role include Workday HCM cloud solutions experience including implementation, integrations, data migrations, development in Workday Extend and support (AMS/AD)",,,,"['Workday', 'Workday Functional', 'Workday Hcm']",2025-06-12 06:08:27
AWS Technical Architect,Puma Energy,10 - 14 years,35-40 Lacs P.A.,"['Mumbai', 'Pune']","We are seeking a highly experienced AWS Technical Architect to lead the design, implementation, and optimization of cloud infrastructure solutions on the Amazon Web Services (AWS) platform. The ideal candidate will have extensive hands-on experience with AWS services, a deep understanding of cloud architecture best practices, and a proven track record of delivering complex, enterprise-scale cloud solutions. Preferably from Energy, Retail sector.\nProvide technical leadership and strategic direction for the design and implementation of AWS-based solutions.\nCollaborate with cross-functional teams, including business stakeholders, to understand requirements and translate them into robust, scalable, and cost-effective technical solutions.\nDevelop and maintain reference architectures, design patterns, and implementation guidelines for AWS services.\nPerform capacity planning, cost optimization, and performance tuning of AWS environments.\nImplement and automate infrastructure as code (IaC) using tools like AWS CloudFormation, Terraform, or AWS CDK\nEnsure compliance with security, governance, and regulatory requirements.\nMentor and provide technical guidance to development teams on AWS best practices.\nStay up to date with the latest AWS services and features and drive their adoption within the organization.\nParticipate in the evaluation and selection of third-party tools and services for integration with AWS.\nImplement security best practices, including IAM, encryption, and network security measures.\nEnsure solutions comply with relevant regulatory standards and corporate policies.\nConduct regular audits and assessments to maintain cloud environment integrity.\nExperience\nShould have around 8-10 years of experience within IT.\nQualifications\n8-10 years of experience in designing and implementing cloud solutions, with a strong focus on AWS\nExtensive hands-on experience with a wide range of AWS services, including EC2, VPC, ELB, RDS, S3, CloudFront, Lambda, and more.\nDeep expertise in infrastructure as code (IaC) tools like AWS CloudFormation, Terraform, or AWS CDK\nStrong understanding of cloud architecture principles, including high availability, scalability, security, and cost optimization\nExperience with containerization technologies like Docker and container orchestration platforms (e.g., ECS, EKS)\nProficiency in at least one programming language (e.g., Python, Java, Node.js)\nExperience with CI/CD pipelines and automation tools (e.g., AWS CodePipeline, Jenkins)\nStrong problem-solving, analytical, and communication skills\nAWS Certified Solutions Architect (Professional) or equivalent certification is required.\nExperience in leading and mentoring technical teams.\nInternal and External",Industry Type: Oil & Gas,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS', 'Aws Migration', 'Solution Architecting', 'Aws Infrastructure', 'Cloud Architecture', 'Designing And Developing']",2025-06-12 06:08:30
Solution Architect,Azine Technologies,5 - 8 years,Not Disclosed,['Ahmedabad'],"We are seeking an experienced Solution Architect to design scalable, high-performance technical solutions aligned with business goals. The role involves collaborating with cross-functional teams and leading architecture strategy and implementation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution Architecting', 'Data Architecture', 'Solution Design', 'System Integration', 'Enterprise Architecture', 'Software Development Life Cycle', 'Cloud Platforms', 'TOGAF', 'Cloud Architecture', 'IT Architecture', 'Enterprise Integration', 'IT Strategy']",2025-06-12 06:08:32
App Dev & Support Engineer III,Conduent,2 - 6 years,Not Disclosed,['Bengaluru'],"Job Track Description:\nCandidate needs to be Oracle Fusion HCM CloudSubject Mater expertin Integration (API/Extract), Fast Formula & Reporting tools.\nAnalyze client requirements and provide design solutions using Oracle Fusion HCM modules.\nEnsure Team customize and configure Fusion HCM applications to align with client business processes.\nEnsure Team develops and maintain reports, interfaces, conversions, and extensions as per project needs.\nLead data migration activities, ensuring accurate and timely transfer of data from legacy systems to Fusion HCM.\nCollaborate with cross-functional teams to integrate Fusion HCM with other enterprise systems.\nProvide technical guidance and support to junior team members.\nStay updated with Oracle Fusion HCM updates and best practices.\nEnsure Team adherence to SOPs - Change Management, Coding Standards, Knowledge Management, Oracle SR, Batch Job Monitoring\nRequires formal education and relevant expertise in professional & technical area.\nPerforms technical-based activities.\nContributes to and manages projects.\nUses deductive reasoning to solve problems and make recommendations.\nInterfaces with and influences key stakeholders.\nLeverages previous knowledge and expertise to achieve results.\nProficiency in independently managing and completing tasks.",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['oracle', 'engineering support', 'oracle fusion hcm', 'oracle fusion', 'change management', 'bi publisher', 'oracle core hr', 'oracle apps technical', 'core hr', 'sql', 'plsql', 'hdl', 'hcm', 'application support', 'linux', 'hcm extracts', 'attendance management', 'payroll', 'oracle hrms']",2025-06-12 06:08:35
App Dev & Support Engineer II,Conduent,2 - 5 years,Not Disclosed,['Bengaluru'],"Job Track Description:\n\n\nCandidate needs to be Oracle Fusion HCM Cloud\\u202FSubject Mater expert\\u202Fin Integration ( API/Extract ), Fast Formula & Reporting tools. ( OTBI , BI Publisher )\n\n\nAnalyze client requirements and provide design solutions using Oracle Fusion HCM modules.\n\n\nEnsure Team customize and configure Fusion HCM applications to align with client business processes.\n\n\nEnsure Team develops and maintain reports, interfaces, conversions, and extensions as per project needs.\\u202F\\u202F\n\n\nLead data migration activities, ensuring accurate and timely transfer of data from legacy systems to Fusion HCM.\n\n\nCollaborate with cross-functional teams to integrate Fusion HCM with other enterprise systems.\n\n\nProvide technical guidance and support to junior team members.\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\\u202F\n\n\nStay updated with Oracle Fusion HCM updates and best practices.\n\n\nEnsure Team adherence to SOPs - Change Management, Coding Standards, Knowledge Management, Oracle SR, Batch Job Monitoring\n\n\nRequires formal education and relevant expertise in professional & technical area.\n\n\nPerforms technical-based activities.\n\n\nContributes to and manages projects.\n\n\nUses deductive reasoning to solve problems and make recommendations.\n\n\nInterfaces with and influences key stakeholders.\n\n\nLeverages previous knowledge and expertise to achieve results.\\u202F\n\n\nProficiency in independently managing and completing tasks.",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['oracle', 'engineering support', 'oracle fusion hcm', 'change management', 'sql', 'bi publisher', 'networking', 'oracle fusion', 'plsql', 'production support', 'technical support', 'unix shell scripting', 'application support', 'linux', 'desktop support', 'hcm extracts', 'troubleshooting', 'unix']",2025-06-12 06:08:38
"Senior Engineer, Application Development",S&P Global Market Intelligence,5 - 8 years,Not Disclosed,['Hyderabad'],"Grade Level (for internal use):\n10\nMarket Intelligence\nThe Role: Senior Full Stack Developer\nGrade level :10\nThe Team: You will work with a team of intelligent, ambitious, and hard-working software professionals. The team is responsible for the architecture, design, development, quality, and maintenance of the next-generation financial data web platform. Other responsibilities include transforming product requirements into technical design and implementation. You will be expected to participate in the design review process, write high-quality code, and work with a dedicated team of QA Analysts, and Infrastructure Teams\nThe Impact: Market Intelligence is seeking a Software Developer to create software design, development, and maintenance for data processing applications. This person would be part of a development team that manages and supports the internal & external applications that is supporting the business portfolio. This role expects a candidate to handle any data processing, big data application development. We have teams made up of people that learn how to work effectively together while working with the larger group of developers on our\nplatform.\nWhats in it for you:\nOpportunity to contribute to the development of a world-class Platform Engineering team .\nEngage in a highly technical, hands-on role designed to elevate team capabilities and foster continuous skill enhancement.\nBe part of a fast-paced, agile environment that processes massive volumes of dataideal for advancing your software development and data engineering expertise while working with a modern tech stack.\nContribute to the development and support of Tier-1, business-critical applications that are central to operations.\nGain exposure to and work with cutting-edge technologies including AWS Cloud , EMR and Apache NiFi .\nGrow your career within a globally distributed team , with clear opportunities for advancement and skill development.\nResponsibilities:\nDesign and develop applications, components, and common services based on development models, languages and tools, including unit testing, performance testing and monitoring and implementation\nSupport business and technology teams as necessary during design, development and delivery to ensure scalable and robust solutions\nBuild data-intensive applications and services to support and enhance fundamental financials in appropriate technologies.( C#, .Net Core, Databricsk, Spark ,Python, Scala, NIFI , SQL)\nBuild data modeling, achieve performance tuning and apply data architecture concepts\nDevelop applications adhering to secure coding practices and industry-standard coding guidelines, ensuring compliance with security best practices (e.g., OWASP) and internal governance policies.\nImplement and maintain CI/CD pipelines to streamline build, test, and deployment processes; develop comprehensive unit test cases and ensure code quality\nProvide operations support to resolve issues proactively and with utmost urgency\nEffectively manage time and multiple tasks\nCommunicate effectively, especially written with the business and other technical groups\nWhat Were Looking For:\nBasic Qualifications:\nBachelorsMasters Degree in Computer Science, Information Systems or equivalent.\nMinimum 5 to 8 years of strong hand-development experience in C#, .Net Core, Cloud Native, MS SQL Server backend development. Proficiency with Object Oriented Programming.\nAdvance SQL programming skills\nPreferred experience or familiarity with tools and technologies such as Odata, Grafana, Kibana, Big Data platforms, Apache Kafka, GitHub, AWS EMR, Terraform, and emerging areas like AI/ML and GitHub Copilot.\nHighly recommended skillset in Databricks, SPARK, Scalatechnologies.\nUnderstanding of database performance tuning in large datasets\nAbility to manage multiple priorities efficiently and effectively within specific timeframes\nExcellent logical, analytical and communication skills are essential, with strong verbal and writing proficiencies\nKnowledge of Fundamentals, or financial industry highly preferred.\nExperience in conducting application design and code reviews\nProficiency with following technologies:\nObject-oriented programming\nPrograming Languages (C#, .Net Core)\nCloud Computing\nDatabase systems (SQL, MS SQL)\nNice to have: No-SQL (Databricks, Spark, Scala, python), Scripting (Bash, Scala, Perl, Powershell)\nPreferred Qualifications:\nHands-on experience with cloud computing platforms including AWS , Azure , or Google Cloud Platform (GCP) .\nProficient in working with Snowflake and Databricks for cloud-based data analytics and processing.\nBenefits:\nHealth & Wellness: Health care coverage designed for the mind and body.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GitHub Copilot', 'AI/ML', 'Kibana', 'python', 'GitHub', 'Scala', 'AWS EMR', 'Grafana', 'Odata', 'Big Data platforms', 'Terraform', 'Apache Kafka', 'Databricks', 'Spark']",2025-06-12 06:08:41
Solution Architect,Amgen Inc,9 - 14 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are seeking a seasoned Solution Architect to drive the architecture, development and implementation of data solutions to Amgen functional groups. The ideal candidate able to work in large scale Data Analytic initiatives, engage and work along with Business, Program Management, Data Engineering and Analytic Engineering teams. Be champions of enterprise data analytic strategy, data architecture blueprints and architectural guidelines. As a Solution Architect, you will play a crucial role in designing, building, and optimizing data solutions to Amgen functional groups such as R&D, Operations and GCO.\nRoles & Responsibilities:\nImplement and manage large scale data analytic solutions to Amgen functional groups that align with the Amgen Data strategy\nCollaborate with Business, Program Management, Data Engineering and Analytic Engineering teams to deliver data solutions\nResponsible for design, develop, optimize, delivery and support of Data solutions on AWS and Databricks architecture\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nProvide expert guidance and mentorship to the team members, fostering a culture of innovation and best practices.\nBe passionate and hands-on to quickly experiment with new data related technologies\nDefine guidelines, standards, strategies, security policies and change management policies to support the Enterprise Data platform.\nCollaborate and align with EARB, Cloud Infrastructure, Security and other technology leaders on Enterprise Data Architecture changes\nWork with different project and application groups to drive growth of the Enterprise Data Platform using effective written/verbal communication skills, and lead demos at different roadmap sessions\nOverall management of the Enterprise Data Platform on AWS environment to ensure that the service delivery is cost effective and business SLAs around uptime, performance and capacity are met\nEnsure scalability, reliability, and performance of data platforms by implementing best practices for architecture, cloud resource optimization, and system tuning.\nCollaboration with RunOps engineers to continuously increase our ability to push changes into production with as little manual overhead and as much speed as possible.\nMaintain knowledge of market trends and developments in data integration, data management and analytics software/tools\nWork as part of team in a SAFe Agile/Scrum model\nBasic Qualifications and Experience:\nMasters degree with 6 - 8 years of experience in Computer Science, IT or related field OR\nBachelors degree with 9 - 12 years of experience in Computer Science, IT or related field OR\nFunctional Skills:\nMust-Have Skills:\n7+ years of hands-on experience in Data integrations, Data Management and BI technology stack.\nStrong experience with one or more Data Management tools such as AWS data lake, Snowflake or Azure Data Fabric\nExpert-level proficiency with Databricks and experience in optimizing data pipelines and workflows in Databricks environments.\nStrong experience with Python, PySpark, and SQL for building scalable data workflows and pipelines.\nExperience with Apache Spark, Delta Lake, and other relevant technologies for large-scale data processing.\nFamiliarity with BI tools including Tableau and PowerBI\nDemonstrated ability to enhance cost-efficiency, scalability, and performance for data solutions\nStrong analytical and problem-solving skills to address complex data solutions\nGood-to-Have Skills:\nPreferred to have experience in life science or tech or consultative solution architecture roles\nExperience working with agile development methodologies such as Scaled Agile.\nProfessional Certifications\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'Data Engineering', 'PySpark', 'Tableau', 'SQL', 'Apache Spark', 'Enterprise Data platform', 'AWS data lake', 'Program Management', 'Databricks architecture', 'Azure Data Fabric', 'PowerBI', 'Snowflake', 'Delta Lake', 'Scaled Agile', 'AWS', 'Python']",2025-06-12 06:08:44
Oracle/ Informatica/ PLSQL/ ETL/ Snaplogic,Photon,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Job Description:\n\nRole: Orcale, Informatica, PLSQL, ETL\nLocation: Chennai/ Bangalore\nExperience: 5+ Years\nMust have: Orcale, Informatica, PLSQL, ETL\n\nLooking for a candidate with expertise on Oracle Database,  Snaplogic and Oracle PL/SQL with knowledge on AWS cloud.",,,,"['Snaplogic', 'PLSQL', 'Informatica', 'ETL', 'ORACE']",2025-06-12 06:08:46
Senior MSBI Developer (SQL & SSRS/SSIS Expert),Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027513\n\nJob Summary\nSynechron is seeking an experienced and detail-oriented Senior MSBI Developerexpertise in MSBI (Microsoft Business Intelligence) to join our data and analytics team. In this role, you will contribute to designing, developing, and maintaining robust reporting and data integration solutions that support our business objectives. Your expertise will help deliver actionable insights, improve decision-making processes, and enhance overall data management efficiency within the organization.\n\nSoftware\n\nRequired\n\nSkills:\nMSBI Suite (including SSIS, SSRS, SSAS)\nSQL Server (including SQL Server Management Studio and Query Performance Tuning)\nVersionsRecent versions of SQL Server (2016 or later preferred)\nProven experience in creating complex reports, data transformation, and integration workflows\nPreferred\n\nSkills:\nPower BI or other visualization tools\nExperience with cloud-based data solutions (e.g., Azure SQL, Synapse Analytics)\nOverall Responsibilities\nDevelop, implement, and maintain MSBI solutions such as SSIS packages, SSRS reports, and data models to meet business requirements\nCollaborate with business stakeholders and data teams to gather reporting needs and translate them into scalable solutions\nOptimize and troubleshoot existing reports and data pipelines to improve performance and reliability\nEnsure data accuracy, security, and compliance within reporting processes\nDocument solution architectures, workflows, and processes for ongoing support and knowledge sharing\nParticipate in team initiatives to enhance data governance and best practices\nContribute to strategic planning for data platform evolution and modernization\nTechnical Skills (By Category)\n\nProgramming Languages:\nRequiredSQL (Advanced proficiency in query writing, stored procedures, and performance tuning)\nPreferredT-SQL scripting for data transformations and automation\nDatabases / Data Management:\nRequiredDeep knowledge of relational database concepts with extensive experience in SQL Server databases\nPreferredFamiliarity with data warehouse concepts, OLAP cubes, and data mart design\nCloud Technologies:\nDesiredBasic understanding of cloud-based data platforms like Azure Data Factory, Azure Synapse\nFrameworks and Libraries:\nNot directly applicable, focus on MSBI tools\nDevelopment Tools and Methodologies:\nExperience working within Agile development environments\nData pipeline development and testing best practices\nSecurity Protocols:\nImplement data security measures, role-based access controls, and ensure compliance with data privacy policies\nExperience\n8 to 10 years of professional experience in software development with substantial hands-on MSBI expertise\nDemonstrated experience in designing and deploying enterprise-level BI solutions\nDomain experience in finance, healthcare, retail, or similar industries is preferred\nAlternative candidacyExtensive prior experience with BI tools and proven success in similar roles may be considered in lieu of exact industry background\nDay-to-Day Activities\nDesign and develop SSIS data integration workflows to automate data loading processes\nCreate and optimize SSRS reports and dashboards for various organizational units\nEngage in troubleshooting and resolving technical issues in existing BI solutions\nCollaborate with data architects, developers, and business analysts to align data solutions with business needs\nConduct code reviews, testing, and validation of reports and data pipelines\nParticipate in scrum meetings, planning sessions, and stakeholder discussions\nEnsure documentation of solutions, processes, and workflows for ease of maintenance and scalability\nQualifications\nBachelors degree or equivalent in Computer Science, Information Technology, or related field\nRelevant certifications in Microsoft BI or SQL Server (e.g., Microsoft Certified Data Engineer Associate) preferred\nOngoing engagement in professional development related to BI, data management, and analytics tools\nProfessional Competencies\nAnalytical mindset with strong problem-solving abilities in data solution development\nCapable of working collaboratively across diverse teams and communicating technical concepts effectively\nStakeholder management skills to interpret and prioritize reporting needs\nAdaptability to evolving technologies and continuous learning mindset\nFocus on delivering high-quality, sustainable data solutions with attention to detail\nEffective time management, prioritizing tasks to meet project deadlines",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['performance tuning', 'stored procedures', 'sql scripting', 'query writing', 'msbi', 'sql server database', 'software development', 'ssas', 'power bi', 'microsoft azure', 'data mart design', 'olap cubes', 'sql server', 'sql azure', 'ssrs', 'data warehousing concepts', 'data transformation', 'ssis']",2025-06-12 06:08:49
"Delivery Lead (.NET, C#, AWS, Agile & Cloud Solutions)",Synechron,12 - 15 years,Not Disclosed,"['Pune', 'Bengaluru', 'Hinjewadi']","job requisition idJR1027350\n\nJob Summary\nSynechron is seeking a experienced and strategic Delivery Lead to oversee complex technology projects utilizing .NET, C#, and AWS. This role is instrumental in managing end-to-end project delivery, guiding cross-functional teams, and ensuring alignment with business objectives. The Delivery Lead will drive improvements in delivery efficiency, quality, and stakeholder satisfaction, contributing significantly to the organizations technological growth and operational excellence.\n\nSoftware\n\nRequired\n\nSkills:\nDevelopment and delivery experience with .NET (preferably version 4.7 or later)\nC# programming proficiency\nHands-on experience with Amazon Web Services (AWS) (EC2, S3, Lambda, etc.)\nProject management toolsJira, SharePoint, MS Excel, PowerPoint, Power BI\nAgile and Waterfall project management methodologies\nPreferred\n\nSkills:\nExperience with DevOps/CI-CD pipelines\nKnowledge of Azure cloud platform\nFamiliarity with software release management\nOverall Responsibilities\nEnd-to-End Project Delivery: Manage multiple projects from initiation to closure, ensuring delivery is on time, within scope, and within budget.\nTeam Leadership: Lead and mentor diverse project teams, fostering collaboration and high performance.\nStakeholder Management: Act as the primary point of contact for clients and internal stakeholders, translating business needs into technical solutions.\nGovernance & Compliance: Ensure adherence to organizational policies, standards, and industry best practices.\nTechnical Oversight: Provide guidance on architecture, technology choices, and solution design aligned with best practices.\nProcess Optimization: Continuously identify opportunities to improve delivery processes, increase efficiency, and reduce risk.\nFinancial Oversight: Monitor project budgets, optimize resource utilization, and report on financial performance.\nRisk & Issue Management: Identify, assess, and mitigate risks impacting project delivery.\nPerformance Measurement: Establish metrics and KPIs to measure project success and customer satisfaction.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: C#, .NET Framework/Core\nPreferred: Java, Python (for integration or automation)\nDatabases/Data Management:\nSQL Server, AWS RDS\nCloud Technologies:\nAWS cloud services (EC2, S3, Lambda, CloudWatch)\nFrameworks & Libraries:\n.NET Core / .NET Framework\nRESTful APIs, Microservices architecture\nDevelopment Tools & Methodologies:\nAgile, Scrum, Kanban\nDevOps tools (Jenkins, Azure DevOps, Git)\nSecurity Protocols:\nAWS security best practices\nData privacy and compliance standards\nExperience\n12 to 15 years of professional experience in managing IT projects and delivery teams.\nDemonstrable experience leading large-scale software development and implementation projects.\nStrong background with .NET/C# development, AWS cloud solutions, and cross-functional team management.\nExperience managing global or distributed teams.\nProven stakeholder management experience with senior management and clients.\nPrior exposure to Agile and Waterfall project methodologies.\nAlternative Experience: Candidates with extensive experience in software delivery, cloud migration, or enterprise application implementation may be considered.\n\nDay-to-Day Activities\nConduct project planning, resource allocation, and status reporting.\nHold regular stand-ups, progress reviews, and stakeholder meetings.\nReview development progress, remove blockers, and ensure adherence to quality standards.\nCollaborate with technical teams on architecture design and problem resolution.\nManage change requests, scope adjustments, and project adjustments.\nTrack project KPIs, update dashboards, and communicate progress to leadership.\nOversee risk registers and implement mitigation strategies.\nFacilitate retrospectives and process improvement initiatives.\nQualifications\nBachelors degree in Computer Science, Engineering, or related field; Masters preferred.\nProject Management certifications such as PMP, PMI-ACP, or ScrumMaster are advantageous.\nTraining or certification in AWS or cloud architecture is preferred.\nCommitment to continuous learning and professional development.\nProfessional Competencies\nStrong analytical and problem-solving skills\nEffective leadership and team management capabilities\nExcellent stakeholder communication and negotiation skills\nAbility to adapt to evolving project requirements and technologies\nStrategic thinking and organizational agility\nData-driven decision-making\nPrioritization and time management skills\nChange management and process improvement orientation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud migration', 'c#', 'software development', 'team management', '.net', 'continuous integration', 'azure devops', 'sql', 'java', 'git', 'it projects', 'aws cloud', 'devops', 'waterfall', 'kanban', 'jenkins', 'jira', 'rest', 'python', 'software delivery', 'amazon ec2', 'lambda expressions', 'scrum', 'agile', 'aws']",2025-06-12 06:08:51
Senior ODI Developer (OCI PaaS/IaaS Expertise),Oracle,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","Role Overview:\nWe are seeking a highly skilled Senior ODI Developer with strong hands-on experience in SQL, PL/SQL, and Oracle Data Integrator (ODI) projects, particularly on OCI (Oracle Cloud Infrastructure) PaaS or IaaS platforms. The ideal candidate will design, implement, and optimize ETL processes, leveraging cloud-based solutions to meet evolving business needs. Prior experience in banking or insurance projects is a significant advantage.\nKey Responsibilities:\nDesign, develop, and deploy ETL processes using Oracle Data Integrator (ODI) on OCI PaaS/IaaS.\nConfigure and manage ODI instances on OCI, ensuring optimal performance and scalability.\nDevelop and optimize complex SQL and PL/SQL scripts for data extraction, transformation, and loading.\nImplement data integration solutions, connecting diverse data sources like cloud databases, on-premise systems, APIs, and flat files.\nMonitor and troubleshoot ODI jobs running on OCI to ensure seamless data flow and resolve any issues promptly.\nCollaborate with data architects and business analysts to understand integration requirements and deliver robust solutions.\nConduct performance tuning of ETL processes, SQL queries, and PL/SQL procedures.\nPrepare and maintain detailed technical documentation for developed solutions.\nAdhere to data security and compliance standards, particularly in cloud-based environments.\nProvide guidance and best practices for ODI and OCI-based data integration projects.\nSkills and Qualifications:\nMandatory Skills:\nStrong hands-on experience with Oracle Data Integrator (ODI) development and administration.\nProficiency in SQL and PL/SQL for complex data manipulation and query optimization.\nExperience deploying and managing ODI solutions on OCI PaaS/IaaS environments.\nDeep understanding of ETL processes, data warehousing concepts, and cloud data integration.\nPreferred Experience:\nHands-on experience in banking or insurance domain projects, with knowledge of domain-specific data structures.\nFamiliarity with OCI services like Autonomous Database, Object Storage, Compute, and Networking.\nExperience in integrating on-premise and cloud-based data sources.\nOther Skills:\nStrong problem-solving and debugging skills.\nExcellent communication and teamwork abilities.\nKnowledge of Agile methodologies and cloud-based DevOps practices.\nEducation and Experience:\nBachelors degree in computer science, Information Technology, or a related field.\n5 to 8 years of experience in ODI development, with at least 2 years of experience in OCI-based projects.\nDomain experience in banking or insurance is an added advantage.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Oracle Data Integrator', 'OCI', 'Data Integrator', 'Data Warehousing', 'ETL', 'SQL']",2025-06-12 06:08:54
MSD365 Finance Technical,Hexaware Technologies,6 - 11 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']",MS Dynamics365 Finance Technical\nHybrid\nPAN India\nRequired:\nIntegration and Data Management:\nDesign and implement integrations with other systems.\nData consistency and accuracy during data migration and transformation.,,,,"['D365', 'MSD365', 'MICROSOFT DYNAMICS', 'X++', 'F&O']",2025-06-12 06:08:56
MS Dynamics365 Finance Functional,Hexaware Technologies,6 - 11 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","MS Dynamics365 Finance Functional\nHybrid\nPAN India\n\nDescription:\nWe are looking for a highly skilled Microsoft Dynamics Functional Consultant to join our team.\nThe ideal candidate will have in-depth knowledge of Microsoft Dynamics 365 (Finance and Operations, Customer Engagement, or other modules) and a strong understanding of business processes.",,,,"['MSD365', 'MS dynamics 365', 'MSDynamics365', 'F&O']",2025-06-12 06:08:58
Lead .Net Developer,Conduent,5 - 9 years,Not Disclosed,['Bengaluru'],"Must have skills: \nHands on experience in design and development of Windows based applications using C#, .Net Core, WPF and WCF, Java, Python and React.js /Angular.\nUnderstanding of Apigee /API Gateway concepts for managing and securing APIs.\nExperience with SignalR for real-time communication in.NET applications.\nWorking knowledge of Kubernetes for container orchestration and managing microservices deployments.\nOutstanding analytical and problem-solving capabilities.\nDocumentation and Review of High-level and detailed design including Component diagrams and sequence diagrams.\nSound knowledge in data structures, OOAD & Design patterns\nHands on experience in Multithreading, Synchronization, and IPC.\nSound in design thinking and architectural level approach to the problems.\nGood analytical capability, sound reasoning and logic - demonstrated in code optimization, ability to debug multi-threaded applications.\nGood knowledge of programming tools, debugging tools and techniques, SCM tools and practices. Good Knowledge in Unit Testing frameworks like NUnit is required.\nSound knowledge of SDLC processes and demonstrated experience on complete end to end product design and roll-out. Having worked in development as well as maintenance projects with strict adherence to SLA norms.\nActively participated in review processes and provided meaningful feedback - at all phases of SDLC.\nGood Knowledge of Agile methodology and processes.\nStrong communication and presentation skills are a must. Having Customer interfacing experience would be helpful. High motivation, self-starter, and ability to take others along would be needed on the Job.\n\n Nice to have :\nTriage the software problems and determine the root cause for the issues report from production environment.\nDesign and implementation of Electronic Fare Payment, distribution, and processing systems which includes system hardware and software as well as back-office servers.\nDesigned and Development of AI based applications Cloud Computing Platforms such as Azure or Amazon Web Services (AWS),\nHands on experience in UART, MFC, Socket Programming, Object Oriented C, and Windows Driver Development Framework\nExposure to C++, STL and Pyhton would be an added advantage. WinCE (5.0/6.0/EC) would be a definite plus.\nExposure to Embedded and/or High availability systems, application development for the same, would be a definite plus.\nInterfacing between C++ & C# (native & managed code); knowledge of IDL/COM/CORBA would be highly desirable.\nHands on experience in working devices and device drivers, writings apps that interact with these external devices, serial port communication",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['design patterns', 'data structures', 'presentation skills', 'product design', 'ooad', 'kubernetes', 'c++', 'api gateway', 'socket programming', 'unit testing', 'apigee', 'react.js', 'java', 'wcf', 'sdlc process', 'c#', 'python', 'microsoft azure', 'stl', 'mfc', 'wpf', 'angular', '.net core', 'nunit', 'aws', 'sdlc']",2025-06-12 06:09:01
Sap Hybris Architect,Pepplo Consulting,14 - 20 years,40-45 Lacs P.A.,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Minimum 12 years of experience in IT, with proven expertise in at least 2 large-scale projects (More than 10 lakh users for a B2C application) involving system design & deployment of enterprise architecture, data architecture & cloud implementations.\n\nRequired Candidate profile\nKnowledge on Hybris ORM/WCMS/Backoffice/Cockpits/SOLR search engine\nExpert in Hybris B2B/B2C Accelerators, Hybris Workflow/Task Mgmt\nExpert in the catalog/order management/promotions/vouchers/coupons",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spring Mvc', 'Sap Hybris Commerce', 'Javascript', 'Html/Css', 'GIT', 'JSP', 'J2Ee', 'SVN', 'JQuery']",2025-06-12 06:09:03
Senior Sap Fico Consultant,IT service and consulting,6 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","We are actively looking out for the candidate having 6- years of experience into SAP FICO\nJob Title - Senior SAP FICO Consultant\nLocation - Bangalore / Pune/ Hyderabad\nWork mode - Hybrid\nNotice period - 15-30 days / Serving notice period\n\nRequired Skills :-\nSAP FI (Finance):\nGeneral Ledger (GL), Accounts Payable (AP), Accounts Receivable (AR), Asset Accounting (AA)\nBank Accounting, Electronic Bank Statement (EBS)\nIntegration with MM and SD\nTax configuration (GST/VAT, Withholding Tax, etc.)\nAutomatic payment program (APP)\nDocument Splitting, Parallel Ledger, Special Purpose Ledger\nSAP CO (Controlling):\nCost Center Accounting (CCA)\nInternal Orders\nProfit Center Accounting (PCA)\nProduct Costing, Profitability Analysis (COPA)\nActivity-Based Costing (ABC)\nIntegration with FI, PP, and SD\nTechnical/Functional Skills:\nStrong SAP FICO configuration and process design experience\nPreparation of Functional Specification Documents (FSDs)\nUnderstanding of ABAP debugging and basic technical design\nExperience with SAP S/4HANA is highly preferred (especially Universal Journal, New Asset Accounting, etc.)\nKnowledge of SAP Fiori apps relevant to finance\nExperience in LSMW, BAPIs, BDCs, and other data migration tools\nWorking knowledge of IDocs, interfaces, and third-party integrations\nGood understanding of authorization concepts and roles in FICO",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP FICO Implementation', 'Sap Configuration', 'SAP Implementation', 'IDOCS', 'mm', 'Sap S Hana', 'gst', 'Profit Center Accounting', 'Bdcs', 'ap', 'ar', 'sd', 'LSMW', 'FSD', 'General Ledger', 'Internal Orders', 'BAPIs', 'Copa', 'Cost Center Accounting', 'Activity Based Costing']",2025-06-12 06:09:05
Womens walkin- Automation testing/ETL/UX on 13th June @Pune,Infosys BPM,2 - 4 years,Not Disclosed,['Pune'],"Greeting from Infosys BPM Ltd,\n\nExclusive Women's Walkin drive\n\nWe are hiring for UX with JavaScript, ETL Testing + Python Programming, Automation Testing with Java, Selenium, BDD, Cucumber, ETL DB Testing, ETL Testing Automation skills. Please walk-in for interview on 13th June 2025 at Pune location\n\n\nNote: Please carry copy of this email to the venue and make sure you register your application before attending the walk-in. Please use below link to apply and register your application. Please mention Candidate ID on top of the Resume ***\n\n\nhttps://career.infosys.com/jobdesc?jobReferenceCode=PROGEN-HRODIRECT-215163\n\n\nInterview details\nInterview Date: 13th June 2025\nInterview Time: 10 AM till 1 PM\n\n\nInterview Venue:\nPune:: Hinjewadi Phase 1\nInfosys BPM Limited, Plot No. 1, Building B1, Ground floor, Hinjewadi Rajiv Gandhi Infotech Park, Hinjewadi Phase 1, Pune, Maharashtra-411057\n\n\nPlease find below Job Description for your reference:\n\n\nWork from Office***\nMin 2 years of experience on project is mandate***\nJob Description: UX with JavaScript\nTechnical Design tools (e.g., Photoshop, XD, Figma), strong knowledge of HTML, CSS, JavaScript, and experience with SharePoint customization.\nExperience with Wireframe, Prototype, intuitive, responsive design, Documentation,\nAble to Lead the Team\nNice to have Understanding of SharePoint Framework (SPFx) and modern SharePoint development.\n\nJob Description: ETL Testing + Python Programming\nExperience in Data Migration Testing (ETL Testing), Manual & Automation with Python Programming. Strong on writing complex SQLs for data migration validations.\nWork experience with Agile Scrum Methodology\nFunctional Testing- UI Test Automation using Selenium, Java\nFinancial domain experience\nGood to have AWS knowledge\n\nJob Description: Automation Testing with Java, Selenium, BDD, Cucumber\nHands on exp in Automation.\nJava, Selenium, BDD , Cucumber expertise is mandatory.\nBanking Domian Experience is good.\nFinancial domain experience\nAutomation Talent with TOSCA skills, Payment domain skills is preferable.\n\nJob Description: ETL DB Testing\nStrong experience in ETL testing, data warehousing, and business intelligence.\nStrong proficiency in SQL.\nExperience with ETL tools (e.g., Informatica, Talend, AWS Glue, Azure Data Factory).\nSolid understanding of Data Warehousing concepts, Database Systems and Quality Assurance.\nExperience with test planning, test case development, and test execution.\nExperience writing complex SQL Queries and using SQL tools is a must, exposure to various data analytical functions.\nFamiliarity with defect tracking tools (e.g., Jira).\nExperience with cloud platforms like AWS, Azure, or GCP is a plus.\nExperience with Python or other scripting languages for test automation is a plus.\nExperience with data quality tools is a plus.\nExperience in testing of large datasets.\nExperience in agile development is must\nUnderstanding of Oracle Database and UNIX/VMC systems is a must\n\nJob Description: ETL Testing Automation\nStrong experience in ETL testing and automation.\nStrong proficiency in SQL and experience with relational databases (e.g., Oracle, MySQL, PostgreSQL, SQL Server).\nExperience with ETL tools and technologies (e.g., Informatica, Talend, DataStage, Apache Spark).\nHands-on experience in developing and maintaining test automation frameworks.\nProficiency in at least one programming language (e.g., Python, Java).\nExperience with test automation tools (e.g., Selenium, PyTest, JUnit).\nStrong understanding of data warehousing concepts and methodologies.\nExperience with CI/CD pipelines and version control systems (e.g., Git).\nExperience with cloud-based data warehouses like Snowflake, Redshift, BigQuery is a plus.\nExperience with data quality tools is a plus.\n\n\n\nREGISTRATION PROCESS:\nThe Candidate ID & SHL Test(AMCAT ID) is mandatory to attend the interview. Please follow the below instructions to successfully complete the registration. (Talents without registration & assessment will not be allowed for the Interview).\n\nCandidate ID Registration process:\nSTEP 1: Visit: https://career.infosys.com/joblist\nSTEP 2: Click on ""Register"" and provide the required details and submit.\nSTEP 3: Once submitted, Your Candidate ID(100XXXXXXXX) will be generated.\nSTEP 4: The candidate ID will be shared to the registered Email ID.\n\nSHL Test(AMCAT ID) Registration process:\nThis assessment is proctored, and talent gets evaluated on Basic analytics, English Comprehension and writex (email writing).\n\nSTEP 1: Visit: https://apc01.safelinks.protection.outlook.com/?url=https://autologin-talentcentral.shl.com/?link=https://amcatglobal.aspiringminds.com/?data=JTdCJTIybG9naW4lMjIlM0ElN0IlMjJsYW5ndWFnZSUyMiUzQSUyMmVuLVVTJTIyJTJDJTIyaXNBdXRvbG9naW4lMjIlM0ExJTJDJTIycGFydG5lcklkJTIyJTNBJTIyNDE4MjQlMjIlMkMlMjJhdXRoa2V5JTIyJTNBJTIyWm1abFpUazFPV1JsTnpJeU1HVTFObU5qWWpRNU5HWTFOVEU1Wm1JeE16TSUzRCUyMiUyQyUyMnVzZXJuYW1lJTIyJTNBJTIydXNlcm5hbWVfc3E5QmgxSWI5NEVmQkkzN2UlMjIlMkMlMjJwYXNzd29yZCUyMiUzQSUyMnBhc3N3b3JkJTIyJTJDJTIycmV0dXJuVXJsJTIyJTNBJTIyJTIyJTdEJTJDJTIycmVnaW9uJTIyJTNBJTIyVVMlMjIlN0Q=&apn=com.shl.talentcentral&ibi=com.shl.talentcentral&isi=1551117793&efr=1&data=05|02|omar.muqtar@infosys.com|a7ffe71a4fe4404f3dac08dca01c0bb3|63ce7d592f3e42cda8ccbe764cff5eb6|0|0|638561289526257677|Unknown|TWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0=|0|||&sdata=s28G3ArC9nR5S7J4j/V1ZujEnmYCbysbYke41r5svPw=&reserved=0\n\nSTEP 2: Click on ""Start new test"" and follow the instructions to complete the assessment.\nSTEP 3: Once completed, please make a note of the AMCAT ID( Access you Amcat id by clicking 3 dots on top right corner of screen).\n\nNOTE:\nDuring registration, you'll be asked to provide the following information:\nPersonal Details: Name, Email Address, Mobile Number, PAN number.\nAvailability: Acknowledgement of work schedule preferences (Shifts, Work from Office, Rotational Weekends, 24/7 availability, Transport Boundary) and reason for career change.\nEmployment Details: Current notice period and total annual compensation (CTC) in the format 390000 - 4 LPA (example).\nCandidate Information: 10-digit candidate ID starting with 100XXXXXXX, Gender, Source (e.g., Vendor name, Naukri/LinkedIn/Found it, or Direct), and Location\nInterview Mode: Walk-in\nAttempt all questions in the SHL Assessment app.\nThe assessment is proctored, so choose a quiet environment.\nUse a headset or Bluetooth headphones for clear communication.\nA passing score is required for further interview rounds.\n5 or above toggles, multi face detected, face not detected, or any malpractice will be considered rejected\nOnce you've finished, submit the assessment and make a note of the AMCAT ID (15 Digit) used for the assessment.\n\nDocuments to Carry:\nPlease have a note of Candidate ID & AMCAT ID along with registered Email ID.\nPlease do not carry laptops/cameras to the venue as these will not be allowed due to security restrictions.\nPlease carry 2 set of updated Resume/CV (Hard Copy).\nPlease carry original ID proof for security clearance.\nPlease carry individual headphone/Bluetooth for the interview.\n\nPointers to note:\nPlease do not carry laptops/cameras to the venue as these will not be allowed due to security restrictions.\nOriginal Government ID card is must for Security Clearance.\n\nRegards,\nInfosys BPM Recruitment team.",Industry Type: BPM / BPO,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['ETL DB', 'UX', 'Automation Testing', 'ETL', 'ETL Testing', 'Javascript', 'Python']",2025-06-12 06:09:08
Transition Analyst,Capco,5 - 7 years,8-10 Lacs P.A.,['Pune'],"About Us\nCapco, a Wipro company, is a global technology and management consulting firm. Awarded with Consultancy of the year in the British Bank Award and has been ranked Top 100 Best Companies for Women in India 2022 by Avtar & Seramount. With our presence across 32 cities across globe, we support 100+ clients across banking, financial and Energy sectors. We are recognized for our deep transformation execution and delivery.\nWHY JOIN CAPCO?\nYou will work on engaging projects with the largest international and local banks, insurance companies, payment service providers and other key players in the industry. The projects that will transform the financial services industry.\nMAKE AN IMPACT\nInnovative thinking, delivery excellence and thought leadership to help our clients transform their business. Together with our clients and industry partners, we deliver disruptive work that is changing energy and financial services.\n#BEYOURSELFATWORK\nCapco has a tolerant, open culture that values diversity, inclusivity, and creativity.\nCAREER ADVANCEMENT\nWith no forced hierarchy at Capco, everyone has the opportunity to grow as we grow, taking their career into their own hands.\nDIVERSITY & INCLUSION\nWe believe that diversity of people and perspective gives us a competitive advantage.\nMAKE AN IMPACT\nJob Title: Transition Analyst\nLocation: Pune\nExperience Required:\nBachelor’s degree is required.\nMinimum 6 to 8 years of total work experience.\nMinimum 1 to 2 years of relevant experience in Project/Program Management or Support roles.\nTechnical & Functional Expertise:\nTechnical:\nProficiency in MS Office products including Office 365, Project Online, SharePoint, Power BI, and other analytics tools.\nStrong understanding of process workflow design, data architecture, and related tools.\nFunctional:\nStrong business acumen and functional understanding.\nExperience in planning and monitoring for program workstreams, project deliverables, and reporting.\nAbility to handle transition-related documentation, administrative tasks, risk management, due diligence, and stakeholder coordination.\nKey Responsibilities:\nSupport planning and execution of program and transition projects.\nTrack deliverables, manage risks, and ensure timely reporting.\nEnsure compliance with GBS methodologies and toolkits.\nManage travel and logistics for transition-related requirements.\nCoordinate with operational teams and business functions for successful transitions.\nLead the documentation of SOPs and manage sign-off processes.\nCollaborate with various business units including Procurement, Finance, and IT.\nSupport project reporting, dashboard preparation, and Power BI-based analytics.\nHandle highly confidential material with discretion and professionalism.\nParticipate in customer-facing meetings and internal stakeholder communications.\nFacilitate workshops, team meetings, and process improvement initiatives.\nKey Challenges:\nNavigating fragmented systems and tools.\nEngaging a wide range of stakeholders across global functions.\nManaging services at a large scale with geographical and cultural diversity.\nAdapting to evolving digital technologies and technical tools.\nEnsuring alignment with global process design standards.\nSkills & Competencies:\nCore Skills:\nProject planning and reporting skills\nWorkflow and process documentation\nRisk identification and mitigation\nData visualization and reporting tools (especially Power BI)\nSoft Skills:\nExcellent multitasking and prioritization skills\nStrong interpersonal, presentation, and written communication skills\nFluency in English (spoken and written)\nKnowledge of local regulations and compliance standards\nFamiliarity with Pune’s local business environment\nAbility to work effectively in a regional service center ecosystem",Industry Type: BPM / BPO,Department: Consulting,"Employment Type: Full Time, Permanent","['Project Management', 'Transition Management', 'Process Transition', 'Process Migration', 'Transition Planning', 'Business Transition', 'Project Governance', 'Project Transition']",2025-06-12 06:09:11
Associate Director-Oracle Techno-Functional,Acuity Knowledge Partners,10 - 17 years,Not Disclosed,['Gurugram'],"Acuity Knowledge Partners\nAcuity Knowledge Partners (Acuity) is a leading provider of bespoke research, analytics and technology solutions to the financial services sector, including asset managers, corporate and investment banks, private equity and venture capital firms, hedge funds and consulting firms. Its global network of over 6,000 analysts and industry experts, combined with proprietary technology, supports more than 600 financial institutions and consulting companies to operate more efficiently and unlock their human capital, driving revenue higher and transforming operations. Acuity is headquartered in London and operates from 10 locations worldwide.",,,,"['Oracle Cloud', 'Oracle Fusion', 'Oracle Finance', 'Oracle Fusion Technical', 'Oracle Cloud Applications', 'Erp Cloud', 'Oic', 'Fusion Financials', 'Oracle Fusion Financials', 'Oracle ERP', 'Oracle Peoplesoft Financials']",2025-06-12 06:09:13
Oracle EBS /Cloud/ Fusion SCM Consultant,Infosys,2 - 5 years,Not Disclosed,['Pune'],"Job Title\nOracle EBS /Cloud/ Fusion SCM Consultant\n\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys consulting team, your primary role would be to actively aid the consulting team in different phases of the project including problem definition, effort estimation, diagnosis, solution generation and design and deployment\nYou will explore the alternatives to the recommended solutions based on research that includes literature surveys, information available in public domains, vendor evaluation information, etc. and build POCs\nYou will create requirement specifications from the business needs, define the to-be-processes and detailed functional designs based on requirements.\nYou will support configuring solution requirements on the products; understand if any issues, diagnose the root-cause of such issues, seek clarifications, and then identify and shortlist solution alternatives\nYou will also contribute to unit-level and organizational initiatives with an objective of providing high quality value adding solutions to customers. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! Technical and Professional :\nMinimum 2 years of implementation experience with Oracle EBS/Cloud/Fusion in Order Management, Procurement, Inventory, Bill of Material, Manufacturing modules\nHave at least 1 full life cycle implementations experience, with hands-on configuration, implementation, and support of Oracle EBS/Cloud/Fusion SCM\nResponsible for leading the requirements elicitation, fit-gap analysis, development, configuration, functional testing, and post-production support\nHave experience with data migration using FBDI\nStrong experience in gathering requirements, designing solutions for Very High transaction volumes and should have good experience of Performance Testing of solutions\nShould have experience of designing and delivering complex custom solutions in highly integrated applications landscape\nExperience in handling integration with external partners/ applications like E-Commerce Portals, Part Catalogs, trading partners - Suppliers & Customers, EDI Preferred Skills:\nTechnology-Oracle eBS Functional-Oracle Order Management Technology-Oracle Cloud-Oracle Planning Cloud Additional Responsibilities:\nAbility to work with clients to identify business challenges and contribute to client deliverables by refining, analyzing, and structuring relevant data\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate\nAbility to assess the current processes, identify improvement areas and suggest the technology solutions\nOne or two industry domain knowledge Educational Master Of Commerce,Master Of Engineering,Master Of Science,Master Of Technology,Master of Business Administration,Bachelor Of Commerce,Bachelor Of Science,Bachelor of Engineering,Bachelor Of Technology Service LineEnterprise Package Application Services* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle EBS', 'solution design', 'order management', 'procurement', 'Cloud', 'oracle e-business suite', 'Fusion SCM Consultant']",2025-06-12 06:09:15
Senior Cloud Engineer,Infrassist Technologies Pvt. Ltd.,2 - 7 years,1-6 Lacs P.A.,['Ahmedabad'],"Number of open positions 2\n\nTitle: Cloud Engineer or Sr. Cloud Engineer\n\nExperience: 3-5 years relevant work experience\n\nQualification, Experience & Skills Required\n\nGood to have Professional Certification in Domain (1. Microsoft 365 Certified: Administrator Expert / Endpoint Administrator 2. Azure Administrator)\nExcellent oral & written communication skills Ready to work in morning & afternoon shifts along with occasional night shifts (if required).\nThe purpose of the role is to be the main customer facing point of contact for all Microsoft Office 365 & Azure Cloud projects implementation & consulting work.\nAbility to work intuitively, along or as part of a team environment, Self-starter with ability to plan and execute work.\nProven hands-on experience in Implementing, Configuring, Testing, Migrating & Securing Microsoft 365 & Azure Cloud Solutions.\nMicrosoft Windows Active Directory, Azure AD Connect Azure AD Premium Features deployment mainly User & Device based Conditional Access, Multi-Factor Authentication Policies.\nEmail & data Migrations from different source environments to Office 365 (Exchange & SharePoint Online, Microsoft Teams & One Drive for Business respectively) using Microsoft methods, tools and/or 3rd party migration solutions bit titan, SkyKick, AvePoint, Sharegate etc.\nMicrosoft Mobile Device Management (Intune MDM & MAM) Solutions, Windows Autopilot Technologies",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Azure Certified', 'Cloud Administration', 'microsoft 365', 'Endpoint', 'Azure Virtual Desktop', 'Entra Id', 'Microsoft Certified', 'Sharegate', 'Networking', 'Migration', 'avepoint', 'Intune', 'Autopilot', 'microsoft mobile device management', 'Security', 'Active Directory', 'MDM', 'Mam', 'skykick', 'Bittitan']",2025-06-12 06:09:18
Senior Devops Engineer,Epam Systems,5 - 8 years,18-22.5 Lacs P.A.,"['Chennai', 'Coimbatore']","Proven experience as a DevOps Engineer, with a focus on designing and implementing cloud-based infrastructure.\nStrong expertise in cloud platforms such as Amazon Web Services (AWS)\nExperience with infrastructure-as-code tools such as Terraform.\nProficiency in scripting languages (e.g., Python/Shell) for automation and infrastructure management.\nDeep understanding of CI/CD concepts and experience with CI/CD tools such as Jenkins, GitLab CI/CD\nFamiliarity with containerization and orchestration technologies like Docker and Kubernetes.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Jenkins', 'Gitlab', 'Terraform', 'Docker', 'Bash Scripting', 'Shell Scripting', 'Ansible', 'Groovy', 'Kubernetes', 'Python']",2025-06-12 06:09:20
Senior Software Engineer-7916,WebMD,5 - 10 years,Not Disclosed,['Navi Mumbai'],"Position: Senior Software Engineer (Data Engineer)\nNo. of Positions: 1\nAbout WebMD:\nHeadquartered in El Segundo, Calif., Internet Brands is a fully integrated online media and software services\norganization focused on four high-value vertical categories: Health, Automotive, Legal, and Home/Travel. The\ncompanys award-winning consumer websites lead their categories and serve more than 250 million monthly\nvisitors, while a full range of web presence offerings has established deep, long-term relationships with SMB and\nenterprise clients. Internet Brands,powerful, proprietary operating platform provides the flexibility and\nscalability to fuel the companys continued growth. Internet Brands is a portfolio company of KKR and Temasek.\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health information services, serving\npatients, physicians, health care professionals, employers, and health plans through our public and private online\nportals, mobile platforms, and health-focused publications. The WebMD Health Network includes WebMD\nHealth, Medscape, Jobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals\nConsumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape Education, and other owned\nWebMD sites. WebMD, Medscape, CME Circle, Medpulse, eMedicine®, MedicineNet®, theheart.org®, and\nRxList® are among the trademarks of WebMD Health Corp. or its subsidiaries.\nFor Company details, visit our website: www.webmd.com / www.internetbrands.com\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex,\nsexual orientation, gender identity, national origin, disability, or veteran status\nEducation: B.E. Computer Science/IT degree (or any other engineering discipline)\nExperience: 5+ years\nWork timings: 2:00 PM to 11:00 PM IST.\nDescription:\nWe are seeking an experienced and passionate Senior Software Developer to join our team. In this role,\nyou will work closely with cross-functional teams, developers, stakeholders, and business units to\ngather and analyze business requirements, design, build and implement ETL Solutions, and maintain\nthe infrastructure. The ideal candidate will have a strong background in business analysis, SQL, Unix,\nPython, ETL Tools to ensure the successful execution of projects.\nResponsibilities:\nLead requirements gathering sessions with key stakeholders to understand business needs and\nobjectives.\nCollaborate with and across Agile teams to design, develop, test, implement and support ETL\nprocesses for data transformation and preparation.\nManage data pipelines for analytics and operational use.\nEnsure data quality, data accuracy and integrity across multiple sources and systems.\nPerform unit tests and conduct reviews with other team members to make sure your code is\nrigorously designed, elegantly coded, and effectively tuned for performance.\nShould be able to come up with multiple approaches to any ETL\nproblem statement/solution/technical challenge and take well informed decision to pick the\nbest solution.\nAutomate ETL Processes using Cron and/or using Job Scheduler tools like AirFlow.\nAdhere to company standards and Serve as a key contributor to the design and development of\nexception handling, code/data standardization procedures, resolution steps and Quality Assurance\ncontrols.\nMaintain a version repository and ensure version control.\nCreate visual aids such as diagrams, charts, and screenshots to enhance documentation.\nWork with Infrastructure/systems team and developers to ensure all modules are up-to- date and\nare compatible with the code.",,,,"['ETL', 'SQL', 'UNIX', 'Python']",2025-06-12 06:09:23
"R&D Member of Technical Staff II, Product Development",Aveva,2 - 6 years,Not Disclosed,['Hyderabad'],"job requisition idR010044\n\nAVEVA is creating software trusted by over 90% of leading industrial companies.\n\nJob TitleR&D Member of Technical Staff II, Product Development\n\nLocationHyderabad\n\nEmployment Type Full-time\n\nBenefits Gratuity, Medical and accidental insurance, very attractive leave entitlement, emergency leave days, childcare support, maternity, paternity and adoption leaves, education assistance program, home office set up support (for hybrid roles), well-being support\n\nThe job\n\nAs a part of this function, the Cloud Unified Engineering team of R&D Technology and Execution Development, the development engineer will be responsible for ensuring the high quality of AVEVA Software Product deliveries to customers on the CONNECT cloud platform.\n\nKey responsibilities\n\nWe are looking for a Developer with skills to design and develop required functionality in the Cloud Unified Engineering solution / platform. This will include implement new user stories, write unit tests and fixing the reported defects by system test, DevOps teams.\n\nAlso, proactively identify improvements and enhancements to existing unit test cases test suites. This role reports into Development manager located in Hyderabad, India.\n\nEssential requirements\n\nDevelopment experience with exposure to programming skills, e.g. C#, .NET.\nProven experience on Amazon Web Services (AWS)Desired skills and competenciesKnowledge of PowerShell or Node JS scripting.Knowledge of AWS CloudFormation and Infrastructure as Code (IaC).\nKnowledge of API, REST, microservices and serverless architecture\nKnowledge and experience of operational support, software development and deployment methodologies and principles.\nHands-on in AWS administration, AWS APIs and tools or equivalent Azure experience.\nStrong written, verbal and presentation skills, able to convey information clearly and concisely to technical and non-technical audiences.R&D at AVEVA Our global team of 2000+ developers work on an incredibly diverse portfolio of over 75 industrial automation and engineering products, which cover everything from data management to 3D design. AI and cloud are at the centre of our strategy, and we have over 150 patents to our name.Our track record of innovation is no fluke its the result of a structured and deliberate focus on learning, collaboration and inclusivity. If you want to build applications that solve big problems, join us.Find out moreaveva.com/en/about/careers/r-and-d-careers/ India Benefits include:Gratuity, Medical and accidental insurance, very attractive leave entitlement, emergency leave days, childcare support, maternity, paternity and adoption leaves, education assistance program, home office set up support (for hybrid roles), well-being support Its possible were hiring for this position in multiple countries, in which case the above benefits apply to the primary location. Specific benefits vary by country, but our packages are similarly comprehensive.Find out moreaveva.com/en/about/careers/benefits/ Hybrid workingBy default, employees are expected to be in their local AVEVA office three days a week, but some positions are fully office-based. Roles supporting particular customers or markets are sometimes remote.Hiring processInterestedGreat! Get started by submitting your cover letter and CV through our application portal. AVEVA is committed to recruiting and retaining people with disabilities. Please let us know in advance if you need reasonable support during your application process.Find out moreaveva.com/en/about/careers/hiring-process",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['presentation skills', 'aws administration', 'microsoft azure', 'aws cloudformation', 'aws', 'c#', 'rest', 'software development', 'web services', 'artificial intelligence', 'microservices', 'node.js', 'devops', 'powershell', 'product development', '.net', 'api', 'product engineering']",2025-06-12 06:09:26
Java + Springboot Lead,Infosys,5 - 9 years,Not Disclosed,['Chennai'],"Job Title\nJava + Springboot Lead\n\nResponsibilities\nLead and mentor a team of Java & Springboot Developers in the design, development, and maintenance of applications.Work with business stakeholders and technical teams to gather and analyze requirements for Java & Springboot applications.Design, develop, and enhance software solutions using Java & Springboot, including Microservices, MVC, Spring Data, and Spring Security.Write efficient and well-structured code to implement business logic and functionality on the Java platform.Perform unit testing and debugging to ensure the quality and reliability of developed applications.Maintain and enhance existing Java & Springboot applications by troubleshooting issues, implementing bug fixes, and optimizing performance.Collaborate with other developers, database administrators, and system administrators to integrate Java & Springboot applications with other systems and databases.Develop and maintain technical documentation, including system design, coding standards, and user manuals.Stay updated with the latest Java & Springboot technologies and industry trends, and recommend improvements or alternative solutions to enhance system performance and efficiency.Collaborate with cross-functional teams to support system integration, data migration, and software deployment activities.Participate in code reviews and provide constructive feedback to ensure adherence to coding standards and best practices.Proactively identify and address potential risks or issues related to Java & Springboot applications and propose appropriate solutions.Provide leadership and guidance to the team and create a positive and productive work environment.Manage the team's workload and ensure that projects are completed on time and within budget.Delegate tasks and responsibilities to team members and provide regular feedback.Identify and develop the team's strengths and weaknesses and provide opportunities for professional growth.\n\nTechnical and Professional :\nPrimary skills:Technology-Java-Springboot\nPreferred Skills:\nTechnology-Java-Springboot Additional Responsibilities:Bachelor's degree in Computer Science, Information Technology, or a related field.Minimum of 5 years of experience as a Java & Springboot Developer, with at least 3 years of team handling experienceStrong understanding of Java programming concepts, including object-oriented programming, data structures, and algorithms.Proficiency in Springboot framework, including Microservices, MVC, Spring Data, and Spring Security.Extensive experience with Java development tools, such as Eclipse and IntelliJ IDEA.Deep familiarity with relational databases, particularly MySQL and PostgreSQL.Expert knowledge of Java performance tuning and optimization techniques.Excellent problem-solving and analytical skills.Strong written and verbal communication skills, with the ability to effectively communicate technical concepts to both technical and non-technical stakeholders.Detail-oriented with a commitment to delivering high-quality software solutions.Proven ability to lead and mentor a team of developers.Leadership and management skills Educational Master Of Comp. Applications,Bachelor Of Comp. Applications,Bachelor of Engineering,Bachelor Of Technology Service LineApplication Development and Maintenance* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'postgresql', 'performance tuning', 'java development', 'mysql', 'spring boot framework', 'relational databases']",2025-06-12 06:09:28
Solution Design Lead & implimantation,Excellerate Global Solutions,13 - 23 years,10-20 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Solution Design & Implementation:\nLead and participate in the full project lifecycle of SAP S/4HANA Public Cloud implementations, focusing on Procurement (Sourcing & Procurement/MM), Finance (FI/CO), and Sales & Distribution (SD) modules.\nConduct in-depth business process analysis, gather requirements, and translate them into robust and scalable SAP S/4HANA Public Cloud solutions aligned with SAP best practices.\nDesign, configure, and customize SAP S/4HANA Public Cloud functionalities for Order-to-Cash (O2C), Procure-to-Pay (P2P), Record-to-Report (R2R), and other relevant cross-functional processes.\nEnsure seamless integration between Procurement, Finance, and SD modules, as well as with other SAP Cloud modules (e.g., EWM, PP) and third-party applications where applicable.\nLeverage SAP Activate methodology for project delivery, guiding clients through fit-to-standard workshops and solution design.\nFunctional Expertise:\nProcurement (MM): Expertise in Material Master, Vendor Master, Purchase Requisitions, Purchase Orders, Contracts, Sourcing, Inventory Management, Invoice Verification, and supplier collaboration.\nFinance (FI/CO): Strong knowledge of General Ledger, Accounts Payable, Accounts Receivable, Asset Accounting, Bank Accounting, Cost Center Accounting, Profit Center Accounting, Internal Orders, Product Costing, Profitability Analysis (CO-PA), and treasury functions. Understanding of the Universal Journal (ACDOCA) and its impact.\nSales & Distribution (SD): Proficiency in Sales Order Management, Pricing, Delivery Processing, Billing, Credit Management, Returns Management, and ATP (Available-to-Promise).\nTechnical Acumen (Public Cloud Specific):\nUnderstanding of SAP S/4HANA Public Cloud architecture, standard scope, extensibility options (e.g., in-app extensibility, side-by-side extensions using SAP BTP).\nFamiliarity with SAP Fiori applications and user interfaces for relevant modules.\nKnowledge of data migration strategies and tools within the Public Cloud environment (e.g., Migration Cockpit).\nExperience with SAP Cloud ALM for implementation, operations, and monitoring.\nClient Engagement & Leadership:\nAct as a trusted advisor to clients, effectively communicating complex technical and functional concepts to both business and IT stakeholders.\nLead workshops, facilitate discussions, and drive decisions throughout the project lifecycle.\nProvide expert guidance on cloud transformation strategies, change management, and user adoption.\nMentor and guide junior consultants, fostering a culture of knowledge sharing and continuous improvement.\nTesting, Training & Support:\nDevelop and execute comprehensive test plans (unit, integration, UAT) to ensure the solution meets business requirements and is defect-free.\nPrepare detailed training materials and conduct engaging training sessions for end-users.\nProvide post-implementation support, troubleshoot issues, and drive resolution in collaboration with technical teams.\nContinuous Improvement & Innovation:\nStay updated with the latest SAP S/4HANA Public Cloud releases, functionalities, and industry best practices.\nIdentify opportunities for process optimization and leverage new SAP innovations (e.g., AI, Machine Learning capabilities within S/4HANA) to enhance client value.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Sap Hana', 'Solution Design', 'SAP FICO', 'SAP SD', 'SAP MM', 'SAP Finance']",2025-06-12 06:09:31
"Spark, Java, Kafka- Hyderabad",Cognizant,12 - 15 years,Not Disclosed,['Hyderabad'],"Skill: Java, Spark, Kafka\nExperience: 10 to 16 years\nLocation: Hyderabad\n As Data Engineer, you will :\n       Support in designing and rolling out the data architecture and infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources\n       Identify data source, design  and implement data schema/models and integrate data that meet the requirements of the business stakeholders",,,,"['hive', 'cloudera', 'modeling', 'scala', 'data warehousing', 'apache pig', 'data pipeline', 'data architecture', 'scalability', 'sql', 'java', 'data modeling', 'spark', 'mysql', 'hadoop', 'etl', 'big data', 'hbase', 'python', 'oozie', 'data processing', 'airflow', 'elt', 'data engineering', 'nosql', 'mapreduce', 'kafka', 'feasibility analysis', 'hdfs', 'sqoop', 'aws']",2025-06-12 06:09:35
MD365 Finance Functional Consultant,Protiviti India,3 - 8 years,6-16 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nWe are seeking an experienced Microsoft Dynamics 365 Finance Functional Consultant to join our team in Bangalore. The ideal candidate will have hands-on experience in implementing, configuring, and supporting D365 Finance solutions for diverse business requirements. This role involves working closely with clients to understand their financial processes and translate them into effective D365 Finance configurations.\nKey Responsibilities\nImplementation & Configuration",,,,"['Finance', 'D365 Functional', 'Microsoft dynamics']",2025-06-12 06:09:38
Sr. Associate Full Stack Software Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will be responsible for designing, developing, and maintaining software applications and solutions that meet business needs and ensuring the availability and performance of critical systems and applications. This role involves working closely with product managers, designers, data engineers, and other engineers to create high-quality, scalable software solutions and automating operations, monitoring system health, and responding to incidents to minimize downtime.\n\nYou will play a key role in a regulatory submission content automation initiative which will modernize and digitize the regulatory submission process, positioning Amgen as a leader in regulatory innovation. The initiative demonstrates innovative technologies, including Generative AI, Structured Content Management, and integrated data to automate the creation, and management of regulatory content.\n\n\n\nRoles & Responsibilities:\nPossesses strong rapid prototyping skills and can quickly translate concepts into working code\nContribute to both front-end and back-end development using cloud technology\nDevelop innovative solution using generative AI technologies\nEnsure code quality and consistency to standard methodologies\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations\nIdentify and resolve technical challenges effectively\nStay updated with the latest trends and advancements\nWork closely with product team, business team, and other collaborators\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDevelop and implement unit tests, integration tests, and other testing strategies to ensure the quality of the software\nIdentify and resolve software bugs and performance issues\nWork closely with multi-functional teams, including product management, design, and QA, to deliver high-quality software on time\nCustomize modules to meet specific business requirements\nWork on integrating with other systems and platforms to ensure seamless data flow and functionality\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nMasters degree and 1 to 3 years of experience in Computer Science, IT or related field OR\nBachelors degree and 3 to 5 years of experience in Computer Science, IT or related field OR\nDiploma and 7 to 9 years of experience in Computer Science, IT or related field\nPreferred Qualifications:\n\n\n\nFunctional\n\nSkills:\nMust-Have Skills:\nProficiency in Python/PySpark development, Fast API, PostgreSQL, Databricks, DevOps Tools, CI/CD, Data Ingestion.\nCandidates should be able to write clean, efficient, and maintainable code.\nKnowledge of HTML, CSS, and JavaScript, along with popular front-end frameworks like React or Angular, is required to build interactive and responsive web applications\nIn-depth knowledge of data engineering concepts, ETL processes, and data architecture principles. Solid understanding of cloud computing principles, particularly within the AWS ecosystem\nSolid understanding of software development methodologies, including Agile and Scrum\nExperience with version control systems like Git\nHands on experience with various cloud services, understand pros and cons of various cloud service in well architected cloud design principles\nStrong problem solving, analytical skills; Ability to learn quickly; Good communication and interpersonal skills\nExperienced with API integration, serverless, microservices architecture.\nExperience in SQL/NOSQL database, vector database for large language models\n\n\n\nGood-to-Have\n\nSkills:\nSolid understanding of cloud platforms (e.g., AWS, GCP, Azure) and containerization technologies (e.g., Docker, Kubernetes)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience with data processing tools like Hadoop, Spark, or similar\n\n\n\nSoft\n\nSkills:\nExcellent analytical and solving skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'PySpark development', 'Data Ingestion', 'PostgreSQL', 'Fast API', 'CI/CD', 'DevOps Tools', 'Databricks']",2025-06-12 06:09:41
Informatica MDM-Manager,Mclaren Strategic Ventures India,9 - 14 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Bengaluru']","Role & responsibilities\nD for IDMC Manager: 8+ years of experience in MDM development, with at least 2 years on the Informatica IDMC platform. Key Responsibilities: • Lead the development and implementation of MDM solutions on the Informatica IDMC platform. • Design and configure Business 360 (B360), Customer 360 (C360), Product 360 (P360), and Reference 360 (R360) solutions. • Implement match/merge logic, survivorship rules, business validations, and workflow orchestration using Cloud Application Integration (CAI). • Configure Data Quality (DQ) services and perform data profiling to support cleansing and validation processes. • Integrate MDM with enterprise systems such as ERP, CRM, and data warehouses using IDMCs standard connectors, APIs, and real-time integration patterns. • Collaborate with data architects, business stakeholders, and project teams to gather requirements and translate them into scalable MDM solutions. • Utilize data modeling best practices to design and maintain golden records for domains like Customer, Product. • Work with real-time REST APIs exposed by Informatica for operations such as search, create, update, and delete. • Provide leadership in solution design reviews, troubleshooting, performance tuning, and production support. JD for IDMC Developer: 4+ years of experience in MDM development, with at least 1 year on the Informatica IDMC platform. Key Responsibilities: • Develop and implement Master Data Management (MDM) solutions on the Informatica IDMC platform. • Configure and maintain IDMC components such as Customer 360 (C360), Product 360 (P360), Business 360 (B360), and Reference 360 (R360). • Design and implement match rules, business rules, survivorship rules, and data validations. • Build and orchestrate workflows using Cloud Application Integration (CAI). • Perform data profiling and implement data quality (DQ) rules and transformations. • Develop data pipelines and workflows using IDMC Data Integration service. • Integrate IDMC MDM with external systems like ERP, CRM, and data lakes using connectors and APIs. • Use real-time REST APIs provided by IDMC for operations such as search, create, update, and delete. • Collaborate with data architects, analysts, and business users to gather requirements and deliver scalable solutions.""\nAdditional Information: SPOC - Rajalaxmi (rajalaxmi.k.kar@pwc.com)\nMandatory Skills IDMC MDM\nNice to have skills CDQ, IDQ\nInterview Mode Virtual Interview\nWork Model Remote\nCleanroom: No\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Informatica Mdm', 'Informatica Master Data Management', 'Master Data Management']",2025-06-12 06:09:43
"Lead FullStack Developer - Java, React.JS",S&P Global Market Intelligence,10 - 15 years,Not Disclosed,['Hyderabad'],"About the Role:\nGrade Level (for internal use):\n11\nThe Team:\nWe are looking for a highly motivated, enthusiastic, and skilled engineering lead for Commodity Insights. We strive to deliver solutions that are sector-specific, data-rich, and hyper-targeted for evolving business needs. Our Software development Leaders are involved in the full product life cycle, from design through release.\nThe resource would be joining a strong innovative team working on the content management platforms which support a large revenue stream for S&P Commodity Insights.Working very closely with the Product owner and Development Manager, teams are responsible for the development of user enhancements and maintaining good technical hygiene.\nThe successful candidate will assist in the design, development, release and support of content platforms. Skills required include ReactJS, Spring Boot, RESTful microservices, AWS services (S3, ECS, Fargate, Lambda, etc.), CSS HTML, AJAX JSON, XML and SQL (PostgreSQL/Oracle), .\nThe candidate should be aware of GEN AI or LLM models like Open AI and Claude etc.\nThe candidate should be enthusiast in working on prompt building related to GenAI and business-related prompts.\nThe candidate should be able to develop and optimize prompts for AI models to improve accuracy and relevance.\nThe candidate must be able to work well with a distributed team, demonstrate an ability to articulate technical solutions for business requirements, have experience with content management/packaging solutions, and embrace a collaborative approach for the implementation of solutions.\nResponsibilities:\nLead and mentor a team through all phases of the software development lifecycle, adhering to agile methodologies (Analyze, design, develop, test, debug, and deploy). Ensure high-quality deliverables and foster a collaborative environment.\nBe proficient with the use of developer tools supporting the CI/CD process including configuring and executing automated pipelines to build and deploy software components\nActively contribute to team planning and ceremonies and commit to team agreement and goals\nEnsure code quality and security by understanding vulnerability patterns, running code scans, and be able to remediate issues.\nMentoringthe junior developers.\nMake sure that code review tasks on all user storiesare added and timely completed.\nPerform reviews and integration testing to assure quality of project development eorts\nDesign database schemas, conceptual data models, UI workows and application architectures that t into the enterprise architecture\nSupport the user base, assisting with tracking down issues and analyzing feedback to identify product improvements\nUnderstand and commit to the culture of S&P Global: the vision, purpose and values of the organization\nBasic Qualifications:\n10+ years experience in an agile team development role, delivering software solutions using Scrum\nJava, J2EE, Javascript, CSS/HTML, AJAX\nReactJS, Spring Boot, Microservices, RESTful services, OAuth\nXML, JSON, data transformation\nSQL and NoSQL Databases (Oracle, PostgreSQL)\nWorking knowledge of Amazon Web Services (Lambda, Fargate, ECS, S3, etc.)\nExperience on GEN AI or LLM models like Open AI and Claude is preferred.\nExperience with agile workflow tools (e.g. VSTS, JIRA)\nExperience with source code management tools (e.g. git), build management tools (e.g. Maven) and continuous integration/delivery processes and tools (e.g. Jenkins, Ansible)\nSelf-starter able to work to achieve objectives with minimum direction\nComfortable working independently as well as in a team\nExcellent verbal and written communication skills\nPreferred Qualifications:\nAnalysis of business information patterns, data analysis and data modeling\nWorking with user experience designers to deliver end-user focused benefits realization\nFamiliar with containerization (Docker, Kubernetes)\nMessaging/queuing solutions (Kafka, etc.)\nFamiliar with application security development/operations best practices (including static/dynamic code analysis tools)\nAbout S&P Global Commodity Insights\nAt S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\nWere a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the worlds foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the worlds leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit .\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\n\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nWe take care of you, so you cantake care of business. We care about our people. Thats why we provide everything youand your careerneed to thrive at S&P Global.\n\nHealth & Wellness: Health care coverage designed for the mind and body.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nGlobal Hiring and Opportunity at S&P Global:\nAt S&P Global, we are committed to fostering a connected andengaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n-----------------------------------------------------------\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\nIf you need an accommodation during the application process due to a disability, please send an email to:and your request will be forwarded to the appropriate person.\n\nUS Candidates Only:The EEO is the Law Poster describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision -\n-----------------------------------------------------------\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.2 - Middle Professional Tier II (EEO Job Group), SWP Priority Ratings - (Strategic Workforce Planning)",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'S3', 'PostgreSQL', 'Spring Boot', 'J2EE', 'Microservices', 'OAuth', 'Fargate', 'ECS', 'ReactJS', 'Javascript', 'CSS/HTML', 'RESTful services', 'Oracle', 'Lambda', 'AJAX']",2025-06-12 06:09:47
Senior SAP MM Functional Consultant,Sopra Steria,14 - 18 years,Not Disclosed,['Chennai'],"Role & responsibilities\nLead the workshop and identify the GAP\nShould be autonomous, able to challenge the Business, to orient the business on core solutions without customizations\nConfigure and customize the SAP MM module in alignment with client-specific needs\nLead full-cycle SAP MM implementation projects, including requirements gathering, design, testing, training, and post-implementation support.",,,,"['SAP MM', 'implementation', 'Hana Implementation', 'Sap Mm Hana', 'SAP MM Implementation']",2025-06-12 06:09:50
SAP DMS Specialist,Sadup Soft,5 - 10 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Locations : Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote\n\nAbout the Role :\nWe are seeking a highly skilled SAP DMS Specialist to manage, configure, and optimize the Document Management System (DMS) within the SAP environment. The successful candidate will be responsible for overseeing document lifecycle management, ensuring seamless integration of DMS with other SAP modules, and maintaining security and compliance standards. This role requires collaboration with cross-functional teams to implement solutions that enhance document management processes, improve data accessibility, and support business operations.\n\nResponsibilities :\n\n- Provide hands-on expertise in SAP DMS configuration and management, with 5-10 years of relevant experience.\n\n- Manage the complete document lifecycle within SAP DMS, including creation, storage, retrieval, version control, and archiving.\n\n- Configure and maintain metadata within SAP DMS to ensure accurate document categorization and efficient searching.\n\n- Implement and manage version control strategies within SAP DMS to track document changes and maintain audit trails.\n\n- Possess strong experience with SAP ECC and SAP S/4HANA, including a deep understanding of how DMS integrates with other SAP modules such as Materials Management (MM), Plant Maintenance (PM), and Quality Management (QM).\n\n- Design, implement, and manage SAP DMS workflows to automate document routing, approval processes, and other document-related tasks.\n\n- Define and implement document categorization strategies within SAP DMS to organize and classify documents effectively.\n\n- Configure and manage various storage solutions integrated with SAP DMS, ensuring optimal performance and accessibility.\n\n- Implement and maintain SAP DMS security protocols, including user roles, authorizations, and access controls, to protect sensitive information.\n\n- Apply knowledge of compliance standards and document retention policies within SAP systems to ensure adherence to regulatory requirements.\n\n- Collaborate with business users and IT teams to gather requirements and translate them into effective SAP DMS solutions.\n\n- Develop and maintain comprehensive documentation for SAP DMS configurations, processes, and user guides.\n\n- Provide end-user training and support for SAP DMS functionalities.\n\n- Troubleshoot and resolve issues related to SAP DMS functionality and integrations.\n\n- Participate in upgrades and enhancements of the SAP DMS system.\n\n- Stay up-to-date with the latest SAP DMS features and best practices.\n\nQualifications : Bachelor's degree in Computer Science, Information Technology, or a related field.\n\nRequired Skills :\n\n- 5-10 years of hands-on experience in SAP DMS configuration and management.\n\n- Deep expertise in document lifecycle management, metadata configuration, and version control in SAP DMS.\n\n- Proven strong experience with SAP ECC and SAP S/4HANA.\n\n- Extensive experience with the integration of SAP DMS with other SAP modules (e.g., MM, PM, QM).\n\n- Proficient in designing, implementing, and managing SAP DMS workflows.\n\n- Strong understanding of document categorization principles and experience in implementing them within SAP DMS.\n\n- Experience in configuring and managing various storage solutions integrated with SAP DMS.\n\n- Comprehensive understanding and experience with SAP DMS security protocols and access controls.\n\n- Solid knowledge of compliance standards and document retention policies within SAP systems.\n\n- Excellent analytical and problem-solving skills.\n\n- Strong communication and interpersonal skills, with the ability to collaborate effectively with cross functional teams.\n\n- Ability to work independently and manage tasks effectively in a remote environment.\n\nPreferred Skills :\n\n- Experience with SAP Engineering Control Center (ECTR).\n\n- Knowledge of SAP Content Server.\n\n- Experience with OpenText Extended ECM for SAP Solutions.\n\n- Familiarity with data migration tools and techniques related to SAP DMS.\n\n- Experience in developing custom solutions or enhancements within SAP DMS.\n\n- SAP DMS certification.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP DMS', 'SAP ECC', 'SAP S/4 HANA', 'SAP PM', 'SAP QM', 'Document Management System', 'SAP MM', 'SAP Integration', 'SAP System Configuration', 'SAP Implementation', 'SAP Support']",2025-06-12 06:09:53
SAP ABAP Developer,Mobile Programming,6 - 9 years,Not Disclosed,['Bengaluru'],"We are looking for an experienced SAP ABAP Developer to join our team in Bengaluru. As an SAP ABAP Developer, you will be responsible for designing, developing, and maintaining custom SAP applications and solutions using the ABAP programming language. You will work on enhancements, custom reports, interfaces, and data migration tasks.\nYour role will involve collaborating with functional teams to ensure the seamless integration of SAP modules and a smooth business workflow.\nKey Responsibilities:\nDevelop and maintain custom ABAP programs and SAP modules to support business needs.\nDesign and implement SAP enhancements, reports, and interfaces.Work on data migration and system integration between SAP and other enterprise systems.Collaborate with functional teams to understand business requirements and translate them into technical solutions.\nPerform unit testing, integration testing, and ensure the quality of deliverables.\nProvide support for SAP troubleshooting and issue resolution.Ensure that all developed solutions comply with SAP standards and best practices.Maintain up-to-date documentation for all technical solutions and processes.Participate in code reviews and contribute to improving the development process.\nRequired Skills and Qualifications:6-9 years of hands-on experience as an SAP ABAP Developer.\nProficiency in ABAP programming language with a deep understanding of SAP architecture.\nExperience with SAP modules (e.g., MM, SD, FICO, etc.) and their integration.\nExpertise in ABAP Workbench, Smart Forms, ALV Reports, and BAPI development.\nExperience in Data Migration using LSMW, IDOC, and BAPI.\nHands-on experience with SAP NetWeaver, SAP Fiori, and HANA integration.\nStrong problem-solving and debugging skills.\nKnowledge of performance optimization and security best practices for ABAP programs.Ability to work with functional teams to define requirements and design solutions.\nStrong communication skills to collaborate with business stakeholders and technical teams.\nTechnical Skills:\nSAP ABAP | ABAP Workbench | Smart Forms | ALV Reports | BAPI | LSMW | IDOC | SAP NetWeaver | SAP Fiori | SAP HANA | SAP Modules (MM, SD, FICO) | Data Migration | SAP OData | ABAP Objects | UI5 | Object-Oriented Programming | SAP PI/PO | Performance Optimization | Debugging | SAP Workflow | Integration",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'SAP Fiori', 'ABAP Workbench', 'integration testing', 'SAP HANA', 'SAP ABAP', 'unit testing', 'BAPI development']",2025-06-12 06:09:56
SAP PP Consultant - S/4 HANA Module,Zettamine Labs,6 - 11 years,Not Disclosed,"['Mumbai', 'Pune', 'Bengaluru']","Location : Pune, Mumbai, Bangalore\n\nNotice Period : Immediate\n\nJob Overview :\n\nWe are looking for an experienced SAP PP (Production Planning) Consultant to join our team. The ideal candidate will have strong expertise in SAP PP module, including configuration, implementation, and integration with other SAP modules.\n\nKey Responsibilities :\n\n- Implement and configure SAP PP to optimize production planning and control.\n\n- Collaborate with stakeholders to gather business requirements and translate them into system solutions.\n\n- Perform end-to-end process mapping for demand planning, capacity planning, and shop floor control.\n\n- Ensure seamless integration with SAP MM, SD, and QM modules.\n\n- Troubleshoot and resolve technical issues in SAP PP environments.\n\n- Provide end-user training and documentation support.\n\n- Work closely with SAP ERP teams to enable master data integration and production planning\nrequirements.\n\n- Support SAP S/4HANA migration and implementation projects.\n\n- Optimize Bill of Materials (BOM), Routing, and Work Centers for efficient production\nprocesses.\n\n- Lead data migration from legacy systems to SAP PP.\n\nRequired Skills and Experience :\n\n- 6-12 years of experience in SAP PP implementation.\n\n- Strong knowledge of production planning, demand management, and shop floor control.\n\n- Hands-on experience in SAP PP configuration, development (ABAP), and integration.\n\n- Expertise in SAP S/4HANA integration for production planning.\n\n- Experience in capacity planning, MRP, and scheduling.\n\n- Strong understanding of warehouse business processes related to production planning.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['S/4 HANA Module', 'SAP QM', 'S/4 HANA', 'SAP SD', 'SAP PP', 'SAP ABAP', 'SAP MM', 'SAP Integration', 'SAP Implementation', 'SAP WM']",2025-06-12 06:10:00
Application Developer - Dot Net IFRS 17 // Mumbai // 4-8 Yrs,2coms,4 - 9 years,Not Disclosed,['Mumbai'],"SUMMARY\nAbout the client:\n\nOur client is an IT Technology & Services Management MNC , supporting millions of internal and external customers with state of-the-art IT solutions to everyday problems & dedicated to bringing digital innovations to every aspect of the landscape of insurance. Our Client is a part of one of the major insurance groups based out of Germany and Europe. The Group is represented in around 26 countries worldwide, with Over 37,000 people worldwide, focusing mainly on Europe and Asia & offers a comprehensive range of insurances, pensions, investments and services by focusing on all cutting edge technologies majorly on Could, Digital, Robotics Automation, IoT, Voice Recognition, Big Data science, advanced mobile solutions and much more to accommodate the customers future needs around the globe.\n\n\n\n\n\nRequirements\nCompentences for .NET\nNET Framework (incl. .net core), SQL Server, Entity Framework\nPrioritize business impact and urgency\nAbility to learn new technology and methodology quickly.\nKnowledge User Task API, GUI (Graphical User Interfaces) Design Compentences for NTS (New Tech Stack)\nNTS (New Tech Stack), Container runtime environment with Docker containers and Kubernetes, Cloud with AWS (Amazon Web Services), CI/CD - Continuous Integration / Continuous Deployment with Jenkins, Knowledge Source Management Github and Nexus\nOpenshift, Kerberos Authentication, Competences for Cluster Workflow\nDesign + implementation of process model, Design + implementation of input interfaces in REST format\nDevelopment of flow services below process model\n\nEducational Qualifications:\nBachelor’s or Master’s  degree in Computer Science /Engineering/Information Technology\nCandidate with non-computer science degree must have minimum 1 year of relevant experience\nMBA in IT / Insurance/Finance   can also apply for Requirements Engineer and Test Engineer role.\n\n\n\nBenefits",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'kubernetes', 'nexus', 'web services', 'openshift', 'ci/cd', 'kerberos', 'ccm', 'business objects administration', 'docker', 'profibus', 'jenkins', 'cd', 'rest', 'e-discovery', 'github', 'entity framework', 'intools', 'sql server', 'application development', 'net application', '.net core', '.net', 'abb dcs', 'aws']",2025-06-12 06:10:03
Technial Lead - backend,Cheq Digital,7 - 9 years,40-55 Lacs P.A.,['Bengaluru'],"About CheQ\nHey Future Teammate! Ready to dive into the fintech revolution with us at CheQ? We're not your typical 9-to-5 crew; we're the dynamic force turning credit management into a fun and rewarding journey.\nImagine instant repayments that are not just manageable but downright enjoyable. Founded by ex-Flipkart executive, Aditya Soni, CheQ has processed over $2 Billion in credit repayments in a short span. Yes, that's buying all IPL teams together! We've built a user base of over a million and raised a whopping $16 M, backed by 3one4 Capital, Venture Highway and marquee angels like Ram Shriram & Dr. LloydMarquee fintech angels like Naveen Kukreja of PaisaBazar and Shailaz Nag of Dotpe trust our venture. We're on a mission to make credit easy, enjoyable, and filled with awesome rewards. Picture being part of a team that turns the credit maze into an adventure. If you're ready to make credit management a journey worth taking, where work feels like play, hit us up. Let's transform the game together! #JoinCheQ #FintechRevolution\n\nEngineering @CheQ\nEngineers at CheQ are true builders. They bring in a myriad of ideas, and create the infrastructure, systems and products that drive our services. Our engineers build Zero to One products, microservices, DBs and everything else (from farm to table).\n\nWith their magic they take a product, feature or experience from ideation to production, always keeping the customers and their interests in mind. This includes understanding consumer & business needs, critiquing & challenging product and design counterparts and owning the execution & delivery of the scope.\n\nWhat youll be doing\nWe are much more than our job descriptions,but here is where you will begin:\nCollaborate with stakeholders, including product owners, project managers, and scrum masters, to define and clarify project requirements.\nTranslate business requirements into technical specifications and ensure all stakeholders have a clear understanding of the project scope and objectives.\nFacilitate effective communication and coordination among cross-functional teams to ensure alignment and successful project delivery.\nDesign, develop, and maintain scalable and efficient software solutions that meet business needs.\nWrite clean, maintainable, and well-documented code while adhering to best practices and coding standards.\nPerform code reviews and provide constructive feedback to team members to ensure code quality and consistency.\nWork closely with the DevOps team to establish and maintain CI/CD pipelines for seamless product building, deployment, and testing across all release cycles from development to production.\nEnhance and apply a strong understanding of modern security principles and practices to the development and deployment of applications.\nImplement security measures such as authentication, authorization, data encryption, and vulnerability assessments to protect applications from security threats.\nRequirements\nLike us, you’ll be deeply committed to delivering impactful outcomes for customers.\n7+ years of demonstrated ability to develop resilient, high-performance, and scalable code tailored to application usage demands.\nAbility to lead by example with hands-on development while managing project timelines and deliverables.\nExperience in agile methodologies and practices, including sprint planning and execution, to drive team performance and project success.\nExperience building RESTful web services using NodeJS.\nExperience writing batch/cron jobs using Python and Shell scripting.\nExperience in web application development using JavaScript and JavaScript libraries.\nHave a basic understanding of Typescript, JavaScript, HTML, CSS, JSON and REST based applications.\nExperience/Familiarity with RDBMS and NoSQL Database technologies like MySQL, MongoDB, Redis, ElasticSearch and other similar databases.\nUnderstanding of code versioning tools such as Git.\nUnderstanding of building applications deployed on the cloud using Google cloud platform or Amazon Web Services (AWS), Docker and kubernetes.\nExperienced in JS-based build /Package tools like Grunt, Gulp, Bower, Webpack and NPM.\nBenefits\nYou define your work\nWe acknowledge that your work does not define you. It’s you who will define your work here. We do not encourage trade-offs between work and life.\nPropelled by courage & care\nWe dare each other with the art of possible and then watch each other’s back delivering the solution with speed, agility, heart and rigor.\nLearn with the best\nWith a strong leadership team from diverse backgrounds, you can expect to get the best of many worlds\n\nAnd much more\n\nIndustry competitive compensation\nESOPs of a boat that you must onboard before it becomes a ship\nWork on real problems of India that will create Impact at scale\nWork with all the jazz and fancy that new and innovative technologies bring\n\nWhat you will not get\nWe come from a place of honesty. So let’s set our expectations right!\nPredictability of work\nYou will be a spider in the web; we will throw everything at you!\nClimbing the slow ladder of Career Growth\nWe all love to hop, skip & grow!\nBureaucracy and slow decision making\nWhat was that again??\nMeetings, meetings & only meetings\nWe believe in agility, empowerment and get the work done!",Industry Type: FinTech / Payments,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MySQL', 'Node.Js', 'MongoDB']",2025-06-12 06:10:06
PostgreSQL DBA / Developer,Srs Business Solutions India,3 - 8 years,6-11 Lacs P.A.,['Jaipur'],"Hello,\n\nWe are hiring for ""PostgreSQL DBA / Developer"" for Jaipur Location.\n\nJob Title PostgreSQL DBA / Developer\nWork Mode: Work from Office\nExp: 3+Years\nLoc: Jaipur\nNotice Period: Immediate joiners(notice period served candidate)\n\nNOTE: We are looking for Immediate joiners(notice period served candidate)\n\nApply only If you are Immediate joiners(notice period served candidate)\n\nApply only If you are from Jaipur or Rajasthan.\n\n\nRequired experience:\nStrong expertise in PostgreSQL database development or administration\nShould have experience in writing queries, replication, Optimization and data migration\n\nNOTE: Let your co-workers or Circle know about this opportunity if you lack these skills set.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Postgresql', 'Postgresql Database Administration', 'Postgresql development']",2025-06-12 06:10:10
Test Lead,Infosys,5 - 10 years,10-20 Lacs P.A.,[],"Role & responsibilities\n\nThe Test Lead oversees the testing strategy and execution for the Microsoft Fabric migration and Power BI reporting solutions. This offshore role ensures quality, reliability, and client satisfaction through rigorous validation.\nThe successful candidate will have a strong testing background and coordination skills.\nResponsibilities\nDevelop and execute the testing strategy for Microsoft Fabric and Power BI deliverables.\nValidate data migration, pipeline functionality, and report accuracy against requirements.\nCoordinate with the Offshore Project Manager to align testing with development milestones.\nCollaborate with onsite technical leads to validate results and resolve defects. • Oversee offshore testers, ensuring comprehensive coverage and quality standards.\nProactively identify risks and articulate solutions to minimize delivery issues.\nSkills\nBachelors degree in IT, computer science, or a related field.\n5+ years of experience in test leadership for data platforms and BI solutions.\nKnowledge of Microsoft Fabric, Power BI, and data migration testing.\nProficiency with testing tools (e.g., Azure DevOps, Selenium) and SQL.\nStrong communication and stakeholder management skills.\nDetail-oriented with a focus on quality and continuous improvement\n1. JD for Data Modeler\nThe Data Modeler designs and implements data models for Microsoft Fabric and Power BI, supporting the migration from Oracle/Informatica. This offshore role ensures optimized data structures for performance and reporting needs. The successful candidate will bring expertise in data modeling and a collaborative approach.\nResponsibilities\nDevelop conceptual, logical, and physical data models for Microsoft Fabric and Power BI solutions.\nImplement data models for relational, dimensional, and data lake environments on target platforms.\nCollaborate with the Offshore Data Engineer and Onsite Data Modernization Architect to ensure model alignment.\nDefine and govern data modeling standards, tools, and best practices.\nOptimize data structures for query performance and scalability.\nProvide updates on modeling progress and dependencies to the Offshore Project Manager.\nSkills\nBachelor’s or master’s degree in computer science, data science, or a related field.\n5+ years of data modeling experience with relational and NoSQL platforms.\nProficiency with modeling tools (e.g., Erwin, ER/Studio) and SQL.\nExperience with Microsoft Fabric, data lakes, and BI data structures.\nStrong analytical and communication skills for team collaboration.\nAttention to detail with a focus on performance and consistency.\nmanagement, communication, and presentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Test lead', 'Migration', 'power bi', 'microsoft fabric']",2025-06-12 06:10:13
Oracle Database Developer,Mobile Programming,6 - 10 years,Not Disclosed,['Bengaluru'],"We are hiring an experienced Oracle Database Developer for a prestigious client, Siemens. The ideal candidate should have deep expertise in Oracle databases with a solid foundation in SQL and PL/SQL development.\nThis role demands experience in database migrations, trigger management, and a basic understanding of PostgreSQL.\nKey Responsibilities:Design, develop, and maintain Oracle database solutions using SQL and PL/SQLHandle database migrations, develop and manage triggers, stored procedures, and packages Optimize and troubleshoot performance issues Collaborate with development and QA teams to ensure seamless data integrationWork with PostgreSQL for basic database interactions and data migration tasks Technical Skills\n(Mandatory):Oracle DB (11g, 12c, 19c)SQL, PL/SQLDatabase Migrations Triggers\nAdditional:Basic knowledge of PostgreSQL",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle Database', 'QA', 'PostgreSQL', 'PL/SQL Database Migrations', 'SQL']",2025-06-12 06:10:16
Technical Lead - PLM,Tata Technologies,8 - 13 years,Not Disclosed,['Pune'],"TC Migration Lead:Minimum 8 + years of experience in developing Teamcenter\nHands-on experience in Teamcenter implementations in various phases of project like business blueprinting, configuration, cut-over and post go-live support\nStrong expertise of Teamce nter product architecture and its integration frameworks like T4EA/T4S/T4O/CAD Integrations etc. Technical capabilities in Part Management, BOM Management, Change Management, Classification, Workflow, Organization\nProficiency in Teamcenter ITK, RAC, BMIDE, and SOA customization\nExperience with Teamcenter workflows, access control, and BMIDE configurations\nExperience with SQL/Oracle and d atabase management systems\nExperience in C/C++ programming language and TC server side (ITK & SOA), BMIDE, Java, HTML, XML and Web services and Active Workspace Configuration and declarative programming\nExperience with data migration, mapping, transformation, data validation to Teamcenter from legacy applications\nShould have exposure to CAD, NX and Teamcenter",Industry Type: Building Material (Cement),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Teamcenter', 'c++', 'caa', 'oracle', 'soa', 'web services', 'product life cycle management', 'engineering change management', 'bmide', 'cad', 'data migration', 'change management', 'catia', 'sql', 'plm', 'java', 'rac', 'xml', 'pdm', 'teamcenter itk', 'html', 'sap plm', 'teamcenter unified']",2025-06-12 06:10:19
Oracle Recruitment Cloud Expert,UST,7 - 12 years,Not Disclosed,"['Kochi', 'Bengaluru', 'Thiruvananthapuram']","Role: Oracle Recruitment Cloud Expert\nLocation: PAN India\n\nKey Responsibilities\n1. Development & Implementation\nInterpret application/component designs and develop accordingly\nCode, debug, test, document, and communicate development stages",,,,"['Oracle Hcm Cloud', 'Oracle Hcm', 'Oracle Recruiting Cloud']",2025-06-12 06:10:22
MDM Testing - Associate Analyst,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for a skilled MDM Testing Associate Analyst who will responsible for ensuring the quality and integrity of Master Data Management (MDM) applications through rigorous testing processes. This role involves collaborating with cross-functional teams to define testing objectives, scope, and deliverables, and to ensure that master data is accurate, consistent, and reliable. and comply with Amgens standard operating procedures, policies, and guidelines. Your expertise will be instrumental in ensuring quality and adherence to required standards so that the engineering teams can build and deploy products that are compliant.\nRoles & Responsibilities:\nTest Planning: Develop and implement comprehensive testing strategies for MDM applications, including defining test objectives, scope, and deliverables. This includes creating detailed test plans, test cases, and test scripts.\nTest Execution: Execute test cases, report defects, and ensure that all issues are resolved before deployment. This involves performing functional, integration, regression, and performance testing.\nData Analysis: Analyze data to identify trends, patterns, and insights that can be used to improve business processes and decision-making. This includes validating data accuracy, completeness, and consistency.\nCollaboration: Work closely with the MDM, RefData and DQDG team and other departments to ensure that the organizations data needs are met. This includes coordinating with data stewards, data architects, and business analysts.\nDocumentation: Maintain detailed documentation of test cases, test results, and any issues encountered during testing. This includes creating test summary reports and defect logs.\nQuality Assurance: Develop and implement data quality metrics to ensure the accuracy and consistency of master data. This includes conducting regular data audits and implementing data cleansing processes.\nCompliance: Ensure that all master data is compliant with data privacy and protection regulations. This includes adhering to industry standards and best practices for data management.\nTraining and Support: Provide training and support to end-users to ensure proper use of MDM systems. This includes creating user manuals and conducting training sessions\nStay current on new technologies, validation trends, and industry best practices to improve validation efficiencies.\nCollaborate and communicate effectively with the product teams.\nBasic Qualifications and Experience:\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\n2+ years of experience in MDM implementations, primarily with testing (pharmaceutical, biotech, medical devices, etc.)\nExtensive experience on ETL/ELT and MDM testing (Creating test plan, test scripts and execution of test scripts and bugs tracking/reporting in JIRA)\nInformatica MDM: Proficiency in Informatica MDM Hub console, configuration, IDD (Informatica Data Director), IDQ, and data modeling\nor\nReltio MDM: Experience with Reltio components, including data modeling, integration, validation, cleansing, and unification.\nAdvanced SQL: Ability to write and optimize complex SQL queries, including subqueries, joins, and window functions.\nData Manipulation: Skills in data transformation techniques like pivoting and unpivoting.\nStored Procedures and Triggers: Proficiency in creating and managing stored procedures and triggers for automation.\nPython: Strong skills in using Python for data analysis, including libraries like Pandas and NumPy etc.\nAutomation: Experience in automating tasks using Python scripts.\nMachine Learning: Basic understanding of machine learning concepts and libraries like scikit-learn.\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nGood-to-Have Skills:\nETL Processes: Knowledge of ETL processes for extracting, transforming, and loading data from various sources.\nData Quality Management: Skills in data profiling and cleansing using tools like Informatica.\nData Governance: Understanding of data governance frameworks and implementation.\nData Stewardship: Ability to work with data stewards to enforce data policies and standards.\nSelenium: Experience with Selenium for automated testing of web applications.\nJIRA: Familiarity with JIRA for issue tracking and test case management.\nPostman: Skills in using Postman for API testing.\nUnderstanding of compliance and regulatory considerations in master data.\nIn depth knowledge of GDPR and HIPPA guidelines.\nProfessional Certifications:\nMDM certification (Informatica or Reltio)\nSQL Certified\nAgile or SAFe certified\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MDM Testing', 'ETL Processes', 'Data Stewardship', 'MDM', 'Agile', 'Data Quality Management', 'Data Governance', 'SQL']",2025-06-12 06:10:25
CRM Technical professional,big 4,4 - 9 years,Not Disclosed,"['Gurugram', 'Bengaluru', 'Mumbai (All Areas)']","Job Summary\n1.1 CRM Technical\n1.1.1 Must Have\nDesign, develop, and implement Microsoft Dynamics 365 solutions to meet business requirements.\nKnowledge of minimum two D365 CRM modules like Sales, Service, Field Service and Customer Insights\nCustomize and configure D365 applications, including workflows, plugins, and integrations.\nStrong technical skills in Power Pages, PowerApps, Power Automate, and related technologies\nExperience with CanvasApp development\nKnowledge of form script using JS, Business Process Flow, Business Rules\nKnowledge of solution deployment, solution management\nKnowledge of Integrations WebAPI\nExperience with development tools like Visual Studio, XRMToolBox, Postman etc\nDevelop and maintain technical documentation for D365 solutions.\nMicrosoft Certification\n1.1.2 Good To have:\n(Must meet 50% for Con level, above 70% match for Sr. Con and above)\nProficiency in programming languages such as C#, .NET, JavaScript, and SQL.\nExperience with D365 integrations using APIs and web services.\nKnowledge of CI/CD pipeline configuration for deployment\nDevelopment knowledge on PCF Controls for both canvas and model driven apps\nAbility to create custom connectors for Power Automate\nExperience with SharePoint integration scenarios with Power Apps and D365\nExperience with Azure platform including functions, LogicApps, ServiceBus, KeyVault\nPowerBI report development Connecting data sources, importing data, and transforming data for Business intelligence.\nExperience with Large enterprise level data migration using Azure Data Factory, Kingsway soft\nKnowledge of CoPilot configuration\n1.1.3 Power Pages:\nDesign, develop, and implement web applications using Microsoft Power Pages.\nCustomize and configure Power Pages to meet business requirements.\nKnowledge of Table Permission, Forms, Lists configuration\nKnowledge of HTML, JS and Liquid for Web Template configuration\nExperience with Power Pages Design Studio and its features",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power apps', 'Microsoft Dynamics 365', 'Power Pages', 'CanvasApp development', 'Microsoft Certification', 'XRMToolBox', 'Power Automate', 'Visual Studio', 'Postman', 'Integrations WebAPI']",2025-06-12 06:10:28
.net fullstack developer - Azure Expert,Kyndryl,10 - 15 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.\n\nThe Role\nAre you ready to join the team of software engineering experts at Kyndryl? We are seeking a talented Software Engineering Technical Specialist to contribute to our software engineering space and provide critical skills required for the development of cutting-edge products.  \n\nAs a Software Engineering Technical Specialist, you will develop solutions in specific domains such as Security, Systems, Databases, Networking Solutions, and more. You will be a leader – contributing knowledge, guidance, technical expertise, and team leadership skills. Your leadership will be demonstrated in your work, to your customers, and within your teams.\n\nAt Kyndryl, we value effective communication and collaboration skills. When you recognise opportunities for business change, you will have the ability to clearly and persuasively communicate complex technical and business concepts to both customers and team members. You’ll be the go-to person for problem-solving of customers’ business and technical issues. You have a knack for effectively identifying and framing problems, leading the collection of elements of information, and integrating this information to produce timely and thoughtful decisions. Your aim throughout, is to improve the effectiveness, efficiency and delivery of services through the use of technology and technical methods and methodologies.\n\nDriving the design, development, integration, delivery, and evolution of highly scalable distributed software you will integrate with other layers and offerings. You will provide deeper functionality and solutions to address customer needs. You will work closely with software engineers, architects, product managers, and partner teams to get high-quality products and features through the agile software development lifecycle.\n\nYour continuous grooming of features/user stories to estimate, identify technical risks/dependencies and clearly communicate them to project stakeholders will ensure the features are delivered with the right quality and within timeline. You will maintain and drive the clearing of technical debt, vulnerabilities, and currency of the 3rd party components within the product.\n\nAs a Software Engineering Technical Specialist, you will also coach and mentor engineers to design and implement highly available, secure, distributed software in a scalable architecture. This is an opportunity to make a real impact and contribute to the success of Kyndryl's innovative software products. Join us and become a key player in our team of software engineering experts!",,,,"['kubernetes', 'server', 'css', 'methods', 'openshift', 'data migration', 'azure migration', 'sql', 'iis', 'apache', 'devops', 'html', 'software engineering', 'api', 'agile principles', 'communication skills', 'process', 'new relic', 'microsoft azure', 'sql server', 'nosql', 'framework', 'devops automation', 'collaboration', 'splunk', '.net', 'aws']",2025-06-12 06:10:32
Marketing Analytics - TL / Assistant Manager,Leading Client,3 - 8 years,Not Disclosed,['Bengaluru'],Candidate Expectations\n3--12 years of work experience\nFlexible to work in shift as per client requirement\nCertification(s) Preferred:\nAdobe Experience Platform\nAdobe Real-Time Customer Data Platform\nAdobe Customer Journey Analytics\nJob Responsibilities\nUtilizing previous experience in CDPs (Customer data platforms) such as Adobe Experience Platform (RTCDP) or similar products;\nHaving an enhanced understanding of customer profile segmentation and experience in 360 degree view of customers in CDP for further analytical processing and decision making;\nShowcasing proven track record of managing successful CDP implementation/management and delivering capabilities that drive business growth;\nDemonstrating work experience in data architecture and data modeling; preferably with experience working in CDP CRM paid media social and offline data;\nEnsuring an enhanced understanding of data-driven marketing analytics and relevance / usage of real-time event data and associated attributes;\nSetting strategic direction and driving execution through collaboration with a cross-functional team;\nDemonstrating familiarity with CRM and Marketing Automation platforms (i.e. Salesforce Sales Cloud Salesforce Marketing Cloud etc.);\nHaving extensive hands on expertise in implementing/administering Adobe Experience Cloud Products and marketing automation platforms and technologies;\nEnsuring an enhanced understanding of identity resolution components and the enabling technology i.e. profile merge rules identity graph identity service provider etc.;\nWorking with audience-based digital marketing strategy either at an agency or brand;\nDriving project management through a full lifecycle including the ability to prioritize sequence execute and deliver projects on time and on budget;\nTranslating business requirements and objectives into segmentation strategies and audience building logic;\nOwning Adobe AEP and driving proactive platform ownership that is directly aligned to the day-to-day delivery of marketing tactics and closely connected to the other digital marketing teams to develop capabilities and manage technology roadmap;\nHaving oversight of the CDP technology solution (e.g. oversee steady state support teams interact with business owner plan for break / fix and other enhancements / maintenance);\nDriving how customer and prospect information is unified across sales marketing and service channels;\nCollaborating with Insights team to refine and optimize measurement process;\nImplementing and refining Adobe Products and marketing automation system;\nCollaborating with Marketing leadership to evolve the use of data within the marketing function allowing us to stay ahead responding to clients in real-time and limiting redundancy.\nContact Person : - Subhikshaa\nContact Number : - 9840114687,Industry Type: BPM / BPO,Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['Adobe Experience Platform', 'Adobe Customer Journey Analytics', 'Salesforce Sales Cloud', 'Sales Accreditation', 'Paid Media', 'Adobe Real-Time Customer Data Platform', 'Salesforce Marketing Cloud', 'Adobe Experience Cloud', 'Marketing Automation platforms', 'CRM']",2025-06-12 06:10:35
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Agra'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n- Configure and refine related workflows within NetSuite\n- Provide post-implementation support and troubleshooting\n- Understand and manage system configuration, data flows, and integration points\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\nQualifications :\n- Strong functional and technical knowledge of NetSuite\n- Experience with ERP data migration, especially from QuickBooks\n- Familiarity with third-party integration tools and NetSuites workflow engine\n- Ability to support post-go-live activities and optimize ERP performance\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:10:37
Fullstack Developer,Grid Dynamics,5 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai']","We are seeking an experienced Full Stack Java Developer skilled in designing, developing, and deploying scalable applications using Java/J2EE, Spring Boot, Microservices, and Angular. The ideal candidate will have hands-on experience with Gradle, Kafka, MySQL, Amazon S3, Docker, and AWS cloud services. You will play a key role in building robust backend systems and intuitive front-end interfaces, leveraging modern DevOps and cloud-native practices.\n\nKey Responsibilities:\nDesign and implement scalable microservices using Java/J2EE and Spring Boot frameworks.\nDevelop RESTful APIs to facilitate communication between distributed services.\nBuild and maintain front-end components using Angular (or similar frameworks).\nIntegrate and manage message-driven architectures using Kafka for event streaming and processing.\nUse Gradle for dependency management and build automation.\nImplement and optimize relational database solutions using MySQL.\nManage file storage and retrieval operations with Amazon S3.\nContainerize applications using Docker and orchestrate deployments on AWS.\nCollaborate with DevOps teams to automate CI/CD pipelines and infrastructure provisioning.\nEnsure application reliability, scalability, and security in a cloud-native (AWS) environment.\nParticipate in code reviews, unit testing, and integration testing to ensure code quality.\nDocument technical designs and implementation processes for future reference.\nEngage in Agile/Scrum ceremonies and collaborate with cross-functional teams.\n\nRequired Qualifications:\nBachelors degree in Computer Science, Information Technology, or a related field.\nProven experience (typically 5+ years) in Java/J2EE application development.\nProficiency in Spring Framework, including Spring Boot and Spring Data JPA.\nStrong understanding of microservices architecture and RESTful API design.\nHands-on experience with Gradle for build automation.\nProficient in integrating and configuring Kafka for messaging solutions.\nExperience with MySQL or other relational databases.\nFamiliarity with Amazon S3 for cloud storage operations.\nFront-end development skills with Angular (TypeScript, HTML, CSS).\nExperience with Docker for containerization and AWS for cloud deployments.\nKnowledge of CI/CD tools and DevOps practices.\nStrong problem-solving, communication, and teamwork skills.\n\nPreferred/Good to Have:\nExperience with additional AWS services (EC2, Lambda, EKS, etc.).\nFamiliarity with security best practices (OAuth2, JWT, etc.).\nExposure to observability tools (Prometheus, Grafana) and infrastructure-as-code (Terraform).\nExperience with Agile methodologies and mentoring junior developers.",Industry Type: IT Services & Consulting,Department: Research & Development,"Employment Type: Full Time, Permanent","['Java', 'Amazon Web Services', 'Kafka', 'Spring Boot', 'Spring', 'AWS', 'Microservices', 'Angular', 'SQL']",2025-06-12 06:10:40
CX Sales and Service Consultant,Gnostic Solutions,10 - 18 years,20-35 Lacs P.A.,[],"Overall, 8+ years of experience relevant to this position.\nDemonstrable experience as a techno-functional lead on at least two large-scale full-life cycle implementations of Oracle CX Applications, with strong implementation expertise in at least two of the following products is a must. \nCX Sales\nB2B Service Cloud\nField Service Cloud\nCPQ\nAsset Based Service\nSubscription Management\nIncentive Compensation\nFamiliarity and exposure to business processes such as Target to Lead, Opportunity to Cash, Request to Resolution.\nStrong techno-functional skills in proposing, designing optimal solutions, including ownership of the overall solution for customization/extension/integrations on Oracle CX Cloud Projects.\nExperience developing process flow diagrams, gathering requirements, conducting workshops, design and prototyping, testing, training, defining support procedures, and implementing practical business solutions.\nOracle CPQ - Hands on experience in configuring Oracle CPQ Cloud module Pricing, Quotation Configuration, Product Management-Items, BOMs, and System Configuration, Document Engine and Big Machines Language (BML), Utilities Libraries, Validation/Hiding/Constraint rules, Layout editor, Commerce layout, Custom CSS, Designing extensions and interfaces in Oracle CPQ Cloud module.\nOSC/B2B Cloud - Prior hands-on experience in functional configurations and customization in the following areas - custom objects, groovy scripting, assignment manager, workflows, triggers email alerts, Data Migration, OTBI Analytics etc.\nOracle Field Service - Prior hands-on experience in configurations and customization in the following areas Core Application, Profiles, Permissions, Business Rules, Routing Plans, Work Schedules, Smart Collaboration, Action Management, Mobility and Manage Displays, Reports, Data migration, Inbound/Outbound messages. Experience developing Oracle Field Service forms and plug-ins.\nExperience developing extensions using Redwood UI, VBCS, VBS, JET\nGood knowledge in web based front end development using – HTML, JavaScript, and CSS\nExperience with React and/or other front-end JavaScript frameworks, would be a plus.\nStrong hands-on experience with design and development of integrations (point to point and Integration Cloud) using various",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['oracle cx applications cloud', 'service cloud', 'Field Service', 'vbcs', 'oracle cs', 'oracle cloud', 'Functional', 'B2B', 'Cpq Cloud', 'ofsc', 'Techno Functional', 'integration', 'fusion sales', 'B2B service', 'fusion service']",2025-06-12 06:10:42
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Kanpur'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'HubSpot', 'Data Migration', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:10:44
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Lucknow'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:10:46
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Bengaluru'],"Responsibilities :\n\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:10:49
ZuroCPQ Developer,Mobile Programming,5 - 10 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:Implement and configure ZuroCPQ to meet business requirements.Customize and extend the functionality of the ZuroCPQ application based on business needs.\nCollaborate with cross-functional teams to understand the requirements and ensure that the CPQ solutions align with sales processes.\nDevelop workflows and pricing rules for complex quotes and configurations.Integrate ZuroCPQ with other enterprise applications, including CRM and ERP systems.Provide technical support and troubleshooting for ZuroCPQ users.\nCreate and maintain technical documentation for the solutions developed.Participate in the design and review process for system enhancements and new features.Monitor and ensure system performance and data integrity.\nRequired Skills and Qualifications:\n5 to 10 years of experience in implementing, configuring, and customizing CPQ solutions.Hands-on experience with ZuroCPQ is essential.\nProficient in pricing and quoting tools, and experience integrating CPQ solutions with CRM (Salesforce) and ERP systems.\nStrong understanding of Salesforce and Salesforce CPQ (if applicable).Ability to work in a collaborative environment with sales, product, and engineering teams.Strong analytical skills and the ability to solve complex business challenges.\nFamiliarity with API integration and data migration.\nUnderstanding of workflow automation and business process optimization.\nAbility to write clean and maintainable code and deliver effective solutions.Strong problem-solving skills and attention to detail.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ZuroCPQ', 'Salesforce CPQ', 'ERP systems', 'CPQ', 'data migration', 'API integration', 'CRM', 'Salesforce']",2025-06-12 06:10:51
Backend Developer - AWS / Java,Sadup Soft,7 - 11 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Designation : AWS Java back end developer\n\nExperience : 7-11yrs\n\nLocation : Remote, Delhi NCR, Bangalore, Chennai, Pune, Kolkata, Ahmedabad, Mumbai, Hyderabad\n\nMust have skills :\n\n- 8-10 years of experience\n\n- 6+ years of experience in JAVA\n\n- Exposure to AWS\n\n- Willingness to work in Kotlin\n\n- Preference is to have the person from product-based company.\n\nRoles &responsibilities :\n\nAs an senior software engineer in our team, you will contribute not only to all aspects of backend, but also get involved in all aspects of software engineering: infra, data, application, deployment, monitoring, architecture, code reviews, pair and mob programming, etc.\n\nYou will also be leading a few initiatives with your design and architectural know-how and working incollaboration with other Architects and Managers.\n\nHere, we own everything we do: we own our successes as well as our mistakes and setbacks. In this spirit, we are looking for someone reliable, curious, and >\nYour day-to-day :\n\n- You will focus on delivering high quality solutions using your Java and Kotlin skills\n\n- You will work as part of an agile team, planning, refining, delivering and inspecting sprint deliveries\n\n- You work closely with product and design to understand context and vision\n\n- You will work with your peers across teams to make platform improvements and increase learning opportunities for all in a supportive environment\n\nWhat do you need to bring :\n\nToday we work with technologies like :\n\n- Java and Kotlin\n\n- Drop wizard, Spring Boot\n\n- Docker, Terraform\n\n- PostgreSQL\n\n- AWS\n\n- Everything is hosted on Amazon Web Services - we use managed cloud services as much as we can.\n\nWe believe that you have :\n\n- 6 to 8 years of experience in software development\n- 5+ years of experience building distributed systems\n\n- 5 + years of experience with SQL and data modelling\n\n- Experience working with continuously delivered systems\n\n- A result and delivery-oriented mindset\n\n- A genuine learning and knowledge sharing spirit\n\n- Unix/Linux/Docker experience\n\n- Interest in or experience of FinTech industry",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Unix', 'Linux', 'PostgreSQL', 'Spring Boot', 'AWS', 'Backend Architecture', 'Kotlin']",2025-06-12 06:10:53
SAP MDM Specialist,Sadup Soft,7 - 10 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Job Title : SAP MDM Consultant.\n\nLocation : Remote,Delhi NCR, Bangalore, Chennai, Pune, Kolkata, Ahmedabad, Mumbai, Hyderabad\n\nExperience : 7-10 years.\n\nKey Responsibilities :\n\n- Own and manage all Master Data Management (MDM) activities for SAP projects, ensuring alignment with business objectives.\n\n- Develop and implement comprehensive MDM strategies and roadmaps.\n\n- Lead the design and implementation of data governance frameworks and processes.\n\n- Lead data migration and cutover activities in SAP S/4HANA projects, including Greenfield implementations, system migrations, and rollouts.\n\n- Develop and execute data migration plans, ensuring data accuracy and consistency.\n\n- Manage data cleansing, transformation, and validation processes.\n\n- Establish and implement MDM best practices and data management capabilities.\n\n- Define and enforce data management principles, policies, and lifecycle strategies.\n\n- Ensure data compliance with regulatory requirements and internal policies.\n\n- Work closely with MDM Leads and stakeholders to drive data governance initiatives.\n\n- Develop and implement data quality metrics and reporting mechanisms.\n\n- Monitor data quality and identify areas for improvement.\n\n- Implement data quality controls and validation rules.\n\n- Track and manage MDM objects, ensuring timely delivery and adherence to project timelines.\n\n- Participate in daily stand-ups, issue tracking, and dashboard updates.\n\n- Collaborate with cross-functional teams, including functional consultants, developers, and business stakeholders.\n\n- Identify risks and process improvements for MDM.\n\n- Conduct training sessions for teams on S/4HANA MDM best practices and processes.\n\n- Develop and maintain training materials and documentation.\n\nRequired Skills & Qualifications :\n\n- 7-10 years of experience in SAP Master Data Management (MDM).\n\n- Strong knowledge of SAP S/4HANA, Data Migration, and Rollouts.\n\n- Expertise in data governance, lifecycle management, and compliance.\n\n- Experience in defining data management principles, policies, and lifecycle strategies.\n\n- Ability to monitor data quality with consistent metrics and reporting.\n\n- Familiarity with KANBAN boards, ticketing tools, and dashboards.\n\n- Strong problem-solving and communication skills.\n\n- Ability to track and manage MDM objects, ensuring timely delivery.\n\nPreferred Skills :\n\n- Experience in training teams on MDM best practices.\n\nAutomation & Productivity Tools :\n\n- Knowledge of automation and productivity improvement tools.\n\n- Familiarity with ABAP and SQL.\n\n- Experience with SAP Data Services or other data migration tools.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP MDM', 'SAP ERP', 'S/4 HANA', 'SAP Implementation', 'SAP projects']",2025-06-12 06:10:55
SAP RAR Consultant - Support & Integration,Sadup Soft,8 - 10 years,Not Disclosed,['Gurugram'],"- We are seeking a skilled SAP RAR Implementation Consultant with relevant 5+ years of experience to lead the implementation and integration of the SAP Revenue Accounting and Reporting (RAR) module.\n\n- The ideal candidate will have a strong understanding of revenue recognition standards (IFRS 15, ASC 606) and hands-on experience in configuring and deploying SAP RAR solutions.\n\n- SAP FI overall experience 8+ years\n\n- Analyze and document business requirements related to revenue recognition.\n\n- Collaborate with stakeholders to understand the intricacies of revenue recognition processes and translate them into system requirements.\n\n- Design and configure the SAP RAR module to meet business needs, ensuring seamless integration with SAP SD, SAP FI, and other relevant modules.\n\n- Customize the system to comply with IFRS 15 and ASC 606 standards.\n\n- Support data migration efforts to ensure accurate transfer of revenue data into the SAP RAR system.\n\n- Oversee data mapping, cleansing, and validation to maintain data integrity.\n\n- Develop and execute comprehensive test plans to validate the implementation of the SAP RAR module.\n\n- Identify and resolve issues during the testing phase to ensure a smooth deployment.\n\n- Provide training and support to end-users to facilitate the adoption of the SAP RAR system.\n\n- Address user queries and issues post-implementation to ensure ongoing system effectiveness.\n\n- Manage project timelines, deliverables, and documentation.\n\n- Ensure projects are completed on time, within scope, and in accordance with predefined quality standards.\n\n- Analyze and document business requirements for revenue recognition.\n\n- Design and configure the SAP RAR module, ensuring seamless integration with SAP SD, FI, and other modules.\n\n- Support data migration and ensure accurate revenue data transfer.\n\n- Develop and execute test plans to validate the implementation.\n\n- Provide training and post-implementation support to end-users.\n\n- Manage project timelines, deliverables, and documentation.\n\n- Proficiency in SAP RAR configuration and integration.\n\n- Experience with SAP S/4HANA is mandatory\n\n- Excellent communication and project management skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP RAR', 'SAP S/4 HANA', 'SAP SD', 'SAP Integration', 'SAP FI', 'SAP Implementation', 'SAP Support']",2025-06-12 06:10:58
Cloud Engineer,S2 Tech,6 - 11 years,Not Disclosed,[],"Role & responsibilities\nMaintain the AWS resource library using the AWS Cloud Development Kit (CDK) written in typescript\nWrite tests and assertions to harden the resiliency of the code\nAdminister cloud network resources including subnets, routes, DNS, load balancers and firewalls\nTroubleshoot Microsoft .NET application performance and functionality\nIdentify and implement opportunities for quality improvement using test automation\nAdminister and interpret results from performance tools such as X-Ray, CloudWatch Logs, and Dynatrace APM to promote continuous improvement of products to our customers\nOptimize cloud configuration from both a technical and budgetary perspective\nReview and enhance runbooks, knowledge base documentation, and log defects\nEnforce security tool actions for both short term remediation and prevent recurrence\nIdentify and remediate application performance and database performance on AWS EC2, Lambda, and RDS (Relational Database Service) instances\nAssist with moving workloads from on premise data center to cloud\nConfigure and maintain Microsoft .NET workloads running on both EC2 instances and AWS Lambda\nLearn and stay up to date with current AWS best practices and service offerings\nRequired Skills\nA Bachelor's Degree from an accredited college or university with a major in Computer Science, Information Systems, Engineering, or other related scientific or technical discipline or three (3) years of equivalent experience in a related field\nAWS Certification\n\nPreferred candidate profile\nAt least five (5) years of experience administering Amazon Web Services\nAt least two (2) years of experience administering serverless workloads\nAWS Certification in one or more of the following\nAdvanced Networking\nDevOps Engineer Professional\nSolutions Architect - Professional\nAt least one (2) year of writing and supporting AWS CDK code\nAn advanced knowledge of typescript development\nAn advanced knowledge of .NET application development\nAt least two (2) years of experience building CI/CD automation processes\nExperience with software development life cycle (SDLC) and agile/iterative methodologies\nExperience in large-scale migrations such Data Center to Cloud\nStrong technical communication skills, both verbal and written\nExcel at being a team player who excels at knowledge sharing and understanding the interpersonal aspects of working in a team.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Migration', 'AWS', 'Devops', 'Subnetting', 'Cloud Development', 'Aws Certified', 'Javascript', 'DNS', 'Routing', 'Ci/Cd']",2025-06-12 06:11:00
Software Engineer,Maxwell Geosystems,2 - 3 years,Not Disclosed,"['Kochi( Marine Drive )', 'Ernakulam']","1. Design, develop, test, and maintain the web/mobile application using various technologies\n2. Write efficient, reusable, and scalable code to ensure high performance, security and reliability.\n3. Identify and resolve bugs and performance bottlenecks to ensure smooth functionality.\n4. Maintain clear and concise documentation for code, design, and technical processes.\n5. Work closely with other developers, product managers, and designers to understand project requirements and deliver high quality solutions.\n6. Write unit tests, conduct code reviews, and participate in deployment activities to ensure high quality code.\n7. Develop and maintain databases, including schema design, data migration, and query optimization.\n8. Mentoring of junior software engineers.",Industry Type: Engineering & Construction,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PHP', 'Python', 'MYSQL']",2025-06-12 06:11:02
LMS Manager,Rachna Sagar,3 - 5 years,Not Disclosed,['New Delhi'],"Location : Delhi\nSkill Set : Proficiency in LMS Platforms\nJob Summary:\nThe LMS Manager will manage the day-to-day operations of the Learning Management System (LMS), ensuring optimal performance, integration with existing systems, and user support. The role also involves content management, data analytics, project management, and coding for system improvements. This position requires a strong background in educational technology, server management, and technical skills related to LMS platforms and web development.\nKey Responsibilities:\nLMS Management: Oversee daily operations, troubleshoot issues, and ensure system efficiency.\nImplementation & Integration: Collaborate to add new features, integrate systems, and manage data migration.\nUser Support: Train and assist teachers, staff, students, and parents in using the LMS.\nContent Management: Maintain course materials, ensuring compliance with standards.\nAnalytics & Reporting: Generate reports for instructional insights and system effectiveness.\nProject & Vendor Management: Handle LMS-related projects and coordinate with vendors for upgrades and support.\nQuality Assurance: Ensure compliance with educational and regulatory requirements.\nDatabase & Server Handling: Manage cloud/shared hosting, data relationships, and server infrastructure.\nSales Support & Play Store Management: Address sales queries and oversee app publishing.\nCoding & Development: Develop LMS features (30% coding) using PHP, JavaScript, React, Node.js, etc.\nKey Skills & Requirements:\nEducation: Bachelors in Educational Technology, Computer Science, or related field.\nExperience: 3-5 years in LMS management, preferably in K-12 education.\nTechnical Skills: LMS platforms (Canvas, Moodle, Blackboard), database & server management, coding (PHP, React, Node.js).\nSoft Skills: Strong project management, communication, and problem-solving abilities.",Industry Type: Education / Training,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['LMS Management', 'User Support', 'Content Management', 'Coding', 'Sales Support', 'PHP', 'Node.js', 'Quality Assurance', 'React', 'Analytics']",2025-06-12 06:11:05
JDE S&D - Freelancers /Business Analyst,Clarisity,10 - 15 years,Not Disclosed,[],"Oracle JDEdwards Sr. Consultant / Freelancers - Sales & Distribution , Business Analyst\n\nJob description:\n\nOracle JD Edwards Sales and Distribution Management- Inventory, Procurement, Sales & Advance Pricing\nApplications Modules - Primary\nJD Edwards EnterpriseOne Distribution Suite\n- Inventory Mgt.\n- Procurement Management\n- Sales Order Management\n- Advanced Pricing\nApplications/technology domain- secondary\n- Transport / Warehouse Management\n- Business Analysis\nSkills Required :\n- 14+ years of Domain /IT experience (at least 10 years of grounds up experience in JDE Distribution functional and as Business analyst)\n- Graduate / Post graduate with Exposure/knowledge in Manufacturing CPG/ Retail, Materials, Procurement & Sales industry domains\n- Should have experience in Oracle JDE EnterpriseOne Projects (Implementation / Roll Outs /Support / Upgrade) as a functional consultant.\n- Experience in JDE Inventory, Sales, Procurement Management & Advanced Pricing is a must\n- Experiences in Transport management will be an added advantage\n- Excellent business communication skills, negotiation skills, and presentation skills\n- Ability to work within large teams and learn new functionality and modules during the project\n- Should have prior experience of working in Onsite/Offshore model\n- Should have knowledge of ERP implementation activities such as Business requirements & analysis, configuration, Conference Room pilot, Gap fit analysis, Application customization functional specifications, testing, data migration and or conversion\n- Should have Industry recognized certifications.\n\nJob Location - Remote/ WFH\nMode - Freelancer / Contractor\nShift timings - 7 pm IST - 3 am IST (Mon-Fri)\nEarly joiners are preferred.\n\n\nWe look forward to receiving your application! All your information will be kept strictly confidential.\n\nPlease furnish below details while applying with your updated resume:\nTotal Experience (yrs) -\nRelevant Experience in JDE SD -\nCurrent CTC -\nExpected CTC -\nNotice period -\nCurrent Location -\n\nThanks & Regards,\nKavita Tikone | Manager - HR & Recruitment\nM: +91 9819386379\neMail: Kavita.tikone@clarisity.com\nWeb: www.clarisity.com",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Freelance/Homebased","['Procurement', 'Inventory Management', 'Advanced Pricing', 'Taxation', 'JDE Sales & distribution', 'Business Analysis']",2025-06-12 06:11:07
Laravel Backend Developer,Ai Mobile Idea Box,3 - 5 years,4-7 Lacs P.A.,[],"URGENT HIRING (REMOTE)\n\nPayrollNinja is a saas provider and software development house. We're looking for an experienced Backend Laravel developer to join our IT team. You will be responsible for the backend logic of our web applications.\n\nIf you have an excellent and strong coding skills and passion for developing challenging web applications or improving existing ones, let's talk. As a backend Laravel developer, you'll work closely with our team to ensure system consistency and improve user experience.\n\nUltimately, you should be able to develop and maintain functional and stable web applications to meet our company's needs.\n\nOur team are actively working on plantation based ERP which consists of many modules from operational to accounting. Potential to work on-site (Malaysia) if you've proven skillful.\n\n\nResponsibilities\nParticipate in the entire application lifecycle, focusing on coding and debugging.\nWrite clean code to develop functional web applications.\nTroubleshoot and debug applications.\nPerform tests to optimize performance and security.\nManage cutting-edge technologies to improve legacy applications.\nCollaborate with Front-end developers to integrate user-facing elements with server side logic.\nGather and address technical and design requirements.\nBuild reusable code and libraries for future use.\nContinuously propose system enhancement and implement security patches.\nFollow emerging technologies.\nCommunicate with client related to system development requirements and progress update.\n\nRequirements\nBachelors degree in computer programming, computer science, or a related field.\nProven work experience as a laravel backend developer (minimum 3 years)\nIn-depth understanding of the entire web development process (design, development and deployment)\nFluent in scripting languages like PHP, Java, Phyton or Node.js. (PHP is compulsory)\nExperience with SQL, MySQL and Oracle database systems (MySQL is compulsory)\nExpert in PHP Laravel framework and API integration\nFamiliar with Amazon Web Services (AWS)\nVersion control, such as Git, CVS or SVN\nFamiliarity with front-end languages (e.g. HTML, JavaScript, Jquery and CSS)\nExcellent analytical and time management skills\nTeamwork skills with a problem-solving attitude\nHigh attention to detail\nCritical thinker and can analyse client documents thoroughly.\nExcellent communication skills (English is compulsory)\nGood internet speed and fine working laptop condition is compulsory for remote job.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Php Laravel', 'Laravel', 'Api Integration', 'JQuery', 'Git Version Control', 'Software Development', 'GIT', 'Git Repository', 'MySQL', 'Web Application', 'Bootstrap', 'Html And Css', 'Ajax']",2025-06-12 06:11:09
Partner Alliance - Solutions,Prodapt Solutions,4 - 7 years,Not Disclosed,['Chennai'],"Overview\n\n2 roles - Presales solution engineers\n\nResponsibilities\n\nknown as a "" Presales Consultant,"" ""Sales Consultant,"" or “Sales Engineer” the Solution Consultant is responsible for leading the solution evaluation throughout the sales cycle and delivering thought leadership to companies to transform their customer’s experience. You will be responsible for developing innovative demonstrations and proof of concepts to prove the value of Salesforce solutions within the context of our customers needs.\nYou will have a proven track record of building innovative multi-product solutions that co-exist with our customers existing environments.\nBeing able to translate our customers business goals into a working demonstrable vision will be a key skill.\n\nYour Responsibilities\nTo develop and present innovative customer solutions to key decision makers to address their business issues and needs whilst showing business value\nCoordinate and own the entire solution cycle through close collaboration with other Salesforce product teams.\nIndustry experience, incorporating and developing a point of view based on Salesforce’s (or similar) solutions.\nTo fully understand and clearly articulate the benefits of Salesforce to customers at all levels, examples include; Administration/IT Staff, managers and ""C"" level executives.\nDevelop new go to market propositions that reflect changing market demands. Such as how AI can benefit our industries.\nDisplay initiative, and self-motivation while delivering high-quality results along with meeting all expectations for both internal and external customers.\nHave a strong interest in growing your career and participating in our internal training programs and mentorship initiatives.\n\nPersonality Attributes/Experience\nExperience will be evaluated based on alignment to the core competencies for the role (e.g. extracurricular leadership roles, military experience, volunteer work, etc.).\nThis role is within our Automotive sector to help to lead UK OEM and Retailer organisations to implement Digital Transformation strategies and deliver more success to their customers. An understanding of these industries is essential.\nAwareness of industry trends, challenges, and common business solutions in use for both Auto OEMs and retailers.\nTrack record of solution engineering, consultancy, or delivery for an enterprise software solution organisation that works within the Auto industry. We are open to a variety of backgrounds for the role.\nSolid oral, written, presentation and interpersonal communication and relationship skills.\nProven time management skills in a dynamic team environment.\nAbility to work as part of a team to solve problems in multifaceted, energizing environments.\nInquisitive, practical and passionate about technology and sharing knowledge.\nLikes to be the first to know something and to understand why and how things happen.\nGood at searching out information and experimenting likes to concentrate on a particular topic and solve puzzles.\nGood at explaining ideas and finding ways to keep people’s attention.\nWilling and able to travel as needed.\n\n\nhiring a ServiceNow Presales Solution Consultant to join their team in London. The successful candidate will drive enterprise-wide transformations, manage relationships with C-level executives and create innovative solutions that elevate the digital experience.\n\nResponsibilities\nAs the ServiceNow Presales Solution Consultant, you will lead sal es and business development initiatives, uncovering new opportunities for ServiceNow solutions.\nLead the pre-sales journey, from crafting proposals to negotiating contracts and securing sign-offs.\nPartner with sales teams to define and present ServiceNow solutions to potential clients.\nServe as a trusted advisor to senior stakeholders, including C-suite executives and IT leaders.\nBuild and maintain strong relationships with ServiceNow leadership and strategic partners for seamless collaboration.\nCollaborate with key business functions (e.g. HR, IT, customer service) to customize ServiceNow solutions for their specific needs.\nStay up to date on market trends and ensure ServiceNow offerings are aligned with evolving business needs.\n\nSkillset\nMaster’s or Bachelor’s degree in Information Technology or similar.\nExtensive knowledge of ITSM, ITOM, CSM, HRSD, Employee Service Center, Case & Knowledge Management and Performance Analytics.\nProficient in system integration, data migration, automation, scripting and ServiceNow customization.\nUnderstanding of HR processes, employee engagement strategies and workflow design.\nSkilled in mapping customer journeys, improving service delivery and boosting customer satisfaction.\nExperienced in demonstrating the value of ServiceNow and advising clients on strategic implementation.\nStrong problem-solving abilities, with a knack for assessing business needs and proposing effective solutions.\nProven experience in driving adoption of new technology and training end-users for smooth transitions.\nCapable of engaging with stakeholders up to the CXO level, simplifying technical concepts for non-technical audiences, and fostering collaboration across teams.\nPreferred CertificationsServiceNow Certified System Administrator (CSA), ServiceNow Certified Implementation Specialist (CIS), ServiceNow Certified Application Developer (CAD), Customer Service Management (CSM) / CRM, Human Resources Service Delivery (HRSD).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['presentation skills', 'servicenow', 'itsm', 'itom', 'csm', 'performance analytics', 'cmdb', 'hrsd', 'business development', 'data migration', 'knowledge management', 'problem management', 'javascript', 'system integration', 'incident management', 'html', 'servicenow development', 'itil']",2025-06-12 06:11:12
SQL Server & Oracle Developer,Mobile Programming,3 - 8 years,Not Disclosed,['Mumbai'],"We are seeking a skilled SQL Server Oracle Developer to join our team.\nThe ideal candidate will have strong expertise in working with SQL Server and Oracle databases, capable of managing and optimizing complex database systems.\nYou will be responsible for writing efficient queries, designing and maintaining database structures, and collaborating with cross-functional teams to ensure seamless integration of database solutions.\nKey Responsibilities:\nDevelop, manage, and optimize SQL queries and stored procedures for SQL Server and Oracle databases.\nDesign, implement, and maintain database systems ensuring high performance, security, and scalability.\nCollaborate with developers to integrate backend services with databases.\nTroubleshoot and resolve performance bottlenecks and database-related issues.\nPerform database backup and recovery, ensuring data integrity and availability.Assist in database migrations and version upgrades.\nProvide support for database-related queries and performance tuning.Ensure compliance with data security and privacy policies.\nRequired Skills:3+ years of experience in SQL Server and Oracle database management.\nStrong proficiency in T-SQL and PL/SQL for writing queries, stored procedures, and functions.In-depth understanding of database design, normalization, and optimization techniques.Experience with performance tuning, query optimization, and database indexing.\nFamiliarity with data migration and database backup/recovery processes.Ability to troubleshoot and resolve complex database issues.\nKnowledge of database security practices and ensuring compliance.\nStrong communication and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'T-SQL', 'PL/SQL', 'Oracle database management', 'Oracle databases']",2025-06-12 06:11:14
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Nashik'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:11:17
Oracle Fusion Technical Consultant ( OIC Specialist ),Strawberry Infotech Pvt. Ltd.,4 - 9 years,Not Disclosed,['Gurugram'],"Role & responsibilities\nKey Responsibilities:\nDevelop and implement integrations using Oracle Integration Cloud (OIC) including REST, SOAP, File, and FTP adapters.\nWork on BIP Reports, OTBI Reports, Fast Formulas, and Personalizations in Oracle Fusion.\nCollaborate with functional consultants and business stakeholders to gather requirements and translate them into technical solutions.\nDesign and develop data migrations using FBDI/ADFDi, and support data conversion and transformation activities.\nTroubleshoot and resolve integration issues, provide performance tuning, and ensure system stability.\nImplement security policies, error handling, and logging for integrations.\nParticipate in unit testing, integration testing, and deployment activities.\nSupport post-implementation, production support, and ongoing enhancements.\nRequired Skills:\n4+ years of experience as an Oracle Fusion Technical Consultant.\nStrong expertise in Oracle Integration Cloud (OIC) REST APIs, SOAP Web Services, File/FTP, and Process Automation.\nExperience with Oracle Fusion SaaS modules like HCM, Finance, or SCM.\nGood knowledge of PaaS components (Visual Builder, PCS, etc.).\nStrong SQL/PLSQL skills and knowledge of Web Services and XML/XSDs.\nExperience with Reports (BIP, OTBI) and Data Extracts.\nHands-on experience in FBDI, HDL, and Data Migration processes.\nGood understanding of Oracle Fusion Cloud data models and structures.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oic', 'Oracle Fusion', 'Fusion Cloud']",2025-06-12 06:11:20
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Surat'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:11:22
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Visakhapatnam'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:11:24
SAP Group Reporting,SRSInfoway,4 - 9 years,Not Disclosed,"['Noida', 'Hyderabad', 'Pune']","Job Title: SAP Group Reporting\nExperience: 4 to 7 Years\nLocation: Noida / Hyderabad / Pune\nEmployment Type: Full-time\nNotice Period: Immediate to 20 Days\n\nJob Summary:\nWe are seeking a highly skilled SAP Group Reporting Consultant with hands-on experience in implementing Group Reporting solutions, data migration, and strong knowledge of SAP BPC or SAP FICO. The ideal candidate should have deep expertise in financial consolidation concepts, configuration, and technical migration at the table level.\n\nKey Responsibilities:\nLead and implement SAP Group Reporting (GR) solutions, including configuration and testing.\nManage end-to-end data migration from legacy systems such as SAP BPC or SAP FICO into Group Reporting tables.\nUnderstand and configure GR master data, dimensions, consolidation units, and hierarchies.\nEnsure data integrity and accuracy throughout the migration and consolidation processes.\nWork closely with business stakeholders to understand financial consolidation requirements and design scalable GR solutions.\nConduct technical validations and reconciliation during migration.\nProvide support during UAT and post-go-live phases.\nParticipate in client interviews and demonstrate strong communication and consulting capabilities.\n\nMust-Have Skills:\nProven experience in SAP Group Reporting implementation projects.\nStrong knowledge in data migration from BPC or FICO to Group Reporting, including table-level understanding.\nSolid background in SAP BPC (Business Planning and Consolidation) or SAP FICO (Financial Accounting and Controlling).\nDeep understanding of FICO consolidation concepts and financial closing procedures.\nStrong analytical and troubleshooting skills.\nExcellent verbal and written communication skills must be able to represent the project during client interactions.\n\nPreferred Qualifications:\nSAP certification in Group Reporting, BPC, or FICO.\nExperience in S/4HANA Financials.\nExposure to SAP Analytics Cloud (SAC) for reporting (optional)\n\n\nMail: krishna.jothi@srsinfoway.com",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Sap Group Reporting', 'FICO', 'FI', 'Co']",2025-06-12 06:11:27
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Delhi / NCR'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:11:29
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Indore'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:11:31
Onix is Hiring MDM Informatica Developer,Onix,3 - 8 years,Not Disclosed,"['Hyderabad', 'Pune']","Job Summary:\nWe are seeking a highly skilled Informatica MDM Developer to join our data integration and management team. The ideal candidate will have extensive experience in Informatica Master Data Management (MDM) solutions and a deep understanding of data quality, data governance, and master data modeling.\nKey Responsibilities:\nDesign, develop, and deploy Informatica MDM solutions (including Hub, IDD, SIF, and MDM Hub configurations).\nWork closely with data architects, business analysts, and stakeholders to understand master data requirements.\nConfigure and manage Trust, Merge, Survivorship rules, and Match/Merge logic.\nImplement data quality (DQ) checks and profiling using Informatica DQ tools.\nDevelop batch and real-time integration using Informatica MDM SIF APIs and ETL tools (e.g., Informatica PowerCenter).\nMonitor and optimize MDM performance and data processing.\nDocument MDM architecture, data flows, and integration touchpoints.\nTroubleshoot and resolve MDM issues across environments (Dev, Test, UAT, Prod).\nSupport data governance and metadata management initiatives.\nRequired Skills:\nStrong hands-on experience with Informatica MDM (10.x or later).\nProficient in match/merge rules, data stewardship, hierarchy management, and SIF APIs.\nExperience with Informatica Data Quality (IDQ) is a plus.\nSolid understanding of data modeling, relational databases, and SQL.\nFamiliarity with REST/SOAP APIs, web services, and real-time data integration.\nExperience in Agile/Scrum environments.\nExcellent problem-solving and communication skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['informatica', 'Informatica Mdm', 'Informatica Master Data Management', 'Mdm Informatica', 'Etl Informatica', 'MDM']",2025-06-12 06:11:33
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Ludhiana'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:11:36
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Pune'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'System Implementation', 'QuickBooks', 'NetSuite Implementation']",2025-06-12 06:11:38
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Hyderabad'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n- Configure and refine related workflows within NetSuite\n- Provide post-implementation support and troubleshooting\n- Understand and manage system configuration, data flows, and integration points\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\nQualifications :\n- Strong functional and technical knowledge of NetSuite\n- Experience with ERP data migration, especially from QuickBooks\n- Familiarity with third-party integration tools and NetSuites workflow engine\n- Ability to support post-go-live activities and optimize ERP performance\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:11:40
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Chennai'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n- Configure and refine related workflows within NetSuite\n- Provide post-implementation support and troubleshooting\n- Understand and manage system configuration, data flows, and integration points\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\nQualifications :\n- Strong functional and technical knowledge of NetSuite\n- Experience with ERP data migration, especially from QuickBooks\n- Familiarity with third-party integration tools and NetSuites workflow engine\n- Ability to support post-go-live activities and optimize ERP performance\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:11:43
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Thane'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:11:45
Actuarial Consultant,"Top B2B Corporate, Management Consulting...",4 - 9 years,22.5-30 Lacs P.A.,['Pune'],"Summary:\nResponsible for providing actuarial support across Life, Health, and Retirement domains, including product development, pricing, valuation, financial reporting, and risk management.\n\nKey Responsibilities:\nSupport actuarial functions: pricing, projection, valuation, illustrations, and reinsurance.\nConduct data migration, validation, testing, and documentation.\nAssist in model conversion, regulatory modeling, and system improvements.\nEnhance valuation efficiency and reporting processes.\nAutomate tools and reports; analyze data for business insights.\nSimplify complex actuarial data for cross-functional teams.\nManage multiple projects with a focus on quality and timeliness.\n\nQualifications & Skills:\nBachelor's/Masters in Mathematics, Statistics, Actuarial Science, or related.\nPursuing actuarial exams (minimum 4 passes, CM1 required; CM2 preferred).\nYears of actuarial experience in Life, Health, or Retirement.\nProficient in valuation, modeling, reporting, and pricing.\nSkilled in actuarial software (Prophet, AXIS, MG ALFA, etc.).\nAdvanced Excel/VBA; exposure to R, SAS, or Python.\nStrong analytical and communication skills.",Industry Type: IT Services & Consulting,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Actuarial Reserving Methodologies', 'Health', 'Life', 'Retirement', 'Retirement Planning', 'Valuation']",2025-06-12 06:11:47
Database Administrator(6 months Contract),T2 Innovations,10 - 15 years,Not Disclosed,['Chennai'],"Job Summary:\nWe are looking for an experienced Database Administrator (DBA) with strong expertise in migrating databases from Oracle to MySQL. The role involves hands-on work in schema conversion, data migration, performance tuning, scripting, and post-migration support.\n\nKey Responsibilities:\nLead end-to-end Oracle to MySQL migration\nPerform schema conversion, data export/import, and validation\nUse tools like MySQL Workbench or Oracle SQL Developer\nAutomate tasks using Python, Perl, or Bash\nOptimize queries, indexing, and ensure performance\nMaintain data integrity, security, and compliance\nProvide post-migration monitoring and support\nKey Skills:\nOracle to MySQL Migration\nMySQL Administration\nSchema & Data Conversion\nQuery Optimization & Performance Tuning\nScripting (Python/Perl/Bash)\nPreferred:\nExperience with large datasets and automation tools\nFamiliarity with cloud databases (AWS RDS, GCP SQL)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Oracle to MySQL Migration', 'MySQL Administration', 'Schema & Data Conversion', 'perl', 'Bash', 'Performance Tuning', 'Scripting', 'Python']",2025-06-12 06:11:49
Specialist Application Management- Kochi,StradaGlobal,8 - 13 years,Not Disclosed,['Kochi'],"Our story\nStrada is a technology-enabled, people powered company committed to delivering world-class payroll, human capital management, and financial management solutions to organizations globally. With a team of more than 8,000 experts and over 30 years of expertise, Strada blends leading-edge technology with human ingenuity to help businesses across the globe design and deliver at scale. Supporting over 1,400 customers in 33 countries, Strada partners with customers at every stage of their journey, to help drive their vision forward. Its why were so driven to connect passion with purpose. Our teams experience in human insights and cloud technology gives companies and employees around the world the ability to power confident decisions, for life.",,,,"['Application Support', 'Application Support Management', 'Application Management', 'Power Bi', 'Application Implementation', 'Cpq', 'Servicenow', 'Workday Financials', 'JIRA', 'Salesforce']",2025-06-12 06:11:52
Python Technical Lead Developer - Only Male candidates,AM Infoweb,2 - 7 years,Not Disclosed,['Pune( Kalyani Nagar )'],"Job Title: Technical Lead Python Developer\nLocation: Kalyani Nagar, Pune\nShift Timing: 3:00 PM to 12:00 AM IST\nAbout the Role:\nWe are looking for a highly skilled and passionate Python Technical Lead to join our growing team at AM Infoweb. This role is purely backend and hands-on coding focused, and ideal for someone who is not only technically strong but also has the capability to design, architect, and lead backend projects.\n\nAs a Technical Lead, you'll take ownership of the backend architecture, manage complex data-driven systems, and guide the team through technical challenges using best-in-class tools and practices.\n\nKey Responsibilities:\nLead backend development efforts using Python and associated frameworks.\nDesign, develop, and maintain scalable backend architecture for web and AI-based applications.\nArchitect solutions around given datasets and business logic.\nGuide junior developers, conduct code reviews, and ensure best coding practices.\nCollaborate with cross-functional teams including AI engineers, frontend developers, DevOps, and product owners.\nWork with CI/CD pipelines to ensure rapid and stable deployment cycles.\nIntegrate and manage databases such as MySQL, PostgreSQL, and MongoDB.\nDeploy, monitor, and maintain services on AWS and Azure cloud platforms.\n\nTechnical Requirements:\nStrong experience with Python backend frameworks Django, Flask, FastAPI (at least two).\nExperience with relational and non-relational databases MySQL, PostgreSQL, MongoDB.\nSolid understanding of API development, RESTful services, and system architecture.\nExperience working with cloud platforms AWS, Azure.\nFamiliarity with CI/CD tools and DevOps practices.\nStrong problem-solving and communication skills.\n\nPreferred Qualifications:\n6+ years of experience in backend development.\n12 years in a technical leadership role.\nExposure to AI and machine learning integrations is a plus.\nExperience with microservices and containerization (Docker, Kubernetes) is an advantage.\n\nWhat You Get:\nInternational exposure to global projects and clients.\nWork with trending AI tools and intelligent bots.\nOnsite cafeteria and break facilities.\nFun and engaging team events and celebrations.\n\n* Know your organization - https://www.aminfoweb.in/\n* Know your workspace! - https://www.youtube.com/watch?v=T1UKFelepCk\n* Our Annual RNR'2022 - https://www.youtube.com/watch?v=T1UKFelepCk\n* Exploring the myths surrounding outsourced healthcare management - https://www.youtube.com/watch?v=fwf3jFa2T-A",Industry Type: Analytics / KPO / Research,Department: Product Management,"Employment Type: Full Time, Permanent","['Python', 'Data Structures And Algorithms', 'Lead Architect', 'Django', 'Postgresql', 'MySQL', 'FastAPI', 'MongoDB', 'Data Architecture', 'AWS', 'Flask', 'Backend Development']",2025-06-12 06:11:55
Snowflake Expert,Tekskills India,6 - 11 years,Not Disclosed,['Hyderabad'],"Location: Hyderabad (Preferred) / Bangalore (If strong candidates are available)\nType: Contractual\nKey Responsibilities:\nUse data mappings and models provided by the data modeling team to build robust Snowflake data pipelines.\nDesign and implement pipelines adhering to 2NF/3NF normalization standards.\nDevelop and maintain ETL processes for integrating data from multiple ERP and source systems.\nBuild scalable and secure Snowflake data architecture supporting Data Quality (DQ) needs.\nRaise CAB requests via Carriers change process and manage production deployments.\nProvide UAT support and ensure smooth transition of finalized pipelines to support teams.\nCreate and maintain comprehensive technical documentation for traceability and handover.\nCollaborate with data modelers, business stakeholders, and governance teams to enable DQ integration.\nOptimize complex SQL queries, perform performance tuning, and ensure data ops best practices.\nRequirements:\nStrong hands-on experience with Snowflake\nExpert-level SQL skills and deep understanding of data transformation\nSolid grasp of data architecture and 2NF/3NF normalization techniques\nExperience with cloud-based data platforms and modern data pipeline design\nExposure to AWS data services like S3, Glue, Lambda, Step Functions (preferred)\nProficiency with ETL tools and working in Agile environments\nFamiliarity with Carrier CAB process or similar structured deployment frameworks\nProven ability to debug complex pipeline issues and enhance pipeline scalability\nStrong communication and collaboration skills",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'ETL', 'aws services', 'SQL']",2025-06-12 06:11:57
Informatica SAP Expert,Randomtrees,3 - 6 years,Not Disclosed,['Hyderabad'],"The Data Steward will play a critical role in ensuring data integrity, quality, and governance within SAP systems.\nThe responsibilities include:\nData Governance:\no Define ownership and accountability for critical data assets to ensure they are effectively managed and maintain integrity throughout systems.\no Collaborate with business and IT teams to enforce data governance policies, ensuring alignment with enterprise data standards.\nData Quality Management:\no Promote data accuracy and adherence to defined data management and governance practices.\no Identify and resolve data discrepancies to enhance operational efficiency.\nData Integration and Maintenance:\no Manage and maintain master data quality for Finance and Material domains within the SAP system.\no Support SAP data migrations, validations, and audits to ensure seamless data integration.\nCompliance and Reporting:\no Ensure compliance with regulatory and company data standards.\no Develop and distribute recommendations and supporting documentation for new or proposed data standards, business rules, and policies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'SAP data migrations', 'Data Maintenance', 'Data Quality Management', 'Data Integration']",2025-06-12 06:11:59
PHP Backend (Platform) Developer,Arbor,6 - 11 years,9-14 Lacs P.A.,['Thiruvananthapuram'],"Role & responsibilities\nAbout you\nExperience of PHP at scale through frameworks such as Symfony /Laravel\nExperience of distributed cloud systems, and specifically Amazon Web Services\nEnterprise Software design patterns and their implementation in real-world enterprise systems\nExperience of message queuing and/or streaming systems such as SQS, ActiveMQ, Apache Kafka, AWS Kinesis, AWS Firehose\nUnderstanding of relational database technologies and their cloud versions (e.g. AWS MySQL Aurora)\nExperience with DataDog, Prometheus or similar observability tools\nA positive and proactive attitude to problem solving\nA team player, willing to muck in and help others when needed, driven personality who asks questions and actively participates in discussions\nCore responsibilities\nDevelop core platform components to aid reusability and stability of the system\nWork with Head of Platform Engineering/SRE to identify and progress platform improvements related to stability, scalability, and performance\nWork with the QA automation framework to ensure functionality is delivered to a high quality\nWork with DevOps Engineers to understand application impacts and system performance and stability, and work with engineering teams to rectify\nAssist in incident response and resolution, and subsequent post-mortems and retrospectives\nContribute to the platform code base and framework which is used by Product Engineers across Engineering\n\nPreferred candidate profile\n\nBonus skills\nPast experience with enterprise solutions running at scale\nFamiliarity with Scrum methodology or other agile development processes\nExperience with Docker and containerization\nExperience with AWS or other Cloud Infrastructure\nFamiliarity with software best practices such as Refactoring, Clean Code, Domain-Driven Design, Test-Driven Development, etc.",Industry Type: Education / Training,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['PHP', 'Symfony Framework', 'Symfony', 'OOPS', 'MVC Framework', 'Oops Design Patterns', 'Yii', 'Zend Framework', 'Oops Programming', 'Laravel', 'Core PHP']",2025-06-12 06:12:01
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Jaipur'],"Responsibilities :\n- Review configurations done by the implementation team and offer recommendations\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n- Configure and refine related workflows within NetSuite\n- Provide post-implementation support and troubleshooting\n- Understand and manage system configuration, data flows, and integration points\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\nQualifications :\n- Strong functional and technical knowledge of NetSuite\n- Experience with ERP data migration, especially from QuickBooks\n- Familiarity with third-party integration tools and NetSuites workflow engine\n- Ability to support post-go-live activities and optimize ERP performance\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:12:03
Sap MM Consultant,Trident Group,4 - 9 years,18-25 Lacs P.A.,"['Barnala', 'Budhni']","Job Title: SAP MM Consultant\nJob Location: Punjab, Madhya Pradesh\nJob Type: Full-Time\nJob Summary: We are looking for an experienced SAP MM Consultant to join our team. The ideal candidate will have a deep understanding of the SAP Material Management module, along with strong analytical skills and the ability to translate business requirements into effective ERP solutions. Your role will involve configuring and implementing the SAP MM module, providing user support, and ensuring seamless integration with other SAP modules.",,,,"['SAP MM Configuration', 'SAP MM Module', 'Problem Solving Skills', 'SAP MM Implementation']",2025-06-12 06:12:05
Specialist Salesforce Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role the Salesforce Specialist IS Engineer for the MyAmgen Product Team will help in driving high-quality, efficient delivery for the companys intranet. This role ensures the development team adheres to best practices and definitions of done while proactively identifying technical debt and collaborating with the Architect and Product Owner to prioritize its resolution. The engineer has expertise in the Salesforce development stack and data model and leverages AI and automation to enhance quality and speed. They take a hands-on approach to innovation, working on proofs of concept (PoCs) to validate the technical feasibility of backlog items. Additionally, the role involves designing and breaking down technical work to minimize dependencies and improve team flow during sprints. The Salesforce Development Lead also supports testing and validation to ensure reliable and impactful deliverables.\nRoles & Responsibilities:\nDeliver high-quality Salesforce solutions using LWC, Apex, Flows and other Salesforce technologies.\nEnsure alignment to established best practices and definitions of done, maintaining high-quality standards in deliverables.\nTake architectural design and translate to code deliverables\nCreate user stories that effectively describe business and technical needs\nProactively identify technical debt and collaborate with the Architect and Product Owner to prioritize and address it effectively.\nInnovate and improve development workflows by leveraging AI and automation tools to increase efficiency, speed, and quality.\nDesign and decompose technical tasks to minimize interdependencies and optimize the team's workflow during sprints.\nSupport the testing and validation of deliverables to ensure reliability, performance, and alignment with business goals.\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The Salesforce professional we seek is a type of person with these qualifications.\nBasic Qualifications:\nMasters degree and 4 to 6 years of experience in Computer Science, IT or related field OR\nBachelors degree and 6 to 8 years of experience in Computer Science, IT or related field OR\nDiploma and 10 to 12 years of Computer Science, IT or related field experience\nFunctional Skills:\nMust-Have Skills:\nExperience with Apex, JavaScript, and Lightning Web Components (LWC)to create scalable applications\nExperienced in using CI/CD automation to deploy Salesforce code and configurations\nStrong understanding of declarative tools like Flows and Process Builder\nProficiency in using Salesforce tools such as SOQL, SOSL, and Data Loader to query, manipulate and export data\nGood ability to lead development teams and collaborate with Product Owners and Architects for task prioritization and execution\nAbility to train and guide junior developers in best practices\nHands-on experience in testing, debugging, and validating deliverables for reliability and performance\nFamiliarity with Agile practices such as User Story Creation and, sprint planning\n\nGood-to-Have Skills:\nExperience with using Copado for CI/CD automations\nExperience using AI, automation, or cutting-edge Salesforce tools.\nProficiency in data migration, modeling, and security configurations.\nExperience in fine-tuning Salesforce org for scalability and speed.\nFamiliarity with industry-specific Salesforce tools like Health Cloud or Financial Services Cloud.\nAbility to identify and resolve technical debt while designing scalable, maintainable solutions\nExperience creating proofs of concept (PoCs) to validate new ideas or backlog items.\n\nProfessional Certifications (preferred):\nSalesforce Advanced Administrator\nSalesforce Developer I and II\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong written and verbal communications skills (English) in translating technology content into business-language at various levels\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong time and task management skills to estimate and successfully meet project timeline with ability to bring consistency and quality assurance across various projects.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Salesforce', 'SOQL', 'Lightning Web Components', 'SOSL', 'JavaScript', 'CI/CD', 'debugging', 'Apex']",2025-06-12 06:12:07
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Ahmedabad'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:12:10
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Nagpur'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:12:12
D365 F&O Finance Functional Consultant,Claranet Group,5 - 8 years,Not Disclosed,[],"We at Claranet are looking for F&O Finance Functional Professionals who will be involved in all aspects of implementing supporting Dynamics solutions through the project life cycle to go live and ongoing support. You can contribute to Solution Design sessions, help with configuration, help with data migration deliverables, create required interface design and functional design documents, troubleshoot customization, etc.\n\nRole- D365 F&O Functional Consultant (Full Time)\nOffice Location Hitech, Hyderabad - India\nWorking mode : Remote\n\nInterested Candidates can your CV on Rakesh.koyya@claranet.com\n\nKey Responsibilities:\nAnalyze business processes (Finance, PMA) to identify opportunities for improvement.\nIdentify creative workarounds to meet requirements without the development of custom code.\nUnderstand the functional capabilities and limitations for out-of-the-box functionality as well as custom code.\nYou will be the one to identify our customers' requirements and match them with technological capabilities and with Microsoft continuous release plans.\n\nKey Competencies:\nDeep functional knowledge of Microsoft Dynamics 365 Finance and PMA.\nExperience of customized solutions to complex business problems.\nDemonstrable consultancy experience.\nStrong working knowledge of business processes and ER framework.\nRelevant Microsoft certification.\nExcellent documentation and communication skills.\nA logical approach to problem-solving and the structured introduction of change into operational systems.\nAbility to multitask and prioritize.\nGood interpersonal skills.\nAttention to detail.\n\nSkills Required:\nHold 6-8 years of experience within D365 F&O implementation, support\nSpecialized in Finances, PMA, Integration, ER reports\nQualified Chartered Accountant / MBA (Finance) desirable\nAre fluent in English.\nStrong communication and consulting skills\nGlobal Exposure must (Interacting with Global clients/customers)\n\nCompany Benefits:\nGroup Medical Insurance (including parents coverage), Group Term Life Policy and Personal Accidental Policy\nTax Saving flexible benefits\nFlexible working hours\nStatutory Benefits (PF, Gratuity)",Industry Type: Miscellaneous,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['PMA', 'Microsoft Dynamics', 'F&O', 'finance', 'Integration', 'configuration', 'Migration', 'Customization', 'implementation', 'Troubleshooting', 'ER', 'Data Migration', 'functional consultant']",2025-06-12 06:12:15
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Kolkata'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:12:17
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Bhopal'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:12:19
NetSuite Consultant,Zethic,7 - 10 years,Not Disclosed,['Mumbai'],"Responsibilities :\n\n- Review configurations done by the implementation team and offer recommendations\n\n- Execute data migration for 3 - 5 years of historical transactional, customer, and vendor data from QuickBooks to NetSuite\n\n- Collaborate on setting up and optimizing third-party integrations (e.g., HubSpot, Stripe, Bill.com)\n\n- Configure and refine related workflows within NetSuite\n\n- Provide post-implementation support and troubleshooting\n\n- Understand and manage system configuration, data flows, and integration points\n\n- Administer and support the NetSuite ERP system, including user setup, roles, permissions, dashboards, and workflows.\n\n- Monitoring logs and manage exceptions (work with finance stakeholders as needed)\n\n- Facilitation of system adoption and satisfaction through user training, user support, and meeting user requirements\n\nQualifications :\n\n- Strong functional and technical knowledge of NetSuite\n\n- Experience with ERP data migration, especially from QuickBooks\n\n- Familiarity with third-party integration tools and NetSuites workflow engine\n\n- Ability to support post-go-live activities and optimize ERP performance\n\n- Excellent problem-solving and communication skills",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['NetSuite', 'Technical Consultant', 'ERP', 'Data Migration', 'Workflows', 'System Implementation', 'NetSuite Implementation']",2025-06-12 06:12:22
SAP SD Consultant,Trident Group,4 - 9 years,18-25 Lacs P.A.,"['Barnala', 'Budhni']","Job Title: SAP SD Consultant\nJob Location: Punjab, Madhya Pradesh\nJob Type: Full-Time\nJob Summary: We are seeking a dedicated SAP SD (Sales and Distribution) Consultant to implement and support our SAP software solutions. The ideal candidate will have a strong understanding of the SAP SD module, along with excellent analytical skills and the ability to translate business requirements into effective ERP solutions. Your role will involve configuring and implementing the SAP SD module, providing user support, and ensuring seamless integration with other SAP modules.",,,,"['SAP MM Configuration', 'SAP MM Module', 'SAP MM Implementation', 'Material Management', 'Problem Solving Skills']",2025-06-12 06:12:25
Netsuite Developer,Advy Chemical,2 - 4 years,Not Disclosed,['Thane'],"Resolve day-to-day NetSuite issues and user queries efficiently.\nDevelop and maintain NetSuite customizations using SuiteScript (1.0 / 2.0 / 2.1) including Client Scripts, User Events, Suitelets, RESTlets, Scheduled Scripts, and Map/Reduce.\nDesign and configure SuiteFlow workflows, Saved Searches, dashboards, and reports using SuiteAnalytics.\nCustomize forms, fields, roles, permissions, and advanced PDF templates through SuiteBuilder.\nIntegrate NetSuite with external systems using APIs (REST/SOAP) or middleware tools.\nPerform data migrations, sandbox-to-production deployments, and manage post-Go-Live support and improvements.\nMaintain technical documentation, apply version control, and ensure structured deployment processes.\nCollaborate with business stakeholders to translate functional requirements into scalable NetSuite solutions.",Industry Type: Pharmaceutical & Life Sciences,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['ERP System', 'Netsuite Erp', 'Suitescript', 'Suite Builder', 'Javascript', 'Accounting Software', 'Suite Bundler', 'Suiteflow']",2025-06-12 06:12:27
Transportation Master Team Lead,Amgen Inc,7 - 8 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will take ownership of Transportation Master data processes, ensuring the accuracy, consistency, and governance of critical data across the organization. This role will lead data validation, cleansing, and enrichment efforts, and collaborate with cross-functional teams to resolve complex data issues and drive process improvements. The Transportation Master Team Lead will also oversee key performance metrics, ensure compliance with data governance standards, and lead data migration and integration initiatives.\nRoles & Responsibilities:\nLead and manage day-to-day MDM operations, including data validation, cleansing, and enrichment processes\nOversee data governance practices, ensuring compliance with internal standards and regulatory requirements\nCollaborate with cross-functional teams, including IT and business units, to resolve complex data issues and improve data workflows.\nImplement and drive continuous improvements in MDM processes to enhance data accuracy, quality, and operational efficiency.\nLead data migration, integration projects, and system upgrades to ensure seamless data consistency across platforms.\nMonitor and report on key performance indicators related to master data quality and operational success.\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nBachelors in a STEM discipline and 7-8 years of experience in enterprise applications like SAP, and Oracle, proven experience in Transportation Master data management and data governance\nIndustry experience preferably in healthcare or biotech supply chain\nProficient in MS Office, visualization tools like Spotfire, Tableau, Power BI\nStrong analytical skills with the ability to collaborate cross-functionally and resolve data issues.\nExperience in leading and managing successful teams\nPreferred Qualifications:\nMust-Have Skills:\nExpertise in master data management processes and data governance\nSolid understanding of SAP ECC and proven experience in data implementation/integration projects\nMaster data knowledge in the Transportation domain, other domains such as Material, Customer, and Production Master are a plus.\nAbility to lead and collaborate with cross-functional teams to set data strategy for a master domain(s) and drive prioritization across different projects and day-to-day operations\nStrong understanding of data governance frameworks and regulatory compliance standards and regulations (e.g., GDPR, HIPAA, GxP).\nExcellent problem-solving and analytical skills, with a focus on driving continuous improvement in data accuracy and quality\nGood-to-Have Skills:\nSAP S/4, SAP MDG, SAP TM\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills.\nAbility to work effectively with global, virtual teams.\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data management processes', 'SAP ECC', 'Transportation Master data management', 'SAP MDG', 'HIPAA', 'data governance', 'SAP TM', 'SAP S/4', 'GDPR', 'GxP', 'STEM']",2025-06-12 06:12:29
SAP WM Consultant - E2E Implementation,Zettamine Labs,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Summary :\nWe are looking for a highly skilled and experienced SAP WM (Warehouse Management) Consultant to join our growing team.\nThe ideal candidate will have a strong background in SAP WM module implementations, support, and integrations, along with excellent problem-solving and communication skills.\nExperience with EWM is a plus.\nKey Responsibilities :\n- Gather business requirements and translate them into SAP WM solutions.\n- Configure and customize SAP WM modules to meet business needs.\n- Lead and support SAP WM module implementation, rollout, and support projects.\n- Collaborate with cross-functional teams including MM, SD, PP, and EWM.\n- Develop functional specifications for custom developments.\n- Perform system testing, integration testing, and support UAT.\n- Provide training and documentation for end-users and support teams.\n- Resolve incidents and provide ongoing maintenance and support.\n\nRequired Skills :\n- Strong expertise in SAP WM configuration and end-to-end implementation.\n- Hands-on experience with Inventory Management (IM), Inbound/Outbound Logistics, Putaway, Picking, and Stock Transfer Processes.\n- Integration experience with SAP MM, SD, and EWM.\n- Experience with RF devices, barcode scanning, and warehouse automation is an advantage.\n- Good understanding of IDoc handling, interfaces, and data migration.\n- Ability to analyze business processes and map them into SAP WM functionality.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP', 'Technical Consultant', 'SAP SD', 'SAP MM', 'SAP PP', 'SAP Integration', 'SAP EWM', 'SAP Implementation', 'IDoc', 'SAP Support', 'SAP WM']",2025-06-12 06:12:32
DevOps Engineer,Easemytrip,1 - 6 years,Not Disclosed,"['Noida', 'Gurugram']","About the Role:\nAs a DevOps Engineer at EaseMyTrip.com, you will be pivotal in optimizing and maintaining our IT infrastructure and deployment processes. Your role involves managing cloud environments, implementing automation, and ensuring seamless deployment of applications across various platforms. You will collaborate closely with development teams to enhance system reliability, security, and efficiency, supporting our mission to provide exceptional travel experiences through robust technological solutions. This position is critical for maintaining high operational standards and driving continuous innovation.\n\nRole & responsibilities:\nCloud Computing Mastery: Expert in managing Amazon Web services (AWS) environments, with skills in GCP and Azure for comprehensive cloud solutions and automation.\nWindows Server Expertise: Profound knowledge of configuring and maintaining Windows Server systems and Internet Information Services (IIS).\nDeployment of .NET Applications: Experienced in deploying diverse .NET applications such as ASP.Net, MVC, Web API, and WCF using Jenkins.\nProficiency in Version Control: Skilled in utilizing GitLab or GitHub for effective version control and collaboration.\nLinux Server Management: Capable of administering Linux servers with a focus on security and performance optimizations.\nScripting and Automation: Ability to write and maintain scripts for automation of routine tasks to improve efficiency and reliability.\nMonitoring and Optimization: Implement monitoring tools to ensure high availability and performance of applications and infrastructure.\nSecurity Best Practices: Knowledge of security protocols and best practices to safeguard systems and data.\nContinuous Integration/Continuous Deployment (CI/CD): Develop and maintain CI/CD pipelines to streamline software updates and deployments.\nCollaboration and Support: Work closely with development teams to troubleshoot deployment issues and enhance the overall operational efficiency.\n\nPreferred candidate profile:\nMigration Project Leadership: Experienced in leading significant migration projects from planning through to execution.\nDatabase Expertise: Strong foundation in both SQL and NoSQL database technologies.\nExperience with Diverse Tech Stacks: Managed projects involving various technologies, including 2-tier, 3-tier, and microservices architectures.\nProficiency in Automation Tools: Hands-on experience with automation and deployment tools such as Jenkins, Bamboo, and Code Deploy.\nAdvanced Code Management: Highly skilled in managing code revisions and maintaining code integrity across multiple platforms.\nStrategic DevOps Experience: Proven track record in developing and implementing DevOps strategies at an enterprise level.\nConfiguration Management Skills: Proficient in using tools like Ansible, Chef, or Puppet for configuration management.\nTechnology Versatility: Experience working with a range of programming languages and frameworks, including .NET, MVC, LAMP, Python, and NodeJS.\nProblem Solving and Innovation: Ability to solve complex technical issues and innovate new solutions to enhance system reliability and performance.\nEffective Communication: Strong communication skills to collaborate with cross-functional teams and articulate technical challenges and solutions clearly.",Industry Type: Travel & Tourism,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Code Deploy', 'Aws Cloud', 'Ci/Cd', 'Devops', 'Devops Automation', 'Apm Tools', 'Cloud Automation Devops', 'Devops Jenkins', 'Aws Codedeploy', 'Aws Api Gateway', 'Vpc', 'Load Balancing', 'Route3', 'Code Pipeline', 'Auto Scaling', 'Pipeline', 'Elastic Cache', 'Aws Devops', 'Aws Lambda', 'Build', 'New Relic', 'Azure Devops', 'Cloudfront']",2025-06-12 06:12:34
Software Engineer Staff,Fourkites,9 - 14 years,30-45 Lacs P.A.,[],"At FourKites we have the opportunity to tackle complex challenges with real-world impacts. Whether its medical supplies from Cardinal Health or groceries for Walmart, the FourKites platform helps customers operate global supply chains that are efficient, agile and sustainable.\nJoin a team of curious problem solvers that celebrates differences, leads with empathy and values inclusivity.\nAs a Staff Software Engineer, you will get an opportunity to work on features end to end (backend & frontend) using the latest technologies such as RoR, Java, GoLang, Angular, React, Redis, PostgreSQL. You will develop products that can change the logistics landscape and will be used by some of the biggest corporations in the world. You will develop integrations with our strategic partners to help expand our ecosystem. You will work closely with our US team and customers to develop features that help shape the logistics and supply chain industry.\nWho you are:\nBachelor’s degree in Computer Science & Engineering or related field from a reputed institution.\nMinimum of 8 years of experience in Software Engineering and Web application development.\nGood understanding of software design, Microservices architecture, object-oriented principles, and design patterns.\nExperience with Design and development of highest quality software/services using RoR/Golang/Java.\nGood knowledge of RESTful APIs and microservices architecture\nStrong understanding of Java, Spring Framework, and object-oriented programming principles\nExperience in one of Azure, Amazon Web Services or other cloud services.\nExperience with databases such as MySQL, PostgreSQL, or MongoDB\nFamiliarity with front-end technologies such as HTML, CSS, and JavaScript is a plus\nStrong knowledge of Git (branches, submodules, rebasing) and other Agile tools such as JIRA & Confluence.\nAgile SDLC experience\nExcellent oral and written communication skills\nWhat you’ll be doing:\nDesign, architect, implement, test, profile, release, and optimize highest quality software/services using RoR/Golang/Java.\nPartner with product manages to analyse product requirements and plan engineering execution\nDocument HLD/LLD for easy knowledge sharing and future scaling\nPerform design and code reviews\nImplement code with very high coverage of unit tests and component tests\nCross-training peers and mentoring teammates\nPossess expert knowledge in performance, security, scalability, architecture, and best practices\nFunctionally decompose complex problems into simple, straight-forward solutions\nCollaborate with UX designers to develop responsive user interface components\nWorking knowledge of SQL based (any RDBMS) and NOSQL data stores (any one) with the ability to write intermediate level SQL\nExperience in building Web application backends using Java Spring Boot or similar\nExperience with frontend libraries/frameworks such as React/Angular is a plus.\nEducation Qualification: Graduate from B.E/ B.Tech / MCA / M.Tech Background.\nWho we are:\nFourKites®, the leader in AI-driven supply chain transformation for global enterprises and pioneer of real-time visibility, turns supply chain data into automated action. FourKites’ Intelligent Control Tower™ breaks down enterprise silos by creating a real-time digital twin of orders, shipments, inventory and assets. This comprehensive view, combined with AI-powered digital workers, enables companies to prevent disruptions, automate routine tasks, and optimize performance across As the leader in AI-driven supply chain transformation, FourKites pioneered the Intelligent Control Tower™ powered by the world’s largest real-time visibility network. Our platform creates comprehensive digital twins of your supply chain with AI-powered digital workers to automate resolution, improve collaboration and drive outcomes across all stakeholders. Unlike traditional control towers, we enable true real-time execution and intelligent fulfillment, transforming both your supply and customer\nBenefits\nMedical benefits start on first day of employment\n36 PTO days( Sick, Casual and Earned) , 5 recharge days, 2 volunteer days\nHome Office setups and Technology reimbursement\nLifestyle & Family benefits\nAnnual Swags/ Festive Swags\nOngoing learning & development opportunities ( Professional development program, Toast Master club etc.)",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Golang', 'Casandra', 'SQL Database', 'Ci/Cd', 'Elastic Search', 'Python']",2025-06-12 06:12:37
Hiring SCM Functional Lead - US Shift - Ahmedabad/Pune/Remote,Synoptek,7 - 12 years,Not Disclosed,[],"Lead Functional Consultant, SCM & Manufacturing\nThis is an amazing opportunity to work within one of the fastest growing Managed Services Providers. We are a company with a heart and soul dedicated to the ongoing success and growth of our employees and continued business success of the customers we support. We foster a fun and connected environment with employee benefits extending beyond general compensation and into company sponsored events and an invested culture of learning.\nThe Lead Functional Consultant, SCM & Manufacturing, is responsible for leading the end-to-end project lifecycle, from project conception to successful implementation, specifically focusing on Supply Chain Management (SCM) and Manufacturing solutions. This role requires a deep understanding of Microsoft Dynamics AX/ F&SCM, strong leadership skills, and the ability to coordinate and manage both functional teams and project scope effectively.",,,,"['Microsoft Dynamics', 'SCM', 'dynamics', 'Supply Chain Management']",2025-06-12 06:12:40
SAP Implementation Engineer,Emkay Placement Consultants,10 - 12 years,10-13 Lacs P.A.,['Kolkata'],"Lead SAP team for implementing SAP modules FICO, MM, SD, PM, PP, QM, PS .Understand business & technology ,digital platforms & drive new initiatives such as GRC, SAP Rise ,Minimize SAP run cost ,ROI analyses for SAP spending and initiatives,\n\nRequired Candidate profile\nimplementing SAP modules FICO, MM, SD, PM, PP, QM, PS .Understand modern business & technology framework, drive new initiatives such as GRC, ROI analyses for SAP spending and initiatives,",Industry Type: Building Material (Cement),Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Sap Data Migration', 'Sap Hana', 'SAP Support', 'Roi Analysis', 'SAP Implementation', 'SAP ECC', 'PP', 'PS', 'Time Management', 'SAP PP', 'Sap S Hana', 'FICO', 'SD', 'Execution', 'MM', 'ECC', 'SAP Security', 'Scheduling', 'ROI', 'Schedule', 'SAP Basis', 'GRC', 'Digitization', 'SAP Production Planning', 'Planning', 'Qm', 'SAP Quality Management']",2025-06-12 06:12:43
Salesforce Developer,Leading Client,5 - 10 years,Not Disclosed,['Hyderabad'],"Name of Projects in which the skills were used (add rows if necessary)\nNo: of months worked in each Project of work done using the skill (Mandatory/ optional)\nApex Customization - M\nSalesforce Configuration - M\nLWC - M\nJavascript/Jquery - M\nSlack - O\nMulesoft - O\nDesign, develop, and deploy customized Salesforce solutions using Apex, Visualforce, and Lightning components Collaborate with stakeholders to gather requirements and translate them into technical designs Create and maintain workflows, process builders, triggers, and validation rules Perform data migration using Data Loader or third-party tools Integrate Salesforce with external systems via REST/SOAP APIs Ensure unit testing, deployment through CI/CD tools, and post-deployment support Maintain platform security and ensure adherence to Salesforce best practices",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Salesforce', 'Lightning Web Components', 'Slack', 'JavaScript', 'Jquery', 'Salesforce Lightning', 'Mulesoft']",2025-06-12 06:12:45
Dot Net Fullstack Developer,BriskWin IT (BWIT),6 - 11 years,Not Disclosed,"['Pune', 'Chennai', 'Mumbai (All Areas)']","Req 1. Microsoft Cloud .Net + Angular + React (Basics)\nExp: 6 to 12 Yrs\nNP : Immediate to 15 Days (Serving Notice Max 20 Days)\nLocation Chennai / Mumbai / Pune / Bangalore\n\nReq 2. API Technical Lead Azure Cloud Integration\nExp: 9+ yrs\nNP : Immediate to 30 Days\nLocation: Chennai, Mumbai, Pune, Noida, Ahmadabad, Bangalore\n\nKey Skills & Experience:\n\nProven experience in a technical leadership role focused on backed/API development, with strong proficiency in Java.\nIn-depth knowledge of Microsoft Azure services, including:\nAzure Table Storage\nAzure Data Lake Storage Gen2\nAzure Functions\nAzCopy for efficient data migration to Azure( Good to have )\nStrong grasp of Restful API design principles, cloud-native architectures, and server less computing models",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['.Net Core', 'Cloud', 'React.Js', 'Angular']",2025-06-12 06:12:47
