title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Machine Learning Engineer,Draft N Craft Legal Outsourcing,2 - 3 years,10-15 Lacs P.A.,['New Delhi'],"Role Overview:\n\nAs an ML Engineer you will embark on a wonderful journey of developing and implementing various Machine Learning models that will ultimately act as an enabler in the companys growth.\nSince Draft Craft is a legal services-oriented company in which we serve clients from across the border, your work will be majorly aimed towards creating ML workflows that will serve the legal industry and help improve efficiency of the in-house teams. Draft n Craft offers you the perfect opportunity to grow and hone your ML/Data Engineering skills while contributing towards building a worthwhile product.\n\nKey Responsibilities:\nDeveloping data ingestion & data preprocessing pipelines for transforming data presented for legal requirements to extract key and actionable insights. \nData cleaning for supplying accurate, consistent & relevant content to ML models.\nExploring and experimenting with different ML models and architectures that can be used with data from the legal industry in a safe and compliant manner.\nDeveloping and deploying ML models that can function in production environments for the use-cases required by the company.\nAnalyzing key metrics for model performance and devising methods to improve efficiencies of the models.\nDocument the steps involved in data preprocessing, model development, and optimizations undertaken.\nExplore techniques for feature extraction, transformation, and selection to improve model performance.\nUnderstanding software development related terminologies to collaborate with the existing team of software engineers within the company.\nDevise methods of integration of the ML models within the company’s already developed software solutions and employee workflows.\n\nRequired Qualifications:\nDegree holder from Computer Science Engineering, Data Science or related fields.\nMinimum experience of 2 years working as an ML engineer or Data Scientist in a professional capacity.\nStrong programming skills including python and familiarity with related libraries Tensorflow, PyTorch, Pandas etc.\nDatabase Querying in terms of SQL/NoSQL.\nWorking with data extracting and ETL pipelines for pre-processing of data/documents.\nRelevant experience working in NLP, Neural Networks, and Gen AI technologies.\nFamiliarity with working or applying transfer-learning on LLM models like Llama, etc.\nSome experience in software development is preferred.\nKnowledge of deploying ML models and data pipelines to cloud services like AWS/Azure.",Industry Type: Legal,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'Retrieval Augmented Generation', 'Python', 'NoSQL', 'Large Language Model', 'Data Extraction', 'Machine Learning', 'SQL']",2025-06-12 14:56:22
Machine Learning Engineer,CompIndia,0 - 1 years,Not Disclosed,"['Tirupati', 'Chennai( Aminjikarai )']",Value-Added Skills:\nCompleted Courses in Python & ML\nCompleted Academic projects involving ML\nParticipated in Kaggle competitions and Hackathons\nBuilt ML projects with real-world datasets,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Python', 'Kaggle', 'Hackathon']",2025-06-12 14:56:25
Machine Learning Engineer,goML,3 - 8 years,Not Disclosed,[],"Looking for a culture to thrive & build a rewarding career for yourself, join the core team of young hustlers building the next generation of Machine Learning platform & services. You will develop training and deployment pipelines for machine learning, implement model compression algorithms, and productionize machine learning research solving challenging business problems.\n\nWe are Hiring Machine Learning Engineers at goML!\n\nKey Responsibilities:\nDesign and develop generative AI models using techniques like RAG, transformers, and other relevant approaches.\nFine-tune pre-trained LLMs for specific tasks and domains.\nConduct research on new techniques for improving the performance and capabilities of generative AI models.\nApply software engineering rigor and best practices to machine learning /Generative AI pipelines.\nEvaluate and analyse the performance of ML/generative AI models.\nStay up to date on the latest advancements in generative AI research.\nFacilitate the development and deployment of proof-of-concept Generative AI systems.\n\nQualifications:\nBachelors/Masters degree in Computer Science, Machine Learning, Artificial Intelligence, or a related field.\n3-8 years of experience in generative AI or related fields.\nExperience in building data pipelines, deploying ML/GenAI models in production, and monitoring and maintaining their performance.\nStrong programming skills in Python.\nFamiliarity with RAG and other techniques for building generative models.\nExtensive experience with Git, Docker and a good understanding of Linux for managing servers.\nExperience with cloud-based ecosystems, especially AWS ML/GenAI services.\nExposure to ML/GenAI frameworks and tools.\nExcellent communication and collaboration skills.\nAbility to work independently and in a team-oriented environment.\nMethodical and meticulous towards work and planning.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative AI', 'Machine Learning', 'AWS', 'Natural Language Processing', 'Computer Vision', 'Deep Learning']",2025-06-12 14:56:27
Machine Learning Engineer 2,Adobe,4 - 7 years,Not Disclosed,['Noida'],"Develop algorithms that apply deep learning and innovative methods in NLP & computer vision, combined with traditional large sophisticated solutions/codebases!\nDeveloping innovative solutions using Generative AI, Python, Machine Learning and Data Science!\nBuild experiments, algorithms and ship solution that not only yield high accuracy but are also crafted and engineered to scale.\nCollaborate across multiple research and engineering teams, making the tradeoffs required to rapidly deliver AI/ML software solutions.\nDriven, Energetic and a team player are must haves.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'deep learning', 'data science', 'Machine learning', 'Research', 'Adobe', 'Software solutions', 'Python']",2025-06-12 14:56:29
Machine Learning Engineer,CADFEM India,1 - 4 years,4-9 Lacs P.A.,['Hyderabad'],"Job Description\nDesign and develop machine learning models tailored to mechanical engineering challenges, including predictive modelling, simulation optimisation, and failure analysis.\nUtilise deep learning and other advanced ML techniques to improve the accuracy and efficiency of CAE simulations.\nPreprocess and analyse large datasets from CAE simulations, experimental tests, and manufacturing processes for modelling.\nTrain, validate, and fine-tune machine learning models using real-world engineering data.\nOptimise models for performance, scalability, and robustness in production environments.\nCollaborate with CAE engineers to integrate ML models into existing simulation workflows (e.g., FEA, CFD, structural analysis).\nAutomate repetitive simulation tasks and enable predictive analytics for design optimisation.\nWork closely with mechanical engineers, data scientists, and software developers to identify business challenges and develop data-driven solutions.\nDeploy machine learning models into production environments and monitor their performance.\nMaintain and update models to ensure reliability and continuous improvement.\nStay abreast of the latest advancements in machine learning, AI, and CAE technologies.\nApply innovative approaches to solve complex engineering problems.\nRequirements\nBachelors or Master’s degree in Mechanical Engineering, Computer Science, or a related field\nProven 2-3 years of experience in developing and deploying machine learning models, preferably in mechanical engineering or CAE domain\nHands-on experience with CAE tools such as ANSYS, Abaqus, or similar FEA/CFD software\nStrong programming skills in Python, R, or Java\nProficiency in machine learning frameworks (TensorFlow, PyTorch, scikit-learn)\nExperience with data preprocessing, feature engineering, and statistical analysis\nSolid understanding of mathematics, statistics, and problem-solving skills\nExcellent analytical thinking and ability to tackle complex engineering challenges\nStrong communication and teamwork skills to collaborate across disciplines\nPreferred: Experience with physics-informed machine learning and digital twin technologies\nPreferred: Familiarity with automation of CAE workflows and predictive modelling for product design\n\nBenefits\nChallenging job and a chance to team up with a young and dynamic professional group\nChance to build yourself as WE grow.\nRemuneration that stays competitive and attractive to retain the best.\nOpportunity to join an organization experiencing year on year growth.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ML/DL Engineer', 'AI Engineer', 'Data Scientist', 'Applied Machine Learning', 'Machine Learning Engineer', 'F1-score', 'Tensorflow', 'Data Preprocessing', 'LightGBM', 'Feature Engineering', 'RMSE', 'Numpy', 'Pytorch', 'Pandas', 'FastAPI', 'CatBoost', 'Flask', 'Python']",2025-06-12 14:56:32
Machine Learning Engineer,Adobe,12 - 15 years,Not Disclosed,[],"The engineer will be part of a team working on the development, operations and support of Adobe s AI Platform team. They will be responsible for the design, architecture and development of new features and maintenance of existing features. They will also handle all phases of development, from early specs and definition to release. They are encouraged to be hands-on problem solver and we'll conversant in analyzing, architecting and implementing Golang/python-based world class high-quality software. Prior experience on ML solutions and cloud platform services, workflow orchestrators, data pipeline solutions would be a plus.\n\nWhat you'll Do\nThis is an individual contributor position.\nHands on product/solution development knowledge are a must.\nThe position involves conceptualization of a product, design, development, debugging/triaging, deployment at scale, monitoring, analyzing, etc\nPlanning, effort estimation and risk analysis of a project.\nThe incumbent will plan, evaluate industry alternatives, design and drive new components, solutions, workflow, features, etc\nShould take the initiative to drive frugality through optimizations without compromising stability or resiliency.\n  Requirements\nbachelors / masters degree in engineering.\n12+ years of relevant industry experience.\n3+ years of experience as a lead/architect.\nA proven expertise with building large scale platforms on Kubernetes.\nProven programming skills with languages such as python and go-lang.\nExperience of the latest ML development tools.\nTrack record of delivering cloud-scale, data-driven products, and services that are widely adopted with large customer bases\nExposure to container runtime environments\nExperience in building, deploying, and managing infrastructures in public clouds (specifically AWS)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Debugging', 'Machine learning', 'Cloud', 'Conceptualization', 'Programming', 'Product design', 'Adobe', 'Monitoring', 'Python']",2025-06-12 14:56:34
Machine Learning Engineer,Whats On India Media,3 - 8 years,Not Disclosed,"['Mumbai', 'Gurugram', 'Bengaluru']","Nielsen is seeking an organized, detail oriented, team player, to join the Engineering team in the role of Software Machine Learning Engineer. Nielsen's Audience Measurement Engineering platforms support the measurement of television viewing in more than 30 countries around the world. The Software Engineer will be responsible to define, develop, test, analyze, and deliver technology solutions within Nielsen's Collections platforms.\nRequired Skills\nBachelor's degree in Computer Science or equivalent degree.\n3+ years of software experience\nExperience with Machine learning frameworks and models. Pytorch experience preferred\nStrong understanding of statistical analysis and mathematical data manipulation\nWork with web technology including Java, Python, JavaScript, React/Redux, Kotlin.\nFollow best practices for software development and deployment\nUnderstanding of relational database, big data, and experience in SQL\nProficient at using GIT, GitFlow, JIRA, Gitlab and Confluence.\nStrong analytical and problem solving skills.\nOpen-minded and passionate to learn and grow technology skills\nStrong sense of accountability\nSolution-focused and ability to drive change within the organization\nExperience in writing unit/integration tests including test automation.\nStrong testing and debugging abilities, functional, analytical and technical abilities, ability to find bugs, attention to detail, troubleshooting\nAdditional Useful Skills\nA fundamental understanding of the AWS ecosystem (EC2, S3, EMR, Lambda, etc)\nExperienced in building RESTful APIs.\nExperience in writing unit/integration tests including test automation.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Ml Algorithms', 'Python', 'Pytorch']",2025-06-12 14:56:37
"AIML - Machine Learning Engineer, Search & AI",Apple,7 - 12 years,Not Disclosed,['Bengaluru'],"Our team is responsible for delivering next-generation Search and Question Answering systems across Apple products including Siri, Safari, Spotlight, and more. This is your chance to shape how people get information by leveraging your Search and applied machine learning expertise along with robust software engineering skills.You will collaborate with outstanding Search and AI engineers on large scale machine learning to improve Query Understanding, Retrieval, and Ranking, developing fundamental building blocks needed for AI powered experiences such as fine-tuning and reinforcement learning. This involves pushing the boundaries on document retrieval and ranking, developing sophisticated machine learning models, using embeddings and deep learning to understand the quality of matches. It also includes online learning to react quickly to change and natural language processing to understand queries. You will work with petabytes of data and combine information from multiple structured and unstructured sources to provide best results and accurate answers to satisfy users information-seeking needs.\n7+ years experience in shipping Search and Q&A technologies and ML systems\nExcellent programming skills in mainstream programming languages such as C++, Python, Scala, and Go\nExperience delivering tooling and frameworks to evaluate individual components and end-to-end quality\nStrong analytical skills to systematically identify opportunities to improve search relevance and answer accuracy\nExcellent communication skills and ability to work in a collaborative research environment\nPassion for building phenomenal products and curiosity to learn\nPreferred Qualifications\nBackground in Search Relevance and Ranking, Question Answering systems, Query Understanding, Personalization or Recommendation systems, or data-informed decision-making\nHands-on experience in Retrieval Augmented Generation, including developing, evaluating and enhancing for both retrievers and generative LLMs\nMS in AI, Machine Learning, Information Retrieval, Computer Science, Statistics, or a related field",Industry Type: Consumer Electronics & Appliances,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'QA', 'Analytical skills', 'C++', 'Machine learning', 'SCALA', 'query', 'Information retrieval', 'Natural language processing', 'Python']",2025-06-12 14:56:39
Machine Learning Engineer,Panacorp Software Solutions,0 - 5 years,Not Disclosed,"['Nagercoil', 'Kanyakumari']","Research Programmer (Python/ MATLAB) Fresher & Experienced\nAbout Panacorp Software Solutions\nPanacorp Software Solutions is a research-driven organization specializing in providing technical assistance for PhD research projects. Our focus is on supporting research scholars with programming, simulations, and computational analysis in various domains, including AI, Machine Learning, and numerical computing.\n\nJob Role & Responsibilities\nAssist in research-based projects related to PhD studies.\nPerform simulations, numerical computing, and data analysis using Python, MATLAB, and Simulink.\nSupport research scholars in implementing Machine Learning (ML) and Deep Learning (DL) models.\nAutomate processes and optimize research workflows through scripting.\nDocument research methodologies, findings, and technical reports.\nWork closely with scholars to analyze and interpret computational results.\nEligibility Criteria\nQualification: BE/B.Tech/MCA\nExperience: 0 5+ years (Freshers with strong academic knowledge can apply).\nStrong understanding of research methodologies and computational tools.\nPreferred Skills\nProficiency in Python, MATLAB, and Simulink.\nKnowledge of data analysis, AI/ML techniques, and numerical simulations.\nAbility to interpret and validate research outcomes.\nStrong analytical and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Machine Learning', 'matlab', 'simulink', 'python', 'data analysis', 'research methodology', 'artificial intelligence']",2025-06-12 14:56:41
Staff Data Engineer - Machine Learning,Netradyne,5 - 8 years,22.5-35 Lacs P.A.,['Bengaluru'],"Role and Responsibilities:\n\nYou will be embedded within a team of machine learning engineers and data scientists; responsible for building and productizing generative AI and deep learning solutions. You will:\nDesign, develop and deploy production ready scalable solutions that utilizes GenAI, Traditional ML models, Data science and ETL pipelines\nCollaborate with cross-functional teams to integrate AI-driven solutions into business operations.\nBuild and enhance frameworks for automation, data processing, and model deployment.\nUtilize Gen-AI tools and workflows to improve the efficiency and effectiveness of AI solutions.\nConduct research and stay updated with the latest advancements in generative AI and related technologies.\nDeliver key product features within cloud analytics.\n\nRequirements:\n\nB. Tech, M. Tech or PhD in Computer Science, Data Science, Electrical Engineering, Statistics, Maths, Operations Research or related domain.\nStrong programming skills in Python, SQL and solid fundamentals in computer science, particularly in algorithms, data structures, and OOP.\nExperience with building end-to-end solutions on AWS cloud infra.\nGood understanding of internals and schema design for various data stores (RDBMS, Vector databases and NoSQL).\nExperience with Gen-AI tools and workflows, and large language models (LLMs).\nExperience with cloud platforms and deploying models at scale.\nStrong analytical and problem-solving skills with a keen attention to detail.\nStrong knowledge of statistics, probability, and estimation theory.\n\nDesired Skills:\n\nFamiliarity with frameworks such as PyTorch, TensorFlow and Hugging Face.\nExperience with data visualization tools like Tableau, Graphana, Plotly-Dash.\nExposure to AWS services like Kinesis, SQS, EKS, ASG, lambda etc.\nExpertise in at least one popular Python web-framework (like FastAPI, Django or Flask).\nExposure to quick prototyping using Streamlit, Gradio, Dash etc.\nExposure to Big Data processing (Snowflake, Redshift, HDFS, EMR)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'AWS', 'Generative Artificial Intelligence', 'Python', 'Big Data Technologies']",2025-06-12 14:56:43
Machine Learning Engineer,Crunchyroll,2 - 7 years,Not Disclosed,['Hyderabad'],"We are considering applicants for the location(s) of Hyderabard, India.\nWe are looking for an experienced Machine Learning Engineer with a focus on MLOps to join our dynamic team and ensure the seamless productionization, maintenance, and monitoring of machine learning and AI applications. You will support a range of applications, from traditional classification, forecasting, and prediction models to recommendation systems and LLM-powered solutions. You will collaborate with Machine Learning Engineers, Data Scientists, and Platform/Software Engineers to architect scalable, maintainable, and systems that adhere to operational excellence principles.\nCore Areas of Responsibility\nDesign, implement, and maintain MLOps pipelines for deploying, monitoring, and scaling machine learning models, including traditional models and LLM-powered applications.\nEnsure the architecture of ML systems prioritises scalability, reliability, and maintainability.\nDevelop automated workflows for model training, testing, deployment, and monitoring.\nImplement monitoring and alerting systems to track model performance, data drift, and system health in production.\nCollaborate with Machine Learning Engineers and Data Scientists to refine model integration into production environments. Work with Platform/Software Engineers to integrate ML applications with existing infrastructure and ensure compatibility with cloud or on-premises systems.\nStay up-to-date with MLOps best practices, tools, and new technologies to enhance system performance and reliability.\nAbout You\n2+ years of experience MLOps, including deploying and maintaining machine learning models in production environments.\nProficiency in Python programming and familiarity with ML frameworks such as TensorFlow, PyTorch, or equivalent.\nExperience with MLOps tools and platforms (e.g., MLflow, Kubeflow, Airflow, or similar).\nKnowledge of cloud platforms (e.g., AWS, Google Cloud, Azure) and containerization technologies (e.g., Docker, Kubernetes).\nFamiliarity with CI/CD pipelines and version control systems like Git.\nUnderstanding of operational excellence principles, including system reliability, scalability, and monitoring.",Industry Type: Film / Music / Entertainment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Version control', 'GIT', 'PDF', 'HP data protector', 'Machine learning', 'Flex', 'Healthcare', 'Forecasting', 'Monitoring', 'Python']",2025-06-12 14:56:45
Machine Learning Engineer,Tek Ninjas,8 - 13 years,Not Disclosed,['Pune'],"Skills:\n     Proficient:\nLanguages/Framework: Fast API, Azure UI Search API (React)\nCloud: Azure Cloud Basics (Azure DevOps)\nGitlab: Gitlab Pipeline\nAnsible and REX: Rex Deployment\nData Science: Prompt Engineering + Modern Testing\nData pipeline development\nUnderstanding of AI/ML algorithms and their applications\nMLOps frameworks\nKnowledge of cloud platforms (Azure ML especially)\nModel deployment process\nData pipeline monitoring\nLanguages/Framework: Azure Open AI\nData Science: Open AI GPT Family of models 4o/4/3, Embeddings + Vector Search\nDatabases and ETL: Azure Storage Account, Postgresql, Cosmos\nExperience with ML frameworks (TensorFlow, PyTorch, Scikit-learn)\nKnowledge of cloud platforms (AWS SageMaker, Google AI Platform)\nExpertise in data preprocessing, feature engineering, and model evaluation\nUnderstanding of software engineering principles (version control, CI/CD, containerization)\nFamiliarity with distributed computing and big data tools (Spark, Hadoop)\nAbility to optimize models for performance and scalability\nExperience with Azure AI Search\nDesired skills*\nAzure DevOps; MLOps frameworks; Postgresql; Cosmos",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['MLOPS', 'Aiml', 'Machine Learning', 'Azure Devops']",2025-06-12 14:56:47
Hiring Machine Learning Engineer,Motivity Labs,5 - 10 years,14-22.5 Lacs P.A.,['Hyderabad'],"Role - Machine Learning Engineer\nRequired Skills & Experience\n\n5+ years of hands-on experience in building, training, and deploying machine learning models in a professional, production-oriented setting.\nDemonstrable experience with database creation and advanced querying (e.g., SQL, NoSQL), with a strong understanding of data warehousing concepts.\nProven expertise in data blending, transformation, and feature engineering, adept at integrating and harmonizing both structured (e.g., relational databases, CSVs) and unstructured (e.g., text, logs, images) data.\nStrong practical experience with cloud platforms for machine learning development and deployment; significant experience with Google Cloud Platform (GCP) services (e.g., Vertex AI, BigQuery, Dataflow) is highly desirable.\nProficiency in programming languages commonly used in data science (e.g., Python is preferred, R).\nSolid understanding of various machine learning algorithms (e.g., regression, classification, clustering, dimensionality reduction) and experience with advanced techniques like Deep Learning, Natural Language Processing (NLP), or Computer Vision.\nExperience with machine learning libraries and frameworks (e.g., scikit-learn, TensorFlow, PyTorch).\nFamiliarity with MLOps tools and practices, including model versioning, monitoring, A/B testing, and continuous integration/continuous deployment (CI/CD) pipelines.\nExperience with containerization technologies like Docker and orchestration tools like Kubernetes for deploying ML models as REST APIs.\nProficiency with version control systems (e.g., Git, GitHub/GitLab) for collaborative development.\n\nInterested candidates share cv to dikshith.nalapatla@motivitylabs.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GCP', 'Machine Learning', 'Deep Learning', 'Python', 'BigQuery', 'MLOps', 'Git', 'Vertex AI', 'GitHub/GitLab', 'A/B testing', 'Dataflow', 'Kubernetes']",2025-06-12 14:56:49
Cloud Machine Learning LLM Serving Engineer,Qualcomm,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nJD for Cloud Machine Learning LLM Serving engineer\n\nJob Overview:\n\nThe Qualcomm Cloud Computing team is developing hardware and software for Machine Learning solutions spanning the data center, edge, infrastructure, automotive market. We are seeking ambitious, bright, and innovative engineers with experience in machine learning framework development. Job activities span the whole product life cycle from early design to commercial deployment. The environment is fast-paced and requires cross-functional interaction daily so good communication, planning and execution skills are a must.\n\nKey Responsibilities\nImprove and optimize key Deep Learning models on Qualcomm AI 100.\nBuild deep learning framework extensions for Qualcomm AI 100 in upstream open-source repositories.\nImplement Kernels for AI workloads\nCollaborate and interact with internal teams to analyze and optimize training and inference for deep learning.\nBuild software tools and ecosystem around AI SW Stack.\nWork on vLLM, Triton, ExecuTorch, Inductor, TorchDynamo to build abstraction layers for inference accelerator.\nOptimize workloads for both scale-up (multi-SoC) and scale-out (multi-card) systems.\nOptimize the entire deep learning pipeline including graph compiler integration.\nApply knowledge of software engineering best practices.\n\n\nDesirable Skills and Aptitudes\nDeep Learning experience or knowledge- LLMs, Natural Language Processing, Vision, Audio, Recommendation systems.\nKnowledge of the structure and function of different components of Pytorch, TensorFlow software stacks.\nExcellent C/C++/Python programming and software design skills, including debugging, performance analysis, and test design.\nAbility to work independently, define requirements and scope, and lead your own development effort.\nWell versed with open-source development practices.\nStrong developer with a research mindset- strives to innovate.\nAvid problem solver- should be able to find solutions to key engineering and domain problems.\n\n\nKnowledge of tiling and scheduling a Machine learning operator is a plus.\nExperience in using C++ 14 (advanced features)\nExperience of profiling software and optimization techniques\nHands on experience writing SIMD and/or multi-threaded high-performance code is a plus.\nExperience of ML compiler, Auto-code generation (using MLIR) is a plus.\nExperiences to run workloads on large scale heterogeneous clusters is a plus.\nHands-on experience with CUDA, CUDNN is a plus.\n\n\nQualifications:\nBachelor's / Masters/ PHD degree in Engineering, Machine learning/ AI, Information Systems, Computer Science, or related field.\n2+ years Software Engineering or related work experience.\n2+ years experience with Programming Language such as C++, Python.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'c++', 'c', 'software design', 'software engineering', 'cuda', 'natural language processing', 'scale', 'machine learning', 'artificial intelligence', 'deep learning', 'tensorflow', 'code generation', 'computer science', 'pytorch', 'debugging', 'machine learning algorithms', 'ml']",2025-06-12 14:56:51
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Bengaluru'],"Key Responsibilities :\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n- Experience with imbalanced datasets and techniques to improve model generalization.\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n- Strong mathematical and statistical knowledge with problem-solving skills.\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n- Experience in automating ML pipelines with MLOps practices.\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'Azure', 'MLOps', 'GCP', 'Big Data', 'Neural Networks', 'AWS', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-12 14:56:54
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Thane'],"Key Responsibilities :\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n- Experience with imbalanced datasets and techniques to improve model generalization.\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n- Strong mathematical and statistical knowledge with problem-solving skills.\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n- Experience in automating ML pipelines with MLOps practices.\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'Azure', 'MLOps', 'GCP', 'Big Data', 'Neural Networks', 'AWS', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-12 14:56:56
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Surat'],"Key Responsibilities :\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n- Experience with imbalanced datasets and techniques to improve model generalization.\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n- Strong mathematical and statistical knowledge with problem-solving skills.\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n- Experience in automating ML pipelines with MLOps practices.\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'Azure', 'MLOps', 'GCP', 'Big Data', 'Neural Networks', 'AWS', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-12 14:56:58
"Engineer, Principal/Manager - Machine Learning, AI",Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"General Summary:\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Systems Engineer, you will research, design, develop, simulate, and/or validate systems-level software, hardware, architecture, algorithms, and solutions that enables the development of cutting-edge technology. Qualcomm Systems Engineers collaborate across functional teams to meet and exceed system-level requirements and standards.\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 8+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 7+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 6+ years of Systems Engineering or related work experience.\nPrincipal Engineer Machine Learning\nWe are looking for a Principal AI/ML Engineer with expertise in model inference, optimization, debugging, and hardware acceleration. This role will focus on building efficient AI inference systems, debugging deep learning models, optimizing AI workloads for low latency, and accelerating deployment across diverse hardware platforms.\nIn addition to hands-on engineering, this role involves cutting-edge research in efficient deep learning, model compression, quantization, and AI hardware-aware optimization techniques. You will explore and implement state-of-the-art AI acceleration methods while collaborating with researchers, industry experts, and open-source communities to push the boundaries of AI performance.\nThis is an exciting opportunity for someone passionate about both applied AI development and AI research, with a strong focus on real-world deployment, model interpretability, and high-performance inference.\nEducation & Experience:\n20+ years of experience in AI/ML development, with at least 5 years in model inference, optimization, debugging, and Python-based AI deployment.\nMasters or Ph.D. in Computer Science, Machine Learning, AI\nLeadership & Collaboration\nLead a team of AI engineers in Python-based AI inference development.\nCollaborate with ML researchers, software engineers, and DevOps teams to deploy optimized AI solutions.\nDefine and enforce best practices for debugging and optimizing AI models\nKey Responsibilities\nModel Optimization & Quantization\nOptimize deep learning models using quantization (INT8, INT4, mixed precision etc), pruning, and knowledge distillation.\nImplement Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) for deployment.\nFamiliarity with TensorRT, ONNX Runtime, OpenVINO, TVM\nAI Hardware Acceleration & Deployment\nOptimize AI workloads for Qualcomm Hexagon DSP, GPUs (CUDA, Tensor Cores), TPUs, NPUs, FPGAs, Habana Gaudi, Apple Neural Engine.\nLeverage Python APIs for hardware-specific acceleration, including cuDNN, XLA, MLIR.\nBenchmark models on AI hardware architectures and debug performance issues\nAI Research & Innovation\nConduct state-of-the-art research on AI inference efficiency, model compression, low-bit precision, sparse computing, and algorithmic acceleration.\nExplore new deep learning architectures (Sparse Transformers, Mixture of Experts, Flash Attention) for better inference performance.\nContribute to open-source AI projects and publish findings in top-tier ML conferences (NeurIPS, ICML, CVPR).\nCollaborate with hardware vendors and AI research teams to optimize deep learning models for next-gen AI accelerators.\nDetails of Expertise:\nExperience optimizing LLMs, LVMs, LMMs for inference\nExperience with deep learning frameworks: TensorFlow, PyTorch, JAX, ONNX.\nAdvanced skills in model quantization, pruning, and compression.\nProficiency in CUDA programming and Python GPU acceleration using cuPy, Numba, and TensorRT.\nHands-on experience with ML inference runtimes (TensorRT, TVM, ONNX Runtime, OpenVINO)\nExperience working with RunTimes Delegates (TFLite, ONNX, Qualcomm)\nStrong expertise in Python programming, writing optimized and scalable AI code.\nExperience with debugging AI models, including examining computation graphs using Netron Viewer, TensorBoard, and ONNX Runtime Debugger.\nStrong debugging skills using profiling tools (PyTorch Profiler, TensorFlow Profiler, cProfile, Nsight Systems, perf, Py-Spy).\nExpertise in cloud-based AI inference (AWS Inferentia, Azure ML, GCP AI Platform, Habana Gaudi).\nKnowledge of hardware-aware optimizations (oneDNN, XLA, cuDNN, ROCm, MLIR, SparseML).\nContributions to open-source community\nPublications in International forums conferences journals",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'AWS Inferentia', 'Azure ML', 'AI/ML', 'ONNX Runtime', 'OpenVINO', 'GCP AI', 'TVM', 'XLA', 'MLIR', 'TensorRT', 'Python']",2025-06-12 14:57:01
Principal Machine Learning Engineer,Amgen Inc,2 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nWe are seeking a highly skilled Machine Learning Engineer with a strong MLOps background to join our team. You will play a pivotal role in building and scaling our machine learning models from development to production. Your expertise in both machine learning and operations will be essential in creating efficient and reliable ML pipelines.\nRoles & Responsibilities:\nCollaborate with data scientists to develop, train, and evaluate machine learning models.\nBuild and maintain MLOps pipelines, including data ingestion, feature engineering, model training, deployment, and monitoring.\nLeverage cloud platforms (AWS, GCP, Azure) for ML model development, training, and deployment.\nImplement DevOps/MLOps best practices to automate ML workflows and improve efficiency.\nDevelop and implement monitoring systems to track model performance and identify issues.\nConduct A/B testing and experimentation to optimize model performance.\nWork closely with data scientists, engineers, and product teams to deliver ML solutions.\nGuide and mentor junior engineers in the team\nStay updated with the latest trends and advancements\n\nBasic Qualifications:\nDoctorate degree and 2 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nMasters degree and 8 to 10 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nBachelors degree and 10 to 14 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nDiploma and 14 to 18 years of years of Computer Science, Statistics, and Data Science, Machine Learning experience\nPreferred Qualifications:\nMust-Have Skills:\nStrong foundation in machine learning algorithms and techniques\nExperience in MLOps practices and tools (e.g., MLflow, Kubeflow, Airflow); Experience in DevOps tools (e.g., Docker, Kubernetes, CI/CD)\nProficiency in Python and relevant ML libraries (e.g., TensorFlow, PyTorch, Scikit-learn)\nOutstanding analytical and problem-solving skills; Ability to learn quickly; Excellent communication and interpersonal skills\nGood-to-Have Skills:\nExperience with big data technologies (e.g., Spark), and performance tuning in query and data processing\nExperience with data engineering and pipeline development\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification\nKnowledge of NLP techniques for text analysis and sentiment analysis\nExperience in analyzing time-series data for forecasting and trend analysis\nFamiliar with AWS, Azure, or Google Cloud;\nFamiliar with Databricks platform for data analytics and MLOps\nProfessional Certifications\nCloud Computing and Databricks certificate preferred\nSoft Skills:\nExcellent analytical and fixing skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Azure', 'NLP', 'MLOps', 'Databricks', 'AWS', 'Google Cloud']",2025-06-12 14:57:03
Machine Learning Engineer (Full Stack),Hubnex,5 - 10 years,Not Disclosed,['Gurugram'],"Machine Learning Engineer (Full Stack)\nLocation: Gurugram, India (On-site/Hybrid)\nType: Full-Time | 5+ Years Experience | AI & Product Engineering\nHubnex Labs is seeking a highly skilled Machine Learning Engineer with strong capabilities in Full Stack Development to lead the development and deployment of production-grade AI systems. This role requires expertise in building end-to-end ML pipelines from data preprocessing to deployment while also contributing to the full stack of software platforms that power our solutions.\nKey Responsibilities Machine Learning & Data Science\nUnderstand business goals and translate them into ML-based solutions\nDevelop, analyze, and compare machine learning algorithms for various problem statements\nBuild robust validation strategies and design appropriate preprocessing and feature engineering pipelines\nPerform data exploration, visualization , and quality verification , including data cleaning and augmentation\nTrain models, tune hyperparameters , and interpret model performance\nAnalyze errors and design strategies to improve model robustness\nDiscover and utilize public datasets for model training and benchmarking\nDeploy models into production environments with real-world performance and latency considerations\nSoftware & System Development\nDesign and develop end-to-end production systems , including backend APIs and frontend interfaces\nMaintain full stack web applications , ensuring seamless ML model integration\nEnsure efficient use of hardware resources for training and inference\nCollaborate cross-functionally with engineering, product, and design teams\nTechnical Skills Required\n5+ years of hands-on experience in machine learning and full stack development\nProficiency in Python and ML libraries like scikit-learn , pandas , NumPy , etc.\nDeep learning experience using TensorFlow , Keras , or equivalent frameworks\nProficiency with OpenCV and image/video processing techniques\nExperience with data visualization tools and big data handling\nStrong understanding of data pipelines , feature engineering , and augmentation techniques\nProficiency in Full Stack Web Development (e.g., React, Node.js, Express, MongoDB or similar)\nExperience deploying models using REST APIs, Flask/FastAPI, Docker, etc.\nFamiliarity with Linux environments and GPU-accelerated compute systems\nUnderstanding of hardware requirements and optimization for real-time ML performance\nWhy Join Hubnex Labs?\nWork on impactful AI products deployed in real-world use cases\nBe a part of a fast-growing tech consulting and product innovation company\nCollaborate with a diverse team of engineers, data scientists, and innovators\nFlexible and collaborative culture based in Gurugram , with hybrid work options\nIdeal Candidate\nPassionate about building smart systems that go live in production\nCan operate independently and take full ownership of ML products\nBlends deep technical skill with product thinking and business awareness",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Product engineering', 'Backend', 'Linux', 'Product innovation', 'Consulting', 'Machine learning', 'Web development', 'MongoDB', 'Business awareness', 'Python']",2025-06-12 14:57:05
Data Scientist,Tesco,1 - 3 years,Not Disclosed,['Bengaluru( Whitefield )'],"Job Summary:\n\nEnable data driven decision making across the Tesco business globally by developing analytics solutions using a combination of math, tech and business knowledge\n\nRoles and Responsibilities:\n- Identifying operational improvements and finding solutions by applying CI tools and techniques\n- Responsible for completing tasks and transactions within agreed KPI's",,,,"['Data Science', 'Advanced Excel', 'Data Analytics', 'Python', 'SQL', 'Applied Mathematics', 'Machine Learning', 'Statistics']",2025-06-12 14:57:07
AI / ML Data Scientist,Sanofi,3 - 5 years,Not Disclosed,['Hyderabad'],"Transform healthcare through innovation. At Sanofi, we're not just developing treatments we're pioneering the future of healthcare by harnessing the power of data insights and responsible AI to accelerate breakthrough therapies.\n  As an AI/ML Scientist on our AI and Computational Sciences team, you'll:\nDrive innovation that directly impacts patient outcomes\nCollaborate with world-class scientists to solve complex healthcare challenges\nApply advanced AI techniques to increase drug development success rates\nShape the responsible use of AI in life-saving medical research\nBe part of a mission that matters. Help us transform data into life-changing treatments and join a team where your expertise can make a meaningful difference in patients lives.\nOur Team :\nThe AI and Computational Sciences team is a key team within R&D Digital, focused on image, omics, wearable sensor data, and clinical data analytics. This team plays a critical role in bridging the gap between general purposed digital products and specific project needs.\nWe are looking for a skilled AI/ML Data Scientist to join our elite AI and Computational Sciences team and harness cutting-edge AI to revolutionize healthcare. As a key player within R&D Digital, you'll transform complex data into life-changing medical breakthroughs.\n  Impact you'll Make\nDrive innovation across multiple high-impact domains:\nPrecision Medicine: Develop patient response prediction models that personalize treatments\nAdvanced Omics Analysis: Pioneer cell type and cell stage quantification techniques\nAdvanced Image/Video Analysis: Lead application of state-of-art computer vision methods for gaining unprecedented insights about drug efficacy from medical images/videos\nDigital Health: Design novel biomarkers from wearable sensor data\nBiological Insights: Create enzyme property prediction algorithms and conduct disease pathway analyses\nYour Growth Journey\nTechnical Mastery: Develop expertise across image analysis, time series modeling, GenAI, AI Agents, and explainable AI\nScientific Impact: Publish in top-tier AI/ML journals and secure patents that protect groundbreaking innovations\nGlobal Influence: Deploy solutions that impact patients worldwide\nYour Environment\nElite Team: Work alongside AI/ML experts and drug development experts in an agile, high-performance environment\nCutting-Edge Resources: Access Sanofis state-of-the-art cloud infrastructure and data platforms\nContinuous Learning: Receive mentorship and training opportunities to sharpen your leadership and AI/ML skills\nJoin Our AI-First Vision\nDevelop your skills through world-class mentorship and training\nChase the miracles of science to improve peoples lives\nReady to transform healthcare through the power of AI?\nMain Responsibilities :\nResearch Phase Excellence\nDesign and implement AI models for target identification and validation using multi-omics data (genomics, proteomics, transcriptomics)\nDevelop predictive algorithms to molecular design for compound selection and accelerate lead optimization\nCreate computer vision systems for high-throughput screening image analysis and cellular phenotyping\nClinical Development Innovation\nEngineer digital biomarkers from wearable sensors and mobile devices to enable objective, continuous patient monitoring\nImplement advanced time-series analysis of real-time patient data to detect early efficacy signals\nDesign AI-powe'red patient stratification models to identify responder populations and optimize trial design\nMulti-Modal Data Integration\nArchitect systems that harmonize diverse data types (imaging, omics, clinical, text, sensor) into unified analytical frameworks\nDevelop novel feature extraction techniques across modalities to enhance predictive power\nCreate visualization tools that present complex multi-modal insights to clinical teams\nScientific Impact\nCollaborate with cross-functional teams to translate AI insights into actionable drug development strategies\nPresent findings to scientific and business stakeholders with varying technical backgrounds\nPublish innovative methodologies in top-tier scientific and AI/ML journals\nContribute to patent applications to protect novel AI/ML approaches\nExperience : 3 to 5 years of experience in AI/ML and computational model development on multimodal data like omics, biomedical imaging, text and clinical trials data\n  Key Functional Requirement:\nDemonstrated track record of successful AI/ML project implementation\n3-5 years of experience in computational modeling or AI/ML algorithm development, or any other related field\nDeep understanding and proven track record of developing model training pipelines and workflows\nExcellent communication and collaboration skills\nWorking knowledge and comfort working with Agile methodologies\nTechnical Skills :\nProgramming Proficiency: Advanced Python skills with experience in ML frameworks (PyTorch, TensorFlow, JAX)\nMachine Learning: Deep expertise in supervised, unsupervised, and reinforcement learning algorithms\nDrug discovery: molecular design, docking, binding site prediction, mRNA vaccine design, ADMET property, protein structure prediction, molecular dynamics simulation\nDeep Learning: Experience designing and implementing neural network architectures (CNNs, RNNs, Transformers)\nComputer Vision: Proficiency in image processing, segmentation, and object detection techniques (SAM, ViT, Diffusion Models, MediaPipe, MMPose, MonoDepth, VoxelNet, SlowFast, C3D)\nNatural Language Processing: Experience with large language models, text mining, and information extraction (OpenAI, Claude, Llama, Qwen, Deepseek model series)\nTime Series Analysis: Expertise in analyzing temporal data from sensors and wearable devices (HAR foundation models, compliance detection models)\nOmics Analysis: Knowledge of computational methods for protein genomics, proteomics, or transcriptomics data\nCloud Computing: Experience deploying ML models on cloud platforms (AWS)\nTools and Technologies :\nData Processing: Experience with data pipelines and ETL processes\nVersion Control: Proficiency with Git and collaborative development workflows, Docker\nMLOps: Experience with model deployment, monitoring, and maintenance\nVisualization: Ability to create compelling data visualizations (Matplotlib, Seaborn, Plotly)\nExperiment Tracking: Familiarity with tools like MLflow, Weights & Biases, or similar platforms\nSoft Skills :\nStrong scientific communication abilities for technical and non-technical audiences\nCollaborative mindset for cross-functional team environments\nProblem-solving approach with ability to translate business needs into technical solutions\nSelf-motivated with capacity to work independently and drive projects forward\nEducation : PhD/MS/BE/BTech/ME/MTech in Computer Science and Engineering, AI/ML, other relevant engineering discipline, Computational Biology, Data Science, Bioinformatics or related fields (with equivalent experience)\n  Preferred : Publications or public github\nLanguages : English\n  Why Choose us\nBring the miracles of science to life alongside a supportive, future-focused team\nDiscover endless opportunities to grow your talent and drive your career, whether it s through a promotion or lateral move, at home or internationally\nEnjoy a thoughtful, well-crafted rewards package that recognizes your contribution and amplifies your impact\nTake good care of yourself and your family, with a wide range of health and wellbeing benefits including high-quality healthcare, prevention and wellness programs\nOpportunity to work in an international environment, collaborating with diverse business teams and vendors, working in a dynamic team, and fully empowe'red to propose and implement innovative ideas.",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analytical', 'Genomics', 'clinical development', 'Clinical trials', 'Healthcare', 'biomedical', 'Bioinformatics', 'Monitoring', 'clinical data']",2025-06-12 14:57:09
ML Engineer/Data Scientist,Altimetrik,6 - 8 years,15-30 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\nData Scientist /ML engineers : ML Engineer with Python, SQL, Machine Learning, Azure skills(Good to have)",Industry Type: IT Services & Consulting,,,"['Machine Learning', 'Python', 'SQL', 'Data Science', 'Ml', 'azure']",2025-06-12 14:57:11
"Data Scientist II, Regulatory Intelligence, Safety, and Compliance",Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"About Amazon Regulatory Intelligence, Safety, and Compliance (RISC).\n\nAmazon RISC s vision is to make Amazon the Earth s most trusted shopping destination for safe and compliant products. Towards this mission, we take a science-first approach to building technology, products and services, that protect customers from unsafe, illegal, controversial, or policy-violating products while offering the optimal selling partner experience.\n\nJob Summary\n\nWe are seeking an exceptional Data Scientist to join a team of experts in the field of AI/ML, and work together to tackle challenging business problems across diverse compliance domains. We leverage and train state-of-the-art multi-modal, large-language-models (LLMs), and vision language models (VLMs) to detect illegal and unsafe products across the Amazon catalog. We work on machine learning problems for generative AI, agentic system, multi-modal classification, intent detection, information retrieval, anomaly and fraud detection.\n\nThis is an exciting and challenging position to deliver scientific innovations into production systems at Amazon-scale to make immediate, meaningful customer impacts while also pursuing ambitious, long-term research. You will work in a highly collaborative environment where you can analyze and process large amounts of images, texts, documents, and tabular data. You will work on hard science problems that have not been solved before, conduct rapid prototyping to validate your hypothesis, and deploy your algorithmic ideas at scale. There will be something new to learn every day as we work in an environment with rapidly evolving regulations and adversarial actors looking to outwit your best ideas.\n\n\nDesign and evaluate state-of-the-art algorithms and approaches in generative AI, agentic system, multi-modal classification, intent detection, information retrieval, anomaly and fraud detection.\nTranslate product and CX requirements into measurable science problems and metrics.\nCollaborate with product and tech partners and customers to validate hypothesis, drive adoption, and increase business impact\nKey author in writing high quality scientific papers in internal and external peer-reviewed conferences.\n\nA day in the life\nUnderstanding customer problems, project timelines, and team/project mechanisms\nProposing science formulations and brainstorming ideas with team to solve business problems\nWriting code, and running experiments with re-usable science libraries\nReviewing labels and audit results with investigators and operations associates\nSharing science results with science, product and tech partners and customers\nWriting science papers for submission to peer-review venues, and reviewing science papers from other scientists in the team.\nContributing to team retrospectives for continuous improvements\nDriving science research collaborations and attending study groups with scientists across Amazon\n\nAbout the team\nWe are a team of scientists and engineers building AI/ML solutions to make Amazon the Earth s most trusted shopping destination for safe and compliant products. PhD, or Masters degree with 2+ years of machine learning experience, or bachelor degree with 3+ years of machine learning experience\nExperience programming in Python, Java, C++, or related language\nExperience with neural deep learning methods, LLM, and natural language processing\nExperience with conducting research in a corporate setting Experience with large scale machine learning systems such as profiling and debugging and understanding of system performance and scalability",,,,"['deep learning', 'C++', 'Debugging', 'Machine learning', 'Information retrieval', 'Natural language processing', 'Scientist II', 'Fraud detection', 'Auditing', 'Python']",2025-06-12 14:57:13
"Data Scientist II, Regulatory Intelligence, Safety, and Compliance",Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"We are seeking an exceptional Data Scientist to join a team of experts in the field of machine learning, and work together to tackle challenging problems across diverse compliance domains. We leverage risk models (including boosted trees and graph neural networks) as well as vision and large-language-models (LLMs) to detect illegal and unsafe products across the Amazon catalog. We work on machine learning problems for multi-modal classification, intent detection, information retrieval, anomaly and fraud detection, and generative AI.\n\nThis is an exciting and challenging position to deliver scientific innovations into production systems at Amazon-scale to make immediate, meaningful customer impacts while also pursuing ambitious, long-term research. You will work in a highly collaborative environment where you can analyze and process large amounts of image, text and tabular data. You will work on hard science problems that have not been solved before, conduct rapid prototyping to validate your hypothesis, and deploy your algorithmic ideas at scale. There will be something new to learn every day as we work in an environment with rapidly evolving regulations and adversarial actors looking to outwit your best ideas.\n\n\nExplore and evaluate state-of-the-art algorithms and approaches in risk modeling and vision/language models\nTranslate product and CX requirements into measurable science problems and metrics.\nCollaborate with product and tech partners and customers to validate hypothesis, drive adoption, and increase business impact\nEvaluate model performance in production and refresh/implement necessary updates to maintain optimal system performance.\n\nA day in the life\nUnderstanding customer problems, project timelines, and team/project mechanisms\nProposing science formulations and brainstorming ideas with team to solve business problems\nWriting code, and running experiments with re-usable science libraries\nReviewing labels and audit results with investigators and operations associates\nSharing science results with science, product and tech partners and customers\nContributing to team retrospectives for continuous improvements\nParticipating in science research collaborations and attending study groups with scientists across Amazon\n\nAbout the team\nWe are a team of scientists building AI/ML solutions to make Amazon Earth s most trusted shopping destination for safe and compliant products. 2+ years of data scientist experience\n3+ years of data querying languages (e.g. SQL), scripting languages (e.g. Python) or statistical/mathematical software (e.g. R, SAS, Matlab, etc.) experience\n3+ years of machine learning/statistical modeling data analysis tools and techniques, and parameters that affect their performance experience\nExperience applying theoretical models in an applied environment\nKnowledge of relevant statistical measures such as confidence intervals, significance of error measurements, development and evaluation data sets, etc. Experience in Python, Perl, or another scripting language\nExperience in a ML or data scientist role with a large technology company",,,,"['Data analysis', 'SAS', 'Neural networks', 'risk modeling', 'Machine learning', 'Information retrieval', 'Perl', 'MATLAB', 'Auditing', 'Python']",2025-06-12 14:57:16
"Data Scientist II, Regulatory Intelligence, Safety, and Compliance",Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"We are seeking an exceptional Data Scientist to join a team of experts in the field of machine learning, and work together to tackle challenging problems across diverse compliance domains. We leverage multi-modal and large-language-models (LLMs) to detect illegal and unsafe products across the Amazon catalog. We work on machine learning problems for multi-modal classification, intent detection, information retrieval, anomaly and fraud detection, and generative AI.\n\nThis is an exciting and challenging position to deliver scientific innovations into production systems at Amazon-scale to make immediate, meaningful customer impacts while also pursuing ambitious, long-term research. You will work in a highly collaborative environment where you can analyze and process large amounts of image, text and tabular data. You will work on hard science problems that have not been solved before, conduct rapid prototyping to validate your hypothesis, and deploy your algorithmic ideas at scale. There will be something new to learn every day as we work in an environment with rapidly evolving regulations and adversarial actors looking to outwit your best ideas.\n\n\nExplore and evaluate state-of-the-art algorithms and approaches in multi-modal classification, large language models (LLMs), intent detection, information retrieval, anomaly and fraud detection, and generative AI\nTranslate product and CX requirements into measurable science problems and metrics.\nCollaborate with product and tech partners and customers to validate hypothesis, drive adoption, and increase business impact\n\nA day in the life\nUnderstanding customer problems, project timelines, and team/project mechanisms\nProposing science formulations and brainstorming ideas with team to solve business problems\nWriting code, and running experiments with re-usable science libraries\nReviewing labels and audit results with investigators and operations associates\nSharing science results with science, product and tech partners and customers\nContributing to team retrospectives for continuous improvements\nParticipating in science research collaborations and attending study groups with scientists across Amazon\n\nAbout the team\nWe are a team of scientists building AI/ML solutions to make Amazon Earth s most trusted shopping destination for safe and compliant products. 2+ years of data scientist experience\n2+ years of data querying languages (e.g. SQL), scripting languages (e.g. Python) or statistical/mathematical software (e.g. R, SAS, Matlab, etc.) experience\n2+ years of machine learning/statistical modeling data analysis tools and techniques, and parameters that affect their performance experience\nExperience applying theoretical models in an applied environment\nKnowledge of relevant statistical measures such as confidence intervals, significance of error measurements, development and evaluation data sets, etc. Experience in Python, Perl, or another scripting language\nExperience in a ML or data scientist role with a large technology company",,,,"['Data analysis', 'Statistical modeling', 'SAS', 'Machine learning', 'Information retrieval', 'Perl', 'MATLAB', 'Fraud detection', 'Auditing', 'Python']",2025-06-12 14:57:18
Data Scientist,New Relic One,2 - 7 years,Not Disclosed,['Bengaluru'],"We are seeking an experienced and dynamic Senior Data Scientist to join our team. You will be primarily responsible for driving data-oriented projects and transforming ambiguous business problems into clear, actionable insights as well as productionalizing insights. The ideal candidate is adept at understanding the business needs that are often quantitatively ambiguous and using large complex data sets to find opportunities for product and process optimization.\nWhat youll do\nAnalyzing complex datasets, applying advanced statistical methods as necessary (e.g., time series forecasting, classification, linear/logistic regression).\nDesigning and deploying data-science and technology-based algorithmic solutions to address business needs.\nTranslating data findings into actionable business insights and plans.\nCollaborating effectively with internal stakeholders, understanding their needs and being able to communicate data-driven recommendations.\nPresenting information using data visualization techniques and clearly communicating complex findings and ideas to non-technical stakeholders.\nThis role requires\n2+ years of experience\nProven experience as a Data Scientist, or in a similar role.\nPhD or Masters degree in Statistics, Mathematics, Computer Science, or related quantitative field.\nStrong understanding and application of advanced statistical techniques and concepts, including but not limited to machine learning algorithms, classification, regression, and time series analysis.\nProficiency with data analysis tools and languages such as Python, SQL, etc.\nFamiliarity with data visualization tools (e.g., Looker, Tableau, PowerBI, etc.).\nStrong problem-solving abilities, business acumen, and excellent communication skills.\nAbility to work independently and with minimal supervision.Proven ability in managing and delivering on multiple, competing priorities.\nPrior experience with stakeholder management and ability to present complex data in a clear manner to non-technical audience.\nBonus points if you have\nExperience in Observability is a plus.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAN', 'Logistic regression', 'Data analysis', 'Process optimization', 'Machine learning', 'Stakeholder management', 'Forecasting', 'SQL', 'Python']",2025-06-12 14:57:20
Data Scientist,An Indian NBFC,3 - 8 years,Not Disclosed,['Chennai'],"Responsibilities:\nCollect, clean, and analyze large sets of structured and unstructured data to extract meaningful insights and trends\nDevelop and implement advanced machine learning algorithms to solve complex business problems\nSupport moving models to production, by creating high quality code modules that can be seamlessly integrated into existing systems (both on-prem and cloud)\nCommunicate complex findings to both technical and non-technical audiences through effective data visualization and storytelling.\nCollaborate with cross-functional teams to identify data-driven opportunities and translate business requirements into actionable data solutions.\nSupport the development and maintenance of data pipelines and infrastructure\nStay up-to-date with industry trends and advancements in Data Science and Machine Learning technologies.\n\nSkills Required:\nStrong foundation in statistics, and machine learning algorithms\nStrong proficiency in programming languages like Python and SQL.\nExcellent problem-solving and analytical skills.\nAbility to work independently and as part of a team.\nShould have built production models using at least 2 of the ML techniques: Clustering, Regression, Classification\nExperience in Banking & Financial Services is preferred.\nExperience working on cloud platforms (e.g., AWS, GCP) is preferred.\nA passion for data and a curiosity to explore new trends and technologies",Industry Type: NBFC,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Pipeline', 'Data Extraction', 'Model Building', 'Artificial Intelligence', 'Cloud', 'Machine Learning']",2025-06-12 14:57:23
Data Scientist,Mindpro Technologies,4 - 9 years,5-12 Lacs P.A.,"['Karur', 'Dharwad']","Greetings From Mind Pro Technologies Pvt ltd (www.mindprotech.com)\n\nJob Title : Data Scientist\nWork Location : Karur (Tamil Nadu) or Dharwad (Karnataka )\nNp : 15days or Less\n\n\nJOB DESCRIPTION:\n Must have At least 4+ Years of experience in Python with Data Science.\n Must have worked on at least one Live project.\nExperience in relevant field such as Statistics, Computer Science or Applied Math or Operational Research.\nMust have Masters in (Maths/Statistics or Applied Mathematics/Machine Learning etc.)\nHistory of successfully performing customer implementations\nStrong customer facing skills, and previous consulting experience.\nExperience of handling high frequency streaming data for real time analysis and reporting.\nFamiliarity with - Natural Language Processing, Statistical Analysis (distribution analysis, correlation, variance, deep learning.\nExperience in tools like AWS, IBM Watson is a plus.\nExperience with open source technologies is a must.\nExcellent communication\nAbility to lead & build strong teams\nAbility to work in an ambiguous environment\n\nDesired Skills and Experience\nLanguages/Tools: Python/R.\nApproaches: Machine Learning\nConcepts: Supervised ANN, Bayesian, Gaussian, Vector Quantization, Logistic Model, Statistical, Predictive Modeling, Minimum Message Length, SVM, Random Forest, Ensembles, ANOVA, Decision Trees, Hidden Markov Models\nUnsupervised ANN, ARL, Clustering Hierarchical, Cluster Analysis\nReinforcement\nGen AI, LLM, LSTM, RNN, CNN, KNN\nBig Data (Good to have): Hadoop /Kafka / Storm / Spark streaming\nOS: Linux, Windows 32/64 bits.\n\nNote:  should know supervised and unsupervised learning,   semi-supervised learning, neural networks concepts, and how ML algorithms works with training and testing data. Experience on particular data set to train, test and roll-out for production use\n\nTool sets : Python, R, MATLAB or  any AI frame work, Neural network, Gen AI, LLM\nContact Details:\n\nRecruitment Team\nMindpro Technologies Pvt Ltd (www.mindprotech.com)\n+91-04324-240904 / +91-9600672304",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Gen AI', 'Statistical Modeling', 'LLM', 'Predictive Modeling', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Machine Learning', 'Deep Learning', 'Python']",2025-06-12 14:57:25
Data Scientist,Puresoftware Technology,8 - 13 years,Not Disclosed,['Bengaluru( Whitefield )'],"Job Title: Data Scientist(5 Positions)/ Lead OR Manager -Data Scientist (3 positions)\n\nExperience: Data scientist (8-10 years) / Lead Data scientist(14+ years)\n\nJob Location: Whitefield, Bangalore\nMode of working: Hybrid\n\nInterview Process: First Round: L1-Internal interview\nSecond Round: Assessment shared by us needs to be completed in 48 hours\nThird Round: Client discussion over the submitted assessment.\nFinal Round: HR Discussion\n\nPreferred Domain: Healthcare Insurance/ Insurance agencies / Health Insurance / Any Insurance\n\nWe are looking for a talented Data Scientist to join our growing team. In this role, you will lead efforts to develop, enhance, and optimize advanced AI and machine learning models with a particular focus on Generative AI, Large Language Models (LLMs), Langchain, and Prompt Engineering. You will oversee the application of statistical modeling techniques to derive insights, build models, and lead research initiatives that push the boundaries of AI technologies.\n\nKey Responsibilities:\nLeadership & Collaboration: Lead a team of data scientists, researchers, and engineers working on high-impact projects related to generative models, NLP, and statistical modeling. Collaborate with cross-functional teams, including engineering, product management, and research, to deliver AI-powered products and solutions.\nGenerative AI Development: Spearhead the development and deployment of Generative AI models and algorithms to address complex problems in areas like content generation, conversational AI, and creative automation.\nLLM Implementation & Optimization: Develop, fine-tune, and optimize large language models (LLMs) for diverse applications, ensuring they are robust, scalable, and accurate in real-world scenarios.\nLangchain Integration: Design and integrate Langchain for managing and deploying sophisticated language models with a focus on complex workflows, multi-agent systems, and real-time applications.\nPrompt Engineering: Lead prompt engineering efforts to optimize AI models' output quality, improve interactions, and enable more effective natural language understanding across a variety of use cases.\nStatistical Modeling: Utilize advanced statistical techniques to analyze and interpret data, build predictive models, and solve business-critical challenges through data-driven insights.\nResearch & Innovation: Stay ahead of trends in AI and ML, particularly in the fields of NLP, LLMs, and generative models. Drive innovation by exploring cutting-edge techniques and methodologies in the AI space.\nMentorship & Knowledge Sharing: Mentor junior team members and promote a collaborative, learning-oriented environment. Share knowledge and foster an atmosphere of continuous improvement within the data science team.\nPerformance Optimization: Ensure model performance meets or exceeds company and client expectations by identifying areas of improvement, testing new methods, and scaling the systems accordingly.\nEthical AI Development: Advocate for and implement ethical considerations in the development and deployment of AI models, including fairness, transparency, and privacy.\n\nQualifications:\nRequired:\nEducation: Ph.D. or Masters degree in Computer Science, Data Science, Mathematics, Statistics, or related field, or equivalent practical experience.\nExperience:\n8+ years of experience in data science, with at least 2-3 years in a leadership role.\nProven expertise in Generative AI, particularly in areas like content generation, deep learning, and language modeling.\nStrong background in Large Language Models (LLMs) such as GPT, T5, BERT, or similar architectures.\nHands-on experience with Langchain for building NLP workflows, pipelines, and integrating external systems with LLMs.\nHands-on experience of Prompt Engineering, including techniques to refine and optimize outputs for various NLP tasks.\nExpertise in statistical modeling and quantitative analysis, with the ability to apply techniques to solve real-world problems.\n\nPreferred:\nExperience working with transformer models and fine-tuning LLMs for specific tasks.\nExpertise in AI model evaluation and metrics (e.g., BLEU, ROUGE, perplexity).\nBackground in developing AI-driven products from concept to deployment.\nStrong publication record in AI research, particularly in NLP and machine learning.\n\nUsed cases( Any of them)\nAutomated Underwriting.\nCustomer experience enhancement.\nFraud detection.\nPredictive analytics.\nAccelerated claims processing.\nRisk assessment and premium calculation.\nCustomer profiling.\ncustomer segmentation.\nCredit Risk Assessment.\nPersonalised marketing .\nAnti-Money Laundering (AML).\nPersonalized patient care.\nMedical training and simulations.\nMedical Data Analysis.\n\nPlease share your updated resume at renuka.rathi@puresoftware.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'data scientist', 'statistical modelling', 'Predictive Modeling', 'Customer profiling', 'Healthcare Insurance', 'Customer Segmentation', 'Automated Underwriting', 'Insurance Domain', 'Credit Risk Assessment', 'insurance agency', 'Fraud detection', 'Healthcare Domain']",2025-06-12 14:57:28
Senior AI Engineer - Machine Learning,ZS,4 - 9 years,Not Disclosed,['Bengaluru'],"Build, Refine and Use ML Engineering platforms and components.\nScaling machine learning algorithms to work on massive data sets and strict SLAs.\nBuild and orchestrate model pipelines including feature engineering, inferencing and continuous model training.\nImplement ML Ops including model KPI measurements, tracking, model drift & model feedback loop.\nCollaborate with client facing teams to understand business context at a high level and contribute in technical requirement gathering.",,,,"['Hospitality', 'Coding', 'Financial planning', 'Management consulting', 'Agile', 'Healthcare', 'Data structures', 'SAGE', 'SQL']",2025-06-12 14:57:30
"Senior Manager -Data Science, Machine Learning and GenAI Technical",Qualcomm,7 - 12 years,Not Disclosed,['Hyderabad'],"Title : Senior Manager -Data Science, Machine Learning and GenAI Technical Leader\n\nJob Area: Information Technology Group, Information Technology Group > IT Management\n\nGeneral Summary:\n\nExperience in engaging with business and technical stakeholders, understanding complex problem statements, and proposing value-driven Data Science & ML solutionsFunctional & technical leadership in developing data science roadmap and guiding the team with end-to-end delivery (design, development, maintenance, and optimization) of advanced analytical solutions with focus on process standardization and best practicesProactive in reaching out to wide variety of stakeholders for increasing the awareness of Data Science, ML and GenAI capabilities and identifying business value driven opportunitiesAnalyze the market and industry trends in the technology and proactively look for opportunities in bringing the best solutions\n\n'Strong analytical skills with the ability to gather information from several sources and identify fundamental patterns/trends in dataConduct research, design statistical studies and develop solutionsDevelop Story Telling dashboards on analytics to assist business in decision makingImplement deep learning models for structured and unstructured data setsPerform Time Series Analysis and Forecast (ARIMA, Exponential Smoothing, VAR etc.)Implementation experience with identifying, prototyping, developing and delivering GenAI solutions using Industry standard RAG models and frameworksAbility to develop capabilities in Text Analytics ranging from Basic to Advanced (Topic Modelling, NLP)Experience with AWS suite of Data Science and ML toolsDevelops predictive models using Machine Learning algorithms (SVM, Random Forest, Neural Network, Decision Tree, Logistic Regression, K-mean Clustering, Catboost etc.)\n\n'Experience with other cloud platforms (GCP and Azure)Understanding of Data Platforms and Data EngineeringExposure to ChatBot solutions and Automation technologies\n\nMinimum Qualifications:\n7+ years of IT-related work experience with a Bachelor's degree.\nOR\n9+ years of IT-related work experience without a Bachelors degree.\n\n4+ years in a leadership role in projects/programs.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['microsoft azure', 'machine learning', 'deep learning', 'gcp', 'aws', 'time series analysis', 'arima', 'algorithms', 'natural language processing', 'forecasting', 'neural networks', 'random forest', 'svm', 'text analytics', 'decision tree', 'data science', 'clustering', 'logistic regression', 'ml']",2025-06-12 14:57:32
"Applied Scientist II, SFT Machine Learning",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"This role is to solve business problems in Machine Learning for the Seller and Fulfilment Tech (SFT) org.\nThe overarching goal of the team is to enhance ML expertise and fluency within SFT and across IST, championing engineering and operational excellence in ML model development and other related parts of the ML model lifecycle. Some of the key areas which the team owns in this space area:\nSelection Recommendations, Registration improvements, Bad actor detection and prevention\nSelection economics, Inventory recommendation, Delivery Promise Predictions, Seller success.\nWithin the ML space, the scientist would have to solve intrinsically hard problems where neither problem nor solution is well defined. So, the leader should have high focus on building a deep understanding of the ML science space, experimentation methodology, as well as a high focus on embracing external trends, especially applications of GenerativeAI and LLMs.\nA large focus area for the role is to also contribute towards the science and research aspects. The ASII leader applies and extends existing scientific techniques, and invents new ones to address specific customers needs or business problems, at a project level. This should also lead to regular contributions to internal or external peer-reviewed publications that validate novelty 3+ years of building models for business application experience\nPhD, or Masters degree and 4+ years of CS, CE, ML or related field experience\nExperience in patents or publications at top-tier peer-reviewed conferences or journals\nExperience programming in Java, C++, Python or related language\nExperience in any of the following areas: algorithms and data structures, parsing, numerical optimization, data mining, parallel and distributed computing, high-performance computing Experience using Unix/Linux\nExperience in professional software development",,,,"['Unix', 'C++', 'Operational excellence', 'Linux', 'Machine learning', 'Data structures', 'model development', 'high performance computing', 'Data mining', 'Python']",2025-06-12 14:57:34
Machine Learning Senior Engineer,Qualcomm,1 - 6 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nJob Overview\n\nJoin a new and growing team at Qualcomm focused on advancing state-of-the-art in Machine Learning. The team uses Qualcomm chips extensive heterogeneous computing capabilities and engineers them to allow the running of trained neural networks on device without a need for connection to the cloud. Our inference engine is designed to help developers run neural network models trained in a variety of frameworks on Snapdragon platforms at blazing speeds while still sipping the smallest amount of power. See your work directly impact billions of mobile devices around the world & also most advanced Autonomous features for AUTO industry. In this position, you will be responsible for the development of test frameworks for Qualcomm Neural Network (QNN). You will work with neural network frameworks like TensorFlow, Pytorch and develop the validation framework to gauge functionality, performance, precision, and power of QNN. You will work with the latest and greatest DNNs emerging from the research community. You will also have to keep up with the fast pace development happening in the industry and academia to continuously enhance our benchmarking and validation infrastructure from software engineering as well as machine learning standpoint.\n\n\n\nMinimum Qualifications\nExpertise in Developing test cases, automating the tests, test case execution and troubleshooting/analyzing problems\nStrong development skills in Python.\nStrong understanding of Machine learning/Deep learning workflows..\nExperience with at least one machine learning framework like TensorFlow, Pytorch, etc.\nLive and breathe quality software development with excellent analytical and debugging skills.\nExcellent communication skills (verbal, presentation, written)\nAbility to collaborate across a globally diverse team and multiple interests 8.Excellent communication skills (verbal, presentation, written), Strong problem-solving skills, Good time management skills,, excellent analytical and debugging skills, must be an effective team player, and should be self-driven.\n\n\n\n\nPreferred Qualifications\n\nStrong exposure to software testing methodologies and reporting. Experience with CI tools like Jenkins and Data visualization tools like power BI ,Tableau Development experience in Python & C++\n\n\n\nWork Experience\n\n1 to 6 years of relevant work experience in software dev/test development\n\n\n\nEducational\n\nMasters/Bachelor's Computer Science, Computer Engineering, or Electrical Engineering\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'software testing', 'test cases', 'machine learning', 'deep learning', 'test case execution', 'automation testing', 'tensorflow', 'tableau', 'java', 'problem analysis', 'test development', 'pytorch', 'debugging', 'troubleshooting', 'testing methodologies', 'software engineering']",2025-06-12 14:57:37
Senior Machine Learning Engineer,Doublu,5 - 10 years,30-37.5 Lacs P.A.,[],"We are hiring Senior Machine Learning Engineer for an MNC\n\nJob Type : Direct , Fulltime role\n\nLocation : PAN India (Remote)\n\nSenior Machine Learning Engineer\n\nResponsibilities\n\nDevelop, implement, and maintain machine learning models using scikit-learn and SciPy.\nBuild and deploy ML models for production use cases.\nWork with regression models and optimization techniques.\nDevelop and integrate APIs using the Flask framework.\nUtilize GCP services (App Engine, Cloud Tasks, Dataflow, BigQuery, Bigtable, Vertex AI).\nOptimize CI/CD pipelines using Azure DevOps.\nCollaborate with cross-functional teams to deploy scalable ML solutions.\nQualifications\n\nStrong proficiency in Python and core ML libraries Scikit-learn and SciPy.\nHands-on experience with regression models and optimization techniques.\nExperience with Flask for API development and deployment.\nProficiency in GCP services and ML operations on cloud infrastructure.\nExperience with CI/CD tools, especially Azure DevOps.\nSolid understanding of data structures, algorithms, and software engineering practices.\nFamiliarity with Agile methodologies and technical documentation.\nStrong analytical, communication, and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Scipy', 'Bigquery', 'Vertex Ai', 'Machine Learning', 'Scikit-Learn', 'GCP', 'Ml Deployment', 'Mlops', 'Azure Devops', 'Flask']",2025-06-12 14:57:40
MDM Data Scientist,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking an accomplished and visionary Data Scientist/ GenAIdeveloper to join Amgens Enterprise Data Management team. As part of MDM team, you will be responsible for designing, developing, and deploying Generative AI and ML models to power data-driven decisions across business domains. This role is ideal for an AI practitioner who thrives in a collaborative environment and brings a strategic mindset to applying advanced AI techniques to solve real-world problems.To succeed in this role, the candidate must have strong AI/ML, Data Science, GenAI experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have AI/ML, data science and GenAI experience on technologies like (PySpark/PyTorch, TensorFlow, LLM, Autogen, Hugging FaceVectorDB,Embeddings, RAGsetc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nDevelop enterprise-level GenAI applications using LLM frameworks such as Langchain, Autogen, and Hugging Face.\nDesign and develop intelligent pipelines using PySpark, TensorFlow, and PyTorch within Databricks and AWS environments.\nImplement embedding models andmanage VectorStores for retrieval-augmented generation (RAG) solutions.\nIntegrate and leverage MDM platforms like Informatica and Reltio to supply high-quality structured data to ML systems.\nUtilize SQL and Python for data engineering, data wrangling, and pipeline automation.\nBuild scalable APIs and services to serve GenAI models in production.\nLead cross-functional collaboration with data scientists, engineers, and product teams to scope, design, and deploy AI-powered systems.\nEnsure model governance, version control, and auditability aligned with regulatory and compliance expectations.\nBasic Qualifications and Experience:\nMasters degree with 4 - 6 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 6 - 9 years of experience in Business, Engineering, IT or related field OR\nDiploma with 10 - 12 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\n6+ years of experience working in AI/ML or Data Science roles, including designing and implementing GenAI solutions.\nExtensive hands-on experience with LLM frameworks and tools such as Langchain, Autogen, Hugging Face, OpenAI APIs, and embedding models.\nStrong programming background with Python, PySpark, and experience in building scalable solutions using TensorFlow, PyTorch, and SK-Learn.\nProven track record of building and deploying AI/ML applications in cloud environments such as AWS.\nExpertise in developing APIs, automation pipelines, and serving GenAI models using frameworks like Django, FastAPI, and DataBricks.\nSolid experience integrating and managing MDM tools (Informatica/Reltio) and applying data governance best practices.\nGuide the team on development activities and lead the solution discussions\nMust have core technical capabilities in GenAI, Data Science space\nGood-to-Have Skills:\nPrior experience in Data Modeling, ETL development, and data profiling to support AI/ML workflows.\nWorking knowledge of Life Sciences or Pharma industry standards and regulatory considerations.\nProficiency in tools like JIRA and Confluence for Agile delivery and project collaboration.\nFamiliarity with MongoDB, VectorStores, and modern architecture principles for scalable GenAI applications.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nData Science and ML Certification\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['MDM', 'GenAI', 'Langchain', 'PySpark', 'VectorStores', 'Hugging Face', 'LLM', 'Data Science', 'DataBricks', 'SK-Learn', 'AI/ML', 'Autogen', 'PyTorch', 'Django', 'OpenAI APIs', 'FastAPI', 'MongoDB', 'Data Modeling', 'PySpark/PyTorch', 'TensorFlow', 'Python']",2025-06-12 14:57:42
"Senior Python Developer (Machine Learning,Data Analysis,Visualization)",Synechron,3 - 5 years,Not Disclosed,"['Pune', 'Hinjewadi']","Software Requirements\nRequired Skills:\nProficiency in Python (version 3.6+) with experience in data analysis, manipulation, and scripting\nKnowledge of SQL for data extraction, transformation, and database querying\nExperience with data visualization tools such as PowerBI, Tableau, or QlikView\nFamiliarity with AI and Machine Learning frameworks such as TensorFlow, Keras, PyTorch, or equivalent\nHands-on experience in developing, deploying, and optimizing machine learning models\nPreferred Skills:\nExperience with R for data analysis\nFamiliarity with cloud platforms like AWS, Azure, or GCP for deploying AI solutions\nKnowledge of version control systems such as Git\nOverall Responsibilities\nAnalyze, interpret, and visualize large and complex datasets to extract actionable insights\nDesign, develop, and implement machine learning and AI models for predictive and prescriptive analytics\nCollaborate with cross-functional teams to understand business requirements and translate them into data-driven solutions\nCommunicate findings, insights, and recommendations via reports, dashboards, and presentations to stakeholders\nEvaluate and refine models and algorithms to maximize accuracy, efficiency, and impact\nStay informed on emerging AI, Data Science, and analytics trends and incorporate best practices into projects\nSupport automation efforts, optimize data pipelines, and enhance existing analytical workflows\nContribute to organizational learning by sharing knowledge and mentoring team members\nStrategic objectives:\nDrive innovation through the application of AI and machine learning\nEnable data-driven decision-making across business units\nImprove operational efficiencies and business outcomes\nPerformance outcomes:\nAccurate, robust, and scalable AI models\nHigh-quality insights delivered on time and aligned with business needs\nWell-documented solutions and knowledge-sharing artifacts\nTechnical Skills (By Category)\nProgramming Languages (Essential):\nPython (required); experience with R is a plus\nSQL (required); experience with data manipulation and querying\nData Analysis & Visualization Tools (Essential):\nPowerBI, Tableau, or QlikView\nFrameworks & Libraries (Essential):\nTensorFlow, Keras, PyTorch, or similar frameworks for AI/ML development\nData Management & Databases (Essential):\nRelational databases (e.g., MySQL, PostgreSQL, Oracle)\nData extraction and transformation (ETL processes)\nCloud & Deployment (Preferred):\nExperience deploying models on cloud platforms such as AWS, Azure, GCP\nDevelopment & Version Control (Preferred):\nGit for code versioning\nOther Skills:\nStrong statistical knowledge and experience with data preprocessing, feature engineering\nFamiliarity with agile development methodologies\nExperience Requirements\n3 to 5 years of relevant experience in AI, Data Science, or Data Analytics roles\nProven track record applying machine learning techniques to real-world problems\nExperience working with large datasets and scalable data pipelines\nExperience collaborating with cross-functional teams to deliver analytics-driven solutions\nIndustry experience in finance, healthcare, retail, or similar data-rich sectors is preferred\nAlternative pathways:\nCandidates with extensive AI & ML project experience, strong programming skills, and relevant certifications can be considered with slightly varied years of experience\nDay-to-Day Activities\nCollect, clean, and explore large datasets to identify patterns and insights\nDevelop and tune machine learning models to address business problems\nCollaborate with business analysts, data engineers, and product owners to align technical solutions with organizational goals\nDocument methodologies, code, and analytical findings to ensure reproducibility and knowledge sharing\nCreate dashboards, visualizations, and reports to communicate insights effectively\nEvaluate model performance regularly and optimize models for accuracy and efficiency\nParticipate in team meetings, project planning, and review sessions\nKeep abreast of advancements in AI/ML technologies, tools, and best practices\nQualifications\nBachelors degree in Computer Science, Data Science, Statistics, or related field\nMasters degree or higher in AI, Data Science, or related disciplines is a plus\nProfessional certifications in AI/ML (e.g., TensorFlow Developer, AWS Machine Learning Specialty) are advantageous\nWilling to learn new tools and stay updated with emerging AI trends\nAbility to work independently and collaborate effectively in a dynamic environment\nProfessional Competencies\nAnalytical and problem-solving mindset with a focus on actionable insights\nExcellent verbal and written communication skills for diverse audiences\nStrong interpersonal skills and stakeholder management\nAdaptability to fast-changing technology landscapes\nGrowth mindset with continuous learning enthusiasm\nOrganizational skills to handle multiple projects and priorities simultaneously\nInnovation-driven approach and proactive problem resolution",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'PostgreSQL', 'MySQL', 'Data Analysis', 'Data Visualization', 'Oracle', 'ETL', 'Machine Learning']",2025-06-12 14:57:45
Senior Machine Learning Engineer - NLP,Avalara Technologies,5 - 8 years,Not Disclosed,[],"What You'll Do\nWe are looking for experienced Machine Learning Engineers with a background in software development and a deep enthusiasm for solving complex problems. You will lead a dynamic team dedicated to designing and implementing a large language model framework to power diverse applications across Avalara. Your responsibilities will span the entire development lifecycle, including conceptualization, prototyping and delivery of the LLM platform features.\nYou will have a blend of technical skills in the fields of AI & Machine Learning especially with LLMs and a deep-seated understanding of software development practices where you'll work with a team to ensure our systems are scalable, performant and accurate. You will be reporting to Senior Manager, AI/ML.",,,,"['Machine Learning', 'LLMs', 'Prometheus', 'Grafana', 'NLP', 'Docker', 'Terraform', 'MLFlow', 'Postgres', 'AWS', 'GitLab', 'Python', 'Kubernetes']",2025-06-12 14:57:47
Senior Machine Learning Engineer,Bebo Technologies,3 - 8 years,Not Disclosed,"['Chandigarh', 'Pune', 'Delhi / NCR']","3+ years of experience in software engineering and ML development.\nStrong proficiency in Python and ML libraries such as Scikit-learn, TensorFlow, or PyTorch.\nExperience building and evaluating models, along with data preprocessing and feature engineering.\nProficiency in REST APIs, Docker, Git, and CI/CD tools.\nSolid foundation in software engineering principles, including data structures, algorithms, and design patterns.\nHands-on experience with MLOps platforms (e.g., MLflow, TFX, Airflow, Kubeflow).\nExposure to NLP, large language models (LLMs), or computer vision projects.\nExperience with cloud platforms (AWS, GCP, Azure) and managed ML services.\nContributions to open-source ML libraries or participation in ML competitions (e.g., Kaggle, DrivenData) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Tensorflow', 'Machine Learning', 'Pytorch', 'Keras', 'Scikit-Learn', 'Python']",2025-06-12 14:57:49
Data Engineer _Technology Lead,Broadridge,6 - 10 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nAnalyzes and solve problems using technical experience, judgment and precedents\nProvides informal guidance to new team members\nExplains complex information to others in straightforward situations\n1. Data Engineering and Modelling:\nDesign & Develop Scalable Data Pipelines: Leverage AWS technologies to design, develop, and manage end-to-end data pipelines with services like .",,,,"['Star Schema', 'Snowflake', 'AWS', 'Apache Airflow']",2025-06-12 14:57:52
Data Scientist,Ltimindtree,8 - 13 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']",We are looking for an experienced AI ML Developers experience in data science specializing in machine learning python statistical modelling and big data technologies pyspark sql.\n\nThe ideal candidate will have a strong background in developing and deploying machine learning models optimizing ML pipelines and handling largescale structured and unstructured data to drive business impact.\n\nDeep understanding of supervised and unsupervised learning including regression classification Multiclass classification clustering and NLP Proficiency in statistical analysis AB testing and causal inference techniques Experience with model deployment and MLOps in cloud environments AWS GCP \n\nKey Responsibilities\n\nDevelop and deploy machine learning models and predictive analytics solutions for business impact\nWork with largescale structured and unstructured data to extract insights and build scalable models\nDesign implement and optimize ML pipelines for realtime and batch processing\nCollaborate with engineering product and business stakeholders to translate business problems into data science solutions\nApply statistical modeling AB testing and causal inference techniques to evaluate business performance\nApply machine learning and statistical techniques for audience segmentation helping to identify patterns and optimise business strategies\nDrive research and innovation by staying updated with cuttingedge MLAI advancements and incorporating them into our solutions\nOptimize data science models for performance scalability and interpretability in production environments\nMentor junior data scientists and contribute to best practices in data science and engineering,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Aiml', 'Ab Testing']",2025-06-12 14:57:54
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Bengaluru'],"Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 14:57:57
Data Scientist,Ltimindtree,7 - 12 years,Not Disclosed,['Hyderabad'],Data Scientist\n\nJob Description\n\nResponsibilities\n\nWork with team members across multiple disciplines to understand the data behind product features user behaviors the security landscape and our goals\nAnalyze data from several large sources then automate solutions using scheduled processes models and alerts\nWork with partners to design and improve metrics that guide our decisions for the product\nDetect patterns associated with fraudulent accounts and anomalous behavior\nSolve scientific problems and create new methods independently\nTranslate requirements and security questions into data insights\nSet up alerting mechanisms so our leadership is always aware of the security posture\n\nQualifications\n\nPostgraduate degree with specialization in machine learning artificial intelligence statistics or related fields or 2 years of equivalent work experience in applied machine learning and analytics\nExperience with SQL Snowflake and NoSQL databases\nProficiency in Python programming\nFamiliarity with statistics modeling and data visualization\n\nExperience\n\nExperience building statistical and machine learning models applying techniques such as regression classification clustering and anomaly detection Time series and Classical ML modeling\nFamiliarity with Snowflake SQL\nFamiliarity with cloud platforms such as AWS\nSome experience to software development or data engineering\nAnalyze business problems or research questions identify relevant data points and extract meaningful insights,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Snowflake Sql', 'AWS']",2025-06-12 14:57:59
Data Scientist,Ltimindtree,8 - 13 years,19-34 Lacs P.A.,"['Pune', 'Bengaluru', 'Mumbai (All Areas)']",10 years of experience in Data ScienceML domain\nShould have experience in Python and libraries like pandas numpy scikitlearn etc\nHave worked on building ML models and integrating it with application end to end\nHave knowledge on Recommender engines and the ML models running behind it like ALS and LightFM\nHave experience in Azure Machine Learning and Azure Services\nHave experience in deploying models in cloud environment and exposing it as an API\nGood communication and presentation skill\nAbility to deliver ML projects as an individual contributor,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Data Science', 'Ml']",2025-06-12 14:58:01
Engineer,Qualcomm,10 - 15 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nJob Overview:\n\nThe Qualcomm Cloud Computing team is developing hardware and software for Machine Learning solutions spanning the data center, edge, infrastructure, automotive market. We are seeking ambitious, bright, and innovative engineers with experience in machine learning framework development. Job activities span the whole product life cycle from early design to commercial deployment. The environment is fast-paced and requires cross-functional interaction daily so good communication, planning and execution skills are a must.\n\nWe are seeking a highly skilled and motivated Language Model Engineer to join our team. The primary role of the engineer will be to train Large Language Models (LLMs) from scratch and fine-tune existing LLMs on various datasets using state-of-the-art techniques.\n\nResponsibilities:\n\n\n\nModel Training and Fine-tuning: Train LLMs from scratch using various datasets. Fine-tune pre-trained models on specific tasks or datasets to improve performance. Implement state-of-the-art LLM training techniques such as Reinforcement Learning from Human Feedback (RLHF), ZeRO (Zero Redundancy Optimizer), Speculative Sampling, and other speculative techniques.\n\n\nData Management: Handle large datasets effectively. Ensure data quality and integrity. Implement data cleaning and preprocessing techniques. Hands-on with EDA is a plus.\n\n\nModel Evaluation: Evaluate model performance using appropriate metrics. Understand the trade-offs between different evaluation metrics.\n\n\nLLM metrics: Sound understanding of various LLM metrics like MMLU, Rouge, BLEU, Perplexity etc.\n\nAWQ: Understanding of Quantization is a plus. Knowledge on QAT will be a plus.\n\n\nResearch and Development: Stay updated with the latest research in NLP and LLMs. Implement state-of-the-art techniques and contribute to research efforts.\n\n\nCollaboration: Work closely with other teams to understand requirements and implement solutions.\n\n\nRequired Skills and Experience:\n\n\nDeep Learning Frameworks: Hands-on experience with PyTorch at a granular level. Familiarity with tensor operations, automatic differentiation, and GPU acceleration in PyTorch.\n\n\nNLP and LLMs: Strong understanding of Natural Language Processing (NLP) and experience working with LLMs.\n\n\nProgramming: Proficiency in Python and experience with software development best practices.\n\n\nData Handling: Experience working with large datasets. Familiarity with data version control tools is a plus.\n\n\nEducation: A degree in Computer Science, Machine Learning, AI, or related field. Advanced degree is a plus.\n\n\nCommunication: Excellent written and verbal communication skills.\n\n\nWork experience : Open, 2- 10 years of relevant experience.\n\n\nPreferred\n\nSkills:\n\n\n\nOptimization: Knowledge of optimization techniques for training large models.\n\n\nNeural Architecture Search (NAS): Experience with NAS techniques for optimizing model architectures is a plus. Hands-on experience with CUDA, CUDNN is a plus.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cuda', 'python', 'natural language processing', 'pytorch', 'machine learning algorithms', 'nas', 'eda', 'version control', 'sampling', 'machine learning', 'deep learning', 'data center', 'computer science', 'product life cycle', 'research and development', 'software engineering']",2025-06-12 14:58:04
Full Stack Data Scientist,Vimo Getinsured,2 - 7 years,Not Disclosed,['Gurugram( Sector 61 Gurgaon )'],"About the Role\nAs a Data Science Engineer, you will need strong technical skills in data modeling, machine learning, data engineering, and software development. You will have the ability to conduct literature reviews and critically evaluate research papers to identify applicable techniques. Additionally, you should be able to design and implement efficient and scalable data processing pipelines, perform exploratory data analysis, and collaborate with other teams to integrate data science models into production systems. Passion for conversational AI and a desire to solve some of the most complex problems in the Natural Language Processing space are essential. You will work on highly scalable, stable, and automated deployments, aiming for high performance. Taking on the challenge of building and scaling a truly remarkable AI platform to impact the lives of millions of customers will be part of your responsibilities. Working in a challenging yet enjoyable environment, where learning new things is the norm, you should think of solutions beyond boundaries. You should also drive outcomes with full ownership, deeply believe in customer obsession, and thrive in a fast-paced environment of learning and innovation.\nYou will work in a challenging, consumer-facing problem space, where you can make an immediate impact. You will get to work with the latest technologies, learn to use new tools and get the opportunity to have your say in the final product. Youll work alongside a great team in an open, collaborative environment. We are part of Vimo, a well-funded, stable mid-size company with excellent salaries, medical/dental/vision coverage, and perks. Vimo is an Equal Opportunity Employer.",,,,"['python', 'Langchain', 'Neural Networks', 'LLM', 'Linux', 'Data Structures', 'Natural Language Processing', 'Jupyter Notebook', 'Machine Learning', 'Deep Learning', 'Numpy', 'Data Science', 'pandas', 'Nltk', 'Langgraph', 'Transformers', 'BERT', 'langsmith']",2025-06-12 14:58:06
Assistant Data Scientist,Rocket Software,0 - 1 years,Not Disclosed,['Pune'],"Face to Face interview in Pune . Please apply only if you are available for a Face to Face interview .\n\nJob highlights\n\nRequired Qualifications . 0 -2 years of relevant industry experience or fresh graduates are welcome to apply.\nBasic experience or understanding in applying Data Science methodologies to extract, process, and transform data from multiple sources.\nPreferred Qualifications . Bachelors degree in Data Science , AI, Statistics ,Computer Science, Economics, or a directly related field.\n\nEssential Duties and Responsibilities\n\nAssist in developing, fine-tuning, and deploying machine learning models.\nAid in consulting with key internal and external stakeholders to understand and frame model requirements and potential applications.\nParticipate in the development of sound analytic plans based on available data sources, business partner needs, and required timelines.\nWork with software engineers in integrating trained models into end-user applications.\nHelp manage deliverables across multiple projects in a deadline-driven environment.\nPresent results, insights, and recommendations to both technical and non-technical stakeholders.\n\nRequired Qualifications\n\n0 -2 years of relevant industry experience or fresh graduates are welcome to apply.\nGood knowledge of Python and Linux, familiarity with ML frameworks, and a willingness to learn.\nDemonstrated problem-solving abilities and creative thinking.\nBasic experience or understanding in applying Data Science methodologies to extract, process, and transform data from multiple sources.\nExcellent communication and interpersonal skills.\nMust be comfortable working in a team-oriented environment.\n\nPreferred Qualifications\n\nBachelor's degree in Statistics, Computer Science, Economics, or a directly related field.\nMasters degree or current enrollment in a Masters program in Statistics, Computer Science, Mathematics, Economics, or directly related fields is a plus.\nDemonstrated passion for continued learning and innovation.\nAs a Data Science Assistant, we expect not just skills and qualifications, but also an enthusiasm for learning and growing within our team. We value those who are adaptable, innovative, and ready to take on challenges in a fast-paced work environment.\n\nDiversity, Inclusion & Equity\n\nAt Rocket we are committed to an inclusive workplace environment, where every Rocketeer can thrive by bringing their full selves to work. Being a Rocketeer means you are part of our movement to continually drive inclusivity, diversity and equity in our workforce.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'NLP', 'Natural Language Processing', 'Artificial Intelligence', 'Machine Learning', 'Python']",2025-06-12 14:58:08
Technical Specialist - Data Scientist,Fidelity International,8 - 9 years,Not Disclosed,['Gurugram'],"Application Deadline: 21 June 2025\nTitle Senior Analyst- Data Scientist\nDepartment Data Value\nLocation Gurgaon\nReports To Suman Kaur\nLevel 3\nWe re proud to have been helping our clients build better financial futures for over 50 years. How have we achieved this? By working together - and supporting each other - all over the world. So, join our Data Value team and feel like you re part of something bigger.\nAbout your team\nData Value team drives the renewed focus of extracting value from Fidelity s data for business and client insights and working as one voice with the business, technology, and data teams. The team s vision is to create measurable business impact by leveraging technology and utilising the skills to generate valuable insights and streamline engagements. The Data Science function within Data Value supports Fidelity International s Sales, Marketing, Propositions, Risk, Finance, Customer Service and HR teams across the globe. The key objectives of the function are:\nTo develop deep customer insights for our businesses helping them segment and target customers more effectively\nTo develop a fact-based understanding of sales trends and identify actionable sales growth opportunities for each of our sales channels\nTo understand customer preferences in terms of products, service attributes and marketing activity to help refine each of these\nTo help develop new services lines e.g. develop customer analytics for key IFAs, DC Clients, Individual clients etc.\nTo develop market and competitive intelligence in our key markets to help shape our business planning in those markets\nThe function works directly with business heads and other senior stakeholder s stakeholders to identify areas of analytics, define problem statements and develop key insights.\nAbout your role\nYou will be expected to take a leading role in developing the Data Science and Advanced Analytics solutions for our business. This will involve:\nEngaging with the key stakeholders to understand Fidelity s sales, marketing, client services and propositions context\nImplement advanced analytics solutions on On-Premises/Cloud platforms, develop proof-of-concepts and engage with internal and external ecosystem to progress the proof of concepts to production.\nEngaging and collaborating with different other internal teams like Data engineering, DevOps, technology team etc for development of new tools, capabilities, and solutions.\nMaximize Adoption of Cloud Based advanced analytics solutions: Build out sandbox analytics environments using Snowflake, AWS, Adobe, Salesforce.\nAbout you\nKey Responsibilities\nDeveloping and Delivering Data Science solutions for business (40%)\nPartner with internal (FIL teams) & external ecosystem to design and deliver advanced analytics enabled Data Science solutions\nCreate advanced analytics solution on quantitative and text data using Artificial Intelligence, Machine Learning and NLP techniques.\nCreate compelling visualisations that enable the smooth consumption of predictions and insights for customer benefit\n. Stakeholder Management (30%)\nWorks with channel heads/stakeholders and other sponsors understand the business problem and translate it into appropriate analytics solution.\nEngages with key stakeholders for smooth execution, delivery, and implementation of solutions\nAdoption of Cloud enabled Data Science solutions: (20%)\nMaximize Adoption of Cloud Based advanced analytics solution\nBuild out sandbox analytics environments using Snowflake, AWS, Adobe, Salesforce\nDeploy solutions in productions while adhering to best practices involving Model Explainability, MLOps, Feature Stores, Model Management, Responsible AI etc\nCollaboration and Ownership (10%)\nSharing of knowledge, best practices with the team including coaching or training in some of deep learning/machine learning methodologies. Provides mentoring, coaching, and consulting advice and guidance to staff, e.g. analytic methodologies, data recommendations\nTakes complete independent ownership of the projects and the initiatives in the team with the minimal support\nExperience and Qualifications Required\nQualifications:\nEngineer from IIT/Master s in field related to Data Science/Economics/Mathematics (Tie1 Institutions like ISI, Delhi School of Economics)/M.B.A from tier 1 institutions\nMust have Skills & Experience Required:\nOverall, 8+ years of experience in Data Science and Analytics\n5+ years of hands-on experience in - Statistical Modelling /Machine Learning Techniques/Natural Language Processing/Deep Learning\n5+ years of experience in Python/Machine Learning/Deep Learning\nExcellent problem-solving skills\nShould be able to run analytics applications such as Python, SAS and interpret statistical results\nImplementation of models with clear measurable outcomes\nGood to have Skills & Experience Required:\nAbility to engage in discussion with senior stakeholders on defining business problems, designing analyses projects, and articulating analytical insights to stakeholders.\nExperience on SPARK/Hadoop/Big Data Platforms is a plus\nExperience with unstructured data and big data\nExperience with secondary data and knowledge of primary market research is a plus.\nAbility to independently own and manage the projects with minimal support.\nExcellent analytical skills and a strong sense for structure and logic\nAbility to develop, test and validate hypotheses.\nFeel rewarded",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAS', 'Senior Analyst', 'Consulting', 'Machine learning', 'Business planning', 'Competitive intelligence', 'Customer service', 'Adobe', 'Stakeholder management', 'Salesforce']",2025-06-12 14:58:10
Graph Engineer- Data Science,HARMAN,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Description\nIntroduction: Digital Transformation Solutions (DTS)\n.\nExtensive experience in defining, developing, and implementing security software, ideally with a strong embedded firmware development background\nAbout the Role\nThis position offers an opportunity to work in a globally distributed team where you will get a unique opportunity of personal development in a multi-cultural environment. You will also get a challenging environment to develop expertise in the technologies useful in the industry.",,,,"['Computer science', 'Product quality', 'UML', 'XML', 'Relationship', 'Javascript', 'HTML', 'Oracle', 'Automotive', 'Python']",2025-06-12 14:58:12
"Associate Director, Data Science/Software Engineering",ATT Communication Services,10 - 15 years,Not Disclosed,['Bengaluru'],"Associate Director, Data Science/Software Engineering:\nAT&T is one of the leading service providers in the telecommunication sector and propelling it into the data and AI driven era is powered by CDO (Chief Data Office) . CDO is empowering AT&T, through execution, self-service, and as a data and AI center of excellence, to unlock transformative insights and actions that drive value for the company and its customers.\nEmployees in CDO imagine, innovate, and unlock data & AI driven insights and actions that create value for our customers and the enterprise. Part of the work, we govern data collection and use, mitigate for potential bias in machine learning models, and encourage an enterprise culture of responsible AI.\nAT&T s Chief Data Office (CDO) is harnessing data and making AT&T s data assets and ground-breaking AI functionality accessible to employees across the firm. In addition, our talented employees are a significant component that contributes to AT&T s place as the U.S. company with the sixth most AI-related patents. CDO also maintains academic and tech partnerships to cultivate the next generation of experts in statistics and machine learning, statistical computing, data visualization, text mining, time series modelling, data stream and database management, data quality and anomaly detection, data privacy, and more.\nWe are looking for an accomplished and visionary professional for the role of Associate Director, Data Science/Software Engineering to join our team and lead the development of cutting-edge software solutions. This is a hands-on leadership position that requires the fine balance of supervising and leading people while providing significant technical contributions to the projects you will be responsible for. As a key technical leader, you will leverage your expertise in full-stack development, DevOps best practices, Data analysis, AI/ML and Generative AI to lead your team in creating scalable, reliable, and efficient systems.\nThis role demands a strategic thinker and hands-on contributor who can work across multiple teams, drive innovation, and ensure technical excellence. You will be instrumental in shaping the technical roadmap, mentoring teams, and delivering transformative solutions that align with business objectives.\nKey Responsibilities:\nTechnical Leadership:\nDefine and drive the technical vision and architecture for scalable, resilient, and secure full-stack applications utilizing data powered insights.\nLead end-to-end software development projects from concept to deployment and maintenance.\nCollaborate with cross-functional teams to translate business requirements into technical solutions.\nServe as a mentor and technical advisor to engineering teams, fostering a culture of innovation and excellence.\nFull-Stack Development:\nDesign and implement scalable and high-performance web applications using modern front-end and back-end frameworks (e.g., React, Angular, Node.js, Python, Java).\nDevelop modular and reusable APIs (RESTful or GraphQL) with an emphasis on maintainability and performance.\nEnsure seamless integration of front-end and back-end systems while maintaining best practices for UI/UX design.\nOptimize database structures and queries for both relational (e.g., MySQL, PostgreSQL) and non-relational (e.g., MongoDB, DynamoDB) databases.\nDevOps and Automation:\nArchitect and implement CI/CD pipelines to streamline build, test, and deployment processes.\nEnsure seamless deployment and scalability of applications through containerization tools (e.g., Docker) and orchestration platforms (e.g., Kubernetes).\nLeverage infrastructure-as-code solutions (e.g., Terraform, Ansible) to automate infrastructure provisioning and management.\nMonitor application performance, troubleshoot issues, and ensure high availability through tools like Prometheus, Grafana, or New Relic.\nShell Scripting and Automation:\nDevelop and maintain shell scripts to automate routine tasks, system monitoring, and application deployments.\nDebug and troubleshoot production issues using scripting techniques to ensure minimal downtime.\nEnhance system efficiency by automating log analysis, error detection, and reporting.\nStrategic Contribution:\nCollaborate with stakeholders to align technical priorities with business goals.\nEvaluate emerging technologies and tools to recommend and implement solutions that advance the organization s technical capabilities.\nEstablish and enforce software engineering best practices, ensuring robust security, scalability, and maintainability.\nQualifications:\nEducation:\nBachelor s or Master s degree in Computer Science, Software Engineering, or a related field. A Ph.D. is a plus.\nExperience:\n13+ years of experience in software engineering, including hands-on experience with full-stack development and DevOps practices.\nProven track record of delivering large-scale, high-impact software solutions in a leadership capacity.\nTechnical Expertise:\nAdvanced proficiency in front-end frameworks (React, Angular, or Vue.js) and back-end technologies (Node.js, Python, Java, Go, etc.).\nStrong experience with DevOps tools (Jenkins, GitLab CI/CD, Docker, Kubernetes).\nDeep understanding of cloud platforms (AWS, Azure, GCP), including architecture and deployment strategies.\nSolid grasp of database technologies (SQL and NoSQL) and optimization techniques.\nProficiency in writing, debugging, and maintaining shell scripts for automation and system monitoring.\nStrong knowledge of microservices architecture, API gateways, and distributed systems.\nSoft Skills:\nExceptional problem-solving and critical-thinking abilities.\nStrong leadership and mentoring skills, with the ability to inspire and guide teams.\nExcellent communication skills, both written and verbal, to collaborate effectively with technical and non-technical stakeholders.\nStrategic mindset, capable of balancing technical depth with business impact.\nPreferred Qualifications:\nExperience with serverless computing frameworks (e.g., AWS Lambda).\nCertifications in cloud platforms (e.g., AWS Certified Solutions Architect, Azure DevOps Engineer Expert).\nKnowledge of security best practices in software development and DevOps.\n#DataEngineering\nLocation:\nIND:KA:Bengaluru / Innovator Building, Itpb, Whitefield Rd - Adm: Intl Tech Park, Innovator Bldg\nJob ID R-66889 Date posted 05/14/2025",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data analysis', 'Front end', 'Postgresql', 'MySQL', 'Shell scripting', 'Telecommunication', 'SQL', 'Python']",2025-06-12 14:58:14
Data Engineer,HARMAN,5 - 10 years,Not Disclosed,['Bengaluru'],"-Strong analytical thinking and problem-solving skills, with the ability to translate complex data into actionable insights\n-Excellent communication skills, with the ability to effectively convey complex findings to both technical and non-technical stakeholders.\nCandidate to work form SRIB Bangalore with 3 days working from office is mandatory\n  What You Will Do",,,,"['Digital media', 'CTV', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Automotive', 'Python']",2025-06-12 14:58:17
Data Scientist For DMAI,Prodapt Solutions,2 - 5 years,Not Disclosed,['Chennai'],"Overview\n\nThe Senior Data Science Engineer will leverage advanced data science techniques to solve complex business problems, guide decision-making processes, and mentor junior team members. This role requires a combination of technical expertise in data analysis, machine learning, and project management skills.\n\nResponsibilities\n\n Data Analysis and Modeling Analyze large-scale telecom datasets to extract actionable insights and build predictive models for network optimization and customer retention.\n Conduct statistical analyses  to validate models and ensure their effectiveness.\n Machine Learning Development Design and implement machine learning algorithms for fraud detection, churn prediction, and network failure analysis.\n Telecom-Specific Analytics Apply domain knowledge to improve customer experience by analyzing usage patterns, optimizing services, and predicting customer lifetime value.\n ETL Processes Develop robust pipelines for extracting, transforming, and loading telecom data from diverse sources.\n Collaboration Work closely with data scientists, software engineers, and telecom experts to deploy solutions that enhance operational efficiency.\n Data Governance :  Ensure data integrity, privacy, security and compliance with industry standards\n\n\nAdvanced degree in Data Science, Statistics, Computer Science, or a related field.\nExtensive experience in data science roles with a strong focus on machine learning and statistical modeling.\nProficiency in programming languages such as Python or R and strong SQL skills.\nFamiliarity with big data technologies (e.g., Hadoop, Spark) is advantageous.\nExpertise in cloud platforms such as AWS or Azure.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'data analysis', 'machine learning', 'sql', 'statistical modeling', 'algorithms', 'python', 'big data technologies', 'microsoft azure', 'cloud platforms', 'r', 'data science', 'spark', 'data governance', 'hadoop', 'aws', 'etl', 'machine learning algorithms', 'statistics']",2025-06-12 14:58:19
Data Scientist,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nThe Data Scientist is responsible for developing and implementing AI-driven solutions to enhance cybersecurity measures within the organization. This role involves leveraging data science techniques to analyze security data, detect threats, and automate security processes. The Data Scientist will work closely with cybersecurity teams to identify data-driven automation opportunities, strengthening the organizations security posture.\nRoles & Responsibilities:\nDevelop analytics to address security concerns, enhancements, and capabilities to improve the organization's security posture.\nCollaborate with Data Engineers to translate security-focused algorithms into effective solutions.\nWork in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.\nPerform exploratory and targeted data analyses using descriptive statistics and other methods to identify security patterns and anomalies.\nDesign and implement security-focused analytics pipelines leveraging MLOps practices.\nCollaborate with data engineers on data quality assessment, data cleansing, and the development of security-related data pipelines.\nContribute to data engineering efforts to refine data infrastructure and ensure scalable, efficient security analytics.\nGenerate reports, annotated code, and other projects artifacts to document, archive, and communicate your work and outcomes.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python) OR\nBachelors degree and 3 to 5 years of experience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python) OR\nDiploma and 7 to 9 years of experience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python)\nPreferred Qualifications:\nExperience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python)\nDemonstrated skill in the use of applied analytics, descriptive statistics, feature extraction and predictive analytics on industrial datasets\nStrong foundation in machine learning algorithms and techniques\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification\nGood-to-Have Skills:\nProficiency in Python and relevant ML libraries (e.g., TensorFlow, PyTorch, Scikit-learn)\nOutstanding analytical and problem-solving skills; Ability to learn quickly; Excellent communication and interpersonal skills\nExperience with data engineering and pipeline development\nExperience in analyzing time-series data for forecasting and trend analysis\nExperience with AWS, Azure, or Google Cloud\nExperience with Databricks platform for data analytics and MLOps\nExperience with Generative AI models (e.g., GPT, DALLE, Stable Diffusion) and their applications in cybersecurity and data analysis\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAny AWS Developer certification (preferred)\nAny Python and ML certification (preferred)\nAny SAFe Agile certification (preferred)\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'R', 'PyTorch', 'SAS', 'predictive analytics', 'Scikit-learn', 'SPSS', 'machine learning', 'data engineering', 'Python', 'TensorFlow']",2025-06-12 14:58:22
Data Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon strives to be the worlds most customer-centric company, where customers can research and purchase anything they might want online\nWe set big goals and are looking for people who can help us reach and exceed them\nThe CPT Data Engineering & Analytics (DEA) team builds and maintains critical data infrastructure that enhances seller experience and protects the privacy of Amazon business partners throughout their lifecycle\nWe are looking for a strong Data Engineer to join our team\n\nThe Data Engineer I will work with well-defined requirements to develop and maintain data pipelines that help internal teams gather required insights for business decisions timely and accurately\nYou will collaborate with a team of Data Scientists, Business Analysts and other Engineers to build solutions that reduce investigation defects and assess the health of our Operations business while ensuring data quality and regulatory compliance\n\nThe ideal candidate must be passionate about building reliable data infrastructure, detail-oriented, and driven to help protect Amazons customers and business partners\nThey will be an individual contributor who works effectively with guidance from senior team members to successfully implement data solutions\nThe candidate must be proficient in SQL and at least one scripting language (e\ng\nPython, Perl, Scala), with strong understanding of data management fundamentals and distributed systems concepts\n\n\nBuild and optimize physical data models and data pipelines for simple datasets\nWrite secure, stable, testable, maintainable code with minimal defects\nTroubleshoot existing datasets and maintain data quality\nParticipate in team design, scoping, and prioritization discussions\nDocument solutions to ensure ease of use and maintainability\nHandle data in accordance with Amazon policies and security requirements Masters degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent\n3+ years of data engineering experience\nExperience with SQL\nExperience with data modeling, warehousing and building ETL pipelines\nKnowledge of distributed systems concepts from data storage and compute perspective\nAbility to work effectively in a team environment Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nFamiliarity with big data technologies (Hadoop, Spark, etc\n)\nKnowledge of data security and privacy best practices\nStrong problem-solving and analytical skills\nExcellent written and verbal communication skills",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'data security', 'Perl', 'Data quality', 'Distribution system', 'Analytics', 'SQL', 'Python']",2025-06-12 14:58:24
Data Analyst-Having Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Analyst\n\nLocation: Bangalore\nExperience: 8 - 15 Yrs\nType: Full-time\n\nRole Overview\n\nWe are seeking a skilled Data Analyst to support our platform powering operational intelligence across airports and similar sectors. The ideal candidate will have experience working with time-series datasets and operational information to uncover trends, anomalies, and actionable insights. This role will work closely with data engineers, ML teams, and domain experts to turn raw data into meaningful intelligence for business and operations stakeholders.\n\nKey Responsibilities\n\nAnalyze time-series and sensor data from various sources\nDevelop and maintain dashboards, reports, and visualizations to communicate key metrics and trends.\nCorrelate data from multiple systems (vision, weather, flight schedules, etc) to provide holistic insights.\nCollaborate with AI/ML teams to support model validation and interpret AI-driven alerts (e.g., anomalies, intrusion detection).\nPrepare and clean datasets for analysis and modeling; ensure data quality and consistency.\nWork with stakeholders to understand reporting needs and deliver business-oriented outputs.\n\n\nQualifications & Required Skills\n\nBachelors or Masters degree in Data Science, Statistics, Computer Science, Engineering, or a related field.\n5+ years of experience in a data analyst role, ideally in a technical/industrial domain.\nStrong SQL skills and proficiency with BI/reporting tools (e.g., Power BI, Tableau, Grafana).\nHands-on experience analyzing structured and semi-structured data (JSON, CSV, time-series).\nProficiency in Python or R for data manipulation and exploratory analysis.\nUnderstanding of time-series databases or streaming data (e.g., InfluxDB, Kafka, Kinesis).\nSolid grasp of statistical analysis and anomaly detection methods.\nExperience working with data from industrial systems or large-scale physical infrastructure.\n\n\nGood-to-Have Skills\n\nDomain experience in airports, smart infrastructure, transportation, or logistics.\nFamiliarity with data platforms (Snowflake, BigQuery, Custom-built using open-source).\nExposure to tools like Airflow, Jupyter Notebooks and data quality frameworks.\nBasic understanding of AI/ML workflows and data preparation requirements.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Kafka', 'SQL', 'airports', 'InfluxDB', 'Airflow', 'structured Data', 'time-series', 'JSON', 'Tableau', 'Grafana', 'R', 'AI/ML', 'Kinesis', 'Snowflake', 'time-series databases', 'Data Preparation', 'Python', 'smart infrastructure', 'BigQuery', 'streaming data', 'Power BI', 'CSV', 'transportation', 'logistic', 'reporting tools']",2025-06-12 14:58:27
Data Scientist,Neoware Technology Solutions,3 - 7 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Scientist - Neoware Technology Solutions Private Limited\nRequirements\nDevelop predictive and prescriptive models to optimize business outcomes and drive growth.\nDesign and build Generative AI solutions to enhance business capabilities.\nWork with leading cloud platforms such as AWS, Azure, or GCP.\nProcess and analyze unstructured data using NLP and Computer Vision techniques.\nLead data-driven initiatives and collaborating with stakeholders to understand business needs and develop strategic solutions.\nConduct exploratory data analysis (EDA) to identify patterns, trends and insights in large, complex datasets.\nMentor and coach junior team members, providing technical guidance and fostering a culture of continuous learning and innovation.\nResponsibilities\nB.E. / Masters in Computer Science, Statistics, Applied Mathematics, Economics or a related quantitative field.\n3-7years of experience in data science, with a proven track record of delivering impactful business solutions.\nStrong proficiency in Python/R and SQL; experience with cloud platforms (AWS, Azure or GCP) is a plus.\nSolid understanding of machine learning techniques (classification, regression, clustering) and statistical methods.\nExcellent communication skills, with the ability to convey complex concepts to diverse audiences.\nStrong problem-solving abilities and capability to work both independently and in a team environment\nChennai / Bangalore / Mumbai\nPrincipal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Computer vision', 'Data analysis', 'GCP', 'Machine learning', 'Cloud', 'Business solutions', 'AWS', 'SQL', 'Python']",2025-06-12 14:58:29
Data Scientist,Swits Digital,5 - 12 years,Not Disclosed,['Chennai'],"Job Title: Data Scientist\nLocation: Chennai\nExperience: 5-12 Years\nJob Summary:\nWe are seeking a highly analytical and results-driven Data Scientist with a strong background in statistics , machine learning , and data science , combined with domain knowledge in mechanical engineering and cost analysis . The ideal candidate will have experience working with Google Cloud Platform (GCP) and will play a key role in transforming engineering and operational data into actionable insights to drive business decisions.\nRequired Skills & Experience:\nStrong knowledge of statistics , machine learning , and data science principles\nHands-on experience with Google Cloud Platform (GCP) , especially BigQuery , Vertex AI , and Cloud Functions\nProficiency in Python or R for data analysis and modeling\nSolid understanding of mechanical engineering concepts and their application in data analysis\nExperience with cost modeling , cost-benefit analysis , or operational performance analytics\nExcellent problem-solving , analytical thinking , and communication skills\nAbility to work with large datasets and create clear, actionable insights",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'data science', 'GCP', 'Analytical', 'Machine learning', 'Cost benefit analysis', 'Operations', 'Mechanical engineering', 'Analytics', 'Python']",2025-06-12 14:58:31
Data Engineer III,Expedia Group,5 - 10 years,Not Disclosed,['Bengaluru'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nData Engineer III\nIntroduction to the Team\nExpedia Technology teams partner with our Product teams to create innovative products, services, and tools to deliver high-quality experiences for travelers, partners, and our employees. A singular technology platform powered by data and machine learning provides secure, differentiated, and personalized experiences that drive loyalty and traveler satisfaction.\nExpedia Group is seeking a skilled and motivated Data Engineer III to join our Finance Business Intelligence team supporting the Product & Technology Finance organization. In this role, you will help drive data infrastructure and analytics solutions that support strategic financial planning, reporting, and operational decision-making across the Global Finance community. You ll work closely with Finance and Technology partners to ensure data accuracy, accessibility, and usability in support of Expedia s business objectives.\nAs a Data Engineer III, you have strong experience working with a variety of datasets, data environments, tools, and analytical techniques. You enjoy a fun, collaborative and stimulating team environment. Successful candidates should be able to own projects end-to-end, including identifying problems and solutions, building and maintain data pipelines and dashboards, distilling key insights and communicate to stakeholders.\nIn this role, you will:\nDevelop new and improve existing end to end Business Intelligence products (data pipelines, Tableau dashboards, and Machine Learning predictive forecasting models).\nDrive internal efficiencies through streamline code/documentation/Tableau development to maintain high data integrity.\nTroubleshoot and resolve production issues with the team products (automation opportunities, optimizations, back-end data issues, data reconciliations).\nProactively reach out to subject matter experts /stakeholders and collaborate to solve problems.\nRespond to ad hoc data requests and conduct analysis to provide valuable insights to stakeholders.\nCollaborate and coordinate with team members/stakeholders to translate complex data into meaningful insights, that improve the analytical capabilities of the business.\nApply knowledge of database design to support migration of data pipelines from on prem to cloud environment (including data extraction, ingestion, processing of large data sets)\nSupport dashboard development on cloud environment to enable self-service reporting.\nCommunicate clearly on current work status and design considerations\nThink broadly and comprehend the how, why, and what behind data architecture designs\nExperience & Qualifications:\nBachelor s in Computer Science, Mathematics, Statistics, Information Systems, or related field\n5+ years experience in a Data Analyst, Data Engineer or Business Analyst role\nProven expertise in SQL, with practical experience utilizing query engines including SQL Server, Starburst, Trino, Querybook and data science tools such as Python/R, SparkSQL.\nProficient visualization skills (Tableau, Looker, or similar) and excel modeling/report automation.\nExceptional understanding of relational and dimensional datasets, data warehouse and data mining and applies database design principles to solve data requirements\nExperience building robust data extract, load and transform (ELT) processes, that source data from multiple databases.\nDemonstrated record of defining and executing key analysis and solving problems with minimal supervision.\nDynamic individual contributor who consistently enhances operational playbooks to address business problems.\n3+ year working in a hybrid environment that uses both on-premise and cloud technologies is preferred.\nExperience working in an environment that manipulates large datasets on the cloud platform preferred.\nBackground in analytics, finance or a comparable reporting and analytics role preferred.\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Database design', 'Machine learning', 'Business intelligence', 'Data mining', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-12 14:58:33
Lead Data Engineer,Conduent,8 - 13 years,Not Disclosed,['Noida'],"Job Overview \n\nWe are looking for a Data Engineer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is the acquisition, transformation, loading and processing of data from a multitude of disparate data sources, including structured and unstructured data for advanced analytics and machine learning in a big data environment.\n\n\n",,,,"['sql coding', 'sql', 'configuration management', 'software engineering', 'release engineering', 'continuous integration', 'rdbms', 'sql queries', 'performance tuning', 'azure synapse', 'ci/cd', 'azure data factory', 'machine learning', 'data engineering', 'powershell', 'olap', 'etl', 'big data']",2025-06-12 14:58:36
Lead Engineer - Data Science,Sasken Technologies,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position takes ownership of a module and associated quality and delivery. Person at this position provides instructions, guidance and advice to team members to ensure quality and on time delivery.\nPerson at this position is expected to be able to instruct and review the quality of work done by technical staff.\nPerson at this position should be able to identify key issues and challenges by themselves, prioritize the tasks and deliver results with minimal direction and supervision.\nPerson at this position has the ability to investigate the root cause of the problem and come up alternatives/ solutions based on sound technical foundation gained through in-depth knowledge of technology, standards, tools and processes.\nPerson has the ability to organize and draw connections among ideas and distinguish between those which are implementable.\nPerson demonstrates a degree of flexibility in resolving problems/ issues that atleast to in-depth command of all techniques, processes, tools and standards within the relevant field of specialisation.\n\n\nRoles & Responsibilities\nResponsible for requirement analysis and feasibility study including system level work estimation while considering risk identification and mitigation.\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals.\nResponsible for traceability of the requirements from design to delivery Code optimization and coverage.\nResponsible for conducting reviews, identifying risks and ownership of quality of deliverables.\nResponsible for identifying training needs of the team.\nExpected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments.\nExpected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\nExpected to be a technical mentor for junior members.\nPerson may be given additional responsibility of managing people based on discretion of Project Manager.\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 5-8 years\n\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Unix', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Machine Learning', 'Python']",2025-06-12 14:58:38
Senior ML Compiler Engineer,Qualcomm,0 - 5 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nInterested in accelerating machine learning and artificial intelligence on mobile devices for millions of usersCome join our team. We are building software platforms that enable users of Qualcomms silicon to construct optimized neural networks and machine learning algorithms. We are looking for software engineers with a machine learning or compiler background who will help us build these software platforms. In this role, you will construct and tune machine learning frameworks, build compilers and tools, and collaborate with Qualcomm hardware and software engineers to enable efficient usage of Qualcomms silicon for machine learning applications.\n\nMinimum qualifications:\nBachelors degree in Engineering, Information Systems, Computer Science, or related field.\nProgramming in C/C++\n0 to 10 years of software engineering or related work experience\n\n\nPreferred qualifications:\nExperience in machine learning frameworks such as MxNet/NNVM/TVM, Pytorch, Tensorflow, Caffe\n\nOR experience in compilers with an interest in machine learning\nDeep knowledge of software engineering\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'software engineering', 'algorithms', 'c++', 'natural language processing', 'caffe', 'neural networks', 'mxnet', 'artificial intelligence', 'sql', 'deep learning', 'r', 'java', 'data science', 'computer vision', 'machine learning algorithms', 'ml']",2025-06-12 14:58:41
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,"['Noida', 'Gurugram']","Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 14:58:43
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Pune'],"ZSs Insights & Analytics group partners with clients to design and deliver solutions to help them tackle a broad range of business challenges. Our teams work on multiple projects simultaneously, leveraging advanced data analytics and problem-solving techniques. Our recommendations and solutions are based on rigorous research and analysis underpinned by deep expertise and thought leadership.\nWhat you'll Do\nDevelop advanced and efficient statistically effective algorithms that solve problems of high",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 14:58:45
Data Scientist,H3 Technologies,3 - 8 years,Not Disclosed,['Thiruvananthapuram'],"Position: Data Scientist\nLocation: Trivandrum\nJob Description :\nWe are urgently looking for a motivated Data Scientist with a focus on Computer Vision and Machine Learning. The candidate will have a passion for solving complex problems using deep learning, image processing, and AI-driven techniques. He shall work closely with a team of data scientists, engineers, etc and to build, optimize, and deploy machine learning models for real-world applications\nKey Responsibilities :\nDevelop, train, and optimize deep learning models for image classification, object detection, segmentation, and other computer vision tasks.\nImplement and fine-tune machine learning algorithms for structured and unstructured data analysis.\nPreprocess and augment image/video datasets to improve model accuracy and robustness.\nWork with frameworks such as YOLO, TensorFlow, PyTorch, and OpenCV to build scalable models.\nAssist in deploying models to production environments, including cloud and edge computing platforms.\nCollaborate with cross-functional teams to integrate AI solutions into existing workflows and products.\nStay up-to-date with the latest research and trends in AI, computer vision, and machine learning.\nQualifications :\nBachelors or masters degree in computer science, Data Science, AI/ML, or a related field.\nMinimum of 3 year of professional experience in Python programming and AI/ML integrations\nSolid understanding of machine learning concepts, neural networks, and deep learning architectures.\nHands-on experience in training and optimizing computer vision models.\nFamiliarity with data preprocessing techniques, image annotation tools, and model evaluation metrics.\nStrong problem-solving skills and the ability to work in a fast-paced environment.\nJoining: Immediate to less than 30 days\nBudget: 13 - 14 LPA\n"",",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Computer vision', 'deep learning', 'Data analysis', 'Image processing', 'data science', 'Neural networks', 'Machine learning', 'Budgeting', 'Python']",2025-06-12 14:58:47
Freelance Online Data Analyst - Hindi Speaker,TELUS Digital,0 - 5 years,Not Disclosed,[],Job description\nAre you a detail-oriented individual with a passion for research and a good understanding of national and local geography? This freelance opportunity allows you to work at your own pace and from the comfort of your own home.\n\n\nA Day in the Life of an Online Data Analyst:,,,,"['Artificial Intelligence', 'Data Analytics', 'Ai Solutions', 'Data Analysis', 'Information Technology']",2025-06-12 14:58:49
Data Engineer,AMERICAN EXPRESS,2 - 4 years,13-17 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Role & responsibilities\nUnderstanding business use cases and be able to convert to technical design\nPart of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers and business partners.\nYou will be designing scalable, testable and maintainable data pipelines\nIdentify areas for data governance improvements and help to resolve data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design changes",,,,"['Spark', 'SQL', 'Python', 'Hadoop', 'Big Data']",2025-06-12 14:58:52
Data Scientist,Jsg. Consulting. Pvt.Ltd.,3 - 5 years,9.6-10.8 Lacs P.A.,['Jaipur'],"Familiarity with MDM (Meter Data Management), HES, and utility billing systems.\nExposure to AMI events analysis, load curves, and customer behavior analytics.\nKnowledge of regulatory requirements, data retention, and data .",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['billing exceptions', 'load profiling', 'Machine Learning', 'Meter Data Management', 'Smart Metering', 'Hes']",2025-06-12 14:58:54
"AI/ML Engineer (Specializing in NLP/ML, Large Data Processing,",Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027361\n\nJob Summary\nSynechron seeks a highly skilled AI/ML Engineer specializing in Natural Language Processing (NLP), Large Language Models (LLMs), Foundation Models (FMs), and Generative AI (GenAI). The successful candidate will design, develop, and deploy advanced AI solutions, contributing to innovative projects that transform monolithic systems into scalable microservices integrated with leading cloud platforms such as Azure, Amazon Bedrock, and Google Gemini. This role plays a critical part in advancing Synechrons capabilities in cutting-edge AI technologies, enabling impactful business insights and product innovations.\n\nSoftware\n\nRequired Proficiency:\nPython (core librariesTensorFlow, PyTorch, Hugging Face transformers, etc.)\nCloud platformsAzure, AWS, Google Cloud (familiarity with AI/ML services)\nContainerizationDocker, Kubernetes\nVersion controlGit\nData management toolsSQL, NoSQL databases (e.g., MongoDB)\nModel deployment and MLOps toolsMLflow, CI/CD pipelines, monitoring tools\nPreferred\n\nSkills:\nExperience with cloud-native AI frameworks and SDKs\nFamiliarity with AutoML tools\nAdditional programming languages (e.g., Java, Scala)\nOverall Responsibilities\nDesign, develop, and optimize NLP models, including advanced LLMs and Foundation Models, for diverse business use cases.\nLead the development of large data pipelines for training, fine-tuning, and deploying models on big data platforms.\nArchitect, implement, and maintain scalable AI solutions in line with MLOps best practices.\nTransition legacy monolithic AI systems into modular, microservices-based architectures for scalability and maintainability.\nBuild end-to-end AI applications from scratch, including data ingestion, model training, deployment, and integration.\nImplement retrieval-augmented generation techniques for enhanced context understanding and response accuracy.\nConduct thorough testing, validation, and debugging of AI/ML models and pipelines.\nCollaborate with cross-functional teams to embed AI capabilities into customer-facing and enterprise products.\nSupport ongoing maintenance, monitoring, and scaling of deployed AI systems.\nDocument system designs, workflows, and deployment procedures for compliance and knowledge sharing.\nPerformance Outcomes:\nProduction-ready AI solutions delivering high accuracy and efficiency.\nRobust data pipelines supporting training and inference at scale.\nSeamless integration of AI models with cloud infrastructure.\nEffective collaboration leading to innovative AI product deployment.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: Python (TensorFlow, PyTorch, Hugging Face, etc.)\nPreferred: Java, Scala\nDatabases/Data Management:\nSQL (PostgreSQL, MySQL), NoSQL (MongoDB, DynamoDB)\nCloud Technologies:\nAzure AI, AWS SageMaker, Bedrock, Google Cloud Vertex AI, Gemini\nFrameworks and Libraries:\nTransformers, Keras, scikit-learn, XGBoost, Hugging Face engines\nDevelopment Tools & Methodologies:\nDocker, Kubernetes, Git, CI/CD pipelines (Jenkins, Azure DevOps)\nSecurity & Compliance:\nKnowledge of data security standards and privacy policies (GDPR, HIPAA as applicable)\nExperience\n8 to 10 years of hands-on experience in AI/ML development, especially NLP and Generative AI.\nDemonstrated expertise in designing, fine-tuning, and deploying LLMs, FMs, and GenAI solutions.\nProven ability to develop end-to-end AI applications within cloud environments.\nExperience transforming monolithic architectures into scalable microservices.\nStrong background with big data processing pipelines.\nPrior experience working with cloud-native AI tools and frameworks.\nIndustry experience in finance, healthcare, or technology sectors is advantageous.\nAlternative Experience:\nCandidates with extensive research or academic experience in AI/ML, especially in NLP and large-scale data processing, are eligible if they have practical deployment experience.\n\nDay-to-Day Activities\nDevelop and optimize sophisticated NLP/GenAI models fulfilling business requirements.\nLead data pipeline construction for training and inference workflows.\nCollaborate with data engineers, architects, and product teams to ensure scalable deployment.\nConduct model testing, validation, and performance tuning.\nImplement and monitor model deployment pipelines, troubleshoot issues, and improve system robustness.\nDocument models, pipelines, and deployment procedures for audit and knowledge sharing.\nStay updated with emerging AI/ML trends, integrating best practices into projects.\nPresent findings, progress updates, and technical guidance to stakeholders.\nQualifications\nBachelors degree in Computer Science, Data Science, or related field; Masters or PhD preferred.\nCertifications in AI/ML, Cloud (e.g., AWS, Azure, Google Cloud), or Data Engineering are a plus.\nProven professional experience with advanced NLP and Generative AI solutions.\nCommitment to continuous learning to keep pace with rapidly evolving AI technologies.\nProfessional Competencies\nStrong analytical and problem-solving capabilities.\nExcellent communication skills, capable of translating complex technical concepts.\nCollaborative team player with experience working across global teams.\nAdaptability to rapidly changing project scopes and emerging AI trends.\nInnovation-driven mindset with a focus on delivering impactful solutions.\nTime management skills to prioritize and manage multiple projects effectively.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data management', 'data processing', 'pipeline', 'big data', 'continuous integration', 'kubernetes', 'deploying models', 'natural language processing', 'ci/cd', 'fms', 'artificial intelligence', 'docker', 'sql', 'microservices', 'tensorflow', 'java', 'pytorch', 'jenkins', 'keras', 'aws']",2025-06-12 14:58:56
Data Scientist,Callaway Digital Technologies,6 - 9 years,Not Disclosed,['Hyderabad'],"JOB OVERVIEW\nThe ideal candidate will be responsible for analyzing and interpreting large data sets related to finance, sales and supply chain operations to optimize business processes, identify opportunities for improvement, and provide strategic insights to support decision-making. The Data Scientist will work closely with cross-functional teams to identify key business questions, design and implement statistical models, and develop innovative data-driven solutions.\nKey Responsibilities:",,,,"['Statistical Modeling', 'Machine Learning', 'Python', 'Data Visualization', 'Azzure', 'R Program', 'SQL']",2025-06-12 14:58:59
Data Scientist,Apcfss,2 - 6 years,Not Disclosed,"['Vijayawada', 'Guntur', 'Mangalagiri']","Location: Vijayawada, Andhra Pradesh\nExperience: 2 to 6 years\nEmployment Type: Full-Time\n\nJob Opening: Data Scientist\nWe are seeking a data-driven problem solver to join our team as a Data Scientist. You will play a key role in transforming data into actionable insights and building models that support strategic decisions across the organization. Collaborating with cross-functional teams, youll help turn complex data into clear value.\nKey Responsibilities\nAnalyze large and complex datasets to uncover trends, patterns, and insights\nBuild, validate, and deploy predictive and statistical models\nWork closely with engineering and product teams to integrate models into production systems\nCommunicate analytical findings and insights clearly to both technical and non-technical stakeholders\nRequirements\nProficiency in Python or R, and strong command of SQL\nHands-on experience with machine learning and statistical modeling\nStrong analytical and problem-solving skills\nExperience with cloud platforms such as AWS, GCP, or Azure\nNice to Have\nExperience in Natural Language Processing (NLP), deep learning, or time-series forecasting\nPrior work in [industry-specific domain, e.g., fintech, healthcare, e-commerce]",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'GCP', 'Machine Learning', 'AWS', 'Deep Learning', 'SQL']",2025-06-12 14:59:01
"MTS 1, Machine Learning Scientist",Xoom,7 - 9 years,Not Disclosed,['Bengaluru'],"Job Summary\nWhat you need to know about the role-\nPayPal Lending ML team is looking for an experienced Machine Learning Scientist to help us develop and enhance machine learning/AI capabilities and innovate to grow PP Credit products. This is an excellent opportunity to join a team of intelligent and passionate engineers and scientists and to help shape the future of the Lending Business at PayPal.\nJob Description\nMeet our team\nPayPal Lending ML team is responsible for building ML/AI solutions that drive impact across PayPal. We build innovative solutions that interact with customers throughout the lending lifecycle with a strong focus on driving PP growth and business.\nYour way to impact\nIdentify AI ML opportunities and build solutions that impact the business\nExplore and execute to get solid results\nImpact team, partners and business through PayPal values\nYour day to day\nIn your day to day role you will\nLead ML Projects and Conduct research to identify new and innovative ML techniques.\nInnovate to create efficiencies for the team and the business.\nWork closely with other engineers , analysts and leaders to implement and opt imize ML models.\nDevelop and implement best practices for ML model management, deployment, and monitoring.\nCollaborate with other teams to ensure ML models are integrated into the product and services.\nAssist with troubleshooting and resolving technical issues.\nResponsible for documentation, project tracking, and quality controls .\nWhat do you need to bring-\nThe ideal candidate will possess a degree in engineering, science, statistics or mathematics with strong technical background in machine learning.\nWe are looking for candidates who have excellent communication skills, an analytical mindset, and a passion for problem-solving.\n7+ years of hands-on experience with problem-solving using Machine Learning\nHands-on experience with Python or Java, along with relevant technologies such as Spark, Hadoop, BigQuery , SQL, is required .\nCandidates should have in-depth knowledge of machine learning algorithms, explainable AI methods, and NLP.\nExperience with Cloud frameworks such as GCP, AWS is preferred.\nExperience with Lending and Financial services is a plus.\nPreferred Qualification\nSubsidiary\nPayPal\nTravel Percent\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https//www.paypalbenefits.com .\nWho We Are\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. .\nBelonging at PayPal\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Project tracking', 'Analytical', 'Diversity and Inclusion', 'Finance', 'Machine learning', 'Wellness', 'Troubleshooting', 'Financial services', 'Monitoring', 'SQL']",2025-06-12 14:59:04
Data Scientist,Celebal Technologies,3 - 6 years,Not Disclosed,"['Navi Mumbai', 'Mumbai (All Areas)']","Job Title: Data Scientists\nLocation: Navi Mumbai\nDuration: Fulltime\nPositions: Multiple\n\nWe are looking for a highly skilled Data Scientists with deep expertise in time series forecasting, particularly in demand forecasting and customer lifecycle analytics (CLV). The ideal candidate will be proficient in Python or PySpark, have hands-on experience with tools like Prophet and ARIMA, and be comfortable working in Databricks environments. Familiarity with classic ML models and optimization techniques is a plus.",,,,"['Demand Forecasting', 'Data Bricks', 'Time Series', 'Pyspark', 'Arima', 'Customer Lifecycle', 'Forecasting', 'Machine Learning', 'Optimization', 'Data Science', 'Xgboost', 'Time Series Analysis', 'Prophet', 'Python']",2025-06-12 14:59:06
Data Scientist,Celebal Technologies,3 - 6 years,Not Disclosed,"['Mumbai', 'Navi Mumbai']","About Us: Celebal Technologies is a leading Solution Service company that provide Services the field of Data Science, Big Data, Enterprise Cloud & Automation. We are at the forefront of leveraging cuttingedge technologies to drive innovation and enhance our business processes. As part of our commitment to staying ahead in the industry, we are seeking a talented and experienced Data & AI Engineer with strong Azure cloud competencies to join our dynamic team.\n\nJob Summary: We are looking for a highly skilled Data Scientist with deep expertise in time series forecasting, particularly in demand forecasting and customer lifecycle analytics (CLV). The ideal candidate will be proficient in Python or PySpark, have hands-on experience with tools like Prophet and ARIMA, and be comfortable working in Databricks environments. Familiarity with classic ML models and optimization techniques is a plus.\n\nKey Responsibilities\n• Develop, deploy, and maintain time series forecasting models (Prophet, ARIMA, etc.) for demand forecasting and customer behavior modeling.\n• Design and implement Customer Lifetime Value (CLV) models to drive customer retention and engagement strategies.\n• Process and analyze large datasets using PySpark or Python (Pandas).\n• Partner with cross-functional teams to identify business needs and translate them into data science solutions.\n• Leverage classic ML techniques (classification, regression) and boosting algorithms (e.g., XGBoost, LightGBM) to support broader analytics use cases.\n• Use Databricks for collaborative development, data pipelines, and model orchestration.\n• Apply optimization techniques where relevant to improve forecast accuracy and business decision-making.\n• Present actionable insights and communicate model results effectively to technical and non-technical stakeholders.\n\nRequired Qualifications\n• Strong experience in Time Series Forecasting, with hands-on knowledge of Prophet, ARIMA, or equivalent Mandatory.\n• Proven track record in Demand Forecasting Highly Preferred.\n• Experience in modeling Customer Lifecycle Value (CLV) or similar customer analytics use cases Highly Preferred.\n• Proficiency in Python (Pandas) or PySpark Mandatory.\n• Experience with Databricks Mandatory.\n• Solid foundation in statistics, predictive modeling, and machine learning",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning Operations', 'Demand Forecasting', 'Data Bricks', 'Pyspark', 'Large Language Model', 'Time Series', 'Spark', 'Machine Learning', 'Python']",2025-06-12 14:59:08
Data Scientist,"Sourced Group, an Amdocs Company",4 - 9 years,Not Disclosed,['Gurugram'],"0px> Who are we?\nIn one sentence\nThis is a hands-on position for a motivated and talented innovator. The Data Scientist performs data mining and develops algorithms that provide insight from data.\nWhat will your job look like?\nYou will be responsible for and perform end-top-end data-based research.\nYou will craft data mining solutions to be implemented and executed with alignment to the planned scope and design coverage and needs/uses, demonstrating knowledge and a broad understanding of E2E business processes and requirements.\nYou will define the data analytics research plan, scope and resources required to meet the objectives of his/her area of ownership.\nYou will identify and analyze new data analytic directions and their potential business impact to determine the accurate prioritization of data analytics activities based on business needs and analytics value.\nYou will identify data sources, supervises the data collection process and crafts the data structure in collaboration with data experts (BI or big-data) and subject matter and business experts. Ensures that data used in the data analysis activities are of the highest quality.\nYou will construct data models (algorithms and formulas) for required business needs and predictions.\nYou will present results, including the preparation of patents and white papers and facilitating presentations during conferences.\nAll you need is...\nPh.D. in Computer Science, Mathematics or Statistics\n4 years experience in tasks related to data analytics\nKnowledge of telecommunications and of the subject area being investigated - advantage\nKnowledge in the product (ACC or other) application knowledge and configuration knowledge\nKnowledge in BSS, billing, Telco and the business processes\nFamiliarity in the Telco Networking - mobile, landline, cable TV, Internet\nknowledge in Oracle SQL\nWhy you will love this job:\nYou will ensure timely resolution or critical issue within the agreed SLA. This includes creating a positive customer support experience and build strong relationships through problem understanding, presenting promptly on progress, and handling customers with a professional demeanour.\nYou will be able to demonstrates an understanding of key business drivers and ensures strategic directions are followed and the organization succeeds\nWe are a dynamic, multi-cultural organization that constantly innovates and empowers our employees to grow. Our people our passionate, daring, and phenomenal teammates that stand by each other with a dedication to creating a diverse, inclusive workplace!\nWe offer a wide range of stellar benefits including health, dental, vision, and life insurance as well as paid time off, sick time, and parental leave!\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Bss', 'Networking', 'Billing', 'Data collection', 'Customer handling', 'Customer support', 'Data mining', 'Amdocs']",2025-06-12 14:59:11
WLAN Phy RTL Design- Sr lead/Staff/Sr Staff/Principal Engineer,Qualcomm,1 - 3 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nQualcomm's Bangalore WLAN PHY (Baseband) team is seeking VLSI Digital Design Engineers to lead IP development for the latest WiFi standards.\n\nOur WLAN PHY team, comprised of highly passionate and seasoned domain experts, prides itself on years of experience in taking WLAN PHY designs from concept to silicon independently.\n\nWLAN PHY team is responsible for delivering the end-to-end Tx/Rx DSP chains- all the way from antenna samples post ADC to raw bits for upper layers and on the reverse path from raw bits to DAC. The team specializes in working with challenges of practical high-speed wireless communication systems and finding innovative solutions to counter them.\n\nThe team works extensively on typical signal processing functions like filters, matrix transformations (e.g.QR, Cholesky decomposition), channel estimation, equalization (MMSE, MRC, ML), decoders/encoders (e.g.LDPC, Viterbi) , demodulators, FFT etc. on a day-to-day basis, and contributes to the development/ enhancement/ evaluation of signal processing algorithms to cater to new requirements.\n\nWe are looking for someone as passionate as us and takes pride in their work.\n\nWiFi's ubiquity in modern times is undeniable, and the IEEE 802.11 Working Group is continually developing new standards to satisfy the growing demand for high throughput and low-latency real-time applications, such as VR and AR.\n\nQualcomm is at the forefront of the WiFi revolution, aiming to become the global leader in WiFi chip solutions. The WLAN PHY team in Bangalore is instrumental in realizing this vision.\n\n\n:\n\nLooking for a candidate with 1 to 3 years of hands-on experience in micro-architecting and developing complex IPs.\n\nExpertise in digital design, VLSI concepts, and experience in creating power/area-efficient IPs across multiple clock domains are essential.\n\nProficiency in RTL coding and familiarity with RTL QA flows such as PLDRC, CDC, and CLP (optional) is expected.\n\nCandidates should be capable of proposing design alternatives to meet area/power/performance specifications and presenting these options for review.\n\nExperience in leading, guiding, or managing junior team members is advantageous.\n\nRepeated success in taking IP designs from requirements to silicon is required.\n\nWhile not mandatory, having developed IPs for wireless technologies (WLAN, LTE, NR, BT, UWB, etc.) or past HLS experience would be beneficial.\n\n\n\n\nSkills:\n\n\n\nMust have: Proficient in Verilog RTL coding, uArch, CDC check, PLDRC, Timing constraints, Python/Perl. Experience in design/debugging complex data-path/control-path IPs. Good communication, analytical & leadership skills.\n\n\nGood to have: System Verilog, Visio, Knowledge of signal processing concepts/algorithms and Wi-Fi standards (802.11a/b/g/n/ac/ax), experience with HLS.\n\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 4+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 3+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 2+ years of Hardware Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['rtl coding', 'perl', 'python', 'cdc', 'verilog', 'antenna', 'enodeb', 'gsm', 'architecting', 'node b', 'digital design', 'wireless communication', 'rtl design', '5g', '3g', 'wcdma', 'debugging', 'telecom', 'system verilog', 'lte', 'ran', 'c', 'clp', 'rnc', 'hardware engineering', 'rtl', 'baseband', 'silicon', '4g', '2g', 'visio', 'vlsi']",2025-06-12 14:59:14
"Senior Staff Engineer, Big Data Engineer",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal experience 10+ years.\nExcellent knowledge and experience in Big data engineer.\nStrong working experience with architecture and development in Apache Spark, Spark, Scala, AWS/Azure/GCP, Data Pipelines, Kafka, SQL Server/NoSQL.\nStrong expertise in Django Rest Framework, Databricks and PostgreSQL.\nHands on experience in building data pipelines and building data frameworks for unit testing, data lineage tracking, and automation.\nFamiliarity with streaming technologies (e.g., Kafka, Kinesis, Flink).\nExperience with Machine Learning and Looker.\nKnowledge of additional server-side programming languages (e.g. Golang, C#, Ruby).\nExperience with building and maintaining a cloud system.\nFamiliarity with data modeling, data warehousing, and building distributed systems.\nExpertise in Spanner for high-availability, scalable database solutions.\nKnowledge of data governance and security practices in cloud-based environments.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers\nIdentifying different solutions and being able to narrow down the best option that meets the client’s requirements\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Django Framework', 'Spark', 'Data Bricks', 'Apache Spark']",2025-06-12 14:59:16
"Senior Data Scientist (AI/ML, Data Analysis, Cloud (AWS), and Model",Synechron,8 - 13 years,Not Disclosed,['Pune'],"job requisition idJR1027352\n\nJob Summary\nSynechron is seeking an analytical and innovative Senior Data Scientist to support and advance our data-driven initiatives. The ideal candidate will have a solid understanding of data science principles, hands-on experience with AI/ML tools and techniques, and the ability to interpret complex data sets to deliver actionable insights. This role contributes to the organizations strategic decision-making and technology innovation by applying advanced analytics and machine learning models in a collaborative environment.\n\nSoftware\n\nRequired\n\nSkills:\nPython (including libraries such as pandas, scikit-learn, TensorFlow, PyTorch) proficiency in developing and deploying models\nR (optional, but preferred)\nData management tools (SQL, NoSQL databases)\nCloud platforms (preferably AWS or Azure) for data storage and ML deployment\nJupyter Notebooks or similar interactive development environments\nVersion control tools such as Git\nPreferred\n\nSkills:\nBig data technologies (Spark, Hadoop)\nModel deployment tools (MLflow, Docker, Kubernetes)\nData visualization tools (Tableau, Power BI)\nOverall Responsibilities\nAnalyze and interpret large and complex data sets to generate insights for business and technology initiatives.\nAssist in designing, developing, and implementing AI/ML models and algorithms to solve real-world problems.\nCollaborate with cross-functional teams including data engineers, software developers, and business analysts to integrate models into production systems.\nStay current with emerging trends, research, and best practices in AI/ML/Data Science and apply them to ongoing projects.\nDocument methodologies, modeling approaches, and insights clearly for technical and non-technical stakeholders.\nSupport model validation, testing, and performance monitoring to ensure accuracy and reliability.\nContribute to the development of data science workflows and standards within the organization.\nPerformance Outcomes:\nAccurate and reliable data models that support strategic decision-making.\nClear documentation and communication of findings and recommendations.\nEffective collaboration with technical teams to deploy scalable models.\nContinuous adoption of best practices in AI/ML and data management.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: Python (best practices in ML development), SQL\nPreferred: R, Java (for integration purposes)\nDatabases/Data Management:\nSQL databases, NoSQL (MongoDB, Cassandra)\nCloud data storage solutions (AWS S3, Azure Blob Storage)\nCloud Technologies:\nAWS (S3, EC2, SageMaker, Lambda)\nAzure Machine Learning (preferred)\nFrameworks & Libraries:\nTensorFlow, PyTorch, scikit-learn, Keras, XGBoost\nDevelopment Tools & Methodologies:\nJupyter Notebooks, Git, CI/CD pipelines\nAgile and Scrum processes\nSecurity Protocols:\nBest practices in data security and privacy, GDPR compliance\nExperience\n8+ years of professional experience in AI, ML, or Data Science roles.\nProven hands-on experience designing and deploying ML models in real-world scenarios.\nDemonstrated ability to analyze complex data sets and translate findings into business insights.\nPrevious experience working with cloud-based data science solutions is preferred.\nStrong portfolio showcasing data science projects, models developed, and practical impact.\nAlternative Pathways:\nCandidates with extensive research or academic experience in AI/ML can be considered, provided they demonstrate practical application of skills.\n\nDay-to-Day Activities\nConduct data exploration, cleaning, feature engineering, and model development.\nCollaborate with data engineers to prepare data pipelines for model training.\nBuild, validate, and refine machine learning models.\nPresent insights, models, and recommendations to technical and business stakeholders.\nSupport deployment of models into production environments.\nMonitor model performance and iterate to improve effectiveness.\nParticipate in team meetings, project planning, and reviewing progress.\nDocument methodologies and maintain version control of codebase.\nQualifications\nBachelors degree in Computer Science, Mathematics, Statistics, Data Science, or a related field; Masters or PhD highly desirable.\nEvidence of relevant coursework, certifications, or professional training in AI/ML.\nProfessional certifications (e.g., AWS Certified Machine Learning Specialty, Microsoft Certified Data Scientist) are a plus.\nCommitment to ongoing professional development in AI/ML methodologies.\nProfessional Competencies\nStrong analytical and critical thinking to solve complex problems.\nEffective communication skills for technical and non-technical audiences.\nDemonstrated ability to work collaboratively in diverse teams.\nAptitude for learning new tools, techniques, and technologies rapidly.\nInnovation mindset with a focus on applying emerging research.\nStrong organizational skills to manage multiple projects and priorities.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['java', 'data science', 'python', 'deploying models', 'aws', 'continuous integration', 'kubernetes', 'scikit-learn', 'ci/cd', 'artificial intelligence', 'sql', 'docker', 'tensorflow', 'spark', 'pytorch', 'keras', 'hadoop', 'big data', 'mongodb', 'microsoft azure', 'nosql', 'pandas', 'amazon ec2', 'r', 'cassandra', 'agile']",2025-06-12 14:59:18
Specialist Data Scientist,Atlasrtx,3 - 7 years,Not Disclosed,['Pune'],"So, what s the role all about\n\nNICE provides state-of-the-art enterprise level AI and analytics for all forms of business communications between speech and digital. We are a world class research team developing new algorithms and approaches to help companies with solving critical issues such as identifying their best performing agents, preventing fraud, categorizing customer issues, and determining overall customer satisfaction. If you have interacted with a major contact center in the last decade, it is very likely we have processed your call.\n\nThe research group partners with all areas of NICE s business to scale out the delivery of new technology and AI models to customers around the world that are tailored to their company, industry, and language needs.\n\n\nHow will you make an impact\n\nConduct cutting-edge research and develop advanced NLP algorithms and models.\n\nBuild and fine-tune deep learning and machine learning models, with a focus on large language models.\n\nWork closely with internal stakeholders to define model requirements and ensure alignment with business objectives.\n\nDevelop AI predictive models and perform data and model accuracy analyses.\n\nProduce and present findings, technical concepts, and model recommendations to both technical and non-technical stakeholders.\n\nDevelop and maintain scripts/tools to automate both new model production and updates to existing model packages.\n\nStay abreast of the latest advancements in data science research and contribute to the development of our knowledge base.\n\nCollaborate with developers to design automation and tool improvements for model building.\n\nMaintain documentation of processes and projects across all supported languages and environments.\n\n\nHave you got what it takes\n\nMasters degree in the field of Computer Science, Technology, Engineering, Math, or equivalent practical experience\n\nMinimum of 8 years of data science work experience, including implementing machine learning and NLP models using real-life data.\n\nExperience with Retrieval-Augmented Generation (RAG) pipelines or LLMOps.\n\nAdvanced knowledge of statistics and machine learning algorithms.\n\nProficiency in Python programming and familiarity with R.\n\nExperience with deep learning models and libraries such as PyTorch, TensorFlow, and JAX.\n\nFamiliarity with relational databases and query languages (e. g. , MSSQL) and basic SQL knowledge.\n\nHands-on experience with transformer models (BERT, FlanT5, Llama, etc. ) and GenAI frameworks (HuggingFace, LangChain, Ollama, etc. ).\n\nExperience deploying NLP models in production environments, ensuring scalability and performance using AWS/GCP/Azure\n\nStrong verbal and written communication skills, including effective presentation abilities.\n\nAbility to work independently and as part of a team, demonstrating analytical thinking and problem-solving skills.\n\n\n\nYou will have an advantage if you also have:\n\nExpertise with Big Data technologies (e. g. , PySpark).\n\nBackground in knowledge graphs, graph databases, or GraphRAG architectures.\n\nUnderstanding of multimodal models (text, audio, vision).\n\nExperience in Customer Experience domains.\n\nExperience with package development and technical writing.\n\nFamiliarity with tools like Jira, Confluence, and source control packages and methodology.\n\nKnowledge and interest in foreign languages and linguistics.\n\nExperience working on international, globe-spanning teams and with AWS.\n\nPast participation in a formal research setting.\n\nExperience as part of a software organization.\n\n\n\nWhat s in it for you\n\n\n\nEnjoy NICE-FLEX!\n\n\n\nRequisition ID : 7481\nReporting into : Tech Manager\nRole Type : Individual Contributor\n\nAbout NICE",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Technical writing', 'GCP', 'Analytical', 'Machine learning', 'Flex', 'Analytics', 'SQL', 'Python']",2025-06-12 14:59:20
Azure Cloud Data Engineering Consultant,Optum,7 - 10 years,17-27.5 Lacs P.A.,['Gurugram'],"Primary Responsibilities:\nDesign and develop applications and services running on Azure, with a strong emphasis on Azure Databricks, ensuring optimal performance, scalability, and security.\nBuild and maintain data pipelines using Azure Databricks and other Azure data integration tools.\nWrite, read, and debug Spark, Scala, and Python code to process and analyze large datasets.\nWrite extensive query in SQL and Snowflake\nImplement security and access control measures and regularly audit Azure platform and infrastructure to ensure compliance.\nCreate, understand, and validate design and estimated effort for given module/task, and be able to justify it.\nPossess solid troubleshooting skills and perform troubleshooting of issues in different technologies and environments.\nImplement and adhere to best engineering practices like design, unit testing, functional testing automation, continuous integration, and delivery.\nMaintain code quality by writing clean, maintainable, and testable code.\nMonitor performance and optimize resources to ensure cost-effectiveness and high availability.\nDefine and document best practices and strategies regarding application deployment and infrastructure maintenance.\nProvide technical support and consultation for infrastructure questions.\nHelp develop, manage, and monitor continuous integration and delivery systems.\nTake accountability and ownership of features and teamwork.\nComply with the terms and conditions of the employment contract, company policies and procedures, and any directives.\nRequired Qualifications:\nB.Tech/MCA (Minimum 16 years of formal education)\nOverall 7+ years of experience.\nMinimum of 3 years of experience in Azure (ADF), Databricks and DevOps.\n5 years of experience in writing advanced level SQL.\n2-3 years of experience in writing, reading, and debugging Spark, Scala, and Python code.\n3 or more years of experience in architecting, designing, developing, and implementing cloud solutions on Azure.\nProficiency in programming languages and scripting tools.\nUnderstanding of cloud data storage and database technologies such as SQL and NoSQL.\nProven ability to collaborate with multidisciplinary teams of business analysts, developers, data scientists, and subject-matter experts.\nFamiliarity with DevOps practices and tools, such as continuous integration and continuous deployment (CI/CD) and Teraform.\nProven proactive approach to spotting problems, areas for improvement, and performance bottlenecks.\nProven excellent communication, writing, and presentation skills.\nExperience in interacting with international customers to gather requirements and convert them into solutions using relevant skills.\nPreferred Qualifications:\nKnowledge of AI/ML or LLM (GenAI).\nKnowledge of US Healthcare domain and experience with healthcare data.\nExperience and skills with Snowflake.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'ETL', 'SQL', 'Python', 'Airflow', 'Pyspark', 'Snowflake', 'SCALA', 'Spark', 'Data Bricks']",2025-06-12 14:59:23
Senior Software Engineer- AWS Machine Learning,Cross Country Infotech,5 - 10 years,Not Disclosed,['Pune'],"[{""Salary"":null , ""Remote_Job"":false , ""Posting_Title"":""Senior Software Engineer- AWS Machine Learning"" , ""Is_Locked"":false , ""City"":""Pune City"",""Industry"":""IT Services"",""Job_Description"":""\nLeverage AWS MLplatform services and frameworks to deliver production ready models acrossmultiple internal and external applications\nBuild predictiveand generative models specific to product needs\nAnalyze large data sets and build data pipelines for model training\nCollaborate with Product Designers, Product Managers, and Software Engineers to deliver",,,,"['Computer science', 'IT services', 'Quality monitoring', 'Automation', 'Machine learning', 'Solution delivery', 'AWS', 'Enterprise software', 'Software solutions', 'Python']",2025-06-12 14:59:26
"Lead Engineer, Senior - Model Orchestration and Accuracy Tools",Qualcomm,5 - 10 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Systems Engineer, you will research, design, develop, simulate, and/or validate systems-level software, hardware, architecture, algorithms, and solutions that enables the development of cutting-edge technology. Qualcomm Systems Engineers collaborate across functional teams to meet and exceed system-level requirements and standards.\n\nThe AI SW team at Qualcomm is focused on advancing state-of-the-art in Artificial Intelligence across various business segments, including Mobile, AR & VR Technology, IoT, and Auto ADAS. The AISW stack leverages Qualcomm chips' extensive heterogeneous computing capabilities, enabling the running of trained neural networks on devices without needing a cloud connection. This allows neural network models trained in various frameworks on Snapdragon platforms to run at blazing speeds while consuming minimal power. As a Senior Lead Engineer, you will see your work directly impact billions of devices worldwide.\n\nKey Responsibilities:\n\nTo design, development, and implementation of AI/ML solutions across multiple domains.\n\nCollaborate with cross-functional teams to ensure seamless integration of AI/ML components within the broader framework\n\nAddress and resolve issues related to AI models finetuning, quantization, compression and graph level optimizations, ensuring high performance and accuracy of AI models.\n\nShould possess good analytical skills - Consistently gather, integrate, and interpret information from different sources and conduct in depth analysis to find the root causes, provide recommendations, and effectively solve moderate to highly complex problems.\n\nConduct research on industry trends and innovations in AI/ML to adopt best practices in solutions and deliverables.\n\nDevelop and optimize quantization techniques for AI/ML models, ensuring efficient execution on Qualcomm hardware\n\nManage project timelines, objectives, and goals, ensuring efficient use of resources across functional areas.\n\nMentor and coach junior engineers, providing development experiences and networking opportunities.\n\n\nMinimum Qualifications:\n\nBachelor's degree in Engineering, Computer science or a related field and 5+ years of experience of Software engineering or related work experience\n\nOR\n\nMasters degree in Engineering, Computer science or a related field and 4+ years of experience of Software engineering or related work experience:\n\nExperience with SW architecture and programming languages.\n\nExperience with tools and frameworks such as PyTorch, TensorFlow, ONNX, and others.\n\n\nPreferred Qualifications:\n\nExcellent development skills in Python / C++\n\nProficient in Data structures and algorithms\n\nHands on expertise in deep learning frameworks such as ONNX, PyTorch, etc\n\nIn depth knowledge of state-of-the-art Computer Vision, NLP, LLM, LVM and LMM.\n\nGood understanding of Quantization (8-bit, 4-bit) and Calibration algorithms\n\nGood understanding of machine learning compiler techniques and graphs optimizations\n\nExcellent analytical, development, and debugging skills\n\nGood understanding of SW design patterns and design philosophies (SOLID principles, design patterns)\n\nKnowledge of machine learning runtimes like ONNX Runtime and execuTorch.\n\nKnowledge of AI model efficiency toolkit (AIMET), Snapdragon Neural processing engine is a plus.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'c++', 'natural language processing', 'data structures', 'deep learning frameworks', 'lvm', 'sw', 'machine learning', 'artificial intelligence', 'tool engineering', 'deep learning', 'tensorflow', 'pytorch', 'debugging', 'software engineering', 'onnx', 'system engineering']",2025-06-12 14:59:29
IT Manager - Data Engineering & Analytics,ZS,12 - 15 years,Not Disclosed,['Pune'],"IT MANAGER, DATA ENGINEERING AND ANALYTICS will lead a team of data engineers and analysts responsible for designing, developing, and maintaining robust data systems and integrations. This role is critical for ensuring the smooth collection, transformation, integration and visualization of data, making it easily accessible for analytics and decision-making across the organization. The Manager will collaborate closely with analysts, developers, business leaders and other stakeholders to ensure that the data infrastructure meets business needs and is scalable, reliable, and efficient.\n",,,,"['Data modeling', 'Project management', 'Analytical', 'Financial planning', 'Management consulting', 'Data quality', 'Troubleshooting', 'Stakeholder management', 'Analytics', 'SQL']",2025-06-12 14:59:31
Machine Learning Scientist (ASR),Wadhwani Ai,6 - 11 years,Not Disclosed,['Mumbai'],"SUMMARY\nWe are looking for a Machine Learning Scientist who is passionate about applying AI and ML to solve high-impact societal challenges. The ideal candidate will have a strong research and/or applied background with 6+ years of relevant experience or a PhD, and expertise in designing and implementing machine learning models, data analysis, and simulations. Candidates with experience in Automated Speech Recognition (ASR) are preferred for this role. They should be skilled in mentoring junior researchers, working in cross-functional teams, and contributing to both practical deployments and academic research. A demonstrated commitment to social good, strong communication skills, and the ability to translate complex problems into deployable ML solutions are essential.\nABOUT US - https://www.wadhwaniai.org/\nWadhwani AI is a nonprofit institute building and deploying applied AI solutions to solve critical issues in public health, agriculture, education, and urban development in underserved communities in the global south. We collaborate with governments, social sector organizations, academic and research institutions, and domain experts to identify real-world problems, and develop practical AI solutions to tackle these issues with the aim of making a substantial positive impact.\nWe have over 30+ AI projects supported by leading philanthropies such as Bill & Melinda Gates Foundation, USAID and Google.org. With a team of over 200 professionals, our expertise encompasses AI/ML research and innovation, software engineering, domain knowledge, design and user research.\nIn the Press:\nOur Founder Donors are among the Top 100 AI Influencers\nG20 India s Presidency: AI Healthcare, Agriculture, & Education Solutions Showcased Globally.\nUnlocking the potentials of AI in Public Health\nWadhwani AI Takes an Impact-First Approach to Applying Artificial Intelligence - data.org\nWinner of the H&M Foundation Global Change Award 2022\nIndian Winners of the 2019 Google AI Impact Challenge, and the first in the Asia Pacific to host Google Fellows\nROLES AND RESPONSIBILITIES\nAs an ML Scientist, you will be responsible for building machine learning solutions to problems of societal importance, and mentoring other team members in this effort. You will participate in problem definition and the development and execution of algorithms and solutions to the problems.\nAs an ASR expert, you will also be assisting in the design of evaluations for ASR solutions submitted for funding to the India AI mission at the Ministry of Electronics and Information Technology (MEITy). You will play an advisory role in the execution of these evaluations.\nIn order to apply machine learning and related technologies for social good, you will need to understand user challenges and their context, curate and transform data, train and validate models, run simulations, and broadly derive insights from data. In doing so, you will work in cross-functional teams spanning engineering, solutions, domain experts, and designers. You will also work closely with social sector organisations, and are encouraged to collaborate with researchers across the world.\nYou will be encouraged to drive fundamental advances to the technology domains themselves as part of the efforts towards their application, present your work in technical and other forums of interest, and publish in leading conferences and journals.\nAt Wadhwani AI, excellence as an individual contributor goes hand-in-hand with good teamwork and collaboration. You will mentor junior researchers, post-docs, and interns, and participate in recruiting and hiring activities. You will also be expected to interact with external partners of Wadhwani AI when required, and to make periodic visits to the communities from where challenges are derived and where the solutions will be deployed.\nREQUIREMENTS\nM.Tech./M.E./M.S./M.Sc. or equivalent in Computer Science, Electrical Engineering, Statistics, Applied Mathematics, Physics, Economics or a relevant quantitative field with work experience of 6+ years or a PHd from a reputed institute. Experience and expertise in ASR is preferred.\nWe are looking for ML Scientists with experience applying AI, machine learning, and data science to real-world problems. Ideal candidates should have a strong research background, have experience in mentoring junior researchers through strong code practices and ideation, and be adept at a variety of data mining/analysis methods and tools, building and implementing models, visualising data, creating/using algorithms, and running simulations. Familiarity with popular deep learning algorithms is a strong plus, however demonstrated willingness and ability to learn will also be considered.\nCandidates should be comfortable working with cross-functional teams and must have excellent oral and written communication skills and a track record of driving projects to completion. Evidence of written skills through high quality research publications is a plus.\nCandidates should be passionate about using their technical skills to solve large societally important problems.",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'ASR', 'Data analysis', 'Artificial Intelligence', 'Machine learning', 'Healthcare', 'Research', 'Data mining', 'Information technology', 'Public health']",2025-06-12 14:59:33
Senior Engineer - Data Science,Sasken Technologies,2 - 5 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position has gained significant work experience to be able to apply their knowledge effectively and deliver results. Person at this position is also able to demonstrate the ability to analyse and interpret complex problems and improve change or adapt existing methods to solve the problem.\nPerson at this position regularly interacts with interfacing groups / customer on technical issue clarification and resolves the issues. Also participates actively in important project/ work related activities and contributes towards identifying important issues and risks. Reaches out for guidance and advice to ensure high quality of deliverables.\nPerson at this position consistently seek opportunities to enhance their existing skills, acquire more complex skills and work towards enhancing their proficiency level in their field of specialisation.\nWorks under limited supervision of Team Lead/ Project Manager.\n\n\nRoles & Responsibilities\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals. Responsible for adhering to guidelines and checklists for all deliverable reviews, sending status report to team lead and following relevant organizational processes. Responsible for customer collaboration and interactions and support to customer queries. Expected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments. Expected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\n\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 2-5 years\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTechnology Standard-\nNA\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Spark', 'machine learning', 'Python']",2025-06-12 14:59:35
Senior Big Data Engineer,Qualcomm,2 - 7 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\nGeneral Summary\n\nPreferred Qualifications\n\n3+ years of experience as a Data Engineer or in a similar role\n\nExperience with\n\ndata modeling, data warehousing, and building ETL pipelines\n\nSolid working experience with\n\nPython, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,)\n\nExperience with\n\nBig Data tools, platforms and architecture with solid working experience with SQL\n\nExperience working in a very large data warehousing environment,\n\nDistributed System.\n\nSolid understanding on various data exchange formats and complexities\n\nIndustry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets\n\nStrong data visualization skills\n\nBasic understanding of Machine Learning; Prior experience in ML Engineering a plus\n\nAbility to manage on-premises data and make it inter-operate with AWS based pipelines\n\nAbility to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus\n\n\nEducation\n\nBachelor's degree in computer science, engineering, mathematics, or a related technical discipline\n\nPreferred QualificationsMasters in CS/ECE with a Data Science / ML Specialization\n\n\nMinimum Qualifications:\n\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\n\nOR\n\nMaster's degree in Engineering, Information Systems, Computer Science, or related field\n\nOR\n\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n3+ years of experience with Programming Language such as C, C++, Java, Python, etc.\n\n\nDevelops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design.\n\nPrincipal Duties and Responsibilities:\n\nCompletes assigned coding tasks to specifications on time without significant errors or bugs.\n\nAdapts to changes and setbacks in order to manage pressure and meet deadlines.\n\nCollaborates with others inside project team to accomplish project objectives.\n\nCommunicates with project lead to provide status and information about impending obstacles.\n\nQuickly resolves complex software issues and bugs.\n\nGathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions.\n\nSeeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently.\n\nParticipates in technical conversations with tech leads/managers.\n\nAnticipates and communicates issues with project team to maintain open communication.\n\nMakes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks.\n\nPrioritizes project deadlines and deliverables with minimal supervision.\n\nResolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues).\n\nWrites readable code for large features or significant bug fixes to support collaboration with other engineers.\n\nDetermines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner.\n\nUnit tests own code to verify the stability and functionality of a feature.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'sql', 'software engineering', 'data visualization', 'aws', 'quicksight', 'c', 'software development', 'glue', 'aws sagemaker', 'data warehousing', 'machine learning', 'business intelligence', 'data engineering', 'java', 'data science', 'data modeling', 'athena', 'wireless', 'big data', 'etl', 'ml']",2025-06-12 14:59:38
Data Scientist (Offshore),HTC Global Services,2 - 7 years,Not Disclosed,['Chennai'],"We are seeking a Data Scientist (Offshore) with minimum experience of 3 or more years. The ideal candidate should be familiar with relational or NoSQL databases such as Oracle, Teradata, SQL Server, Hadoop and ELK etc.\nRequirements:\nMinimum 3 or more years working with languages such as R, Python or Java\nAt least 3 or more years working with advanced statistical methods such as regressions, classifiers, recommenders, anomaly detection, optimization algorithms, tree methods and neural nets etc.",,,,"['tableau', 'NoSQL', 'Hadoop', 'Agile', 'Teradata SQL', 'data visualization', 'Oracle', 'Powerpoint', 'SDLC', 'Python']",2025-06-12 14:59:40
"Lead, Engineer",Qualcomm,10 - 15 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nJob Overview:\n\nThe Qualcomm Cloud Computing team is developing hardware and software for Machine Learning solutions spanning the data center, edge, infrastructure, automotive market. We are seeking ambitious, bright, and innovative engineers with experience in machine learning framework development. Job activities span the whole product life cycle from early design to commercial deployment. The environment is fast-paced and requires cross-functional interaction daily so good communication, planning and execution skills are a must.\n\nWe are seeking a highly skilled and motivated Language Model Engineer to join our team. The primary role of the engineer will be to train Large Language Models (LLMs) from scratch and fine-tune existing LLMs on various datasets using state-of-the-art techniques.\n\nResponsibilities:\n\nModel Training and Fine-tuning:\n\nTrain LLMs from scratch using various datasets. Fine-tune pre-trained models on specific tasks or datasets to improve performance. Implement state-of-the-art LLM training techniques such as Reinforcement Learning from Human Feedback (RLHF), ZeRO (Zero Redundancy Optimizer), Speculative Sampling, and other speculative techniques.\n\n\nData Management: Handle large datasets effectively. Ensure data quality and integrity. Implement data cleaning and preprocessing techniques. Hands-on with EDA is a plus.\n\n\nModel Evaluation: Evaluate model performance using appropriate metrics. Understand the trade-offs between different evaluation metrics.\n\n\nLLM metrics: Sound understanding of various LLM metrics like MMLU, Rouge, BLEU, Perplexity etc.\n\n\nAWQ: Understanding of Quantization is a plus. Knowledge on QAT will be a plus.\n\n\nResearch and Development: Stay updated with the latest research in NLP and LLMs. Implement state-of-the-art techniques and contribute to research efforts.\n\n\nCollaboration: Work closely with other teams to understand requirements and implement solutions.\n\n\nRequired Skills and Experience:\n\n\n\nDeep Learning Frameworks: Hands-on experience with PyTorch at a granular level. Familiarity with tensor operations, automatic differentiation, and GPU acceleration in PyTorch.\n\n\nNLP and LLMs: Strong understanding of Natural Language Processing (NLP) and experience working with LLMs.\n\n\nProgramming: Proficiency in Python and experience with software development best practices.\n\n\nData Handling: Experience working with large datasets. Familiarity with data version control tools is a plus.\n\n\nEducation: A degree in Computer Science, Machine Learning, AI, or related field. Advanced degree is a plus.\n\n\nCommunication: Excellent written and verbal communication skills.\n\n\nWork experience : Open, 2- 10 years of relevant experience.\n\n\n\n\nPreferred\n\nSkills:\n\n\n\n\nOptimization: Knowledge of optimization techniques for training large models.\n\n\nNeural Architecture Search (NAS): Experience with NAS techniques for optimizing model architectures is a plus. Hands-on experience with CUDA, CUDNN is a plus.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'c++', 'natural language processing', 'pytorch', 'software engineering', 'cuda', 'nas', 'eda', 'version control', 'sampling', 'machine learning', 'deep learning', 'java', 'data center', 'computer science', 'product life cycle', 'research and development', 'machine learning algorithms']",2025-06-12 14:59:43
Lead AI/ML Engineer,Synechron,8 - 12 years,Not Disclosed,['Bengaluru'],"job requisition idJR1027508\n\nOverall Responsibilities:\nExperience with Machine Learning, Deep Learning, Computer Vision, NLP, Generative AI.\nKnowledge of algorithms, object-oriented and functional design principles, and best-practice patterns\nSolid understanding of common programming languages used in AI, such as Python, Java, C++, and R\nAdvanced knowledge of statistical and algorithmic models as well as of fundamental mathematical concepts, such as linear algebra and probability\nExperience working with large data sets and writing efficient code capable of processing large data streams at speed.\n\n\nSkills:\nMachine Learning, Deep Learning, Computer Vision, NLP\nStrong hands-on experience in AI/ML/Data Science, including machine learning algorithms, deep learning, NLP, computer vision, etc.\nKnowledge of programming languages such as Python, R, SQL, etc.\nExperience with AI/ML/Data Science tools and frameworks such as TensorFlow, PyTorch, etc.\nExcellent problem-solving skills and ability to find creative solutions to complex data science problems.\nStrong communication and interpersonal skills to effectively collaborate with cross-functional teams and clients\nExperience:\nMinimum 8-12 years of experience in AI/ML/Data Science, with at least 8+ years in a leadership role\nProven track record of successfully delivering AI/ML/Data Science projects\nExperience in mentoring and leading a team of AI/ML/Data Science professionals\nDay-to-Day Activities:\nLead AI/ML/Data Science projects, ensuring successful delivery\nMentor and guide junior team members\nCollaborate with cross-functional teams to provide subject matter expertise in AI/ML/Data Science\nStay updated with latest AI/ML/Data Science trends and technologies\nIdentify and evaluate new business opportunities in AI/ML/Data Science\nQualification:\nBachelor's or Master's degree in Computer Science, Data Science, AI, or related field\nSoft\n\nSkills:\nExcellent leadership and team management skills\nStrong interpersonal and communication skills\nAbility to work in a fast-paced and dynamic environment\nGood negotiation and stakeholder management skills\nPassionate about AI/ML/Data Science and staying updated with latest trends and technologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['team management', 'aiml', 'artificial intelligence', 'data science', 'ml', 'algorithms', 'python', 'c++', 'data analytics', 'natural language processing', 'machine learning', 'sql', 'deep learning', 'tensorflow', 'r', 'java', 'computer vision', 'pytorch', 'machine learning algorithms', 'statistics']",2025-06-12 14:59:46
Lead Software Engineering - Python Developer,JPMorgan Chase Bank,1 - 9 years,Not Disclosed,['Bengaluru'],"Are you ready to elevate your career in software engineeringJoin our dynamic team as a Python Developer, where your expertise will drive cutting-edge solutions and contribute to impactful projects. We offer unparalleled opportunities for career growth and a collaborative environment where you can thrive and make a significant impact.\nAs a Lead Software Engineer at JPMorgan Chase within the Technology and Engineering division, you will execute software solutions, design, and development, collaborating with cross-functional teams to deploy machine learning services. You will be responsible for producing architecture and design artifacts for complex applications, ensuring design constraints are met. Your role will involve contributing to the engineering community and influencing the use of leading-edge technologies.\nJob Responsibilities\nExecute software solutions, design, development, and technical troubleshooting, thinking beyond routine approaches.\nCreate secure and high-quality production code, maintaining algorithms that run synchronously with systems.\nProduce architecture and design artifacts for complex applications, ensuring design constraints are met.\nCollaborate with cross-functional teams, including Data Science partners, to design and deploy machine learning services.\nContribute to the engineering community as an advocate of firmwide frameworks, tools, and practices.\nInfluence peers and project decision-makers to consider leading-edge technologies.\nAdd to the team culture of diversity, equity, inclusion, and respect.\nRequired Qualifications, Capabilities, and Skills\nFormal training or certification on Software Engineering concepts and 5+ years applied experience.\nExperience in building complex software systems in both private and public cloud environments (AWS).\nHands-on practical experience delivering system design, application development, testing, and operational stability.\nAdvanced Python Programming Skills including Pandas, Numpy.\nProficiency with AIM algorithms.\nAdvanced knowledge of software applications and technical processes in technical disciplines (e. g. , cloud, AI, ML).\nAbility to tackle design and functionality problems independently with little oversight.\nPreferred Qualifications, Capabilities, and Skills\nAdvanced skills in additional programming languages (Java).\nFamiliarity with building services and consuming data via GraphQL, REST, or gRPC and SQL\nExperience in building and deploying machine learning models, with knowledge of the ML Lifecycle; expertise in MLOps and AIOps is an advantage.\nWorking knowledge of security best practices and compliance standards for machine learning systems.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Lead Software', 'Machine learning', 'System design', 'Deployment', 'Application development', 'Troubleshooting', 'Operations', 'Software solutions', 'SQL', 'Python']",2025-06-12 14:59:48
Machine Learning Scientist (LLM),Wadhwani Ai,6 - 11 years,Not Disclosed,['Mumbai'],"SUMMARY\nWe are looking for a Machine Learning Scientist who is passionate about aiding India s emergence into an AI superpower and about applying AI and ML to solve high-impact societal challenges, especially in the area of large language models (LLMs). The ideal candidate will have a strong research or applied background with 6+ years of relevant experience or a PhD, and expertise in designing and implementing machine learning models, including LLMs, data analysis, and simulations. They should enjoy interfacing with disparate internal and external stakeholders, mentoring junior scientists and developers, working in cross-functional teams, and contributing to ML evaluations and practical deployments. A demonstrated commitment to social good, strong communication skills, and the ability to translate complex problems into deployable ML solutions are essential.\nABOUT US - https://www.wadhwaniai.org/\nWadhwani AI is a nonprofit institute building and deploying applied AI solutions to solve critical issues in public health, agriculture, education, and urban development in underserved communities in the global south. We collaborate with governments, social sector organizations, academic and research institutions, and domain experts to identify real-world problems, and develop practical AI solutions to tackle these issues with the aim of making a substantial positive impact.\nPertinent to this role, Wadhwani AI also works closely with the Ministry of Electronics and Information Technology (MEITy), Government of India, in helping flesh out the various pillars of the India AI mission.\nWe have over 30+ AI projects supported by leading philanthropies such as Bill & Melinda Gates Foundation, USAID and Google.org. With a team of over 200 professionals, our expertise encompasses AI/ML research and innovation, software engineering, domain knowledge, design and user research.\nIn the Press:\nOur Founder Donors are among the Top 100 AI Influencers\nG20 India s Presidency: AI Healthcare, Agriculture, & Education Solutions Showcased Globally.\nUnlocking the potentials of AI in Public Health\nWadhwani AI Takes an Impact-First Approach to Applying Artificial Intelligence - data.org\nWinner of the H&M Foundation Global Change Award 2022\nIndian Winners of the 2019 Google AI Impact Challenge, and the first in the Asia Pacific to host Google Fellows\nROLES AND RESPONSIBILITIES\nAs an ML Scientist working with our team at the India AI Mission, you will be responsible for critically evaluating LLM solutions, building machine learning solutions to problems of societal importance, and mentoring other team members in this effort. You will participate in stakeholder engagements, setting critical requirements, problem definition and the development and execution of algorithms and solutions to the problems.\nYou will need to understand user challenges and their context, curate and transform data, train and validate models, run simulations, and broadly derive insights from data. In doing so, you will work in cross-functional teams spanning administration, engineering, solutions, domain experts, and designers. You will also work closely with social sector organisations and are encouraged to collaborate with researchers across the world.\nREQUIREMENTS\nThis role requires the candidate to work out of the India AI Mission office at MEITy, Government of India.\nM.Tech./M.E./M.S./M.Sc. or equivalent in Computer Science, Electrical Engineering, Statistics, Applied Mathematics, Physics, Economics or a relevant quantitative field with work experience of 6+ years or a PHd from a reputed institute.\nWe are looking for ML Scientists with experience applying AI, machine learning, and data science to real-world problems. Ideal candidates should have a strong research background, be familiar with the state of the art especially in foundational models and LLMs, have experience in mentoring junior scientists and developers through strong code practices and ideation, and be adept at a variety of data mining/analysis methods and tools, building and implementing models, visualising data, creating/using algorithms, and running simulations.\nCandidates should be completely comfortable working with cross-functional teams in an uncertain and fast-paced environment, and must have excellent oral and written communication skills and a track record of driving projects to completion. Evidence of written skills through high quality research publications is a plus.\nCandidates should be passionate about using their technical skills to solve large societally important problems.",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'data science', 'Artificial Intelligence', 'Machine learning', 'Healthcare', 'Research', 'Data mining', 'Information technology', 'Public health']",2025-06-12 14:59:51
Security and Access control-Lead Engineer,Qualcomm,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Systems Engineer, you will research, design, develop, simulate, and/or validate systems-level software, hardware, architecture, algorithms, and solutions that enables the development of cutting-edge technology. Qualcomm Systems Engineers collaborate across functional teams to meet and exceed system-level requirements and standards.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.\n\nJob Overview\n\nWork with Qualcomm's security architecture / IP and access control team on next generation SOC for smartphone, tablet, automotive and IOT product categories. is responsible for assisting product development teams throughout the company to apply secure HW design principles to individual blocks, computing cores, and at the SoC level. SW/HW co-design, HW development experience. Familiarity with debug architectures such as JTAG and ARM coresight are a plus\n\nSuccessful candidates will be able to engage with product teams independently with minimal supervision to detect and mitigate security vulnerabilities in hardware architecture and implementations, involve in access control issues at both SW and HW.\n\nMinimum Qualifications\n\n5 to 7+ years of industry or academic experience in Security are required.\n\nAdditionally,",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['vhdl', 'hypervisor', 'system verilog', 'system engineering', 'verilog', 'jtag', 'arm architecture', 'c++', 'storage domain', 'soc', 'artificial intelligence', 'power analysis', 'fpga', 'access control', 'i2c', 'ml', 'asic', 'python', 'side', 'c', 'systemc', 'cryptography', 'aiml', 'spi', 'asic design', 'embedded c', 'uart']",2025-06-12 14:59:53
Data Science & AI Engineer,Blue Altair,5 - 8 years,Not Disclosed,['Pune'],"Greetings from Blue Altair!\nJob Overview:\nWe are seeking an experienced and highly skilled Data Science and AI Engineer to join our dynamic team. The ideal candidate will have 5+ years of experience working on cutting-edge data science and AI technologies across various cloud platforms with a strong focus to work on LLMs and SLMs. The role demands a professional capable of performing in a client-facing environment, as well as mentoring and guiding junior team members.\n\nTitle: Consultant/Sr. Consultant - Data Science Engineer\nExperience: 5-8 years\nLocation: Pune/Bangalore (Hybrid)\n\nRoles and responsibilities:\nDevelop, implement, and optimize machine learning models and AI algorithms to solve complex business problems.\nDesign, build, and fine-tune AI models, particularly focusing on LLMs and SLMs, using state-of-the-art techniques and architectures.\nApply advanced techniques in prompt engineering, model fine-tuning, and optimization to tailor models for specific business needs.\nDeploy and manage machine learning models and pipelines on cloud platforms (AWS, GCP, Azure, etc.).\nWork closely with clients to understand their data and AI needs and provide tailored solutions.\nCollaborate with cross-functional teams to integrate AI solutions into broader software architectures.\nMentor junior team members and provide guidance in implementing best practices in data science and AI development.\nStay up-to-date with the latest trends and advancements in data science, AI, and cloud technologies.\nPrepare technical documentation and present insights to both technical and non-technical stakeholders.\n\nRequirement:\n5+ years of experience in data science, machine learning, and AI technologies.\nProven experience working with cloud platforms such as Google Cloud, Microsoft Azure, or AWS.\nExpertise in programming languages such as Python, R, Julia, and AI frameworks like TensorFlow, PyTorch, Scikit-learn, Hugging face Transformers.\nKnowledge of data visualization tools (e.g., Matplotlib, Seaborn, Tableau)\nSolid understanding of data engineering concepts including ETL, data pipelines, and databases (SQL, NoSQL).\nExperience with MLOps practices and deployment of models in production environments.\nFamiliarity with NLP (Natural Language Processing) tasks and working with large-scale datasets.\nHands-on experience with generative AI models like GPT, Gemini, Claude, Mistral etc.\nClient-facing experience with strong communication skills to manage and engage stakeholders.\nStrong problem-solving skills and analytical mindset.\nAbility to work independently and as part of a team and mentor and provide technical leadership to junior team members.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLMs', 'Artificial Intelligence', 'MLOps', 'RAG', 'Natural Language Processing', 'Neural Networks', 'LLM', 'Machine Learning', 'AI Models', 'Data Science', 'PyTorch', 'SLM', 'AI Automation']",2025-06-12 14:59:56
V&V Vehicle System Test Lead Engineer (Staff - AD/ADAS),Qualcomm,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Test Engineering\n\nGeneral Summary:\n\nAt Qualcomm, we are transforming the automotive industry with our Snapdragon Digital Chassis and building the next generation software defined vehicle (SDV).\n\nSnapdragon Ride is an integral pillar of our Snapdragon Digital Chassis, and since its launch it has gained momentum with a growing number of global automakers and Tier1 suppliers. Snapdragon Ride aims to address the complexity of autonomous driving and ADAS by leveraging its high-performance, power-efficient SoC, industry-leading artificial intelligence (AI) technologies and pioneering vision and drive policy stack to deliver a comprehensive, cost and energy efficient systems solution.\n\nThe Software Test and Quality infrastructure team is centrally defining, establishing, and rolling out the software test frameworks and software quality infrastructure used by multiple projects within Automated Driving.\n\nWe are looking for smart, innovative, and motivated individuals with strong vehicle testing background and experience to test the ADAS SW platform. The chosen candidate will get an opportunity to lead ADAS vehicle validation team working for a leading Indian car manufacturer.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Test Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Test Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Test Engineering or related work experience.\n\n2+ years of work or academic experience with Software Test or System Test, developing and automating test plans and/or tools (e.g., Source Code Control Systems, Continuous Integration Tools, and Bug Tracking Tools).\nPrepare test strategy based on customer requirement/Tier1 Catalogue. Review on and Test Cases.\n\nPlan and execute multiple level testing smoke test, L0/L1 testing, software in loop testing and vehicle testing for AD entry +/ Mid\n\nDesigns test plans, test cases, test scenarios, scripts, or procedures with the target to ensure the best coverage of the requirements for features.\n\nResponsible for preparing consolidated test report with test coverage, Known issues, functional/nonfunctional test results, observations, and bugs\n\nReprocess and analyze the events regression testing in application dataset from OEM project.\n\nSupport team to test in vehicle the System integration OEM specific hardware, error guessing, configurability testing, issue reproducibility, exploratory and feature combining tests for ADAS SW platform for features like:\n\no Adaptive Cruise Control\n\no Autonomous Emergency Braking\n\no Collision avoidance features (Lane Support System, Traffic Assist, Risk Mitigation Support)\n\no Road Sign Information\n\nDocuments systems-level defects, using a bug tracking system, and report defects to developers.\n\nIdentifies, analyzes, troubleshoots, and documents problems with program function, output, or content. Develops testing programs that assess effectiveness of a new system or modification of an existing system.\n\nAssure that the project is developed according to Qualcomm quality standards.\n\nPreferred Qualifications:\n\nBachelor's degree in Computer Science, Informatics or equivalent.\n\nMinimum of 7+ years of relevant work experience.\n\nKnowledge of CAN/Ethernet communication protocol experience with the associated tools form Vector (i.e. CANoe, CANalyzer, CANdela) and Star Corporation tools (i.e. Fl3X)\n\nExperience with flashing ADAS systems\n\nFamiliarity with C, CAPL programming languages\n\nExcellent analytical skills\n\nDriver Certification\n\nNice to have:\n\nAdvanced pilot passenger vehicle tests driver certification\n\nExperience in designing test cases and test automation solutions.\n\nGIT knowledge\n\nBasic C++ Programming\n\nPython scripting\n\nContinuous Integration knowledge\n\nISTQB certification\n\nAgile mindset and experience with SCRUM",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['canoe', 'test engineering', 'system testing', 'canalyzer', 'automating', 'c++', 'redhat openshift', 'hiring', 'candela', 'staffing', 'git', 'data warehouse testing', 'microsoft visual studio', 'can bus', 'python', 'c', 'software testing', 'svn', 'siebel', 'seibel', 'ethernet', 'capl', 'visio', 'siebel crm', 'sdlc', 'mqc']",2025-06-12 14:59:58
Automotive Software Lead Engineer Sr. - C/C++,Qualcomm,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAt Qualcomm, we are transforming the automotive industry with our Snapdragon Digital Chassis and building the next generation software defined vehicle (SDV).\n\nSnapdragon Ride is an integral pillar of our Snapdragon Digital Chassis, and since its launch it has gained momentum with a growing number of global automakers and Tier1 suppliers. Snapdragon Ride aims to address the complexity of autonomous driving and ADAS by leveraging its high-performance, power-efficient SoC, industry-leading artificial intelligence (AI) technologies and pioneering vision and drive policy stack to deliver a comprehensive, cost and energy efficient systems solution.\n\nEnabling safe, comfortable, and affordable autonomous driving includes solving some of the most demanding and challenging technological problems. From centimeter-level localization to multimodal sensor perception, sensor fusion, behavior prediction, maneuver planning, and trajectory planning and control, each one of these functions introduces its own unique challenges to solve, verify, test, and deploy on the road.\n\nWe are looking for smart, innovative, and motivated individuals with strong SW background and programming experience with languages such as C/C++, python, and more. Job responsibilities include design and development of SW framework and middleware. Development of sensor drivers to bring in sensors (IMU, GPS, Camera, Radar, Lidar, Ultrasonic) to our platform, sensor synchronization, and efficient techniques to share sensor across different SW modules. Work closely with different teams to implement SW optimization on Snapdragon Ride platform as well as involved in Ride SDK development. Work closely with test engineers to develop test plans and validation of SW. Will be\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n5 to 8 years of C++ development, C++11 and C++14 is a strong plus\n\nEmbedded SW design and development for safety critical systems is a strong plus\n\nExperience with Programming languages such as C++, Python, Shell, etc.\n\nExperience with multi-threaded / multi-core SW development and design\n\nKnowledge/experience on Linux and embedded platform with QNX, AGL, Safe Linux, etc.\n\nKnowledge of Linux network stack and any experience with network device drivers is a plus\n\nFamiliarity with ROS/ROS2, DDS, Adaptive AUTOSAR middleware and frameworks\n\nKnowledge / experience with safety critical software development process (Functional Safety), including ASPICE, ASIL, ISO26262, MISRA C++, AUTOSAR C++\n\nFamiliarity with static analysis tools, code coverage metrics, unit test generation\n\nExperience with source code management tools such as git, git-lfs, github/gitlab\n\nExcellent written and verbal communication skills, ability to work with a cross-functional",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'java', 'linux', 'software engineering', 'qnx', 'dd', 'sw design', 'python', 'github', 'aspice', 'c', 'embedded sw', 'device drivers', 'static analysis', 'embedded sw development', 'autosar', 'rts', 'git', 'misra', 'adas', 'shell scripting', 'multithreading', 'gitlab', 'functional safety']",2025-06-12 15:00:01
"Embedded Platform Dev- Lead Engineer, Senior",Qualcomm,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nQualcomm ADAS/Autonomy team is engaged in offering optimized solutions built on DSP, computer vision and machine learning algorithms for the Qualcomm ADAS/Autonomy SoCs. We are seeking engineers with experience in system and SoC SW level functional safety concepts. The job requires understanding and defining of the Safety Concept and Architecture, Software Safety requirements, defining and deploying safety processes and development of Safety software by following the ISO26262 software processes. Interaction with customers, architects and test/integration teams are required as part of the job. The job also involves working with the Software quality team for adherence of ISO26262 and ASPICE processes.\n\nIn this role, the candidate will work with local and global teams to understand, define and implement and productize Automotive specific features including software enablement (drivers/BSP/RTOS/AUTOSAR MCAL), security, functional safety, and power applied to Automotive products on our current and next generation SoCs. The candidate will also have the responsibility to coordinate and execute plans which will encompass validation of all the feature requirements. The Candidate will have the responsibility to identify and address any abnormal discoveries by root-causing and providing detailed corrective actions in the form of optimizations and/or fixes. When possible, the candidate is expected to prototype and pre-validate recommended fixes. Additionally, the candidate will be responsible for any automation of design under test along with validation efforts and working closely with design/production/bench IP teams.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\n 3-6 years of Embedded Software Development experience, including low level drivers, and RTOS \n\n The candidate should possess 3 to 6 years of industry experience in embedded software driver development and having expertise in one or more below areas would be preferred: \n\n Should be able to ramp up fast and must have the attitude to work with the team. \n\n Strong C and Assembly Programming with OS & Multi-Processor concepts \n\n Embedded software development in C and C++ on ARM or similar cores. \n\n Hands on experience of driver development on any RTOS, \n\n Experience in SafeRTOS/FreeRTOS based development is nice to have \n\n Experience in Autosar MCAL development is nice to have \n\n Experience in Autosar BSW integration and validation is nice to have \n\n ARM Trust-Zone & ARMv7/v8 architecture. \n\n Good debugging skills with experience on debugging with Lauterbach JTAG debuggers. \n\n Work on challenging customer requirements and issues. \n\n Basic understanding one or more of hardware blocks - Clocks, PLLs, GPIO, Interrupt Controllers (GIC), Peripherals (SPI/I2C/UART/CAN/Ethernet/Clock/etc)  \n\n Automotive SW development experience is must have \n\n Experience in ISO26262/functional safety and ASPICE is highly desirable \n\n Basic knowledge on Power Mgmt. IC is desirable \n\n Knowledge of Software/Hardware Security concepts is desirable \nClosely work with the hardware team to contribute/suggest modifications to the hardware design.\nAny past working experience on Qualcomm chips nice to have",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['embedded software development', 'assembly programming', 'c', 'sw', 'embedded software', 'dsp', 'algorithms', 'jtag', 'c++', 'aspice', 'freertos', 'rtos', 'java', 'debugging', 'software engineering', 'ic', 'i2c', 'can bus', 'arm', 'functional safety', 'python', 'spi', 'autosar', 'ethernet', 'mcal', 'uart', 'adas', 'bsp']",2025-06-12 15:00:04
Senior Data Engineer,Qualcomm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Data Engineer\n\nGeneral Summary:\n\nDeveloper will play an integral role in the PTEIT Machine Learning Data Engineering team. Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics. Design, develop and support CI/CD of data pipelines and services. - 5+ years of experience with Python or equivalent programming using OOPS, Data Structures and Algorithms - Develop new services in AWS using server-less and container-based services. - 3+ years of hands-on experience with AWS Suite of services (EC2, IAM, S3, CDK, Glue, Athena, Lambda, RedShift, Snowflake, RDS) - 3+ years of expertise in scheduling data flows using Apache Airflow - 3+ years of strong data modelling (Functional, Logical and Physical) and data architecture experience in Data Lake and/or Data Warehouse - 3+ years of experience with SQL databases - 3+ years of experience with CI/CD and DevOps using Jenkins - 3+ years of experience with Event driven architecture specially on Change Data Capture - 3+ years of Experience in Apache Spark, SQL, Redshift (or) Big Query (or) Snowflake, Databricks - Deep understanding building the efficient data pipelines with data observability, data quality, schema drift, alerting and monitoring. - Good understanding of the Data Catalogs, Data Governance, Compliance, Security, Data sharing - Experience in building the reusable services across the data processing systems. - Should have the ability to work and contribute beyond defined responsibilities - Excellent communication and inter-personal skills with deep problem-solving skills.\n\nMinimum Qualifications:\n3+ years of IT-related work experience with a Bachelor's degree in Computer Engineering, Computer Science, Information Systems or a related field.\nOR\n5+ years of IT-related work experience without a Bachelors degree.\n\n2+ years of any combination of academic or work experience with programming (e.g., Java, Python).\n1+ year of any combination of academic or work experience with SQL or NoSQL Databases.\n1+ year of any combination of academic or work experience with Data Structures and algorithms.\n5 years of Industry experience and minimum 3 years experience in Data Engineering development with highly reputed organizations- Proficiency in Python and AWS- Excellent problem-solving skills- Deep understanding of data structures and algorithms- Proven experience in building cloud native software preferably with AWS suit of services- Proven experience in design and develop data models using RDBMS (Oracle, MySQL, etc.)\n\nDesirable - Exposure or experience in other cloud platforms (Azure and GCP) - Experience working on internals of large-scale distributed systems and databases such as Hadoop, Spark - Working experience on Data Lakehouse platforms (One House, Databricks Lakehouse) - Working experience on Data Lakehouse File Formats (Delta Lake, Iceberg, Hudi)\n\nBachelor's or Master's degree in Computer Science, Software Engineering, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data quality', 'data structures', 'aws', 'schema', 'continuous integration', 'glue', 'amazon redshift', 'event driven architecture', 'ci/cd', 'data engineering', 'sql', 'alerts', 'java', 'data modeling', 'spark', 'devops', 'data flow', 'nosql databases', 'sql database']",2025-06-12 15:00:06
Display System Performance - Lead Engineer,Qualcomm,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nJob Summary:\n\nWe are seeking a highly motivated and skilled Performance and Power Analysis Engineer to join our Display Systems team in Bengaluru. In this critical role, you will be responsible for the analysis, modeling, and optimization of performance and power consumption across various stages of our cutting-edge chip development process. You will take the lead and collaborate closely with architecture, design, Software and verification teams to ensure our products meet stringent performance targets and power efficiency requirements. As an independent collaborator, contribute with cross functional teams, SoC performance and SW/HW teams to enhance or optimize the process. This is an exciting opportunity to contribute to the development of next-generation semiconductor technology.\n\nResponsibilities:\nDevelop and maintain architectural-level and/or cycle-accurate models for performance and power estimation.\nAnalyze trade-offs between performance, power, and area (PPA) at the architecture and microarchitecture levels.\nDrive performance and power analysis early in the design cycle to influence architecture and design decisions.\nCollaborate with architecture and design teams to explore and evaluate different design options and trade-offs to optimize performance and power.\nConduct detailed analysis to identify performance bottlenecks and power inefficiencies in chip architectures and microarchitectures.\nPerform power profiling and characterization of designs under various operating conditions and workloads.\nDevelop and implement power reduction techniques at different design stages (e.g., clock gating, power gating, voltage scaling).\nAnalyze and debug performance and power-related issues during simulation, emulation, and silicon bring-up.\nGenerate comprehensive reports and presentations summarizing analysis results and providing actionable recommendations to the design teams, cross-functional teams and senior leadership.\nStay abreast of the latest industry trends, tools, and methodologies in performance and power analysis.\nContribute to the development and improvement of internal tools and flows for performance and power analysis.\nCollaborate with verification teams to define and execute performance and power validation plans.\nValidate model accuracy through correlation with RTL simulations, emulation, and silicon measurements.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.\n\nQualifications:\nBachelor's or Master's degree in Electrical Engineering, Computer Engineering, or a related field.\n5 to 8+ years of experience in performance and power analysis for ASIC or SoC designs.\nStrong understanding of computer architecture, microarchitecture, and digital design principles.\nStrong experience in developing and utilizing performance and power models using languages such as SystemC, Python, C++, or custom in-house frameworks.\nProficiency in using industry-standard performance and power analysis tools (e.g., Synopsys PrimeTime PX)\nSolid understanding of power management techniques and low-power design methodologies.\nExperience with simulation and emulation environments.\nStrong analytical and problem-solving skills with the ability to interpret complex data and draw meaningful conclusions.\nExcellent communication and interpersonal skills with the ability to collaborate effectively with cross-functional teams.\nFamiliarity with silicon bring-up and post-silicon power/performance characterization is a plus.\nExperience with machine learning techniques for power/performance prediction is a plus.\nExperience with IOS and Xcode profiling/development is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['digital design', 'microservices', 'computer architecture', 'analysis tools', 'design principles', 'asic', 'python', 'c++', 'systemc', 'xcode', 'ios', 'machine learning', 'power analysis', 'silicon', 'power management', 'soc design', 'machine learning algorithms', 'system engineering']",2025-06-12 15:00:09
Big Data Developer/Data Engineer,Grid Dynamics,5 - 10 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nExperience: 5 - 8 years\nEmployment Type: Full-Time\n\nJob Summary:\nWe are looking for a highly skilled Scala and Spark Developer to join our data engineering team. The ideal candidate will have strong experience in building scalable data processing solutions using Apache Spark and writing robust, high-performance applications in Scala. You will work closely with data scientists, data analysts, and product teams to design, develop, and optimize large-scale data pipelines and ETL workflows.\n\nKey Responsibilities:\nDevelop and maintain scalable data processing pipelines using Apache Spark and Scala.\nWork on batch and real-time data processing using Spark (RDD/DataFrame/Dataset).\nWrite efficient and maintainable code following best practices and coding standards.\nCollaborate with cross-functional teams to understand data requirements and implement solutions.\nOptimize performance of Spark jobs and troubleshoot data-related issues.\nIntegrate data from multiple sources and ensure data quality and consistency.\nParticipate in design reviews, code reviews, and provide technical leadership when needed.\nContribute to data modeling, schema design, and architecture discussions.\nRequired Skills:\nStrong programming skills in Scala.\nExpertise in Apache Spark (Core, SQL, Streaming).\nHands-on experience with distributed computing and large-scale data processing.\nExperience with data formats like Parquet, Avro, ORC, and JSON.\nGood understanding of functional programming concepts.\nFamiliarity with data ingestion tools (Kafka, Flume, Sqoop, etc.).\nExperience working with Hadoop ecosystem (HDFS, Hive, YARN, etc.) is a plus.\nStrong SQL skills and experience working with relational and NoSQL databases.\nExperience with version control tools like Git.\nPreferred Qualifications:\nBachelor's or Masters degree in Computer Science, Engineering, or related field.\nExperience with cloud platforms like AWS, Azure, or GCP (especially EMR, Databricks, etc.).\nKnowledge of containerization (Docker, Kubernetes) is a plus.\nFamiliarity with CI/CD tools and DevOps practices.ndidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Pyspark', 'Spark']",2025-06-12 15:00:11
Senior Data Engineer - AWS,Blend360 India,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nQualifications\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 15:00:14
Data Engineer,Grid Dynamics,4 - 9 years,Not Disclosed,['Bengaluru'],"Required Qualifications:\n4+ years of professional experience in data engineering and data analysis roles.\nStrong proficiency in SQL and experience with database management systems such as MySQL, PostgreSQL, Oracle, and MongoDB.\nHands-on experience with big data tools like Hadoop and Apache Spark.\nProficient in Python programming.\nExperience with data visualization tools such as Tableau, Power BI, and Jupyter Notebooks.\nProven ability to design, build, and maintain scalable ETL pipelines using tools like Apache Airflow, DBT, Composer (GCP), Control-M, Cron, and Luigi.\nFamiliarity with data engineering tools including Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nExperience working with cloud data warehouses and services (Snowflake, Redshift, BigQuery, AWS Glue, GCP Dataflow, Azure Data Factory).\nUnderstanding of data modeling concepts and data lake/data warehouse architectures.\nExperience supporting CI/CD practices with Git, Docker, Terraform, and DevOps workflows.\nKnowledge of both relational and NoSQL databases, including PostgreSQL, BigQuery, MongoDB, and DynamoDB.\nExposure to Agile and DevOps methodologies.\nExperience with at least one cloud platform:\nGoogle Cloud Platform (BigQuery, Dataflow, Composer, Cloud Storage, Pub/Sub)\nAmazon Web Services (S3, Glue, Redshift, Lambda, Athena)\nMicrosoft Azure (Data Factory, Synapse Analytics, Blob Storage)\nEssential functions\nKey Responsibilities:\nDesign, develop, and maintain robust, scalable ETL pipelines using Apache Airflow, DBT, Composer (GCP), Control-M, Cron, Luigi, and similar tools.\nBuild and optimize data architectures including data lakes and data warehouses.\nIntegrate data from multiple sources ensuring data quality and consistency.\nCollaborate with data scientists, analysts, and stakeholders to translate business requirements into technical solutions.\nAnalyze complex datasets to identify trends, generate actionable insights, and support decision-making.\nDevelop and maintain dashboards and reports using Tableau, Power BI, and Jupyter Notebooks for visualization and pipeline validation.\nManage and optimize relational and NoSQL databases such as MySQL, PostgreSQL, Oracle, MongoDB, and DynamoDB.\nWork with big data tools and frameworks including Hadoop, Spark, Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nUtilize cloud data services and warehouses like AWS Glue, GCP Dataflow, Azure Data Factory, Snowflake, Redshift, and BigQuery.\nSupport CI/CD pipelines and DevOps workflows using Git, Docker, Terraform, and related tools.\nEnsure data governance, security, and compliance standards are met.\nParticipate in Agile and DevOps processes to enhance data engineering workflows.\nQualifications\nData Engineer with experience in MySQL or SQL or PL/SQL and any cloud experience like GCP or AWS or Azure\nWould be a plus\nPreferred Skills:\nStrong problem-solving and communication skills.\nAbility to work independently and collaboratively in a team environment.\nExperience with service development, REST APIs, and automation testing is a plus.\nFamiliarity with version control systems and workflow automation.\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Automation', 'Data modeling', 'MySQL', 'Workflow', 'Informatica', 'Oracle', 'Apache', 'SSIS', 'Analytics']",2025-06-12 15:00:16
Software Development Engineer 1,Meesho,0 - 1 years,20-25 Lacs P.A.,['Bengaluru'],"Join us for an exciting SDE Traineeship at Meesho.\nBased on the performance at Meesho assessment, successful candidates will be considered for a full-time opportunity (FTE).\n\nAPPLY HERE: https://p.hck.re/6TcJ\n\nAbout the role:\nAs an SDE Trainee , we expect you to be motivated in solving real-life complex problems and creating compelling experiences for our resellers. Being a small company we have a culture of creative problem- solving, intellectual design, fast-paced development, and passionate product delivery. The pace of our growth is incredible. If you want to tackle hard, interesting and UNIQUE problems, and create an impact within an entrepreneurial environment, JOIN US!\nKey Responsibilities:\nCollaborate with teams to develop new features for Meesho customers and suppliers\nLeverage state-of-the-art technologies and write highly performant code\nTake end-to-end ownership of features, from ideation to production\n\nTechnical Requirements:\n0-1 years of experience\nStrong problem-solving skills\nExcellent understanding of data structures and algorithms, and their space & time complexities\nStrong hands-on and practical working experience with at least one programming language: Java/Python/Javascript\nExcellent coding skills should be able to convert design into code fluently.",Industry Type: Internet (E-Commerce),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Generative Ai', 'Gen AI', 'java', 'Artificial Intelligence', 'Data Structures', 'Javascript', 'Python']",2025-06-12 15:00:18
"Data Analytics Fresher , Data Analyst Fresher",Ablycon Global Angalore,0 - 1 years,4.25-6.5 Lacs P.A.,"['Hyderabad', 'Bengaluru', 'Delhi / NCR']","NOTE- Please do not call. Apply through Naukri or email your resume at ankit@ablyconglobal.com. or whatsapp on 9821833955 - Don't CALL Please .\n\n\nJob Title: Data Analytics Fresher\nEmployment Type: Full-Time\nExperience: 0 - 1 Year\nQualification- ANY UG , ANY PG\n\n\nAbout the Role:\nWe are seeking a highly motivated and detail-oriented individual to join our Data Analytics team as a Data Analyst Fresher. This position offers a launchpad into the world of data analytics. Youll work on structured and unstructured datasets, assist in building dashboards and models, and get practical exposure to tools like SQL, Python, and BI platforms. Ideal for someone with a strong analytical foundation and a hunger to grow into a full-stack data professional.\n\nKey Responsibilities:\n\nCollect, organize, and analyze large datasets from various internal and external sources.\nAssist in preparing dashboards, reports, and visualizations to present insights and findings.\nSupport the team in identifying trends, anomalies, and patterns that impact business performance.\nWork with different departments (marketing, sales, operations, etc.) to understand data requirements.\nPerform exploratory data analysis (EDA) to help refine business strategies.\nMaintain and ensure data integrity and consistency across databases and reporting tools.\nSupport the automation of repetitive reporting processes using scripting or BI tools.\n\nRequired Skills & Qualifications:\n\nStrong analytical and problem-solving skills.\nProficiency in Excel and a basic understanding of SQL.\nFamiliarity with data visualization tools (e.g., Tableau, Power BI) is a plus.\nKnowledge of programming languages such as Python or R is an advantage.\nStrong communication skills to explain technical results to non-technical audiences.\nAttention to detail and a strong sense of responsibility.\nEagerness to learn new tools, technologies, and business domains.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Analytics', 'Business Analytics', 'Power Bi', 'Artificial Intelligence', 'Data Interpretation', 'Data Management', 'Data Extraction', 'Tableau', 'Machine Learning', 'Statistics', 'data analyst', 'SQL', 'Data Science', 'Excel', 'MySQL', 'Data Analysis', 'Data Visualization', 'Data Processing', 'Python']",2025-06-12 15:00:20
"AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING",Zensar,2 - 7 years,Not Disclosed,['Pune'],"Zensar Technologies is looking for AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING, ANALYTICS AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING, ANALYTICS to join our dynamic team and embark on a rewarding career journey\n\nDevelops and executes test plans for AI and machine learning models\n\nValidates model accuracy, fairness, performance, and edge-case behavior\n\nImplements automation tools and creates synthetic test datasets\n\nEnsures compliance with model validation protocols and documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Design engineering', 'deep learning', 'Technology consulting', 'Focus', 'Agile', 'Conceptualization', 'Management', 'Data mining', 'Analytics', 'Testing']",2025-06-12 15:00:23
Bangalore Non-Agent Requirement,Startek,0 - 3 years,Not Disclosed,['Bengaluru'],"STARTEK is looking for Bangalore Non-Agent Requirement to join our dynamic team and embark on a rewarding career journey\n\nSupports backend or operational roles not directly customer-facing\n\nHandles documentation, data entry, process execution, and reporting\n\nCollaborates with internal departments for workflow accuracy\n\nEnsures compliance with operational guidelines and SLAs",Industry Type: Oil & Gas,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Wireless', 'Business transformation', 'orchestration', 'Artificial Intelligence', 'Customer support', 'Customer experience', 'Management', 'Troubleshooting', 'Customer engagement', 'digital transformation']",2025-06-12 15:00:25
Senior Data Scientist,Epsilon,6 - 9 years,Not Disclosed,['Bengaluru'],"Responsibilities: -\nContribute and build an internal product library that is focused on solving business problems related to prediction & recommendation.\nResearch unfamiliar methodologies, techniques to fine tune existing models in the product suite and, recommend better solutions and/or technologies.\nImprove features of the product to include newer machine learning algorithms in the likes of product recommendation, real time predictions, fraud detection, offer personalization etc\nCollaborate with client teams to on-board data, build models and score predictions.\nParticipate in building automations and standalone applications around machine learning algorithms to enable a One Click solution to getting predictions and recommendations.\nAnalyze large datasets, perform data wrangling operations, apply statistical treatments to filter and fine tune input data, engineer new features and eventually aid the process of building machine learning models.\nRun test cases to tune existing models for performance, check criteria and define thresholds for success by scaling the input data to multifold.\nDemonstrate a basic understanding of different machine learning concepts such as Regression, Classification, Matrix Factorization, K-fold Validations and different algorithms such as Decision Trees, Random Forrest, K-means clustering.\nDemonstrate working knowledge and contribute to building models using deep learning techniques, ensuring robust, scalable and high-performance solutions\nMinimum Qualifications:\nEducation: Master's or PhD in a quantitative discipline (Statistics, Economics, Mathematics, Computer Science) is highly preferred.\nDeep Learning Mastery: Extensive experience with deep learning frameworks (TensorFlow, PyTorch, or Keras) and advanced deep learning projects across various domains, with a focus on multimodal data applications.\nGenerative AI Expertise: Proven experience with generative AI models and techniques, such as RAG, VAEs, Transformers, and applications at scale in content creation or data augmentation.\nProgramming and Big Data: Expert-level proficiency in Python and big data/cloud technologies (Databricks and Spark) with a minimum of 4-5 years of experience.\nRecommender Systems and Real-time Predictions: Expertise in developing sophisticated recommender systems, including the application of real-time prediction frameworks.\nMachine Learning Algorithms: In-depth experience with complex algorithms such as logistic regression, random forest, XGBoost, advanced neural networks, and ensemble methods.\nExperienced with machine learning algorithms such as logistic regression, random forest, XG boost, KNN, SVM, neural network, linear regression, lasso regression and k-means.\nDesirable Qualifications:\nGenerative AI Tools Knowledge: Proficiency with tools and platforms for generative AI (such as OpenAI, Hugging Face Transformers).\nDatabricks and Unity Catalog: Experience leveraging Databricks and Unity Catalog for robust data management, model deployment, and tracking.\nWorking experience in CI/CD tools such as GIT & BitBucket",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Engineering', 'Pyspark', 'Azure Aws', 'Generative AI', 'Big Data', 'AWS', 'Data Bricks', 'Deep Learning', 'Python', 'SQL']",2025-06-12 15:00:28
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-12 15:00:30
Data Scientist - Immediate Joiners Only,Reyika,5 - 10 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","Role: Data Scientist\nExperience: 5+ years\nLocation: Any - Hybrid (Bangalore, Hyderabad, Pune, Chennai and Gurgaon)\nJob Summary:\nWe're seeking a highly skilled NLP Engineer with expertise in Large Language Models (LLMs) and text summarization to join our team. The ideal candidate will have hands-on experience with Amazon Bedrock, OpenAI, or Hugging Face transformers and a strong background in Python programming. This role involves working with unstructured audio-to-text data, such as call transcripts, and developing innovative solutions using LLMs.\n\nRequirements:\nStrong expertise in NLP, text summarization, semantic search, and LLM APIs.\nPractical experience with Amazon Bedrock, OpenAI, or Hugging Face transformers.\nFamiliar with prompt tuning and few-shot learning.\nPython (pandas, langchain, boto3, NumPy, etc.)\nExperience working with unstructured audio-to-text data (e.g., call transcripts).\n\nKey Responsibilities:\nDesign and Development: Design, develop, and deploy LLM-based solutions for text summarization, semantic search, and other NLP tasks\nLLM APIs: Integrate LLM APIs from Amazon Bedrock, OpenAI, or Hugging Face transformers into existing applications\nPrompt Tuning and Few-Shot Learning: Implement prompt tuning and few-shot learning techniques to improve LLM performance\nUnstructured Audio-to-Text Data: Work with unstructured audio-to-text data, such as call transcripts, to develop accurate and efficient NLP models\nPython Programming: Utilize Python libraries like pandas, LangChain, boto3, and NumPy for data processing and model development.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Large Language Model', 'Natural Language Processing', 'Python']",2025-06-12 15:00:32
Principal Engineer,Wells Fargo,7 - 12 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Principal Engineer\n\nIn this role, you will:\nAct as an advisor to leadership to develop or influence applications, network, information security, database, operating systems, or web technologies for highly complex business and technical needs across multiple groups",,,,"['Underwriting', 'VSAM', 'JCL', 'Git', 'DB2', 'REXX', 'COBOL', 'SonarQube', 'debugging', 'IMS']",2025-06-12 15:00:35
Principal Engineer,Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 8+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 7+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 6+ years of Software Engineering or related work experience.\n\n4+ years of work experience with Programming Language such as C, C++, Java, Python, etc.\n\nAdditional\n\nPreferred requirements:\n15+ or more overall years of relevant experience in software design, including debugging, performance analysis.\nWorking knowledge of operating systems and hypervisors like Linux, QNX and other RTOSs\nSystem SW development experience including kernels, device drivers and BSP.\nUnderstanding of OS internals, storage, peripherals, and interfaces e.g., UFS/EMMC, PCIe, SPI/UART/I2C, USB, Ethernet etc.\nUnderstanding of secure and safe automotive SW architecture design and development involving safety subsystems and monitors,\nSystem level boot, power, performance, and latency optimizations.\nExposure to automotive SW development processes and standards (e.g., ASPCE, ISO26262 and ISO21434).\n\n\nPrincipal Duties and Responsibilities:\nThe idle candidate might have demonstrated ability to work with engineers/partners/customers across different geographies and contribute to large-scale SoC SW product development and customer support.\nHands-on technical lead/engineer who is not hesitant to dig into the details where needed to get first-hand knowledge of the issues and play an active role in steering team success.\nWork with management team on roadmap and strategy planning\nWorking with Automotive T1/OEMs and commercialization of Automotive HW/SW platforms is a plus.\n\nLeverages advanced Software knowledge and experience to design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs.\nDesign, develop, code, test software modules\nGather customer requirements, distill requirements to software architecture, create software architecture documents.\nAnalyzes user needs, software requirements, and time and cost constraints to design and customize software for optimal operational efficiency.\nDesigns and implements software modules for large-scale products and systems.\nParticipates in and leads design, coding, unit testing, debugging, and integration efforts to ensure projects are completed to specifications and schedules.\nPerforms complex code reviews and regression tests as well as triages and fixes issues to ensure the quality of code.\nCollaborates with individuals outside the software function (e.g., Hardware, Systems, and Test engineers) to ensure solutions work with other components of a specific project.\nWrites detailed technical documentation for complex Software projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'software design', 'linux', 'debugging', 'software engineering', 'usb', 'soc', 'unit testing', 'device drivers', 'hw', 'test engineering', 'java', 'computer science', 'product development', 'voip', 'sip', 'python', 'c', 'sw', 'spi', 'ethernet', 'cucm', 'uart', 'qnx', 'technical documentation', 'h323', 't1', 'bsp']",2025-06-12 15:00:37
Senior Data Scientist - AI/ML,Inumellas Consultancy Services,9 - 14 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Role - Senior Data Scientist / Senior Gen AI Engineer\nExp Range - 8 to 18 yrs\nPosition - Permanent Fulltime\nCompany - Data Analytics & AIML MNC\nLocation - Hyderabad, Pune, Bangalore (Relocation accepted)\nAbout the Role:\n\nWe are seeking a Software Engineer with expertise in Generative AI and Microsoft technologies to design, develop, and deploy AI-powered solutions using the Microsoft ecosystem. You will work with cross-functional teams to build scalable applications leveraging generative AI models and Azure services.\n\nSkills Required:\n\nExperience with Large Language Models (LLMs) like GPT, LLaMA, Claude, etc.\nProficiency in Python for building and fine-tuning AI/ML models\nFamiliarity with LangChain, LLMOps, or RAG (Retrieval-Augmented Generation) pipelines\nExperience with Vector Databases (e.g. FAISS, Pinecone, Weaviate)\nKnowledge of Prompt Engineering and model evaluation techniques\nExposure to cloud platforms (Azure, AWS or GCP) for deploying GenAI solutions\n\nPreferred Skills:\n\nExperience with Azure OpenAI, Databricks or Microsoft Fabric\nHands-on with Hugging Face Transformers, OpenAI APIs or custom model training",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Deep Learning', 'Prompt Engineering', 'Large Language Model', 'Vector Database', 'Retrieval Augmented Generation', 'GenAI', 'Langchain', 'Artificial Intelligence', 'LLMOps', 'LLaMa', 'GPT', 'Azure OpenAI', 'Machine Learning', 'ML Models', 'Model Evaluation', 'Huggingface', 'Aiml', 'OpenAI', 'Azure Machine Learning', 'Python']",2025-06-12 15:00:40
Principal Engineer - GCP,Wells Fargo,7 - 12 years,Not Disclosed,['Bengaluru'],"In this role, you will:\nAct as an advisor to leadership to develop or influence applications, network, information security, database, operating systems, or web technologies for highly complex business and technical needs across multiple groups\nLead the strategy and resolution of highly complex and unique challenges requiring in-depth evaluation across multiple areas or the enterprise, delivering solutions that are long-term, large-scale and require vision, creativity, innovation, advanced analytical and inductive thinking",,,,"['GCP', 'architecture roadmap', 'architects engineering', 'ETL Tools', 'enterprise principles', 'Google Cloud Platform', 'Informatica', 'Abinitio']",2025-06-12 15:00:43
Senior Data Scientist,Toast,6 - 11 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist - S&A\nNow, more than ever, the Toast team is committed to our customers. We re taking steps to help restaurants navigate these unprecedented times with technology, resources, and community. We focus on building the restaurant platform that helps restaurants adapt, take control, and get back to what they do best: building the businesses they love. And because our technology is purpose-built for restaurants, by restaurant people, restaurants can trust that we ll deliver on their needs for today while investing in experiences that will power their restaurant of the future.\nBready*\nto make a change?\n\nAs the Senior Data Scientist in our Bangalore Data Science team, you will contribute to building machine learning algorithms using our huge reservoir of point of sale transaction data. You will work with architects, engineers and product managers to solve business and customer problems and turn machine learning models into business impact across product lines, including financial processing and fraud.\nAbout this\nRoll*\n:\nDesign, build, train and evaluate machine learning models to drive business value for Toast and our restaurant customers\nCollaborate closely with internal and external product stakeholders, both technical and non-technical and help translate deep machine learning knowledge to product applications\nBreak down larger ML initiatives into smaller problems that enables data science to deliver incremental business value and lead the team to execute on them\nWork closely with Production Engineering and Data Platform teams to deploy models to production and regularly monitor for efficiency, key KPIs and enhance them as needed\nWork with incident response and problem resolution teams to check and resolve any problems/challenges as and when identified in the model deployed in the production\nEffectively document all steps and manage code repositories for easy scalability and knowledge sharing with the the team\nIncorporate up-to-date ML technology and DS approach as best practice for the team\nHelp in continuing to build out and expand the Data Science and ML Engineering teams\nWork effectively in a dynamic, changing environment while focusing on key goals and objectives\nDo you have the right\ningredients*\n?\nAdvanced degree in Data Science, Statistics, Applied Math, Computer Science, Engineering or other equivalent quantitative disciplines\n6 + years of industry experience in the field of Data Science and Machine Learning\nExperience in time series modelling. Familiarity with ARIMA, SARIMA, ETS, VAR models. Familiarity with forecasting tools like Facebook Prophet, GluonTS, or NeuralProphet.\nStrong proficiency in Python and SQL; experience with some of the following languages, tools, and frameworks: R, Spark, Scala, scikit-learn, Tensorflow, PyTorch, etc.\nFamiliarity with standard software engineering practices and tools including object-oriented programming, test-driven development, CI/CD, git, shell scripting, task orchestration (Airflow, Luigi, etc.) and preferably AWS tooling (Sagemaker, DynamoDB, ECS, etc.)\nStrong knowledge of underlying mathematical foundations of statistics and machine learning\nPrior success deploying machine learning solutions in large-scale production environments\nExperience collaborating with cross-functional teams and stakeholders to evaluate new Machine Learning opportunities\nProblem solver who loves to dig into different kinds of data and can communicate their findings to cross-functional stakeholders\nBonus\ningredients*\n:\nPassion for research and curiosity that calls you to go beyond good enough to create something innovative and exciting\nDiversity, Equity, and Inclusion is Baked into our Recipe for Success\nAt Toast, our employees are our secret ingredient when they thrive, we thrive. The restaurant industry is one of the most diverse, and we embrace that diversity with authenticity, inclusivity, respect, and humility. By embedding these principles into our culture and design, we create equitable opportunities for all and raise the bar in delivering exceptional experiences.\nWe Thrive Together\nWe embrace a hybrid work model that fosters in-person collaboration while valuing individual needs. Our goal is to build a strong culture of connection as we work together to empower the restaurant community. To learn more about how we work globally and regionally, check out: https: / / careers.toasttab.com / locations-toast .\nApply today!\nToast is committed to creating an accessible and inclusive hiring process. As part of this commitment, we strive to provide reasonable accommodations for persons with disabilities to enable them to access the hiring process. If you need an accommodation to access the job application or interview process, please contact .\n------\nFor roles in the United States, It is unlawful in Massachusetts to require or administer a lie detector test as a condition of employment or continued employment. An employer who violates this law shall be subject to criminal penalties and civil liability.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'orchestration', 'data science', 'Production engineering', 'Machine learning', 'Shell scripting', 'test driven development', 'Forecasting', 'SQL', 'Python']",2025-06-12 15:00:45
Principal Engineer - .Net Full Stack,Wells Fargo,7 - 9 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Principal Engineer\n\nIn this role, you will:\nAct as an advisor to leadership to develop or influence applications, network, information security, database, operating systems, or web technologies for highly complex business and technical needs across multiple groups",,,,"['.Net', 'Java', 'Gen AI', 'DevOps', 'CI/CD', 'micro services architecture', 'Full Stack', 'ETL', 'SDLC', 'Python']",2025-06-12 15:00:48
Senior Data Scientist,Hindustan Unilever (HUL),2 - 5 years,Not Disclosed,['Bengaluru'],"Job Title: Senior Data Scientist\nLocation: Bangalore\nJob Title: Assistant Manager - Security Engineering\nLocation: UniOps Bangalore\nABOUT UNILEVER:\nEvery individual here can bring their purpose to life through their work. Join us and you ll be surrounded by inspiring leaders and supportive peers. Among them, you ll channel your purpose, bring fresh ideas to the table, and simply be you. As you work to make a real impact on the business and the world, we ll work to help you become a better you.\nABOUT UNIOPS:\nUnilever Operations (UniOps) is the global technology and operations engine of Unilever offering business services, technology, and enterprise solutions. UniOps serves over 190 locations and through a network of specialized service lines and partners delivers insights and innovations, user experiences and end-to-end seamless delivery making Unilever Purpose Led and Future Fit\nBackground\nFor Unilever to remain competitive in the future, the business needs to continue on the path to become data intelligent. The Data Analytics team will persevere to make Unilever Data Intelligent, powering key decisions with data, insights, advanced analytics and AI. Our ambition is to enable democratization of data, information and insights as a completely agile organization that builds fantastic careers for our people and is accountable for delivering great work that maximizes impact and delivers growth.\nThis Data Analytics function endeavours to create clear accountability for all aspects of Data Strategy, Data Management, Information Management, Analytics, and Insights. We are accountable for impact of solutions, maintaining market relevance and minimising unnecessary overlaps in analytics products, ensuring simplicity and that our solutions better meet the needs of our users. We partner with the Digital and Data Legal Counsel to ensure that our Data Defence (Privacy, Governance, Quality, etc) is well structured and sufficiently robust to use data and AI correctly throughout the enterprise. We democratize information across the business, while supporting the culture shift required for data driven decision making.\nOur vision is to make Unilever data intelligent, partnering with the business to power key decisions with data, advanced analytics and AI to accelerate growth. Our 5 strategies to achieve this are:\nAccelerate simplify access to relevant data, information and insights Build in-house, leading-edge data, information, insights analytics capability Lead the data insights culture and careers to empower employees across Unilever Rapidly embed analytics products, solutions and services to drive growth Advance Information Automation at Scale\nThe Senior Data Scientist is an exciting role in the Data Foundation. This team builds state of the art machine learning algorithms, maximising the impact of analytic solutions in driving enterprise performance. Typical initiatives include optimizing trade promotion investments, accurately forecasting customer demand, using NLP to glean insight on consumer trends from search data, and making individual assortment recommendations for each of the millions of stores that sell Unilever products.\nMain Purpose of the Job:\nThe Senior Data Scientist improves business performance in the functional area of Unilever they serve, through the application of world class data science capability. They own delivery of data science on moderate projects or specific modules of a major global initiative.\nKey accountabilities:\nInteract with relevant teams to identify business challenges where data science can help\nApply comprehensive data science knowledge to propose optimal techniques for key business challenges\nCreate detailed data science proposals and project plans, flagging any limitations of proposed solution\nDesign and prototype experimental solutions, particularly machine learning models\nDesign scaled solutions and ensure high quality and timely delivery\nFacilitate industrialization and ongoing operation of solutions through well organised code, clear documentation and collaboration with ML Ops resources\nGovern the work of 3rd party vendors where needed to support delivery, while maximising creation of Unilever IP\nRepresent Data Science in cross-functional governance of projects, engaging with stakeholders up to Director level\nHighlight recent developments in data science capability which could solve additional challenges\nLead a team of up 1-2 data scientists / interns, providing career mentorship and line management\nProvide technical guidance to data scientists across DA, particularly on the projects you lead\nSupport the growth of DA s data science capability by contributing to activities such as tool and vendor selection, best practice definition, recruitment, and creation of training materials\nBuild the reputation of DA s data science capability within Unilever and externally, through activities such as community engagement (e. g. Yammer), publications or blogs\nProvide ad-hoc immediate support to the business when needed (for example Covid-19 crisis support)\nDepending on the specific project, the Senior Data Scientist can expect 60-90% of their work to be hands-on prototyping solutions, with the remainder spent planning and designing, overseeing and reviewing work of project staff, interfacing with stakeholders and managing team members.\nExperience and qualifications required:\nStandards of Leadership Required in This Role\nPersonal Mastery (Data-science and advanced analytics)\nAgility\nBusiness acumen\nPassion for High Performance\nKey Skills Required\nProfessional Skills\nMachine learning - Expert\nStatistical modelling - Expert\nForecasting - Expert\nOptimisation techniques and tools - Fully Operational\nPython coding - Fully Operational\nData science platform tools e. g. MS Azure, Databricks - Fully Operational\nDeep learning (and applications to NLP Computer Vision) - Fully Operational\nCollaborative development using Git repos - Fully Operational\nAutomated Machine Learning platforms - Foundational knowledge\nWhile a broad data science technical background is required, the role will benefit from deeper skills (for example graduate studies or prior work experience) in one of the following areas, optimization, simulation, forecasting, natural language processing, computer vision or geospatial analysis.\nGeneral Skills\nProject Management - Expert\nCommunication / presentation skills - Expert\n3rd party resource management - Expert\nCPG Industry analytics - Expert\nStrong communication and stakeholder engagement skills are essential, including the ability to influence peers and senior business stakeholders across Unilever.\nRelevant Experience:\nMinimum of B. E. in a relevant technical field (e. g. Computer Science, Engineering, Statistics, Operations Research); preferably a postgraduate (Masters or Doctorate) degree\nAt least 4 years building data science solutions to solve business problems, preferably in the CPG industry (less experience may be acceptable if balanced by strong post-grad qualifications)\nExperience with open source languages (eg. Python) and preferably with distributed computing (PySpark)\nExperience deploying solutions in a modern cloud-based architecture\nExperience managing the work of team members and 3rd party resource vendors\nExperience presenting insights and influencing decisions of senior non-technical stakeholders\nKey interfaces\nInternal\nUnilever operational, marketing, customer development, supply chain, product finance teams\nInternal DA teams (Engagement teams; Data CoE; Solution Factory; BDL Factory; Information Factory; Tech Transformation)\nWider Unilever analytics and data science professionals\nExternal\n3rd party Data Science vendors\nUniversities\nIndustry bodies",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Operations research', 'Automation', 'data science', 'Data management', 'Project management', 'Information management', 'Resource management', 'Forecasting', 'Recruitment']",2025-06-12 15:00:50
Senior Data Scientist with GCP,TVS Next,5 - 7 years,Not Disclosed,['Bengaluru'],"What you’ll do:\nUtilize advanced mathematical, statistical, and analytical expertise to research, collect, analyze, and interpret large datasets from internal and external sources to provide insight and develop data driven solutions across the company\nBuild and test predictive models including but not limited to credit risk, fraud, response, and offer acceptance propensity\nResponsible for the development, testing, validation, tracking, and performance enhancement of statistical models and other BI reporting tools leading to new innovative origination strategies within marketing, sales, finance, and underwriting",,,,"['analytical', 'scikit-learn', 'searching', 'bi', 'pyspark', 'numpy', 'sql', 'analytics', 'apache', 'automation', 'data science', 'spark', 'gcp', 'bigquery', 'data visualization', 'xgboost', 'programming', 'reporting', 'ml', 'advanced analytics', 'python', 'data processing', 'predictive', 'jupyter notebook', 'bert', 'pandas', 'matplotlib', 'statistics']",2025-06-12 15:00:53
Data Engineer - SAS Migration,Crisil,2 - 4 years,Not Disclosed,['Mumbai'],"The SAS to Databricks Migration Developer will be responsible for migrating existing SAS code, data processes, and workflows to the Databricks platform\n\nThis role requires expertise in both SAS and Databricks, with a focus on converting SAS logic into scalable PySpark and Python code\n\nThe developer will design, implement, and optimize data pipelines, ensuring seamless integration and functionality within the Databricks environment\n\nCollaboration with various teams is essential to understand data requirements and deliver solutions that meet business needs",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'scala', 'pyspark', 'data warehousing', 'data migration', 'azure data factory', 'sql', 'sql azure', 'java', 'spark', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'sas', 'microsoft azure', 'power bi', 'machine learning', 'sql server', 'data bricks', 'migration', 'sqoop', 'aws', 'ssis']",2025-06-12 15:00:55
Design Verification WLAN - Principal Engr / Mgr,Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Hardware Engineer, you will plan, design, optimize, verify, and test electronic systems, bring-up yield, circuits, mechanical systems, Digital/Analog/RF/optical systems, equipment and packaging, test systems, FPGA, and/or DSP systems that launch cutting-edge, world class products. Qualcomm Hardware Engineers collaborate with cross-functional teams to develop solutions and meet performance requirements.\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 8+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 7+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 6+ years of Hardware Engineering or related work experience.\n\n\nAs a design verification engineer you will work with a fast paced Integrated Wireless Technology (IEEE 802.11) team, with various wireless technologies embedded into an ARM based SOC infrastructure.\n\n\nYou will be responsible for developing HW blocks (IP design), conduct High/Mid/Low level Design review and delivery IP to Subsystem team for making complex SoCs.\n\nYou will be a critical part of the WLAN subsystem, contribute to IP design, sign-off the core to the SOC design team.\n\nSkills/Experience:\n\n- 6-15 years experience in Digital Design with a leading chipset company\n\n- Decent knowledge in Wireless connectivity technologiesIEEE 802.11 a/b/g/n/ac/ax/be\n\n- Knowledge in SoC architecture, including CPUs (preferably ARM), communications peripherals, multi-domain clocking, bus & interconnect structures, and power management\n\n- Strong fundamentals in one or few of these domain areas - Wireless and Mobile communications, Information theory, Coding theory, Signal processing\n\n- Strong knowledge on fixed-point implementation Truncation/Rounding/Saturation concepts\n\n- Strong knowledge on Digital communication engines viz., Demodulator, Deinterleaver, Viterbi/Turbo Decoders, Sigma-Delta modulation, Base band filters, FFT etc.",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['signal processing', 'digital design', 'mobile communication', 'digital communication', 'wireless', 'soc', 'gts', 'hardware engineering', 'system testing', 'packaging', 'design management', 'vhdl', 'verilog', 'electricals', 'rtl design', 'fpga', 'soc design', 'rtl coding', 'design review', 'arm']",2025-06-12 15:00:57
Data Scientist,Grid Dynamics,10 - 20 years,Not Disclosed,['Hyderabad'],"Role & responsibilitiMes\n\nCandiate needs to be 8+ Years of Experience\n\nDetails on tech stack\nPython\nPrompt engineering\nBest practices for prompt engineering\nHow LLM can be used in applications for a variety of tasks\nNLP\nUnderstanding of typical NLP problems: classification, NER, summarization, question answering, sentiment analysis, etc.\nTheoretical intuitive understanding of how Transformers work (tokenization, attention, etc).\nWord and sentence embeddings\nVector search\nVector databases, performance tuning\nDocument chunking techniques\nLLM applications development\nLangChain, LlamaIndex\nChain of Thoughts, DSP, and other techniques\nAgents and tools\nGoogle cloud (GCP)\nNice to have requirements to the candidate\nPreferable, the engineers are expected to have IT services/consulting experience.\nProficient in developing LLM-powered systems using advanced prompt engineering techniques, RAG and agentic design patterns. Experienced with frameworks like LangChain, LlamaIndex, and DSPy.\nFamiliar with evaluation approaches and metrics for different types of LLM-based systems.\nExperienced with keyword and vector search methods, including understanding of their underlying algorithms. Familiar with popular vector search engines.\nCompetent in various document understanding models and techniques to parse complex documents and implement effective chunking strategies for RAG systems.\nFamiliar with LLM and embedding models fine-tuning techniques.\nCompetent in using joint vision-language and generative models to solve various problems related to image generation, visual question answering, and multi-modal search. Familiar with diffusion models and associated techniques like LoRA, Dreambooth, and ControlNet.\nUnderstanding of the challenges and risks associated with the development of Generative AI systems and how to mitigate them.\nFamiliar with various architecture design patterns for different types of LLM-based applications such as chatbots, text2sql, document understanding, etc. Familiar with various approaches to scalability and cost reduction in Generative AI systems.\nAbility to stay updated with the latest advancements in Generative AI and integrate emerging technologies to drive innovation and improve the performance of AI systems.\nFamiliar with Responsible AI principles and Human-AI interaction design best practices.\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Lora', 'Natural Language Processing', 'Deep Learning', 'Python']",2025-06-12 15:00:59
Principal Engineer - Post Si Validation,Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Hardware Engineer, you will plan, design, optimize, verify, and test electronic systems, bring-up yield, circuits, mechanical systems, Digital/Analog/RF/optical systems, equipment and packaging, test systems, FPGA, and/or DSP systems that launch cutting-edge, world class products. Qualcomm Hardware Engineers collaborate with cross-functional teams to develop solutions and meet performance requirements.\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 8+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 7+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 6+ years of Hardware Engineering or related work experience.\n\nSilicon Validation Lead - Graphics Silicon Team, Bangalore\n\nThe Qualcomm Graphics Silicon Team in Bangalore is seeking a Silicon Validation Lead. Our power-efficient GPU solutions are fundamental to enabling exciting new markets such as Virtual Reality (VR), Internet of Things (IoT), Artificial Intelligence (AI), drones, and autonomous driving. We are looking for a talented Silicon Lead to deliver power-optimized, high-quality, high-performance graphics and computing solutions. The Graphics Silicon team in Bangalore is part of a global team responsible for developing and delivering GPU solutions that set industry benchmarks. Qualcomm boasts a strong portfolio of GPU cores, providing engineers with the opportunity to work with a world-class engineering team that leads the industry through innovation and disciplined execution.\n\nRoles and Responsibilities\n\nAs a GPU Silicon Validation Engineer, you will be part of the GPU Silicon Team and drive:\n\nThe new feature, use case enablement, and their validation.\n\nCollaboration with GPU design and verification teams to develop GPU bring-up and validation test plans.\n\nPreparation for GPU bring-up through pre-work on emulation and FPGA platforms.\n\nCoordination with SoC bring-up teams and software teams to plan GPU bring-up.\n\nTriage and debugging of failures on silicon.\n\nDevelopment of test contents and testing strategies to assist in the validation of GPU on silicon.\n\nWorking with GPU verification teams to reproduce silicon failures on emulators and FPGAs.\n\nCollaboration with the design team to suggest and architect new debug features to improve future GPU bring-ups.\n\nPower and performance characterization of GPU.\n\nPlanning and implementation of new efficiency improvement methodologies in GPU.\n\n\nQualifications\n\nThe ideal candidate should possess deep knowledge of scripting and software languages, including PERL/TCL, Linux/Unix shell, and C.\n\nMinimum Qualifications\n\nBachelor's or Masters degree in Electrical or Electronic Engineering from a reputed institution.\n\nOver 14 years of experience in silicon validation and bring-up.\n\n\nMinimum\n\nStrong understanding of microprocessor architecture.\n\nStrong understanding of power management.\n\nExperience in silicon bring-up and validation of GPU features.\n\nExperience in debugging functional, power, performance, and/or physical design issues in silicon.\n\nExperience in GPU silicon validation and debug basics.\n\nExperience in test development for validation of GPU features on silicon.\n\nExperience in developing test vectors for tester bring-up.\n\nImplementation of assembly, C, and Python language programming.\n\nExperience with HW tools like JTAG, Kratos, LA, Emulation platforms, DMM, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['python', 'hardware engineering', 'silicon validation', 'power management', 'perl', 'jtag', 'c', 'soc', 'system testing', 'spi', 'artificial intelligence', 'emulators', 'hw', 'silicon', 'fpga', 'fpga platforms', 'test development', 'linux', 'debugging', 'shell scripting', 'tcl', 'electronics engineering', 'fpgas']",2025-06-12 15:01:02
CPU Micro-architect/RTL Designer -Principal Engineer,Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nGeneral Summary Qualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in. We are hiring talented engineers for CPU RTL development targeted for high performance, low power devices. As a CPU Micro-architecture and RTL Design Engineer, you will work with chip architects to conceive of the micro-architecture, and also help with architecture/product definition through early involvement in the product life-cycle.\n\nRoles And Responsibilities\n\nPerformance exploration. Explore high performance strategies working with the CPU modeling team.\n\nMicroarchitecture development and specification. From early high-level architectural exploration, through micro architectural research and arriving at a detailed specification.\n\nRTL ownership. Development, assessment and refinement of RTL design to target power, performance, area and timing goals.\n\nFunctional verification support. Help the design verification team execute on the functional verification strategy.\n\nPerformance verification support. Help verify that the RTL design meets the performance goals.\n\nDesign delivery. Work with multi-functional engineering team to implement and validate physical design on the aspects of timing, area, reliability, testability and po\n\n\nPreferred Qualifications\n\nThorough knowledge of microprocessor architecture including expertise in one or more of the following areasinstruction fetch and decode, branch prediction, instruction scheduling and register renaming, out-of-order execution, integer and floating point execution, load/store execution, prefetching, cache and memory subsystems\n\nKnowledge of Verilog and/or VHDL. Experience with simulators and waveform debugging tools\n\nKnowledge of logic design principles along with timing and power implications\n\nUnderstanding of low power microarchitecture techniques\n\nUnderstanding of high performance techniques and trade-offs in a CPU microarchitecture\n\nExperience using a scripting language such as Perl or Python\n\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 8+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 7+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 6+ years of Hardware Engineering or related work experience.\n\nPreferred Qualifications:\n\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field.\n\n15+ years of Hardware Engineering or related work experience.\n4+ years of experience with circuit/logic design/validation (e.g., digital, analog, RF).\n\n4+ years of experience utilizing schematic capture and circuit stimulation software.\n\n4+ years of experience with hardware design and measurement instruments such as oscilloscopes, spectrum analyzers, RF tools, etc.\n\n4+ years in a technical leadership role with or without direct reports.\n\nPrincipal Duties and Responsibilities:\n\nLeverages expert Hardware knowledge and experience to plan, optimize, verify, and test highly critical electronic systems, bring-up yield, circuits, mechanical systems, Digital/Analog/RF/optical systems, equipment and packaging, test systems, FPGA, and/or DSP systems.\n\nDrives the development of design rules and processes for electronic hardware, equipment, and/or integrated circuitry.\n\nServes as an expert resource for conducting highly complex simulations and analyses of designs as well as for the implementation of designs with the best power, performance, and area.\n\nCollaborates with high-level representatives across functions (e.g., design, verification, validation, software and systems engineering, architecture development teams, etc.) to implement and drive new requirements and the latest test solutions in the production program to improve the yield, test time, and quality.\n\nEvaluates, characterizes, and develops the novel manufacturing solutions for leading edge products in highly advanced processes and bring-up product to meet customer expectations and schedules.\n\nServes as an expert resource for the evaluation of reliability for highly critical materials, properties, and techniques and brings innovation, automation, and optimization to maximize productivity.\n\nAdvises multiple teams of engineers in the development of complex hardware designs, evaluating various design features to identify potential flaws or issues.\n\nWrites detailed technical documentation for highly complex Hardware projects; reviews technical documentation for experienced engineers.\n\nLevel of Responsibility:\n\nProvides supervision to direct reports.\n\nDecision-making is critical in nature and highly impacts program, product, or project success.\n\nRequires verbal and written communication skills to convey highly complex and/or detailed information. May require strong negotiation and influence with large groups or high-level constituents.\n\nWorks within the prescribed budgetary objectives of the department.\n\nHas a great degree of influence over key organizational decisions.\n\nTasks often require multiple steps which can be performed in various orders; extensive planning, problem-solving, and prioritization must occur to complete the tasks effectively.",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['circuit', 'hardware engineering', 'order execution', 'hardware design', 'logic design', 'python', 'physical design', 'data validation', 'simulation', 'design verification', 'rtl', 'vhdl', 'uvm', 'verilog', 'schematic capture', 'rtl design', 'instruments', 'axi', 'fpga', 'debugging', 'perl', 'system verilog']",2025-06-12 15:01:04
"Associate Scientist, Data Sourcing & Solutions",XL India Business Services Pvt. Ltd,1 - 5 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Associate Scientist - Data Sourcing & Solutions Gurgaon/Bangalore, India AXA XL recognises data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XLs executive leadership team to maximise benefits and facilitate sustained enterprise advantage\n\nOur Innovation, Data, and Analytics Office (IDA) is focused on driving innovation by optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward a greater focus on the use of data and data-driven insights, we are seeking an Associate Scientist for our Data Sourcing & Solutions team\n\nThe role sits across the IDA Department to make sure customer requirements are properly captured and transformed into actionable data specifications\n\nSuccess in the role will require a focus on proactive management of the sourcing and management of data from source through usage\n\nWhat you ll be DOING What will your essential responsibilities include? Accountable for documenting data requirements (Business and Function Requirements) and assessing the reusability of Axiom assets\n\nBuild processes to simplify and expedite data sourcing to focus on delivering data to AXA XL business stakeholders frequently\n\nDevelops and operationalizes strategic data products, and answers and proactively manages the sourcing and management of data from source through usage (reusable Policy and Claim Domain data assets)\n\nData Validation Testing of the data products in partnership with the AXA XL business to ensure the accuracy of the data and validation of the requirements\n\nAssesses all data required as part of the Data Ecosystem to make sure data has a single version of the truth\n\nRespond to ad-hoc data requests to support AXA XLs business\n\nInstill a customer-first attitude, prioritizing service for our business stakeholders above all else\n\nInternalize and execute IDA and company-wide goals to become a data-driven organization\n\nContribute to best practices and standards to make sure there is a consistent and efficient approach to capturing business requirements and translating them into functional, non-functional, and semantic specifications\n\nDevelop a comprehensive understanding of the data and our customers\n\nDrive root cause analysis for identified data deficiencies within reusable data assets delivered via IDA\n\nIdentify solution options to improve the consistency, accuracy, and quality of data when captured at its source\n\nYou will report to the Team Lead - Data Sourcing & Solutions\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Experience in a data role (business analyst, data analyst, analytics) preferably in the Insurance industry and within a data division\n\nA minimum of a bachelor s or masters degree (preferred) in a relevant discipline\n\nRobust SQL knowledge and technical ability to query AXA XL data sources to understand our data\n\nExcellent presentation, communication (oral & written), and relationship-building skills, across all levels of management and customer interaction\n\nInsurance experience in data, underwriting, claims, and/or operations, including influencing, collaborating, and leading efforts in complex, disparate, and interrelated teams with competing priorities\n\nPassion for data and experience working within a data-driven organization\n\nWork together internal data with external industry data to deliver holistic answers\n\nWork with unstructured data to unlock information needed by the business to create unique products for the insurance industry\n\nPossesses robust exploratory analysis skills and high intellectual curiosity\n\nDisplays exceptional organizational skills and is detail-oriented\n\nThe robust conceptual thinker who connects dots, and has critical thinking, and analytical skills\n\nDesired Skills and Abilities: Ability to work with team members across the globe and departments\n\nAbility to take ownership, work under pressure, and meet deadlines\n\nBuilds trust and rapport within and across groups\n\nApplies in-depth knowledge of business and specialized areas to solve business problems and understand integration challenges and long-term impact creatively and strategically\n\nAbility to manage data needs of an individual project(s) while being able to understand the broader enterprise data perspective\n\nExpected to recommend innovation and improvement to policies, and procedures, deploying resources, and performing core activities\n\nExperience with SQL Server, Azure Databricks Notebook, Qlikview, PowerBI, and Jira/Confluence a plus",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data validation', 'Claims', 'Underwriting', 'Agile', 'QlikView', 'Business strategy', 'JIRA', 'Analytics', 'SQL', 'Customer interaction']",2025-06-12 15:01:07
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-12 15:01:09
Data Scientist,Xoom,2 - 4 years,Not Disclosed,['Bengaluru'],"Job Summary\nWhat you need to know about the role\n\nEach Data Scientist on this team has full ownership of a portfolio of a product and is responsible for end-to-end management of loss and decline rates. Day-to-day duties include data analysis, monitoring and forecasting, creating the logic for and implementing risk rules and strategies, providing requirements to data scientists and technology teams on attribute, model and platform requirements, and communicating with global stakeholders to ensure we deliver the best possible customer experience while meeting loss rate targets.\n\nMeet our team\n\nPayPals Global Fraud Protection team is responsible for partnering with global business units to manage a variety of risk of various types, including identity fraud, account takeover, stolen financial fraud, and credit issues. This is an exciting department that plays an important role in contributing PayPals bottom line financial savings, ensuring safe and secure global business growth, and delivering the best customer experience.\n\nThis open opportunity is within the Large Merchant and Markets Fraud Risk team. This portfolio is comprised of PayPal s newest leading-edge payments solutions, such as Risk-as-Service, Fastlane, PayPal Complete Payments, etc. as well as customized experiences developed for the company s highest-priority strategic Markets and Partnerships.\nJob Description\nYour way to impact\nYou will be the Data Scientist in the Fraud Risk team , where you will work on leading new projects to build and improve the Risk strategies to prevent fraud using the Risk tooled and custom data & AL/ML models. In this position, you will be partnering with the corresponding Business Units to align with and influence their strategic priorities, educate business partners about Risk management principles, and collaboratively optimize the Risk treatments and experiences for these unique products and partners.\nYour day to day\nIn your day to day role you will -\nIn this role you will have full ownership of a portfolio of merchants and is responsible for end-to-end management of loss and decline rates.\nCollaborate with different teams to develop strategies for fraud prevention, loss savings, and optimize transaction declines or improve customer friction.\nYou will work together with cross-functional teams to deliver solutions and providing Risk analytics on frustration trend/ KPIs monitoring or alerting for fraud events.\nThese solutions will adapt PayPal s advanced proprietary fraud prevention tools enabling business growth.\nWhat do you need to bring-\n2-4 years of relevant experience working with large-scale complex dataset.\nStrong analytical mindset, ability to decompose business requirements into an analytical plan, and execute the plan to answer those business questions\nExcellent communication skills, equally adept at working with engineers as well as business leaders\nWant to build new solutions and invent new approaches to big, ambiguous, critical problems\nStrong working knowledge of Excel, SQL and Python/R\nTechnical Proficiency Exploratory Data Analysis and expertise in preparing a clean and structured data for model development. Experience in applying AI/ML techniques for business decisioning including supervised and unsupervised learning (e.g., regression, classification, clustering, decision trees, anomaly detection, etc.). Knowledge of model evaluation techniques such as Precision, Recall, ROC-AUC Curve, etc. along with basic statistical concepts.\nPreferred Qualification\nSubsidiary\nPayPal\nTravel Percent\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https//www.paypalbenefits.com .\nWho We Are\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. .\nBelonging at PayPal\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Risk analytics', 'Analytical', 'Diversity and Inclusion', 'ROC', 'Wellness', 'Risk management', 'Forecasting', 'Monitoring', 'SQL']",2025-06-12 15:01:11
Data ML Program Manager - Product Operations,Apple,3 - 8 years,Not Disclosed,['Bengaluru'],"Apple is where individual imaginations gather together, committing to the values that lead to great work\nEvery new product we build, service we create, or Apple Store experience we deliver is the result of us making each other s ideas stronger\nThat happens because every one of us shares a belief that we can make something wonderful and share it with the world, changing lives for the better\nIt s the diversity of our people and their thinking that inspires the innovation that runs through everything we do\nWhen we bring everybody in, we can do the best work of our lives\nHere, you ll do more than join something you ll add something\nJoin Apple, and help us leave the world better than we found it\nProduct Operations group is looking to add a Data and Machine Learning Program Manager to support data systems, automation, machine learning, tools that enhance manufacturing of Apples products\nThis team is made up of creative innovators and problem solvers who tackle unique challenges to ideate, create POCs and deliver new solutions in the data space\nIt takes deeply dedicated, intelligent and hard-working individuals to maintain and exceed the high expectations for the exciting products at Apple\nThe Product Operations Data Team is looking for an extraordinary Program manager to join our team\nYou will craft, design and implement our machine learning strategy to the massive supply chain and help build the future of our manufacturing systems\nDo you love using your creative left brain and structured, tactical right brain to drive projects to build systems, innovate with ML/analytics, and find operations efficiencies\nDescription\nIn this role you will be responsible for planning and managing deployment of various business solutions for the Product Operations organization\nWe lead and participate in efforts for process standardization, ideation and development of tools, and implementation through successful completion\n- Lead the ideation, prototyping, and business justification/validation of solutions for Product Ops- Work closely with engineering teams on technical development and deployment of high-quality solutions- Lead the definition and execution of scalable solutions to support core internal customer needs and to improve process efficiencies- Process re-engineering or build out enhancements- Identify inefficient process and help enable better decisions leveraging data, AI, Machine Learning or all 3 together- Prototype solutions before providing recommendations on directions\ne\ng\ncreate dashboard mock-ups before investing in development effort- Program management of machine learning initiatives - scoping, prioritization, resourcing, and implementation- Comfortable and flexible in serving both supporting and leadership roles for various projects\n3+ years of project management experience in highly technical field\nBachelors or Masters in Computer Science, Engineering or similar field and hands on experience in technical roles\nPreferred Qualifications\nKnowledge of emerging concepts like machine learning and predictive analytics (Deep Learning, Python, R, Neural Networks, applied Data Science) a strong plus\nStrong Excel skills, including pivots, vlookups, conditional formatting, large record sets\nFamiliarity with databases, large data sets, reporting Tableau and SQL a plus\nDomestic and international travel up to 25% and work hour flexibility required in this dynamic, global position\nAble to work independently when managing multiple priorities in an unstructured, global and virtual environment is essential\nHighly desirable to have high tech manufacturing, Supply Chain Management, NPI or Manufacturing Operations related education / experience\nMBA is considered a plus",Industry Type: Consumer Electronics & Appliances,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Supply chain management', 'Prototype', 'Project management', 'Machine learning', 'Resourcing', 'Business solutions', 'SQL', 'Python']",2025-06-12 15:01:13
Data ML Program Manager - Product Operations,Apple,7 - 12 years,Not Disclosed,['Bengaluru'],"In this role you will be responsible for planning and managing deployment of various business solutions for the Product Operations organization\nWe lead and participate in efforts for process standardization, ideation and development of tools, and implementation through successful completion\n- Lead the ideation, prototyping, and business justification/validation of solutions for Product Ops- Work closely with engineering teams on technical development and deployment of high-quality solutions- Lead the definition and execution of scalable solutions to support core internal customer needs and to improve process efficiencies- Process re-engineering or build out enhancements- Identify inefficient process and help enable better decisions leveraging data, AI, Machine Learning or all 3 together- Prototype solutions before providing recommendations on directions\ne\ng\ncreate dashboard mock-ups before investing in development effort- Program management of machine learning initiatives - scoping, prioritization, resourcing, and implementation- Comfortable and flexible in serving both supporting and leadership roles for various projects\n7+ years of project management experience in highly technical field\nBachelors or Masters in Computer Science, Engineering, or similar field or hands on experience in technical roles\nPreferred Qualifications\nKnowledge of emerging concepts like machine learning and predictive analytics (Deep Learning, Python, R, Neural Networks, applied Data Science) a strong plus\nStrong Excel skills, including pivots, vlookups, conditional formatting, large record sets\nFamiliarity with databases, large data sets, reporting Tableau and SQL a plus\nDomestic and international travel up to 25% and work hour flexibility required in this dynamic, global position\nAble to work independently when managing multiple priorities in an unstructured, global and virtual environment is essential\nHighly desirable to have high tech manufacturing, Supply Chain Management, NPI or Manufacturing Operations related education / experience\nMBA is considered a plus",Industry Type: Consumer Electronics & Appliances,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Supply chain management', 'Prototype', 'Project management', 'Machine learning', 'Resourcing', 'Business solutions', 'SQL', 'Python']",2025-06-12 15:01:16
Lead Data Scientist,Bizopp Management Consultants,11 - 18 years,25-35 Lacs P.A.,['Chennai'],"• Proficiency in Python and SQL for data extraction, manipulation, and analysis\n\n• Exploratory data analysis (EDA), developing, and deploying machine learning models\n\n• Expertise in deploying ML models on cloud platforms such as AWS or Azure",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['EDA', 'Data Scientist', 'Python', 'SQL', 'Azure', 'Exploratory data analysis', 'Machine Learning', 'AWS', 'ML']",2025-06-12 15:01:18
Lead Data Scientist,Trion Consultancy Services,10 - 18 years,20-35 Lacs P.A.,['Chennai'],"LD Scientist with 12 yrs of industry exp, including at least 5 yrs of hands-on exp in data science & a proven track record of delivering impactful data science solutions.\nData Analysis &Exploration\nTime Series Analysis\nModel Deployment & Integration\n\nRequired Candidate profile\n12+ yrs/including 5+ yrs in data science\nExp in Python and SQL for data extraction, manipulation & analysis\nDS & Model Development: Demonstrated exp in performing exploratory data analysis (EDA)",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Lead Data science', 'Machine Learning', 'Python']",2025-06-12 15:01:20
Data Engineering Lead,Yotta Techports,10 - 15 years,30-35 Lacs P.A.,['Hyderabad'],"Responsibilities:\nLead and manage an offshore team of data engineers, providing strategic guidance, mentorship, and support to ensure the successful delivery of projects and the development of team members.\nCollaborate closely with onshore stakeholders to understand project requirements, allocate resources efficiently, and ensure alignment with client expectations and project timelines.\nDrive the technical design, implementation, and optimization of data pipelines, ETL processes, and data warehouses, ensuring scalability, performance, and reliability.\nDefine and enforce engineering best practices, coding standards, and data quality standards to maintain high-quality deliverables and mitigate project risks.\nStay abreast of emerging technologies and industry trends in data engineering, and provide recommendations for tooling, process improvements, and skill development.\nAssume a data architect role as needed, leading the design and implementation of data architecture solutions, data modeling, and optimization strategies.\nDemonstrate proficiency in AWS services such as:\nExpertise in cloud data services, including AWS services like Amazon Redshift, Amazon EMR, and AWS Glue, to design and implement scalable data solutions.\nExperience with cloud infrastructure services such as AWS EC2, AWS S3, to optimize data processing and storage.\nKnowledge of cloud security best practices, IAM roles, and encryption mechanisms to ensure data privacy and compliance.\nProficiency in managing or implementing cloud data warehouse solutions, including data modeling, schema design, performance tuning, and optimization techniques.\nDemonstrate proficiency in modern data platforms such as Snowflake and Databricks, including:\nDeep understanding of Snowflake's architecture, capabilities, and best practices for designing and implementing data warehouse solutions.\nHands-on experience with Databricks for data engineering, data processing, and machine learning tasks, leveraging Spark clusters for scalable data processing.\nAbility to optimize Snowflake and Databricks configurations for performance, scalability, and cost-effectiveness.\nManage the offshore team's performance, including resource allocation, performance evaluations, and professional development, to maximize team productivity and morale.\n\nQualifications:\nBachelor's degree in Computer Science, Engineering, or a related field; advanced degree preferred.\n10+ years of experience in data engineering, with a proven track record of leadership and technical expertise in managing complex data projects.\nProficiency in programming languages such as Python, Java, or Scala, as well as expertise in SQL and relational databases (e.g., PostgreSQL, MySQL).\nStrong understanding of distributed computing, cloud technologies (e.g., AWS), and big data frameworks (e.g., Hadoop, Spark).\nExperience with data architecture design, data modeling, and optimization techniques.\nExcellent communication, collaboration, and leadership skills, with the ability to effectively manage remote teams and engage with onshore stakeholders.\nProven ability to adapt to evolving project requirements and effectively prioritize tasks in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team Handling', 'Snowflake', 'Data Services', 'Cloud Infrastructure', 'Data Bricks']",2025-06-12 15:01:22
Senior Manager-Decision Scientist,Tesco Plc,3 - 8 years,Not Disclosed,['Bengaluru'],"Senior Manager-Decision Scientist\nBack to job search results\nTesco India Bengaluru, Karnataka, India Hybrid Full-Time Permanent Apply by 19-Jun-2025\nAbout the role\nPlease refer to you are Responsible for :-\nWhat is in it for you\nAt Tesco, we are committed to providing the best for you.\nAs a result, our colleagues enjoy a unique, differentiated, market- competitive reward package, based on the current industry practices, for all the work they put into serving our customers, communities and planet a little better every day.\nOur Tesco Rewards framework consists of pillars - Fixed Pay, Incentives, and Benefits.\nTotal Rewards offered at Tesco is determined by four principles - simple, fair, competitive, and sustainable.\nSalary - Your fixed pay is the guaranteed pay as per your contract of employment.\nPerformance Bonus - Opportunity to earn additional compensation bonus based on performance, paid annually\nLeave & Time-off - Colleagues are entitled to 30 days of leave (18 days of Earned Leave, 12 days of Casual/Sick Leave) and 10 national and festival holidays, as per the company s policy.\nMaking Retirement Tension-FreeSalary - In addition to Statutory retirement beneets, Tesco enables colleagues to participate in voluntary programmes like NPS and VPF.\nHealth is Wealth - Tesco promotes programmes that support a culture of health and wellness including insurance for colleagues and their family. Our medical insurance provides coverage for dependents including parents or in-laws.\nMental Wellbeing - We offer mental health support through self-help tools, community groups, ally networks, face-to-face counselling, and more for both colleagues and dependents.\nFinancial Wellbeing - Through our financial literacy partner, we offer one-to-one financial coaching at discounted rates, as well as salary advances on earned wages upon request.\nSave As You Earn (SAYE) - Our SAYE programme allows colleagues to transition from being employees to Tesco shareholders through a structured 3-year savings plan.\nPhysical Wellbeing - Our green campus promotes physical wellbeing with facilities that include a cricket pitch, football field, badminton and volleyball courts, along with indoor games, encouraging a healthier lifestyle.\nYou will be responsible for\nDeveloping and leading a high performing team, creating an environment for success by setting direction and coaching them to succeed through inspiring conversations every day. (Refer to the expectations of a manager at Tesco- the minimum standards)\n- Promoting a culture of CI within their teams to drive operational improvements\n- Accountable for achieving teams objectives, stakeholder management and escalation management\n- Provides inputs that impact the functions plans, policies, influences the budget and resources in their scope.\nAccountable to EA and market leaderships for building the analytics road-map and improve analytical maturity of partnering functions with in depth understanding of key priorities & outcome.\n- Accountable to shape & own the analytics workplan, proactively spot size able opportunities and deliver programs successfully that will result in disproportionate returns\n- Thought leadership in scoping the business problems, solutions and bringing disruptive / depth oriented solutions to complex problems and institutionalize robust ways of working with business partners\n- Partner with TBS and markets finance team to measure the value delivered through analytics initiatives\n- Build impact driven teams by creating an environment for success by setting direction, objectives and mentor managers, and guide teams to craft analytical assets which will deliver value in sustainable manner\n- Be the voice and represent Enterprise Analytics on internal and external forums\n- Provides inputs that impact the functions plans, policies, influences the budget and resources in their scope\n- Developing managers and colleagues to succeed through inspiring conversations every day\nYou will need\nUnderstanding of machine learning techniques, Linear & Logistics regression, Decision Trees, Random Forest, XGBoost and Neural Network\n- Knowledge of Python, SQL, Hive and Visualization tools (e.g. Tableau )\n- Retail Expertise, Partnership management, Analytics\n- Conceptual application to larger business context, Storyboarding, Managing managers\nAbout us\nTesco in Bengaluru is a multi-disciplinary team serving our customers, communities, and planet a little better every day across markets. Our goal is to create a sustainable competitive advantage for Tesco by standardising processes, delivering cost savings, enabling agility through technological solutions, and empowering our colleagues to do even more for our customers. With cross-functional expertise, a wide network of teams, and strong governance, we reduce complexity, thereby offering high-quality services for our customers.\nTesco in Bengaluru, established in 2004 to enable standardisation and build centralised capabilities and competencies, makes the experience better for our millions of customers worldwide and simpler for over 3,30,000 colleagues.\nTesco Business Solutions:\nEstablished in 2017, Tesco Business Solutions (TBS) has evolved from a single entity traditional shared services in Bengaluru, India (from 2004) to a global, purpose-driven solutions-focused organisation. TBS is committed to driving scale at speed and delivering value to the Tesco Group through the power of decision science. With over 4,400 highly skilled colleagues globally, TBS supports markets and business units across four locations in the UK, India, Hungary, and the Republic of Ireland. The organisation underpins everything that the Tesco Group does, bringing innovation, a solutions mindset, and agility to its operations and support functions, building winning partnerships across the business. TBSs focus is on adding value and creating impactful outcomes that shape the future of the business. TBS creates a sustainable competitive advantage for the Tesco Group by becoming the partner of choice for talent, transformation, and value creation.\nApply",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analytical', 'Machine learning', 'Medical insurance', 'Business solutions', 'Stakeholder management', 'Operations', 'Analytics', 'SQL', 'Python', 'Logistics']",2025-06-12 15:01:25
MLOps Engineer,Affine Analytics,4 - 8 years,Not Disclosed,['Bengaluru'],"Machine Learning & Data Pipelines\nStrong understanding of Machine Learning principles, lifecycle, and deployment practices\nExperience in designing and building ML pipelines\nKnowledge of deploying ML models on AWS Lambda, EKS, or other relevant services\nWorking knowledge of Apache Airflow for orchestration of data workflows\nProficiency in Python for scripting, automation, and ML model development with Data Scientists",,,,"['Machine Learning', 'S3', 'AWS Lambda', 'MLOps', 'SQS', 'AWS Glue', 'SNS', 'EKS', 'Lambda', 'Python']",2025-06-12 15:01:27
Data Engineer - Databricks,Inorg,2 - 5 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']",InOrg Global is looking for Data Engineer - Databricks to join our dynamic team and embark on a rewarding career journey.\n\nLiaising with coworkers and clients to elucidate the requirements for each task.\nConceptualizing and generating infrastructure that allows big data to be accessed and analyzed.\nReformulating existing frameworks to optimize their functioning.\nTesting such structures to ensure that they are fit for use.\nPreparing raw data for manipulation by data scientists.\nDetecting and correcting errors in your work.\nEnsuring that your work remains backed up and readily accessible to relevant coworkers.\nRemaining up - to - date with industry standards and technological advancements that will improve the quality of your outputs.,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent",['Data Engineer - Databricks'],2025-06-12 15:01:30
Data Engineer - Databricks,KPI Partners,3 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About KPI Partners.\nKPI Partners is a leading provider of data analytics solutions, dedicated to helping organizations transform data into actionable insights. Our innovative approach combines advanced technology with expert consulting, allowing businesses to leverage their data for improved performance and decision-making.\n\nJob Description.\nWe are seeking a skilled and motivated Data Engineer with experience in Databricks to join our dynamic team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and data processing solutions that support our analytics initiatives. You will collaborate closely with data scientists, analysts, and other engineers to ensure the consistent flow of high-quality data across our platforms.",,,,"['python', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'data pipeline', 'data architecture', 'data engineering', 'sql', 'data bricks', 'cloud', 'analytics', 'data quality', 'data modeling', 'gcp', 'teamwork', 'integration', 'aws', 'etl', 'programming', 'communication skills', 'etl scripts']",2025-06-12 15:01:33
Data Engineer,Aqilea Softech,5 - 9 years,13-20 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Job Title: Data Engineer\nCompany : Aqilea India(Client : H&M India)\nEmployment Type: Full Time\nLocation: Bangalore(Hybrid)\nExperience: 4.5 to 9 years\nClient : H&M India\n\nAt H&M, we welcome you to be yourself and feel like you truly belong. Help us reimagine the future of an entire industry by making everyone look, feel, and do good. We take pride in our history of making fashion accessible to everyone and led by our values we strive to build a more welcoming, inclusive, and sustainable industry. We are privileged to have more than 120,000 colleagues, in over 75 countries across the world. Thats 120 000 individuals with unique experiences, skills, and passions. At H&M, we believe everyone can make an impact, we believe in giving people responsibility and a strong sense of ownership. Our business is your business, and when you grow, we grow.\nWebsite : https://career.hm.com/\n\nWe are seeking a skilled and forward-thinking Data Engineer to join our Emerging Tech team. This role is designed for someone passionate about working with cutting-edge technologies such as AI, machine learning, IoT, and big data to turn complex data sets into actionable insights.\nAs the Data Engineer in Emerging Tech, you will be responsible for designing, implementing, and optimizing data architectures and processes that support the integration of next-generation technologies. Your role will involve working with large-scale datasets, building predictive models, and utilizing emerging tools to enable data-driven decision-making across the business. You ll collaborate with technical and business teams to uncover insights, streamline data pipelines, and ensure the best use of advanced analytics technologies.\n\nKey Responsibilities:\nDesign and build scalable data architectures and pipelines that support machine learning, analytics, and IoT initiatives.\nDevelop and optimize data models and algorithms to process and analyse large-scale, complex data sets.\nImplement data governance, security, and compliance measures to ensure high-quality\nCollaborate with cross-functional teams (engineering, product, and business) to translate business requirements into data-driven solutions.\nEvaluate, integrate, and optimize new data technologies to enhance analytics capabilities and drive business outcomes.\nApply statistical methods, machine learning models, and data visualization techniques to deliver actionable insights.\nEstablish best practices for data management, including data quality, consistency, and scalability.\nConduct analysis to identify trends, patterns, and correlations within data to support strategic business initiatives.\nStay updated on the latest trends and innovations in data technologies and emerging data management practices.\n\nSkills Required :\nBachelors or masters degree in data science, Computer Science, Engineering, Statistics, or a related field.\n4.5-9 years of experience in data engineering, data science, or a similar analytical role, with a focus on emerging technologies.\nProficiency with big data frameworks (e.g., Hadoop, Spark, Kafka) and experience with modern cloud platforms (AWS, Azure, or GCP).\nSolid skills in Python, SQL, and optionally R, along with experience using machine learning libraries such as Scikit-learn, TensorFlow, or PyTorch.\nExperience with data visualization tools (e.g., Tableau or Power BI or D3.js) to communicate insights effectively.\nFamiliarity with IoT and edge computing data architectures is a plus.\nUnderstanding of data governance, compliance, and privacy standards.\nAbility to work with both structured and unstructured data.\nExcellent problem-solving, communication, and collaboration skills, with the ability to work in a fast-paced, cross-functional team environment.\nA passion for emerging technologies and a continuous desire to learn and innovate.\nInterested Candidates can share your Resumes to mail id karthik.prakadish@aqilea.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Powerbi', 'Hadoop', 'Kafka', 'Tableau', 'Azure', 'GCP', 'Data Engineer', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-12 15:01:35
Data Engineer,Talent Aspire,2 - 7 years,Not Disclosed,"['Chandigarh', 'Bengaluru']","As the Data Engineer, you will play a pivotal role in shaping our data infrastructure and\nexecuting against our strategy. You will ideate alongside engineering, data and our clients to\ndeploy data products with an innovative and meaningful impact to clients. You will design, build, and maintain scalable data pipelines and workflows on AWS. Additionally, your expertise in AI and machine learning will enhance our ability to deliver smarter, more predictive solutions.\n\nKey Responsibilities\nCollaborate with other engineers, customers to brainstorm and develop impactful data\nproducts tailored to our clients.\nLeverage AI and machine learning techniques to integrate intelligent features into our\nofferings.\nDevelop, and optimize end-to-end data pipelines on AWS\nFollow best practices in software architecture and development.\nImplement effective cost management and performance optimization strategies.\nDevelop and maintain systems using Python, SQL, PySpark, and Django for front-end\ndevelopment.\nWork directly with clients and end-users and address their data needs\nUtilize databases and tools including and not limited to, Postgres, Redshift, Airflow, and\nMongoDB to support our data ecosystem.\nLeverage AI frameworks and libraries to integrate advanced analytics into our solutions.\nQualifications\n\nExperience:\nMinimum of 3 years of experience in data engineering, software development, or\nrelated roles.\nProven track record in designing and deploying AWS cloud infrastructure\nsolutions\nAt least 2 years in data analysis and mining techniques to aid in descriptive and\ndiagnostic insights\nExtensive hands-on experience with Postgres, Redshift, Airflow, MongoDB, and\nreal-time data workflows.\n\nTechnical Skills:\nExpertise in Python, SQL, and PySpark\nStrong background in software architecture and scalable development practices.\nTableau, Metabase or similar viz tools experience\nWorking knowledge of AI frameworks and libraries is a plus.\nLeadership & Communication:\nDemonstrates ownership and accountability for delivery with a strong\ncommitment to quality.\nExcellent communication skills with a history of effective client and end-user\nengagement.\nStartup & Fintech Mindset:\nAdaptability and agility to thrive in a fast-paced, early-stage startup environment.\nPassion for fintech innovation and a strong desire to make a meaningful impact\non the future of finance.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'performance optimization strategies', 'PySpark', 'Django', 'cost management', 'AWS', 'AI frameworks', 'Python', 'SQL']",2025-06-12 15:01:37
Data Science,Global Banking Organization,5 - 10 years,Not Disclosed,['Bengaluru'],"Key Skills: Machine Learning, Data Science, Azure, Python, Hadoop.\nRoles and Responsibilities:\nStrong understanding of Math, Statistics, and the theoretical foundations of Statistical & Machine Learning, including Parametric and Non-parametric models.\nApply advanced data mining techniques to curate, process, and transform raw data into reliable datasets.\nUse various statistical techniques and ML methods to perform predictive modeling/classification for problems related to clients, distribution, sales, client profiles, and segmentation, and provide actionable insights for business decision-making.\nDemonstrate expertise in the full Machine Learning lifecycle--feature engineering, training, validation, scaling, deployment, scoring, monitoring, and feedback loops.\nProficiency in Python visualization libraries such as matplotlib and seaborn.\nExperience with cloud computing infrastructure like Azure, including Machine Learning Studio, Azure Data Factory, Synapse, Python, and PySpark.\nAbility to develop, test, and deploy models on cloud/web platforms.\nExcellent knowledge of Deep Learning Architectures, including Convolutional Neural Networks and Transformer/LLM Foundation Models.\nStrong expertise in supervised and adversarial learning techniques.\nRobust working knowledge of deep learning frameworks such as TensorFlow, Keras, and PyTorch.\nExcellent Python coding skills.\nExperience with version control tools (Git, GitHub/GitLab) and data version control.\nExperience in end-to-end model deployment and productionization.\nDemonstrated proficiency in deploying, scaling, and optimizing ML models in production environments with low latency, high availability, and cost efficiency.\nSkilled in model interpretability and CI/CD for ML using tools like MLflow and Kubeflow, with the ability to implement automated monitoring, logging, and retraining strategies.\nExperience Requirement:\n5-12 years of experience in designing and deploying deep learning and machine learning solutions.\nProven track record of delivering AI/ML solutions in real-world business applications at scale.\nHands-on experience working in cross-functional teams including data engineers, product managers, and business stakeholders.\nExperience mentoring junior data scientists and providing technical leadership within a data science team.\nExperience working with big data tools and environments such as Hadoop, Spark, or Databricks is a plus.\nPrior experience in managing model lifecycle in enterprise production environments including drift detection and retraining pipelines.\nEducation: B.Tech.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure', 'Hadoop.', 'Machine Learning', 'Python']",2025-06-12 15:01:40
Lead Data Engineer,Prolegion,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Lead Data Engineer/Associate Architect to lead the design, implementation, and optimization of scalable data architectures. The ideal candidate will have a deep understanding of data modeling, ETL processes, cloud data solutions, and big data technologies. You will work closely with cross-functional teams to build robust, high-performance data pipelines and infrastructure to enable data-driven decision-making.\n\nExperience: 8 - 12+ years\nWork Location: Hyderabad (Hybrid)\nMandatory skills: Python, SQL, Snowflake\n\nResponsibilities:\nDesign and Develop scalable and resilient data architectures that support business needs, analytics, and AI/ML workloads.\nData Pipeline Development: Design and implement robust ETL/ELT processes to ensure efficient data ingestion, transformation, and storage.\nBig Data & Cloud Solutions: Architect data solutions using cloud platforms like AWS, Azure, or GCP, leveraging services such as Snowflake, Redshift, BigQuery, and Databricks.\nDatabase Optimization: Ensure performance tuning, indexing strategies, and query optimization for relational and NoSQL databases.\nData Governance & Security: Implement best practices for data quality, metadata management, compliance (GDPR, CCPA), and security.\nCollaboration & Leadership: Work closely with data engineers, analysts, and business stakeholders to translate business requirements into scalable solutions.\nTechnology Evaluation: Stay updated with emerging trends, assess new tools and frameworks, and drive innovation in data engineering.\n\nRequired Skills:\nEducation: Bachelors or Masters degree in Computer Science, Data Engineering, or a related field.\nExperience: 8 - 12+ years of experience in data engineering\nCloud Platforms: Strong expertise in AWS data services.\nBig Data Technologies: Experience with Hadoop, Spark, Kafka, and related frameworks.\nDatabases: Hands-on experience with SQL, NoSQL, and columnar databases such as PostgreSQL, MongoDB, Cassandra, and Snowflake.\nProgramming: Proficiency in Python, Scala, or Java for data processing and automation.\nETL Tools: Experience with tools like Apache Airflow, Talend, DBT, or Informatica.\nMachine Learning & AI Integration (Preferred): Understanding of how to architect data solutions for AI/ML applications\n\n,",Industry Type: Defence & Aerospace,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'Data modeling', 'Postgresql', 'Informatica', 'Apache', 'Analytics', 'SQL', 'Python']",2025-06-12 15:01:42
Gcp Data Engineer,Saama Technologies,3 - 8 years,Not Disclosed,"['Pune', 'Chennai', 'Coimbatore']","We are looking for immediate joiners only.\nPosition: GCP Data Engineer\nWe are seeking a skilled and experienced GCP Data Engineer to join our dynamic team. The ideal candidate will have a strong background in Google Cloud Platform (GCP), BigQuery, Dataform, and data warehouse concepts. Experience with Airflow/Cloud Composer and cloud computing knowledge will be a significant advantage.\nResponsibilities:\n- Designing, developing, and maintaining data pipelines and workflows on the Google Cloud Platform.",,,,"['Pyspark', 'GCP', 'Python', 'SQL', 'Google Cloud Platforms']",2025-06-12 15:01:44
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Gurugram'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nBengaluru\nSenior Flexera Data Analyst\nData Science\nIndia\nChennai\nData Scientist\nData Science\nIndia\nBengaluru\nGurugram, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Gurugram\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-12 15:01:46
Senior Data Engineer -Bangalore,Happiest Minds Technologies,6 - 10 years,Not Disclosed,['Bengaluru'],"Job Overview:\nThe primary purpose of this role is to translate business requirements and functional specifications into logical program designs and to deliver dashboards, schema, data pipelines, and software solutions. This includes developing, configuring, or modifying data components within various complex business and/or enterprise application solutions in various computing environments. You will partner closely with multiple Business partners, Product Owners, Data Strategy, Data Platform, Data Science and Machine Learning (MLOps) teams to drive innovative data products for end users. Additionally, you will help shape overall solution & data products, develop scalable solutions through best-in-class engineering practices.",,,,"['NoSQL', 'big data systems', 'Data Pipeline', 'MongoDB', 'SQL', 'Hive', 'GIT', 'Hadoop', 'Kafka', 'Agile', 'MQL', 'Ci/Cd']",2025-06-12 15:01:49
Azure Data Engineer ( Azure Databricks),Apex One,4 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Job Summary\nWe are seeking a skilled Azure Data Engineer with 4 years of overall experience, including at least 2 years of hands-on experience with Azure Databricks (Must). The ideal candidate will have strong expertise in building and maintaining scalable data pipelines and working across cloud-based data platforms.\nKey Responsibilities\nDesign, develop, and optimize large-scale data pipelines using Azure Data Factory, Azure Databricks, and Azure Synapse.\nImplement data lake solutions and work with structured and unstructured datasets in Azure Data Lake Storage (ADLS).\nCollaborate with data scientists, analysts, and engineering teams to design and deliver end-to-end data solutions.\nDevelop ETL/ELT processes and integrate data from multiple sources.\nMonitor, debug, and optimize workflows for performance and cost-efficiency.\nEnsure data governance, quality, and security best practices are maintained.\nMust-Have Skills\n4+ years of total experience in data engineering.\n2+ years of experience with Azure Databricks (PySpark, Notebooks, Delta Lake).\nStrong experience with Azure Data Factory, Azure SQL, and ADLS.\nProficient in writing SQL queries and Python/Scala scripting.\nUnderstanding of CI/CD pipelines and version control systems (e.g., Git).\nSolid grasp of data modeling and warehousing concepts.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Azure Data Factory', 'SQL queries', 'PySpark', 'Delta Lake', 'Azure Databricks', 'Notebooks', 'Azure SQL']",2025-06-12 15:01:51
It Recruiter,IonIdea,0 - 3 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nTalent Sourcing: Utilize various channels such as job boards, social media, LinkedIn, networking events, and internal databases to source and attract high-quality candidates for a variety of technical positions (software developers, systems engineers, data scientists, etc.).\nCandidate Screening: Review resumes, conduct initial phone screenings, and assess candidates technical skills, experience, and cultural fit.\nInterview Coordination: Schedule and facilitate interviews with hiring managers, ensuring a smooth and efficient process for all parties involved.\nCandidate Engagement: Build relationships with both active and passive candidates to maintain a strong pipeline of qualified talent. Keep candidates informed throughout the hiring process.\nOffer Management: Work with HR and hiring managers to present offers, negotiate terms, and ensure a positive candidate experience during the offer process.\n\nQualifications:\nExperience: Fresher-3years\n\nTechnical Knowledge: A solid understanding of IT roles, including knowledge of programming languages, software development frameworks, network infrastructure, cloud technologies, and emerging IT trends.\nRecruitment Tools: Proficient in using Applicant Tracking Systems (ATS), job boards (e.g., LinkedIn, Indeed), and social media platforms for sourcing candidates.\nCommunication Skills: Excellent written and verbal communication skills with the ability to engage with both technical and non-technical stakeholders.",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['IT Recruitment', 'C2H', 'Contract Hiring']",2025-06-12 15:01:53
Sr Engineer/Sr. Lead - Generative AI,Qualcomm,3 - 8 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\n\nLocation - Hyderabad\n\n\nExperience - 3-8 Years\n\n\nWe are seeking an experienced Machine Learning Engineers specializing in Generative AI to join our core AI team.\n\nThe ideal candidate will be responsible for designing, developing, and deploying cutting-edge generative AI solutions, with a focus on Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Intelligent agent systems.\n\nKey Responsibilities:\n\nDesign and implement RAG-based solutions to enhance LLM capabilities with external knowledge sources\n\nDevelop and optimize LLM fine-tuning strategies for specific use cases and domain adaptation\n\nCreate robust evaluation frameworks for measuring and improving model performance\n\nBuild and maintain agentic workflows for autonomous AI systems\n\nCollaborate with cross-functional teams to identify opportunities and implement AI solutions\n\n\nRequired Qualifications:\n\nBachelor's or Master's degree in Computer Science, or related technical field\n\n3+ years of experience in Machine Learning/AI engineering\n\nStrong programming skills in Python and experience with ML frameworks (PyTorch, TensorFlow)\n\nPractical experience with LLM deployments and fine-tuning\n\nExperience with vector databases and embedding models\n\nFamiliarity with modern AI/ML infrastructure and cloud platforms (AWS, GCP, Azure)\n\nStrong understanding of RAG architectures and implementation\n\n\nPreferred Qualifications:\n\nExperience with popular LLM frameworks (Langchain, LlamaIndex, Transformers)\n\nKnowledge of prompt engineering and chain-of-thought techniques\n\nExperience with containerization and microservices architecture\n\nBackground in NLP and deep learning\n\nBackground in Reinforcement Learning\n\nContributions to open-source AI projects\n\nExperience with ML ops and model deployment pipelines\n\n\nSkills and Competencies:\n\nStrong problem-solving and analytical skills\n\nExcellent communication and collaboration abilities\n\nExperience with agile development methodologies\n\nAbility to balance multiple projects and priorities\n\nStrong focus on code quality and best practices\n\nUnderstanding of AI ethics and responsible AI development",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'software engineering', 'c++', 'c', 'natural language processing', 'microsoft azure', 'artificial intelligence', 'microservices', 'reinforcement learning', 'deep learning', 'java', 'computer science', 'gcp', 'agile', 'aws', 'ml']",2025-06-12 15:01:56
AI/ML framework Staff Engineer,Qualcomm,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nLooking for ""ML framework and AI compiler Engineer"" responsible for\nDesigning, implementing, and deploying machine learning models using PyTorch\nFocusing on backend infrastructure and system architecture.\nResponsibilities often include developing framework, integrating with other AI tools, and ensuring scalability and reliability.\n\nHere's a more detailed breakdown of what you might see in such a job description:\n\nKey Responsibilities:\n\n\nModel Development and DeploymentDesigning, building, and deploying AI models, particularly those leveraging PyTorch for deep learning.\n\n\nBackend InfrastructureDeveloping and maintaining the backend systems that power AI applications, including data ingestion, processing, and storage.\n\n\nSystem ArchitectureDesigning scalable and high-performance backend architectures to handle AI workloads.\n\n\nModel OptimizationOptimizing model performance for speed, accuracy, and resource efficiency.\n\n\nIntegrationIntegrating AI models with other systems and applications.\n\n\nAPI DevelopmentCreating and maintaining APIs for communication between frontend and backend components.\n\n\nData HandlingManaging data ingestion, preprocessing, and storage for AI training and inference.\n\n\nCollaborationWorking with data scientists, product managers, and other engineers to bring AI solutions to life.\n\nTools, Technologies, Skills and Programming:\n\n\nC, C++: Strong programming capability using advanced techniques to design and develop AI compilers and backends.\n\n\nScripting: Strong expertise in Python with design, develop, release and maintain projects.\n\n\nAI Frameworks: Familiarity with other AI frameworks like PyTorch, TensorFlow, Hugging Face, etc.\n\n\nMachine Learning Knowledge: Understanding of machine learning principles and algorithms starting Computer vision to large language models and continuously update to new trends.\nExpertise to deep learning accelerator programming (GPU, NPU). Any parallel programming experience (Like CUDA, OpenCL, MKLDNN ..etc) is a plus.\nExperience with deep leaning compilers like Glow, TVM ""etc is a plus.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'system engineering', 'c#', 'cuda', 'algorithms', 'c++', 'parallel programming', 'artificial intelligence', 'opencl', 'deep learning', 'java', 'product management', 'computer vision', 'asp.net', 'multithreading', 'mvc', 'ml']",2025-06-12 15:01:58
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-12 15:02:01
AI Engineer - Lead,Blend360 India,6 - 10 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for someone who is ready for the next step in their career and is excited by the idea of solving problems and designing best in class. However, they also need to be aware of the practicalities of making a difference in the real world - whilst we love innovative advanced solutions, we also believe that sometimes a simple solution can have the most impact.\nOur AI Engineer is someone who feels the most comfortable around solving problems, answering questions and proposing solutions. We place a high value on the ability to communicate and translate complex analytical thinking into non-technical and commercially oriented concepts, and experience working on difficult projects and/or with demanding stakeholders is always appreciated.\nWhat can you expect from the role?\nContribute to design, develop, deploy and maintain AI solutions\nUse a variety of AI Engineering tools and methods to deliver\nOwn parts of projects end-to-end\nContributing to solutions design and proposal submissions\nSupporting the development of the AI engineering team within Blend\nMaintain in-depth knowledge of the AI ecosystems and trends\nMentor junior colleagues\n\n\nQualifications\nContribute to the design, development, testing, deployment, maintenance, and improvement of robust, scalable, and reliable software systems, adhering to best practices.",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Version control', 'orchestration', 'GIT', 'GCP', 'Analytical', 'System integration', 'Software development life cycle', 'Mentor', 'Monitoring', 'Python']",2025-06-12 15:02:03
Data Engineering Manager,NOVARTIS,6 - 8 years,Not Disclosed,['Hyderabad'],"Summary\nWe are seeking a highly skilled and motivated GCP Data Engineering Manager to join our dynamic team. As a Data Engineering manager specializing in Google Cloud Platform (GCP), you will play a crucial role in designing, implementing, and maintaining scalable data pipelines and\nsystems. You will leverage your expertise in Google Big Query, SQL, Python, and analytical skills to drive data-driven decision-making processes and support various business functions.\nAbout the Role\nKey Responsibilities:\nData Pipeline Development: Design, develop, and maintain robust data pipelines using GCP services like Dataflow, Dataproc, ensuring high performance and scalability.\nGoogle Big Query Expertise: Utilize your hands-on experience with Google Big Query to manage and optimize data storage, retrieval, and processing.\nSQL Proficiency: Write and optimize complex SQL queries to transform and analyze large datasets, ensuring data accuracy and integrity.\nPython Programming: Develop and maintain Python scripts for data processing, automation, and integration with other systems and tools.\nData Integration: Collaborate with data analysts, and other stakeholders to integrate data from various sources, ensuring seamless data flow and consistency.\nData Quality and Governance: Implement data quality checks, validation processes, and governance frameworks to maintain high data standards.\nPerformance Tuning: Monitor and optimize the performance of data pipelines, queries, and storage solutions to ensure efficient data processing.\nDocumentation: Create comprehensive documentation for data pipelines, processes, and best practices to facilitate knowledge sharing and team collaboration.\nMinimum Qualifications:\nProven experience (minimum 6 - 8 yrs) in Data Engineer, with significant hands-on experience in Google Cloud Platform (GCP) and Google Big Query.\nProficiency in SQL for data transformation, analysis and performance optimization.\nStrong programming skills in Python, with experience in developing data processing scripts and automation.\nProven analytical skills with the ability to interpret complex data and provide actionable insights.\nExcellent problem-solving abilities and attention to detail.\nStrong communication and collaboration skills, with the ability to work effectively in a team enviro\nDesired Skills :\nExperience with Google Analytics data and understanding of digital marketing data.\nFamiliarity with other GCP services such as Cloud Storage, Dataflow, Pub/Sub, and Dataproc.\nKnowledge of data visualization tools such as Looker, Tableau, or Data Studio.\nExperience with machine learning frameworks and libraries.\nWhy Novartis: Helping people with disease and their families takes more than innovative science. It takes a community of smart, passionate people like you. Collaborating, supporting and inspiring each other. Combining to achieve breakthroughs that change patients lives. Ready to create a brighter future together? https://www. novartis. com / about / strategy / people-and-culture\nJoin our Novartis Network: Not the right Novartis role for you? Sign up to our talent community to stay connected and learn about suitable career opportunities as soon as they come up: https://talentnetwork. novartis. com/network\nBenefits and Rewards: Read our handbook to learn about all the ways we ll help you thrive personally and professionally:",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'Google Analytics', 'Machine learning', 'Data processing', 'Data quality', 'data visualization', 'Digital marketing', 'SQL', 'Python']",2025-06-12 15:02:06
GCP Data Engineer,TVS Next,3 - 5 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['kubernetes', 'pyspark', 'data pipeline', 'sql', 'docker', 'cloud', 'tensorflow', 'java', 'spark', 'gcp', 'pytorch', 'bigquery', 'programming', 'ml', 'cloud sql', 'cd', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'application engine', 'machine learning', 'apache flink', 'data engineering', 'dataproc', 'kafka', 'cloud storage', 'terraform', 'bigtable']",2025-06-12 15:02:08
Data Engineer,Luxoft,5 - 8 years,Not Disclosed,['Pune'],"Help Group Enterprise Architecture team to develop our suite of EA tools and workbenches\nWork in the development team to support the development of portfolio health insights\nBuild data applications from cloud infrastructure to visualization layer\nProduce clear and commented code\nProduce clear and comprehensive documentation\nPlay an active role with technology support teams and ensure deliverables are completed or escalated on time\nProvide support on any related presentations, communications, and trainings\nBe a team player, working across the organization with skills to indirectly manage and influence\nBe a self-starter willing to inform and educate others\nSkills\nMust have\nB.Sc./M.Sc. degree in computing or similar\n5-8+ years experience as a Data Engineer, ideally in a large corporate environment\nIn-depth knowledge of SQL and data modelling/data processing\nStrong experience working with Microsoft Azure\nExperience with visualisation tools like PowerBI (or Tableau, QlikView or similar)\nExperience working with Git, JIRA, GitLab\nStrong flair for data analytics\nStrong flair for IT architecture and IT architecture metrics\nExcellent stakeholder interaction and communication skills\nUnderstanding of performance implications when making design decisions to deliver performant and maintainable software.\nExcellent end-to-end SDLC process understanding.\nProven track record of delivering complex data apps on tight timelines\nFluent in English both written and spoken.\nPassionate about development with focus on data and cloud\nAnalytical and logical, with strong problem solving skills\nA team player, comfortable with taking the lead on complex tasks\nAn excellent communicator who is adept in, handling ambiguity and communicating with both technical and non-technical audiences\nComfortable with working in cross-functional global teams to effect change\nPassionate about learning and developing your hard and soft professional skills\nNice to have\nExperience working in the financial industry\nExperience in complex metrics design and reporting\nExperience in using artificial intelligence for data analytics\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Power BI Developer\nBI Engineering\nIndia\nBengaluru\nSenior Power BI Developer\nBI Engineering\nIndia\nChennai\nSenior Power BI Developer\nBI Engineering\nIndia\nGurugram\nPune, India\nReq. VR-114797\nBI Engineering\nBCM Industry\n02/06/2025\nReq. VR-114797\nApply for Data Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Enterprise architecture', 'Analytical', 'Artificial Intelligence', 'Data processing', 'Data analytics', 'QlikView', 'JIRA', 'SDLC', 'SQL']",2025-06-12 15:02:11
Data Engineer II - Marketplace (Experimentation Track),Booking Holdings,5 - 10 years,Not Disclosed,['Bengaluru'],"We are looking for a Data Engineer to join our team and help us to improve the platform that supports one of the best experimentation tools in the world.\nYou will work side by side with other data engineers and site reliability engineers to improve the reliability, scalability, maintenance and operations of all the data products that are part of the experimentation tool at Booking.com.\nYour day to day work includes but is not limited to: maintenance and operations of data pipelines and products that handles data at big scale; the development of capabilities for monitoring, alerting, testing and troubleshooting of the data ecosystem of the experiment platform; and the delivery of data products that produce metrics for experimentation at scale. You will collaborate with colleagues in Amsterdam to achieve results the right way. This will include engineering managers, product managers, engineers and data scientists.\nKey Responsibilities and Duties\nTake ownership of multiple data pipelines and products and provide innovative solutions to reduce the operational workload required to maintain them\nRapidly developing next-generation scalable, flexible, and high-performance data pipelines.\nContribute to the development of data platform capabilities such as testing, monitoring, debugging and alerting to improve the development environment of data products\nSolve issues with data and data pipelines, prioritizing based on customer impact.\nEnd-to-end ownership of data quality in complex datasets and data pipelines.\nExperiment with new tools and technologies, driving innovative engineering solutions to meet business requirements regarding performance, scaling, and data quality.\nProvide self-organizing tools that help the analytics community discover data, assess quality, explore usage, and find peers with relevant expertise.\nServe as the main point of contact for technical and business stakeholders regarding data engineering issues, such as pipeline failures and data quality concerns\nRole requirements\nMinimum 5 years of hands-on experience in data engineering as a Data Engineer or as a Software Engineer developing data pipelines and products.\nBachelors degree in Computer Science, Computer or Electrical Engineering, Mathematics, or a related field or 5 years of progressively responsible experience in the specialty as equivalent\nSolid experience in at least one programming language. We use Java and Python\nExperience building production data pipelines in the cloud, setting up data-lakes and server-less solutions\nHands-on experience with schema design and data modeling\nExperience designing systems E2E and knowledge of basic concepts (lb, db, caching, NoSQL, etc)\nKnowledge of Flink, CDC, Kafka, Airflow, Snowflake, DBT or equivalent tools\nPractical experience building data platform capabilities like testing, alerting, monitoring, debugging, security\nExperience working with big data.\nExperience working with teams located in different timezones is a plus\nExperience with experimentation, statistics and A/B testing is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Airflow', 'Java', 'CDC', 'NoSQL', 'Snowflake', 'DBT', 'Kafka', 'Python']",2025-06-12 15:02:13
Microsoft Fabrics Data Engineer,Swits Digital,5 - 10 years,Not Disclosed,['Bengaluru'],"Job TItle: Microsoft Fabric Data Engineer\nLocation: Bangalore\nJob Type: Conract (24 Months)\nJob Description:\nWe are seeking a highly skilled and experienced Microsoft Fabric Data Engineer/Architect to design, develop, and maintain robust, scalable, and secure data solutions within the Microsoft Fabric ecosystem. This role will leverage the full suite of Microsoft Azure data services, including Azure Data Bricks, Azure Data Factory, and Azure Data Lake, to build end-to-end data pipelines, data warehouses, and data lakehouses that enable advanced analytics and business intelligence.\nRequired Skills & Qualifications:\nBachelors degree in Computer Science, Engineering, or a related field.\n5+ years of experience in data architecture and engineering, with a strong focus on Microsoft Azure data platforms.\nProven hands-on expertise with Microsoft Fabric and its components, including:\nOneLake\nData Factory (Pipelines, Dataflows Gen2)\nSynapse Analytics (Data Warehousing, SQL analytics endpoint)\nLakehouses and Warehouses\nNotebooks (PySpark)\nExtensive experience with Azure Data Bricks, including Spark development (PySpark, Scala, SQL).\nStrong proficiency in Azure Data Factory for building and orchestrating ETL/ELT pipelines.\nDeep understanding and experience with Azure Data Lake Storage Gen2.\nProficiency in SQL (T-SQL, Spark SQL), Python, and/or other relevant scripting languages.\nSolid understanding of data warehousing concepts, dimensional modeling, and data lakehouse architectures.\nExperience with data governance principles and tools (e.g., Microsoft Purview).\nFamiliarity with CI/CD practices, version control (Git), and DevOps for data pipelines.\nExcellent problem-solving, analytical, and communication skills.\nAbility to work independently and collaboratively in a fast-paced, agile environment.\nPreferred Qualifications:\nMicrosoft certifications in Azure Data Engineering (e.g., DP-203, DP-600: Microsoft Fabric Analytics Engineer Associate).\nExperience with Power BI for data visualization and reporting.\nFamiliarity with real-time analytics and streaming data processing.\nExposure to machine learning workflows and integrating ML models with data solutions",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Analytical', 'microsoft azure', 'data visualization', 'microsoft', 'Business intelligence', 'Data warehousing', 'Analytics', 'Data architecture', 'Python']",2025-06-12 15:02:16
Sr Data Engineer,Lowes Services India Private limited,5 - 10 years,Not Disclosed,['Bengaluru'],"We are seeking a seasoned Senior Data Engineer to join our Marketing Data Platform team. This role is pivotal in designing, building, and optimizing scalable data pipelines and infrastructure that support our marketing analytics and customer engagement strategies. The ideal candidate will have extensive experience with big data technologies, cloud platforms, and a strong understanding of marketing data dynamics.\n\nData Pipeline Development & Optimization\nDesign, develop, and maintain robust ETL/ELT pipelines using Apache PySpark on GCP services like Dataproc and Cloud Composer.\nEnsure data pipelines are scalable, efficient, and reliable to handle large volumes of marketing data.\nData Warehousing & Modeling\nImplement and manage data warehousing solutions using BigQuery, ensuring optimal performance and cost-efficiency.\nDevelop and maintain data models that support marketing analytics and reporting needs.\nCollaboration & Stakeholder Engagement\nWork closely with marketing analysts, data scientists, and cross-functional teams to understand data requirements and deliver solutions that drive business insights.\nTranslate complex business requirements into technical specifications and data architecture.\nData Quality & Governance\nImplement data quality checks and monitoring to ensure the accuracy and integrity of marketing data.\nAdhere to data governance policies and ensure compliance with data privacy regulations.\nContinuous Improvement & Innovation\nStay abreast of emerging technologies and industry trends in data engineering and marketing analytics.\nPropose and implement improvements to existing data processes and infrastructure\n  Years of Experience\n5 Years in Data Engineer space\n  Education Qualification & Certifications\nB.Tech or MCA\n  Experience\nProven experience with Apache PySpark, GCP (including Dataproc, BigQuery, Cloud Composer), and data pipeline orchestration.\nTechnical Skills\nProficiency in SQL and Python.\nExperience with data modeling, ETL/ELT processes, and data warehousing concepts.",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['orchestration', 'Data modeling', 'data governance', 'Data quality', 'Apache', 'Continuous improvement', 'Monitoring', 'SQL', 'Python', 'Data architecture']",2025-06-12 15:02:18
Associate Advanced Services Engineer (Oracle Apps/Cloud),Oracle,3 - 6 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","The Customer Success Services (CSS) is a unit within Oracle s Customer Service Organization that enables long term advanced support relationships with many of Oracles largest customers.\nGlobal Expertise Center Engineers (GEC) provide support in the continuous operational improvement of Oracle environments.\nGEC Engineers do this by leveraging Oracle s support-based intellectual property and customers experiences throughout their involvement with Oracle s technologies.\nOur goal is for every customer to gain ever-more value from their Oracle Solutions by helping them make well informed decisions regarding the implementation; management and use of Oracle technologies.\nCareer Level - IC1\nB.E./B.Tech. with 3 to 12 months of relevant experience.\nDemonstrates good communication, customer management, customer engagement and project management skills.\nTechnical aptitude: Has a basic understanding of application, middleware, hardware and/or Cloud technologies, and curiosity in cloud technology concepts such as Artificial Intelligence, Blockchain, Machine Learning, DevOps, Security and Oracle Cloud infrastructure.\nFluency in English.\nWork involves some problem solving with assistance and guidance in understanding and applying Oracle policies and procedures.\nAble to demonstrate time management.\nGoal oriented individual.\nAble to complete individual goals as well as work in a team environment.\nDemonstrated ability to communicate using technical concepts. Working knowledge of Oracle products a plus but not necessary.\nProfessional demeanor.\nAvailability to work in scheduled out of hours operations when required",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle Apps', 'Time management', 'Artificial Intelligence', 'Project management', 'Machine learning', 'Intellectual property', 'Customer service', 'Oracle', 'Customer engagement', 'Middleware']",2025-06-12 15:02:20
Associate Advanced Services Engineer (Oracle Apps/Cloud),Oracle,3 - 6 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","The Customer Success Services (CSS) is a unit within Oracle s Customer Service Organization that enables long term advanced support relationships with many of Oracles largest customers.\nGlobal Expertise Center Engineers (GEC) provide support in the continuous operational improvement of Oracle environments.\nGEC Engineers do this by leveraging Oracle s support-based intellectual property and customers experiences throughout their involvement with Oracle s technologies.\nOur goal is for every customer to gain ever-more value from their Oracle Solutions by helping them make well informed decisions regarding the implementation; management and use of Oracle technologies.\nCareer Level - IC1\nB.E./B.Tech. with 3 to 12 months of relevant experience.\nDemonstrates good communication, customer management, customer engagement and project management skills.\nTechnical aptitude: Has a basic understanding of application, middleware, hardware and/or Cloud technologies, and curiosity in cloud technology concepts such as Artificial Intelligence, Blockchain, Machine Learning, DevOps, Security and Oracle Cloud infrastructure.\nFluency in English.\nWork involves some problem solving with assistance and guidance in understanding and applying Oracle policies and procedures.\nAble to demonstrate time management.\nGoal oriented individual.\nAble to complete individual goals as well as work in a team environment.\nDemonstrated ability to communicate using technical concepts. Working knowledge of Oracle products a plus but not necessary.\nProfessional demeanor.\nAvailability to work in scheduled out of hours operations when required",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle Apps', 'Time management', 'Artificial Intelligence', 'Project management', 'Machine learning', 'Intellectual property', 'Customer service', 'Oracle', 'Customer engagement', 'Middleware']",2025-06-12 15:02:24
Senior Data Scientist,Straive,5 - 10 years,Not Disclosed,"['Hyderabad', 'Gurugram', 'Bengaluru']","Role & responsibilities\nRequires 5-8 years of proven experience in banking/payments/other domains\nStrong experience in developing Machine Learning models, Python & SQL\nExperience working with pre-trained models, awareness of state-of-art in embeddings and applicability for use cases\nDetailed oriented with a proactive mindset towards problem-solving\nExcellent communication and presentation skills with the ability to convey complex information clearly and concisely",,,,"['Machine Learning', 'Python', 'SQL', 'Xgboost', 'Neural Networks', 'Random Forest']",2025-06-12 15:02:26
Senior Data Engineer,Eurofins Digital Testing,7 - 10 years,Not Disclosed,['Bengaluru'],"Job Description\nSenior Data Engineer - Quality Engineering\nExperience Range: 7-10 Years\nLocation: Bangalore (Hybrid Mode)\nResillion, a leading quality engineering company with offices around the world, is seeking a talented Data Engineer to join our growing India team. In this role, you will play a critical part in building and maintaining the data infrastructure that supports our AI-powered testing tools and analytics solutions. You will have technical responsibility for the entire data lifecycle, from data acquisition and ingestion to storage, processing, and analysis.\nResponsibilities:\nCollaborate with GenAI engineers, Software Engineers, Test automation engineers and other stakeholders within Resillion to understand data needs and translate them into technical solutions, including designing data pipelines for training & deploying models and data pre-processing for AI, including generative AI applications.\nDesign, implement, and advise on data migration testing strategies and data quality assurance strategies for Resillion customers, ensuring a smooth transition of data to new customer systems.\nDesign, develop, and implement scalable data pipelines using cloud-based data engineering tools and technologies, with a focus on both Microsoft Azure solutions (e.g., Azure Data Factory, Azure Databricks) and Google Cloud Platform (GCP) solutions (e.g., Google Cloud Dataflow, Google Cloud Dataproc).\nWrite efficient and maintainable code to extract, transform, and load data from various sources, leveraging your expertise in Azure Data Lake Storage and other relevant Azure services, as well as Google Cloud Storage and other relevant GCP services.\nBuild and manage data warehouses and data lakes for quality engineering data, utilizing your knowledge of Azure Synapse Analytics or similar technologies, and Google BigQuery or similar technologies.\nDevelop and implement data quality checks and monitoring procedures to ensure data integrity using Azure Data Catalog or other appropriate tools, as well as Google Cloud Data Catalog or other appropriate tools.\nAutomate data engineering tasks and workflows using Azure automation tools and GCP automation tools (e.g., Cloud Functions, Cloud Composer).\nSet up quality intelligence dashboards for quality assurance data using Microsoft Power BI to provide stakeholders with clear and actionable insights.\nStay up-to-date with the latest data engineering tools and technologies, including advancements in AI, Machine Learning (MLOps), and generative AI for data processing in both the Azure and GCP environments.\nAdvise on and implement test data generation strategies and solutions for various testing needs.\n\n\nQualifications\nMinimum 7+ years of experience in data engineering or a related field\nProven experience in designing, developing, and deploying data pipelines",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Engineering services', 'Manager Quality Assurance', 'GCP', 'Quality engineering', 'Machine learning', 'Quality Engineer', 'Monitoring', 'Analytics', 'SQL', 'Python']",2025-06-12 15:02:28
Senior Data Engineer,Talentien Global Solutions,4 - 8 years,12-18 Lacs P.A.,"['Hyderabad', 'Chennai', 'Coimbatore']","We are seeking a skilled and motivated Data Engineer to join our dynamic team. The ideal candidate will have experience in designing, developing, and maintaining scalable data pipelines and architectures using Hadoop, PySpark, ETL processes, and Cloud technologies.\n\nResponsibilities:\nDesign, develop, and maintain data pipelines for processing large-scale datasets.\nBuild efficient ETL workflows to transform and integrate data from multiple sources.\nDevelop and optimize Hadoop and PySpark applications for data processing.\nEnsure data quality, governance, and security standards are met across systems.\nImplement and manage Cloud-based data solutions (AWS, Azure, or GCP).\nCollaborate with data scientists and analysts to support business intelligence initiatives.\nTroubleshoot performance issues and optimize query executions in big data environments.\nStay updated with industry trends and advancements in big data and cloud technologies.\nRequired Skills:\nStrong programming skills in Python, Scala, or Java.\nHands-on experience with Hadoop ecosystem (HDFS, Hive, Spark, etc.).\nExpertise in PySpark for distributed data processing.\nProficiency in ETL tools and workflows (SSIS, Apache Nifi, or custom pipelines).\nExperience with Cloud platforms (AWS, Azure, GCP) and their data-related services.\nKnowledge of SQL and NoSQL databases.\nFamiliarity with data warehousing concepts and data modeling techniques.\nStrong analytical and problem-solving skills.\n\nInterested can reach us at +91 7305206696/ saranyadevib@talentien.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Hadoop', 'Spark', 'ETL', 'Airflow', 'Etl Pipelines', 'Big Data', 'EMR', 'Gcp Cloud', 'Data Bricks', 'Azure Cloud', 'Data Pipeline', 'SCALA', 'Snowflake', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'AWS', 'Python']",2025-06-12 15:02:30
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Bengaluru'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nGurugram\nSenior Flexera Data Analyst\nData Science\nIndia\nChennai\nBusiness Analyst\nData Science\nPoland\nRemote Poland\nBengaluru, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Bengaluru\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-12 15:02:33
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Chennai'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nGurugram\nSenior Flexera Data Analyst\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nChennai, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Chennai\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-12 15:02:35
GStreamer multimedia framework Lead Engineer Senior,Qualcomm,3 - 8 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nWe are seeking a skilled Engineer with extensive experience in the GStreamer multimedia framework. The ideal candidate will be responsible for designing, developing, and optimizing multimedia applications and systems. This role requires a deep understanding of multimedia processing, pipeline architecture, and the ability to work on complex projects.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\nExperience with majority in Multimedia framework & Gstreamer plugins development. Strong programming skills in C and C++ for embedded systems Good knowledge about AI/ML applications developements Exposure to developing solutions on Linux is must Strong in multi-threaded programming, synchronization and IPCs Strong Software design skills and ability to guide team of engineers Good knowledge on software development processes Need very good Communication skills and ability to work with cross functional teams Exposure to other media frameworks such as ffmpeg, directshow, stagefright is a plus Good knowledge on V4L2, Pulseaudio, Alsa, OpenGLES is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'c', 'embedded systems', 'linux', 'gstreamer', 'python', 'software development', 'software design', 'plugins', 'pipeline architecture', 'artificial intelligence', 'multimedia', 'stagefright', 'multimedia framework', 'java', 'synchronization', 'multithreading', 'software engineering', 'ffmpeg', 'ml']",2025-06-12 15:02:38
Sr Manager of Software Engineering,JPMorgan Chase Bank,14 - 20 years,Not Disclosed,['Bengaluru'],"When you mentor and advise multiple technical teams and move financial technologies forward, it s a big challenge with big impact. You were made for this.\n\n\nAs a Senior Manager of Software Engineering at JPMorgan Chase within the Consumer and Community Banking Technology Team, you serve in a leadership role by providing technical coaching and advisory for multiple technical teams, as well as anticipate the needs and potential dependencies of other functions within the firm. As an expert in your field, your insights influence budget and technical considerations to advance operational efficiencies and functionalities.\n\nJob responsibilities\n\n\n\nProvide direction, oversight, and coaching for a team of entry-level to mid-level software engineers working on basic to moderately complex tasks.\n\nBe accountable for decisions affecting team resources, budget, tactical operations, and the execution and implementation of processes and procedures.\n\nLead the design, development, testing, and implementation of data visualization projects to support business objectives.\n\nCollaborate with data analysts, data scientists, and business stakeholders to understand data requirements and translate them into effective visual solutions.\n\nWork in an Agile development environment with team members, including Product Managers, UX Designers, QA Engineers, and other Software Engineers.\n\nValidate the technical feasibility of UI/UX designs and provide regular technical guidance to support business and technical teams, contractors, and vendors.\n\nDevelop secure, high-quality production code, review and debug code written by others, and drive decisions influencing product design, application functionality, and technical operations.\n\nServe as a subject matter expert in one or more areas of focus and actively contribute to the engineering community as an advocate of firmwide frameworks, tools, and practices of the Software Development Life Cycle.\n\nInfluence peers and project decision-makers to consider the use and application of leading-edge technologies.\n\nDevelop and maintain dashboards, reports, and interactive visualizations using tools such as Tableau, ensuring data accuracy and integrity by implementing best practices in data visualization and management.\n\nStay current with industry trends and emerging technologies in data visualization and analytics, communicate complex data insights clearly to various audiences, including senior management, and manage a team of data visualization associates, providing guidance and mentorship to junior team members.\n\n\n\nRequired qualifications, capabilities, and skills\n\n\n\nFormal training or certification in software engineering concepts and 5+ years of applied experience.\n\n5+ Years of experience as a Web/UI Lead Architect\n\nProficiency in Javascript, Typescript, HTML, CSS\n\nExpert knowledge in ReactJs, Redux, React hooks.\n\nStrong understanding of front-end coding and development technologies\n\nHands-on practical experience delivering system design, application development, testing, and operational stability\n\nAdvanced knowledge of software applications and technical processes with considerable in-depth knowledge in UI and Web Technologies\n\nAbility to tackle design and functionality problems independently with little to no oversight\n\nPractical cloud native experience\n\nExperience in Computer Science, Computer Engineering, Mathematics, or a related technical field\n\n\n\nPreferred qualifications, capabilities, and skills\n\n\n\nFull stack development with Node/. NET/Java\n\nFamiliarity with working in event driven environments\n\nA good understanding of cross-browser compatibility issues and their solutions along with Typescript\n\nExperience working with Databases and ability to write SQL queries along with experience with messaging platforms\n\nBachelor s degree in data science, Computer Science, Information Systems, Statistics, or a related field.\n\nProblem solver and solution oriented. Strong written and verbal communication skills. Jira and Agile practices\n\nExperience with big data technologies and machine learning is a plus.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Front end', 'Coding', 'Javascript', 'Agile', 'System design', 'HTML', 'Application development', 'JIRA', 'Analytics']",2025-06-12 15:02:40
"Software Engineer III - Java, Spring, AWS",JPMorgan Chase Bank,3 - 10 years,Not Disclosed,['Bengaluru'],"As a Software Engineer III at JPMorgan Chase within the Consumer Community Banking Team, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm s business objectives.\nJob responsibilities\nExecutes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems\nCreates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems\nProduces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems\nProactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture\nContributes to software engineering communities of practice and events that explore new and emerging technologies\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 3+ years applied experience in Java, Springboot, Terraforms, AWS\nHands-on practical experience in system design, application development, testing, and operational stability.\nHands-on experience working with AWS stack/services, Java and Spring\nExperience of AWS RDS/Aurora Database/AWS EKS/ECS/Lambda\nKnowledge of AWS SQS/SNS, Terraform for AWS resource/service provisioning\nExperience building and debugging large-scale web services, and microservices based, Kubernetes-orchestrated applications.\nExperience of software applications and technical processes within AWS public cloud\nStrong communication skills and stakeholder management is must.\nExperience in providing technical leadership and guidance to the team.\nSolid understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security.\nDemonstrated knowledge of software applications and technical processes within a technical discipline (e. g. , cloud, artificial intelligence, machine learning, mobile\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies, web services is a plus",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System architecture', 'Front end', 'Coding', 'Debugging', 'Machine learning', 'Technical leadership', 'Agile', 'System design', 'Application development', 'Troubleshooting']",2025-06-12 15:02:42
"Software Engineer III - Java, AWS, Terraforms",JPMorgan Chase Bank,3 - 10 years,Not Disclosed,['Bengaluru'],"As a Software Engineer III at JPMorgan Chase within the Consumer Community Banking Team, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm s business objectives.\nJob responsibilities\nExecutes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems\nCreates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems\nProduces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems\nProactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture\nContributes to software engineering communities of practice and events that explore new and emerging technologies\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 3+ years applied experience in Java, AWS, Terraforms\nHands-on practical experience in system design, application development, testing, and operational stability\nProficient in coding in one or more languages\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nOverall knowledge of the Software Development Life Cycle\nSolid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\nDemonstrated knowledge of software applications and technical processes within a technical discipline (e. g. , cloud, artificial intelligence, machine learning, mobile, etc. )\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System architecture', 'Front end', 'Coding', 'Artificial Intelligence', 'Debugging', 'Machine learning', 'Agile', 'System design', 'Application development', 'Troubleshooting']",2025-06-12 15:02:45
Software Engineer II - Java AWS Terraforms,JPMorgan Chase Bank,3 - 10 years,Not Disclosed,['Bengaluru'],"You re ready to gain the skills and experience needed to grow within your role and advance your career and we have the perfect software engineering opportunity for you.\nAs a Software Engineer II at JPMorgan Chase within the Consumer Community Banking Team, you are part of an agile team that works to enhance, design, and deliver the software components of the firm s state-of-the-art technology products in a secure, stable, and scalable way. As an emerging member of a software engineering team, you execute software solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role.\nJob responsibilities\nExecutes standard software solutions, design, development, and technical troubleshooting\nWrites secure and high-quality code using the syntax of at least one programming language with limited guidance\nDesigns, develops, codes, and troubleshoots with consideration of upstream and downstream systems and technical implications\nApplies knowledge of tools within the Software Development Life Cycle toolchain to improve the value realized by automation\nApplies technical troubleshooting to break down solutions and solve technical problems of basic complexity\nGathers, analyzes, and draws conclusions from large, diverse data sets to identify problems and contribute to decision-making in service of secure, stable application development\nLearns and applies system processes, methodologies, and skills for the development of secure, stable code and systems\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 2+ years applied experience in Java, AWS, Terraforms\nHands-on practical experience in system design, application development, testing, and operational stability, with hands-on experience in developing and deploying applications on AWS\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nDemonstrable ability to code in one or more languages\nExperience across the whole Software Development Life Cycle\nExposure to agile methodologies such as CI/CD, Application Resiliency, and Security\nEmerging knowledge of software applications and technical processes within a technical discipline (e. g. , cloud, artificial intelligence, machine learning, mobile, etc. )\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Artificial Intelligence', 'Debugging', 'Machine learning', 'Agile', 'System design', 'Application development', 'Troubleshooting', 'Downstream']",2025-06-12 15:02:47
Software Engineer II,JPMorgan Chase Bank,3 - 10 years,Not Disclosed,['Bengaluru'],"You re ready to gain the skills and experience needed to grow within your role and advance your career and we have the perfect software engineering opportunity for you.\nAs a Software Engineer II at JPMorgan Chase within the Consumer Community Banking Team, you are part of an agile team that works to enhance, design, and deliver the software components of the firm s state-of-the-art technology products in a secure, stable, and scalable way. As an emerging member of a software engineering team, you execute software solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role.\nJob responsibilities\nExecutes standard software solutions, design, development, and technical troubleshooting\nWrites secure and high-quality code using the syntax of at least one programming language with limited guidance\nDesigns, develops, codes, and troubleshoots with consideration of upstream and downstream systems and technical implications\nApplies knowledge of tools within the Software Development Life Cycle toolchain to improve the value realized by automation\nApplies technical troubleshooting to break down solutions and solve technical problems of basic complexity\nGathers, analyzes, and draws conclusions from large, diverse data sets to identify problems and contribute to decision-making in service of secure, stable application development\nLearns and applies system processes, methodologies, and skills for the development of secure, stable code and systems\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 2+ years applied experience in Java, AWS, Terraforms\nHands-on practical experience in system design, application development, testing, and operational stability\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nDemonstrable ability to code in one or more languages\nExperience across the whole Software Development Life Cycle\nExposure to agile methodologies such as CI/CD, Application Resiliency, and Security\nEmerging knowledge of software applications and technical processes within a technical discipline (e. g. , cloud, artificial intelligence, machine learning, mobile, etc. )\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Artificial Intelligence', 'Debugging', 'Machine learning', 'Agile', 'System design', 'Application development', 'Troubleshooting', 'Downstream']",2025-06-12 15:02:50
SOC Thermal engineer,Qualcomm,1 - 3 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Hardware Engineer, you will plan, design, optimize, verify, and test electronic systems, bring-up yield, circuits, mechanical systems, Digital/Analog/RF/optical systems, equipment and packaging, test systems, FPGA, and/or DSP systems that launch cutting-edge, world class products. Qualcomm Hardware Engineers collaborate with cross-functional teams to develop solutions and meet performance requirements.\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field.\n\nJob Function:\n\nSOC Thermal analysis methodology developmentencompassing SoC, package and systems, for various Qualcomm products going into smartphones, laptops, automotives and virtual reality headsets.\n\nDeveloping strategies to predict and improve thermally constrained scores (AnTuTu/Geekbench) and also measure/tune it on post-Si platforms, including infrared imaging.\n\nThermal measurement The candidate will be responsible for improving thermal mitigation strategies through power and thermal management to achieve optimal system thermal performance.\n\n\n\n\nSkills:\n\n\nShould possess strong analytical skills and mathematical modeling abilities.\n\nKnowledge of SOC stack up is must\n\nGood knowledge of scripting in PERL/Python required; some exposure to Machine Learning algorithms/frameworks will be a plus.\n\nNeed to have thorough knowledge of heat transfer mechanisms and CFD.\n\nProficiency with state-of-the-art thermal analysis tools- Ansys Icepak/Flotherm\n\nThermal Measurement will be preferred skill\n\n\nExperience: BE/ME graduates with 1 to 3 years of experience.",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['python', 'soc', 'machine learning', 'mathematical modeling', 'perl', 'dsp', 'data analytics', 'natural language processing', 'hardware engineering', 'sql', 'deep learning', 'r', 'fpga', 'data science', 'predictive modeling', 'machine learning algorithms', 'logistic regression', 'statistics']",2025-06-12 15:02:52
Auto AI Systems Engineer,Qualcomm,1 - 6 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nSummary - We are seeking experts with a robust background in the field of deep learning (DL) to design state-of-the-art low-level perception (LLP) as well as end-to-end AD models, with a focus on achieving accuracy-latency Pareto optimality. This role involves comprehending state-of-the-art research in this field and deploying networks on the Qualcomm Ride platform for L2/L3 Advanced Driver Assistance Systems (ADAS) and autonomous driving.\n\nThe ideal candidate must be well-versed in recent advancements in Vision Transformers (Cross-attention, Self-attention), lifting 2D features to Bird's Eye View (BEV) space, and their applications to multi-modal fusion. This position offers extensive opportunities to collaborate with advanced R&D teams of leading automotive Original Equipment Manufacturers (OEMs) as well as Qualcomm's internal stack teams. The team is responsible for enhancing the speed, accuracy, power consumption, and latency of deep networks running on Snapdragon Ride AI accelerators.\n\nA thorough understanding of machine learning algorithms, particularly those related to automotive use cases (autonomous driving, vision, and LiDAR processing ML algorithms), is essential. Research experience in the development of efficient networks, various Neural Architecture Search (NAS) techniques, network quantization, and pruning is highly desirable.\n\nStrong communication and interpersonal skills are required, and the candidate must be able to work effectively with various horizontal AI teams.\n\nMinimum Qualifications:\n\nBachelor's degree in Computer Science, Engineering, Information Systems, or related field and 1+ years of Hardware Engineering, Software Engineering, Systems Engineering, or related work experience.ORMaster's degree in Computer Science, Engineering, Information Systems, or related field and 1+ year of Hardware Engineering, Software Engineering, Systems Engineering, or related work experience.ORPhD in Computer Science, Engineering, Information Systems, or related field.\n\nPreferred Qualifications:\nGood at software development with excellent analytical, development, and problem-solving skills.\nStrong understanding of Machine Learning fundamentals\nHands-on experience with deep learning network design and implementation. Ability to define network from scratch in PyTorch, ability to add new loss function, modify network with torch.fx. Adept at version control system like GIT.\nExperience in neural network quantization, compression, pruning algorithms.\nExperience in deep learning kernel/compiler optimization\nStrong communication skills\n\n\nPrincipal Duties and Responsibilities:\n\nApplies Machine Learning knowledge to extend training or runtime frameworks or model efficiency software tools with new features and optimizations.\n\nModels, architects, and develops machine learning hardware (co-designed with machine learning software) for inference or training solutions.\n\nDevelops optimized software to enable AI models deployed on hardware (e.g., machine learning kernels, compiler tools, or model efficiency tools, etc.) to allow specific hardware features; collaborates with team members for joint design and development.\n\nAssists with the development and application of machine learning techniques into products and/or AI solutions to enable customers to do the same.\n\nDevelops, adapts, or prototypes complex machine learning algorithms, models, or frameworks aligned with and motivated by product proposals or roadmaps with minimal guidance from more experienced engineers.\n\nConducts complex experiments to train and evaluate machine learning models and/or software independently.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hardware engineering', 'machine learning', 'software engineering', 'machine learning algorithms', 'system engineering', 'switching', 'pruning', 'algorithms', 'eigrp', 'kernel', 'rstp', 'networking', 'vrrp', 'ospf', 'deep learning', 'routing', 'git', 'computer science', 'vlan', 'adas', 'hsrp', 'control system']",2025-06-12 15:02:54
XR Perception Systems Engineer,Qualcomm,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nAt Qualcomm XR Perception, were a passionate team of engineers who want to change the world through virtual and augmented reality products and technologies. We develop state-of-the-art, power efficient XR perception algorithms optimized for Qualcomm hardware. The perception algorithms enable XR systems to map the users environment and track the user within the environment.\n\nTo scale and strengthen our offering, Qualcomm XR Perception team in Bengaluru is rapidly expanding and seeking candidates to investigate and develop the fundamental perception algorithm that enables self-contained XR platforms. We are looking for innovators who will push the boundaries of mobile perception technology to offer a robust platform to our customers.\n\nJob Function/General Responsibilities\n\nIn this job the candidate would\n\nBe part of a world-class XR team researching and developing XR perception\n\nWork with building blocks of Simultaneous Localization and Mapping (SLAM)\n\no Multi-view geometry\n\no Sensor Fusion\n\no Pose estimation\n\no Non-linear optimization\n\nApply ML/ DL techniques to enhance accuracy and robustness of traditional SLAM systems\n\nImplement efficient and optimized algorithms in C++\n\nUnderstand and analyze requirements for perception systems for XR platforms\n\nHave excellent verbal and written communication and presentation skills\n\n\n\nCandidate that fits this role should be well versed in the basics of\n\nLinear algebra\n\nProbability\n\nMulti-view geometry\n\nSensor fusion\n\nEstimation techniques\n\nNon-linear optimization\n\nMachine learning techniques\n\nObject oriented programming in C++\n\n\n\nKeywords\n\nVirtual reality, Augmented reality, Computer vision, Machine learning, Perception, VIO, SLAM\n\n\n\nEducational requirements:\n\nBachelor's degree in Engineering, Applied Mathematics, Computer Science, or related field\n\n+3 years of work experience OR Master's degree in Engineering, Applied Mathematics, Computer Science, or related field\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['presentation skills', 'fusion', 'geometry', 'sensor', 'linear algebra', 'algorithms', 'c++', 'python', 'machine learning', 'estimation', 'slam', 'deep learning', 'tensorflow', 'computer science', 'computer vision', 'keras', 'object oriented programming', 'vio', 'system engineering']",2025-06-12 15:02:57
Software Development Engineer,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Own feature enhancements end to end, collaborate with product and science , design, develop and launch in production.\nParticipate in design reviews/code reviews and contribute to the teams deliverables\nA Software Development Engineer in Alexa Sensitive Content Intelligence will develop the design for complex, cross functional, multimodal and multi locale applications. The engineer in this role will be the owner of the technical areas supporting Generative AI Large Language Model (LLM) based services and customer facing applications. An Software Development Engineer will be forging alliance with upstream and downstream engineering teams to deliver the services and create mechanisms for end-to-end integration. You will build low-latency services that are optimized for performance to meet the classic Alexa and LLM based customer interaction expectations.\nAn Software Development Engineer in Alexa Sensitive Content Intelligence will own full stack software development and contribute to all aspects of an agile software development lifecycle, including design, architecture, development, documentation, testing and operations. You will work with Scientists, Engineers and Product managers and influence the design of the customer facing features and products.",,,,"['Coding', 'Machine learning', 'Architectural design', 'Agile', 'Software development life cycle', 'Customer experience', 'Internship', 'Downstream', 'Customer interaction']",2025-06-12 15:02:59
Engineering - L2 - Associate-Software Engineering,Goldman Sachs,2 - 5 years,Not Disclosed,['Bengaluru'],"The Entitlements Platform team sits at the very heart of Goldman Sachs, responsible for protecting the firm against the risks associated with both over-privilege and under-privilege. We are the guardians of access, ensuring that only the right people have access to the right data at the right time. Our team builds the critical security controls that facilitate the firms mandatory adherence to regulatory and compliance policies. We are an elite team of full-stack developers that provide hosted applications, standalone libraries, and customizable workflow tooling to empower the firms authorization needs for applications and their associated data.\nWe are seeking passionate software engineers who crave more than just coding. We need individuals who are driven to build robust control solutions and understand the power of platforms to drive enterprise-wide leverage. We are looking for individuals who are not only technically skilled, but also possess the leadership qualities, strategic thinking, and communication skills to drive change and make a real impact on the firm.\n  As a Software Engineer on the team, you will:\nDesign, develop, and test cross-platform software solutions that are secure, scalable, and maintainable\nBuild applications, libraries, and services to manage the lifecycle of application privileges\nProvide externalized authorization capabilities and allow secure, auditable access to the production environment\nCollaborate with colleagues across diverse technology teams to understand their needs and build solutions that meet their requirements\nBe a proactive problem solver, identifying potential issues and developing mitigation plans\nContinuously learn and stay current with the latest technologies and industry best practices\nTake ownership of your work and be accountable for its success\nRequired Qualifications:\nStrong core and server-side Java software development experience\nExperience implementing RESTful APIs and microservice architecture\nFamiliarity with SQL and/or NoSQL databases, database design principles and object-relational mapping frameworks\nThe ability to communicate technical concepts effectively, both in writing and orally, as we'll as the interpersonal skills required to collaborate effectively with colleagues across diverse technology teams\nProficiency in designing, developing, and testing cross-platform software\nSolid experience of version control, continuous integration, deployment, and configuration management tools\nThe ability to understand and effectively debug both new and existing software\nSelf-motivated and ability to work in a high-paced environment with tight deadlines\nAn understanding of customer-first and data driven design\nPreferred Qualifications:\nA grounding in identity and privilege management concepts, including relationships between employee and privilege lifecycle, authorization vs. authentication, and segregation of duties\nExpertise in Web display technologies, particularly modern JavaScript libraries and frameworks (eg, React/Redux), CSS (including experience with SASS, LESS, and/or PostCSS), and HTML\nExperience with Kubernetes and container management\nProficiency in Apache Kafka and its core principles",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Coding', 'Database design', 'Configuration management', 'Machine learning', 'Javascript', 'HTML', 'Investment banking', 'Apache', 'SQL']",2025-06-12 15:03:02
Sr Engineer,Qualcomm,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nCPU architecture team is driving the core math libraries needed for ML/AI acceleration. This position/s will expose you to Qualcomms cutting edge SoC and ML/AI platforms in the industry. Participate in Optimizing the core ML kernels using the latest advancements like SME, SVE of the ARM CPU architecture and enhance the performance of the ML models on the CPU of the QCOM Soc\n\nRequired Skills==\n3+ experince with Understanding of ARM CPU architecture fundamentals and ARM Arch64 ISA\nOptimizing kernels for vector Processors\nUnderstanding of the basic linear algebra functions used in AI/ML\nAlgorithm design (logic, critical thinking)\nPerformance Evaluation and Optimization of the applications for ARM architecture\nInferencing of the ML models written in Pytorch/TensorFlow/Keras\nUnderstanding of the typical Open-Source Library framework design\n\n\n\nPreferred Skills===\nStrong Programming skills and deep understanding of the ARM ISA\nunderstanding of the algorithms suitable for Vector and matrix accelerators\nStrong Analytical and debugging skills\nGood understanding of Optimizing the Linear Algebra algorithms\nPerformance evaluation using QEMU, Simulators, Emulators and on Real Hardware\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cpu', 'isa', 'debugging', 'software engineering', 'arm', 'algorithms', 'python', 'c++', 'c', 'natural language processing', 'soc', 'neural networks', 'machine learning', 'artificial intelligence', 'deep learning', 'tensorflow', 'java', 'computer science', 'computer vision', 'pytorch', 'keras', 'dhcp']",2025-06-12 15:03:05
"Software Development Engineer II, International Emerging Stores",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Shaping the Future of Global E-commerce!!\n\nAt Amazons International Emerging Stores (IES), were reinventing how millions of customers discover and shop online. Our team is architecting foundational platforms and mechanisms that will power Amazons next generation of shopping experiences globally. Were tackling intrinsically hard problems at the intersection of AI, personalization, and scalable systems challenges that require innovative solutions while maintaining our commitment to operational excellence. Our charter extends beyond traditional e-commerce boundaries, focusing on creating competitive advantages through technical innovation. Were building solutions that not only serve immediate business needs but establish new patterns and practices that can be adopted across Amazon. Our work demands deep technical judgment, cross-organizational collaboration, and the ability to influence at the highest levels of the organization.\n\nThe Opportunity: Software Development Engineer\n\nAt IES, were building the future of retail, and were looking for a talented Software Development Engineer to join our innovative team. As an SDE, youll be instrumental in revolutionizing how customers make purchase decisions across our retail platform by developing next-generation shopping experiences powered by artificial intelligence and adaptive technologies.\n\nIn this role, youll design and implement intelligent systems that deliver personalized shopping experiences, working with cutting-edge generative AI and ML models to create innovative visual experiences. Youll develop sophisticated algorithms for adaptive layout optimization, product visualization features, theme-based recommendation systems, and customer behavior analysis. Your work will span multiple customer touchpoints, requiring you to write high-quality, scalable code while collaborating with product managers, designers, and data scientists to drive technical solutions.\n\nWere seeking someone with strong programming skills and software design expertise, particularly in distributed systems and scalable architectures. Knowledge of AI/ML technologies and their practical applications is essential, as is the ability to translate complex business requirements into technical solutions. Youll participate in architecture discussions, technical design reviews, and contribute to the continuous improvement of customer experience metrics while debugging complex production issues and optimizing system performance.\n\nThis position offers an exciting opportunity to work on cutting-edge technologies while solving complex engineering challenges that impact millions of customers globally. Youll be part of a team that values technical excellence and innovation, with the chance to shape the future of e-commerce through technological advancement.\n\n3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'Software design', 'Operational excellence', 'Software Development Engineer II', 'Coding', 'Artificial Intelligence', 'Debugging', 'Continuous improvement', 'Internship', 'Distribution system']",2025-06-12 15:03:07
AutoIT Solutioning Engineer-Staff,Qualcomm,3 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nAs a Site Reliability Engineer (SRE), youll be part of a highly collaborative team focused on provisioning and maintaining infrastructure and services with stability, sustainability, and security always on your mind. You will work in a self-guided, cross-functional team responsible for everything from modernizing traditional services and applications to deploying new technology. You'll collaborate closely with software engineers, data scientists, and product managers to maintain and optimize our systems. If you're passionate about automotive technology, software reliability, and continuous improvement, this role is perfect for you.\n\nYour Guiding Principles:\n\n\nAutomationYou understand the power of automation and ""infrastructure as code"" concepts. Automation is your primary consideration in problem-solving.\n\n\nCollaboration: You share a common language with fellow engineers, understand their needs, and thrive working in a high trust collaborate culture in which people are rewarded for taking risks.\n\n\nData-drivenYou understand why decisions are supported by facts and not opinions. You have experience applying logical approach to decision making. Skilled at metric collection and using that data to drive change.\n\n\nDebuggingYou understand debugging principles and are adept at applying them routinely and successfully.\n\n\nDevSecOps: You understand that DevSecOps is a culture which needs to be cultivated and you can help nurture those philosophies.\n\n\nSecurityYou know how to layer appropriate security within solutions across the lifecycle. You understand the security implications and consequences of any deployment.\n\n\nSelf-Driven: You understand how to prioritize work and time allocation at a personal and team level.\n\n\nStability: You know what it means to deliver a service with a high degree of reliability and are intimately familiar with how disruptions impact consumers.\n\n\nSustainability: You avoid one off solutions which are challenging to support. Instead, your solutions are aligned with team goals and strategic vision. You routinely dedicate cycles to reducing technical debt.\n\n\nWhat you have:\nExtensive Linux experience with servers and workstations. You can easily navigate the CLI, knowledgeable with typical Linux troubleshooting tools, and have a broad understanding of Ubuntu and RedHat.\nThe ability to automate through scripting languages such as Python, Bash, Go, etc.\nThe skill to provide sufficient automated test coverage of various implementations.\nYou have familiarity with Jenkins, Puppet, Splunk, JIRA, Vault, Docker, AWS, Cloud services, etc.\nAbility to respond rapidly to changing landscapes while providing stable, reliable, and secure services to customers.\nYou have a passion for continuous learning and leverage the scientific method to ensure nothing is taken for granted.\n\n\nResponsibilities:\nSystem Monitoring and Incident Response:\nMonitor system health, detect anomalies, and respond promptly to incidents.\nInvestigate and troubleshoot issues related to services.\nImplement proactive measures to prevent service disruptions.\nInfrastructure Automation:\nDevelop and maintain infrastructure-as-code (IaC) scripts for deployment and scaling.\nAutomate routine tasks to improve efficiency and reduce manual intervention.\nPerformance Optimization:\nCollaborate with development teams to optimize software performance.\nIdentify bottlenecks and implement solutions to enhance system speed and reliability.\nCapacity Planning:\nForecast resource requirements based on traffic patterns and business growth.\nScale infrastructure to accommodate increasing demand.\nSecurity and Compliance:\nEnsure compliance with industry standards and best practices.\nImplement security controls and participate in security audits.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['docker', 'linux', 'python', 'puppet', 'aws', 'kubernetes', 'owasp', 'golang', 'redhat linux', 'vulnerability assessment', 'ansible', 'microservices', 'java', 'devops', 'jenkins', 'debugging', 'penetration testing', 'vault', 'jira', 'cloud services', 'ubuntu', 'microsoft azure', 'splunk', 'bash', 'devsecops', 'terraform']",2025-06-12 15:03:10
Senior Software Engineer - AI/ML,Qualcomm,5 - 10 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nThe AI SW team at Qualcomm is focused on advancing state-of-the-art in Artificial Intelligence across various business segments, including Mobile, AR & VR Technology, IoT, and Auto ADAS. The AISW stack leverages Qualcomm chips' extensive heterogeneous computing capabilities, enabling the running of trained neural networks on devices without needing a cloud connection. This allows neural network models trained in various frameworks on Snapdragon platforms to run at blazing speeds while consuming minimal power. As a Senior Lead Engineer, you will see your work directly impact billions of devices worldwide.\n\nKey Responsibilities:\n\nDesign, develop, and maintain high-quality software solutions using Python for running machine learning models on Qualcomm devices.\n\nContribute to the development and optimization of AI models, using popular frameworks like Pytorch.\n\nBuild tools and infrastructure for onboarding, debugging and analysis of AI models.\n\nParticipate in code reviews and ensure adherence to best practices and coding standards.\n\nDebug accuracy and performance on devices, addressing any challenges that arise.\n\nCollaborate with cross-functional teams to define, design, and ship new features.\n\nOwn end-to-end development and release of features and lead and mentor a sub-team of engineers.\n\n\nMinimum Qualifications:\n\nBachelors degree in engineering, Computer science or a related field and 5+ years of professional experience in software engineering or related work experience\n\nOR\n\nMasters degree in engineering, Computer science or a related field and 4+ years of experience of Software engineering or related work experience\n\nSolid understanding of fundamental computer science concepts, general programming principles and practices.\n\n4+ years of hands-on professional experience in programming with Python (preferred) / Java / C++.\n\nStrong problem-solving skills and the ability to work independently and as part of a team.\n\nBasic knowledge of AI concepts and techniques\n\nExcellent communication skills and the ability to articulate complex technical concepts.\n\nWillingness to learn advanced concepts in AI and machine learning and keep updated with latest industry trends\n\n\nPreferred Qualifications:\n\nExperience with machine learning frameworks and tools such as PyTorch, ONNX.\n\nFamiliarity with Large Language Models (LLMs) and Transformers\n\nFamiliarity working with Linux systems and hardware devices\n\nExperience with mobile development frameworks and tools (e.g., Android SDK, Kotlin).\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'computer science', 'software engineering', 'system engineering', 'algorithms', 'c++', 'natural language processing', 'android', 'kotlin', 'c+', 'aiml', 'artificial intelligence', 'iot', 'deep learning', 'tensorflow', 'java', 'linux', 'computer vision', 'pytorch', 'onnx', 'ml']",2025-06-12 15:03:12
Sr Staff Video Codec Systems Engineer,Qualcomm,15 - 20 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nResponsibilities:\n\nPart of video IP systems team and will be responsible for video technology roadmap ; interaction with different teams including design, verification, system, firmware, software, SOC and power teams; video algorithms, image quality analysis; video processing and compression trends with standardization committees.\n\nQualcomm is the innovation leader in the area of integrated chipsets that power advanced mobile devices, XR/IoT/Automotive & compute platforms. We are building on and expanding our reputation as the industry powerhouse for innovation in both wireless technologies and enabling advanced multimedia capabilities. We are seeking experienced system engineers for our cutting-edge efforts in the architecture and design of our video codec hardware. The video Systems group provides video solutions on all of Qualcomms Snapdragon mobile processors. The teams scope includes video processing algorithms and IP architecture design for video compression, visual signal processing and analytics, with power and performance optimization.\n\nThe selected candidate, along with his/her colleagues and other team members, will have responsibilities in one or more of the following areas:\n\nDesigning and evaluating video algorithms to be implemented in hardware video encoders and decoders .\n\nDefine systems architecture for video solutions including data flow, task partition, interface and systems interoperation.\n\nImplement models to accurately model the HW (functional, performance), and supporting HW verification & SW development via behavioral model vectors .\n\nCollaborate with systems, software, hardware teams at various stages of chipset life in design/validation/commercialization.\n\nResearch and develop video algorithms for mobile, automotive, compute and VR/AR applications with performance and power efficiency.\n\n\nMinimum Qualifications:\n\nMasters degree in Electrical/Electronics Engineering, Computer Science, or related field and 15+ years of systems engineering experience\n\nPhD in Electrical/Electronics Engineering, Communications - Signal Processing, Computer Science, or related field and 12+ years of systems engineering experience\n\nKnowledge & Experience in video coding standards such as VVC, AV1, HEVC, H.264/AVC, VP9.\n\nHands on Knowledge & Experience in Video Codec Design and implementation with in-depth understanding of codec algorithms and flow\n\nSolid C/C++ programming, Python scripting skills.\n\nStrong communication skills\n\nGood analytical and problem solving skills.\n\n\nPreferred Qualifications:\n\nHW C modeling experience\n\nImage quality evaluation and metric comparisons\n\nSignal / Image processing basicsComputer Vision and Machine Learning algorithms for Video Compression and Video/Image processing.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 8+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 7+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 6+ years of Systems Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'c++', 'system engineering', 'python', 'c', 'dsp', 'cuda', 'vod', 'dvb', 'embedded systems', 'linux', 'hevc', 'data structures', 'multithreading', 'ffmpeg', 'stb', 'image processing', 'matlab', 'video codecs', 'machine learning', 'ott', 'av', 'mpeg', 'computer vision', 'embedded c', 'h264', 'vp']",2025-06-12 15:03:15
Wealth Management-Bengaluru-Associate-Software Engineering,Goldman Sachs,3 - 5 years,Not Disclosed,['Bengaluru'],"Associate GenAI Developer\nWe are seeking a highly skilled GenAI Developer to join our dynamic, global team. The ideal candidate will have a strong background in applied generative AI. This role will involve developing and implementing AI solutions, working with various technologies, and collaborating with cross-functional teams to drive innovation. The GenAI Developer will play a crucial role in advancing our GenAI capabilities and contributing to the success of our Wealth Management division.\nKey Responsibilities:\nWork with stakeholders to understand requirements and deliver AI solutions across several domains in Wealth Management.\nStay updated with the latest advancements in AI and machine learning technologies.\nConduct research and experiments to improve AI capabilities within the division.\nRequired Competencies:\nRetrieval-Augmented Generation (RAG): Experience in developing and implementing RAG models to enhance information retrieval and generation tasks.\nVector Stores: Knowledge of Vector Stores for efficient data storage and retrieval.\nPrompt Engineering: Skills in designing and optimizing prompts for AI models to improve accuracy and relevance.\nLarge Language Model APIs (LLM APIs): Understanding of different LLMs, both commercial and open source, and their capabilities (e.g., OpenAI, Gemini, Llama, Claude).\nProgramming Languages: Proficiency in Python, Java, or other relevant programming languages.\nData Analysis: Strong analytical skills and experience with data analysis tools.\nProblem-Solving: Excellent problem-solving abilities and attention to detail.\nCommunication: Strong verbal and written communication skills.\nPreferred Competencies:\nGraph RAG: Proficiency in using Graph RAG for complex data relationships and insights.\nKnowledge Graphs: Expertise in building and managing Knowledge Graphs to represent and query complex data structures.\nMachine Learning Frameworks: Experience with TensorFlow, PyTorch, or similar frameworks.\nExperience with cloud platforms such as AWS, Google Cloud, or Azure.\nFamiliarity with natural language processing (NLP) and computer vision technologies.\nPrevious experience in a similar role or industry.\nMaster s or Ph.D. in Computer Science, Data Science, or a related field.\nYears of experience: 3-5",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Computer vision', 'Data analysis', 'Wealth management', 'Machine learning', 'Data structures', 'HTML', 'Investment banking', 'Open source', 'Python']",2025-06-12 15:03:18
Wealth Management - Vice President-Software Engineering,Goldman Sachs,6 - 10 years,Not Disclosed,['Bengaluru'],"We are seeking a highly skilled GenAI Developer to join our dynamic, global team. The ideal candidate will have a strong background in applied generative AI. This role will involve developing and implementing AI solutions, working with various technologies, and collaborating with cross-functional teams to drive innovation. The GenAI Developer will play a crucial role in advancing our GenAI capabilities and contributing to the success of our Wealth Management division.\n  Key Responsibilities:\nWork with stakeholders to understand requirements and deliver AI solutions across several domains in Wealth Management.\nStay updated with the latest advancements in AI and machine learning technologies.\nConduct research and experiments to improve AI capabilities within the division.\nRequired Competencies:\nRetrieval-Augmented Generation (RAG): Experience in developing and implementing RAG models to enhance information retrieval and generation tasks.\nVector Stores: Knowledge of Vector Stores for efficient data storage and retrieval.\nPrompt Engineering: Skills in designing and optimizing prompts for AI models to improve accuracy and relevance.\nLarge Language Model APIs (LLM APIs): Understanding of different LLMs, both commercial and open source, and their capabilities (eg, OpenAI, Gemini, Llama, Claude).\nProgramming Languages: Proficiency in Python, Java, or other relevant programming languages.\nData Analysis: Strong analytical skills and experience with data analysis tools.\nProblem-Solving: Excellent problem-solving abilities and attention to detail.\nCommunication: Strong verbal and written communication skills.\nPreferred Competencies:\nGraph RAG: Proficiency in using Graph RAG for complex data relationships and insights.\nKnowledge Graphs: Expertise in building and managing Knowledge Graphs to represent and query complex data structures.\nMachine Learning Frameworks: Experience with TensorFlow, PyTorch, or similar frameworks.\nExperience with cloud platforms such as AWS, Google Cloud, or Azure.\nFamiliarity with natural language processing (NLP) and computer vision technologies.\nPrevious experience in a similar role or industry.\nmasters or Ph.D. in Computer Science, Data Science, or a related field.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer vision', 'Data analysis', 'Wealth management', 'Machine learning', 'Data structures', 'HTML', 'Investment banking', 'Open source', 'Python']",2025-06-12 15:03:20
Automotive cybersecurity engineer Sr/Staff,Qualcomm,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Hardware Engineer, you will plan, design, optimize, verify, and test electronic systems, bring-up yield, circuits, mechanical systems, Digital/Analog/RF/optical systems, equipment and packaging, test systems, FPGA, and/or DSP systems that launch cutting-edge, world class products. Qualcomm Hardware Engineers collaborate with cross-functional teams to develop solutions and meet performance requirements.\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 4+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 3+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 2+ years of Hardware Engineering or related work experience.\n\n\nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in. systems architecture.ADAS/Autonomy/Infotainment Qualcomm is utilizing its traditional strengths in digital wireless technologies to play a central role in the evolution of automotive infotainment and autonomous driving. We are investing in several supporting technologies including 4G, 5G, ADAS, and Deep Learning. The Qualcomm Automotive Security team is engaged in offering optimized solutions built on Safety CPU cores, DSP, computer vision and machine learning algorithms for the Qualcomm\n\nWe are seeking engineers with experience in system and SoC level automotive cybersecurity concepts and implementations and knowledge of the ISO21434 cyber security standards and process. This position will be a hands-on role in in analyzing and improving the current cybersecurity hardware development process to meet ISO 21434. Candidate must have thorough knowledge on ISO 21434 and ISO 26262 standards to independently perform functional safety and cybersecurity audits and assessments.\n\nResponsibilities shall include the following\n\nDevelopment/ enhancement of hardware development process and related work products meeting cyber security standards.\n\nProvide training on ISO 21434/ ISO 26262 with respect to application of Qualcomms best practices.\n\nFacilitate TARA and VARA and review them\n\nIndependently carry out cybersecurity audits and assessments, guiding the hardware design and development teams in the process. Provide feedback and identify process improvements and follow up with internal teams.\n\nWork closely with SoC Design and IP teams, 3rd party IP vendors, Software team, the functional safety and cybersecurity manager(s), Quality team as well as customers to ensure the defined process is deployed.\n\nParticipate in external and customer cybersecurity and functional safety audits and assessments and assist in corrective actions.\n\nMaintain a strong knowledge of Automotive Industry cybersecurity best practices. Influence internal stakeholders with actionable data to ensure gaps to expectations are closed in a timely fashion.\n\nEstablish and maintain healthy relationships with influential/decision making on Cybersecurity, Functional Safety, and ongoing Quality assessments throughout the organization.\n\nFacilitate in qualitative and quantitative safety analyses and review of them.\n\nMinimum Qualifications\n\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field.\n\n8+ years Systems Engineering / VLSI design or related work experience.\n\nHas knowledge of product development flow and validation.\n\nHas led audits of different development processes and is comfortable with assessments and reporting Quality metrics.\n\nHands-on experience with automotive quality processesISO 21434, ISO 26262, TARA, VARA, DFMEA, FTA, FMEDA, 8D, quality agreements, PCNs / EOLs, customer audits, AEC- Q100 requirements, ISO9001, IATF 16949, etc.\n\nStructured problem-solving capability and ability to work with teams on root cause analyses.\n\nExcellent verbal/written communication, interpersonal skills, and presentation skills",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['vlsi design', 'presentation skills', 'hardware engineering', 'dfmea', 'system engineering', 'matlab', 'c++', 'python', 'c', 'hiring', 'networking', 'vhdl', 'verilog', 'staffing', 'talent acquisition', 'technical support', 'recruitment', 'desktop engineering', 'embedded systems', 'desktop support']",2025-06-12 15:03:23
Automotive Platform - Engineer Sr.,Qualcomm,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nQualcomm ADAS/Autonomy team is engaged in offering optimized solutions built on DSP, computer vision and machine learning algorithms for the Qualcomm ADAS/Autonomy SoCs. We are seeking engineers with experience in system and SoC SW level functional safety concepts. The job requires understanding and defining of the Safety Concept and Architecture, Software Safety requirements, defining and deploying safety processes and development of Safety software by following the ISO26262 software processes. Interaction with customers, architects and test/integration teams are required as part of the job. The job also involves working with the Software quality team for adherence of ISO26262 and ASPICE processes.\n\nIn this role, the candidate will work with local and global teams to understand, define and implement and productize Automotive specific features including software enablement (drivers/BSP/RTOS/AUTOSAR MCAL), security, functional safety, and power applied to Automotive products on our current and next generation SoCs. The candidate will also have the responsibility to coordinate and execute plans which will encompass validation of all the feature requirements. The Candidate will have the responsibility to identify and address any abnormal discoveries by root-causing and providing detailed corrective actions in the form of optimizations and/or fixes. When possible, the candidate is expected to prototype and pre-validate recommended fixes. Additionally, the candidate will be responsible for any automation of design under test along with validation efforts and working closely with design/production/bench IP teams.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\n 3-6 years of Embedded Software Development experience, including low level drivers, and RTOS \n\n The candidate should possess 3 to 6 years of industry experience in embedded software driver development and having expertise in one or more below areas would be preferred: \n\n Should be able to ramp up fast and must have the attitude to work with the team. \n\n Strong C and Assembly Programming with OS & Multi-Processor concepts \n\n Embedded software development in C and C++ on ARM or similar cores. \n\n Hands on experience of driver development on any RTOS, \n\n Experience in SafeRTOS/FreeRTOS based development is nice to have \n\n Experience in Autosar MCAL development is nice to have \n\n Experience in Autosar BSW integration and validation is nice to have \n\n ARM Trust-Zone & ARMv7/v8 architecture. \n\n Good debugging skills with experience on debugging with Lauterbach JTAG debuggers. \n\n Work on challenging customer requirements and issues. \n\n Basic understanding one or more of hardware blocks - Clocks, PLLs, GPIO, Interrupt Controllers (GIC), Peripherals (SPI/I2C/UART/CAN/Ethernet/Clock/etc)  \n\n Automotive SW development experience is must have \n\n Experience in ISO26262/functional safety and ASPICE is highly desirable \n\n Basic knowledge on Power Mgmt. IC is desirable \n\n Knowledge of Software/Hardware Security concepts is desirable \nClosely work with the hardware team to contribute/suggest modifications to the hardware design.\nAny past working experience on Qualcomm chips nice to have",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['assembly programming', 'c', 'sw', 'embedded software development', 'embedded software', 'aspice', 'python', 'c++', 'canoe', 'spi', 'freertos', 'autosar', 'mcal', 'rtos', 'java', 'embedded systems', 'embedded c', 'uart', 'debugging', 'software engineering', 'ic', 'i2c', 'can bus', 'functional safety']",2025-06-12 15:03:25
Senior Staff Engineer - AI-IOT Product,Infineon,6 - 8 years,Not Disclosed,['Bengaluru'],"We are seeking an experienced AI-IOT Product Engineer to design and develop AI and IoT products, with a focus on technical product management and product design. This role requires a strong technical background, excellent problem-solving skills, and the ability to collaborate with cross-functional teams to deliver innovative solutions.\n\nJob Description\nProduct Design: Design and develop AI and IoT product concepts, including hardware, firmware, and software components. This includes:\nDeveloping product requirements and technical specifications.\nCollaborating with engineering teams to design and develop product architectures.\nEnsuring product designs meet technical and business requirements.\nTechnical Product Management: Manage the technical aspects of AI and IoT products, including:\nDefining and prioritizing product features and requirements.\nCollaborating with engineering teams to develop and deploy products.\nEnsuring products meet quality and reliability standards.\nSensor Algorithm Development: Collaborate with engineering teams to develop and implement sensor algorithms that enable accurate and efficient data collection and processing. This includes:\nProviding technical guidance on sensor fusion algorithms and machine learning models.\nCollaborating with engineering teams to integrate sensor algorithms with IoT devices and platforms.\nLLM RAG Systems: Collaborate with engineering teams to design and develop Large Language Model (LLM) Retrieval-Augmented Generation (RAG) systems that enable efficient and effective natural language processing. This includes:\nProviding technical guidance on LLM models and RAG systems.\nCollaborating with engineering teams to integrate LLM RAG systems with IoT devices and platforms.\nApplied Deep Learning: Collaborate with engineering teams to develop and implement deep learning models that enable accurate and efficient data analysis and processing. This includes:\nProviding technical guidance on deep learning models and architectures.\nCollaborating with engineering teams to integrate deep learning models with IoT devices and platforms.\nTechnical Road mapping: Develop and maintain technical roadmaps for AI and IoT products, including:\nIdentifying and prioritizing technical requirements and features.\nCollaborating with engineering teams to develop and deploy products.\nYou are best equipped for this task if you have:\n6-8 years of experience in AI, IoT, or related fields.\nBachelors or Masters degree in Computer Science, Electrical/Electronics Engineering, or a related field.\nExcellent problem-solving skills and attention to detail.\nExperience with AI and IoT technologies, such as AWS or Azure.\nStrong technical knowledge of sensor algorithms, LLM RAG systems, and applied deep learning.\nExperience with technical product management and product design.\nExperience with agile development methodologies.\nKnowledge of IoT protocols and technologies (eg, MQTT, CoAP, BLE, Zigbee, LoRa, NB-IoT).\nExperience with data analytics and machine learning.\nCertifications in AI or IoT technologies (eg, AWS Certified IoT Architect, Microsoft Certified Azure AI Engineer).\nExperience with DevOps tools and practices.",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Semiconductor', 'Data analysis', 'Machine learning', 'Data collection', 'Product design', 'Natural language processing', 'Firmware', 'microsoft']",2025-06-12 15:03:28
Staff System Test Engineer,Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Test Engineering\n\nGeneral Summary:\n\n\n\nWe are seeking a Senior Staff AI System-Level Test Engineer to lead end-to-end testing of Retrieval-Augmented Generation (RAG) AI systems for Hybrid, Edge-AI Inference solutions. This role will focus on designing, developing, and executing comprehensive test strategies for evaluating the reliability, accuracy, usability and scalability of large-scale AI models integrated with external knowledge retrieval systems.\n\nThe ideal candidate needs to have deep expertise in AI testing methodologies, experience with large language models (LLMs), expertise in building test solutions for AI Inference stacks, RAG, search/retrieval architecture, and a strong background in automation frameworks, performance validation, and building E2E automation architecture.\n\nExperience testing large-scale generative AI applications, familiarity with LangChain, LlamaIndex, or other RAG-specific frameworks, and knowledge of adversarial testing techniques for AI robustness are preferred qualifications\n\nKey Responsibilities:\n\nTest Strategy & Planning\nDefine end-to-end test strategies for RAG, retrieval, generation, response coherence, and knowledge correctness\nDevelop test plans & automation frameworks to validate system performance across real-world scenarios.\nHands-on experience in benchmarking and optimizing Deep Learning Models on AI Accelerators/GPUs\nImplement E2E solutions to integrate Inference systems with customer software workflows\nIdentify and implement metrics to measure retrieval accuracy, LLM response quality\n\n\nTest Automation\nBuild automated pipelines for regression, integration, and adversarial testing of RAG workflows.\nValidate search relevance, document ranking, and context injection into LLMs using rigorous test cases.\nCollaborate with ML engineers and data scientists to debug model failures and identify areas for improvement.\nConduct scalability and latency tests for retrieval-heavy applications. Analyze failure patterns, drift detection, and robustness against hallucinations and misinformation.\n\n\nCollaboration\nWork closely with AI research, engineering teams & customer teams to align testing with business requirements.\nGenerate test reports, dashboards, and insights to drive model improvements.\nStay up to date with the latest AI testing frameworks, LLM evaluation benchmarks, and retrieval models.\n\n\nRequired Qualifications:\n8+ years of experience in AI/ML system testing, software quality engineering, or related fields.\nBachelors or masters degree in computer science engineering/ data science / AI/ML\nHands-on experience with test automation frameworks (e.g., PyTest, Robot Framework, JMeter).\nProficiency in Python, SQL, API testing, vector databases (e.g., FAISS, Weaviate, Pinecone) and retrieval pipelines.\nExperience with ML model validation metrics (e.g., BLEU, ROUGE, MRR, NDCG).\nExpertise in CI/CD pipelines, cloud platforms (AWS/GCP/Azure), and containerization (Docker, Kubernetes).\n\n\nWhy Join Us\nWork on cutting-edge AI retrieval-augmented generation technologies\nCollaborate with world-class AI researchers and engineers.\n\nIf you are passionate about AI system testing and ensuring the reliability of next-generation generative models, apply now!\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 6+ years of Systems Test Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 5+ years of Systems Test Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 4+ years of Systems Test Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['automation framework', 'continuous integration', 'python', 'sql', 'ci cd pipeline', 'kubernetes', 'ci/cd', 'cloud platforms', 'software quality', 'artificial intelligence', 'docker', 'test engineering', 'quality engineering', 'e2e', 'testing methodologies', 'vector', 'aws', 'api testing']",2025-06-12 15:03:30
AI Solutions Engineer,Edwisely,0 - 3 years,Not Disclosed,['Hyderabad'],"AI Solutions Engineer at Edwisely | Jobs at Edwisely\nExperience: 0-3 Years\nRole Overview\nWe re seeking an AI Solutions Engineer to lead the integration of GenAI tools such as ChatGPT or Claude into our core platform. You ll design, prototype, and deploy features that elevate student outcomes, make teaching exciting, and enrich dashboards with intelligent insights, all while upholding privacy and governance standards.\nKey Responsibilities\nDesign and build GenAI-powered features like guided study assistants, automated remediation engine, and intelligent feedback tools.\nDevelop end-to-end pipelines (prompt design model integration CI/CD deployment) that align with Edwisely s Intelligent Learning Infrastructure.\nCollaborate with product teams, faculty, and UX designers to deploy AI features in classrooms and dashboards.\nEnsure all AI implementations meet ISO 27001 security and CERT-In data protection guidelines.\nMeasure adoption and impact via analytics dashboards support evidence-based learning outcomes.\nWhat You ll Bring\nStrong in prompt engineering, RAG, embeddings, or QA systems using frameworks like OpenAI API, LangChain, Hugging Face Transformers.\nFamiliarity with ML deployment FastAPI, Docker, AWS/GCP within production-grade applications.\nExperience with education data, knowledge graphs, student modeling, or assessment systems is a big plus.\nResults-driven: ability to turn prototypes into scalable features.\nPassionate about improving higher education with impactful, AI-driven solutions.\nWhy Edwisely?\nBe part of a high-impact EdTech startup, transforming learning journeys of engineering students nationwide.\nWork with a product that uses AI + analytics to personalize learning, support faculty, and guide institutional decisions .\nCareer growth: define your ownership architecture, implementation, or research-led product development.\nCompetitive package, fast-paced work culture, and freedom to drive real-world impact.\nApplication Process\nSubmit your CV + GitHub or project links (especially showcasing GenAI integrations or NLP projects). for Edwisely",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['QA systems', 'remediation', 'github', 'Prototype', 'Architecture', 'HP data protector', 'GCP', 'ISO 27001', 'Deployment', 'Analytics']",2025-06-12 15:03:32
Data & Gen AI Specialist,Altimetrik,1 - 4 years,Not Disclosed,['Bengaluru'],"Job Title: Data & GenAI AWS Specialist\nExperience: 1-4 Years\nLocation: Bangalore\nMandatory Qualification: B.E./ B.Tech/ M.Tech/ MS from IIT or IISc ONLY\nJob Overview:\nWe are seeking a seasoned Data & GenAI Specialist with deep expertise in AWS Managed Services (PaaS) to join our innovative team. The ideal candidate will have extensive experience in designing sophisticated, scalable architectures for data pipelines and Generative AI (GenAI) solutions leveraging cloud services.",,,,"['Generative Ai', 'Cloud', 'Data Science', 'Open Source', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Snowflake', 'Machine Learning', 'AWS']",2025-06-12 15:03:34
Data Annotation hiring For Fresher || Excellent communication skills,Multinational Company,0 - 4 years,1-3 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Lead data annotation and collection projects.\nDevelop and implement data annotation guidelines and processes.\nTrain and manage data annotation teams.\nCollaborate with data scientists and engineers to understand data requirements.\n\nHR - 63980 09438\n\nRequired Candidate profile\nQualification - Graduate\nSalary :-\nCTC\n25,000 / experience\n20,000 / fresher\nExperience - Data Annotation only\nTransport:- Both Side\n\n5 Day working / Rotation shift / 2 day Rotation week off",Industry Type: BPM / BPO,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Object Detection', 'Data Annotation', 'Business Intelligence', 'Digital Image Processing', 'Data Management', 'Image Recognition', 'Image Analysis', 'Annotation', 'Deep Learning', 'Pattern Recognition', 'Image Processing', 'Imaging', 'Content Moderation', 'Data Warehousing', 'Data Analytics']",2025-06-12 15:03:36
Trainee Solution Engineer,Qualitykiosk Technologies,0 - 1 years,Not Disclosed,"['Mumbai', 'Navi Mumbai']","Key ResponsibilitiesLearning and Development: Engage in training sessions and workshops to enhance your skills and knowledge relevant to your role\nAssisting Senior Staff: Support senior team members in their tasks, which may include research, data analysis, and project management\nProject Participation: Contribute to team projects, providing fresh perspectives and innovative ideas to foster collaboration and creativity\nProblem-Solving: Participate in problem-solving activities, making decisions based on available information and seeking guidance when necessary\nCollaboration: Work with cross-functional teams to gain a holistic understanding of the company s operations and contribute to multidisciplinary projects",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['project management', 'python', 'data analysis', 'data analytics', 'data management', 'c', 'mis reporting', 'business analysis', 'pivot table', 'vlookup', 'machine learning', 'presales', 'javascript', 'sql', 'excel', 'react.js', 'tableau', 'node.js', 'advanced excel', 'data visualization', 'pega', 'powerpoint']",2025-06-12 15:03:38
"Business Intelligence Engineer, RBS ARTS",Amazon,5 - 10 years,Not Disclosed,['Chennai'],"An candidate will be a self-starter who is passionate about discovering and solving complicated problems, learning complex systems, working with numbers, and organizing and communicating data and reports. You will be detail-oriented and organized, capable of handling multiple projects at once, and capable of dealing with ambiguity and rapidly changing priorities. You will have expertise in process optimizations and systems thinking and will be required to engage directly with multiple internal teams to drive business projects/automation for the RBS team. Candidates must be successful both as individual contributors and in a team environment, and must be customer-centric. Our environment is fast-paced and requires someone who is flexible, detail-oriented, and comfortable working in a deadline-driven work environment. Responsibilities Include Works across team(s) and Ops organization at country, regional and/or cross regional level to drive improvements and enables to implement solutions for customer, cost savings in process workflow, systems configuration and performance metrics.\n\nBasic Qualifications\nBachelors degree in Computer Science, Information Technology, or a related field\nProficiency in automation using Python\nExcellent oral and written communication skills\nExperience with SQL, ETL processes, or data transformation\n\nPreferred Qualifications\nExperience with scripting and automation tools\nFamiliarity with Infrastructure as Code (IaC) tools such as AWS CDK\nKnowledge of AWS services such as SQS, SNS, CloudWatch and DynamoDB\nUnderstanding of DevOps practices, including CI/CD pipelines and monitoring solutions\nUnderstanding of cloud services, serverless architecture, and systems integration\n\n\nAs a Business Intelligence Engineer in the team, you will collaborate closely with business partners, architect, design, implement, and BI projects & Automations.\n\nResponsibilities:\n\nDesign, development and ongoing operations of scalable, performant data warehouse (Redshift) tables, data pipelines, reports and dashboards.\nDevelopment of moderately to highly complex data processing jobs using appropriate technologies (eg SQL, Python, Spark, AWS Lambda, etc)\nDevelopment of dashboards and reports.\nCollaborating with stakeholders to understand business domains, requirements, and expectations. Additionally, working with owners of data source systems to understand capabilities and limitations.\nDeliver minimally to moderately complex data analysis; collaborating as needed with Data Science as complexity increases.\nActively manage the timeline and deliverables of projects, anticipate risks and resolve issues.\nAdopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\nInternal job description\n\nRetail Business Service, ARTS is a growing team that supports the Retail Efficiency and Paid Services business and tech teams. There is ample growth opportunity in this role for someone who exhibits Ownership and Insist on the Highest Standards, and has strong engineering and operational best practices experience.\n\nBasic qualifications:\n\n5+ years of relevant professional experience in business intelligence, analytics, statistics, data engineering, data science or related field.\nExperience with Data modeling, SQL, ETL, Data Warehousing and Data Lakes.\nStrong experience with engineering and operations best practices (version control, data quality/testing, monitoring, etc)\nExpert-level SQL.\nProficiency with one or more general purpose programming languages (eg Python, Java, Scala, etc)\nKnowledge of AWS products such as Redshift, Quicksight, and Lambda.\nExcellent verbal/written communication & data presentation skills, including ability to succinctly summarize key findings and effectively communicate with both business and technical teams.\n\nPreferred qualifications:\n\nExperience with data-specific programming languages/packages such as R or Python Pandas.\nExperience with AWS solutions such as EC2, DynamoDB, S3, and EMR.\nKnowledge of machine learning techniques and concepts. 3+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc experience\nExperience with data visualization using Tableau, Quicksight, or similar tools\nExperience with data modeling, warehousing and building ETL pipelines\nExperience in Statistical Analysis packages such as R, SAS and Matlab\nExperience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling Experience with AWS solutions such as EC2, DynamoDB, S3, and Redshift\nExperience in data mining, ETL, etc and using databases in a business environment with large-scale, complex datasets",,,,"['SAS', 'Data modeling', 'Oracle', 'Business intelligence', 'MATLAB', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-12 15:03:41
Software Development Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"As a Software Development Engineer, you will get the opportunity to own problems end-to-end and work with some of the best minds in Amazon. This role is for a full-stack developer with an emphasis on designing highly scalable and extensible applications. You will design flexible and scalable solutions, and work on some of the most complex challenges in computing by utilizing your skills in data structures, algorithms, and principle programming. You will have a broad range of responsibilities from design, development, testing, deployment and operations. You would have easy access to Sr SDE, and Principal Engineers to bounce off your ideas and discuss tech solutions.\n\n\nN/A\n\nA day in the life\nN/A 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'Software Development Engineer II', 'development testing', 'Coding', 'Architectural design', 'Software development life cycle', 'Design development', 'Programming', 'Data structures', 'Internship']",2025-06-12 15:03:43
Artificial Engineer,Care Allianz,1 - 4 years,Not Disclosed,['Pune'],Care Allianz is looking for Artificial Engineer to join our dynamic team and embark on a rewarding career journey\n\nDevelops and maintains AI/ML models for automation and analytics\n\nTrains data pipelines and manages model lifecycle\n\nCollaborates on product features using artificial intelligence\n\nMonitors model performance and ensures quality outcomes,Industry Type: Software Product,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['project management', 'python', 'mechanical engineering', 'software testing', 'production', 'plc', 'manual testing', 'engineering', 'autocad', 'catia', 'kaizen', 'electricals', 'manpower handling', 'automation', 'java', 'selenium', 'computer science', 'quality assurance', 'design', 'production engineering']",2025-06-12 15:03:46
Data Science Manager,ZS,10 - 15 years,Not Disclosed,"['Pune', 'Bengaluru']","A key enabler of our services is leveraging data in delivering client solutions. The data available about customers is getting richer and the problems that our customers are trying to answer continue to evolve. In our endeavor to stay ahead in providing solutions to these evolving complex problems, ZS has set up an Advanced Data Science which has three major focus areas:\nResearch the evolving datasets and advanced analytical techniques to develop new offerings/solutions\nDeliver client impact by collaboratively implementing these solutions",,,,"['Team management', 'data science', 'Pharma', 'Analytical', 'Management consulting', 'Financial planning', 'Healthcare', 'Project planning', 'Predictive modeling', 'Financial services']",2025-06-12 15:03:48
Apprentice - Operations,Startek,0 - 1 years,Not Disclosed,['Kolkata'],"STARTEK is looking for Apprentice - Operations to join our dynamic team and embark on a rewarding career journey\n\nSupport daily operations and gain exposure to process workflows\n\nAssist with documentation, reporting, and coordination tasks\n\nLearn industry standards and company-specific systems\n\nParticipate in training and mentorship activities",Industry Type: Oil & Gas,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Wireless', 'Business transformation', 'orchestration', 'Artificial Intelligence', 'Customer support', 'Customer experience', 'Management', 'Troubleshooting', 'Customer engagement', 'digital transformation']",2025-06-12 15:03:50
Executive - Operations,Startek,0 - 6 years,Not Disclosed,['Kolkata'],"STARTEK is looking for Executive - Operations to join our dynamic team and embark on a rewarding career journey An Executive - Operations is responsible for overseeing the day-to-day operations of a company to ensure efficiency, profitability, and growth\n\nSome of the key responsibilities for this role include:\n\nDeveloping and implementing operational strategies\n\n\n\nMonitoring and analyzing operational metrics to identify areas for improvement and implementing continuous improvement initiatives\n\n\n\nExpress their operations strategies & objectives to make sure that the company which they are working for reaches its target and operates effectively\n\n\n\nStrong leadership and problem-solving skills, as well as the ability to analyze data and make informed decisions, are essential for success in this role",Industry Type: Oil & Gas,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Wireless', 'Business transformation', 'orchestration', 'Artificial Intelligence', 'Customer support', 'Operation Executive', 'Customer experience', 'Troubleshooting', 'Customer engagement', 'digital transformation']",2025-06-12 15:03:53
Senior Applied AI Scientist,ZS,4 - 9 years,Not Disclosed,['Bengaluru'],"Write complex SQL queries for data extraction, perform exploratory data analysis (EDA) to uncover insights.\nStrong proficiency in Python and Py Spark for scalable data processing and analytics.\nCreate, transform, and optimize features to enhance model performance.\nTrain, evaluate, and maintain machine learning models in production.\nWrite efficient, maintainable, and version-controlled code that handles large datasets.\nRegularly update internal teams and clients on project progress, results, and insights.\nConduct hypothesis testing and experiment analysis to drive data-driven decisions using AB testing.",,,,"['Data analysis', 'data security', 'Financial planning', 'Management consulting', 'Machine learning', 'Data processing', 'Analytics', 'Data extraction', 'Python']",2025-06-12 15:03:55
"Catalog Specialist I, Amazon Business Catalog Taxonomy",Amazon,0 - 7 years,Not Disclosed,['Hyderabad'],"Amazon Business Catalog Taxonomy Operations team is focused on building solutions that enable B2B customers to find, research, and buy products and services from a vast selection, across multiple devices, marketplaces and regions. The team ensures that our selection is classified for AB customers to perform business specific functions such as approval routing, spend analysis, create procurement policies for compliance, and do forecasting and reporting.\n\n\nWe re looking for people who have the ability to follow given guidelines and make decisions in ambiguous situations, as they work with a team focused on assigning global classifications to our Amazon catalog selection for business customers. They ll use internal tools to manage workload and should feel confident to actively contribute to process improvement initiatives. We d love to speak to candidates who already have proficiency in Microsoft Office, with an emphasis on basic Excel competencies.\n\nA day in the life\nYou will 1) Create Machine Learning rules / Classify ASINs for global classification standards (e.g. UNSPSC) 2) Use tools to create and manage classification mappings between internal catalog and external taxonomy 3) actively troubleshoot and respond to issues that are caused by incorrect classification, mappings or rationales.\n\nAbout the team\nWe are a global and multicultural team who interacts daily with teammates across other regions (EU, JP) and global stakeholders.\nThe teams vision is to have the product catalog perfectly classified for our AB customers.\nWe classify millions of items daily for 10 marketplaces which helps our business customers` ordering experience more smooth. Bachelors degree\nSpeak, write, and read fluently in English\nExperience with Microsoft Office products and applications\nExperience with Excel Catalog knowledge\nSQL Query knowledge",,,,"['Procurement', 'Compliance', 'Process improvement', 'Machine learning', 'Spend analysis', 'MS Office', 'Troubleshooting', 'Forecasting', 'SQL', 'Recruitment']",2025-06-12 15:03:57
Software Development Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Want to join a team that protects and improves the buyer experience of millions Amazon customers and builds earths most customer-centric sellers daily using innovative technology including machine learning, data mining and big data analytics, cloud computing services, and highly available/scalable distributed systems that support hundreds of millions of transactions across the globe?\n\nWe have an exciting opportunity with the Regulatory Intelligence, Safety, and Compliance (RISC) engineering team, to architect and build next-generation engineering systems to quickly and accurately identify and mitigate product safety issues and potential risks to the customer experience.\n\nAs a Software Development Engineer, you will work with your team of highly skilled software, data, and ML engineers to invent, design, build and manage highly scalable distributed systems that provide availability, scalability and latency guarantees. You will work with your internal customers to balance customer requirements with team requirements and help your team and business evolve, by working with LLMs and large data sets. You will be using the latest AI, AWS and industry technologies to deliver a one-stop risk identification and remediation ecosystem for Amazon, keeping our customers safe and products compliant, building the software creating the world s most trustworthy data set on everything companies and customers need to know related to the safety and compliance of products and chains.\n\nEach and every person buying, selling, or handling Amazon products will be your customer.\n\nAs a member of this growing team, you ll be able to build the groundwork and influence its direction for the years to come. Our work cuts across various disciplines from delivering an awesome user experience via great UI/UX, to building massively scalable backend systems to support the most high-traffic pages on Amazon.com, to analytical and feedback systems which give us data-driven customer insights, to using machine learning and AI to influence recommendations and marketing. If you have a passion for consumer-facing applications, and are obsessed with customer experience, we want you!\n\nIf you d like to make a real-world difference by working hard, having fun, and making history, this is the team for you!\n\n\nIn this role you will:\nHelp define the system architecture, own and implement specific components, and help shape the overall experience\nCollaborate closely with product managers, UX designers, and other SDE team members to help define the scope of the product\nTake responsibility for technical problem solving, creatively meeting product objectives, and developing best practices\nDemonstrate cross-functional resource interaction to accomplish your goals\nWrite high-quality, efficient, testable code in Java and other object-oriented languages\nDesign Amazon-scale tools to facilitate internal business\nBuild highly available, secure, and low-latency systems\nMentor other developers\nFind out what it takes to engineer systems for ""Amazon Scale""\nDesign and build microservices\nOwn and operate the systems that you build based on real-time customer data and demanding service-level agreements\nContribute to planning, design, implementation, testing, operations, and process improvement\n\nA day in the life\nHigh-level designs, cross-team alignment, long-term architectural roadmap and technical strategy, understanding the business domain and proposing solutions to address customer and business problems, helping scope and analyze product requirements, mentorship, reviewing CRs, writing high-quality code to be an example for the team. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\n3+ years of Video Games Industry (supporting title Development, Release, or Live Ops) experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Cloud computing', 'Backend', 'Coding', 'Analytical', 'Machine learning', 'Data mining', 'Internship', 'Distribution system']",2025-06-12 15:03:59
Senior/Lead MLops Engineer,Tiger Analytics,7 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","JOB DESCRIPTION\n\nSenior MLE / Architect MLE (ML Ops) Chennai / Bangalore / Hyderabad (Hybrid)\n\nWho we are Tiger Analytics is a global leader in AI and analytics, helping Fortune 1000 companies solve their toughest challenges. We offer fullstack AI and analytics services & solutions to empower businesses to achieve real outcomes and value at scale. We are on a mission to push the boundaries of what AI and analytics can do to help enterprises navigate uncertainty and move forward decisively. Our purpose is to provide certainty to shape a better tomorrow. Our team of 4000+ technologists and consultants are based in the US, Canada, the UK, India, Singapore and Australia, working closely with clients across CPG, Retail, Insurance, BFS, Manufacturing, Life Sciences, and Healthcare. Many of our team leaders rank in Top 10 and 40 Under 40 lists, exemplifying our dedication to innovation and excellence. We are a Great Place to Work-Certified (2022-24), recognized by analyst firms such as Forrester, Gartner, HFS, Everest, ISG and others. We have been ranked among the Best and Fastest Growing analytics firms lists by Inc., Financial Times, Economic Times and Analytics India Magazine.",,,,"['MLops', 'Azure', 'Snowflake', 'Deployment', 'Ci/Cd', 'Machine Learning']",2025-06-12 15:04:02
Data Science Analyst (Lead),Infogain,8 - 11 years,Not Disclosed,['Gurugram'],"- Strong understanding of ML algorithms (regression, classification, clustering) with the ability to independently develop and scale models using Python.\n- Experience in commercial analytics with a knack for translating business problems into analytical solutions and strategic recommendations.\n- Proficient in Power BI to build intuitive dashboards and deliver insights in a clear, actionable format.\n- Strong storytelling and communication skills to convey complex analytical findings to both technical and non-technical audiences.\n- Demonstrates ownership and leadership, driving projects from idea to impact with minimal supervision.\n  SKILLS\nPrimary Skill: Data Science\nSub Skill(s): Data Science\nAdditional Skill(s): Python, Data Science",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analyst', 'Translation', 'data science', 'Analytical', 'power bi', 'Analytics', 'Supervision', 'Python']",2025-06-12 15:04:04
"Software Development Engineer II, RISC",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Want to join a team that protects and improves the buyer experience of millions Amazon customers and builds earths most customer-centric sellers daily using innovative technology including machine learning, data mining and big data analytics, cloud computing services, and highly available/scalable distributed systems that support hundreds of millions of transactions across the globe?\n\nWe have an exciting opportunity with the Regulatory Intelligence, Safety, and Compliance (RISC) engineering team, to architect and build next-generation engineering systems to quickly and accurately identify and mitigate product safety issues and potential risks to the customer experience.\n\nAs a Software Development Engineer, you will work with your team of highly skilled software, data, and ML engineers to invent, design, build and manage highly scalable distributed systems that provide availability, scalability and latency guarantees. You will work with your internal customers to balance customer requirements with team requirements and help your team and business evolve, by working with LLMs and large data sets. You will be using the latest AI, AWS and industry technologies to deliver a one-stop risk identification and remediation ecosystem for Amazon, keeping our customers safe and products compliant, building the software creating the world s most trustworthy data set on everything companies and customers need to know related to the safety and compliance of products and chains.\n\nEach and every person buying, selling, or handling Amazon products will be your customer.\n\nAs a member of this growing team, you ll be able to build the groundwork and influence its direction for the years to come. Our work cuts across various disciplines from delivering an awesome user experience via great UI/UX, to building massively scalable backend systems to support the most high-traffic pages on Amazon.com, to analytical and feedback systems which give us data-driven customer insights, to using machine learning and AI to influence recommendations and marketing. If you have a passion for consumer-facing applications, and are obsessed with customer experience, we want you!\n\nIf you d like to make a real-world difference by working hard, having fun, and making history, this is the team for you!\n\n\nIn this role you will:\nHelp define the system architecture, own and implement specific components, and help shape the overall experience\nCollaborate closely with product managers, UX designers, and other SDE team members to help define the scope of the product\nTake responsibility for technical problem solving, creatively meeting product objectives, and developing best practices\nDemonstrate cross-functional resource interaction to accomplish your goals\nWrite high-quality, efficient, testable code in Java and other object-oriented languages\nDesign Amazon-scale tools to facilitate internal business\nBuild highly available, secure, and low-latency systems\nMentor other developers\nFind out what it takes to engineer systems for ""Amazon Scale""\nDesign and build microservices\nOwn and operate the systems that you build based on real-time customer data and demanding service-level agreements\nContribute to planning, design, implementation, testing, operations, and process improvement\n\nA day in the life\nHigh-level designs, cross-team alignment, long-term architectural roadmap and technical strategy, understanding the business domain and proposing solutions to address customer and business problems, helping scope and analyze product requirements, mentorship, reviewing CRs, writing high-quality code to be an example for the team. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\n3+ years of Video Games Industry (supporting title Development, Release, or Live Ops) experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Cloud computing', 'Backend', 'Coding', 'Analytical', 'Machine learning', 'Data mining', 'Internship', 'Distribution system']",2025-06-12 15:04:07
Process Associate- Invoice To Cash,Genpact,0 - 3 years,Not Disclosed,['Jodhpur'],"Ready to shape the future of work?\nAt Genpact, we don't just adapt to changewe drive it. AI and digital innovation are redefining industries and wereleading the charge. Genpact’s AI Gigafactory, our industry-first accelerator,is an example of how advanced technology solutions were scaling to help globalenterprises work smarter, grow faster, and transform at scale. From large-scalemodels to agentic AI, our breakthrough solutions tackle companies’ most complexchallenges.\nIf you thrive in a fast-moving, tech-drivenenvironment, love solving real-world problems, and want to be part of a teamthat’s shaping the future, this is your moment",,,,"['accounts receivable', 'law', 'verbal communication', 'interpersonal skills', 'pay', 'hrsd', 'accounting', 'order to cash', 'cash applications', 'reconciliation', 'technology solutions', 'artificial intelligence', 'operations', 'automation', 'recruitment', 'writing', 'english', 'operational excellence']",2025-06-12 15:04:09
Senior Verification Engineer - GPU Fullchip,Nvidia,4 - 10 years,Not Disclosed,['Bengaluru'],"NVIDIA has continuously reinvented itself. Our invention of the GPU sparked the growth of the PC gaming market, redefined modern computer graphics, and revolutionized parallel computing. Today, research in artificial intelligence is booming worldwide, which calls for highly scalable and massively parallel computation horsepower that NVIDIA GPUs excel. NVIDIA is a learning machine that constantly evolves by adapting to new opportunities that are hard to solve, that only we can address, and that matter to the world. This is our life s work , to amplify human creativity and intelligence. As an NVIDIAN, you ll be immersed in a diverse, supportive environment where everyone is inspired to do their best work. Come join our diverse team and see how you can make a lasting impact on the world!\nThe GPU started out as an engine for simulating human imagination, conjuring up the amazing virtual worlds of video games and Hollywood films. Today, NVIDIA s GPU simulates human intelligence, running deep learning algorithms and acting as the brain of computers, robots, and self-driving cars that can perceive and understand the world. NVIDIA is increasingly known as the AI computing company.\nWhat you ll be doing:\nAs a key member of our ASIC Verification team, you will verify the design and implementation of the industrys leading GPUs.\nResponsible for verification of the ASIC design, architecture, golden models and micro-architecture of the GPUs and full-chip level using advanced verification tools and methodologies.\nBuild reusable bus functional models, traffic generators, monitors, checkers and scoreboards following coverage driven verification methodology.\nYou are expected to understand the design and implementation, define the verification scope, develop the verification infrastructure and verify the correctness of the design.\nWorking with architects, designers, and pre and post silicon verification teams to accomplish your tasks.\nWhat we need to see:\nB. Tech. / M. Tech or equivalent experience with 4+ years of relevant experience\nProficiency in Verilog and C\nFamiliarity with OOPs concepts\nHands-on experience in verification at Unit/Sub-system/SOC level\nGood understanding of computer architecture concepts\nStrong technical fundamentals with superior analytical skills and problem-solving skills\nKnowledge of SystemVerilog or similar HVL is highly desirable\nExperience in verification methodologies like UVM/VMM is highly desirable\nExposure to industry standard verification tools for simulation and debug\nWays to stand out from the crowd:\nPrior experience in 3D graphics processing or processor verification\nC/C++ programming language experience\nExperience in scripting and tool development using Perl and Python\nExcellent debugging skills\nGood interpersonal and communication skills & dream to work as a great teammate\n#LI-Hybrid",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Graphics', 'Simulation', 'SOC', 'Artificial Intelligence', 'Verilog', 'VMM', 'Perl', 'Silicon', 'UVM', 'Python']",2025-06-12 15:04:12
Senior SOC Design Engineer,Nvidia,5 - 8 years,Not Disclosed,['Bengaluru'],"NVIDIA has continuously reinvented itself. Our invention of the GPU sparked the growth of the PC gaming market, redefined modern computer graphics, and revolutionized parallel computing. Today, research in artificial intelligence is booming worldwide, which calls for highly scalable and massively parallel computation horsepower that NVIDIA GPUs excel. NVIDIA is a learning machine that constantly evolves by adapting to new opportunities that are hard to solve, that only we can address, and that matter to the world. This is our life s work , to amplify human creativity and intelligence. As an NVIDIAN, you ll be immersed in a diverse, supportive environment where everyone is inspired to do their best work. Come join our diverse team and see how you can make a lasting impact on the world!\nNVIDIA System-On-Chip (SOC) group is hiring for a Senior SOC Design Engineer! The complexity of the chips we build has increased manifold over the years. We are now packing tens of billions of transistors in a chip to meet the growing computing demand. We are looking for a star candidate with strong inclination in RTL integration and chip level front-end design, including padring, pinmuxing, SOC Assembly process, retiming etc. You must have a real passion for methodologies and automation solutions that enable SOC creation in the most optimized way. In this position, you will get the chance to build sophisticated Tegra SOCs, work closely with chip management to set ASIC execution timelines goals while directly interacting with System Architecture, unit-level ASIC, Physical Design, CAD, Package Design, DFT and other teams. Additionally, you will be involved in defining and crafting methodologies that build more efficient and flexible SOCs in future.\nWhat youll be doing:\nDrive SOC Assembly and design chip level functions for Tegra SOCs.\nResponsible for front-end design quality/correctness checks, reviews and driving those with multi-functional teams.\nDrive SOC execution across chip milestones working with all multi-functional teams to help define, track and drive complex dependencies.\nDefine and develop system-level methodologies, tools, and IPs to build SOCs in an efficient and scalable manner.\nIdentify difficulties and inefficiencies in the front-end chip implementation process and propose and implement ideas to solve them.\nWhat we need to see:\nB. Tech or M. Tech in Electronics Engineering.\n5+ years of proven experience in chip design, specializing in SOC integration and design automation. Padring and fuse/floorsweep design experience is a bonus.\nExcellent analytical and problem-solving skills.\nExperience in RTL design (Verilog), System-On-Chip design/implementation flow.\nStrong coding skills in Perl, Python, or other industry-standard scripting languages.\nExposure to various Chip Design Functions to be able to collaborate and solve complex multi-functional problems.\nExcellent interpersonal skills to work with multiple teams to drive consensus.\nGood teamwork spirit and collaboration skills with team members.\nBackground in SOC Verification, Synthesis, Physical design and DFT is a bonus.\nExperience in RTL Build flows and Makefiles is a plus.\n#LI-Hybrid",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['System architecture', 'Automation', 'ASIC', 'DFT', 'Coding', 'Verilog', 'CAD', 'Perl', 'Python', 'Physical design']",2025-06-12 15:04:14
Lead Engineer - App F/W&MW - Linux,Sasken Technologies,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position takes ownership of a module and associated quality and delivery. Person at this position provides instructions, guidance and advice to team members to ensure quality and on time delivery.\nPerson at this position is expected to be able to instruct and review the quality of work done by technical staff.\nPerson at this position should be able to identify key issues and challenges by themselves, prioritize the tasks and deliver results with minimal direction and supervision.\nPerson at this position has the ability to investigate the root cause of the problem and come up alternatives/ solutions based on sound technical foundation gained through in-depth knowledge of technology, standards, tools and processes.\nPerson has the ability to organize and draw connections among ideas and distinguish between those which are implementable.\nPerson demonstrates a degree of flexibility in resolving problems/ issues that atleast to in-depth command of all techniques, processes, tools and standards within the relevant field of specialisation.\n\n\nRoles & Responsibilities\nResponsible for requirement analysis and feasibility study including system level work estimation while considering risk identification and mitigation.\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals.\nResponsible for traceability of the requirements from design to delivery Code optimization and coverage.\nResponsible for conducting reviews, identifying risks and ownership of quality of deliverables.\nResponsible for identifying training needs of the team.\nExpected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments.\nExpected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\nExpected to be a technical mentor for junior members.\nPerson may be given additional responsibility of managing people based on discretion of Project Manager.\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 5-8 years\n\nCompetencies DescriptionApplication Protocol & Engines - Linux engineer is one:\nwho has done one or more of the following on Embedded Linux\ndesign, development/customization, bug fixing/sustenance\nwho has experience in one or more of the following domains\nMultimedia\nTelephony\nConnectivity\nSensor\nSecurity\nPlatforms-\nMandatory to have worked on one or more of the following:\nEmbedded Linux\nTools-\nMandatory to have worked on one or more of the following;\ngdb/ddd; linux editors; top; ps; meminfo\nLanguages-\nMandatory to have worked on one or more of the following;\nC; C++\nSpecialization-\nMULTIMEDIA, CONNECTIVITY, TELEPHONY, CARRIER GRADE PLATFORM, GENERIC FRAMEWORK",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['LINUX', 'Unix', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Machine Learning', 'Python']",2025-06-12 15:04:16
STEM Innovation Engineer,Stemrobo,0 - 4 years,2.5-3.5 Lacs P.A.,"['Kottayam', 'Delhi / NCR', 'Mumbai (All Areas)']","1. Develop strong expertise in the field of Robotics, IOT, AI, Drone & Coding.\n2. Regular visits to schools and provide mentorships to K-12 students.\n3. Organize, Develop and coordinate special STEM Engineering and technology based events/activities\n\nRequired Candidate profile\nPractical knowledge of Arduino, Raspberry Pi and other microcontrollers is required.\nSound knowledge in C and Python Language.\nGood verbal and written communication.\nFlexible with working hours.",Industry Type: Education / Training,Department: Teaching & Training,"Employment Type: Full Time, Permanent","['Arduino', 'IOT', 'Mechatronics', 'C', 'Coding', 'Artificial Intelligence', 'Electronics', 'Drone', 'Robotics', 'Python']",2025-06-12 15:04:18
Senior Associate Data Scientist,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will identify trends, root causes, and potential improvements in our products and processes, ensuring that patient voices are heard and addressed with utmost precision.\nAs the Sr Associate Data Scientist at Amgen, you will be responsible for developing and deploying basic machine learning, operational research, semantic analysis, and statistical methods to uncover structure in large data sets. This role involves creating analytics solutions to address customer needs and opportunities.\nCollect, clean, and manage large datasets related to product performance and patient complaints.\nEnsure data integrity, accuracy, and accessibility for further analysis.\nDevelop and maintain databases and data systems for storing patient complaints and product feedback.\nAnalyze data to identify patterns, trends, and correlations in patient complaints and product issues.\nUse advanced statistical methods and machine learning techniques to uncover insights and root causes.\nDevelop analytics or predictive models to foresee potential product issues and patient concerns to address customer needs and opportunities.\nPrepare comprehensive reports and visualizations to communicate findings to key collaborators.\nPresent insights and recommendations to cross-functional teams, including product development, quality assurance, and customer service.\nCollaborate with regulatory and compliance teams to ensure adherence to healthcare standards and regulations.\nFind opportunities for product enhancements and process improvements based on data analysis.\nWork with product complaint teams to implement changes and monitor their impact.\nStay abreast of industry trends, emerging technologies, and standard methodologies in data science and healthcare analytics.\nEvaluate data to support product complaints.\nWork alongside software developers and software engineers to translate algorithms into commercially viable products and services.\nWork in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.\nPerform exploratory and targeted data analyses using descriptive statistics and other methods.\nWork with data engineers on data quality assessment, data cleansing and data analytics\nGenerate reports, annotated code, and other projects artifacts to document, archive, and communicate your work and outcomes.\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience OR\nBachelors degree and 3 to 5 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience OR\nDiploma and 7 to 9 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience\nPreferred Qualifications:\nDemonstrated skill in the use of applied analytics, descriptive statistics, feature extraction and predictive analytics on industrial datasets.\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification.\nExperience in analyzing time-series data for forecasting and trend analysis.\nExperience with Data Bricks platform for data analytics.\nExperience working with healthcare data, including patient complaints, product feedback, and regulatory requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'data bricks', 'hypothesis testing', 'predictive analytics', 'data visualization', 'machine learning', 'statistics']",2025-06-12 15:04:20
Artificial Intelligence Engineer,Wissen Technology,4 - 9 years,Not Disclosed,"['Bengaluru', 'Mumbai (All Areas)']","Roles and Responsibilities\nDesign, develop, test, and deploy AI models using Python and deep learning frameworks such as TensorFlow, PyTorch, or scikit-learn.\nCollaborate with cross-functional teams to integrate AI solutions into existing systems and applications.\nDevelop REST APIs using Flask or Django for data processing and integration purposes.\nTroubleshoot issues related to model performance, data quality, and system architecture.\nStay up-to-date with industry trends and advancements in artificial intelligence research.\nDesired Candidate Profile\n4-9 years of experience in developing AI models using Python programming language.\nStrong proficiency in at least two deep learning frameworks (TensorFlow, PyTorch, or scikit-learn).\nExperience working on projects involving natural language processing (NLP), computer vision, or other areas of machine learning.\nBachelor's degree in Any Specialization (B.Tech/B.E.).\nHands-on experience with Pandas library for data manipulation and analysis.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Rest Api Development', 'Tensorflow', 'Pytorch', 'Pandas', 'Scikit-Learn']",2025-06-12 15:04:22
Data Scientist,Big Oh Tech,4 - 6 years,Not Disclosed,['Noida'],"Key Responsibilities:\n\nDesign, build, and maintain robust and scalable data pipelines to support analytics and reporting needs.\nManage and optimize data lake architectures, with a focus on Apache Atlas for metadata management, data lineage, and governance.\nIntegrate and curate data from multiple structured and unstructured sources to enable advanced analytics.\nCollaborate with data scientists and business analysts to ensure availability of clean, well-structured data.\nImplement data quality, validation, and monitoring processes across data pipelines.\nDevelop and manage Power BI datasets and data models, supporting dashboard and report creation.\nSupport data cataloging and classification using Apache Atlas for enterprise-wide discoverability and compliance.\nEnsure adherence to data security, privacy, and compliance policies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advanced analytics', 'metadata', 'Compliance', 'Business Analyst', 'data security', 'power bi', 'Data quality', 'Management', 'Apache', 'Monitoring']",2025-06-12 15:04:24
Artificial Intelligence Engineer,Indovation Lab,5 - 10 years,Not Disclosed,"['Bengaluru', 'Delhi / NCR']","The Opportunity:\nWe are seeking a talented and experienced Senior AI Engineer to play a pivotal role in developing and enhancing our conversational AI capabilities across multiple modalities (text, voice/telephony) and building intelligent agents to drive workflow automation. You will work closely with our Director of AI Engineering, product managers, and other engineering teams to bring our AI vision to life, primarily focusing on fine-tuning off-the-shelf Large Language Models (LLMs) and developing agentic systems for end-users (members, CBO staff, healthcare providers).\nThis is a unique opportunity to apply your AI expertise to solve meaningful problems in the social and healthcare space, directly impacting people's lives.\nWhat You'll Do:\nConversational AI Development & Fine-Tuning (Text & Voice):\nLead the fine-tuning, evaluation, and deployment of pre-trained LLMs (e.g., Gemini, GPT series, open-source models) to create natural, empathetic, and effective conversational experiences for various user interactions via text-based channels (chat, SMS) and voice-based telephonic systems (IVR chatbots).\nDevelop and implement strategies for data collection, preparation, and augmentation to support model fine-tuning and continuous improvement for both text and voice modalities.\nDesign and implement robust evaluation frameworks to measure conversational AI performance, including metrics for accuracy, fluency, empathy, task completion, and call handling efficiency for telephonic agents.\nWork on prompt engineering, context management, and dialogue flow design to optimize conversational AI interactions across channels.\nIntegrate and manage speech-to-text (STT) and text-to-speech (TTS) services for telephonic AI solutions.\nAgentic System Development:\nDesign, build, and deploy AI agents that can reason, make decisions, and take actions to automate and optimize key workflows within the platform (e.g., intelligent referral initiation, proactive follow-ups, task management assistance).\nDevelop agents capable of interacting with internal platform APIs, external data sources, and potentially third-party tools to achieve their goals.\nExplore and implement techniques for agent planning, tool usage, and multi-step reasoning.\nCollaboration & Technical Leadership:\nCollaborate closely with the Director of AI Engineering to define AI strategy, architecture, and technical roadmap for conversational AI and agentic systems.\nPartner with Product Managers to understand user needs and translate them into technical requirements for AI features.\nWork with platform and application engineers to integrate AI models and agents into the broader ecosystem.\nMentor junior engineers and contribute to building a strong AI engineering culture.\nStay up-to-date with the latest advancements in LLMs, conversational AI (text and voice), agent-based systems, and MLOps.\nMLOps & Productionization:\nContribute to the development and maintenance of our MLOps infrastructure for training, deploying, monitoring, and iterating on AI models and agents in production.\nEnsure AI systems are scalable, reliable, and maintainable.\nBack-end Development:\nSolid understanding of back-end development principles and experience building or integrating with APIs (e.g., RESTful services) to connect AI models and agents with broader application systems.\nFamiliarity with database technologies (SQL and/or NoSQL) and practical experience in how AI systems interact with data storage and retrieval for training, inference, and logging.\nWhat You'll Bring:\nEducation: Bachelor's or Master's degree in Computer Science, Artificial Intelligence, Machine Learning, or a related field.\nExperience:\n5+ years of hands-on experience in AI/ML engineering, with a significant focus on Natural Language Processing (NLP) and conversational AI.\nDemonstrable experience developing and deploying AI-powered telephonic chatbots or Interactive Voice Response (IVR) systems, including integration with STT/TTS technologies.\nProven experience fine-tuning and deploying Large Language Models (LLMs) for specific tasks and domains. Strong understanding of model architectures, training techniques, and evaluation metrics.\nDemonstrable experience designing and building AI agents or systems that exhibit autonomous behavior, decision-making, and/or tool usage.\nProficiency in Python and common AI/ML frameworks (e.g., TensorFlow, PyTorch, Hugging Face Transformers, LangChain, LlamaIndex).\nExperience with cloud platforms (GCP preferred, AWS/Azure acceptable) and their AI/ML services (e.g., Vertex AI, SageMaker, cloud telephony APIs).\nSolid understanding of MLOps principles and experience with tools for model deployment, monitoring, and CI/CD for ML.\nSkills:\nStrong analytical and problem-solving skills.\nExcellent communication and collaboration abilities.\nAbility to translate complex technical concepts to non-technical stakeholders.\nProactive, self-starter with a passion for building impactful AI solutions.\nNice to Haves:\nExperience working in the healthcare or social care domain.\nFamiliarity with data privacy and security considerations in regulated environments (e.g., HIPAA).\nExperience with specific telephony platforms or APIs (e.g., Twilio, Vonage, Google Dialogflow CX).\nContributions to open-source AI/ML projects.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Conversational Ai', 'Python', 'TTS', 'NLP', 'GCP', 'SST', 'Artificial Intelligence', 'Natural Language Processing', 'Chatbot Development', 'Dialogflow']",2025-06-12 15:04:26
Test Automation Software Engineer - CDOT,National Institute for Smart Government (NISG),3 - 6 years,8-12 Lacs P.A.,['Bengaluru'],"Job Description\nName of the Post: Test Automation Software Engineer\nNo of Posts: 1\nLocation: Bangalore\nRoles and Responsibilities:\nDesign, Develop and Build Test Automation frameworks\nDesign and Development of Test suites for the 4G and 5G system.\nWrite, design, and execute automated tests by creating scripts that run testing functions automatically\nMaximize test coverage for the most critical features of the system\nDetermine the priority for test scenarios and create execution plans to implement these scenarios\n\nSkills and Abilities:\n\nComprehensive knowledge in the field of software development.\nKnowledge of Artificial Intelligence/Machine Language\nKnowledge on Cloud computing, CI/CD tools like Jekins\nUnderstanding the Industry Standards: 3gpp standards, Tec standards\nModerate Knowledge on simulators: 2G/3G/4G node Simulators, UE simulators\nModerate knowledge on Software testing tools: Wire shark, Scripting languages.\nLinux administration and Linux networking\nProgramming expertise: High level languages C,C++, Perl, Java and Python\nModerate knowledge on Databases: SQL, ORACLE etc.\nKnowledge on Android OS to Support Mobile-Telephone Automations\nFull stack developers for Test Automation of Validation Test Cases Execution, Configuration, monitoring and maintaining the Test Environment\n\nMinimum Qualifications:\n\nB.E/B.Tech in Computer Science & Engineering or equivalent degree from a recognized college/university\nMinimum 3 years of professional work experience post qualification.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Software Development', '4G', '3G', 'Artificial Intelligence', 'Machine Learning', '2G']",2025-06-12 15:04:28
Manager Data Science,Optum,12 - 17 years,Not Disclosed,['Noida'],"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start  Caring. Connecting. Growing together. \n\nAt Optum AI, we leverage data and resources to make a significant impact on the healthcare system. Our solutions have the potential to improve healthcare for everyone. We work on cutting-edge projects involving ML, NLP, and LLM techniques, continuously developing and improving generative AI methods for structured and unstructured healthcare data. Our team collaborates with world-class experts and top universities to develop innovative AI/ML solutions, often leading to patents and published papers.\n\n Primary Responsibilities \nDevelop and implement AI and machine learning strategies for several healthcare domains\nCollaborate with cross-functional teams to identify and prioritize AI and machine learning initiatives\nManage the development and deployment of AI and machine learning solutions\nDevelop and run pipelines for data ingress and model output egress\nDevelop and run scripts for ML model inference\nDesign, implement, and maintain CI/CD pipelines for MLOps and DevOps functions\nIdentify technical problems and develop software updates and fixes\nDevelop scripts or tools to automate repetitive tasks\nAutomate the provisioning and configuration of infrastructure resources\nProvide guidance on the best use of specific tools or technologies to achieve desired results\nCreate documentation for infrastructure design and deployment procedures\nUtilize AI/ML frameworks and tools such as MLFlow, TensorFlow, PyTorch, Keras, Scikit-learn, etc.\nLead and manage AI/ML teams and projects from ideation to delivery and evaluation\nApply expertise in various AI/ML techniques, including deep learning, NLP, computer vision, recommender systems, reinforcement learning, and large language models\nCommunicate complex AI/ML concepts and results to technical and non-technical audiences effectively\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\n Required Qualifications: \nBachelors/master degree in computer science, engineering, mathematics, statistics, or a related discipline\n12+ years of experience in Software Engineering, Data Science, or Analytics with 8+ years of experience in AI/ML engineering or related fields\nExperience with cloud platforms and services, such as AWS, Azure, GCP, etc.\nExperience in developing solutions in the NLP space and relevant projects\nHands on Experience in AI and drive the development of innovative AI and machine learning solutions\nDemonstrated experience in leading and managing AI/ML teams and projects, from ideation to delivery and evaluation\nExperience with Azure development environments\nKnowledge of NLP literature, thrust areas, conference venues, and code repositories\nFamiliarity with both open-source and OpenAI LLMs and RAG architecture\nFamiliarity with UI tools like Streamlit, Flask, FAST APIs, Rest APIs, Docker containers\nUnderstanding of common NLP tasks such as text classification, entity recognition, entity extraction, and question answering\nProficient in Python and one of PySpark or Scala. Familiarity with python tools for data processing\nProficiency in multiple machine learning and AI techniques such as supervised, unsupervised, reinforcement learning, deep learning, and NLP\nProficiency in Python, R, or other programming languages for data analysis and AI/ML development\nProficiency in libraries such as Hugging Face and OpenAI API\nProven ability to develop and deploy data pipelines, machine learning models, or applications on cloud platforms (Azure, Databricks, AzureML)\nProven excellent communication, presentation, and interpersonal skills, with the ability to explain complex AI/ML concepts and results to technical and non-technical audiences\nProven solid analytical, problem-solving, and decision-making skills, with the ability to balance innovation and pragmatism\nProeven passion for learning and staying updated with the latest AI/ML trends and research",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'machine learning', 'artificial intelligence', 'r', 'continuous integration', 'data analysis', 'scala', 'scikit-learn', 'presentation skills', 'ci/cd', 'microsoft azure', 'docker', 'tensorflow', 'data science', 'ai techniques', 'devops', 'pytorch', 'keras', 'software engineering', 'aws']",2025-06-12 15:04:31
"Principal Engineer/Manager/Director, CAD tools & Methodology",Qualcomm,8 - 13 years,Not Disclosed,['Noida'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Hardware Engineer, you will plan, design, optimize, verify, and test electronic systems, bring-up yield, circuits, mechanical systems, Digital/Analog/RF/optical systems, equipment and packaging, test systems, FPGA, and/or DSP systems that launch cutting-edge, world class products. Qualcomm Hardware Engineers collaborate with cross-functional teams to develop solutions and meet performance requirements.\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 8+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 7+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 6+ years of Hardware Engineering or related work experience.\nPosition- Principal Engineer/Manager, CAD tools & Methodology\n\nLocationNoida\n\n:\nWe are looking for a senior leader to lead a 20+ CAD team in Noida.\nThe Noida CAD team delivers tools/flows/methodologies to enable Qualcomm to build its most complex SoCs in cutting edge process nodes.\nThe person will be responsible for:\nManaging all CAD functions in Noida- including front-end and RTL2GDS tools.\nDrive tools, flows, methodologies globally as part of world-wide CAD organization.\nDrive local EDA vendor eco-system.\nBe the interface to Qualcomm execution teams in Noida.\nExperience:\nAtleast 15 years experience in development of tools/flows/methodologies in either RTL, DV, synthesis, PnR or Signoff.\nShould have a proven record of driving new innovative tool/flow/methodology solutions.\nShould have managed a medium sized team.\nEducational Qualification:\nPreferred- Masters in VLSI or Computer Science\nMinimum- Bachelors in Electronics/Electrical Engineering/Computer Science",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['synthesis', 'cad', 'hardware engineering', 'rtl', 'pnr', 'catia v5', 'system testing', 'cad tools', 'electrical engineering', 'design engineering', 'autocad', 'vhdl', 'catia', 'verilog', 'electricals', 'rest assured', 'solid works', 'creo', 'pro-e', 'electronics engineering', 'digital transformation', 'gd']",2025-06-12 15:04:33
Full Stack Engineer- Manager,Axtria,5 - 10 years,20-35 Lacs P.A.,"['Noida', 'Pune', 'Bengaluru']","Job description:\nAxtria: -Axtria is a global provider of award-winning cloud software and data analytics to the life sciences industry. Axtria enables life sciences organizations to transform the product commercialization journey and deliver much-improved healthcare outcomes for patients worldwide. We are acutely aware that our work impacts millions of people and are incredibly passionate about the improvement we can bring to patients lives.\nOur focus is on delivering solutions that help pharmaceutical, medical device, and diagnostics companies complete the journey from data to insights to action and get superior returns from their investments. As a participant of the United Nations Global Compact, Axtria is committed to aligning strategies and operations with universal principles on human rights, labor, environment, and anti-corruption and taking actions that advance societal goals.\nOur people are our core strength, and they make us proud of our work; this has helped us grow exponentially and make tremendous strides toward developing great products. It is for the grit, thinking of an entrepreneur, and a family-like environment where each member is valued and treasured that we are growing rapidly.\nFor more information, visit www.axtria.com.\n\nJob Title: - Full Stack Experts ( Open across levels Senior Associate to Associate Director)\nJob Location: -Gurgaon/Bangalore/Pune/Hyderabad\n\nJob Responsibilities: -\nBe an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on UI\nSoftware development experience in REACT, JavaScript/TypeScript, python is needed as frontend and backend for UI based applications\nBe a part of large delivery teams working on advanced projects when expert assistance is required.\nDeliver advanced Data Science capabilities to businesses in a meaningful manner through successful proof-of-concept solutions, and later smoothly transition the proof-of-concept into production.\nCreate Technical documents, develop, test, and deploy data analytics processes using REACT, Python, SQL on Azure/AWS platforms\nCan interact with client on GenAI related capabilities and use cases\n\nQualification: -\n- B-Tech or BE in Computer Science / Computer Applications from Tier 1-2 college\nOR\n- Masters degree in Machine Learning / Statistics / Econometrics, or related discipline from Tier 1-2 college\n\nMust have Skills: -\nRequire 3-15 years of experience to develop, test, and deploy React, JavaScript, Python based applications on Azure/AWS platforms\nMust have experience in building production grade application with frontend & backend technologies like HTML, JavaScript, REACT, Python etc.\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.\nBasic knowledge on concepts of Generative AI / LLMs / GPT will be beneficial\nSkills that give you an edge: -\nStrong analytical skills to solve and model complex business requirements are a plus. With life sciences or pharma background.\nWe will provide (Employee Value Proposition)\nOffer an inclusive environment that encourages diverse perspectives and ideas\nDeliver challenging and unique opportunities to contribute to the success of a transforming organization\nOpportunity to work on technical challenges that may impact across geographies\nVast opportunities for self-development: online Axtria Institute, knowledge sharing opportunities globally, learning opportunities through external certifications\nSponsored Tech Talks & Hackathons\nPossibility to relocate to any Axtria office for short and long-term projects\nBenefit package:\n-Health benefits\n-Retirement benefits\n-Paid time off\n-Flexible Benefits\n-Hybrid /FT Office/Remote\nAxtria is an equal-opportunity employer that values diversity and inclusiveness in the workplace.\nWho we are\nAxtria 14 years journey\nAxtria, Great Place to Work\nLife at Axtria\nAxtria Diversity",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Javascript', 'React.Js', 'Python', 'AWS', 'SQL']",2025-06-12 15:04:35
MDM Data Scientist,Amgen Inc,4 - 9 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking an accomplished and visionary Data Scientist/ GenAIdeveloper to join Amgens Enterprise Data Management team. As part of MDM team, you will be responsible for designing, developing, and deploying Generative AI and ML models to power data-driven decisions across business domains. This role is ideal for an AI practitioner who thrives in a collaborative environment and brings a strategic mindset to applying advanced AI techniques to solve real-world problems.\nTo succeed in this role, the candidate must have strong AI/ML, Data Science, GenAI experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have AI/ML, data science and GenAI experience on technologies like (PySpark/PyTorch, TensorFlow, LLM, Autogen, Hugging FaceVectorDB,Embeddings, RAGsetc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nDevelop enterprise-level GenAI applications using LLM frameworks such as Langchain, Autogen, and Hugging Face.\nDesign and develop intelligent pipelines using PySpark, TensorFlow, and PyTorch within Databricks and AWS environments.\nImplement embedding models andmanage VectorStores for retrieval-augmented generation (RAG) solutions.\nIntegrate and leverage MDM platforms like Informatica and Reltio to supply high-quality structured data to ML systems.\nUtilize SQL and Python for data engineering, data wrangling, and pipeline automation.\nBuild scalable APIs and services to serve GenAI models in production.\nLead cross-functional collaboration with data scientists, engineers, and product teams to scope, design, and deploy AI-powered systems.\nEnsure model governance, version control, and auditability aligned with regulatory and compliance expectations.\nBasic Qualifications and Experience:\nMasters degree with 4 - 6 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 6 - 9 years of experience in Business, Engineering, IT or related field OR\nDiploma with 10 - 12 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\n6+ years of experience working in AI/ML or Data Science roles, including designing and implementing GenAI solutions.\nExtensive hands-on experience with LLM frameworks and tools such as Langchain, Autogen, Hugging Face, OpenAI APIs, and embedding models.\nStrong programming background with Python, PySpark, and experience in building scalable solutions using TensorFlow, PyTorch, and SK-Learn.\nProven track record of building and deploying AI/ML applications in cloud environments such as AWS.\nExpertise in developing APIs, automation pipelines, and serving GenAI models using frameworks like Django, FastAPI, and DataBricks.\nSolid experience integrating and managing MDM tools (Informatica/Reltio) and applying data governance best practices.\nGuide the team on development activities and lead the solution discussions\nMust have core technical capabilities in GenAI, Data Science space\nGood-to-Have Skills:\nPrior experience in Data Modeling, ETL development, and data profiling to support AI/ML workflows.\nWorking knowledge of Life Sciences or Pharma industry standards and regulatory considerations.\nProficiency in tools like JIRA and Confluence for Agile delivery and project collaboration.\nFamiliarity with MongoDB, VectorStores, and modern architecture principles for scalable GenAI applications.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nData Science and ML Certification\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'GenAI', 'Langchain', 'PySpark', 'Hugging Face', 'OpenAI API', 'Autogen', 'PyTorch', 'Django', 'MDM', 'FastAPI', 'Data Modeling', 'ETL', 'TensorFlow', 'Python']",2025-06-12 15:04:38
Full Stack Engineer,Axtria,4 - 9 years,Not Disclosed,"['Noida', 'Hyderabad', 'Bengaluru']","Axtria: -Axtria is a global provider of award-winning cloud software and data analytics to the life sciences industry. Axtria enables life sciences organizations to transform the product commercialization journey and deliver much-improved healthcare outcomes for patients worldwide. We are acutely aware that our work impacts millions of people and are incredibly passionate about the improvement we can bring to patients lives.\nOur focus is on delivering solutions that help pharmaceutical, medical device, and diagnostics companies complete the journey from data to insights to action and get superior returns from their investments. As a participant of the United Nations Global Compact, Axtria is committed to aligning strategies and operations with universal principles on human rights, labor, environment, and anti-corruption and taking actions that advance societal goals.\nOur people are our core strength, and they make us proud of our work; this has helped us grow exponentially and make tremendous strides toward developing great products. It is for the grit, thinking of an entrepreneur, and a family-like environment where each member is valued and treasured that we are growing rapidly.\nFor more information, visit www.axtria.com.\nJob Title: - Full Stack Experts ( Open across levels – Senior Associate to Associate Director)\n\nJob Location: -Gurgaon/Bangalore/Pune/Hyderabad\nJob Responsibilities: -\nBe an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on UI\nSoftware development experience in REACT, JavaScript/TypeScript, python is needed as frontend and backend for UI based applications\nBe a part of large delivery teams working on advanced projects when expert assistance is required.\nDeliver advanced Data Science capabilities to businesses in a meaningful manner through successful proof-of-concept solutions, and later smoothly transition the proof-of-concept into production.\nCreate Technical documents, develop, test, and deploy data analytics processes using REACT, Python, SQL on Azure/AWS platforms\nCan interact with client on GenAI related capabilities and use cases\nQualification: -\n- B-Tech or BE in Computer Science / Computer Applications from Tier 1-2 college\nOR\n- Master’s degree in Machine Learning / Statistics / Econometrics, or related discipline from Tier 1-2 college\nMust have Skills: -\nRequire 3-15 years of experience to develop, test, and deploy React, JavaScript, Python based applications on Azure/AWS platforms\nMust have experience in building production grade application with frontend & backend technologies like HTML, JavaScript, REACT, Python etc.\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.\nBasic knowledge on concepts of Generative AI / LLMs / GPT will be beneficial\nSkills that give you an edge: -\nStrong analytical skills to solve and model complex business requirements are a plus. With life sciences or pharma background.\nWe will provide– (Employee Value Proposition)\nOffer an inclusive environment that encourages diverse perspectives and ideas\nDeliver challenging and unique opportunities to contribute to the success of a transforming organization\nOpportunity to work on technical challenges that may impact across geographies\nVast opportunities for self-development: online Axtria Institute, knowledge sharing opportunities globally, learning opportunities through external certifications\nSponsored Tech Talks & Hackathons\nPossibility to relocate to any Axtria office for short and long-term projects\nBenefit package:\n-Health benefits\n-Retirement benefits\n-Paid time off\n-Flexible Benefits\n-Hybrid /FT Office/Remote\nAxtria is an equal-opportunity employer that values diversity and inclusiveness in the workplace.\nWho we are\nAxtria 14 years journey\nAxtria, Great Place to Work\nLife at Axtria\nAxtria Diversity",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['React.Js', 'Python', 'AWS']",2025-06-12 15:04:40
Principal Engineer Software Developer,Indian / Global Engineering & Manufactur...,9 - 14 years,Not Disclosed,['Manesar'],"Key Skills: Embedded Software Development, C++, Microcontroller, Embedded C, Automotive embedded, LIN, CAN, CANalyzer\nRoles and Responsibilities:\nResponsible for Automotive Embedded Technology Product like ECU, TCU, Controllers, On/Off board charger based Embedded Electronics like Analog, Digital, MCU, Sensors, Power Supplies and Power Electronics\nSoftware requirement understanding and product architecture.\nSoftware flaw less launch of product/Product life cycle management, align with group Goals.\nDevelops new function/module, contribute for new process development/tailoring existing process.\nResponsible for software development, design documents and test setup.\nAlign hardware test activities to meet Product Development Process schedules using best practices and tools.\nTesting automation and maintaining manual documentation regression suites for Part components for Software releases.\nParticipating in project team discussions on product design and presenting test results to development teams and management.\nContributing in a meaningful way to team goals and initiatives to increase quality and efficiency of software test processes.\nSkills Required:\nTechnical/Functional Competencies Embedded Software/Hardware:\nHands-on experience in application software and embedded software in automotive application.\nExperience in digital controls and interfaces like PID/ PWM timers/LCD/EEPROM/interface of sensors for volt, current, and temperature etc.\nwork experience in Embedded C/C++.\nMust have worked on 8 bit, 16bit, 32bit, Renesas, Cypress, Fujitsu, ARM M0/M1/M2/M3/M4 microcontrollers.\nPreferable if candidate has worked on Renesas microcontrollers.\nWork experience on PWM Timer and controls, LCD interface, UART, ADC, DAC, PGA, DMA, GPIO, Interrupts handling, Exception handling, WatchDog Timer, Software Timers/UART etc.\nMust have hands-on experience in communication protocols like LIN, CAN, I2C, SPI, UART, RS232, RS485.\nFirmware debugging experience with JTAG, Single wire debug interface, RS232, UART.\nUnderstanding of IVN Network, UDS, KWP2000, IO vehicle test.\nHands on Toll like CANalyser, CANoe, CAPL scripting etc.\nShould have ability to create test cases for Embedded C code and design documents.\nTechnical/Functional Competencies Hardware:\nExperience in writing/Software debugging, Software Compliance Standard\nInterpret test cases as per the OEM test case with relevant test standards.\nGood organizational and communication skills.\nAbility to work effectively with cross-functional teams and suppliers.\nA knowledge of Controller Area Network CAN and LIN communications protocols\nEducation: Bachelor's or Master's degree in Computer Science, Engineering, or a related technical field",Industry Type: Electronics Manufacturing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Embedded C', 'C++', 'Microcontroller', 'Automotive embedded', 'Embedded Software Development', 'LIN', 'CAN', 'CANalyzer']",2025-06-12 15:04:42
WLAN Test- Staff/ Sr Staff/ Principal Engineer,Qualcomm,6 - 11 years,Not Disclosed,['Chennai'],"Job Area: Engineering Group, Engineering Group > Systems Test Engineering\n\nGeneral Summary:\n\nYou will join the System Test team that is responsible for defining and implementing the overall testing strategy for WiFi & Networking access points. This involves development of test plans, tools and automation framework for validating and qualifying the WiFi routers. You will work closely with systems team, development, and architecture team to understand the features and define test plans and solutions needed to deliver production grade software/firmware to the end customer.\n\nYou will be collaborating with a variety of internal teams in Qualcomm covering multiple engineering disciplines including software, systems, and hardware. The successful applicant should have a diverse skill set and a strong background in continuous testing and automation strategies\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 6+ years of Systems Test Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 5+ years of Systems Test Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 4+ years of Systems Test Engineering or related work experience.\n\nResponsibilities:\n\nMust be responsible for analyzing new feature and develop/create new test plans and adding new test cases\n\nManage Infrastructure, develop test topologies and prepare use cases for validation\n\nWork with cross functional teams for supporting end-to-end release\n\nDirects small team of engineers on gathering, integrating, and interpreting information from a variety of sources in-order to troubleshoot issues and find solutions.\n\nDevelop the right skill and train the team. Serves as a mentor to Engineers and Senior Engineers and teaches them about complex features, systems, testing strategies, and automation.\n\nConduct log analysis with team members to identify where an issue has occurred and makes recommendations for how to address the issue.\n\nNetworks with colleagues within own domain to gain insight, ideas, and connections. Shares information with peers and junior engineers.\n\nCollaborates with functional and lab teams, IO teams, network operators, field teams, and product management teams to ensure that the testing plan is accurate for addressing feature issues.\n\nMinimum Qualifications\n\nBachelor's or Masters degree in Engineering, Information Systems, Computer Science, Electronics & communications or related field.\n\n15+ years in WiFi/Networking Test, automation or Software Engineering\n\nExperience with Programming Language such as Python, Shell Script (optional)\n\nRequired Skills and Aptitudes\n\nShould possess strong knowledge in WLAN/networking and manual testing of networking products\n\nMust have good experience in testing of layer-2 to layer 7 protocols\n\nPossess high Debugging capability\n\nStrong problem-solving skills\n\nAbility to prioritize and execute tasks across multiple projects with tight deadlines and aggressive goals.\n\nExperience in scripting languages like python (optional)\n\nExperience in shell scripting and windows batch commands (optioinal)\n\nExcellent English communication (written and verbal) and interpersonal skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['software testing', 'manual testing', 'networking', 'networking products', 'layer 2', 'python', 'regression testing', 'automation testing', 'functional testing', 'test engineering', 'wlan testing', 'linux', 'test planning', 'debugging', 'software engineering', 'shell scripting']",2025-06-12 15:04:45
Engineer/Senior Engineer/Lead Engineer/Principal Engineer - Wastewater,Aecom,2 - 7 years,Not Disclosed,['Bengaluru'],"Engineer/Senior Engineer/Lead Engineer/Principal Engineer - Wastewater Modelling.\nProvide technical mentorship to junior modellers, review their work, and deliver training sessions on modelling concepts.\nCapable for leading the project, coordinating with Lead Offices, mentoring juniors and ensuring quality checks\nComplete design/hydraulic modellingactivities as per specified standards for different regional projects.\nAssist in establishing processes for working with ANZ/US/UK&I/ME offices.\nIndependently manage project tasks, ensuring on-time delivery while adhering to AECOM's quality standards.\nLead project execution, liaise with the Lead Office, and effectively manage project resources.\nConduct comprehensive QA/QC reviews on models to ensure compliance with client standards and industry best practices.\nIndependently troubleshoot and enhance hydraulic models for improved accuracy and performance.\nCollaborate with multidisciplinary teams, including engineers, GIS specialists, and project managers, to drive seamless project execution.\nSupport bid preparation and program management, contributing to business growth and project acquisition.\n\n\nQualifications\nMaster of Engineering degree (or equivalent education) in an appropriate engineering discipline from an accredited college or university.\nChartered Engineer (CEng), or Professional Engineer (PE) license or equivalent in the relevant field from any global organization (e.g., Institution of Civil Engineers, UK) oractively working towards achieving chartership.\nHaving 2 to 12 years of working experience on projects from UK & Ireland, US, Middle East, and ANZ regions.\nWork on model build and calibration, flood mitigation schemes, CSO spill reduction program etc. using InfoWorks ICM for wastewater modelling projects across UK & Ireland, US, Middle East, and ANZ regions. Proficiency in 2D and water quality modelling is an added advantage.\nProficiency on hydraulic modelling & design of foul sewer, storm sewer, urban drainage networks using Infoworks ICM, SewerGEMS, PCSWMM etc.\nHighly motivated, hardworking, interpersonal, and enthusiastic team player that is willing to learn and adapt to change.\nUS/UK/Canada Experience will be added advantage\nGood communication skills, and ability to work well independently at times.\nAble to see the bigger picture and take a birds-eye view of projects\nConfident, with the ability to work either independently or as part of a team.\nAbility to work to deadlines and under pressure.\nAccountability for assigned work.\nAccuracy & precision of work.\nWillingness to learn and develop.\nExcellent written and verbal communication skills\nStrong problem-solving skills\nEnthusiastic and Self-motivated.\nWork well within a multidisciplinary team",Industry Type: Building Material (Cement),Department: Construction & Site Engineering,"Employment Type: Full Time, Permanent","['hydraulic modelling', 'wastewater modelling', 'project management', 'program management', 'water quality modelling']",2025-06-12 15:04:47
"Machine Learning, Technical Lead - NLP / LLM",Avalara Technologies,6 - 10 years,Not Disclosed,[],"What You'll Do\nWe are looking for experienced Machine Learning Engineers with a background in software development and a deep enthusiasm for solving complex problems. You will lead a dynamic team dedicated to designing and implementing a large language model framework to power diverse applications across Avalara.\nYour responsibilities will span the entire development lifecycle, including conceptualization, prototyping and delivery of the LLM platform features. You will build core agent infrastructureA2A orchestration and MCP-driven tool discoveryso teams can launch secure, scalable agent workflows. You will be reporting to Senior Manager, Machine Learning",,,,"['Machine Learning', 'NLP', 'Docker', 'Terraform', 'MLFlow', 'Prometheus', 'LLM', 'AWS', 'Grafana', 'GitLab', 'Kubernetes']",2025-06-12 15:04:50
WiFi Connectivity - Sr Staff/ Principal Engineer,Qualcomm,15 - 20 years,Not Disclosed,['Chennai'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\nMinimum 15 yrs in SW Engineering life cycle.\nMinimum 10 yrs in SW engineering roles that covers SW development and test.\nStrong technology focus and experience in networking technology areas that include WLAN, Ethernet, Bridging and Routing.\nStrong understanding SW architecture and real time embedded system design with Linux Operating System\nExperience in detailed planning, reporting, defining and managing engineering / technology metrics\nDecisive and ability to quickly identify problems & make decisions to solve them.\nStrong interpersonal and communication skills\nOwnership of commercialization of the key Wi-Fi access point products\nTechnical product owner of the releases responsible of driving all technology areas to deliver world class AP and Wi-Fi Routers\nPrimary technology and commercialization interface with product management and engineering teams\nEnd to End ownership to drive programs from start to the post launch in the field.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 6+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 5+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Engineering or related work experience.\n\n3+ years of work experience with Programming Language such as C, C++, Java, Python, etc.\nExperience in managing programs & meets required program specifications with required quality, content & cost\nGlobal program management experience across geos\nWorking with cross geo teams in US and China\nCustomer interactions and Product Marketing interfacing experience",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'linux', 'software engineering', 'sw', 'embedded system design', 'switching', 'eigrp', 'networking', 'routing', 'java', 'rtos', 'computer science', 'embedded systems', 'lan', 'pcb designing', 'arm', 'tcp', 'matlab', 'python', 'c', 'software testing', 'ospf', 'ethernet', 'embedded c', 'microsoft windows', 'ccna']",2025-06-12 15:04:52
Data Scientist IV - Python / LLM,Sadup Soft,6 - 8 years,Not Disclosed,['Hyderabad'],"Must have skills :\n\n- 6+ Years of Experience.\n\n- Statics, SQL, Big query, LLM, AI, Python\n\n- Work experience in the payments, ecommerce, or financial services industry is a plus .\n\nResponsibilities :\n\n- At least 6 years of experience analyzing large, multi-dimensional data sets and synthesizing insights into actionable solutions\n\n- Bachelor's/Master's degree in a quantitative field (such as Analytics, Statistics, Mathematics, Economics or Engineering) or equivalent field experience\n\n- Advanced SQL experience, preferable with Big Query analytics (Google Cloud) on Jupyter Notebooks and experience analyzing very large, complex, multi-dimensional data sets.\n\n- Understanding of statistics (e.g hypothesis testing, statistical inference, regression) and experience designing and evaluating complex experiments-\n\n- Ability to solve problems analytically and create actionable recommendations\n\n- Advanced ability to use reporting tools like Tableau and/or Excel to share analysis\n\n- Strong written and verbal communication skills with the ability to translate complex problems into simpler terms, expertise in stitching together findings to convey coherent insights and effectively influence both peers and senior leadership\n\n- Prior work experience in a product analytics space would be highly valued\n\n- A passion for problem-solving and comfort with ambiguity\n\n- Work experience in the payments, ecommerce, or financial services industry is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'Data Science', 'BigQuery', 'Data Management', 'Jupyter', 'LLM', 'Statistics']",2025-06-12 15:04:54
RTL Design - Sr Staff/ Principal Engineer,Qualcomm,15 - 20 years,Not Disclosed,['Chennai'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n15+ years of experience in SoC design\nShould have knowledge of AMBA protocols - AXI, AHB, APB, SoC clocking/reset/debug architecture and peripherals like USB, PCIE and SDCC.\nUnderstanding of Memory controller designs and microprocessors is an added advantage\nHands on experience in constraint development and timing closure\nWork closely with the SoC verification and validation teams for pre/post Silicon debug\nHands on experience in Low power SoC design is required\nExperience in Synthesis / Understanding of timing concepts for ASIC is required.\nHands on experience in Multi Clock designs, Asynchronous interface is a must.\nExperience in using the tools in ASIC development such as Lint, CDC, Design compiler and Primetime is required\n\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 6+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 5+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 4+ years of Hardware Engineering or related work experience.\n12+ years of experience with a Bachelor's/ Masters degree in Electrical/ Electronics engineering",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['asynchronous', 'soc design', 'asic development', 'hardware engineering', 'lint', 'soc verification', 'usb', 'soc', 'amba', 'apex', 'salesforce', 'design compiler', 'apb', 'rtl design', 'axi', 'ahb', 'microprocessors', 'protocols', 'synthesis', 'asic', 'cdc', 'primetime', 'rtl', 'verilog', 'silicon', 'timing closure', 'pcie']",2025-06-12 15:04:57
Senior Data Analyst-Azure Data Factory,Lumen Technologies,8 - 12 years,Not Disclosed,['Bengaluru'],"Were looking for a Senior Data Analyst with a strong foundation in Azure-based data engineering and Machine Learning to design, develop, and optimize robust data pipelines, applications, and analytics infrastructure. This role demands deep technical expertise, cross-functional collaboration, and the ability to align data solutions with dynamic business needs.\nKey Responsibilities:\nData Pipeline Development:\nDesign and implement efficient data pipelines using Azure Databricks with PySpark to transform and process large datasets.\nOptimize data workflows for scalability, reliability, and performance.\nApplication Integration:\nCollaborate with cross-functional teams to develop APIs using the .NET Framework for Azure Web Application integration.\nEnsure smooth data exchange between applications and downstream systems.\nData Warehousing and Analytics:\nBuild and manage data warehousing solutions using Synapse Analytics and Azure Data Factory (ADF).\nDevelop and maintain reusable and scalable data models to support business intelligence needs.\nAutomation and Orchestration:\nUtilize Azure Logic Apps, Function Apps, and Azure DevOps to automate workflows and streamline deployments.\nImplement CI/CD pipelines for efficient code deployment and testing.\nInfrastructure Management:\nOversee Azure infrastructure management and maintenance, ensuring a secure and optimized environment.\nProvide support for performance tuning and capacity planning.\nBusiness Alignment:\nGain a deep understanding of AMO data sources and their business implications.\nWork closely with stakeholders to provide customized solutions aligning with business needs.\nBAU Support:\nMonitor and support data engineering workflows and application functionality in BAU mode.\nTroubleshoot and resolve production issues promptly to ensure business continuity.\nTechnical Expertise:\nProficiency in Microsoft SQL for complex data queries and database management.\nAdvanced knowledge of Azure Databricks and PySpark for data engineering and ETL processes.\nExperience with Azure Data Factory (ADF) for orchestrating data workflows.\nExpertise in Azure Synapse Analytics for data integration and analytics.\nProficiency in .NET Framework for API development and integration.\nCloud and DevOps Skills:\nStrong experience in Azure Infrastructure Management and optimization.\nHands-on knowledge of Azure Logic Apps, Function Apps, and Azure DevOps for CI/CD automation.\n""We are an equal opportunity employer committed to fair and ethical hiring practices. We do not charge any fees or accept any form of payment from candidates at any stage of the recruitment process. If anyone claims to offer employment opportunities in our company in exchange for money or any other benefit, please treat it as fraudulent and report it immediately.""\n#LI-BS1",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'orchestration', 'Infrastructure management', 'Machine learning', 'Business intelligence', 'Business continuity', 'Analytics', 'Downstream', 'Capacity planning']",2025-06-12 15:05:00
Sr. Principal Software Engineer,Morningstar,12 - 18 years,Not Disclosed,"['Thane', 'Navi Mumbai', 'Mumbai (All Areas)']","Position Title: Senior Principal Software Engineer\nThe Area: Morningstar Data for equities provides comprehensive coverage of global stock markets from as early as 1975. We are continuously broadening our coverage and creating new products to help clients prepare for regulatory changes and other industry shifts. Our data features proprietary statistics and is developed using stringent quality screens to ensure accuracy. From APIs to data feeds, our solutions are delivered quickly to help institutions meet a broad range of functions. The Role: In this role, you will collaborate with technology manager, scrum Master, functional experts, and developers to build technology solutions for Morningstar`s Equity applications. The team is looking for forward-thinking problem solvers who thrive in a fastpaced environment. The Lead is responsible in mentoring the team members, provide guidance and opportunities for the team members to expand their capabilities and skills. They will have to coordinate and work with the members in a global team. We are looking for an individual that can apply discipline, create solid software products.\n\nResponsibilities:\n• Design & develop web and enterprise solutions to be flexible, scalable & extensible.\n• Improve complex data flow, data structures and db design to move to next platform.\n• Enforce good agile practices like test driven development, Continuous Integration.\n• Hands-on development will be an integral part of the responsibilities.\n• Develop areas of continuous and automated deployment.\n• Introduce and follow good development practices, innovative frameworks and technology solutions that help business move faster.\n• Follow best practices like estimation, planning, reporting and improvement brought to processes in every day work.\n• Analyses and reviews system requirements. Uses requirement and other design documents to gain overall understanding of the functionality of the new or enhanced application.\n• Participate actively in the design, architecture and build phases, to aim at producing high quality deliverables.\n• Provide recommendations on product and development environment improvements.\n\nRequirements:\nThese are the most important skills, qualities, etc. that we’d like for this role.\n• Minimum 12 Years of experience\n• Bachelor of Science in Computer Science, Engineering, or equivalent.\n• At least 2+ years as a software Lead/Architect\n• Demonstrated familiarity with AI-powered assistants (e.g., GitHub Copilot, ChatGPT) for code generation, debugging, and/or other technical tasks.\n• Hands-on in Java 8, Adv Java, Spring Framework\n• Strong knowledge and hands-on on micro-services based architecture.\n• Very Strong knowledge of databases and hands on MS SQL/MySQL/PostgreSQL or NoSQL DB DynamoDB/MongoDB.\n• Experience with building REST based APIs.\n• Experience in analysis, design, coding and implementation of large-scale, n-tier Java based platforms.\n• Knowledge of any JavaScript framework like Vue, Angular JS (version >2)/ NodeJS etc.\n• Be aware of activity in the open source world. Contributing back to open source is a big plus.\n• Distributed computing, with experience in cloud computing (Amazon Web Services platform and associated technologies)\n• Experience on agile practices\n• Experience with modern development practices in areas of Product design, Requirement Analysis, Test Driven Development, Automation & Unit Testing, in a product development environment.\n• Excellent listening, written and verbal communication skills. Good to Have: • Machine Learning knowledge.\n• Exposure to Capital Market domain preferred (Indexes, Equity etc.) Morningstar is an equal opportunity employer",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java Fullstack', 'Java', 'Vue.Js', 'Java Spring Boot', 'Spring Boot', 'React.Js', 'AWS', 'Angular']",2025-06-12 15:05:02
"Applied Scientist, Amazon Autos",Amazon,3 - 8 years,Not Disclosed,['Gurugram'],"Interested in building something new? Join the Amazon Autos team on an exhilarating journey to redefine the vehicle shopping experience.\nThis is an opportunity to be part of the Amazons new business ventures. Our goal is to create innovative automotive discovery and shopping experiences on Amazon, providing customers with greater convenience and a wider selection.\nYoull work in a creative, fast-paced, and entrepreneurial environment at the center of Amazons innovation. As a key member, youll play a pivotal role in helping us achieve our mission. We are looking for a highly accomplished Applied Science professional drive our science strategy, foster a culture of data-driven decision-making, and drive impactful business outcomes through advanced state-of-the-art science methodologies.\nIf youre enthusiastic about innovating and delivering exceptional shopping experiences to customers, thrive on new challenges, and excel at solving complex problems using top-notch ML models, LLM and GenAI techniques, then youre the perfect candidate for this role. Strong business acumen and interpersonal skills are a must, as youll work closely with business owners to understand customer needs and design scalable solutions.\nJoin us on this exhilarating journey and be part of redefining the vehicle shopping experience.\n\n\nAs an Applied Scientist in Amazon Autos, you will:\n\nShape the roadmap and strategy for applying science to solve customer problems in the Amazon AutoStore domain.\nDrive big picture innovations with clear roadmaps for intermediate delivery.\nApply your skills in areas such as deep learning and reinforcement learning while building scalable solutions for business problems.\nProduce and deliver models that help build best-in-class customer experiences and build systems that allow us to deploy these models to production with low latency and high throughput.\nUtilize your Generative AI, time series and predictive modeling skills, and creative problem-solving skills to drive new projects from ideation to implementation.\nInterface with business customers, gathering requirements and delivering science solutions.\nCollaborate with cross-functional teams, including software engineers, data scientists, and product managers, to define project requirements, establish success metrics, and deliver high-quality solutions.\nEffectively communicate complicated machine learning concepts to multiple partners.\nResearch new and innovative machine learning approaches.\n\nA day in the life\nIn this role, you will be part of a multidisciplinary team working on one of Amazons newest business ventures. As a key member, you will collaborate closely with engineering, product, design, operations, and business development to bring innovative solutions to our customers.\nYour science expertise will be leveraged to research and deliver novel solutions to existing problems, explore emerging problem spaces, and create new knowledge. You will invent and apply state-of-the-art technologies, such as large language models, machine learning, natural language processing, and computer vision, to build next-generation solutions for Amazon.\nYoull publish papers, file patents, and work closely with engineers to bring your ideas to production.\n\nAbout the team\nThis is a critical role for Amazon Autos team with a vision to create innovative automotive discovery and shopping experiences on Amazon, providing customers better convenience and more selection. We re collaborating with other experienced teams at Amazon to define the future of how customers research and shop for cars online. 3+ years of building models for business application experience\nPhD, or Masters degree and 4+ years of CS, CE, ML or related field experience\nExperience in patents or publications at top-tier peer-reviewed conferences or journals\nExperience programming in Java, C++, Python or related language\nExperience in any of the following areas: algorithms and data structures, parsing, numerical optimization, data mining, parallel and distributed computing, high-performance computing Experience using Unix/Linux\nExperience in professional software development\nExperience building complex software systems, especially involving deep learning, machine learning and computer vision, that have been successfully delivered to customers",,,,"['Unix', 'Computer vision', 'C++', 'Linux', 'Machine learning', 'Data structures', 'Product design', 'Data mining', 'Automotive', 'Python']",2025-06-12 15:05:05
ML Data Associate-II,Amazon,2 - 7 years,Not Disclosed,['Chennai'],"AI is the most transformational technology of our time, capable of tackling some of humanity s most challenging problems. Amazon is investing in generative AI and the responsible development and deployment of large language models (LLMs) across all of our businesses. Come build the future of human-technology interaction with us.\n\nWe are looking for those candidates who just don t think out of the box, but make the box they are in Bigger . The future is now, do you want to be a part of it? Then read on!\n\n\nMaintain and follow strict confidentiality as customer privacy is our most important tenet\nWork with a range of different types of data including, but not limited to: text, speech, audio, image, and video\nDeliver high-quality labelled data, using guidelines provided to meet our KPIs and using in-house tools and software, as part of Amazons commitment to developing and deploying AI responsibly.\nDemonstrate proficiency in generating high quality human insight data across a range of modalities, inclusive of text, image video and audio.\nCapable of making sound judgments and logical decisions when faced with ambiguous or incomplete information while performing tasks.\nEye for detail and ability to pivot from one category of requirement to another instantaneously.\nDemonstrate support on daily operational deliverables for multiple task types assigned to you and the team\nAnalyze root causes, identify error patterns, and propose solutions to enhance the quality of labeling tasks and their outputs.\nResponsible for identifying day-to-day process and operational issues in Standard Operating Procedure, tools and suggest changes to unblock operations\nDemonstrate ownership in floor support to clarify internal queries during execution on need basis\n\nA day in the life\nWe are looking for a ML Data Associate (MLDA) to undertake the task of foundational labeling functions, such as dialogue evaluation on speech, text, audio, video data.\n\nYour ability to concentrate, multi-task and your high attention to detail helps you deliver high-quality work as well as maintaining strict confidentiality and follow all applicable Amazon policies for securing confidential information. You will be a part of a diverse team with the shared vision of improving customers lives with practical, useful generative AI innovations. An inner drive, individuality, and a creative mind are extremely beneficial.\nAn Associate s Degree or related work experience\nStrong business writing skills with ability to create reports, proposals, and professional correspondence\nAdvanced reading comprehension with ability to analyze complex business documents\nDeveloped analytical thinking and structured problem-solving capabilities\nStrong ability to interpret and implement detailed instructions across various projects\nProficient research skills with experience gathering and synthesizing information from multiple sources\nProven attention to detail in managing complex tasks and documents\nExperience managing stakeholder relationships across departments\nAdvanced proficiency in Microsoft Office Suite and common business applications.\nBachelor s degree in a relevant field May vary in other locations like India\nC1+ or equivalent fluency in English language\n2+ years of professional work experience with demonstrated task execution ability\nProven capacity to leverage open-source resources effectively for comprehensive research purposes\nAbility to adapt well to fast-paced environments with changing circumstances, direction, and strategy\n1+ years project coordination or management experience (for support functions teams)\nExperience managing stakeholder relationships across departments\nAdvanced proficiency in Microsoft Office Suite and common business applications.",,,,"['Business writing', 'Analytical', 'Project coordination', 'Associate II', 'Research', 'Open source', 'Business applications', 'MS Office', 'Operations', 'Data Associate']",2025-06-12 15:05:07
Deep Learning Research Engineer (Vision & Audio Focus),HIREXpert,4 - 7 years,15-20 Lacs P.A.,"['Bhubaneswar', 'Bengaluru']","Strong programming skills in Python, with experience in PyTorch or TensorFlow.\nHands-on experience with CNNs\nSolid knowledge of Vision Transformers, including recent architectures (e.g., Swin, DeiT).\nYOLO, SSD, Faster R-CNN, RetinaNet, DETR\n\nRequired Candidate profile\nExperience in video analysis and temporal modeling.\nStrong grasp of audio classification workflows and features.\nlarge-scale datasets and designing data pipelines.",Industry Type: Medical Services / Hospital (Diagnostics),Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Pytorch', 'Deep Learning', 'Deep Learning Frameworks', 'Research And Development', 'Machine Learning', 'Python']",2025-06-12 15:05:09
Data & Analytics Specialist,Hoffmann La Roche,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\n.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\nA healthier future drives us to innovate. Together, more than 100 000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. Our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our Diagnostics products. We empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-12 15:05:11
Data & Analytics Specialist,Roche Diagnostics,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\nAt Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we ve become one of the world s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\n.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-12 15:05:14
Data Engineer,Xenonstack,2 - 5 years,Not Disclosed,['Mohali( Phase 8B Mohali )'],"At XenonStack, We committed to become the Most Value Driven Cloud Native, Platform Engineering and Decision Driven Analytics Company. Our Consulting Services and Solutions towards the Neural Company and its Key Drivers.\nXenonStacks DataOps team is looking for a Data Engineer who will be responsible for employing techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field.\nYou should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. The ideal candidate should be highly skilled in all aspects of Python, Java/Scala, SQL and analytical skills.\nJob Responsibilities:\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with co-workers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analysed quickly by BI & AI Teams.\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant co-workers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nRequirements:\nTechnical Requirements\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch, Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores.\nProfessional Attributes:\nExcellent communication skills & Attention to detail.\nAnalytical mind and problem-solving Aptitude with Strong Organizational skills & Visual Thinking.\nBenefits:\nDiscover the benefits of joining our team:\nDynamic and purposeful work culture in a people-oriented organization contributing to multi-million-dollar projects with guaranteed job security.\nOpen, authentic, and transparent communication fostering a warm work environment.\nRegular constructive feedback and exposure to diverse technologies.\nRecognition and rewards for exceptional performance achievements.\nAccess to certification courses & Skill Sessions to develop continually and refine your skills.\nAdditional allowances for team members assigned to specific projects.\nSpecial skill allowances to acknowledge and compensate for unique expertise.\nComprehensive medical insurance policy for your health and well-being.\nTo Learn more about the company -\nWebsite - http://www.xenonstack.com/",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Spark', 'ETL', 'Python', 'SQL', 'Java', 'Data Processing', 'Machine Learning']",2025-06-12 15:05:16
"Data Engineer, AVP",NatWest Markets,16 - 18 years,Not Disclosed,['Gurugram'],"Join us as a Data Engineer\nWe re looking for someone to build effortless, digital first customer experiences to help simplify our organisation and keep our data safe and secure\nDay-to-day, you ll develop innovative, data-driven solutions through data pipelines, modelling and ETL design while inspiring to be commercially successful through insights\nIf you re ready for a new challenge, and want to bring a competitive edge to your career profile by delivering streaming data ingestions, this could be the role for you\nWere offering this role at assistant vice president level\nWhat you ll do\nYour daily responsibilities will include you developing a comprehensive knowledge of our data structures and metrics, advocating for change when needed for product development. You ll also provide transformation solutions and carry out complex data extractions.\nWe ll expect you to develop a clear understanding of data platform cost levels to build cost-effective and strategic solutions. You ll also source new data by using the most appropriate tooling before integrating it into the overall solution to deliver it to our customers.\nYou ll also be responsible for:\nDriving customer value by understanding complex business problems and requirements to correctly apply the most appropriate and reusable tools to build data solutions\nParticipating in the data engineering community to deliver opportunities to support our strategic direction\nCarrying out complex data engineering tasks to build a scalable data architecture and the transformation of data to make it usable to analysts and data scientists\nBuilding advanced automation of data engineering pipelines through the removal of manual stages\nLeading on the planning and design of complex products and providing guidance to colleagues and the wider team when required\nThe skills you ll need\nTo be successful in this role, you ll have an understanding of data usage and dependencies with wider teams and the end customer. You ll also have experience of extracting value and features from large scale data.\nWe ll expect you to have experience of ETL technical design, data quality testing, cleansing and monitoring, data sourcing, exploration and analysis, and data warehousing and data modelling capabilities.\nYou ll also need:\nExperience of using programming languages alongside knowledge of data and software engineering fundamentals\nGood knowledge of modern code development practices\nGreat communication skills with the ability to proactively engage with a range of stakeholders\nHours\n45\nJob Posting Closing Date:\n16/06/2025",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Usage', 'Technical design', 'Programming', 'Data structures', 'Data quality', 'Assistant Vice President', 'Data warehousing', 'Monitoring', 'Data architecture']",2025-06-12 15:05:18
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nAs part of the cybersecurity organization, In this vital role you will be responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The role sits at the intersection of data infrastructure and business insight delivery, requiring the Data Engineer to design and build robust data pipelines while also translating data into meaningful visualizations for stakeholders across the organization. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nBuild data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nDevelop and maintain interactive dashboards and reports using tools like Tableau, ensuring data accuracy and usability\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\n\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, GitLab, LucidChart, etc.\nHands-on experience with data visualization and dashboarding toolsTableau, Power BI, or similar is a plus\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\n\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\n\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\n\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data engineering', 'data analysis', 'data modeling', 'analysis tools', 'data warehousing', 'troubleshooting', 'data architecture', 'data integration', 'etl process']",2025-06-12 15:05:20
AI Python Data Science Engineer,Probeseven,4 - 5 years,Not Disclosed,['Coimbatore'],"AI Python Data Science Engineer\nHot Openings\nAs a data science and analytics engineer, you will be involved in developing computer visions and data algorithms. Artificial intelligence development and deep machine learning implementations will be part of your development and deployment to the cloud.\n\nExperience for senior positions: 4 - 5+ years\nExperience for junior positions: 2 - 3 years\n\nRequired Tech Skills\nExperience in Python (must have) and/or R language.\nExperience with computer vision algorithms and data science.\nExperience in deep machine learning models.\nWell-versed in data visualization techniques.\nTroubleshoot and resolve code issues.\nCollaborate with data engineers to design and integrate the data sources.\nExperience in handling multiple priorities with Agile development.\nExperience with Git and working in a collaborative and distributive team environment.\nRequired Soft Skills\nExcellent listening, verbal, and written communication skills.\nStrong interpersonal & customer relationship skills.\nStrong analytical, problem solving, and decision-making skills.\nDocumentation skills.\nApply now\nHot Openings\nPHP + Node.js Developers\nFull Time\nTech Development\nExperience 4 - 6+ years",Industry Type: Film / Music / Entertainment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'GIT', 'data science', 'Analytical', 'Artificial Intelligence', 'Machine learning', 'PHP', 'Customer relationship', 'Analytics', 'Python']",2025-06-12 15:05:23
Associate Specialist Data Science,Merck Sharp & Dohme (MSD),2 - 7 years,Not Disclosed,['Pune'],"Primary Responsibilities\nSupport in establishing frameworks to standardize, productize and scale existing and new capabilities / analytical solutions\nImplement the vision, roadmap, and best practices for the Data Science Center of Excellence ( CoE ) to align with business goals\nSupport establishing governance frameworks to measure the value of products, standardize data science methodologies, coding practices, and project workflows\nWork with senior CoE members in development and maintenance of best practices for model and algorithm development and design, deployment, and monitoring across the enterprise functions\nCollaborate with product team on product development incorporating Agile framework and latest industry best practices and norms\nSupport in development of MLOps and ModelOps frameworks to streamline the development-to-deployment product pipeline\nDrive innovation by identifying, evaluating, and implementing cutting-edge data science methodologies based on latest published literature\n\nQualifications\nEducation & Work Experience Requiremen ts:\nMaster s degree (relevant field like Economics, Statistics, Mathematics, Operational Research) with 2+ years work experience.\nBachelor s degree (in Engineering or related field, such as Computer Science, Data Science, Statistics, Business, etc.) with at least 3 + years relevant experience\nPrior experience in research publications in reputed journal is a plus\nSkillset:\nCandidates must have -\nStrong programming skills in languages such as Python or R, and SQL with experience in data manipulation and analysis libraries (e.g., pandas, NumPy, scikit-learn, stats models)\nExperience with data science principles, machine learning (supervised and unsupervised) and GenAI algorithms, test-control analysis, propensity score matching etc.\nExposure to product roadmaps, Agile methodologies and backlog management, ensuring iterative and incremental product improvements\nStrong problem solving, business analysis and quantitative skills\nAbility to effectively communicate proposals to key stakeholders\nCandidates are desired but not mandatory to have -\nExperience and familiarity with underlying concepts such as Patient analytics, MMx etc.\nUnderstanding of Pharma commercial landscape will be a plus\nExperience working with healthcare, financial, or enterprise SaaS products\n  Search Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nNot Applicable\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness Intelligence (BI), Database Design, Data Engineering, Data Modeling, Data Science, Data Visualization, Machine Learning, Software Development, Stakeholder Relationship Management, Waterfall Model",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Relationship management', 'Business analysis', 'Coding', 'Pharma', 'Analytical', 'Healthcare', 'Business intelligence', 'Analytics', 'Monitoring', 'SQL']",2025-06-12 15:05:25
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nAs part of the cybersecurity organization, the Data Engineer is responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nCreate data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, Gitlab, LucidChart,etc.\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data engineering', 'data security', 'Agile', 'cloud data platforms', 'Databricks', 'data governance frameworks', 'ETL', 'AWS', 'SQL', 'Python']",2025-06-12 15:05:28
Data Engineering Specialist,Sanofi,5 - 10 years,Not Disclosed,['Hyderabad'],"We are seeking an experienced Data Engineering Specialist interested in challenging the status quo to ensure the seamless creation and operation of the data pipelines that are needed by Sanofi s advanced analytic, AI and ML initiatives for the betterment of our global patients and customers.\nSanofi has recently embarked into a vast and ambitious digital transformation program. A cornerstone of this roadmap is the acceleration of its data transformation and of the adoption of artificial intelligence (AI) and machine learning (ML) solutions, to accelerate R&D, manufacturing and commercial performance and bring better drugs and vaccines to patients faster, to improve health and save lives\nMain Responsibilities:\nEstablish technical designs to meet Sanofi requirements aligned with the architectural and Data standards\nOwnership of the entire back end of the application, including the design, implementation, test, and troubleshooting of the core application logic, databases, data ingestion and transformation, data processing and orchestration of pipelines, APIs, CI/CD integration and other processes\nFine-tune and optimize queries using Snowflake platform and database techniques\nOptimize ETL/data pipelines to balance performance, functionality, and other operational requirements.\nAssess and resolve data pipeline issues to ensure performance and timeliness of execution\nAssist with technical solution discovery to ensure technical feasibility.\nAssist in setting up and managing CI/CD pipelines and development of automated tests\nDeveloping and managing microservices using python\nConduct peer reviews for quality, consistency, and rigor for production level solution\nDesign application architecture for efficient concurrent user handling, ensuring optimal performance during high usage periods\nOwn all areas of the product lifecycle: design, development, test, deployment, operation, and support\nQualifications:\n5+ years of relevant experience developing backend, integration, data pipelining, and infrastructure\nExpertise in database optimization and performance improvement\nExpertise in Python, PySpark, and Snowpark\nExperience data warehousing and object-relational database (Snowflake and PostgreSQL) and writing efficient SQL queries\nExperience in cloud-based data platforms (Snowflake, AWS)\nProficiency in developing robust, reliable APIs using Python and FastAPI Framework\nExpert in ELT and ETL & experience working with large data sets and performance and query optimization. IICS is a plus\nUnderstanding of data structures and algorithms\nUnderstanding of DBT is a plus\nExperience in modern testing framework (SonarQube, K6 is a plus)\nStrong collaboration skills, willingness to work with others to ensure seamless integration of the server-side and client-side\nKnowledge of DevOps best practices and associated tools is a plus, especially in the setup, configuration, maintenance, and troubleshooting of associated tools:\nContainers and containerization technologies (Kubernetes, Argo, Red Hat OpenShift)\nInfrastructure as code (Terraform)\nMonitoring and Logging (CloudWatch, Grafana)\nCI/CD Pipelines (JFrog Artifactory)\nScripting and automation (Python, GitHub, Github actions)\nExperience with JIRA & Confluence\nWorkflow orchestration (Airflow)\nMessage brokers (RabbitMQ)\nEducation: bachelors degree in computer science, engineering, or similar quantitative field of study\nWhy choose us\nBring the miracles of science to life alongside a supportive, future-focused team.\nDiscover endless opportunities to grow your talent and drive your career, whether it s through a promotion or lateral move, at home or internationally.\nEnjoy a thoughtful, we'll-crafted rewards package that recognizes your contribution and amplifies your impact.\nTake good care of yourself and your family, with a wide range of health and we'llbeing benefits including high-quality healthcare, prevention and we'llness programs and at least 14 weeks gender-neutral parental leave.\nOpportunity to work in an international environment, collaborating with diverse business teams and vendors, working in a dynamic team, and fully empowe'red to propose and implement innovative ideas.",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'github', 'Postgresql', 'Data structures', 'Healthcare', 'Troubleshooting', 'JIRA', 'Monitoring', 'Python']",2025-06-12 15:05:30
Data Engineer,Atyeti,2 - 4 years,Not Disclosed,['Pune'],"Role & responsibilities\n\nDevelop and Maintain Data Pipelines: Design, develop, and manage scalable ETL pipelines to process large datasets using PySpark, Databricks, and other big data technologies.\nData Integration and Transformation: Work with various structured and unstructured data sources to build efficient data workflows and integrate them into a central data warehouse.\nCollaborate with Data Scientists & Analysts: Work closely with the data science and business intelligence teams to ensure the right data is available for advanced analytics, machine learning, and reporting.",,,,"['Azure Synapse', 'Pyspark', 'ETL', 'Python']",2025-06-12 15:05:33
Enterprise Data Operations Manager,Pepsico,12 - 17 years,Not Disclosed,['Hyderabad'],"Overview\n\nDeputy Director - Data Engineering\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCos global business scale to enable business insights, advanced analytics, and new product development. PepsiCos Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\nIncrease awareness about available data and democratize access to it across the company.\nAs a data engineering lead, you will be the key technical expert overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be empowered to create & lead a strong team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\nResponsibilities\n\nData engineering lead role for D&Ai data modernization (MDIP)\n\nIdeally Candidate must be flexible to work an alternative schedule either on tradition work week from Monday to Friday; or Tuesday to Saturday or Sunday to Thursday depending upon coverage requirements of the job. The candidate can work with immediate supervisor to change the work schedule on rotational basis depending on the product and project requirements.\nResponsibilities\nManage a team of data engineers and data analysts by delegating project responsibilities and managing their flow of work as well as empowering them to realize their full potential.\nDesign, structure and store data into unified data models and link them together to make the data reusable for downstream products.\nManage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\nCreate reusable accelerators and solutions to migrate data from legacy data warehouse platforms such as Teradata to Azure Databricks and Azure SQL.\nEnable and accelerate standards-based development prioritizing reuse of code, adopt test-driven development, unit testing and test automation with end-to-end observability of data\nBuild and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality, performance and cost.\nCollaborate with internal clients (product teams, sector leads, data science teams) and external partners (SI partners/data providers) to drive solutioning and clarify solution requirements.\nEvolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects to build and support the right domain architecture for each application following well-architected design standards.\nDefine and manage SLAs for data products and processes running in production.\nCreate documentation for learnings and knowledge transfer to internal associates.\nQualifications\n\n12+ years of engineering and data management experience\n\nQualifications\n12+ years of overall technology experience that includes at least 5+ years of hands-on software development, data engineering, and systems architecture.\n8+ years of experience with Data Lakehouse, Data Warehousing, and Data Analytics tools.\n6+ years of experience in SQL optimization and performance tuning on MS SQL Server, Azure SQL or any other popular RDBMS\n6+ years of experience in Python/Pyspark/Scala programming on big data platforms like Databricks\n4+ years in cloud data engineering experience in Azure or AWS.\nFluent with Azure cloud services. Azure Data Engineering certification is a plus.\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Great Expectations.\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one business intelligence tool such as Power BI or Tableau\nExperience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.\nExperience with version control systems like ADO, Github and CI/CD tools for DevOps automation and deployments.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nUnderstanding of metadata management, data lineage, and data glossaries is a plus.\nBA/BS in Computer Science, Math, Physics, or other technical fields.\nCandidate must be flexible to work an alternative work schedule either on tradition work week from Monday to Friday; or Tuesday to Saturday or Sunday to Thursday depending upon product and project coverage requirements of the job.\nCandidates are expected to be in the office at the assigned location at least 3 days a week and the days at work needs to be coordinated with immediate supervisor\nSkills, Abilities, Knowledge:\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\nProven track record of leading, mentoring data teams.\nStrong change manager. Comfortable with change, especially that which arises through company growth.\nAbility to understand and translate business requirements into data and technical requirements.\nHigh degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\nPositive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\nStrong leadership, organizational and interpersonal skills; comfortable managing trade-offs.\nFoster a team culture of accountability, communication, and self-management.\nProactively drives impact and engagement while bringing others along.\nConsistently attain/exceed individual and team goals.\nAbility to lead others without direct authority in a matrixed environment.\nComfortable working in a hybrid environment with teams consisting of contractors as well as FTEs spread across multiple PepsiCo locations.\nDomain Knowledge in CPG industry with Supply chain/GTM background is preferred.",Industry Type: Beverage,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Pyspark', 'Azure', 'Power BI', 'Github', 'Azure Databricks', 'Tableau', 'ADO', 'Scala programming', 'SQL', 'Azure Data Factory', 'Azure Machine learning', 'Data Lakehouse', 'Azure Data Engineering', 'CI/CD', 'Data Warehousing', 'Data Analytics', 'AWS', 'Python']",2025-06-12 15:05:36
"4 To 8 years of exp. as a Data Analyst @ Banglore, Hyderabad , Chennai",A Client of Career Focus Consultancy,4 - 8 years,5-10 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Strong proficiency in Advanced SQL with experience in writing optimized queries for large datasets.\nMandatory skill : Data Analyst, Python ,SQL, Power BI\n\n\nExposure in, including predictive modeling and machine learning techniques.\n\nRequired Candidate profile\nHands-on experience with Python, R, or similar analytical tools is a plus.\nFamiliarity with cloud platforms such as AWS, Azure, or GCP for data processing and analytics.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['R', 'Power BI', 'Data Analyst', 'Python', 'SQL', 'Azure', 'GCP', 'AWS']",2025-06-12 15:05:38
"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon",One of the largest insurance providers.,5 - 10 years,Not Disclosed,['Gurugram'],"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon\n\nSummary: An excellent opportunity for someone having a minimum of five years of experience with expertise in building data pipelines. A person must have experience in Python, Pyspark and AWS.\n\nLocation- Gurgaon (Hybrid)\n\nYour Future Employer- One of the largest insurance providers.\n\nResponsibilities-\nTo design, develop, and maintain large-scale data pipelines that can handle large datasets from multiple sources.\nReal-time data replication and batch processing of data using distributed computing platforms like Spark, Kafka, etc.\nTo optimize the performance of data processing jobs and ensure system scalability and reliability.\nTo collaborate with DevOps teams to manage infrastructure, including cloud environments like AWS.\nTo collaborate with data scientists, analysts, and business stakeholders to develop tools and platforms that enable advanced analytics and reporting.\n\nRequirements-\nHands-on experience with AWS services such as S3, DMS, Lambda, EMR, Glue, Redshift, RDS (Postgres) Athena, Kinesics, etc.\nExpertise in data modeling and knowledge of modern file and table formats.\nProficiency in programming languages such as Python, PySpark, and SQL/PLSQL for implementing data pipelines and ETL processes.\nExperience data architecting or deploying Cloud/Virtualization solutions (Like Data Lake, EDW, Mart ) in the enterprise.\nCloud/hybrid cloud (preferably AWS) solution for data strategy for Data lake, BI and Analytics.\nWhat is in for you-\nA stimulating working environment with equal employment opportunities.\nGrowing of skills while working with industry leaders and top brands.\nA meritocratic culture with great career progression.\n\nReach us- If you feel that you are the right fit for the role please share your updated CV at randhawa.harmeen@crescendogroup.in\n\nDisclaimer- Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Pipeline', 'AWS', 'Data Ingestion', 'Data Engineering', 'Data Processing']",2025-06-12 15:05:40
Data Architect with Azure Databricks + Power BI,Tech Mahindra,15 - 19 years,Not Disclosed,['Pune'],"Skill Name - Data Architect with Azure & Databricks + Power BIExperience: 15 - 19 years Responsibilities: Architect and design end-to-end data solutions on Cloud Platform, focusing on data warehousing and big data platforms. Collaborate with clients, developers, and architecture teams to understand requirements and translate them into effective data solutions.Develop high-level and detailed data architecture and design documentation. Implement data management and data governance strategies, ensuring compliance with industry standards. Architect both batch and real-time data solutions, leveraging cloud native services and technologies. Design and manage data pipeline processes for historic data migration and data integration. Collaborate with business analysts to understand domain data requirements and incorporate them into the design deliverables. Drive innovation in data analytics by leveraging cutting-edge technologies and methodologies. Demonstrate excellent verbal and written communication skills to communicate complex ideas and concepts effectively. Stay updated on the latest advancements in Data Analytics, data architecture, and data management techniques. Requirements Minimum of 5 years of experience in a Data Architect role, supporting warehouse and Cloud data platforms/environments. Extensive Experience with common Azure services such as ADLS, Synapse, Databricks, Azure SQL etc. Experience on Azure services such as ADF, Polybase, Azure Stream Analytics Proven expertise in Databricks architecture, Delta Lake, Delta sharing, Unity Catalog, data pipelines, and Spark tuning. Strong knowledge of Power BI architecture, DAX, and dashboard optimization. In-depth experience with SQL, Python, and/or PySpark. Hands-on knowledge of data governance, lineage, and cataloging tools such as Azure Purview and Unity Catalog. Experience in implementing CI/CD pipelines for data and BI components (e.g., using DevOps or GitHub). Experience on building symantec modeling in Power BI. Strong knowledge of Power BI architecture, DAX, and dashboard optimization. Strong expertise in data exploration using SQL and a deep understanding of data relationships. Extensive knowledge and implementation experience in data management, governance, and security frameworks. Proven experience in creating high-level and detailed data architecture and design documentation. Strong aptitude for business analysis to understand domain data requirements. Proficiency in Data Modelling using any Modelling tool for Conceptual, Logical, and Physical models is preferred Hands-on experience with architecting end-to-end data solutions for both batch and real-time designs. Ability to collaborate effectively with clients, developers, and architecture teams to implement enterprise-level data solutions. Familiarity with Data Fabric and Data Mesh architecture is a plus. Excellent verbal and written communication skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Symantec', 'Data migration', 'github', 'Data management', 'Business analysis', 'Data Architect', 'data governance', 'SQL', 'Python', 'Data architecture']",2025-06-12 15:05:43
Data Engineer 4,Comcast,5 - 11 years,Not Disclosed,['Chennai'],".\nResponsible for designing, building and overseeing the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs. Work with data modelers/analysts to understand the business problems they are trying to solve then create or augment data assets to feed their analysis. Integrates knowledge of business and functional priorities. Acts as a key contributor in a complex and crucial environment. May lead teams or projects and shares expertise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBachelors Degree\nWhile possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.\n7-10 Years\nComcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.",,,,"['Engineering services', 'Assurance', 'Process optimization', 'MySQL', 'Machine learning', 'Data structures', 'Data quality', 'Troubleshooting', 'Downstream', 'Python']",2025-06-12 15:05:45
Data Engineer,Databeat,3 - 7 years,Not Disclosed,['Hyderabad( Rai Durg )'],"Experience Required: 3+ years\n\nTechnical knowledge: AWS, Python, SQL, S3, EC2, Glue, Athena, Lambda, DynamoDB, RedShift, Step Functions, Cloud Formation, CI/CD Pipelines, Github, EMR, RDS,AWS Lake Formation, GitLab, Jenkins and AWS CodePipeline.\n\n\n\nRole Summary: As a Senior Data Engineer,with over 3 years of expertise in Python, PySpark, SQL to design, develop and optimize complex data pipelines, support data modeling, and contribute to the architecture that supports big data processing and analytics to cutting-edge cloud solutions that drive business growth. You will lead the design and implementation of scalable, high-performance data solutions on AWS and mentor junior team members.This role demands a deep understanding of AWS services, big data tools, and complex architectures to support large-scale data processing and advanced analytics.\nKey Responsibilities:\nDesign and develop robust, scalable data pipelines using AWS services, Python, PySpark, and SQL that integrate seamlessly with the broader data and product ecosystem.\nLead the migration of legacy data warehouses and data marts to AWS cloud-based data lake and data warehouse solutions.\nOptimize data processing and storage for performance and cost.\nImplement data security and compliance best practices, in collaboration with the IT security team.\nBuild flexible and scalable systems to handle the growing demands of real-time analytics and big data processing.\nWork closely with data scientists and analysts to support their data needs and assist in building complex queries and data analysis pipelines.\nCollaborate with cross-functional teams to understand their data needs and translate them into technical requirements.\nContinuously evaluate new technologies and AWS services to enhance data capabilities and performance.\nCreate and maintain comprehensive documentation of data pipelines, architectures, and workflows.\nParticipate in code reviews and ensure that all solutions are aligned to pre-defined architectural specifications.\nPresent findings to executive leadership and recommend data-driven strategies for business growth.\nCommunicate effectively with different levels of management to gather use cases/requirements and provide designs that cater to those stakeholders.\nHandle clients in multiple industries at the same time, balancing their unique needs.\nProvide mentoring and guidance to junior data engineers and team members.\n\n\n\nRequirements:\n3+ years of experience in a data engineering role with a strong focus on AWS, Python, PySpark, Hive, and SQL.\nProven experience in designing and delivering large-scale data warehousing and data processing solutions.\nLead the design and implementation of complex, scalable data pipelines using AWS services such as S3, EC2, EMR, RDS, Redshift, Glue, Lambda, Athena, and AWS Lake Formation.\nBachelor's or Masters degree in Computer Science, Engineering, or a related technical field.\nDeep knowledge of big data technologies and ETL tools, such as Apache Spark, PySpark, Hadoop, Kafka, and Spark Streaming.\nImplement data architecture patterns, including event-driven pipelines, Lambda architectures, and data lakes.\nIncorporate modern tools like Databricks, Airflow, and Terraform for orchestration and infrastructure as code.\nImplement CI/CD using GitLab, Jenkins, and AWS CodePipeline.\nEnsure data security, governance, and compliance by leveraging tools such as IAM, KMS, and AWS CloudTrail.\nMentor junior engineers, fostering a culture of continuous learning and improvement.\nExcellent problem-solving and analytical skills, with a strategic mindset.\nStrong communication and leadership skills, with the ability to influence stakeholders at all levels.\nAbility to work independently as well as part of a team in a fast-paced environment.\nAdvanced data visualization skills and the ability to present complex data in a clear and concise manner.\nExcellent communication skills, both written and verbal, to collaborate effectively across teams and levels.\n\nPreferred Skills:\nExperience with Databricks, Snowflake, and machine learning pipelines.\nExposure to real-time data streaming technologies and architectures.\nFamiliarity with containerization and serverless computing (Docker, Kubernetes, AWS Lambda).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'SQL', 'Data Pipeline', 'Python', 'Amazon Ec2', 'Data Engineering', 'Data Bricks', 'Aws Lambda', 'Amazon Redshift', 'Azure Cloud', 'Data Lake', 'Data Modeling', 'Athena']",2025-06-12 15:05:47
Data Engineer IV - Big Data / Spark,Sadup Soft,5 - 7 years,Not Disclosed,['Chennai'],"Must have skills :\n\n- Minimum of 5-7 years of experience in software development, with a focus on Java and infrastructure tools.\n\n- Min 6+ years of experience as a Data Engineer.\n\n- Good Experience in handling Big Data Spark, Hive SQL, BigQuery, SQL.\n\n- Candidate worked on cloud platforms and GCP would be an added advantage.\n\n- Good understanding of Hadoop based ecosystem including hard sequel, HDFS would be very essential.\n\n- Very good professional knowledge of PySpark or using Scala\n\nResponsibilities :\n\n- Collaborate with cross-functional teams such as Data Scientists, Product Partners and Partner Team Developers to identify opportunities for Big Data, Query ( Spark, Hive SQL, BigQuery, SQL ) tuning opportunities that can be solved using machine learning and generative AI.\n\n- Write clean, high-performance, high-quality, maintainable code.\n\n- Design and develop Big Data Engineering Solutions Applications for above ensuring scalability, efficiency, and maintainability of such solutions.\n\nRequirements :\n\n- A Bachelor or Master's degree in Computer Science or a related field.\n\n- Proven experience working as a Big Data & MLOps Engineer, with a focus on Spark, Scala Spark or PySpark, Spark SQL, BigQuery, Python, Google Cloud,.\n\n- Deep understanding and experience in tuning Dataproc, BigQuery, Spark Applications.\n\n- Solid knowledge of software engineering best practices, including version control systems (e.g Git), code reviews, and testing methodologies.\n\n- Strong communication skills to effectively collaborate and present findings to both technical and non-technical stakeholders.\n\n- Proven ability to adapt and learn new technologies and frameworks quickly.\n\n- A proactive mindset with a passion for continuous learning and research.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Data Engineering', 'BigQuery', 'GCP', 'Spark', 'Machine Learning', 'Python', 'SQL']",2025-06-12 15:05:50
Artificial Intelligence Intern,Kumaran Systems,0 - 1 years,4.5-5 Lacs P.A.,['Chennai( Siruseri Sipcot IT Park )'],"We are looking for a passionate and motivated AI Developer Fresher to join our growing AI team. This role will focus on Generative AI (GenAI) technologies such as large language models (LLMs), diffusion models, and other cutting-edge machine learning techniques.\n\nAs a fresher, youll work closely with senior AI engineers and data scientists to build and fine-tune generative models, contribute to prompt engineering, and support model integration into real-world applications.",,,,"['Data Science', 'Mechine Learning', 'Artificial Intelligence', 'GEN AI', 'Python']",2025-06-12 15:05:52
Ai Ml Engineer,Optum,5 - 10 years,Not Disclosed,['Noida'],"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health optimization on a global scale. Join us to start Caring. Connecting. Growing together.  \nAI Engineer is tasked with the design, development, and deployment of advanced generative AI models and systems. This position requires close collaboration with data scientists, product managers, and other stakeholders to integrate generative AI solutions into existing products and develop new innovative features. Proficiency in the Agentic AI framework is vital for coordinating multiple autonomous AI agents to accomplish complex tasks.\n\nPrimary Responsibilities:\nImplement Generative AI Models: Develop sophisticated generative AI algorithms and models to create new data samples, patterns, or content based on existing data or inputs\nData Processing: Collaborate with stakeholders to preprocess, analyze, and interpret extensive datasets\nModel Deployment: Deploy generative AI models into production environments, ensuring scalability and robustness\nOptimization: Conduct model testing, validation, and optimization to enhance performance\nIntegration: Work with cross-functional teams to seamlessly integrate generative AI solutions into products\nResearch: Stay current with the latest advancements in generative AI technologies and practices\nAgentic AI Framework: Utilize the Agentic AI framework to coordinate multiple AI agents for the completion of complex tasks\nMentorship: Provide mentorship to junior team members and offer technical guidance\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or a related field\n5+ years of experience in software engineering with a focus on AI/ML\nExperience with data preprocessing and analysis\nKnowledge of the Agentic AI framework and its application in AI systems\nProficiency in machine learning frameworks such as TensorFlow and PyTorch\nSolid programming skills in Python, Java, or C++\nFamiliarity with cloud platforms (e.g., AWS, Google Cloud, Azure)\nProven excellent problem-solving abilities and algorithmic thinking\nProven solid communication and teamwork skills\n\nPreferred Qualifications:\nExperience with data processing\nKnowledge of version control systems like Git\nUnderstanding of Generative AI, associated technologies and frameworks like RAG, agents etc.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Agentic Ai', 'Gen AI', 'Cloud', 'RAG', 'LLM']",2025-06-12 15:05:54
Data Engineer,Infoobjects Inc.,3 - 6 years,Not Disclosed,['Jaipur'],"Role & responsibilities:\nDesign, develop, and maintain robust ETL/ELT pipelines to ingest and process data from multiple sources.\nBuild and maintain scalable and reliable data warehouses, data lakes, and data marts.\nCollaborate with data scientists, analysts, and business stakeholders to understand data needs and deliver solutions.\nEnsure data quality, integrity, and security across all data systems.\nOptimize data pipeline performance and troubleshoot issues in a timely manner.\nImplement data governance and best practices in data management.\nAutomate data validation, monitoring, and reporting processes.\n\n\n\nPreferred candidate profile:\nBachelor's or Masters degree in Computer Science, Engineering, Information Systems, or related field.\nProven experience (X+ years) as a Data Engineer or similar role.\nStrong programming skills in Python, Java, or Scala.\nProficiency with SQL and working knowledge of relational databases (e.g., PostgreSQL, MySQL).\nHands-on experience with big data technologies (e.g., Spark, Hadoop).\nFamiliarity with cloud platforms such as AWS, GCP, or Azure (e.g., S3, Redshift, BigQuery, Data Factory).\nExperience with orchestration tools like Airflow or Prefect.\nKnowledge of data modeling, warehousing, and architecture design principles.\nStrong problem-solving skills and attention to detail.\n\nPerks and benefits\nFree Meals\nPF and Gratuity\nMedical and Term Insurance",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Kafka', 'AWS', 'Python', 'Pyspark', 'Java', 'Postgresql', 'Hadoop', 'Spark', 'ETL', 'SQL']",2025-06-12 15:05:57
Senior ML Compiler Engineer,Qualcomm,2 - 4 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nInterested in accelerating machine learning and artificial intelligence on mobile devices for millions of usersCome join our team. We are building software platforms that enable users of Qualcomms silicon to construct optimized neural networks and machine learning algorithms. We are looking for software engineers with a machine learning or compiler background who will help us build these software platforms. In this role, you will construct and tune machine learning frameworks, build compilers and tools, and collaborate with Qualcomm hardware and software engineers to enable efficient usage of Qualcomms silicon for machine learning applications.\n\nMinimum qualifications:\nBachelors degree in Engineering, Information Systems, Computer Science, or related field.\nProgramming in C/C++\n2 to 4 years of software engineering or related work experience\n\n\nPreferred qualifications:\nExperience in machine learning frameworks such as MxNet/NNVM/TVM, Pytorch, Tensorflow, Caffe\n\nOR experience in compilers with an interest in machine learning\nDeep knowledge of software engineering\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'software engineering', 'algorithms', 'c++', 'natural language processing', 'caffe', 'neural networks', 'mxnet', 'artificial intelligence', 'sql', 'deep learning', 'r', 'java', 'data science', 'computer vision', 'machine learning algorithms', 'ml']",2025-06-12 15:06:00
Speech Engineer,BUSINESSNEXT,2 - 5 years,Not Disclosed,['Noida'],"What would you do?\nSystem Design: Architect and design end-to-end speech processing pipelines, from data acquisition to model deployment. Ensure systems are scalable, efficient, and maintainable.\nAdvanced Modeling: Develop and implement advanced machine learning models for speech recognition, speaker diarization, and related tasks. Utilize state-of-the-art techniques such as deep learning, transfer learning, and ensemble methods.\nResearch and Development: Conduct research to explore new methodologies and tools in the field of speech processing. Publish findings and present at industry conferences.",,,,"['Tensorflow', 'Pytorch', 'Stt', 'Speech Recognition', 'transfer learning', 'deep speech', 'Natural Language Processing', 'model deployment', 'Data Acquisition', 'Machine Learning', 'Deep Learning', 'Sts', 'speech processing', 'whisper', 'text to speech']",2025-06-12 15:06:02
AI Engineer,HCLTech,10 - 14 years,Not Disclosed,['Noida'],"Seniority: Senior\nDescription & Requirements\nPosition Summary\nThe Senior AI Engineer with GenAI expertise is responsible for developing advanced technical solutions, integrating cutting-edge generative AI technologies. This role requires a deep understanding of modern technical and cloud-native practices, AI, DevOps, and machine learning technologies, particularly in generative models. You will support a wide range of customers through the Ideation to MVP journey, showcasing leadership and decision-making abilities while tackling complex challenges.",,,,"['AI engineering', 'VMware', 'Java', 'Azure', 'Data engineering', 'AI models', 'Node.js', 'NLP', 'Azure AKS', 'Machine Learning Operations', 'AWS', 'Kubernetes', 'Python']",2025-06-12 15:06:05
Data Science Lead,Protiviti India,9 - 14 years,25-40 Lacs P.A.,['Mumbai (All Areas)'],"Role & responsibilities\n8+ year bachelors or master’s degree from reputed University with concentration on finance, economics or other quantitative field such as statistics or engineering.\nManage multiple client engagements in Financial Services locally in India\nActively drive pre-sales, sales activities primarily for FS clients locally in Data Science Domain\nUnderstand client requirements in detail and create technical & commercial proposal\nDrive client conversations specifically for business development activities",,,,"['Data Science', 'Natural Language Processing', 'Presales', 'Machine Learning', 'AWS', 'GCP', 'Cloud Platform', 'Python']",2025-06-12 15:06:07
Business Analyst/ Data Scientist - SAS & SQL,Khushboo,3 - 8 years,10-20 Lacs P.A.,['Hyderabad'],"hands on SQL/ SAS programming experience & handling complex/large data\nMust have experience inTableau/Power BI\nExperience in campaign performance measurement, customer targeting framework\nProven ability to design and lead strategic projects\n\nRequired Candidate profile\nMust - SAS , SQL, Python\nGood in Statistical model , Predictive model, Logistic regression, Linear regression\nBFSI Mandatory - Credit risk, Credit Card, Retail Banking",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Decision Tree', 'sas', 'sql', 'Advanced Analytics', 'Strategy Building', 'Predictive Modeling', 'python', 'Logistic Regression', 'Segmentation', 'Random Forest', 'Linear Regression', 'Classification', 'Statistical Modeling', 'Credit Risk']",2025-06-12 15:06:09
Data Science_ Lead,Rishabh Software,8 - 13 years,Not Disclosed,"['Ahmedabad', 'Bengaluru', 'Vadodara']","Job Description\n\nWith excellent analytical and problem-solving skills, you should understand business problems of the customers, translate them into scope of work and technical specifications for developing into Data Science projects. Efficiently utilize cutting edge technologies in AI, Generative AI areas and implement solutions for business problems. Good exposure technology platforms for Data Science, AI, Gen AI, cloud with implementation experience. Ability to provide end to end technical solutions leveraging latest AI, Gen AI tools, frameworks for the business problems. This Job requires the following:",,,,"['Data Science', 'gen ai', 'Computer Vision', 'Machine Learning', 'Deep Learning', 'Tensorflow', 'NLP', 'Artificial Intelligence', 'Dl', 'Python']",2025-06-12 15:06:12
"Data Engineer Openings at Advantum Health, Hyderabad",Advantum Health,3 - 5 years,Not Disclosed,['Hyderabad'],"Data Engineer openings at Advantum Health Pvt Ltd, Hyderabad.\nOverview:\nWe are looking for a Data Engineer to build and optimize robust data pipelines that support AI and RCM analytics. This role involves integrating structured and unstructured data from diverse healthcare systems into scalable, AI-ready datasets.\nKey Responsibilities:\nDesign, implement, and optimize data pipelines for ingesting and transforming healthcare and RCM data.\nBuild data marts and warehouses to support analytics and machine learning.\nEnsure data quality, lineage, and governance across AI use cases.\nIntegrate data from EMRs, billing platforms, claims databases, and third-party APIs.\nSupport data infrastructure in a HIPAA-compliant cloud environment.\nQualifications:\nBachelors in Computer Science, Data Engineering, or related field.\n3+ years of experience with ETL/ELT pipelines using tools like Apache Airflow, dbt, or Azure Data Factory.\nStrong SQL and Python skills.\nExperience with healthcare data standards (HL7, FHIR, X12) preferred.\nFamiliarity with data lake house architectures and AI integration best practices\nPh: 9177078628\nEmail id: jobs@advantumhealth.com\nAddress: Advantum Health Private Limited, Cyber gateway, Block C, 4th floor Hitech City, Hyderabad.\nDo follow us on LinkedIn, Facebook, Instagram, YouTube and Threads\nAdvantum Health LinkedIn Page:\nhttps://lnkd.in/gVcQAXK3\n\nAdvantum Health Facebook Page:\nhttps://lnkd.in/g7ARQ378\n\nAdvantum Health Instagram Page:\nhttps://lnkd.in/gtQnB_Gc\n\nAdvantum Health India YouTube link:\nhttps://lnkd.in/g_AxPaPp\n\nAdvantum Health Threads link:\nhttps://lnkd.in/gyq73iQ6",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'SQL', 'Python', 'Airflow', 'ETL', 'Elt']",2025-06-12 15:06:14
Walk In Drive II (June 13 Friday) II B2B Tele Sales II Noida,Info Edge,0 - 4 years,1-5 Lacs P.A.,['Noida'],"About Info Edge\nInfoEdges mission is to create world-class platforms that transform lives by continuously innovating. Our products and services are built keeping our customers in mind. We always delight our customers by delivering superior value through enhanced offerings on the internet and other platforms. Through our continuous investment across various businesses, especially in cutting-edge technology, machine learning and artificial intelligence (AI), we have built a robust system that constantly increases our predictive powers on customer behaviour, and optimizes and improves our systems. Our various teams tirelessly work together to solve problems, innovate, and create something to empower our customers.",,,,"['B2B Sales', 'Virtual Sales', 'Sales Process', 'Direct Sales', 'Crm Tool', 'Client Engagement', 'Business Development', 'Salesforce CRM', 'sales', 'Salesforce']",2025-06-12 15:06:16
HIH - Data Science Lead Analyst - Evernorth,ManipalCigna Health Insurance,5 - 8 years,Not Disclosed,['Hyderabad'],"Internal Title: Data Science Lead Analyst\nExternal Title: Data Science Lead Analyst\nRole Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\n\nThe role will support the development and maintenance of machine learning models, with a focus on ensuring that models meet Cigna s requirements for governance and legal compliance. The role will require collaboration with other data scientists and involve work across many lines of business.\nKey Responsibilities:\nAnalyze model performance of new models with specific regards to requirements for legal compliance and governance standards around accuracy and bias;\nPerform periodic analyses of performance of existing models to ensure continued compliance with internal and external standards for accuracy and bias;\nConduct research (i.e. literature review) to understand when bias may be biologically or medically justifiable, and to what degree, for example: finding evidence from literature that heart disease is more prevalent among older populations\nUsing machine learning development tools to mitigate model bias when this is determined to be necessary\nCollaborating with data scientists, business stakeholders, and governance/compliance teams to ensure models meet compliance and governance standards\nQualifications:\nBachelors or Masters/PhD (preferred) in statistics or computer science or equivalent field with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR)\nAbility to promote best coding practices, championing a culture of documentation/logging\nThorough understanding of ML lifecycle, including necessary tradeoffs and associated risks\nLeadership in Data Science\nCan own a project end-to-end e.g., scoping, business value estimation, ideation, dev, prod, timeline\nCollaborates and guides junior team members in completion of projects and career development\nWorks cross functionally with technical (e.g., Data Science, Data Engineering) and business (e.g., clinical, marketing, pricing, business analysts) to implement solutions with measurable value\nScope and Impact\nIndependently delivers clear and well-developed presentations for both technical and business audiences\nCreates data science specific project goals associated with project deliverables\nArticulates timeline changes, rationale, and goals to meet deadlines moving forward\nValues diversity, growth mindset, and improving health outcomes of our customers\n\nLevel of Influence\nCommunicate with stakeholders to identify opportunities and possible solutions based on business need\nDraft project charter, timeline, and features/stories\nInfluence matrix-partner leadership\nAbout Evernorth Health Services",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Version control', 'Claims', 'data science', 'Legal compliance', 'Coding', 'Pharmacy', 'Machine learning', 'SQL', 'Python']",2025-06-12 15:06:19
HIH - Data Science Lead Analyst - Evernorth,ManipalCigna Health Insurance,5 - 8 years,Not Disclosed,['Hyderabad'],"Internal Title: Data Science Lead Analyst\nExternal Title: Data Science Lead Analyst\nRole Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\nThe role will support the development and maintenance of proprietary advanced neural network ( AI ) foundation models in support of Cigna s business operations.\nKey Responsibilities:\nWrite code using PyTorch and/or Tensorflow to implement, test, and operationalize deep learning models\nCollaborate with data scientists and engineers to improve deep learning models and implement business-facing solutions built on top of those models\nTake responsibility for improving code performance and quality\nFollow developments in deep learning technology to identify opportunities to improve models\nQualifications:\nBachelors or Masters(preferred) in computer science or statistics or any other equivalent discipline with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR), with emphasis on Tensorflow and Pytorch\nKnows and follows best coding and software engineering practices\nSuccessfully completes technical project components with limited guidance\nFamiliarity with deploying machine learning and predictive models to production and cloud environments\nLeadership in Data Science\nUnderstands how assigned work is related to purpose of the overall project\nIndependently identifies project roadblocks, and solutions\nSeeks to understand the health insurance domain\nScope and Impact\nDocuments the business considerations, methodology, process, code, and results associated with their work\nCollaborates to deliver clear and well developed presentations for both technical and business audiences\nConsistently communicates decisions, considerations, and needs for support\nReceives and responds to feedback in a professional and appropriate manner\nLevel of Influence\nPresent technical topics and results to non-technical stakeholders\nCommunicate and gather domain knowledge from non-technical stakeholders\nAbout Evernorth Health Services",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Version control', 'GIT', 'Claims', 'Coding', 'Machine learning', 'Lead Analyst', 'SQL', 'Python', 'Business operations']",2025-06-12 15:06:21
Walk In || Corporate Sales Role || Naukri.com || Ahmedabad,Info Edge,0 - 2 years,Not Disclosed,['Ahmedabad'],"About Info Edge India Ltd\nInfo Edge is Indias leading consumer internet company known for its strong brands in recruitment (naukri.com, naukrigulf.com, iimjobs.com, firstnaukri.com), real estate (99acres.com), matrimony (Jeevansathi.com) and education (shiksha.com).\nStarting with a classified recruitment online business, naukri.com, the Company has grown and diversified rapidly, setting benchmarks as a pioneer for others to follow either through setting up of in-house brands or through the route of strategic investments and acquisitions. Zomato.com, policybazaar.com & Happily Unmarried Marketing Private Limited are our investee companies to name a few out of many. With years of experience in the domain, strong cash flow generation, and a diversified business portfolio, Info Edge is one of the very few profitable pure play internet companies in the country.",,,,"['Sales', 'B2B Sales', 'Lead Generation', 'Client Acquisition', 'Corporate Sales']",2025-06-12 15:06:24
Corporate Sales Executive,Info Edge,0 - 2 years,Not Disclosed,['Ahmedabad'],"About Info Edge India Ltd\nInfo Edge is Indias leading consumer internet company known for its strong brands in recruitment (naukri.com, naukrigulf.com, iimjobs.com, firstnaukri.com), real estate (99acres.com), matrimony (Jeevansathi.com) and education (shiksha.com).Starting with a classified recruitment online business, naukri.com, the Company has grown and diversified rapidly, setting benchmarks as a pioneer for others to follow either through setting up of in-house brands or through the route of strategic investments and acquisitions. Zomato.com, policybazaar.com & Happily Unmarried Marketing Private Limited are our investee companies to name a few out of many. With years of experience in the domain, strong cash flow generation, and a diversified business portfolio, Info Edge is one of the very few profitable pure play internet companies in the country.These are exciting times for Info Edge as we continue to grow in our businesses and scale newer heights. We are investing across various businesses, especially in cutting-edge technology, machine learning, and artificial intelligence (AI) to increase our predictive powers on customer behavior and continuously optimize and improve our systems.At Info Edge, people are our core competitive advantage and we will continue doing all that is needed to attract and retain the best available talent. Driven by innovation, an experienced and talented leadership team, and a strong entrepreneurial orientation, we pride ourselves on having a culture that promotes meritocracy. Our numerous milestones can largely be credited to an incredibly smart team working in an environment that encourages creativity and going the extra mile to develop products that people love to use and add value to our clients.About BU: Naukri.comNaukri.com, online recruitment classifieds, is a significant player and a market leader in Indias well-established business space. The recruitment space provides all the job seekers with advisory services and caters to different elements of the job listing, employer branding, resume short-listing, career site management, and campus recruitment. With over 67 Million resumes searches daily, Naukri.com has 5 Million job listings, 59 Thousand+ more unique clients and 4.9 Million recruiters connect with the job seekers via emails.The platform, in the online recruitment space, continues to reinforce its established leadership position in India which has given it a competitive edge in the market.",,,,"['B2B Corporate Sales', 'New Client Acquisition', 'Corporate Sales']",2025-06-12 15:06:26
Manager Data Engineer – Research Data and Analytics,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will create and develop data lake solutions for scientific data that drive business decisions for Research. You will build scalable and high-performance data engineering solutions for large scientific datasets and collaborate with Research collaborators. You will also provide technical leadership to junior team members. The ideal candidate possesses experience in the pharmaceutical or biotech industry, demonstrates deep technical skills, is proficient with big data technologies, and has a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nLead, manage, and mentor a high-performing team of data engineers\nDesign, develop, and implement data pipelines, ETL processes, and data integration solutions\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks\nDevelop and maintain data models for biopharma scientific data, data dictionaries, and other documentation to ensure data accuracy and consistency\nOptimize large datasets for query performance\nCollaborate with global multi-functional teams including research scientists to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nCollaborate with Data Architects, Business SMEs, Software Engineers and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve data-related challenges\nAdhere to best practices for coding, testing, and designing reusable code/component\nExplore new tools and technologies that will help to improve ETL platform performance\nParticipate in sprint planning meetings and provide estimations on technical implementation\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nDoctorate Degree OR\nMasters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications:\n3+ years of experience in implementing and supporting biopharma scientific research data analytics (software platforms)\n\n\nFunctional Skills:\nMust-Have Skills:\nProficiency in SQL and Python for data engineering, test automation frameworks (pytest), and scripting tasks\nHands on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing\nExcellent problem-solving skills and the ability to work with large, complex datasets\nAble to engage with business collaborators and mentor team to develop data pipelines and data models\n\n\nGood-to-Have Skills:\nA passion for tackling complex challenges in drug discovery with technology and data\nGood understanding of data modeling, data warehousing, and data integration concepts\nGood experience using RDBMS (e.g. Oracle, MySQL, SQL server, PostgreSQL)\nKnowledge of cloud data platforms (AWS preferred)\nExperience with data visualization tools (e.g. Dash, Plotly, Spotfire)\nExperience with diagramming and collaboration tools such as Miro, Lucidchart or similar tools for process mapping and brainstorming\nExperience writing and maintaining technical documentation in Confluence\nUnderstanding of data governance frameworks, tools, and best practices\n\n\nProfessional Certifications:\nDatabricks Certified Data Engineer Professional preferred\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Spotfire', 'PySpark', 'PostgreSQL', 'Plotly', 'SparkSQL', 'SQL server', 'SQL', 'process mapping', 'Dash', 'MySQL', 'ETL', 'Oracle', 'data governance frameworks', 'Python']",2025-06-12 15:06:29
Senior Data Engineer : 7+ Years,Jayam Solutions Pvt Ltd - CMMI Level III Company,5 - 9 years,Not Disclosed,['Hyderabad( Madhapur )'],"Job Description:\nPosition: Sr.Data Engineer\nExperience: Minimum 7 years\nLocation: Hyderabad\nJob Summary:\n\nWhat Youll Do\n\nDesign and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.\nParticipate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.\nIndependently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.\nMonitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.\nMaintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.\nAcquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.\nEngage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.\nDrive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.\n\nWhat Youll Bring\n\nBachelors degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.\n7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.\nExtensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.\nCloud - AWS\nApache Flink/Spark , Redis\nDatabase platform- Databricks.\nProficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.\nExpertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.\nExperience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).\nAbility to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.\nStrong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.\nExceptional attention to detail and professionalism in report and dashboard creation.\nExcellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.\nOutstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.\n\nPlease feel free to contact us: 9440806850\nEmail ID : careers@jayamsolutions.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Flink', 'Redis', 'Spark', 'Python', 'SCALA', 'Ci/Cd', 'Devops', 'AWS']",2025-06-12 15:06:31
Senior Azure Data Engineer,Cloud Angles Digital Transformation,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Data Engineer with expertise in leveraging Data Lake architecture and the Azure cloud platform to develop, deploy, and optimise data-driven solutions. . You will play a pivotal role in transforming raw data into actionable insights, supporting strategic decision-making across the organisation.\nResponsibilities\nDesign and implement scalable data science solutions using Azure Data Lake, Azure Data Bricks, Azure Data Factory and related Azure services.\nDevelop, train, and deploy machine learning models to address business challenges.\nCollaborate with data engineering teams to optimise data pipelines and ensure seamless data integration within Azure cloud infrastructure.\nConduct exploratory data analysis (EDA) to identify trends, patterns, and insights.\nBuild predictive and prescriptive models to support decision-making processes.\nExpertise in developing end-to-end Machine learning lifecycle utilizing crisp-DM which includes of data collection, cleansing, visualization, preprocessing, model development, model validation and model retraining\nProficient in building and implementing RAG systems that enhance the accuracy and relevance of model outputs by integrating retrieval mechanisms with generative models.\nEnsure data security, compliance, and governance within the Azure cloud ecosystem.\nMonitor and optimise model performance and scalability in production environments.\nPrepare clear and concise documentation for developed models and workflows.\nSkills Required:\nGood experience in using Pyspark, Python, MLops (Optional), ML flow (Optional), Azure Data Lake Storage. Unity Catalog\nWorked and utilized data from various RDBMS like MYSQL, SQL Server, Postgres and NoSQL databases like MongoDB, Cassandra, Redis and graph DB like Neo4j, Grakn.\nProven experience as a Data Engineer with a strong focus on Azure cloud platform and Data Lake architecture.\nProficiency in Python, Pyspark,\nHands-on experience with Azure services such as Azure Data Lake, Azure Synapse Analytics, Azure Machine Learning, Azure Databricks, and Azure Functions.\nStrong knowledge of SQL and experience in querying large datasets from Data Lakes.\nFamiliarity with data engineering tools and frameworks for data ingestion and transformation in Azure.\nExperience with version control systems (e.g., Git) and CI/CD pipelines for machine learning projects.\nExcellent problem-solving skills and the ability to work collaboratively in a team environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Data Engineering', 'Azure Databricks', 'Pyspark', 'Azure Data Lake', 'Python']",2025-06-12 15:06:34
Senior Data Engineer - AWS,Blend360 India,6 - 10 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data engineering , with at lea",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 15:06:36
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nRole Description:\nWe are seeking a Senior Data Engineer with expertise in Graph Data technologies to join our data engineering team and contribute to the development of scalable, high-performance data pipelines and advanced data models that power next-generation applications and analytics. This role combines core data engineering skills with specialized knowledge in graph data structures, graph databases, and relationship-centric data modeling, enabling the organization to leverage connected data for deep insights, pattern detection, and advanced analytics use cases. The ideal candidate will have a strong background in data architecture, big data processing, and Graph technologies and will work closely with data scientists, analysts, architects, and business stakeholders to design and deliver graph-based data engineering solutions.\nRoles & Responsibilities:\nDesign, build, and maintain robust data pipelines using Databricks (Spark, Delta Lake, PySpark) for complex graph data processing workflows.\nOwn the implementation of graph-based data models, capturing complex relationships and hierarchies across domains.\nBuild and optimize Graph Databases such as Stardog, Neo4j, Marklogic or similar to support query performance, scalability, and reliability.\nImplement graph query logic using SPARQL, Cypher, Gremlin, or GSQL, depending on platform requirements.\nCollaborate with data architects to integrate graph data with existing data lakes, warehouses, and lakehouse architectures.\nWork closely with data scientists and analysts to enable graph analytics, link analysis, recommendation systems, and fraud detection use cases.\nDevelop metadata-driven pipelines and lineage tracking for graph and relational data processing.\nEnsure data quality, governance, and security standards are met across all graph data initiatives.\nMentor junior engineers and contribute to data engineering best practices, especially around graph-centric patterns and technologies.\nStay up to date with the latest developments in graph technology, graph ML, and network analytics.\nWhat we expect of you\nMust-Have Skills:\nHands-on experience in Databricks, including PySpark, Delta Lake, and notebook-based development.\nHands-on experience with graph database platforms such as Stardog, Neo4j, Marklogic etc.\nStrong understanding of graph theory, graph modeling, and traversal algorithms\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies with strong problem-solving and analytical skills\nExcellent collaboration and communication skills, with experience working with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SPARQL', 'Maven', 'PySpark', 'GSQL', 'Subversion', 'AWS services', 'Stardog', 'Cypher', 'SAFe', 'Jenkins', 'DevOps', 'Git', 'Neo4j', 'Delta Lake', 'Graph Databases', 'Spark', 'Marklogic', 'Gremlin']",2025-06-12 15:06:38
Software Engineer II ( Java Fullstack),JPMorgan Chase Bank,3 - 10 years,Not Disclosed,['Hyderabad'],"You re ready to gain the skills and experience needed to grow within your role and advance your career and we have the perfect software engineering opportunity for you.\nAs a Software Engineer II at JPMorgan Chase within the Consumer anc Community Banking , you are part of an agile team that works to enhance, design, and deliver the software components of the firm s state-of-the-art technology products in a secure, stable, and scalable way. As an emerging member of a software engineering team, you execute software solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role.\nJob responsibilities\nExecutes standard software solutions, design, development, and technical troubleshooting\nWrites secure and high-quality code using the syntax of at least one programming language with limited guidance\nDesigns, develops, codes, and troubleshoots with consideration of upstream and downstream systems and technical implications\nApplies knowledge of tools within the Software Development Life Cycle toolchain to improve the value realized by automation\nApplies technical troubleshooting to break down solutions and solve technical problems of basic complexity\nGathers, analyzes, and draws conclusions from large, diverse data sets to identify problems and contribute to decision-making in service of secure, stable application development\nLearns and applies system processes, methodologies, and skills for the development of secure, stable code and systems\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 2+ years applied experience\nHands-on practical experience in system design, application development, testing, and operational stability\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nDemonstrable ability to code in one or more languages\nExperience across the whole Software Development Life Cycle\nExpereince in Java, J2EE, Fullstack, React JS, AWS.\nExposure to agile methodologies such as CI/CD, Application Resiliency, and Security\nEmerging knowledge of software applications and technical processes within a technical discipline (e. g. , cloud, artificial intelligence, machine learning, mobile, etc. )\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Artificial Intelligence', 'Debugging', 'Machine learning', 'Agile', 'System design', 'Application development', 'Troubleshooting', 'Downstream']",2025-06-12 15:06:41
"Engineer, Staff",Qualcomm,4 - 9 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nJob description:\n\nSkill set\n\nlooking for Python programming, Machine Learning concepts and Automation Testing ( Python framework) Mandatory\n\nPrincipal Duties and Responsibilities:\n\nApplies Software knowledge to assist and support the design, development, creation, modification, and validation of embedded and cloud edge software, applications, and/or specialized utility programs.\n\nAnalyzes user needs and software requirements.\n\nDesigns and implements small software features for products and systems.\n\nParticipates in the design, coding for small features, unit testing, minor debugging fixes, and integration efforts to ensure projects are completed on schedule.\n\nAssists in performing code reviews and regression tests as well as the triaging of issues to ensure the quality of code.\n\nCollaborates with others inside project team to accomplish project objectives.\n\nWrites technical documentation for Software projects.\n\nLevel of Responsibility:\n\nWorks under supervision.\n\nDecision-making affects direct area of work and/or work group.\n\nRequires verbal and written communication skills to convey basic, routine factual information.\n\nTasks require multiple steps which can be performed in various orders; some planning, problem-solving, and prioritization must occur to complete the tasks effectively.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\n\n2+ years of work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['software engineering', 'python', 'automation testing', 'machine learning', 'python framework', 'css', 'jquery', 'sql', 'react.js', 'java', 'git', 'selenium', 'debugging', 'html', 'mysql', 'data structures', 'rest', 'c', 'python development', 'javascript', 'django framework', 'node.js', 'django', 'php', 'agile', 'aws']",2025-06-12 15:06:43
Staff Engineer - Camera Systems,Qualcomm,3 - 8 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Systems Engineer, you will research, design, develop, simulate, and/or validate systems-level software, hardware, architecture, algorithms, and solutions that enables the development of cutting-edge technology. Qualcomm Systems Engineers collaborate across functional teams to meet and exceed system-level requirements and standards.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.\nCandidate should have 10+ years of experience\n\nExperience in C/C++, Computer vision/ Image processing is must\n\nExperience in camera technology, ML/DL is good to have\n\nExperience in Embedded/arm programming is good to have but not necessary\n\nResponsibilities\n\nThe job responsibilities may include a subset of the following\n\nDesigning computer vision /image processing for mobile devices\nDesigning and evaluating algorithms to be implemented in hardware on software prototypes\nDeveloping or Optimizing image processing and computer vision algorithms for HW acceleration\nSupport product teams for commercialization, such as solution optimization, performance profiling and benchmarking.\nTest regression and release support\n\nPreferred Qualifications:\n\nExposure or working experience in Vision or Multimedia accelerators\nWorking experience with image processing algorithms.\nKnowledge/working experience in computer vision algorithms\nStrong knowledge in data structures and working experience with C/C++ programming\nSoftware optimizations experience in various SIMD and multi-threading",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['image processing', 'c++', 'c', 'computer vision', 'data structures', 'python', 'natural language processing', 'scikit-learn', 'dl', 'machine learning', 'artificial intelligence', 'deep learning', 'tensorflow', 'data science', 'embedded systems', 'keras', 'multithreading', 'arm', 'system engineering', 'ml']",2025-06-12 15:06:46
Software Engineer II,Chegg,3 - 8 years,Not Disclosed,['New Delhi'],"About the Team\nChegg's engineering team is a group of passionate engineers who, in close collaboration with data scientists, product managers, designers, and other backend developers, build the future of the online education industry. We develop our products to scale and to last, we dont take shortcuts (hello unit tests and documentation), and we take pride in delivering high-quality solutions on time. We are cloud native.\nRole\nWe are looking for software engineers passionate about solving real-world problems for students in online education using technology. The ideal candidate can think outside the box, is passionate about technology, is adaptable, thinks big, and is passionate about making an impact. Chegg is evolving very fast, and we are constantly redefining our offerings to match the requirements of our student community; the candidate should have the appetite to pivot fast and be interested in continuous improvement and learning. Chegg has a very open and vibrant engineering culture where the candidate will get the opportunity to work with the best in the industry; the role demands ideating and sharing creative ideas as you never know the next big thing Chegg works on can come from you !! If you have dreamt of leveraging your skills and knowledge to impact something big enough to matter, Chegg provides those opportunities, and the candidate should make the best use of them.\nResponsibilities\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, solution development, and proposed solutions;\nCross-team collaboration in driving the end-to-end delivery of SDN on Edge;\nParticipating in the code reviews and design discussions of other engineers;\nHave a strong sense of end-to-end ownership;\nAdhere to key principles: Code and design for best performance, scalability, and resiliency;\nParticipate in daily SCRUM meetings;\nParticipates in the testing process through test review and analysis, test witnessing, and certification of software;\nBe a self-starter, capable of solving ambiguous and challenging technical problems with wide scope;\nFull stack development of new features/tools, including design, documentation, implementation, and testing;\nWork alongside other engineers on the team to elevate technology and consistently apply best practices.\nSkills and Qualifications [Must Have]\nB.E., B.Tech, . degree in Computer Science or a related technical field\n3+ years of product lifecycle experience (from customer requirements -> functional spec -> design -> development/testing -> deployment and monitoring);\nStrong interpersonal and communication skills;\nStrong hands-on development/scripting experience with Python and shell.\nUse tools and methodologies to create representations of workflows, user interfaces, data schemas, etc;\nSolid understanding of software design and development;\nExperience with third-party libraries and APIs;\nExcellent design and problem-solving skills.\nStrong experience with Cloud technologies such as AWS\nExperience with Unit testing frameworks for TDD (Test Driven Development) methodology\nSkills and Qualifications [Good To Have]\nSolid understanding of Agile methodologies and experience working in Agile teams.\nHands-on experience with CI/CD pipelines, preferably using GitLab.\nDevelopment knowledge of mobile apps (android/iOS)",Industry Type: E-Learning / EdTech,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'schema', 'continuous integration', 'software testing', 'software design', 'unit testing', 'android', 'ci/cd', 'solution development', 'ios', 'cloud technologies', 'tdd', 'full stack', 'scrum', 'gitlab', 'shell scripting', 'software engineering', 'code review', 'agile', 'api', 'agile methodology']",2025-06-12 15:06:49
Staff Engineer- Compiler and library development,Qualcomm,4 - 9 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Systems Engineer, you will research, design, develop, simulate, and/or validate systems-level software, hardware, architecture, algorithms, and solutions that enables the development of cutting-edge technology. Qualcomm Systems Engineers collaborate across functional teams to meet and exceed system-level requirements and standards.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nBelow is the JD\n\nInterested in enabling open source developers to build software for millions of devicesInterested in leading optimization solutions for AI on the EdgeCome join our team! Our team builds open source compiler toolsets for Qualcomm silicon. This includes compilers, assemblers, linkers, libraries, debuggers, profilers, and other developer tools. The toolsets enable internal and external developers to build software ecosystems on Qualcomm hardware. We are looking for engineers who will work actively in open source communities to establish and augment compiler and system software toolsets. In this role, you will add and enhance support for Qualcomm hardware in open source projects. You will collaborate with Qualcomm hardware and software engineers to enable efficient usage of Qualcomms silicon for a broad set of applications including machine learning. You will work with the team on the entire compilation stack including optimizing code generation, improving performance, and programmer usability.Responsibilities:Work in the GCC, LLVM, glibc, and related open source communities to add features and improve performance for Qualcomm processorsIdentify areas for improvement in compiler toolsets via benchmarking and code analysisCollaborate with hardware teams to plan, identify, and contribute support in open source projects for hardware features in Qualcomm siliconIdentify areas for improvement in tool usability via interaction with users.Explore new optimization frameworks for leveraging advance CPU features.Design, develop and contribute features to open source ML frameworks.Minimum qualifications:Knowledge and/or experience in compiler frameworks such as GCC or LLVMExperience in working with open source communitiesProgramming in C/C++Bachelors degree in Engineering, Information Systems, Computer Science, or related field.Preferred qualifications:Masters degree or PhD. in Engineering, Information Systems, Computer Science, or related field.Established record of contributions to open source compiler project.Strong background in computer architecture",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['gcc', 'machine learning', 'computer architecture', 'system engineering', 'ml', 'c#', 'algorithms', 'rest', 'developer tools', 'simulation', 'system software', 'javascript', 'sql server', 'sql', 'visual studio', 'open source', 'silicon', 'java', 'computer science', 'asp.net', 'html', 'digital transformation']",2025-06-12 15:06:51
Software tools development Engineer,Qualcomm,2 - 7 years,Not Disclosed,['Chennai'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nQualcomm's Corporate Engineering division in Chennai is looking for software tools development engineer. The candidate will work in a development role to put together software for tool development and test automation across various technologies that are part of Access points, mobile platform, RF, Machine learning platforms. The candidate is expected to have full proficiency on C++ or C# or Python and have experience on developing applications, APIs, software automation using a combination of commercial test equipment and custom hardware designs.\n\nThe ideal candidate will be responsible for implementing novel test plans and supporting those test plans from the R&D lab environment through manufacturing. Candidate will also be responsible for evaluating new complex hardware designs and providing feedback regarding design for testability. Candidate will be responsible to own the test infrastructure, build automation framework and enable other developers towards achieving deployable, scalable test frameworks. Candidate will be responsible for implementing automated test solutions for those hardware designs using a combination of custom test software/hardware and commercial test equipment.\n\nThe candidate will interface with internal staff and outside partners in the fast-paced execution of a variety of multi-disciplined projects. The candidate will have an opportunity to influence and help adopt new test, tool development methodologies and enhance existing processes. International travel might be required. All Qualcomm employees are expected to actively support diversity on their teams, and in the Company.\n\nMinimum Qualifications:\n\nB.E/B.Tech. with industry experience in the following areas:\n\n2+ years of programming experience across C++ / C# / Python\n\nStrong lab skills and experience with standard lab equipment is required\n\nStrong experience in various software technologies, methodologies and applied software engineering practices/standards such as Object-Oriented Design (OOD), cloud and embedded software test automation\n\nPreferred Qualifications:\n\nStrong programming skills in C++/C#\n\nExperience with embedded software and device drivers\n\nApplication UI design Winforms/WPF\n\nExperience with hardware debug equipment such as JTAG and scope\n\nExperience with scripting languages (Perl, Python etc.)\n\nFamiliarity with AI frameworks models performance, quantization, and accuracy metrics\n\nGood analytical, debug and problem-solving abilities\n\nGood communication skills and ability to work in a cross-functional team environment\n\nEffectively delegates tasks to other team members, multitasks and meets aggressive schedules in a dynamic environment.\n\nFPGA/CPLD design, JTAG/boundary scan\n\nExperience with RF test equipment measurements such as signal generator and spectrum analyzer and HW/SW issue troubleshooting\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field.\nEducation requirements:\n\nRequiredB.E. or B.Tech. in Electronics and Communication or Electrical engineering or Computer Science or equivalent. PreferredMasters",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c#', 'c++', 'python', 'software engineering', 'perl', 'rest', 'software development', 'ood', 'device drivers', 'wpf', 'machine learning', 'artificial intelligence', 'software programming', 'winforms', 'embedded software', 'computer science', 'debugging', 'troubleshooting', 'api', 'scripting languages']",2025-06-12 15:06:54
AI Model System Software Performance Optimization Engineer,Qualcomm,1 - 6 years,Not Disclosed,['Hyderabad'],"Title : AI Model System Software Performance Optimization Engineer / Senior Engineer / Lead Engineer / Staff\n\nJob Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nWe are seeking candidates with strong optimized software development knowledge and hands-on experience AI. You will be working in a team responsible for comprehensiveness and enhancement of Performance Optimization tools of state-of-the-art machine learning solutions on Snapdragon platform.You will be working on technical initiatives to continuously benchmark the AI optimization workflow that will serve as relevant, reference case studies for application developers for Windows on Snapdragon. You will drive improvements into the SW stack including SDK, Tools, and documentation that will directly impact the ease of use and performance realization by Windows Application Developers on Snapdragon. You will work closely with development leads, software and hardware architects, customer engineers, and application developers.\n\nResponsibilities:\nUnderstand trends in ML model design, and workflow through application developer engagements and latest academic research\nContinuously measure KPIs for AI development tools on Windows on Snapdragon in terms of level of automation, ease of use, and resulting performance and accuracy preservation\nCompetitive benchmarking of tools and workflow on competitive platforms on state-of-the-art models\nEnhancement of AI performance debug, analysis, and optimization tools for AI application development for Windows on Snapdragon so that Application Developers have nil to very low barrier to entry for Windows on Snapdragon\nInterface with 3rd party application developers and other cross-site and cross-functional teams to arrive at best-in-class performant tools, and documentation that are directly leveraged by 3rd party app developers for Windows on Snapdragon\nContribute new features and designs to the Qualcomm AI toolkit to enhance the workflow experience of Application Developers\n\nSkills and Experience:\n1-10 years experience in AI application development\nExperience in building LLM applications using AI/ML tools/workflow preferably on Windows on CPU, GPU, NPU\nAbility to code in C, C++, and Python\nExperience with performance optimization of AI on GPU, NPU, CPU a plus\nStrong communication skills (written and verbal)\nDemonstrated ability to learn, think and adapt in a fast-changing environment\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['software development', 'c', 'application development', 'java', 'software engineering', 'rest', 'algorithms', 'python', 'c++', 'system software', 'cpu', 'gps', 'machine learning', 'artificial intelligence', 'sql server', 'sql', 'computer science', 'microsoft windows', 'oops', '.net', 'data structures', 'sdk', 'ml']",2025-06-12 15:06:56
Senior AI Camera Systems Engineer,Qualcomm,2 - 4 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n2-4 years of experiences in image processing/computer vision/camera domain.\nWorking experience with machine learning framework/packages (e.g, PyTorch, TensorFlow, Keras etc.)\nStrong hands on experience on developing object detection, tracking or face detection algorithms.\nStrong background in image and signal processing, statistics, and data analysis.\nDeveloping machine learning algorithms for advanced imaging features\nStrong programming skills and working experience in C/C++\\ assembly programming skills, multithreading and RTOS/OS concepts\\fundamentals and Python.\nStrong debugging skills to debug complex system level issues.\nCollaborate with cross-functional teams to design, implement and debug camera\\multimedia features for mobiles.\nGood analytical and problem-solving skills.\n\n\nResponsibilities:\nDevelopment and productize camera essential features on Qualcomm chipsets for mobile\nInfluence camera HW architecture in Qualcomm chipsets\nCollaborate with systems, software, hardware teams at various stages of chipset life in design/validation/commercialization.\nCustomer interaction to commercialize Qualcomm camera solutions.\nIndividual contributions and working with cross functional teams on camera essential features design/planning/execution/commercialization for future Snapdragon chipsets\n\n\nEducation requirements:\nRequiredBachelor's/Masters/PHd Computer Engineering and/or Electrical / Electronic Engineering\nPreferred Masters\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['algorithms', 'data analysis', 'signal processing', 'debugging', 'statistics', 'image processing', 'python', 'c++', 'c', 'object detection', 'machine learning', 'imaging', 'mac', 'hw', 'tensorflow', 'rtos', 'computer science', 'computer vision', 'pytorch', 'keras', 'multithreading', 'system engineering']",2025-06-12 15:06:59
Senior Engineer I,AMERICAN EXPRESS,10 - 15 years,Not Disclosed,['Gurugram'],"Here, your voice and ideas matter, your work makes an impact, and together, you will help us define the future of American Express.\nHow will you make an impact in this role\nAmerican Express is embarking on an exciting transformation driven by an energetic new team of high performers. This group is nimble and creative with the power to shape our technology and product roadmap. If you have the talent and desire to deliver innovative digital and servicing products at a rapid pace, serving our customers seamlessly across physical, digital, mobile, and social media, join our transformation team! You will be part of a fast-paced, entrepreneurial team responsible for delivering projects platform supporting our global customer base. Our Engineers that join our Technologies team will be assigned to one of several exciting teams that are responsible for development and management of business-critical platforms.",,,,"['Computer science', 'Automation', 'Social media', 'Analytical', 'Machine learning', 'Agile', 'Workflow', 'Scrum', 'application architecture', 'Testing']",2025-06-12 15:07:02
Senior ETL Engineer/Consultant Specialist,Hsbc,3 - 6 years,Not Disclosed,['Hyderabad'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Consultant Specialist\nIn this role you will be\nDesign and Develop ETL Processes: Lead the design and implementation of ETL processes using all kinds of batch/streaming tools to extract, transform, and load data from various sources into GCP.\nCollaborate with stakeholders to gather requirements and ensure that ETL solutions meet business needs.\nData Pipeline Optimization: Optimize data pipelines for performance, scalability, and reliability, ensuring efficient data processing workflows.\nMonitor and troubleshoot ETL processes, proactively addressing issues and bottlenecks.\nData Integration and Management: I ntegrate data from diverse sources, including databases, APIs, and flat files, ensuring data quality and consistency.\nManage and maintain data storage solutions in GCP (e. g. , BigQuery, Cloud Storage) to support analytics and reporting.\nGCP Dataflow Development: Write Apache Beam based Dataflow Job for data extraction, transformation, and analysis, ensuring optimal performance and accuracy.\nCollaborate with data analysts and data scientists to prepare data for analysis and reporting.\nAutomation and Monitoring: Implement automation for ETL workflows using tools like Apache Airflow or Cloud Composer, enhancing efficiency and reducing manual intervention.\nSet up monitoring and alerting mechanisms to ensure the health of data pipelines and compliance with SLAs.\nData Governance and Security: Apply best practices for data governance, ensuring compliance with industry regulations (e. g. , GDPR, HIPAA) and internal policies.\nCollaborate with security teams to implement data protection measures and address vulnerabilities.\nDocumentation and Knowledge Sharing: Document ETL processes, data models, and architecture to facilitate knowledge sharing and onboarding of new team members.\nConduct training sessions and workshops to share expertise and promote best practices within the team.\n\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nEducation: Bachelor s degree in Computer Science, Information Systems, or a related field.\nExperience: Minimum of 5 years of industry experience in data engineering or ETL development, with a strong focus on Data Stage and GCP.\nProven experience in designing and managing ETL solutions, including data modeling, data warehousing, and SQL development.\nTechnical Skills: Strong knowledge of GCP services (e. g. , BigQuery, Dataflow, Cloud Storage, Pub/Sub) and their application in data engineering.\nExperience of cloud-based solutions, especially in GCP, cloud certified candidate is preferred.\nExperience and knowledge of Bigdata data processing in batch mode and streaming mode, proficient in Bigdata eco systems, e. g. Hadoop, HBase, Hive, MapReduce, Kafka, Flink, Spark, etc.\nFamiliarity with Java Python for data manipulation on Cloud/Bigdata platform.\nAnalytical Skills: Strong problem-solving skills with a keen attention to detail.\nAbility to analyze complex data sets and derive meaningful insights.\nBenefits: Competitive salary and comprehensive benefits package.\nOpportunity to work in a dynamic and collaborative environment on cutting-edge data projects.\nProfessional development opportunities to enhance your skills and advance your career.\nIf you are a passionate data engineer with expertise in ETL processes and a desire to make a significant impact within our organization, we encourage you to apply for this exciting opportunity!",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data modeling', 'HIPAA', 'Data quality', 'Apache', 'Monitoring', 'Analytics', 'Financial services', 'Python']",2025-06-12 15:07:04
Cloud Support Engineer / Senior Software Engineer,Hsbc,2 - 5 years,Not Disclosed,['Pune'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Senior Software Engineer\nIn this role, you will:\nProvide three tier (L1, L2, L3) support to all applications and provide assistance to all end users.\nProactively identify any issues in production via automated monitoring, history of production issues and trends.\nMaintain schedule jobs and perform troubleshoot on processes.\nAnalyze all vendor applications and provide operational support.\nDocument all production applications and resolve all application issues and answer all requests.\nMonitor all performance metrics for various production systems and identify root cause for all technical issues and recommend solutions.\nAnalyze all applications and recommend necessary upgrades and patches and perform troubleshoot on all issues.\nMaintain effective relationships with various system administrators and development teams.\nParticipate in periodic meetings and maintain all applications for productions and plan appropriate various strategies.\nPublishing GCP cost Dashboards, Alerting and monitoring\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nKnowledge of Incident Management Problem Management is mandatory.\nProduction support ticketing knowledge is an advantage (Remedy, Jira, SQL Assistant, Blade logic, Splunk, MuleSoft, App Dynamics, GitHUB Knowledge/ Websphere etc. )\nExcellent communication skills in both Oral and Written communication.\nExcellent understanding of machine learning setup in Google architecture and google Analytics products\nShould have experience working in agile and devops environment using team collaboration tools such as Confluence, JIRA.\nProgramming skills and hands-on experience in Python desirable\nProficiency in working with cloud based native data stores/databases\nKnowledge on design patterns for GCP third party tools setup and native tools usage\nExperience in publishing GCP cost Dashboards, Alerting and monitoring",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Ticketing', 'operational support', 'Publishing', 'Google Analytics', 'Production support', 'Problem management', 'Incident management', 'Financial services', 'SQL', 'Remedy']",2025-06-12 15:07:08
Automation Engineer,Synechron,3 - 8 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027505\n\nOverall Responsibilities:\nLead the development and implementation of projects using emerging technologies\nMentor and guide team members to ensure the successful delivery of projects\nIdentify and evaluate new technology solutions to improve business processes\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nStay up-to-date with the latest technological advancements and industry trends\n\n\nSkills:\nStrong expertise in emerging technologies such as blockchain, IoT, AI, etc.\nStrong technical knowledge of software development lifecycle\nExcellent problem-solving and critical thinking skills\nGood understanding of software architecture and design patterns\nAbility to lead and manage a team of technical experts\nExperience:\nAt least 3+ years of experience in software development and leading technology projects\nProven track record of delivering projects using emerging technologies\nExperience in mentoring and guiding junior team members\nExperience in working with cross-functional teams\nDay-to-Day Activities:\nManage the development and delivery of projects using emerging technologies\nProvide technical guidance and mentorship to junior team members\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nEvaluate and recommend new technology solutions to improve business processes\nStay up-to-date with the latest technological advancements and industry trends\nQualification:\nBachelor's or Master's degree in Computer Science, Information Technology, or related field\nRelevant certifications in emerging technologies\nSoft\n\nSkills:\nStrong communication and leadership skills\nAbility to work well under pressure and meet tight deadlines\nExcellent interpersonal and team-working skills\nAbility to effectively communicate technical information to non-technical stakeholders\nPassionate about technology and a desire to stay up-to-date with the latest advancements.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['emerging technologies', 'artificial intelligence', 'iot', 'software development life cycle', 'blockchain', 'project management', 'python', 'software development', 'information technology', 'business analysis', 'machine learning', 'java', 'automation engineering', 'design patterns', 'agile']",2025-06-12 15:07:10
Senior Engineer - Network Stack Development with AI,Qualcomm,2 - 7 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\nTitleSenior EngineerJob FunctionNetwork Stack Development with AISkills/Experience:\n""ƒ""ƒ3-5 years of proficiency in C/C++, Python programming languages and Linux operating systems\n""ƒ""ƒStrong understanding of Networking concepts, particularly with L3/L4 (Layer 3/Layer 4) experience\n""ƒ""ƒKnowledge of AI/ML conceptsResponsibilities:\n""ƒ""ƒContribute to the design and implementation of AI modules for network stack components\n""ƒ""ƒPerform thorough testing to ensure the reliability and performance of the developed componentsEducation :\n""ƒ""ƒBE/MTech/MS in a relevant field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'python', 'c', 'networking concepts', 'linux', 'network expansion', 'channel sales', 'networking', 'network development', 'dealer development', 'business development', 'artificial intelligence', 'sales', 'channel development', 'marketing', 'java', 'software engineering', 'dealer management']",2025-06-12 15:07:12
Senior Engineer - Fingerprint SW,Qualcomm,2 - 7 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\n\nJob Overview\n\nAs a member of the Fingerprint SW team, the developer shall design, modify, and implement Fingerprint SW in Middleware Layer for Qualcomm Fingerprint Solution\n\n\n\nSW design and development on embedded platforms SW Stack development in Middleware layer. Debug and resolve issues in SW reported by internal test teams as well as by customers.Minimum Qualifications 3 to 5 years of experience with embedded systems Must be proficient in C and Database Concepts. Understanding of Linux User and Kernel space development. Good analytical and problem solving skills Strong understanding of basic real-time/embedded programming concepts & real time operating systems concepts Preferred Qualifications Good understanding of microprocessor, multiprocessor architecture. Good to have exposure with ARM based processor and Trustzone awareness. Good to have some basic understanding of Machine Learning and Deep learning techniques.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c', 'database creation', 'embedded systems', 'software engineering', 'embedded programming', 'rest', 'python', 'c++', 'kernel', 'machine learning', 'deep learning', 'test engineering', 'java', 'computer science', 'linux', 'debugging', 'arm', 'digital transformation', 'middleware']",2025-06-12 15:07:15
Senior AIML Engineer,Synechron,6 - 11 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027526\n\nOverall Responsibilities:\nLead the development and implementation of projects using emerging technologies\nMentor and guide team members to ensure the successful delivery of projects\nIdentify and evaluate new technology solutions to improve business processes\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nStay up-to-date with the latest technological advancements and industry trends\n\n\nSkills:\nStrong expertise in emerging technologies such as blockchain, IoT, AI, etc.\nStrong technical knowledge of software development lifecycle\nExcellent problem-solving and critical thinking skills\nGood understanding of software architecture and design patterns\nAbility to lead and manage a team of technical experts\nExperience:\nAt least 6+ years of experience in software development and leading technology projects\nProven track record of delivering projects using emerging technologies\nExperience in mentoring and guiding junior team members\nExperience in working with cross-functional teams\nDay-to-Day Activities:\nManage the development and delivery of projects using emerging technologies\nProvide technical guidance and mentorship to junior team members\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nEvaluate and recommend new technology solutions to improve business processes\nStay up-to-date with the latest technological advancements and industry trends\nQualification:\nBachelor's or Master's degree in Computer Science, Information Technology, or related field\nRelevant certifications in emerging technologies\nSoft\n\nSkills:\nStrong communication and leadership skills\nAbility to work well under pressure and meet tight deadlines\nExcellent interpersonal and team-working skills\nAbility to effectively communicate technical information to non-technical stakeholders\nPassionate about technology and a desire to stay up-to-date with the latest advancements.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['emerging technologies', 'artificial intelligence', 'iot', 'software development life cycle', 'blockchain', 'soft skills', 'project management', 'software development', 'information technology', 'program management', 'strategy consulting', 'design patterns', 'it strategy', 'agile']",2025-06-12 15:07:17
L2 Support Engineer,Synechron,5 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027519\n\nOverall Responsibilities:\nLead the development and implementation of projects using emerging technologies\nMentor and guide team members to ensure the successful delivery of projects\nIdentify and evaluate new technology solutions to improve business processes\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nStay up-to-date with the latest technological advancements and industry trends\n\n\nSkills:\nStrong expertise in emerging technologies such as blockchain, IoT, AI, etc.\nStrong technical knowledge of software development lifecycle\nExcellent problem-solving and critical thinking skills\nGood understanding of software architecture and design patterns\nAbility to lead and manage a team of technical experts\nExperience:\nAt least 5+ years of experience in software development and leading technology projects\nProven track record of delivering projects using emerging technologies\nExperience in mentoring and guiding junior team members\nExperience in working with cross-functional teams\nDay-to-Day Activities:\nManage the development and delivery of projects using emerging technologies\nProvide technical guidance and mentorship to junior team members\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nEvaluate and recommend new technology solutions to improve business processes\nStay up-to-date with the latest technological advancements and industry trends\nQualification:\nBachelor's or Master's degree in Computer Science, Information Technology, or related field\nRelevant certifications in emerging technologies\nSoft\n\nSkills:\nStrong communication and leadership skills\nAbility to work well under pressure and meet tight deadlines\nExcellent interpersonal and team-working skills\nAbility to effectively communicate technical information to non-technical stakeholders\nPassionate about technology and a desire to stay up-to-date with the latest advancements.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['emerging technologies', 'artificial intelligence', 'iot', 'software development life cycle', 'blockchain', 'project management', 'software development', 'engineering support', 'information technology', 'sql', 'production support', 'design patterns', 'application support', 'agile']",2025-06-12 15:07:20
Power Architecture -Staff Engineer,Qualcomm,8 - 13 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nJob Overview:\n\nWe are looking for software engineers who can define software architectures while viewing software as part of a larger system comprising both software and hardware. Previous architecture experience is not necessary as long as you have good software engineering skills and are willing to approach problems at the system level.In this role you will have two related areas of responsibilities:1) Participating in the definition of next generation architectures for future Qualcomm SoCs, and2) Driving the software design to realize the architecture on each SoC. This role is focused on power, thermal and limits management but you must also consider other important metrics such as performance and cost.Qualcomm SoCs serve many product categories including smartphones, tablets, wearables, IoT, servers, AR/VR and automotive (telemetry, IVI and ADAS). One challenge in this role is to drive commonality in the architecture across these diverse product categories.\n\nJob function / Responsibilities\nWork with engineers across a range of disciplines (e.g. hardware, software and systems) and technologies (e.g. advanced CPUs, Hexagon DSPs, Adreno GPUs, AR/VR, ML/AI, 5G modems, Wireless LAN, and GPS)\nParticipate in defining and communicating next generation architectures for Qualcomm SoCs with a focus on power, thermal and limits management\nDrive the process of converting the power, thermal and limits management architecture into a software design and software requirements for each SoC\nWork with software teams to provide guidance on the architecture and design, and to help resolve issues\nDesign tools to identify and debug power consumption issues on development platforms and commercial devices\n\n\nPreferred skills/experience\n\n8+ yrs of experience in software development for SoCs and platforms in wireless, automotive and/or IOT\n\nStrong analytical skills and the ability to approach problems at a system level\n\nOne or more of the following:\nDevice driver or board support package (BSP) knowledge or development experience\nExperience with one or more RTOSs\nExperience with ADAS or in vehicle infotainment systems\nUnderstanding of ARM processor architectures\nExperience in power, thermal and/or limits management at the system or device driver level\nExperience with virtualization and hypervisors\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\n\n2+ years of work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'software development', 'iot', 'java', 'software engineering', 'board support package', 'python', 'virtual reality', 'c', 'software design', 'vxworks', 'device drivers', 'spi', 'artificial intelligence', 'rtos', 'computer science', 'embedded systems', 'embedded c', 'linux', 'information systems', 'i2c']",2025-06-12 15:07:22
Engineer staff -Gstreamer Plugin development,Qualcomm,7 - 12 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nWe are seeking a skilled Engineer with extensive experience in the GStreamer multimedia framework. The ideal candidate will be responsible for designing, developing, and optimizing multimedia applications and systems. This role requires a deep understanding of multimedia processing, pipeline architecture, and the ability to work on complex projects.Minimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 7+ years of Software Engineering or related work experience. ORMaster's degree in Engineering, Information Systems, Computer Science, or related field and 5+ years of Software Engineering or related work experience. ORPhD in Engineering, Information Systems, Computer Science, or related field and 2+ year of Software Engineering or related work experience.\n7+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.Experience with majority in Multimedia framework & Gstreamer plugins development.Strong programming skills in C and C++ for embedded systemsGood knowledge about AI/ML applications developementsExposure to developing solutions on Linux is mustStrong in multi-threaded programming, synchronization and IPCsStrong Software design skills and ability to guide team of engineersGood knowledge on software development processesNeed very good Communication skills and ability to work with cross functional teamsExposure to other media frameworks such as ffmpeg, directshow, stagefright is a plusGood knowledge on V4L2, Pulseaudio, Alsa, OpenGLES is a plus.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\n\n2+ years of work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'c', 'multimedia framework', 'linux', 'gstreamer', 'python', 'software development', 'software design', 'plugins', 'opengl es', 'pipeline architecture', 'artificial intelligence', 'multimedia', 'stagefright', 'java', 'alsa', 'computer science', 'multithreading', 'software engineering', 'ffmpeg']",2025-06-12 15:07:25
Senior Consultanr - AI Cloud Engineer,AstraZeneca India Pvt. Ltd,5 - 10 years,Not Disclosed,['Chennai'],"Job Title: Senior Consultant - AI Cloud Engineer Career Level: D2 Introduction to role:\nAre you ready to tackle some of the most exciting machine-learning challenges in drug discovery? We are seeking a Senior AI Platform Engineer to join our innovative AI platform team, IGNITE. With your expertise in AWS cloud environments, youll design and deploy large-scale production infrastructure that will redefine healthcare and improve the lives of millions worldwide. As part of a close-knit team of technical specialists, youll create tools that support major AI initiatives, from clinical trial data analysis to imaging and Omics. Your role will be pivotal in providing frameworks for data scientists to develop scalable machine learning models safely and robustly. Are you prepared to bridge the gap between science and engineering with your deep expertise?\nAccountabilities:\nDesign, implement, and manage cloud infrastructure on AWS using Infrastructure as Code (IaC) tools such as Terraform or AWS CloudFormation.\nMaintain and enhance CI/CD pipelines using tools like GitHub Actions, AWS CodePipeline, Jenkins, or ArgoCD.\nEnsure platform reliability, scalability, and high availability across development, staging, and production environments.\nAutomate operational tasks, environment provisioning, and deployments using scripting languages such as Python, Bash, or PowerShell.\nEnable and maintain Amazon SageMaker environments for scalable ML model training, hosting, and pipelines.\nIntegrate AWS Bedrock to provide foundation model access for generative AI applications, ensuring security and cost control.\nLead and publish curated infrastructure templates through AWS Service Catalogue to enable consistent and compliant provisioning.\nCollaborate with security and compliance teams to implement best practices around IAM, encryption, logging, monitoring, and cost optimization.\nImplement and manage observability tools like Amazon CloudWatch, Prometheus/Grafana, or ELK for monitoring and alerting.\nSupport container orchestration environments using EKS (Kubernetes), ECS, or Fargate.\nContribute to incident response, post-mortems, and continuous improvement of the platform s operational excellence.\nEssential Skills/Experience:\nBachelor s degree in Computer Science, Engineering, or related field (or equivalent experience).\n5+ years of hands-on experience with AWS cloud services.\nStrong experience with Terraform, AWS CDK, or CloudFormation.\nProficiency in Linux system administration and networking fundamentals.\nSolid understanding of IAM policies, VPC design, security groups, and encryption.\nExperience with Docker and container orchestration using Kubernetes (EKS preferred).\nHands-on experience with CI/CD tools and version control (Git).\nExperience with monitoring, logging, and alerting systems.\nStrong solving skills and ability to work independently or in a team.\nDesirable Skills/Experience:\nAWS Certification (e.g., AWS Certified DevOps Engineer, Solutions Architect - Associate/Professional).\nExperience with serverless technologies like AWS Lambda, Step Functions, and EventBridge.\nExperience supporting machine learning or big data workloads on AWS.\nExperience with SAFe agile principles and practices.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Version control', 'Networking', 'Machine learning', 'Agile', 'Healthcare', 'Monitoring', 'Python', 'Recruitment']",2025-06-12 15:07:28
Security Engineer,Indian / Global Digital Organization,6 - 8 years,Not Disclosed,['Gurugram'],"Key Skills: Cloud Security, Cyber Security, AI Artificial intelligence.\nRoles and Responsibilities:\nSecurity Engineering:\nBuild and integrate security solutions such as firewalls, encryption tools, and intrusion detection systems to safeguard critical infrastructure and data.\nCollaborate with development teams to embed security measures throughout the software development lifecycle (SDLC).\nAutomate security workflows, vulnerability management, and incident response processes to enhance efficiency and response time.\nLead security initiatives to address emerging threats and ensure systems are resilient to evolving cyber risks.\nCloud Security:\nDesign, implement, and manage secure cloud architectures across AWS, Azure, and Google Cloud platforms.\nLeverage AI/ML-driven security tools for enhanced cloud monitoring, threat detection, and automated incident response.\nAutomate cloud security configurations and utilize AI to conduct predictive vulnerability assessments.\nWork closely with DevOps and infrastructure teams to implement robust, automated security controls in cloud environments.\nData Protection Controls:\nDesign and manage comprehensive data protection strategies including encryption, tokenization, and data masking practices to ensure data confidentiality and integrity.\nExperience Requirement:\n6-8 years of experience in building and integrating security solutions across diverse environments.\nStrong knowledge of cloud platforms such as AWS, Azure, and Google Cloud, including cloud security frameworks and best practices.\nHands-on experience with AI/ML-powered security tools for monitoring, detection, and response.\nProficiency in automating security operations and infrastructure configurations.\nExperience working in cross-functional teams, including DevOps and software engineering, to enforce security standards throughout the development lifecycle.\nFamiliarity with industry-standard protocols, tools, and methodologies in vulnerability assessment and incident management.\nEducation: B.Tech M.Tech (Dual), B.Tech.",Industry Type: Beauty & Personal Care,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Cloud Security', 'Cyber Security', 'AI Artificial intelligence.']",2025-06-12 15:07:30
Staff Engineer,Qualcomm,8 - 13 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\n\n\nIf youre interested in advancing and applying mathematics, programming languages theory, and advanced algorithms to program optimization for cutting-edge machine learning accelerators, then you really want to be talking to us!\n\n\n\nWe are looking to hire ML Compiler engineers to join our team. We work tactically on improving existing ML compilers and strategically on developing new and innovative ML compilers.\n\n\n\nOur technical approach to compilers emphasizes powerful representations for precisely and compactly modeling programs and the optimization challenges and using advanced mathematics and algorithms for performing optimizations.\n\n\n\nWe are also solid in using ""old school"" compiler technologies as they apply to contemporary ML challenges, and in meticulous software engineering to produce beautiful compilers. We are also keen about seeing our compilers used and having large impacts on Qualcomms business.\n\n\n\nMapping ML algorithms to ML accelerators is currently one of the most interesting and challenging problems for compilers. Our compiler targets include the Qualcomm Neural Signal Processor, Adreno GPUs, low-power ML accelerators, and CPU accelerators.\n\nThis job description spans multiple levels, from entry to experienced. Our team is a good home for compiler developers with advanced degrees, and we have solid mentoring and give substantial responsibility quickly for entry level engineers.\n\n\n\nResponsibilities Work on a wide range of ML compilers\n\nImprove ML compiler optimization capabilities through benchmark analysis and profiling\n\nInnovate new ML compiler and optimization algorithms\n\nUpstream compiler algorithms to open-source compiler projects Author research publications and represent the company in conferences and industry forums\n\n\n\nRequired\n\nExperience with compiler development and computer architecture\n\nML experience\n\nA degree in the field of computer science or applied mathematics\n\nExperience with software engineering\n\nSolid intellectual ability, motivation, and a strong history of achievementExcellent oral and written communication skills\n\nDesired Experience with MLIR, MLIR Dialects (LinAlg, Affine), Pytorch 2.0, TVM, Triton, and/or LLVM\n\nSYCL experience\n\nML applications and ML optimization experience\n\nML architecture experience\n\nHigh performance computing experience\n\nPolyhedral compiler optimization experience\n\nLoop transformation and vectorization experience\n\nGPU programming, parallel programming experience\n\nGeneral optimization experience\n\n8+ years of relevant work experience\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\n\n2+ years of work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'python', 'gps', 'java', 'software engineering', 'algorithms', 'database management system', 'c', 'software development', 'dbms', 'sql', 'spring', 'rtos', 'computer architecture', 'design patterns', 'embedded systems', 'linux', 'oops', 'embedded c', 'multithreading', 'data structures', 'html']",2025-06-12 15:07:33
J PED Engineer,Tata Technologies,2 - 5 years,Not Disclosed,"['Pune', 'Bengaluru']","Role Summary:\nThe Performance, Efficiency & Drivability (PED) Attributes Engineer is responsible for supporting the PED Lead Engineer with data analysis, summaries and judgement to support the delivery of Propulsion Efficiency Performance and Drivability Attributes and Features across various vehicle architectures and powertrains. A PED Engineer s scope of work spans the entire vehicle development cycle, contributing to targets definition, competitors benchmark analysis, management of attribute inputs and trades, vehicle development and sign off to brand DNA.\nWe are looking for a junior engineer to add to our dynamic and growing team. The successful candidate will work on the delivery of our new lineup of Battery Electric Vehicles, supporting the Lead Engineers in the delivery of Propulsion Efficiency.",,,,"['J PED Engineering', 'benchmark analysis', 'D&R', 'UK Driving License', 'vehicle development cycle', 'PED tools', 'FMA process', '8D', 'Quality processes']",2025-06-12 15:07:36
Technical Lead,Ericsson,10 - 15 years,Not Disclosed,['Bengaluru'],"About this opportunity\nWe are seeking a Tech Lead FPGA Designer to join the Ericsson Silicon organization. In this pivotal role, you will provide technical leadership to a group of dedicated engineers committed to developing world-class Radio and RAN Compute products.\nYou will lead the FPGA team in designing, integrating, and optimizing complex systems for high-efficiency data transfer and processing with embedded subsystems. As part of our global organization, youll collaborate with talented teams across our various sites.\nWe are committed to Agile principles, fostering a collaborative and innovative work environment that encourages creativity, teamwork, and strategic thinking.",,,,"['VHDL', 'Communication protocols', 'Hardware design', 'Verilog', 'Ethernet', 'Machine learning', 'Shell scripting', 'PCIE', 'SPI', 'Python']",2025-06-12 15:07:38
Python Developer Lead {ENG - Infosys @ Pan India - G },Infosys,4 - 9 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Responsibilities A day in the life of an Infoscion • As part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain. • You will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements. • You will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers. • You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nTechnical and Professional Requirements: • Primary skills:Process->Testing processes->Test Automation Process,Technology->Machine Learning->Python\n\nPreferred Skills: Process->Testing processes->Test Automation Process\nTechnology->Machine Learning->Python\n\nAdditional Responsibilities: • Knowledge of design principles and fundamentals of architecture • Understanding of performance engineering • Knowledge of quality processes and estimation techniques • Basic understanding of project domain • Ability to translate functional / nonfunctional requirements to systems requirements • Ability to design and code complex programs • Ability to write test cases and scenarios based on the specifications • Good understanding of SDLC and agile methodologies • Awareness of latest technologies and trends • Logical thinking and problem solving skills along with an ability to collaborate\n\nEducational RequirementsMCA,MSc,MTech,Bachelor of Engineering,BCA,BE,BSc,BTech Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Django Framework', 'Python Development', 'Python']",2025-06-12 15:07:40
Technical Lead FPGA,Ericsson,10 - 15 years,Not Disclosed,['Bengaluru'],"About this opportunity\nWe are seeking a Tech Lead FPGA Designer to join the Ericsson Silicon organization. In this pivotal role, you will provide technical leadership to a group of dedicated engineers committed to developing world-class Radio and RAN Compute products.\nYou will lead the FPGA team in designing, integrating, and optimizing complex systems for high-efficiency data transfer and processing with embedded subsystems. As part of our global organization, youll collaborate with talented teams across our various sites.\nWe are committed to Agile principles, fostering a collaborative and innovative work environment that encourages creativity, teamwork, and strategic thinking.",,,,"['VHDL', 'Communication protocols', 'Hardware design', 'Verilog', 'Ethernet', 'Machine learning', 'Shell scripting', 'PCIE', 'SPI', 'Python']",2025-06-12 15:07:42
BCN Labs_ Project leader - Full Stack,Bain,3 - 8 years,Not Disclosed,['Bengaluru'],"We're seeking a Project Leader, who is a self-starter , and brings a unique mix of data engineering expertise and analytical problem-solving ability to play a key role in the delivery of cutting-edge analytical solutions at BCN Labs. This role sits at the intersection of robust data platform engineering, software development and client-oriented delivery, requiring both hands-on implementation skills and the knack for strategic thinking to solve real world business problems.\nAs a PL, you will drive the end-to-end data pipeline lifecycle - from designing robust architectures to deploying production-grade analytical solutions. you'll also work closely with analysts, data scientists and business stakeholders to frame problems, validate solutions, and lead teams in client delivery.\n  A PL will be responsible to:\nArchitect and Deliver Scalable Data Pipelines : Build, optimize, and maintain batch and streaming pipelines using modern data engineering tools and frameworks (eg, PySpark, Airflow, Snowflake etc).\nEnd-to-End Project Leadership: Own the full delivery cycle from data ingestion and transformation to application deployment and monitoring in the cloud.\nAnalytical Framing : Work with project teams to understand business needs and help shape technical solutions that are analytically sound and measurable in terms of business value.\nMentorship and Team Leadership : Lead a team of engineers and analysts, providing technical guidance, code reviews, and project oversight to ensure quality and impact. Help build the next layer of people with full-stack capabilities at BCN Labs.\nHands-on Development : Write and review clean, modular, production-ready code. Ensure scalability, reusability, and maintainability of solutions.\nClient & Stakeholder Engagement : Communicate complex technical concepts and insights clearly and persuasively to non-technical audiences, both internally and externally.\nData Infrastructure Innovation: Contribute to internal tooling, frameworks, and automation efforts to accelerate the Labs data engineering capabilities.\nCollaborate on Analytical Solutions : Work with data scientists by enabling high-performance, we'll-governed data environments and workflows.\nEducation & Experience:\nbachelors or masters degree in Computer Science, Information Technology, Engineering, or a related field.\n5+ years (Masters + 3+ years) of proven experience in data engineering, software development, and building scalable data pipelines in a production environment.\nDemonstrated expertise in driving end-to-end analytical solution delivery from data ingestion and transformation to cloud deployment and performance optimization.\nYou will fit into our team-oriented structure with a college/hostel-style way of working, having the comfort of reaching out to anyone for support that can enable our clients better\nCore Technical Skills:\nExpertise in Python with solid experience in writing efficient, maintainable, and testable code for data pipelines and services.\nStrong skills in SQL (and NoSQL DB) for data transformation, analysis, and performance tuning.\nProficiency with HTML, CSS, JavaScript, AJAX to build data-driven UIs or Web Apps.\nExperience in developing, integrating and consuming RESTful APIs and working with microservices architecture.\nFrameworks & Platforms:\nHands-on experience with Python-based frameworks like FastAPI, Django and/or Streamlit for building data apps or APIs.\nSolid frontend development skills, with experience using modern JavaScript frameworks such as React and/or Vue.js to build interactive, data-driven UIs and Web Apps.\nFamiliarity with Docker, Git, CI/CD pipelines, and modern software delivery practices.\nExperience deploying and managing data solutions on AWS or Azure (eg, Lambda, EC2, S3, Data Factory).\nStrong preference for candidates with real-world experience in Apache Airflow, PySpark, and Snowflake.\nKnowledge of container orchestration (Eg: Kubernetes) is a plus",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'Consulting', 'HTML', 'Apache', 'Information technology', 'Monitoring', 'SQL', 'Ajax', 'Python']",2025-06-12 15:07:45
Associate- Referral - Decision Science / Data Science,Axtria,3 - 5 years,Not Disclosed,['Gurugram'],"Position Summary \n\nThis Requisition is for the Employee Referral Campaign.\n\nWe are seeking high-energy, driven, and innovative Data Scientists to join our Data Science Practice to develop new, specialized capabilities for Axtria, and to accelerate the company’s growth by supporting our clients’ commercial & clinical strategies.\n\n Job Responsibilities \n\nBe an Individual Contributor tothe Data Science team and solve real-world problems using cutting-edge capabilities and emerging technologies.\n\nHelp clients translate the business use cases they are trying to crack into data science solutions. Provide genuine assistance to users by advising them on how to leverage Dataiku DSS to implement data science projects, from design to production.\n\nData Source Configuration, Maintenance, Document and maintain work-instructions.\n\nDeep working onmachine learning frameworks such as TensorFlow, Caffe, Keras, SparkML\n\nExpert knowledge in Statistical and Probabilistic methods such as SVM, Decision-Trees, Clustering\n\nExpert knowledge of python data-science and math packages such as NumPy , Pandas, Sklearn\n\nProficiency in object-oriented languages (Java and/or Kotlin),Python and common machine learning frameworks(TensorFlow, NLTK, Stanford NLP, Ling Pipe etc\n\n\n Education \n\nBachelor Equivalent - Engineering\nMaster's Equivalent - Engineering\n\n Work Experience \n\nData Scientist 3-5 years of relevant experience in advanced statistical and mathematical models and predictive modeling using Python. Experience in the data science space prior relevant experience in Artificial intelligence and machine Learning algorithms for developing scalable models supervised and unsupervised techniques likeNLP and deep Learning Algorithms. Ability to build scalable models using Python, R-Studio, R Shiny, PySpark, Keras, and TensorFlow. Experience in delivering data science projects leveraging cloud infrastructure. Familiarity with cloud technology such as AWS / Azure and knowledge of AWS tools such as S3, EMR, EC2, Redshift, and Glue; viz tools like Tableau and Power BI. Relevant experience in Feature Engineering, Feature Selection, and Model Validation on Big Data. Knowledge of self-service analytics platforms such as Dataiku/ KNIME/ Alteryx will be an added advantage.\n\nML Ops Engineering 3-5 years of experience with MLOps Frameworks like Kubeflow, MLFlow, Data Robot, Airflow, etc., experience with Docker and Kubernetes, OpenShift. Prior experience in end-to-end automated ecosystems including, but not limited to, building data pipelines, developing & deploying scalable models, orchestration, scheduling, automation, and ML operations. Ability to design and implement cloud solutions and ability to build MLOps pipelines on cloud solutions (AWS, MS Azure, or GCP). Programming languages like Python, Go, Ruby, or Bash, a good understanding of Linux, knowledge of frameworks such as Keras, PyTorch, TensorFlow, etc. Ability to understand tools used by data scientists and experience with software development and test automation. Good understanding of advanced AI/ML algorithms & their applications.\n\nGen AI :Minimum of 4-6 years develop, test, and deploy Python based applications on Azure/AWS platforms.Must have basic knowledge on concepts of Generative AI / LLMs / GPT.Deep understanding of architecture and work experience on Web Technologies.Python, SQL hands-on experience.Expertise in any popular python web frameworks e.g. flask, Django etc. Familiarity with frontend technologies like HTML, JavaScript, REACT.Be an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT.Can interact with client on GenAI related capabilities and use cases.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'gpm', 'machine learning', 'python data', 'statistics', 'kubernetes', 'microsoft azure', 'numpy', 'javascript', 'sql', 'docker', 'pandas', 'tensorflow', 'java', 'django', 'predictive modeling', 'python web framework', 'mathematical modeling', 'pytorch', 'keras', 'aws', 'flask', 'advanced statistical']",2025-06-12 15:07:48
Director - Data Science,Axtria,12 - 17 years,Not Disclosed,['Noida'],"Minimum 12+ years of relevant experience in building software applications in data and analytics field\nEnhance the go-to-market strategy by designing new and relevant solution frameworks to accelerate our clients’ journeys for impacting patient outcomes. Pitch for these opportunities and craft winning proposals to grow the Data Science Practice.\nBuild and lead a team of data scientists and analysts, fostering a collaborative and innovative environment.\nOversee the design and delivery of the models, ensuring projects are completed on time and meet business objectives.\nEngaging in consultative selling with clients to grow/deliver business.\nDevelop and operationalize scalable processes to deliver on large & complex client engagements.\nExtensive hands-on experience with Python, R, or Julia, focusing on data science and generative AI frameworks.\nExpertise in working with generative models such as GPT, DALL-E, Stable Diffusion, Codex, and MidJourney for various applications.\nProficiency in fine-tuning and deploying generative models using libraries like Hugging Face Transformers, Diffusers, or PyTorch Lightning.\nStrong understanding of generative techniques, including GANs, VAEs, diffusion models, and autoregressive models.\nExperience in prompt engineering, zero-shot, and few-shot learning for optimizing generative AI outputs across different use cases.\nExpertise in managing generative AI data pipelines, including preprocessing large-scale multimodal datasets for text, image, or code generation.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['application software', 'python', 'artificial intelligence', 'r', 'julia', 'hive', 'natural language processing', 'neural networks', 'predictive analytics', 'machine learning', 'sql', 'deep learning', 'java', 'data science', 'spark', 'predictive modeling', 'pytorch', 'hadoop', 'statistics']",2025-06-12 15:07:50
GEN AI DevOps Engineer,Care Allianz,2 - 5 years,Not Disclosed,['Pune'],"Care Allianz is looking for GEN AI DevOps Engineer to join our dynamic team and embark on a rewarding career journey\nCollaborating with coworkers to conceptualize, develop, and release software\n\nConducting quality assurance to ensure that the software meets prescribed guidelines\n\nRolling out fixes and upgrades to software, as needed\n\nSecuring software to prevent security breaches and other vulnerabilities\n\nCollecting and reviewing customers' feedback to enhance user experience\n\nSuggesting alterations to workflow in order to improve efficiency and success\n\nPitching ideas for projects based on gaps in the market and technological advancements",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['continuous integration', 'kubernetes', 'cd', 'nexus', 'github', 'configuration', 'nagios', 'sonarqube', 'maven', 'ci/cd', 'sonar', 'docker', 'ansible', 'continuous delivery', 'puppet', 'git', 'devops', 'linux', 'jenkins', 'terraform', 'shell scripting', 'aws']",2025-06-12 15:07:53
"Platform Dev & Support Engineer (Python, NoSQL, Devops)",Qualcomm,3 - 7 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Software Developer\n\nGeneral Summary:\n\nQualcomm EDAAP (Engineering Solutions and AIML) team is seeking an experienced develop and support scalable Machine learning platform. The ideal candidate will have a strong background in building and operating distributed systems, with expertise in Rust, Python, Kubernetes, and Linux. You will play a critical role in developing, supporting and debugging our Generative AI platforms.Experience:3 to 7 years of experience strong knowledge of Python or Rust, NoSQL (Mongo/Redis), working experience of developing/supporting large scale end user facing applications.Responsibilities\nDevelop, Debug and support end to end components of large-scale Generative AI platform.\nSet up and operate Kubernetes clusters for efficient deployment and management of containerized applications\nImplement distributed microservices architecture to enable scalable and fault-tolerant inference pipelines\nEnsure optimal performance, security, and reliability of inference platforms, leveraging expertise in Linux, networking, servers, and data centers\nDevelop and maintain scripts and tools for automating deployment, monitoring, and maintenance tasks\nTroubleshoot issues and optimize system performance, using knowledge of data structures and algorithms\nWork closely with users to debug issues and address performance and scalability issues.\nParticipate in code reviews, contributing to the improvement of the overall code quality and best practices/Skills\n3 to 7 years of experience in software development, with a focus on building scalable and distributed systems\nProficiency in Rust and Python programming languages, with experience in developing high-performance applications\nExperience setting up and operating Kubernetes clusters, including deployment, scaling, and management of containerized applications\nStrong understanding of distributed microservices architecture and its application in large-scale systems\nExcellent knowledge of Linux, including shell scripting, package management, and system administration\nGood understanding of networking fundamentals, including protocols, architectures, and network security\nFamiliarity with data structures and algorithms, including trade-offs and optimization techniques\nExperience debugging complex production issues in large scale application platforms.\nExperience working with cloud-native technologies, such as containers, orchestration, and service meshes\nStrong problem-solving skills, with the ability to debug complex issues and optimize system performance\nExcellent communication and collaboration skills, with experience working with cross-functional teams and customers\n\nMinimum Qualifications:\n3+ years of IT-relevant work experience with a Bachelor's degree in a technical field (e.g., Computer Engineering, Computer Science, Information Systems).\nOR\n5+ years of IT-relevant work experience without a Bachelors degree.\n\n3+ years of any combination of academic or work experience with Full-stack Application Development (e.g., Java, Python, JavaScript, etc.)\n1+ year of any combination of academic or work experience with Data Structures, algorithms, and data stores.\nDevelop, Debug and support end to end components of large-scale Generative AI platform.\nSet up and operate Kubernetes clusters for efficient deployment and management of containerized applications\nImplement distributed microservices architecture to enable scalable and fault-tolerant inference pipelines\nEnsure optimal performance, security, and reliability of inference platforms, leveraging expertise in Linux, networking, servers, and data centers\nDevelop and maintain scripts and tools for automating deployment, monitoring, and maintenance tasks\nTroubleshoot issues and optimize system performance, using knowledge of data structures and algorithms\nWork closely with users to debug issues and address performance and scalability issues.\nParticipate in code reviews, contributing to the improvement of the overall code quality and best practices\n\n3 to 7 years of experience in software development, with a focus on building scalable and distributed systems\nProficiency in Rust and Python programming languages, with experience in developing high-performance applications\nExperience setting up and operating Kubernetes clusters, including deployment, scaling, and management of containerized applications\nStrong understanding of distributed microservices architecture and its application in large-scale systems\nExcellent knowledge of Linux, including shell scripting, package management, and system administration\nGood understanding of networking fundamentals, including protocols, architectures, and network security\nFamiliarity with data structures and algorithms, including trade-offs and optimization techniques\nExperience debugging complex production issues in large scale application platforms.\nExperience working with cloud-native technologies, such as containers, orchestration, and service meshes\nStrong problem-solving skills, with the ability to debug complex issues and optimize system performance\nExcellent communication and collaboration skills, with experience working with cross-functional teams and customers\n\nBachelors (Engineering) or Masters",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['kubernetes', 'python', 'networking', 'linux', 'shell scripting', 'container', 'algorithms', 'css', 'software development', 'javascript', 'redis', 'microservices', 'nosql', 'application development', 'docker', 'ansible', 'rust', 'system administration', 'java', 'devops', 'data structures', 'html', 'aws', 'mongodb']",2025-06-12 15:07:55
Associate Director - Data Science,Axtria,10 - 15 years,Not Disclosed,['Noida'],"Minimum of 10+ years in development, testing and deployment of React, JavaScript, Python based applications on Azure/AWS platforms\nExtensive experience with frontend and backend technologies to develop AI/GenAI applications.\nSoftware development experience in REACT, JavaScript/TypeScript, Python, FastAPI/Flask/Django is needed for UI based applications.\nMust have experience in building production grade application with frontend & backend technologies like HTML, JavaScript, REACT, Python etc.\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'production', 'javascript', 'react.js', 'python web framework', 'software testing', 'natural language processing', 'neural networks', 'microsoft azure', 'aws stack', 'machine learning', 'sql', 'deep learning', 'django', 'data science', 'html', 'typescript', 'flask', 'aws']",2025-06-12 15:07:58
Python Engineer - ML/Big Query - Hyd/Chennai/Bangalore,People staffing Solutions,5 - 10 years,12-20 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Key Responsibilities:\nDesign, develop, and maintain scalable and optimized ETL pipelines using Python and SQL.\nWork with Google BigQuery and other cloud-based platforms to build data warehousing solutions.\nDevelop and deploy ML models; collaborate with Data Scientists for productionizing models.\nWrite efficient and optimized SQL queries for large-scale data processing.\nBuild APIs using Flask/Django for machine learning and data applications.\nWork with both SQL and NoSQL databases including Elasticsearch.\nImplement data ingestion using batch and streaming technologies.\nEnsure data quality, integrity, and governance across the data lifecycle.\nAutomate and optimize CI/CD pipelines for data solutions.\nCollaborate with cross-functional teams to gather data requirements and deliver solutions.\nTroubleshoot and monitor data pipelines for seamless operations.\nRequired Skills & Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or related field.\n5+ years of experience with Python in a data engineering and/or ML context.\nStrong hands-on experience with SQL, BigQuery, and cloud data platforms (preferably GCP).\nPractical knowledge of ML concepts and experience developing ML models.\nProficiency in frameworks such as Flask and Django.\nExperience with NoSQL databases and data streaming technologies.\nSolid understanding of data modeling, warehousing, and ETL frameworks.\nFamiliarity with CI/CD tools and automation best practices.\nExcellent communication, problem-solving, and collaboration skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Machine Learning', 'Python', 'SQL', 'Pandas', 'Numpy', 'Ml', 'Flask']",2025-06-12 15:08:00
AI Engineer - Manager,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for someone who is ready for the next step in their career and is excited by the idea of solving problems and designing best in class. However, they also need to be aware of the practicalities of making a difference in the real world - whilst we love innovative advanced solutions, we also believe that sometimes a simple solution can have the most impact.\nOur AI Engineer is someone who feels the most comfortable around solving problems, answering questions and proposing solutions. We place a high value on the ability to communicate and translate complex analytical thinking into non-technical and commercially oriented concepts, and experience working on difficult projects and/or with demanding stakeholders is always appreciated.\n\nWhat can you expect from the role?\nOwn tasks end-to-end and lead on project delivery and project governance\nManagement of AI Engineer(s)\nPreparing and presenting data driven solutions to stakeholders\nDesign, develop, deploy and maintain AI solutions.\nUse a variety of AI Engineering tools and methods to deliver\nContributing to solutions design and proposal submissions\nSupporting the development of the AI engineering team within Blend\nMaintain in-depth knowledge of AI ecosystems and trends\nMentor junior colleagues\nContributing to proposal submissions and business development initiatives under the direction of the Leadership team\n\n\nQualifications\nProven ability to design, develop, test, deploy, maintain, and improve robust, scalable, and reliable software systems following best practices.\nExpertise in Python pro",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Version control', 'orchestration', 'GCP', 'Analytical', 'System integration', 'Software development life cycle', 'Project delivery', 'Operations', 'Monitoring', 'Python']",2025-06-12 15:08:02
Senior Associate - Data Science,Axtria,3 - 8 years,Not Disclosed,['Noida'],"Job Summary-\nData Scientist with good hands-on experience of 3+ years in developing state of the art and scalable Machine Learning models and their operationalization, leveraging off-the-shelf workbench production.\n\nJob Responsibilities-\n\n1. Hands on experience in Python data-science and math packages such as NumPy, Pandas, Sklearn, Seaborn, PyCaret, Matplotlib\n2. Proficiency in Python and common Machine Learning frameworks (TensorFlow, NLTK, Stanford NLP, PyTorch, Ling Pipe, Caffe, Keras, SparkML and OpenAI etc.)\n3. Experience of working in large teams and using collaboration tools like GIT, Jira and Confluence\n4. Good understanding of any of the cloud platform - AWS, Azure or GCP\n5. Understanding of Commercial Pharma landscape and Patient Data / Analytics would be a huge plus\n6. Should have an attitude of willingness to learn, accepting the challenging environment and confidence in delivering the results within timelines. Should be inclined towards self motivation and self-driven to find solutions for problems.\n7. Should be able to mentor and guide mid to large sized teams under him/her\n\nJob -\n1. Strong experience on Spark with Scala/Python/Java\n2. Strong proficiency in building/training/evaluating state of the art machine learning models and its deployment\n3. Proficiency in Statistical and Probabilistic methods such as SVM, Decision-Trees, Bagging and Boosting Techniques, Clustering\n4. Proficiency in Core NLP techniques like Text Classification, Named Entity Recognition (NER), Topic Modeling, Sentiment Analysis, etc. Understanding of Generative AI / Large Language Models / Transformers would be a plus",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'java', 'spark', 'machine learning algorithms', 'python', 'confluence', 'scikit-learn', 'nltk', 'training', 'numpy', 'tensorflow', 'git', 'seaborn', 'gcp', 'pytorch', 'keras', 'spark mllib', 'jira', 'sentiment analysis', 'lingpipe', 'caffe', 'microsoft azure', 'pandas', 'matplotlib', 'aws', 'statistics']",2025-06-12 15:08:05
GCP Adobe Engineer II,AMERICAN EXPRESS,6 - 9 years,Not Disclosed,['Chennai'],"We are looking for energetic, high-performing and highly skilled Engineers to help shape our technology and product roadmap. You will be part of the fast-paced, entrepreneurial Enterprise Personalization Portfolio focused on delivering the next generation of digital global marketing capabilities. Under Personalization portfolio, GCT team is responsible for Global Campaign Tracking of new accounts acquisition and bounty payments and leverages transformational technologies, such as Adobe Analytics, Google Analytics, SQL, GCP, Big Query, Spark & Java.\n  Focus",,,,"['Unix', 'Data analysis', 'Google Analytics', 'Coding', 'Finance', 'Data processing', 'Data mining', 'Digital marketing', 'Monitoring', 'SQL']",2025-06-12 15:08:07
Senior Associate - Data Science,Axtria,2 - 5 years,Not Disclosed,['Noida'],"Be an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT\nSoftware development experience in python is needed as backend for UI based applications\nBe a part of large delivery teams working on advanced projects when expert assistance is required.\nDeliver advanced Data Science capabilities to businesses in a meaningful manner through successful proof-of-concept solutions, and later smoothly transition the proof-of-concept into production.\nCreate Technical documents, develop, test, and deploy data analytics processes using Python, SQL on Azure/AWS platforms\nCan interact with client on GenAI related capabilities and use cases\n\n\nMust have\n\nSkills:\n\n\nMinimum of 3-5years develop, test, and deploy Python based applications on Azure/AWS platforms\nMust have basic knowledge on concepts of Generative AI / LLMs / GPT\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.\nFamiliarity with frontend technologies like HTML, JavaScript, REACT",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'software testing', 'gpm', 'microsoft azure', 'python web framework', 'data analytics', 'neural networks', 'aws stack', 'machine learning', 'javascript', 'artificial intelligence', 'sql', 'react.js', 'deep learning', 'django', 'data science', 'html', 'flask', 'aws']",2025-06-12 15:08:10
Manager - Data Science,Axtria,6 - 11 years,Not Disclosed,['Noida'],"Job Summary-\n\nData Scientist with good hands-on experience of 6+ years in developing state of the art and scalable Machine Learning models and their operationalization, leveraging off-the-shelf workbench production.\n\nJob Responsibilities-\n\nHands on experience in Python data-science and math packages such as NumPy, Pandas, Sklearn, Seaborn, PyCaret, Matplotlib\nProficiency in Python and common Machine Learning frameworks (TensorFlow, NLTK, Stanford NLP, PyTorch, Ling Pipe, Caffe, Keras, SparkML and OpenAI etc.)\nExperience of working in large teams and using collaboration tools like GIT, Jira and Confluence\nGood understanding of any of the cloud platform – AWS, Azure or GCP\nUnderstanding of Commercial Pharma landscape and Patient Data / Analytics would be a huge plus\nShould have an attitude of willingness to learn, accepting the challenging environment and confidence in delivering the results within timelines. Should be inclined towards self motivation and self-driven to find solutions for problems.\nShould be able to mentor and guide mid to large sized teams under him/her\n\n\nJob -\nStrong experience on Spark with Scala/Python/Java\nStrong proficiency in building/training/evaluating state of the art machine learning models and its deployment\nProficiency in Statistical and Probabilistic methods such as SVM, Decision-Trees, Bagging and Boosting Techniques, Clustering\nProficiency in Core NLP techniques like Text Classification, Named Entity Recognition (NER), Topic Modeling, Sentiment Analysis, etc. Understanding of Generative AI / Large Language Models / Transformers would be a plus",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'java', 'spark', 'machine learning algorithms', 'python', 'confluence', 'scikit-learn', 'nltk', 'training', 'numpy', 'tensorflow', 'git', 'seaborn', 'gcp', 'pytorch', 'keras', 'spark mllib', 'jira', 'sentiment analysis', 'lingpipe', 'caffe', 'microsoft azure', 'pandas', 'matplotlib', 'aws', 'statistics']",2025-06-12 15:08:12
Senior Associate - Data Science,Axtria,4 - 6 years,Not Disclosed,['Noida'],"o Minimum of 4-6 years' experience in developing, testing, and deploying Python based applications on Azure/AWS platforms\no Must have basic knowledge on concepts of Generative AI / LLMs / GPT\no Deep understanding of architecture and work experience on Web Technologies\no Python, SQL hands-on experience\no Expertise in any popular python web frameworks e.g. flask, Django etc.\no Familiarity with frontend technologies like HTML, JavaScript, REACT",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'gpm', 'django', 'python web framework', 'flask', 'natural language processing', 'neural networks', 'microsoft azure', 'object detection', 'machine learning', 'javascript', 'artificial intelligence', 'sql', 'react.js', 'deep learning', 'data science', 'computer vision', 'html', 'aws']",2025-06-12 15:08:14
Senior Lead business execution consultant,Wells Fargo,7 - 12 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Senior Lead business execution consultant\n\nIn this role, you will:\nAct as a Business Execution advisor to leadership to drive performance and initiatives, and develop and implement information delivery or presentations to key stakeholders and senior management",,,,"['Business execution', 'Business Implementation', 'Data Engineering', 'NLP', 'generative AI', 'Data Mining', 'machine learning', 'Strategic Planning', 'agentic AI']",2025-06-12 15:08:17
Lead ML Ops Engineer with GCP,TVS Next,8 - 10 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['hive', 'kubernetes', 'data pipeline', 'sql', 'docker', 'tensorflow', 'java', 'product management', 'gcp', 'spark', 'pytorch', 'bigquery', 'hadoop', 'big data', 'programming', 'hbase', 'ml', 'cloud sql', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'machine learning', 'data engineering', 'ops', 'mapreduce', 'kafka', 'cloud storage', 'hdfs', 'bigtable', 'aws']",2025-06-12 15:08:19
Artificial Intelligence Architect,Emerson,10 - 20 years,Not Disclosed,['Pune'],"Role & responsibilities\nDesign robust and scalable AI/ML architectures that support the development and deployment of machine learning models and AI solutions.\nDevelop and guide the implementation of end-to-end AI/ML solutions, including model development, data processing, and system integration.\nEvaluate and recommend the latest AI/ML technologies, frameworks, and tools to enhance system capabilities and performance.\nCollaborate with software engineers and other development teams to integrate AI/ML solutions into existing systems and applications. Ensure seamless operation and performance.\nWork with cross-functional teams, including developers, data scientists, machine learning engineers, and business stakeholders, to understand requirements and design solutions that align with business objectives.\n\nPreferred candidate profile\nBachelors degree in computer science, Data Science, Statistics, or a related field or a master's degree or higher is preferred.\nMore than 3 years of experience in designing and implementing AI/ML architectures, with a proven track record of successful projects.\nExtensive experience with machine learning frameworks (e.g., Go, TensorFlow, PyTorch), programming languages C#, .Net, NodeJS and data processing tools.\nStrong understanding of system architecture principles, including distributed systems, microservices, and cloud computing.\nExperience with Microsoft Azure cloud services and their AI/ML offerings\nExperience with event-handling systems such as Kafka\nExperience with big data technologies and data engineering practices.\nExcellent verbal and written communication skills, with the ability to convey complex technical concepts to non-technical stakeholders.",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Ml', 'Python', 'Tensorflow', 'Pytorch', 'Architecture', 'Artificial Intelligence', '.Net', 'Machine Learning', 'Scikit-Learn']",2025-06-12 15:08:22
Freelance AI Agent Assistant,Mindrift,0 - 4 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","At Mindrift , innovation meets opportunity. We believe in using the power of collective intelligence to ethically shape the future of AI.\nWhat we do\nThe Mindrift platform connects specialists with AI projects from major tech innovators. Our mission is to unlock the potential of Generative AI by tapping into real-world expertise from across the globe.\nAbout the Role\nIf you re a professional who works with AI Data Annotation and friendly user of LLMs, Mindrift offers a unique opportunity to apply your editing, annotating, fact-checking and creative skills to an AI training project.\nThis is a freelance role for a project, and your typical tasks may include:\nConduct high-quality web searches to verify facts, gather supporting data, and cross-check AI responses.\nPerform fact-checking and intent verification to ensure AI responses align with the users goals.\nCarefully review and flag any inaccuracies, inconsistencies, or irrelevant answers.\nProvide structured feedback on AI-generated content to help improve model performance.\nWork effectively with large language models (LLMs), understanding their capabilities and limitations, and applying best practices when interacting with them.\nPrompt generation with a purpose to receive the best quality result of LLMs.\nHow to get started\nSimply apply to this post, qualify, and get the chance to contribute to projects aligned with your skills, on your own schedule. From creating training prompts to refining model responses, you ll help shape the future of AI while ensuring technology benefits everyone.\n\n\nYou are currently enrolled in or completed a Bachelors degree or higher.\nYou have professional and/or educational experience in data annotation, demonstrate a deeper-than-user-level interest in AI, and possess intellectual b",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Training', 'Web technologies', 'Manager Technology', 'Fact']",2025-06-12 15:08:24
Python and Machine Learning Programmer,Panacorp Software Solutions,1 - 3 years,Not Disclosed,['Nagercoil'],"Job Overview:\nWe are looking for a skilled Python and Data Science Programmer to develop and implement data-driven solutions. The ideal candidate should have strong expertise in Python, machine learning, data analysis, and statistical modeling.\n\nKey Responsibilities:\nData Analysis & Processing: Collect, clean, and preprocess large datasets for analysis.\nMachine Learning: Build, train, and optimize machine learning models for predictive analytics.\nAlgorithm Development: Implement data science algorithms and statistical models for problem-solving.\nAutomation & Scripting: Develop Python scripts and automation tools for data processing and reporting.\nData Visualization: Create dashboards and visual reports using Matplotlib, Seaborn, Plotly, or Power BI/Tableau.\nDatabase Management: Work with SQL and NoSQL databases for data retrieval and storage.\nCollaboration: Work with cross-functional teams, including data engineers, business analysts, and software developers.\nResearch & Innovation: Stay updated with the latest trends in AI, ML, and data science to improve existing models.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'NoSQL', 'Power BI', 'Database Management', 'Plotly', 'Data Analysis', 'Seaborn', 'Tableau', 'SQL']",2025-06-12 15:08:27
Python and Machine Learning Programmer,Panacorp Software Solutions,1 - 3 years,Not Disclosed,['Nagercoil'],"Job Overview:\nWe are looking for a skilled Python and Data Science Programmer to develop and implement data-driven solutions. The ideal candidate should have strong expertise in Python, machine learning, data analysis, and statistical modeling.\n\nKey Responsibilities:\nData Analysis & Processing: Collect, clean, and preprocess large datasets for analysis.\nMachine Learning: Build, train, and optimize machine learning models for predictive analytics.\nAlgorithm Development: Implement data science algorithms and statistical models for problem-solving.\nAutomation & Scripting: Develop Python scripts and automation tools for data processing and reporting.\nData Visualization: Create dashboards and visual reports using Matplotlib, Seaborn, Plotly, or Power BI/Tableau.\nDatabase Management: Work with SQL and NoSQL databases for data retrieval and storage.\nCollaboration: Work with cross-functional teams, including data engineers, business analysts, and software developers.\nResearch & Innovation: Stay updated with the latest trends in AI, ML, and data science to improve existing models.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'tableau', 'data analysis', 'data science', 'predictive analytics', 'statistical modeling', 'machine learning', 'sql', 'nosql']",2025-06-12 15:08:29
Artificial Intelligence Engineer,Infosys,5 - 10 years,5-10 Lacs P.A.,['Pune'],"Our client INFOSYS is looking for Artificial Intelligence Engineer position with 5+ years of experience in Pune location. CONTRACT TO HIRE AND WORK FROM OFFICE\n\nJob Description :\nMandatory Skills : Python + LLMs + AI + Azure Certified.\nole Definition:\nThis is a specialized role for an AI Software Engineers design, build, and deploy scalable AI models and systems. They work with machine learning frameworks, cloud platforms, and data engineering tools to create and optimize AI solutions.\nSkills:\nProficient:\nLanguages/Framework: Fast API, Azure UI Search API (React)\nCloud: Azure Cloud Basics (Azure DevOps)\nGitlab: Gitlab Pipeline\nAnsible and REX: Rex Deployment\nData Science: Prompt Engineering + Modern Testing\nData pipeline development\nUnderstanding of AI/ML algorithms and their applications\nMLOps frameworks\nKnowledge of cloud platforms (Azure ML especially)\nModel deployment process\no             Data pipeline monitoring\n              Expert: (in addition to proficient skills)\no             Languages/Framework: Azure Open AI\no             Data Science: Open AI GPT Family of models 4o/4/3, Embeddings + Vector Search\no             Databases and ETL: Azure Storage Account, Postgresql, Cosmos\no             Experience with ML frameworks (TensorFlow, PyTorch, Scikit-learn)\no             Knowledge of cloud platforms (AWS SageMaker, Google AI Platform)\no             Expertise in data preprocessing, feature engineering, and model evaluation\no             Understanding of software engineering principles (version control, CI/CD, containerization)\no             Familiarity with distributed computing and big data tools (Spark, Hadoop)\no             Ability to optimize models for performance and scalability\no             Experience with Azure AI Search",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Temporary/Contractual","['llm', 'Python', 'Artificial Intelligence']",2025-06-12 15:08:31
Data Science Analyst (Standard),Infogain,3 - 5 years,Not Disclosed,['Gurugram'],"- Strong understanding of ML algorithms (regression, classification, clustering) with the ability to independently develop and scale models using Python with minimal supervision\n- Experience in commercial analytics with a knack for translating business problems into analytical solutions and strategic recommendations.\n- Proficient in Power BI to build intuitive dashboards and deliver insights in a clear, actionable format.\n  SKILLS\nPrimary Skill: Data Science\nSub Skill(s): Data Science\nAdditional Skill(s): Python, Data Science",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analyst', 'Translation', 'data science', 'Analytical', 'power bi', 'Analytics', 'Supervision', 'Python']",2025-06-12 15:08:34
AI Test Lead,Naukri,8 - 13 years,20-32.5 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nKey Responsibilities:\nAI Testing Strategy and Planning\nCollaborate with cross-functional teams to develop comprehensive AI testing strategies and plans for AI-powered applications.\nWork closely with product managers, data scientists, and developers to understand AI model requirements, use cases, and project goals.\nDefine the scope and objectives of AI testing efforts, including performance, accuracy, bias detection, and robustness of AI models. Test Execution for AI Models and Algorithms\nDesign, develop, and execute test cases for AI systems and models (including machine learning and deep learning algorithms).\nTest and validate AI solutions across various stages of the development lifecycle, including model training, testing, and deployment.\nEnsure that AI models meet business requirements and perform accurately under various real-world conditions.\nEvaluate the performance of AI models by assessing speed, efficiency, scalability, and resource utilization.\nPerform manual and automated testing on AI-based applications, platforms, and solutions.\nAI Model Accuracy and Validation\nTest AI models for accuracy, precision, recall, F1 score, and other performance metrics.\nEnsure AI models' fairness by conducting tests for potential bias in decisionmaking processes, especially in clinical or medical applications.\nValidate AI model predictions against real-world data, ensuring that results are consistent, reliable, and actionable. Also, need to look the test results from a business perspective and help evaluate the balance between risks and benefits.\nCollaboration and Knowledge Sharing\nWork with data scientists, AI engineers and Test Manager to improve testing methodologies and continuously optimize AI model testing processes.\nProvide feedback on AI models, pointing out any potential improvements in testing coverage or areas for model retraining.\nCommunicate findings, bugs, and issues related to AI models to technical teams, ensuring prompt resolution.\nHelp the team set up AI Testing standards, make informed decisions, and build knowledge across projects\nHelp the team in decision-making processes, such as whether to continue or stop investments based on testing results. Test Automation for AI Projects\nDevelop and implement automated testing scripts and frameworks specifically designed for AI applications.\nUtilize AI testing tools and frameworks (RAGAS etc.) to automate the validation of AI models and algorithms.\nIntegrate automated AI testing within continuous integration and continuous deployment (CI/CD) pipelines.\nCompliance and Regulatory Testing\nEnsure that AI applications comply with industry-specific regulations, especially in the pharma and healthcare sectors (e.g., FDA regulations, HIPAA compliance).\nVerify that all AI-driven processes adhere to ethical standards and data privacy laws.\nContinuous Improvement and Research\nStay up-to-date with the latest trends, tools, and techniques in AI testing and apply these advancements to optimize the testing process.\nParticipate in AI testing forums and workshops, contributing insights to improve best practices within the team. Reporting and Documentation\nDocument test results, methodologies, and issues clearly, providing insights into test coverage, risk analysis, and performance benchmarks.\nPrepare detailed reports for both technical and non-technical stakeholders, summarizing testing outcomes and potential risks associated with AI implementations.\nAssist in the creation and maintenance of knowledge-sharing platforms related to AI testing best practices.\nKey Skills and Qualifications:\nTechnical Expertise\nStrong knowledge of AI/ML testing methodologies and best practices.\nExperience with any AI development frameworks and libraries such as TensorFlow, Keras, PyTorch, scikit-learn, RAGAS and MLlib.\nExperience in testing tools and environments for AI-based systems (e.g., Jupyter Notebooks, Apache Spark, and DataRobot).\nExperience with performance testing tools like Grafana K6 and JMeter for AI solutions.\nKnowledge of Python (Must to have), R, JavaScript or other programming languages frequently used in AI/ML.\nKnowledge of cloud technologies like Microsoft Azure / AWS.\nUnderstanding of test automation frameworks and experience in tools like Cypress, Playwright and Pytest for automating AI tests. AI Model Evaluation\nSolid understanding of machine learning and deep learning models, including supervised and unsupervised learning techniques.\nFamiliarity with evaluating AI models on metrics such as accuracy, precision, recall, F1 score, confusion matrices, and AUC.\nAbility to identify and test for model biases, fairness, and ethical implications, especially in sensitive applications like healthcare and pharma. Analytical and Problem-Solving Skills\nStrong problem-solving abilities and keen attention to detail, with a systematic approach to diagnosing and resolving AI-related issues.\nAbility to perform root cause analysis of issues in AI algorithms and suggest actionable fixes.\nCollaboration and Communication\nExcellent teamwork and communication skills, with the ability to collaborate with cross-functional teams, including data scientists, engineers, and product managers.\nStrong verbal and written communication skills to convey technical information clearly and concisely to both technical and non-technical stakeholders.\nExperience\nMinimum of 8+ years experience in software testing, with at least 2 years focused on testing AI/ML models or AI-based applications.\nProven experience in testing AI/ML algorithms in production or staging environments.\nExperience in testing Visual AI Assistant Applications is good to have.\nExperience working in a regulated industry (such as pharmaceuticals or healthcare) is a plus.\nPreferred Qualifications:\nExperience with cloud platforms (e.g., AWS, Azure) for deploying AI applications and models. Certification in AWS/Azure will be good to have.\nFamiliarity with DevOps practices and integrating AI testing into CI/CD pipelines.\nCertification in AI/ML or related testing frameworks (e.g. ISTQB AI Tester)\nThis AI Tester role is a unique opportunity to shape the future of AI in the pharmaceutical industry. If youre passionate about AI, testing, and making a difference in healthcare, we encourage you to apply.\n\nPreferred candidate profile",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation Testing', 'AI/ ML', 'Python', 'Performance Testing', 'Automation Strategy', 'AI Framework', 'AI testing']",2025-06-12 15:08:36
Data Science Analyst (Senior),Infogain,6 - 8 years,Not Disclosed,['Gurugram'],"- Strong understanding of ML algorithms (regression, classification, clustering) with the ability to independently develop and scale models using Python.\n- Experience in commercial analytics with a knack for translating business problems into analytical solutions and strategic recommendations.\n- Proficient in Power BI to build intuitive dashboards and deliver insights in a clear, actionable format.\n- Strong storytelling and communication skills to convey complex analytical findings to both technical and non-technical audiences.\n- Demonstrates ownership and leadership, driving projects from idea to impact with minimal supervision.\nSKILLS\nPrimary Skill: Data Science\nSub Skill(s): Data Science\nAdditional Skill(s): Python, Data Science",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analyst', 'Translation', 'data science', 'Analytical', 'power bi', 'Analytics', 'Supervision', 'Python']",2025-06-12 15:08:38
Information Security Engineer,BMC Software,4 - 9 years,Not Disclosed,['Pune'],"In this role, your primary responsibilities include implementing, configuring, and supporting application security and identity access management technology solutions including generate reports and threat identification\nThe candidate needs to have experience in application security and identity management area.\n\n\nHere is how, through this exciting role, YOU will contribute to BMC's and your own success:\n\nResponsible for developing and maintaining application security and identify access management technology solutions including Sailpoint/IIQ, Okta Single Sign On, Azure AD, AWS SSO, Cloudflare Web application firewall, penetration testing, developing and maintaining internally developed Python tools and utilities.\nIdentify and develop integration opportunities between security solutions and automation but not exclusively.\nWork with virtual team/management to collect and prioritize system requirements, develop delivery plans and meet aggressive deadlines, develop code, perform unit as well as system integration testing, participate in architecture of new capabilities and debug, troubleshoot production support.\nCoordinate quality assurance and testing with users of the new functionalities/capabilities.\nGenerate reports for capability implementation\nReview report data and identify threats to discuss with management for mitigation\nEnsure that project issues are communicated in a timely and effective manner\nOther duties as assigned.\n\nTo ensure you re set up for success, you will bring the following skillset & experience:\n\nExperiences in Sailpoint IIQ, Python and Java development (automation, integration, etc) and application security are must-have.\nFamiliar with security tools in software development lifecycle as well as Azure AD, AWS APIs/CLI, containers experiences are nice to have.\nKnowledge of Artificial Intelligence learning model\nAbility to work with little supervision as well as being a team player\nExcellent verbal, written, and interpersonal communication skills\nExperience working with remote teams\n4 years + experience\nShould be willing to work in 12.30 PM to 9.30 PM shift",Industry Type: Software Product,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Information Security', 'python', 'java', 'application security', 'sailpoint', 'java development', 'aws', 'artificial intelligence']",2025-06-12 15:08:40
Principal Architect (Data and Cloud),Neoware Technology Solutions,10 - 15 years,Not Disclosed,"['Chennai', 'Bengaluru']","Principal Architect (Data and Cloud) - Neoware Technology Solutions Private Limited Principal Architect (Data and Cloud)\nRequirements\nMore than 10 years of experience in Technical, Solutioning, and Analytical roles.\n5+ years of experience in building and managing Data Lakes, Data Warehouse, Data Integration, Data Migration and Business Intelligence/Artificial Intelligence solutions on Cloud (GCP/AWS/Azure).\nAbility to understand business requirements, translate them into functional and non-functional areas, define non-functional boundaries in terms of Availability, Scalability, Performance, Security, Resilience etc.\nExperience in architecting, designing, and implementing end to end data pipelines and data integration solutions for varied structured and unstructured data sources and targets.\nExperience of having worked in distributed computing and enterprise environments like Hadoop, GCP/AWS/Azure Cloud.\nWell versed with various Data Integration, and ETL technologies on Cloud like Spark, Pyspark/Scala, Dataflow, DataProc, EMR, etc. on various Cloud.\nExperience of having worked with traditional ETL tools like Informatica / DataStage / OWB / Talend , etc.\nDeep knowledge of one or more Cloud and On-Premise Databases like Cloud SQL, Cloud Spanner, Big Table, RDS, Aurora, DynamoDB, Oracle, Teradata, MySQL, DB2, SQL Server, etc.\nExposure to any of the No-SQL databases like Mongo dB, CouchDB, Cassandra, Graph dB, etc.\nExperience in architecting and designing scalable data warehouse solutions on cloud on Big Query or Redshift.\nExperience in having worked on one or more data integration, storage, and data pipeline tool sets like S3, Cloud Storage, Athena, Glue, Sqoop, Flume, Hive, Kafka, Pub-Sub, Kinesis, Dataflow, DataProc, Airflow, Composer, Spark SQL, Presto, EMRFS, etc.\nPreferred experience of having worked on Machine Learning Frameworks like TensorFlow, Pytorch, etc.\nGood understanding of Cloud solutions for Iaas, PaaS, SaaS, Containers and Microservices Architecture and Design.\nAbility to compare products and tools across technology stacks on Google, AWS, and Azure Cloud.\nGood understanding of BI Reporting and Dashboarding and one or more tool sets associated with it like Looker, Tableau, Power BI, SAP BO, Cognos, Superset, etc.\nUnderstanding of Security features and Policies in one or more Cloud environments like GCP/AWS/Azure.\nExperience of having worked in business transformation projects for movement of On-Premise data solutions to Clouds like GCP/AWS/Azure.\nBe a trusted technical advisor to customers and solutions for complex Cloud & Data related technical challenges.\nBe a thought leader in architecture design and development of cloud data analytics solutions.\nLiaison with internal and external stakeholders to design optimized data analytics solutions.\nPartner with SMEs and Solutions Architects from leading cloud providers to present solutions to customers.\nSupport Sales and GTM teams from a technical perspective in building proposals and SOWs.\nLead discovery and design workshops with potential customers across the globe.\nDesign and deliver thought leadership webinars and tech talks alongside customers and partners.\nResponsibilities\nLead multiple data engagements on GCP Cloud for data lakes, data engineering, data migration, data warehouse, and business intelligence.\nInterface with multiple stakeholders within IT and business to understand the data requirements.\nTake complete responsibility for the successful delivery of all allocated projects on the parameters of Schedule, Quality, and Customer Satisfaction.\nResponsible for design and development of distributed, high volume multi-thread batch, real-time, and event processing systems.\nImplement processes and systems to validate data, monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.\nWork with the Pre-Sales team on RFP, RFIs and help them by creating solutions for data.\nMentor young Talent within the Team, Define and track their growth parameters.\nContribute to building Assets and Accelerators.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Db2', 'Cognos', 'MySQL', 'Datastage', 'Presales', 'Informatica', 'Oracle', 'Teradata', 'Business intelligence', 'SQL']",2025-06-12 15:08:43
Senior Software Engineer,Dynamic Yield,5 - 10 years,Not Disclosed,"['Pune', 'Gurugram']","Our Purpose\nTitle and Summary\nSenior Software Engineer\nWhat is Mastercard?\n\nMastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.\nOur decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.\n\nOur Team:\n\nThe AI & DPE team is responsible for product decisioning management and innovative product development under services business unit to address the evolving risk and security needs of all of Mastercard s various customer segments. AI & DPE team focuses on defining the strategic direction for underlying platforms to enable the successful implementation of real-time, data-driven innovative products and services focused on network, security, fraud, digital identity and authentication. The team is responsible to look across all the products/services to drive efficiency, re-usability and increase speed to market for our products and services.\n\nThe candidate for this position will focus on driving actionable insights and build innovative solutions out of multiple data sources using analytics, machine learning and reporting capabilities.\n\nThe Role:\n\nWe are seeking a Senior Software Engineer who will:\nPartner with various teams (i.e., Product Manager, Data Science, Platform Strategy, Technology) on requirement gathering in order to deliver analytics solutions that generate business value\n\nPerform data preparation by ingestion, aggregation, processing to drive and enable relevant insights from available data sets\n\nIdentify and code the best suited data algorithm model for the relevant insights\n\nManipulate and analyze complex, high-volume, high-dimensionality data from varying sources using a variety of tools and data analysis techniques.\n\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions\n\nCollect and synthesize feedback from clients, development, product and sales teams for new solutions or product enhancements\n\nAll about You\n\nStrong SQL knowledge for data preparation and mining\nStrong knowledge of writing data/machine learning algorithm in Python or R\nExperience in doing data analysis and extraction on Hadoop\nExperience of working on at least one reporting tool - Tableau and PowerBI is a plus\nExperience in data modeling, programming, querying, data mining and report development using large volumes of granular data to deliver business intelligence and custom reporting solutions\nExposure to collecting and/or working with data including standardizing, summarizing, offering initial observations and highlighting inconsistencies\nStrong understanding of the application of analytical methods and data visualization to support business decisions\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor\nAbility to easily move between business, analytical, and technical teams and articulate solution requirements for each group",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Data modeling', 'Information security', 'Analytical', 'Network security', 'Data mining', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 15:08:46
Senior Software Engineer,Mastercard,10 - 15 years,Not Disclosed,"['Pune', 'Gurugram']","The AI & DPE team is responsible for product decisioning management and innovative product development under services business unit to address the evolving risk and security needs of all of Mastercard s various customer segments. AI & DPE team focuses on defining the strategic direction for underlying platforms to enable the successful implementation of real-time, data-driven innovative products and services focused on network, security, fraud, digital identity and authentication. The team is responsible to look across all the products/services to drive efficiency, re-usability and increase speed to market for our products and services.\n\nThe candidate for this position will focus on driving actionable insights and build innovative solutions out of multiple data sources using analytics, machine learning and reporting capabilities.\n\nThe Role:\n\nWe are seeking a Senior Software Engineer who will:\nPartner with various teams (ie, Product Manager, Data Science, Platform Strategy, Technology) on requirement gathering in order to deliver analytics solutions that generate business value\n\nPerform data preparation by ingestion, aggregation, processing to drive and enable relevant insights from available data sets\n\nIdentify and code the best suited data algorithm model for the relevant insights\n\nManipulate and analyze complex, high-volume, high-dimensionality data from varying sources using a variety of tools and data analysis techniques.\n\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions\n\nCollect and synthesize feedback from clients, development, product and sales teams for new solutions or product enhancements\n\nAll about You\n\nStrong SQL knowledge for data preparation and mining\nStrong knowledge of writing data/machine learning algorithm in Python or R\nExperience in doing data analysis and extraction on Hadoop\nExperience of working on at least one reporting tool - Tableau and PowerBI is a plus\nExperience in data modeling, programming, querying, data mining and report development using large volumes of granular data to deliver business intelligence and custom reporting solutions\nExposure to collecting and/or working with data including standardizing, summarizing, offering initial observations and highlighting inconsistencies\nStrong understanding of the application of analytical methods and data visualization to support business decisions\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor\nAbility to easily move between business, analytical, and technical teams and articulate solution requirements for each group\n",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Data modeling', 'Information security', 'Analytical', 'Network security', 'Data mining', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 15:08:49
Senior Software Engineer - Python Developer,FactSet,5 - 10 years,Not Disclosed,['Hyderabad'],"FactSet creates flexible, open data and software solutions for over 200,000 investment professionals worldwide, providing instant access to financial data and analytics that investors use to make crucial decisions.\nAt FactSet, our values are the foundation of everything we do. They express how we act and operate , serve as a compass in our decision-making, and play a big role in how we treat each other, our clients, and our communities. We believe that the best ideas can come from anyone, anywhere, at any time, and that curiosity is the key to anticipating our clients needs and exceeding their expectations.",,,,"['Computer science', 'C++', 'Data analysis', 'GCP', 'Analytical', 'Machine learning', 'Technical leadership', 'Monitoring', 'SQL', 'Python']",2025-06-12 15:08:52
Manager - Data Science,Axtria,6 - 11 years,Not Disclosed,['Gurugram'],"JOB OBJECTIVE Manager with good hands-on experience of 6+ years in developing state of the art and scalable Machine Learning models and their operationalization, leveraging off-the-shelf workbench production.\n\nKEY RESPONSIBILITIES\n\n\nNecessary Skills –\n6+ years of experience of model development using Python/PySpark libraries. Development on Databricks or Dataiku DSS (Data Science Studio) environment would be a plus\nStrong experience on Spark with Scala/Python/Java\nStrong proficiency in building/training/evaluating state of the art machine learning models and its deployment\nProficiency in Statistical and Probabilistic methods such as SVM, Decision-Trees, Bagging and Boosting Techniques, Clustering\nProficiency in Core NLP techniques like Text Classification, Named Entity Recognition (NER), Topic Modeling, Sentiment Analysis, etc. Understanding of Generative AI / Large Language Models / Transformers would be a plus\nHands on experience in Python data-science and math packages such as NumPy, Pandas, Sklearn, Seaborn, PyCaret, Matplotlib\nProficiency in Python and common Machine Learning frameworks (TensorFlow, NLTK, Stanford NLP, PyTorch, Ling Pipe, Caffe, Keras, SparkML and OpenAI etc.)\nExperience of working in large teams and using collaboration tools like GIT, Jira and Confluence\nGood understanding of any of the cloud platform – AWS, Azure or GCP\nUnderstanding of Commercial Pharma landscape and Patient Data / Analytics would be a huge plus\nShould have an attitude of willingness to learn, accepting the challenging environment and confidence in delivering the results within timelines. Should be inclined towards self motivation and self-driven to find solutions for problems.\nShould be able to mentor and guide mid to large sized teams under him/her",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'java', 'spark', 'machine learning algorithms', 'python', 'confluence', 'scikit-learn', 'nltk', 'pyspark', 'training', 'numpy', 'tensorflow', 'git', 'seaborn', 'gcp', 'pytorch', 'model development', 'keras', 'jira', 'sentiment analysis', 'caffe', 'microsoft azure', 'pandas', 'matplotlib', 'aws', 'statistics']",2025-06-12 15:08:54
Artificial Intelligence Engineer,Sightspectrum,6 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai']","AI Engineer/ Artificial Intaligence Engineer\n\nShould Experticed as AI Engineer/ Artificial Intaligence Engineer Previously\n\nExperience in :Python, API\n\nWork From Office\n\nJob Location : Chennai/Hyderabad\n\nyears of Experience : 6 to 8 years,\n\nShould Experticed as AI Engineer Previously",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'API', 'Python']",2025-06-12 15:08:56
Senior Software Engineer,Dynamic Yield,5 - 8 years,Not Disclosed,['Pune'],"Our Purpose\nTitle and Summary\nSenior Software Engineer\nWho is Mastercard?\nMastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.\nOur decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.\n\nOur Team:\n\nThe AI & DPE team is responsible for product management and innovative product development of products and services to address the evolving risk and security needs of all of Mastercard s various customer segments. Platform Services team in AI & DPE focuses on defining the strategic direction for underlying platforms to enable the successful implementation of real-time, data-driven innovative products and services focused on network, security, fraud, digital identity and authentication. The team is responsible to look across all the products/services across AI & DPE and drive efficiency, re-usability and increase speed to market for our products and services.\n\nThe candidate for this position will focus on Data Unification across different data assets, enabling a single unified view of data from multiple sources and support the development of new innovative data driven cyber products, services and actionable insights.\n\nThe Role:\nWe are seeking a Senior Software Engineer who will:\n\nPerform data ingestion, aggregation, processing to drive and enable relevant insights from available data sets.\nPartner with various teams (i.e., Product Manager, Data Science, Platform Strategy, Technology) on data needs/requirements in order to deliver data solutions that generate business value.\nManipulate and analyze complex, high-volume, high-dimensionality data from varying sources using a variety of tools and data analysis techniques.\nIdentify innovative ideas and deliver proof of concepts, prototypes to deliver against the existing and future needs and propose new products, services and enhancements.\nIntegrate & Unify new data assets which increase the value proposition for our customers and enhance our existing solutions and services.\nAnalyse large volumes of transaction and product data to generate insights and actionable recommendations to drive business growth\nCollect and synthesize feedback from clients, development, product and sales teams for new solutions or product enhancements.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nMinimum 5-8 years of relevant experience.\nGood understanding of programming language preferably PySpark and Big Data technologies.\nExperience with Enterprise Business Intelligence Platform/Data platform.\nStrong SQL and higher-level programming languages with solid knowledge of data mining, machine learning algorithms and tools\nExperience with data integration tools - ETL/ELT tools (i.e. Apache NiFi, Azure Data Factory, Databricks)\nExposure to collecting and/or working with data including standardizing, summarizing, offering initial observations and highlighting inconsistencies.\nStrong understanding of the application of analytical methods and data visualization to support business decisions.\nAbility to understand complex operational systems and analytics/business intelligence tools for the delivery of information products and analytical offerings to a large, global user base.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor\nAbility to easily move between business, analytical, and technical teams and articulate solution requirements for each group",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Data analysis', 'Information security', 'Analytical', 'Network security', 'Data mining', 'Business intelligence', 'Operations', 'Analytics', 'SQL']",2025-06-12 15:09:00
Senior Software Engineer,Mastercard,5 - 8 years,Not Disclosed,['Pune'],"Senior Software Engineer\n?\n\n\n\n\nThe AI & DPE team is responsible for product management and innovative product development of products and services to address the evolving risk and security needs of all of Mastercard s various customer segments. Platform Services team in AI & DPE focuses on defining the strategic direction for underlying platforms to enable the successful implementation of real-time, data-driven innovative products and services focused on network, security, fraud, digital identity and authentication. The team is responsible to look across all the products/services across AI & DPE and drive efficiency, re-usability and increase speed to market for our products and services.\n\nThe candidate for this position will focus on Data Unification across different data assets, enabling a single unified view of data from multiple sources and support the development of new innovative data driven cyber products, services and actionable insights.\n\nThe Role:\nWe are seeking a Senior Software Engineer who will:\n\nPerform data ingestion, aggregation, processing to drive and enable relevant insights from available data sets.\nPartner with various teams (i.e., Product Manager, Data Science, Platform Strategy, Technology) on data needs/requirements in order to deliver data solutions that generate business value.\nManipulate and analyze complex, high-volume, high-dimensionality data from varying sources using a variety of tools and data analysis techniques.\nIdentify innovative ideas and deliver proof of concepts, prototypes to deliver against the existing and future needs and propose new products, services and enhancements.\nIntegrate & Unify new data assets which increase the value proposition for our customers and enhance our existing solutions and services.\nAnalyse large volumes of transaction and product data to generate insights and actionable recommendations to drive business growth\nCollect and synthesize feedback from clients, development, product and sales teams for new solutions or product enhancements.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nMinimum 5-8 years of relevant experience.\nGood understanding of programming language preferably PySpark and Big Data technologies.\nExperience with Enterprise Business Intelligence Platform/Data platform.\nStrong SQL and higher-level programming languages with solid knowledge of data mining, machine learning algorithms and tools\nExperience with data integration tools - ETL/ELT tools (i.e. Apache NiFi, Azure Data Factory, Databricks)\nExposure to collecting and/or working with data including standardizing, summarizing, offering initial observations and highlighting inconsistencies.\nStrong understanding of the application of analytical methods and data visualization to support business decisions.\nAbility to understand complex operational systems and analytics/business intelligence tools for the delivery of information products and analytical offerings to a large, global user base.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor\nAbility to easily move between business, analytical, and technical teams and articulate solution requirements for each group",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Data analysis', 'Information security', 'Analytical', 'Network security', 'Data mining', 'Business intelligence', 'Operations', 'Analytics', 'SQL']",2025-06-12 15:09:03
Manager - Data Science,Axtria,4 - 6 years,Not Disclosed,['Gurugram'],"Be an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on UI\nSoftware development experience in REACT, JavaScript/TypeScript, python is needed as frontend and backend for UI based applications\nBe a part of large delivery teams working on advanced projects when expert assistance is required.\nDeliver advanced Data Science capabilities to businesses in a meaningful manner through successful proof-of-concept solutions, and later smoothly transition the proof-of-concept into production.\nCreate Technical documents, develop, test, and deploy data analytics processes using REACT, Python, SQL on Azure/AWS platforms\nCan interact with client on GenAI related capabilities and use cases\n\n\nMust have\n\nSkills:\n\n\nMinimum of 4-6 years develop, test, and deploy React, JavaScript, Python based applications on Azure/AWS platforms\nMust have experience in building production grade application with frontend & backend technologies like HTML, JavaScript, REACT, Python etc.\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.\nBasic knowledge on concepts of Generative AI / LLMs / GPT will be beneficial",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'production', 'javascript', 'react.js', 'python web framework', 'data analytics', 'software testing', 'natural language processing', 'gpm', 'neural networks', 'microsoft azure', 'aws stack', 'machine learning', 'sql', 'deep learning', 'django', 'data science', 'html', 'typescript', 'flask', 'aws']",2025-06-12 15:09:06
"Principal Engineer, Performance Modelling",Renesas Electronics India Pvt. Ltd.,7 - 11 years,Not Disclosed,['Noida'],"We are seeking a highly skilled and experienced Engineers to join our team. The ideal candidate will be responsible for performance verification of System-on-Chip (SoC) designs using Platform Architect and Emulation Platform. This role involves working closely with SoC architects and cross-functional teams to optimize SoC performance and ensure the successful delivery of high-quality products.\nKey Responsibilities:\nDevelop and maintain performance models for SoC designs using Synopsys Platform Architect or Emulation Platform\nCollaborate with architecture, design, software and verification teams to define performance requirements and ensure alignment with overall system goals.\nAnalyze and optimize system performance, including DDR, CPU, GPU, Interconnects and high-speed interface like PCIe, UCIe etc\nIdentify performance bottlenecks and propose solutions to improve system efficiency.\nConduct performance simulations and provide detailed analysis and reports.\nMentor and guide junior engineers in performance modelling and analysis techniques and best practices.\nStay updated with the latest advancements in SoC performance modelling and industry solutions.\n\nQualifications\n\nBachelors or Masters degree in Electrical Engineering, Computer Engineering, or a related field.\n15+ years of experience in SoC performance modelling and analysis.\nProficiency in using Platform Architect or Emulation platform for performance modelling and analysis\nStrong understanding of SoC architecture, including CPU, GPU, DDR and interconnect subsystems. Any knowledge of NPUs and AI accelerators would be an added advantage.\nExperience with performance simulation tools and methodologies.\nExcellent problem-solving skills and attention to detail.\nStrong communication and collaboration skills.\nAbility to work independently and as part of a team.\n\nCompany Description\n\nRenesas is one of the top global semiconductor companies in the world. We strive to develop a safer, healthier, greener, and smarter world, and our goal is to make every endpoint intelligent by offering product solutions in the automotive, industrial, infrastructure and IoT markets. Our robust product portfolio includes world leading MCUs, SoCs, Analog and power products, plus Winning Combination solutions that curate these complementary products. We are a key supplier to the world s leading manufacturers of electronics you rely on every day; you may not see our products, but they are all around you.\n\n\n\nRenesas employs roughly 21, 000 people in more than 30 countries worldwide. As a global team, our employees actively embody the Renesas Culture, our guiding principles based on five key elements: Transparent, Agile, Global, Innovative, and Entrepreneurial. Renesas believes in, and has a commitment to, diversity and inclusion, with initiatives and a leadership team dedicated to its resources and values. At Renesas, we want to build a sustainable future where technology helps make our lives easier. Join us and build your future by being part of what s next in electronics and the world.",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Architect', 'Semiconductor', 'Simulation', 'Analog', 'SOC', 'Diversity and Inclusion', 'Agile', 'PCIE', 'Emulators', 'Automotive']",2025-06-12 15:09:08
Senior Cloud Engineer - AWS,S&P Global Market Intelligence,5 - 10 years,Not Disclosed,"['Hyderabad', 'Gurugram']","Grade Level (for internal use):\n10\nS&P Global Commodity Insights\nThe Role: Senior Cloud Engineer\nThe Location: Hyderabad, Gurgaon\nThe Team: The Cloud Engineering Team is responsible for designing, implementing, and maintaining cloud infrastructure that supports various applications and services within the S&P Global Commodity Insights organization. This team collaborates closely with data science, application development, and security teams to ensure the reliability, security, and scalability of our cloud solutions.\nThe Impact: As a Cloud Engineer, you will play a vital role in deploying and managing cloud infrastructure that supports our strategic initiatives. Your expertise in AWS and cloud technologies will help streamline operations, enhance service delivery, and ensure the security and compliance of our environments.\nWhats in it for you: This position offers the opportunity to work on cutting-edge cloud technologies and collaborate with various teams across the organization. You will gain exposure to multiple S&P Commodity Insights Divisions and contribute to projects that have a significant impact on the business. This role opens doors for tremendous career opportunities within S&P Global.\nResponsibilities:\nDesign and deploy cloud infrastructure using core AWS services such as EC2, S3, RDS, IAM, VPC, and CloudFront, ensuring high availability and fault tolerance.\nDeploy, manage, and scale Kubernetes clusters using Amazon EKS, ensuring high availability, secure networking, and efficient resource utilization.\nDevelop secure, compliant AWS environments by configuring IAM roles/policies, KMS encryption, security groups, and VPC endpoints.\nConfigure logging, monitoring, and alerting with CloudWatch, CloudTrail, and GuardDuty to support observability and incident response.\nEnforce security and compliance controls via IAM policy audits, patching schedules, and automated backup strategies.\nMonitor infrastructure health, respond to incidents, and maintain SLAs through proactive alerting and runbook execution.\nCollaborate with data science teams to deploy machine learning models using Amazon SageMaker, managing model training, hosting, and monitoring.\nAutomate and schedule data processing workflows using AWS Glue, Step Functions, Lambda, and EventBridge to support ML pipelines.\nOptimize infrastructure for cost and performance using AWS Compute Optimizer, CloudWatch metrics, auto-scaling, and Reserved Instances/Savings Plans.\nWrite and maintain Infrastructure as Code (IaC) using Terraform or AWS CloudFormation for repeatable, automated infrastructure deployments.\nImplement disaster recovery, backups, and versioned deployments using S3 versioning, RDS snapshots, and CloudFormation change sets.\nSet up and manage CI/CD pipelines using AWS services like CodePipeline, CodeBuild, and CodeDeploy to support application and model deployments.\nManage and optimize real-time inference pipelines using SageMaker Endpoints, Amazon Bedrock, and Lambda with API Gateway to ensure reliable, scalable model serving.\nSupport containerized AI workloads using Amazon ECS or EKS, including model serving and microservices for AI-based features.\nCollaborate with SecOps and SRE teams to uphold security baselines, manage change control, and conduct root cause analysis for outages.\nParticipate in code reviews, design discussions, and architectural planning to ensure scalable and maintainable cloud infrastructure.\nMaintain accurate and up-to-date infrastructure documentation, including architecture diagrams, access control policies, and deployment processes.\nCollaborate cross-functionally with application, data, and security teams to align cloud solutions with business and technical goals.\nStay current with AWS and AI/ML advancements, suggesting improvements or new service adoption where applicable.\nWhat Were Looking For:\nStrong understanding of cloud infrastructure, particularly AWS services and Kubernetes.\nProven experience in deploying and managing cloud solutions in a collaborative Agile environment.\nAbility to present technical concepts to both business and technical audiences.\nExcellent multi-tasking skills and the ability to manage multiple projects under tight deadlines.\nBasic Qualifications:\nBA/BS in computer science, information technology, or a related field.\n5+ years of experience in cloud engineering or related roles, specifically with AWS.\nExperience with Infrastructure as Code (IaC) tools such as Terraform or AWS CloudFormation.\nKnowledge of container orchestration and microservices architecture.\nFamiliarity with security best practices in cloud environments.\nPreferred Qualifications:\nExtensive Hands-on Experience with AWS Services.\nExcellent problem-solving skills and the ability to work independently as well as part of a team.\nStrong communication skills and the ability to influence stakeholders at all levels.\nExperience with greenfield projects and building cloud infrastructure from scratch.\nBenefits:\n\nHealth & Wellness: Health care coverage designed for the mind and body.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS', 'Terraform', 'AWS CloudFormation', 'Cloud', 'cloud infrastructure']",2025-06-12 15:09:11
"Data Science Specialist - R/Python, Statistical Analysis, AI/Ml",Cisco,4 - 8 years,Not Disclosed,['Bengaluru'],"Responsibilities:\nAnalysis of cross-customer and customer specific data.\nAnalysis for diagnosis of product and customers specific problems and also to demonstrate value of our data to customers.\nSupport sales and product adoption for data related use-cases (occupancy, captive portal, behavioral metrics, BMS integrations etc)\nHelp design monitoring tools to detect product and customer relative issues around product\nCustomer demonstrations of more sophisticated data products like Firehose. Engineering/Product linkages\nCollaborate with specialist teams to help deliver solutions. (Webex, Meraki etc)\nLeverage on ML based approaches for fault detection tools, for trends and also customer/category analysis\n\nQualifications:\nAdvanced degree or equivalent experience in Engineering, Computer Science, Maths or a related technical field\nProficiency in programming and scripting languagesRand/orPython\nExperience using relational database -SQL\nBasic proficiency withMachine Learning methods and applications\n\nSkills:\nPassion for problem solving.\nHighly driven and customer oriented.\nExcellent communication.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'rest', 'python', 'data analysis', 'natural language processing', 'machine learning', 'relational databases', 'artificial intelligence', 'javascript', 'sql', 'spring', 'r', 'tableau', 'java', 'computer science', 'html', 'mysql', 'data structures', 'data visualization', 'ml', 'statistics']",2025-06-12 15:09:14
AI Lead - L1,Wipro,5 - 8 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of this role is to develop minimum viable product (MVP) and comprehensive AI solutions that meet and exceed clients expectations and add value to business.\n\nDo\nManage the product/ solution development using the desired AI techniques\nLead development and implementation of custom solutions through thoughtful use of modern AI technology\nReview and evaluate the use cases and decide whether a product can be developed to add business value\nCreate the overall product development strategy and integrating with the larger interfaces\nCreate AI models and framework and implement them to cater to a business problem\nDraft the desired user Interface and create AI models as per business problem\nAnalyze technology environment and client requirements to define product solutions using AI framework/ architecture\nImplement the necessary security features as per products requirements\nReview the used case and see the latest AI that can be used in products development\nIdentify problem areas and perform root cause analysis and provide relevant solutions to the problem\nTracks industry and application trends and relates these to planning current and future AI needs\nCreate and delegate work plans to the programming team for product development\nInteract with Holmes advisory board for knowledge sharing and best practices\nResponsible for developing and maintaining client relationships with the key strategic partners and decision makers\nDrive discussions and provide consultation around product design as per customer needs\nParticipate in client interactions and gather insights regarding product development\nInteract with vertical delivery and business teams and provide and correct responses to RFP/ client requirements\nAssist in products demonstration and receive feedback from the client\nDesign presentations for seminars, meetings and enclave primarily focused over product\nTeam Management\nResourcing\nForecast talent requirements as per the current and future business needs\nHire adequate and right resources for the team\nTalent Management\nEnsure adequate onboarding and training for the team members to enhance capability & effectiveness\nBuild an internal talent pool and ensure their career progression within the organization\nManage team attrition\nDrive diversity in leadership positions\nPerformance Management\nSet goals for the team, conduct timely performance reviews and provide constructive feedback to own direct reports\nEnsure that the Performance Nxt is followed for the entire team\nEmployee Satisfaction and Engagement\nLead and drive engagement initiatives for the team\nTrack team satisfaction scores and identify initiatives to build engagement within the team\nDeliver\n\nNo.Performance ParameterMeasure1.Continuous technical project management & deliveryAdoption of new technologies, IP creation, MVP creation, Number of patents filed, Research papers created2.Client CentricityNo. of automation done, On-Time Delivery, cost of delivery, optimal resource allocation3.Capability Building & Team Management% trained on new age skills, Team attrition %, Number of webinars conducted (internal/external)\n\nMandatory Skills: Generative AI. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Generative AI', 'project management', 'resource allocation', 'team management', 'performance management', 'artificial intelligence']",2025-06-12 15:09:16
Senior Data Manager/ Lead,Codeforce 360,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description:\nWe are looking for a highly experienced and dynamic Senior Data Manager / Lead to oversee a team of Data Engineers and Data Scientists. This role demands a strong background in data platforms such as Snowflake and proficiency in Python, combined with excellent people management and project leadership skills. While hands-on experience in the technologies is beneficial, the primary focus of this role is on team leadership, strategic planning, and project delivery .\n\nJob Title : Senior Data Manager / Lead\nLocation: Hyderabad (Work From Office)\nShift Timing: 10AM-7PM\nKey Responsibilities:\nLead, mentor, and manage a team of Data Engineers and Data Scientists.\nOversee the design and implementation of data pipelines and analytics solutions using Snowflake and Python.\nCollaborate with cross-functional teams (product, business, engineering) to align data solutions with business goals.\nEnsure timely delivery of projects, with high quality and performance.\nConduct performance reviews, training plans, and support career development for the team.\nSet priorities, allocate resources, and manage workloads within the data team.\nDrive adoption of best practices in data management, governance, and documentation.\nEvaluate new tools and technologies relevant to data engineering and data science.\n\nRequired Skills & Qualifications:\n6+ years of experience in data-related roles, with at least 23 years in a leadership or management position.\nStrong understanding of Snowflake architecture, performance tuning, data sharing, security, etc.\nSolid knowledge of Python for data engineering or data science tasks.\nExperience in leading data migration, ETL/ELT, and analytics projects.\nAbility to translate business requirements into technical solutions.\nExcellent leadership, communication, and stakeholder management skills.\nExposure to tools like Databricks, Dataiku, Airflow, or similar platforms is a plus.\nBachelors or Master’s degree in Computer Science, Engineering, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Bricks', 'Python', 'Airflow', 'Data Migration', 'Dataiku', 'Data Warehousing', 'ETL', 'ELT', 'SQL']",2025-06-12 15:09:18
Conversational AI Technical Lead,Qualcomm,4 - 9 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Programmer Analyst\n\nGeneral Summary:\n\nQualcomm IT is seeking a Lead Conversational AI Developer for Intelligent Automation Center (IAC)Responsibilities include:\nExperience on designing and implementing Conversational AI solutions using Microsoft Azure and Copilot Stack in combination with GenAI\nHands-on experience with Microsoft Copilot Studio, Microsoft Bot Framework, NLP, Azure AI Search and Azure OpenAI\nExtensive hands-on experience in implementing end-to-end projects utilizing Generative AI using Retrieval-Augmented Generation (RAG) or Agentic AI architecture\nStrong expertise in Python for building bot solutions\nExperience with Azure Cognitive Services (LUIS/CLU, QnA Maker/CQA, Spell Check,Speech API) for advanced NLP features.\nKnowledge of Power Automate, Azure Logic Apps, and APIs for extending Copilot Agent and bot functionalities.\nExperience in software development with a focus on Conversational AI and Machine Learning.\nProficiency with tools and Frameworks such as LangChain, LlamaIndex, and Streamlit.\nKnowledge and implementation experience of chatbot technologies using Microsoft Azure Services and Power Platform.\nEnsure quality of coded components by performing thorough unit testing and develop reusable test cases\nWork collaboratively with test teams for supporting Product testing and UAT\nReport status, issues and risks to tech leads on a regular basis\nImprove skills in automation products through certifications\nTrain and coach team members on Conversational AI related technologies\nWork independently with minimal supervision and good team management skills\nExcellent communication and collaboration skills\nProvide timely status on assignments, planned activities, issues, and dependencies\n\nGood knowledge on Conversational AI on Microsoft Stack (Copilot Studio, Azure AI Foundry, Azure AI Search, Azure OpenAI)\nGood understanding of Generative AI concepts and Frameworks like Langchain\nHands-on programming experience on Python and any frontend technology like Angular\n\nMinimum Qualifications:\n4+ years of work experience in programming, scripting, and/or automation or IT-relevant work experience with a Bachelor's degree.\nOR\n6+ years of work experience in programming, scripting, and/or automation or IT-relevant work experience without a Bachelors degree.\n\n2+ years experience with Database Design structures such as Mongo DB, MySQL.\n\nGood understanding of conversational AI and Intelligent Automation methodologies and associated tools & technologies\nKnowledge of Process Mining concepts and implementation expereience using Celonis inclusing data models and dasboards\nExperience in business process diagrams and process flow charts with Automation Anywhere\nCertification in Industry Leading Robotic Automation products is a plus.\nExperience in identifying the right processes for Automation and providing estimates for implementations\nProgramming concepts and coding background in Python\nUnderstanding of RDBMS concepts and writing SQL queries\nExpereience in Cloud (preferrably AWS) and certifications is a plus\nExperience in Agile development using standard tools like Jira\n\nBachelor's degree and 5+ years IT-relevant work experience",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'microsoft azure', 'sql', 'database design', 'aws', 'luis', 'rdbms', 'microsoft bot framework', 'software development', 'natural language processing', 'data mining', 'azure cognitive services', 'machine learning', 'angular', 'cqa', 'qna maker', 'speech', 'mysql', 'agile', 'api', 'mongodb', 'jira']",2025-06-12 15:09:20
Immediate Opening For Data Science,Happiest Minds Technologies,8 - 13 years,Not Disclosed,['Bengaluru( Madiwala )'],"Machine Learning, Deep Learning models, Data Science. (Important);-R / python programming (mandatory) ;- Fast API development ;- deployment of models experience ; - any cloud Azure (good to have - for this requirement); - basics of Generative AI , NLP (optional - Good to have)\n\nGIS data, Geospatial data, Google Maps, ArcGIS, Demand pattern analysis\n\n5 to 15 Yrs",,,,"['Data Science', 'Machine Learning', 'Deep Learning', 'Python', 'GenAi', 'Natural Language Processing']",2025-06-12 15:09:23
Data Science Lead,Algoleap Technologies,10 - 15 years,Not Disclosed,['Hyderabad'],"As we build our Data Science Center of Excellence (CoE), we are looking for an entrepreneurial and technically strong Data Science Lead who can lay the foundation for a high-performing team. You will work directly with stakeholders across multiple business units to define use cases, lead model development, and ensure successful deployment and value realization.\n\nKey Responsibilities:\nServe as the technical and strategic lead for the Data Science CoE.",,,,"['customer analytics', 'Usage', 'data science', 'GCP', 'Machine learning', 'model development', 'Deployment', 'Stakeholder management', 'SQL', 'Python']",2025-06-12 15:09:25
Team Leader - Operations,Startek,4 - 10 years,Not Disclosed,['Noida'],STARTEK is looking for Team Leader - Operations to join our dynamic team and embark on a rewarding career journey\n\nManage and lead operations team\n\nMonitor performance and implement improvements\n\nEnsure operational targets are met\n\nCoordinate with management for strategic goals,Industry Type: Oil & Gas,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Wireless', 'Business transformation', 'orchestration', 'Artificial Intelligence', 'Team Leader Operations', 'Customer support', 'Customer experience', 'Troubleshooting', 'Customer engagement', 'digital transformation']",2025-06-12 15:09:28
HIH - Data Science Lead Analyst - Evernorth,Cigna Medical Group,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\nThe role will support the development and maintenance of proprietary advanced neural network (AI) foundation models in support of Cigna business operations.\nKey Responsibilities:\nWrite code using PyTorch and/or Tensorflow to implement, test, and operationalize deep learning models\nCollaborate with data scientists and engineers to improve deep learning models and implement business-facing solutions built on top of those models\nTake responsibility for improving code performance and quality\nFollow developments in deep learning technology to identify opportunities to improve models\nQualifications:\nBachelors or Masters(preferred) in computer science or statistics or any other equivalent discipline with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR), with emphasis on Tensorflow and Pytorch\nKnows and follows best coding and software engineering practices\nSuccessfully completes technical project components with limited guidance\nFamiliarity with deploying machine learning and predictive models to production and cloud environments\nLeadership in Data Science\nUnderstands how assigned work is related to purpose of the overall project\nIndependently identifies project roadblocks, and solutions\nSeeks to understand the health insurance domain\nScope and Impact\nDocuments the business considerations, methodology, process, code, and results associated with their work\nCollaborates to deliver clear and well developed presentations for both technical and business audiences\nConsistently communicates decisions, considerations, and needs for support\nReceives and responds to feedback in a professional and appropriate manner\nLevel of Influence\nPresent technical topics and results to non-technical stakeholders\nCommunicate and gather domain knowledge from non-technical stakeholders",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Health insurance', 'Version control', 'GIT', 'Claims', 'Coding', 'Machine learning', 'SQL', 'Python', 'Business operations']",2025-06-12 15:09:30
Data Quality Lead by Domain,Horizon Therapeutics,3 - 13 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgens data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leverages state-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. You will leverage domain, technical and business process expertise to provide exceptional support of Amgen s data governance framework. This role involves working closely with business stakeholders and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with the Product Owner and other Business Analysts to ensure operational support and excellence from the team.\nRoles Responsibilities:\nResponsible for the data governance and data management framework implementation for a given domain of expertise (Research, Development, Supply Chain, etc. ).\nResponsible for the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nDrives cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles. Develop and implement data quality frameworks. Design and enforce standards for data quality and governance to ensure consistent, accurate, and reliable data.\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. data quality best practices and tools for assigned domains.\nEnsure compliance requirements with data privacy, security, and regulatory policies for the assigned domains. Collaborate with stakeholders and work closely with data stewards, analysts, IT, and business units to understand data requirements and address quality concerns.\nJointly with Technology teams, business functions, and enterprise teams (e. g. , MDM, Enterprise Data Fabric, etc. ) define the specifications shaping the development and implementation of data foundations .\nBuild strong relationships with key business leaders and partners to ensure their needs are being met. Lead data quality initiatives aimed at improving data quality, including data cleansing, enrichment, and validation processes.\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills with knowledge of Pharma processes with specialization in a domain (e. g. , Research, Clinical Trials, Commercial, etc. ).\nIn depth knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc.\nIn depth experience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer-focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nIn depth experience of working with or supporting systems used to data governance framework. E. g. Collibra, Alation\nExcellent problem-solving skills and committed attention to detail in finding solutions. Proficiency in data analysis and quality tools (e. g. , SQL, Excel, Python, or SAS)\nGood-to-Have Functional Skills:\nExperience of working with data governance councils or forums\nExperience with Agile software development methodologies (Scrum)\n3-5 years of experience in data quality management, data governance, or related roles.\nSoft Skills:\nHighly organized and able to work under minimal supervision\nExcellent analytical and assessment skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAmbitious to further develop their skills and career\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills\nHigh degree of initiative and self-motivation.\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management and customer focus.\nBasic Qualifications:\nAny Degree and 9-13 years of experience\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Business process', 'Data analysis', 'Change management', 'SAS', 'Pharma', 'Analytical', 'Clinical trials', 'Data quality', 'SQL']",2025-06-12 15:09:32
Senior Principal Engineer,Marvell Technology,7 - 11 years,Not Disclosed,"['Pune', 'Bengaluru']","About Marvell\n.\nYour Team, Your Impact\nThe Data Centre Engineering Group develops Custom Silicon products tailored for the Data Centre market, focusing on cutting-edge Accelerated Infrastructure solutions for Networking, Switching, Connectivity, and Compute. The team works on high-performance and scalable architectures, ensuring optimized performance, power efficiency, and reliability to meet evolving data center demands. By collaborating across multiple teams, the group delivers best-in-class silicon solutions that drive innovation in next-generation data center applications.\nWhat You Can Expect\nLead the DV execution and sign-off for the entire SoC\nDefine and drive improvements in DV processes for efficient and high-quality execution\nCollaborate with IP, Subsystem, and SoC teams on test plan creation, testbench architecture, and milestone reviews\nWork closely with Design and DV teams across IP, Subsystem, and SoC levels for test plan development, execution, debug, coverage closure, and gate-level simulations\nCoordinate with cross-functional teams including Architecture, Chip Lead, Emulation, and Program Management to drive SoC-level DV execution\nPartner with Silicon bring-up and Firmware teams to support post-silicon validation and bring-up activities\nOwn and debug simulation failures to identify and resolve root causes\nArchitect and implement simulation testbenches using UVM\nDevelop and execute test plans to verify design correctness and performance\nCollaborate with logic designers for thorough verification coverage and closure\nWhat Were Looking For\nBachelor s degree in CS/EE with 22+ years of relevant experience or Master s degree in CS/EE with 20+ years of relevant experience\nStrong background in IP and SoC verification methodologies and testbench development using Verilog, SystemVerilog, UVM, and C/C++\nDeep understanding of verification techniques including object-oriented programming, white-box/black-box testing, directed/random testing, formal verification, coverage analysis, and gate-level simulations\nProven experience in DV sign-off for Functional, Power, Performance, and Security metrics\nStrong knowledge of Unix/Linux environments; scripting experience in Shell, Perl, or Python is a plus\nDemonstrated analytical and problem-solving capabilities\nAbility to manage multiple tasks in a fast-paced, dynamic environment\nExcellent interpersonal, teamwork, and communication skills\nProven ability to interface across all levels of internal and external stakeholders\nExperience leading DV execution and sign-off for complex SoCs\nHands-on involvement in 10+ successful tape-outs and post-silicon bring-up\nAdditional Compensation and Benefit Elements\nWith competitive compensation and great benefits, you will enjoy our workstyle within an environment of shared collaboration, transparency, and inclusivity. We re dedicated to giving our people the tools and resources they need to succeed in doing work that matters, and to grow and develop with us. For additional information on what it s like to work at Marvell, visit our Careers page.\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status.\n#LI-KP1",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Unix', 'C++', 'Linux', 'Networking', 'Verilog', 'Test planning', 'Perl', 'Firmware', 'Automotive', 'Python']",2025-06-12 15:09:35
HIH - Data Science Lead Analyst - Evernorth,Cigna Medical Group,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\n\nThe role will support the development and maintenance of machine learning models, with a focus on ensuring that models meet Cigna requirements for governance and legal compliance. The role will require collaboration with other data scientists and involve work across many lines of business.\nKey Responsibilities:\nAnalyze model performance of new models with specific regards to requirements for legal compliance and governance standards around accuracy and bias;\nPerform periodic analyses of performance of existing models to ensure continued compliance with internal and external standards for accuracy and bias;\nConduct research (i.e. literature review) to understand when bias may be biologically or medically justifiable, and to what degree, for example: finding evidence from literature that heart disease is more prevalent among older populations\nUsing machine learning development tools to mitigate model bias when this is determined to be necessary\nCollaborating with data scientists, business stakeholders, and governance/compliance teams to ensure models meet compliance and governance standards\nQualifications:\nBachelors or Masters/PhD (preferred) in statistics or computer science or equivalent field with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR)\nAbility to promote best coding practices, championing a culture of documentation/logging\nThorough understanding of ML lifecycle, including necessary tradeoffs and associated risks\nLeadership in Data Science\nCan own a project end-to-end e.g., scoping, business value estimation, ideation, dev, prod, timeline\nCollaborates and guides junior team members in completion of projects and career development\nWorks cross functionally with technical (e.g., Data Science, Data Engineering) and business (e.g., clinical, marketing, pricing, business analysts) to implement solutions with measurable value\nScope and Impact\nIndependently delivers clear and well-developed presentations for both technical and business audiences\nCreates data science specific project goals associated with project deliverables\nArticulates timeline changes, rationale, and goals to meet deadlines moving forward\nValues diversity, growth mindset, and improving health outcomes of our customers\n\nLevel of Influence\nCommunicate with stakeholders to identify opportunities and possible solutions based on business need\nDraft project charter, timeline, and features/stories\nInfluence matrix-partner leadership",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Version control', 'GIT', 'Claims', 'data science', 'Legal compliance', 'Coding', 'Machine learning', 'SQL', 'Python']",2025-06-12 15:09:37
"Principal Engineer, Emulation",Marvell Technology,6 - 11 years,Not Disclosed,['Bengaluru'],"About Marvell\n.\nYour Team, Your Impact\nCustom Compute and Storage (CCS) Business Unit closely collaborates with strategic customers in the development of advanced and highly complex SoCs, from architecture and design all the way through layout, packaging, prototype validation and production ramp up. The Emulation Center of Excellence (CoE) team is key part of this group, with global ownership and responsibility for delivering emulation infrastructure, validating the design on emulation and drive left shift of SW and post-silicon readiness for all of CCS products.\n\nAs part of the Emulation CoE leadership, you will drive the emulation strategy, vendor platform enablement, testplan execution for a high-quality design tape-out.\nWhat You Can Expect\nAs part of a strong technical team of emulation experts, define the emulation strategy and platform requirements, develop emulation testplan, and drive execution of the emulation verification for large CCS products on emulation platform such as Veloce, Zebu and Palladium.\nWork with project lead and various stakeholders to define the emulation HW requirements for CCS products, including platforms, hardware/software collaterals, transactors, speed-bridges etc.\nWork closely with emulation hardware vendor application engineers (AEs) to keep the emulation hardware, software ecosystem updated, drive debug and resolution of issues with the vendor and design team.\nDefine and develop new capabilities HW/SW tools to enable acceleration of RTL and improve emulation/FPGA model usability for pre-Silicon and post-Silicon functional validation as well as SW development/validation\nInterface with and provide guidance to pre-silicon Validation teams for optimizing pre-Si validation environments, test suites and methodologies for emulation efficiency\nDevelop and apply automation aids, flows and scripts in support of emulation ease of use and improvement of equipment utilization.\nWhat Were Looking For\nBachelor s degree in Computer Science, Electrical Engineering or related fields and 15+ years of related professional experience or Master s degree and/or PhD in Computer Science, Electrical Engineering or related fields with 10+ years of experience.\nSubstantial knowledge of emulation platforms offerings from various vendors such as Synopsys, Cadence, Seimens including extensive experience in building complex SOC emulation models\nWorking knowledge in one or more of the following: Processor architecture, SOC components, SOC inter-connect buses, IO protocols (PCIe, CXL, Ethernet) and memory technologies interfaces (DDR, HBM)\nStrong understanding of product development process of large SOCs and verification/debug experience in emulation platforms.\nStrong experience in coding in scripting languages like Perl, Python, Tcl & UNIX Shell etc\nAdditional Compensation and Benefit Elements\nWith competitive compensation and great benefits, you will enjoy our workstyle within an environment of shared collaboration, transparency, and inclusivity. We re dedicated to giving our people the tools and resources they need to succeed in doing work that matters, and to grow and develop with us. For additional information on what it s like to work at Marvell, visit our Careers page.\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status.\n#LI-CP1",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Automation', 'Semiconductor', 'Prototype', 'FPGA', 'Coding', 'Packaging', 'Perl', 'Automotive', 'Python']",2025-06-12 15:09:40
Data Architect - AWS,Happiest Minds Technologies,10 - 15 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Roles and responsibilities\nWork closely with the Product Owners and stake holders to design the Technical Architecture for data platform to meet the requirements of the proposed solution.\nWork with the leadership to set the standards for software engineering practices within the machine learning engineering team and support across other disciplines\nPlay an active role in leading team meetings and workshops with clients.\nChoose and use the right analytical libraries, programming languages, and frameworks for each task.",,,,"['SQL', 'data architect', 'Python', 'Pyspark', 'Apache Airflow', 'GLUE', 'Kinesis', 'Amazon Redshift', 'Data Architecture Principles', 'Data Modeling', 'Data Warehousing', 'Athena', 'Lambda', 'AWS']",2025-06-12 15:09:42
Data Analyst - Python/Hadoop,Sadup Soft,3 - 6 years,Not Disclosed,['Bengaluru'],"- Minimum of 3 years of hands-on experience.\n\n- Python/ML, Hadoop, Spark : Minimum of 2 years of experience.\n\n- At least 3 years of prior experience as a Data Analyst.\n\n- Detail-oriented with a structured thinking and analytical mindset.\n\n- Proven analytic skills, including data analysis, data validation, and technical writing.\n\n- Strong proficiency in SQL and Excel.\n\n- Experience with Big Query is mandatory.\n\n- Knowledge of Python and machine learning algorithms is a plus.\n\n- Excellent communication skills with the ability to be precise and clear.\n\n- Learning Ability : Ability to quickly learn and adapt to new analytic tools and technologies.\n\nKey Responsibilities :\n\nData Analysis :\n\n- Perform comprehensive data analysis using SQL, Excel, and Big Query.\n\n- Validate data integrity and ensure accuracy across datasets.\n\n- Develop detailed reports and dashboards that provide actionable insights.\n\n- Create and deliver presentations to stakeholders with clear and concise findings.\n\n- Document queries, reports, and analytical processes clearly and accurately.\n\n- Leverage Python/ML for advanced data analysis and model development.\n\n- Utilize Hadoop and Spark for handling and processing large datasets.\n\n- Work closely with cross-functional teams to understand data requirements and provide analytical support.\n\n- Communicate findings effectively and offer recommendations based on data analysis.\n\nEducation : Bachelor's degree in Computer Science, Data Science, Statistics, or a related field.\n\nExperience : Minimum of 3 years of experience as a Data Analyst with a strong focus on SQL, Excel, and Big Query.\n\nTechnical Skills : Proficiency in SQL, Excel, and Big Query; experience with Python, ML, Hadoop, and Spark is preferred.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Data Validation', 'Big Query', 'Data Integrity', 'Hadoop', 'Spark', 'Python', 'SQL']",2025-06-12 15:09:45
Azure Data Bricks (4-15 Yrs) - Bangalore,Happiest Minds Technologies,4 - 9 years,Not Disclosed,['Bengaluru'],"Hi,\n\nGreetings from Happiest Minds Technologies\n\nCurrently we are hiring for below positions and looking for immediate joiners.\n1. Azure Databricks Bangalore 5 to 10 Yrs - Bangalore\nAs a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools. Proficiency in Python PySpark, Pandas, NumPy, SciPy, Spark SQL, DataFrames, RDDs, Delta Lake, Databricks Notebooks, and MLflow is required, along with hands-on experience in Azure Data Lake, Blob Storage, and Synapse Analytics.",,,,"['Pyspark', 'Azure', 'Data Bricks', 'sql', 'ETL']",2025-06-12 15:09:47
Principal Engineer - STA,Alphawave Semi,14 - 19 years,Not Disclosed,['Bengaluru'],"The Opportunity\n\nWere looking for the Wavemakers of tomorrow.\nWhat youll need:\nGood understanding of overall design Flow RTL to GDS.\nMust have 14+ years of experience on signing off the full chip synthesis/STA for tape outs\nHands on Experience on Both Block level and Full chip timing Constraints Development and Management for hierarchical designs.\nDeep Understanding of DFT Constraints.\nHand on Synthesis & STA Experience on Lower node Technologies with Synopsys/Cadence Tools.\ngood understanding of overall ASIC Physical Design/DFT, Tools and implication on Timing Convergence\nMust have in-depth understanding of relevant areas of Library / Memory / Other collaterals and dependencies on STA\nMust understand Ultra Submicron issues, Variation aware/Aging Aware Design Sign-off Must understand CTS/Other clock Distribution methodologies well.\nGood knowledge on Timing Budgets.\nKnowledge on Perl / TCL / Python scripting language\nExperience on multi voltage designs using CPF/UPF.\nGood understanding on timing/area/power/complexity tradeoffs on complex interface design\nHands on experience on power analysis using PTPX\nGood understanding of VHDL / Verilog Constructs.\nFamiliarity with IP level verification and strong RTL debugging capabilities is an added bonus\nA, enthusiastic team player who enjoys working with others\nExperience troubleshooting issues with users Experience communicating updates and resolutions to customers and other partners complex technical concepts to other design peers in verbal and written form\nWhat Youll Do:\nWe are looking for experienced STA engineer to lead the timing convergence of the SoCs.Responsibilities include\nSTA setup, convergence, reviews and sign-off for Multi-Mode and Multi-corner Multi voltage domain designs.\nConstraint Generation & Maintenance for Block / SOC for complex hierarchical Designs for all the Modes\nTiming analysis, and timing closure at Full chip level while supporting the PD team on Block/SS level timing convergence.\nInteraction with Design, DFT, IP&PD teams for Timing Convergence & Resolving Constraint Conflicts.\nSupport Verification team to enable GLS.\nGuide the CTS strategies and provide feedback to Implementation Team.\nManage the timing ECO generation and strategize the implementation methodology.\nDevelop Automation scripts with-in STA tools for Methodology development.\nYou will be reporting to Director - ASIC engineering.\nWe have a flexible work environment to support and help employees thrive in personal and professional capacities""\nAs part of our commitment to the well-being and satisfaction of our employees, we have designed a comprehensive benefits package that includes:\nCompetitive Compensation Package\nRestricted Stock Units (RSUs)\nProvisions to pursue advanced education from Premium Institute, eLearning content providers\nMedical Insurance and a cohort of Wellness Benefits\nEducational Assistance\nAdvance Loan Assistance\nOffice lunch & Snacks Facility\nEqual Employment Opportunity Statement\nAlphawave Semi is an equal opportunity employer, welcoming all applicants regardless of age, gender, race, disability, or other protected characteristics. We value diversity and provide accommodations during the recruitment process.",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Wireless', 'Automation', 'VHDL', 'ASIC', 'DFT', 'Verilog', 'Perl', 'Troubleshooting', 'Physical design', 'Python']",2025-06-12 15:09:49
Design Verification Principal Engineer,Marvell Technology,10 - 15 years,Not Disclosed,"['Pune', 'Bengaluru']","About Marvell\n.\nYour Team, Your Impact\nThe Data Centre Engineering Group develops Custom Silicon products tailored for the Data Centre market, focusing on cutting-edge Accelerated Infrastructure solutions for Networking, Switching, Connectivity, and Compute. The team works on high-performance and scalable architectures, ensuring optimized performance, power efficiency, and reliability to meet evolving data center demands. By collaborating across multiple teams, the group delivers best-in-class silicon solutions that drive innovation in next-generation data center applications.\nWhat You Can Expect\nArchitect and implement simulation test bench in UVM.\nDevelop and execute test-plans for verifying correctness and performance of the design.\nOwn and debug failures in simulation to root-cause problems\nClosely work with logic designers of the block being verified for test plan development, execution, debug, coverage closure and gate level simulations\nWhat Were Looking For\nBachelor s degree in CS/EE with 13-15 years of relevant experience, or Master s degree in CS/EE with 10-12 years of relevant experience\nStrong background in IP and SoC verification, including methodology and testbench development\nProficient in hardware verification languages such as Verilog, SystemVerilog, UVM, and C/C++\nSolid understanding of verification methodologies: object-oriented programming, white-box/black-box testing, directed/random testing, coverage analysis, and gate-level simulations\nExperience in Unix/Linux environments; scripting skills in Shell, Perl, or Python are a plus\nStrong analytical and problem-solving skills\nAbility to manage multiple tasks in a fast-paced environment\nExcellent communication, interpersonal, and teamwork skills\nCapable of interfacing effectively at all levels within and outside the organization\nProactive in participating in problem-solving and quality improvement initiatives\nAdditional Compensation and Benefit Elements\nWith competitive compensation and great benefits, you will enjoy our workstyle within an environment of shared collaboration, transparency, and inclusivity. We re dedicated to giving our people the tools and resources they need to succeed in doing work that matters, and to grow and develop with us. For additional information on what it s like to work at Marvell, visit our Careers page.\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status.\n#LI-KP1",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Unix', 'C++', 'Linux', 'Networking', 'Verilog', 'Test planning', 'Perl', 'Automotive', 'Python', 'White box']",2025-06-12 15:09:51
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Gurugram'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nData Engineer with Neo4j\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nGurugram, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Gurugram\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-12 15:09:54
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Chennai'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nGurugram\nData Engineer with Neo4j\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nChennai, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Chennai\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-12 15:09:56
"Principal Engineer, Bridges & Civils, REC",Ramboll,6 - 10 years,Not Disclosed,['Gurugram'],"Principal Engineer, Bridges & Civils, REC\nCompany Description\nAbout Ramboll\nFounded in Denmark, Ramboll is a foundation-owned people company. We have more than 18,000 experts working across our global operations in 35 countries. Our experts are leaders in their fields, developing and delivering innovative solutions in diverse markets including Buildings, Transport, Planning & Urban Design, Water, Environment & Health, Energy, and Management Consulting. We invite you to contribute to a more sustainable future working in an open, collaborative, and empowering company. Combining local experience with global knowledge, we together shape the societies of tomorrow.\nEquality, diversity, and inclusion are at the heart of what we do\nWe believe in the strength of diversity and know that unique experiences and perspectives are vital for creating truly sustainable societies. Therefore, we are committed to providing an inclusive and supportive work environment where everyone can flourish and reach their potential. We welcome applications from candidates of all backgrounds and encourage you to contact our recruitment team to discuss any accommodations you need during the application process.\nJob Description\nPrincipal Engineer, Bridges & Civils, REC \nRamboll in Middle East and Asia Pacific \nAt Ramboll, our 15,000 consulting engineers and scientists; designers and management consultants are based in more than 300 offices in 35 countries across the globe. In the Middle East and Asia Pacific region, we have more than 1,500 experts working across 15 offices present in India, Malaysia, Singapore, China, Hong Kong, Australia, New Zealand, Qatar, and the United Arab Emirates. Our experts are applying their passion to deliver innovative solutions in markets as diverse as Buildings, Transport, Planning & Urban Design, Water, Environment & Health, and Energy. Founded in Denmark, Ramboll is a people company. We invite you to contribute to a sustainable future working in an open, collaborative, and empowering culture. \nJob Description \nWe invite you to bring your strong knowledge on bridge design and Lusas/Sofistik into play as you would be key player in the technical delivery of the project and would carry out the design and would also be responsible of the delivery of design/drawings. To succeed in this role, you must have Knowledge of design codes like Euro code/DMRB/AASHTO/any other international standards and M. Tech degree in Structural Engineering with more than 7 years of experience. Are you our new Principal Engineer - Bridges & Civils? Click the apply-button to send your application. \nInviting bright minds \nDo you want to push the boundaries of your profession and develop your excellence in an open, collaborative, and empowering culture? We work to create a sustainable future, and our inspiring projects and innovative solutions strive to set the standard among our peers. You will join a global company that has been growing successfully since its founding in 1945. Together, we lead and leave a positive impact on societies, companies, and people around the world. \nYou will join our RECdepartment \nAs our new Principal Engineer - Bridges & Civils you will be part of a world class, innovation driven engineering design center owned by an independent trust and its employees. REC is a highly sophisticated center of engineering excellence and based in our India head office in Gurgaon. Working in partnership with all our established offices globally, the Ramboll Engineering Centre (REC) is a center for excellence in design by offering optimized solutions to the rest of the organization. \nYour key tasks and responsibilities will be: \nCarrying out the design and review of the work of Asst Engineer / Design engineers and modelers in the team and maintaining the quality of deliverables. \nParticipate in the design and delivery of Bridge Projects, coordinate with other team members of the drafting/modelling team in accomplishing complex tasks. \nMentoring and supervising asst. Engineers/ Design Engineers and provide inputs to Team Lead for the continued development of the staff in the team \nIs responsible for technical correctness and timely delivery of the design documents and 3D models corresponding to the design. \nAs a REC Project Manager for global engineering Bridge Projects, coordinated with the design team for project planning/preparation of schedule/WBS and delivering projects to time and budget. \nPerform complex analysis using computer modelling and increase efficiencies in the processes and technical design. \nParticipate in skill enhancements of the Team. \nExercises self-discipline and work ethic, respect and follow company policies and procedures.  \nYour starting point for constant growth \nFrom the moment you join Ramboll, we will support your personal and professional development so that you grow with the company. For this role, we believe your starting point is: \nWe are looking for self-motivated team members who meet the following requirements: \nME/ M. Tech degree in Structural Engineering from an institute of repute. \nShould have more than M. Tech + 7 years of experience in Bridge design preferably on existing bridges and structures (strengthening and assessments). Knowledge of design codes like Euro code is mandatory DMRB/AASHTO would be an added advantage. \nGood Knowledge of detailed design of concrete Bridges and steel composite using Lusas/Sofistik/MIDAS with Eurocodes. \nHave hands-on experience in using any of the bridge design software (LUSAS/Sofistik/MIDAS/STAAD Pro etc), \nShould be a good team member and should coordinate with other team members and the project manager for timely delivery of project \nSelf-motivated, team player and able to work independently with minimum supervision. \nFlexible attitude, in an environment with frequently changing deadlines can be relied on to meet deadlines. \nWelcome to our Transport division \nRamboll is a global transportation consultancy, and we work on some of the biggest and most innovative infrastructure projects in the world. We are close to 3,000 bright minds working within Transport worldwide, creating practical, sustainable, and economic solutions for national transport authorities, private contractors, and municipalities alike. \nRamboll in India \nRamboll has more than 1,000 experts working across five offices in India applying their passion to deliver innovative solutions in markets as diverse as Buildings, Transport, Planning & Urban Design, Water, Environment & Health, and Energy. Founded in Denmark, Ramboll is a people company. We invite you to contribute to a sustainable future working in an open, collaborative, and empowering culture. \n  Qualification\nExperience – ME/M. Tech with 7+ years of experience (preferred from IITs/NITs) \nSkills Required – Hands-on experience in detailed design of RCC & prestressed concrete and steel composite bridges based on Eurocodes of practice (Eurocode is mandatory). Exposure to working in an international environment would be preferred. Experience working with Danish bridges/Danish Annexes would be an added advantage. \nSoftware Skills - Sofistik, Midas, Lusas, basics of BIM software, computational design would be an added advantage. \nAdditional Information\nRamboll globally \nRamboll is a leading engineering, architecture, and consultancy company. Working at one of our offices in 35 countries you will join more than 16,000 fellow bright minds in creating innovative and sustainable solutions within Buildings, Transport, Energy, Environment and Health, Architecture, Landscape and Urbanism, Water and Management Consulting. Combining local experience with global knowledge, we help shape the society of tomorrow. \nAlle your information will be kept confidential according to EEO guidelines.  \nWhat we can offer you\nInvestment in your development\nLeaders you can count on, guided by our Leadership Principles\nBe valued for the unique person you are.\nNever be short of inspiration from colleagues, clients, and projects.\nThe long-term thinking of a foundation-owned company\nWe offer:\nA challenging and interesting workday characterized by continuous learning, in an environment where you have many to spar with and learn from.\nOpportunity to work with varied work tasks, across the organization.\nOpportunity to develop and influence your own area of responsibility.\nWork at the heart of sustainable change\nRamboll is a global architecture, engineering, and consultancy company. We believe that the purpose of sustainable change is to create a thriving world for both nature and people. So, that’s where we start – and how we work. At Ramboll, our core strength is our people, and our history is rooted in a clear vision of how a responsible company should act. Being open and curious is a cornerstone of our culture. We embrace an inclusive mindset that looks for fresh, diverse, and innovative perspectives. We respect, embrace, and invite diversity in all forms to actively cultivate an environment where everyone can flourish and realize their full potential.\nReady to join us?\nPlease submit your application. Be sure to include all relevant documents including your CV, cover letter, etc.\nThank you for taking the time to apply! We look forward to receiving your application.",Industry Type: IT Services & Consulting,Department: Construction & Site Engineering,"Employment Type: Full Time, Permanent","['concrete', 'danish', 'structural engineering', 'project management', 'software', 'etabs', 'bridge design', 'bim', 'staad', 'staad pro', 'midas', 'bridges', 'prestressed concrete', 'environment', 'rcc', 'java', 'lusas', 'civil engineering', 'code writing', 'design', 'international', 'structural design', 'bridge engineering']",2025-06-12 15:09:59
AI/ML,Larsen & Toubro (L&T),2 - 4 years,Not Disclosed,"['Chennai', 'Bengaluru']","Experience Required\n\n2 to 4 years of experience in AI/ML model development, deployment, and optimization. Hands-on experience in building machine learning pipelines and working with large datasets\n\nDomain Experience (Functional)\nExperience in domains such as natural language processing (NLP), computer vision, predictive analytics, or recommendation systems. Exposure to industry-specific AI applications (e.g., healthcare, finance, retail, manufacturing) is a plus.\n\nQualification\nBachelors or Masters degree in Computer Science, Artificial Intelligence, Data Science, Mathematics, or a related field\n\nRoles & Responsibilities\nDesign, develop, and deploy machine learning and deep learning models.\nCollaborate with data engineers and domain experts to collect, clean, and preprocess data.\nConduct experiments, evaluate model performance, and iterate for improvement.\nIntegrate AI models into production systems and monitor their performance.\nStay updated with the latest research and advancements in AI/ML.\nDocument model development processes and contribute to knowledge sharing.\n\nTechnical Skills\n\nProficient in Python and core ML libraries: TensorFlow, PyTorch, Scikit-learn.\nStrong with Pandas, NumPy for data handling.\nSolid grasp of ML algorithms, statistics, and model evaluation.\nFamiliar with cloud platforms (AWS/Azure/GCP).\nExperience with Git and basic CI/CD for model deployment",Industry Type: Engineering & Construction,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Npl', 'Aiml', 'Tensorflow', 'Ci/Cd', 'Machine Learning', 'Deep Learning', 'Scikit-Learn', 'Numpy', 'Pytorch', 'GCP', 'Pandas', 'Microsoft Azure', 'AWS', 'Python']",2025-06-12 15:10:01
IT Software Developer,Qualcomm,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Information Technology Group, Information Technology Group > IT Software Developer\n\nGeneral Summary:\n\nWhats in it for you""Qualcomm is enabling a world where everyone and everything can be intelligently connected. Qualcomm 5G and AI innovations are the power behind the connected intelligent edge. You will find our technologies behind and inside the innovations that deliver significant value across multiple industries and to billions of people every day.""Qualcomm engineering teams rely heavily on the latest High-Performance Computing (HPC) technologies to design and develop new products using electronic design automation (EDA) tools. This role provides an opportunity to work on the latest HPC technologies and gain experience in building scalable and fault-tolerant software solutions that are deployed on some of the largest supercomputing infrastructures across the globe.""What are we looking for""Engineering Data Analytics and Applications team (EDAAP) is looking for an experienced developer with a strong programming background. The EDAAP team is responsible for the development of software solutions enabling High Performance Compute grid and large-scale, distributed, analytical applications. They work on components and services for HPC infrastructure optimization, hardware IP management systems, petabyte-scale cloud data platforms and development of machine learning solutions and pipelines.""This role involves designing and developing high-quality software solutions to manage compute environments, including compute grids, EDA licenses, storage, data synchronization, and IT infrastructure. The ideal candidate is an experienced software developer skilled in multiple programming languages and frameworks, parallel programming, efficient algorithms and data structures, design patterns, task automation through tools and scripting.What will you do""This roles responsibilities include:"". Design and develop high-quality software using suitable programming languages and frameworks for HPC infrastructure.. Create reusable components and libraries for future use.. Continuously monitor and upgrade existing applications to ensure they meet current standards and requirements.. Develop and maintain technical documentation to guide future software development projects.. Perform comprehensive code reviews to ensure quality, maintainability, and adherence to best pr. actices.. Collaborate with internal teams to resolve issues by leveraging expertise from various departments.What do we want to see""The ideal candidate will be able to demonstrate some of the following skills:"". Over 2 years of hands-on experience in full stack development.. Proficient in programming languages and frameworks such as Python, Java/J2EE and Angular.. Expertise in using relational databases (PostgreSQL, MySQL) and familiarity with anyone of NoSQL databases (Redis, MongoDB).. Expertise in software lifecycle management, version control, coding, and CI/CD best practices to ensure quality, agility, and security.. Exposure to AI and ML technologies is a plus.. Ability to clearly explain technical concepts and analysis implications to a diverse audience.. Team-oriented, with a strong inclination to work collaboratively.. Bachelors or Masters degree in Computer Science, Computational Science, or a related field.\n\nMinimum Qualifications:\nBachelor's degree in a technical field (e.g., Computer Engineering, Computer Science, Information Systems).\nOR\nBachelors degree in a non-technical field (e.g., Business, Humanities, Marketing) and 1+ year of IT-relevant experience.\nOR\nHigh School Diploma or equivalent and 3+ years of IT-relevant experience.\n\n2+ years of any combination of academic or work experience with programming (e.g., Java, Python, etc.).\n1+ year of any combination of academic or work experience with Data Structures, algorithms, and data stores.\nBachelors / Masters or equivalent degree in computer engineering or in equivalent stream",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'version control', 'ci/cd', 'relational databases', 'data structures', 'algorithms', 'fullstack development', 'python', 'software development', 'redis', 'artificial intelligence', 'nosql', 'angular', 'java', 'postgresql', 'computer science', 'j2ee', 'mysql', 'mongodb', 'ml']",2025-06-12 15:10:03
Python Developer Lead @ Infosys- PAN INDIA,Infosys,3 - 8 years,Not Disclosed,"['Kolkata', 'Pune', 'Delhi / NCR']","Responsibilities A day in the life of an Infoscion • As part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain. • You will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements. • You will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers. • You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nTechnical and Professional Requirements: • Primary skills:Process->Testing processes->Test Automation Process,Technology->Machine Learning->Python\n\nPreferred Skills: Process->Testing processes->Test Automation Process Technology->Machine Learning->Python\n\nAdditional Responsibilities: • Knowledge of design principles and fundamentals of architecture • Understanding of performance engineering • Knowledge of quality processes and estimation techniques • Basic understanding of project domain • Ability to translate functional / nonfunctional requirements to systems requirements • Ability to design and code complex programs • Ability to write test cases and scenarios based on the specifications • Good understanding of SDLC and agile methodologies • Awareness of latest technologies and trends • Logical thinking and problem solving skills along with an ability to collaborate\n\nEducational RequirementsMCA,MSc,MTech,Bachelor of Engineering,BCA,BE,BSc,BTech",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Django', 'Python Development', 'Flask']",2025-06-12 15:10:05
Early Career Professional-E,Conduent,2 - 6 years,Not Disclosed,['Bengaluru'],"Join Us \nIf you are seeking an opportunity to make a real impact in a company that appreciates ideas and new ways of thinking, come join us and grow with a team of people who will challenge and inspire you to be the best!\nAbout the Role\nThis role is designed as an entry-level position for applicants with strong skill sets in programming (logical reasoning, analytical skills), systems configuration and testing related to systems that support different business functions.\n : \nBachelor\\u2019s degree in computer science, Information Technology, or a related field from a reputable institution with 7 CGPA and Above. Strong foundational knowledge of computer science principles and programming concepts.\nProficiency in at least one programming language, such as Java, Python, .Net, C, JavaScript or SQL.\nFamiliarity with software development lifecycle (SDLC) methodologies and best practices. Ability to quickly learn and adapt to new technologies and tools.\nGood understanding of data structures, algorithms, and object-oriented design principles. Excellent problem-solving skills with keen attention to detail.\nAbility to work effectively in a fast-paced, collaborative environment.\nDemonstrated ability to work independently and take initiative to complete tasks and solve problems.\nCommitment to continuous learning and professional development.\nGood to Have:\nFamiliarity with Oracle technologies, Like SQL, PLSQL, Reports, Shell Scripts Knowledge of database concepts and experience with SQL. Familiarity of REST, SOAP etc.\nUnderstanding of fundamental of cloud computing concepts Exposure to DevOps practices and tools, such as Git, Jenkins, Docker, and Kubernetes.\nUnderstanding of Artificial Intelligence & Machine Learning concepts\nParticipation in relevant internships, co-op programs, or personal projects demonstrating practical experience and initiative.\nGREAT\n OPPORTUNITY FOR FRESHERS \n Technical\n\nSkills:\n \nKnowledge of Software Development Life Cycle (SDLC) principles/concepts.\nKnowledge in Simple & Complex SQL Queries\nWrite intermediate SQL Queries\nCommunication and Excellence:\nExcellent logical and communication skills (Oral, written and listening ability)\nStrong communication and interpersonal skills, with the ability to effectively communicate.\n  \nConduent delivers mission-critical services and solutions on behalf of businesses and governments creating exceptional outcomes for its clients and the millions of people who count on them. Through process, technology, and our diverse and dedicated associates, Conduent solutions and services automate workflows, improve efficiencies, reduce costs, and enable revenue growth. It\\u2019s why most Fortune 100 companies and over 500 government entities depend on Conduent every day to manage their essential interactions and move their operations forward.\nConduent\\u2019s differentiated services and solutions improve experiences for millions of people every day, including 3 out of every 4 U.S. insured patients, 10 million employees who use its HR Services, and nearly 18 million benefits recipients. Conduent\\u2019s solutions deliver exceptional outcomes for its clients including :1c billion in savings from medical bill review of workers compensation claims, up to 40% efficiency increase in HR operations, up to 27% reduction in government benefits costs, up to 40% improvement in finance, accounting and procurement expense, and improved customer service interaction times by up to 20% with higher end-user satisfaction. Learn more at https://www.conduent.com",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'javascript', 'sql', 'java', 'computer science', 'rest', 'algorithms', 'kubernetes', 'oracle', 'c', 'sql queries', 'machine learning', 'artificial intelligence', 'docker', 'plsql', 'software development life cycle', 'git', 'devops', 'jenkins', '.net', 'data structures', 'shell scripting', 'sdlc', 'soap']",2025-06-12 15:10:08
Architect (AI and Cloud),Rakuten Symphony,10 - 18 years,Not Disclosed,['Bengaluru( Kadubeesanahalli )'],"Why should you choose us? Rakuten Symphony is reimagining telecom, changing supply chain norms and disrupting outmoded thinking that threatens the industrys pursuit of rapid innovation and growth. Based on proven modern infrastructure practices, its open interface platforms make it possible to launch and operate advanced mobile services in a fraction of the time and cost of conventional approaches, with no compromise to network quality or security. Rakuten Symphony has operations in Japan, the United States, Singapore, India, South Korea, Europe, and the Middle East Africa region. For more information, visit: https://symphony.rakuten.com\n\nBuilding on the technology Rakuten used to launch Japans newest mobile network, we are taking our mobile offering global. To support our ambitions to provide an innovative cloud-native telco platform for our customers, Rakuten Symphony is looking to recruit and develop top talent from around the globe. We are looking for individuals to join our team across all functional areas of our business from sales to engineering, support functions to product development. Lets build the future of mobile telecommunications together!\n\nAbout Rakuten Rakuten Group, Inc. (TSE: 4755) is a global leader in internet services that empower individuals, communities, businesses and society. Founded in Tokyo in 1997 as an online marketplace, Rakuten has expanded to offer services in ecommerce, fintech, digital content and communications to approximately 1.5 billion members around the world. The Rakuten Group has over 27,000 employees, and operations in 30 countries and regions. For more information visit https://global.rakuten.com/corp/\n\nJob Summary:\nThe AI Architect is a senior technical leader responsible for designing and implementing the overall AI infrastructure and architecture for the organization. This role will define the technical vision for AI initiatives, select appropriate technologies and platforms, and ensure that AI systems are scalable, reliable, secure, and aligned with business requirements. The AI Architect will work closely with CTO Office, product manager, engineering manager, data scientists, machine learning engineers, and other stakeholders to build a robust and efficient AI ecosystem.\n\nMandatory Skills:\nCloud Computing Platforms (AWS, Azure, GCP).\nAI/ML Frameworks (TensorFlow, PyTorch, scikit-learn) .\nData Engineering Tools (Spark, Hadoop, Kafka).\nMicroservices Architecture.\nAI/ML as a service Deployment.\nDevOps Principles (CI/CD/CT).\nStrong understanding of AI/ML algorithms and techniques\n\nRoles & Responsibilities:\nDefine the overall AI architecture and infrastructure strategy for the organization.\nSelect appropriate technologies and platforms for AI development and deployment. • Design scalable, reliable, and secure AI systems.\nDevelop and maintain architectural blueprints and documentation.\nProvide technical leadership and guidance to tech lead, engineering manager, data scientists, machine learning engineers, and other stakeholders.\nEnsure that AI systems are aligned with business requirements and industry best practices. Evaluate new AI technologies and trends.\nCollaborate with security and compliance teams to ensure that AI systems meet regulatory requirements.\nCollaborate with CTO Office to ensure the AI strategy implemented aligned with overall business unit strategy.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Architect', 'Artificial Intelligence', 'Cloud', 'Microservice Based Architecture', 'Tensorflow', 'Pytorch', 'Ai', 'Machine Learning', 'Solution Architect', 'Scikit-Learn', 'Technical Architecture']",2025-06-12 15:10:10
Analytics Manager,Flipkart,8 - 12 years,Not Disclosed,['Bengaluru'],"Skills Required :\nStrong background in statistical modelling and experience with machine learning / data mining tools such as R, Python, SQL, Spark, SAS, Excel. High expertise in implementing machine learning and AI models\nEducation/Qualification :\nBachelors in Engineering, Computer Science, Math, Statistics, or related discipline from a reputed institute or an MBA from a reputed institute\nDesirable Skills :",,,,"['Analytics', 'R', 'Excel', 'Power BI', 'SAS', 'Qlikview', 'Tableau', 'Datastudio', 'Spark', 'Python', 'SQL']",2025-06-12 15:10:13
Development Manager AI Center of Excellence,IBM,12 - 15 years,Not Disclosed,['Bengaluru'],"Your Role & Responsibilities:\nLooking to make a significant impact\nThis is your chance to become a key part of a dynamic team of talented professionals, leading the development and deployment of innovative, industry-leading, cloud-based AI services.\nWe are seeking an experienced AI & Cloud Software Engineering Manager to join us. This leadership role focuses on guiding, mentoring, and managing a team of engineers in designing, developing, and deploying AI-based services. You will be instrumental in problem-solving, automating wide ranges of tasks, and interfacing with other teams, offering managers, and customers to solve complex problems.\nYou will build a strong, agile, and modern team culture aimed at creating world-class development and deployment environments. Your efforts will ensure industry-leading user experiences for our customers. As an essential part of the leadership team, you will contribute to the cloud services architecture and design while mentoring the next generation of cloud engineers.\nResponsibilities:\nLead, mentor, and manage a team of AI and cloud software engineers.\nOversee the planning, execution, and delivery of AI-based services and cloud solutions.\nProvide technical direction and architectural oversight for AI and cloud projects.\nInterface with other teams, managers, and customers to align project goals and deliverables.\nAddress and resolve technical challenges and roadblocks faced by the team.\nMonitor and evaluate team performance, providing feedback and development opportunities.\nStay updated with the latest trends in AI and cloud technologies to encourage continuous improvement.\nEnsure the delivery of industry-leading user experiences for customers.\nAllocate resources effectively to meet project and organizational goals.\nMaintain comprehensive documentation of projects and processes, and report progress to senior management.\nRequired education\nBachelor's Degree\nRequired technical and professional expertise\nMinimum 12-15 years of experience as Full Stack Developer with a focus on AI projects\nExperience with AI and machine learning frameworks such as scikit-learn, TensorFlow, PyTorch, LLMs, Generative AI.\nFamiliarity with AI model deployment and integration.\nSolid understanding of backend technologies, including server-side languages (Node.js, Python, Java, etc.) and databases (Cassandra, PostgreSQL, etc.).\nUnderstanding and experience with RESTful APIs, Java/J2EE, Kafka & GitHub.\nStrong experience with Cloud Technologies, Kubernetes based microservices architecture, Kafka, Object Storage, Cassandra database and docker container technologies. Knowledge on IBM Cloud Technologies will be an added advantage.\nAt least 6 years of hands-on development experience building applications with one or more of the following: Java, Spring, Liberty, Node.js, Express.js, Golang, NoSQL DB, Redis, distributed caches, containers etc.,\nAt least 3 years of experience in building and operating highly secured, distributed cloud services with one or more of the following: IBM Cloud, AWS, Azure, SRE, CI/CD, Docker, Container orchestration, performance testing, etc.,\nAt least 3 years of experience in web technologies: HTTP, REST, JSON, HTML, Ajax, JavaScript etc.,\nSolid understanding of the micro-services architecture and modern cloud programming practices. Strong ability to design a clean, developer-friendly API.\nPassionate about constant, continuous learning and applying new technologies as well as mentoring others.\nKeen troubleshooting skills and strong verbal/written communication skills.\nPreferred technical and professional experience\nPreferred Skills:\nExperience in using messaging brokers like RabbitMQ, Kafka etc.\nOperating Systems (such as Red Hat, Ubuntu, etc.)\nKnowledge of network protocols such as TCP/IP, HTTP, etc.\nExperience and working knowledge of version Control systems like Github and build tools like Maven/Gradle\nAbility to learn and apply new technologies quickly\nExperience in working on a SaaS application with high industry standard CI/CD, and development cycle processes\nStrong sense of ownership of deliverables\nUI test automation skills - Selenium and/or Puppeteer\nBeyond the requirements, candidates should be passionate about in the role:\nContinuous learning and ability to adapt to change\nWorking across global teams and collaborating across teams and organization boundaries\nFinding innovative ways to solve complex problems with cutting edge technologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Artificial intelligence', 'RESTful API', 'Java', 'Generative AI', 'scikit-learn', 'Kafka', 'J2EE', 'Node.js', 'LLM', 'machine learning', 'PyTorch', 'TensorFlow', 'Python']",2025-06-12 15:10:16
IN Senior Associate AWS AI/ML/GenAI Developer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\n& Summary We are looking for a seasoned AWS\nAI/ML/GenAI Developer\nResponsibilities\nDesign and implement AI/ML/GenAI models using AWS services such as AWS Bedrock, SageMaker, Comprehend, Rekognition, and others.\nStrong programming skills in Python, R etc\nExperience with machine learning frameworks such as TensorFlow, PyTorch, or Scikitlearn.\nKnowledge of data preprocessing, feature engineering, and model evaluation techniques.\nDevelop and deploy generative AI solutions to solve complex business problems and improve operational efficiency.\nCollaborate with data scientists, engineers, and product teams to understand requirements and translate them into technical solutions.\nOptimize and finetune machine learning models for performance and scalability. Ensure the security, reliability, and scalability of AI/ML solutions by adhering to best practices.\nMaintain and update existing AI/ML models to ensure they meet evolving business needs.\nStay uptodate with the latest advancements in AI/ML and GenAI technologies and integrate relevant innovations into our solutions.\nProvide technical guidance and mentorship to junior developers and team members.\nExcellent problemsolving skills and ability to work in a fastpaced, collaborative environment.\nGood to have AWS Certified Machine Learning Specialty or other relevant AWS certifications.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired 48 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Bachelor of Engineering, Bachelor of Technology\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'GCP', 'Data modeling', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Corporate advisory', 'Operations', 'AWS']",2025-06-12 15:10:18
IN Senior Associate AWS AI/ML/GenAI Developer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\n& Summary We are looking for a seasoned AWS\nAI/ML/GenAI Developer\nResponsibilities\nDesign and implement AI/ML/GenAI models using AWS services such as AWS Bedrock, SageMaker, Comprehend, Rekognition, and others.\nStrong programming skills in Python, R etc\nExperience with machine learning frameworks such as TensorFlow, PyTorch, or Scikitlearn.\nKnowledge of data preprocessing, feature engineering, and model evaluation techniques.\nDevelop and deploy generative AI solutions to solve complex business problems and improve operational efficiency.\nCollaborate with data scientists, engineers, and product teams to understand requirements and translate them into technical solutions.\nOptimize and finetune machine learning models for performance and scalability. Ensure the security, reliability, and scalability of AI/ML solutions by adhering to best practices.\nMaintain and update existing AI/ML models to ensure they meet evolving business needs.\nStay uptodate with the latest advancements in AI/ML and GenAI technologies and integrate relevant innovations into our solutions.\nProvide technical guidance and mentorship to junior developers and team members.\nExcellent problemsolving skills and ability to work in a fastpaced, collaborative environment.\nGood to have AWS Certified Machine Learning Specialty or other relevant AWS certifications.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired 48 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'GCP', 'Data modeling', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Corporate advisory', 'Operations', 'AWS']",2025-06-12 15:10:20
Senior AI/ML Manager,IQVIA,9 - 14 years,Not Disclosed,"['Kochi', 'Pune', 'Bengaluru']","Project Role: Senior AI/ML Manager\nWork Experience: 8 to 14 Years\nWork location: Bengaluru/Kochi\nMust Have Skills: Machine Learning, NLP, Deep Learning, AI, Python, GenAI.\nJob Description\nDevelop/leverage fit for purpose AIML models/algorithms/foundation models/processes to address pharma/healthcare applications and innovative products upon completion of prototypes followed by the building of production grade algorithms/automation engines for client deliverables. Test for viability to deliver final products to clients. Able to bring newly researched ideas to reality quickly and on a large scale. Design, build, test, and deliver products from post-prototype to client delivery.\n\nEssential Functions\nAssists with the ongoing development and implementation of an enterprise architecture. May devise and present business cases and program release plans to senior management with priority recommendations to maintain and evolve this architecture.\nBuilds effective business relationships with business line managers and provides technical and system expertise as input to product concepts.\nMay assist product development management to define IT strategic direction and assists in the mapping of projects to that strategic direction whilst ensuring product capabilities and process improvements are delivered over time within the framework of the IQVIA enterprise architecture.\nParticipates in cross-functional product development teams, may also act as a consultant to provide system and technical advice.\nKeeps up to date with technology changes and identifies opportunities for implementation in future systems.\nParticipates in R&D projects and may run those projects in compliance with standard project management practices.\nMay mentor and assist lower-level architects and business analysts.\n\nQualifications\n10+ years of experience in engineering roles with team management experience\n6-8 relevant years experience on NLP,GenAI/LLM, Machine Learning and Deep learning\n6-8 relevant years of experience on Python\nExtensively work on NLP applications, ability to work on Machine learning model, Deep learning development\nExtensive knowledge in leveraging GenAI/LLM for developing innovative solutions to complex business problems.\nSolid understanding of LLMs, including fine-tuning methodologies and deployment strategies.\nExperience on text to transform natural language into useful features.\nFind and implement the right algorithms and tools for NLP tasks.\nExtend ML libraries and frameworks to apply in NLP tasks.\nDemonstrated experience in writing effective, scalable, and maintainable code\nExperience on Writing on effective and scalable code\nExperience on Unit Testing using Pytest or equivalent framework.\nA deep understanding and multi-process architecture and the threading limitations of Python.\nMust have experience working with REST APIs, and frameworks like Flask API and Fast API. Experience with Django or Pyramid framework is a good to have\nKnowledge of NER, Knowledge graphs is good to have.\n\nEducation Qualification\nMaster's Degree masters degree in Machine Learning, Statistics, Computer Science, Physics, Math, or related field",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Natural Language Processing', 'Aiml', 'Machine Learning', 'Python', 'Agentic AI', 'Genrative Ai', 'Deep Learning']",2025-06-12 15:10:23
AI Technical Architect,Care Allianz,7 - 11 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Care Allianz is looking for AI Technical Architect_ to join our dynamic team and embark on a rewarding career journey\n\nDesigns AI-based system architectures for scalable solutions\n\nCollaborates with data scientists and engineers for model integration\n\nEnsures performance, scalability, and security of AI platforms\n\nGuides development teams in implementing AI strategies",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Technical Architect', 'Manager Technology']",2025-06-12 15:10:25
RAG Architect,Qualcomm,13 - 18 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Test Engineering\n\nGeneral Summary:\n\nJob description\n\nWe are seeking an experienced AI Architect to design, develop, and deploy Retrieval-Augmented Generation (RAG) solutions for Qualcomm Cloud AI Platforms.\n\nRoles and Responsibilities\nLead the design and development of applications for RAG AI models and provide APIs for frontend consumption. Manage the interaction between retrieval-augmented techniques and generative models.\nBuild services that connect AI models (e.g., transformers, embeddings, and vector search) to handle tasks such as query retrieval, model inference, and generating responses. Leverage frameworks like Flask, FastAPI, or Django for API development.\nDesign pipelines to preprocess, clean, and prepare data for AI model training, as well as for serving the models in production environments. Optimize these pipelines to support both batch and real-time data processing. Implement RESTful APIs or GraphQL endpoints for seamless frontend-backend interaction.\nImplement cloud solutions to host Python-based services, ensuring that AI models are scalable and that the infrastructure can handle high traffic. Leverage containerization (Docker) and orchestration (Kubernetes) for model deployment and management.\nSet up monitoring, logging, and alerting for Python backend services, ensuring smooth operation of AI features. Use tools like Prometheus, Grafana, and ELK stack for real-time performance tracking.\nContinuously optimize model performance by fine-tuning and adapting Python-based AI models for real-time use cases. Manage trade-offs between computation load, response time, and quality of generated content.\nPartner with data scientists, machine learning engineers, and mobile/web developers to ensure tight integration between AI models, mobile/web front-end, and backend infrastructure.\n\n- Experience:\n13+ years of overall SW development experience\n10+ years Strong experience in working with technologies (e.g., React, React Native, Flutter, Django, Flask, FastAPI).\n5+ years of experience in building AI applications with a focus on NLP, machine learning, generative models, and retrieval-augmented systems.\nProven experience in designing and deploying AI systems that integrate retrieval-based techniques (e.g., FAISS, Weaviate) and generative models (e.g., GPT, BERT). - Expertise in cloud platforms (e.g., AWS, GCP, Azure) and deployment of Python-based microservices.\nBuilding RESTful APIs or GraphQL services (using frameworks like Flask, FastAPI, or Django).\nHandling AI model inference and data processing (using libraries like NumPy, Pandas, TensorFlow, PyTorch, and Hugging Face Transformers).\nIntegrating vector search solutions (e.g., FAISS, Pinecone, Weaviate) with the AI models for efficient retrieval-augmented generation. - Experience with containerization (Docker) and Kubernetes for deploying scalable Python-based services.\nProficient in cloud infrastructure management, with a focus on managing Python services in the cloud.\nExperience in End-to-End product development and Software Lifecycle\n\n\nKey\n\nSkills:\n\nAdvanced proficiency in Python for building backend services and data processing pipelines. Familiarity with frameworks like Flask, Django, and FastAPI. Experience with AI libraries and frameworks (TensorFlow, PyTorch, Hugging Face Transformers).\nFamiliarity with vector databases (e.g., Pinecone, FAISS, Weaviate) and integration with retrieval-augmented systems.\nStrong knowledge of RESTful API design, GraphQL, and API security best practices (e.g., OAuth, JWT).\nExcellent problem-solving abilities and a strong focus on creating highly scalable and performant solutions.\nStrong communication skills, with the ability to collaborate across different teams and geography\nAbility to mentor junior team members and lead technical discussions.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 6+ years of Software Test Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 5+ years of Software Test Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Test Engineering or related work experience.\n\n2+ year of work experience with Software Test or System Test, developing and automating test plans, and/or tools (e.g., Source Code Control Systems, Continuous Integration Tools, and Bug Tracking Tools).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'cloud platforms', 'api', 'graphql', 'natural language processing', 's w development', 'rest api design', 'system testing', 'react native', 'machine learning', 'pipeline', 'react.js', 'flutter', 'test engineering', 'django', 'cloud infrastructure management', 'flask']",2025-06-12 15:10:27
"Sr Software Eng: Generative AI, Go/Python, AWS, Kubernetes 7-12 Yrs",Cisco,7 - 12 years,Not Disclosed,['Bengaluru'],"Meet The Team\nThe Cisco AI Software & Platform Group drives the development of groundbreaking generative AI applications. We empower Cisco's diverse product portfolio, spanning networking and security, with intelligent assistants and agents. We work on pioneering technologies that proactively defend against threats, safeguard critical business assets, and simplify security operations. Fueled by a passion for AI/ML, we strive to create a secure future for businesses. Our collaborative and passionate team thrives with tackling sophisticated challenges and delivering innovative solutions.",,,,"['Golang', 'Generative Ai', 'AWS', 'Python', 'Kubernetes', 'Java']",2025-06-12 15:10:30
Senior AI/ML Architect- Iqvia,IQVIA,10 - 18 years,Not Disclosed,"['Kochi', 'Noida', 'Bengaluru']","Job Description\nDevelop fit for purpose AIML models/algorithms/processes to address pharma/healthcare applications and innovative products upon completion of prototypes followed by the building of production grade algorithms/automation engines for client deliverables. Test for viability in order to deliver final products to clients. Able to bring newly researched ideas to reality quickly and on a large scale. Design, build, test, and deliver products from post-prototype to client delivery.\nEssential Functions\nAssists with the ongoing development and implementation of an enterprise architecture.\nMay devise and present business cases and program release plans to senior management with priority recommendations to maintain and evolve this architecture. Builds effective business relationships with business line managers and provides technical and system expertise as input to product concepts.\nMay assist product development management to define IT strategic direction and assists in the mapping of projects to that strategic direction whilst ensuring product capabilities and process improvements are delivered over time within the framework of the IQVIA enterprise architecture.\nParticipates in cross-functional product development teams, may also act as a consultant to provide system and technical advice.\nKeeps up to date with technology changes and identifies opportunities for implementation in future systems.\nParticipates in R&D projects and may run those projects in compliance with standard project management practices.\nMay mentor and assist lower level architects and business analysts\nQualifications\n10+ years of experience in engineering roles with team management experience\n5-7 relevant years experience on NLP, Machine Learning and Deep learning\n5-7 relevant years of experience on Python\nExtensively work on NLP applications, ability to work on Machine learning model, Deep learning development\nKnowledge of LLMs, fine tuning and deployment\nExperience annotated datasets for Supervised Learning methods and correction\nExperience on text to transform natural language into useful features\nFind and implement the right algorithms and tools for NLP tasks\nPerform statistical analysis of results and refine models\nExtend ML libraries and frameworks to apply in NLP tasks\nExperience on ORM, SQL Alchemy, Alembic is a must\nExperience on Writing on effective and scalable code\nExperience on Unit Testing using Pytest or equivalent framework\nA deep understanding and multi-process architecture and the threading limitations of Python.\nFamiliarity with server-side templating languages including Jinja 2 and Mako is good to have\nExperience on REST API, Flask API, Fast API is must, Django or Pyramid framework is good to have\nKnowledge of NER, Knowledge graphs is good to have\n\nEducation Qualification\nMaster's Degree Masters Degree in Machine Learning, Statistics, Computer Science, Physics, Math, or related field",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['AI', 'Machine Learning', 'Python', 'GenAI', 'NLP', 'Natural Language Processing', 'Ml']",2025-06-12 15:10:32
Lead AI Engineer,Insnapsys Technologies,4 - 8 years,Not Disclosed,[],"Lead AI Engineer: Nashik/Remote\nExperience: 7+ years in Software Engineering and 3+years in AI/ML development\n\nWhat you will do:\nLead the end-to-end design, development, and deployment of AI/ML systems and generative AI applications.\nBuild & fine-tune custom AI agents, chatbots, and automation tools.\nArchitect scalable pipelines for LLMs optimized for performance and cost-efficiency.\nImplement advanced techniques like RAG, vector search, and hybrid architectures.\nCollaborate with cross-functional Product, Engineering, and Data teams.\nEstablish MLOps infrastructure (monitoring, CI/CD, versioning, A/B testing).\nMentor and inspire a high-performing AI team.\nChampion ethical, scalable, and secure AI development practices\n\n\nWhat you Bring\n7+ years in software engineering or data science, with 3+ years in AI/ML development.\n3+ years leading AI teams and managing end-to-end projects.\nHands-on experience with LLMs like GPT, Claude, Mistral, Falcon including fine-tuning, RAG, and agent orchestration.\nStrong command over Python, PyTorch/TensorFlow, HuggingFace.\nExperience on AWS: SageMaker, Bedrock, Lambda, EC2, S3.\nKnowledge of vector databases: Pinecone, FAISS, or Weaviate.\nSystems-thinking mindset with great communication & leadership skills.\n\n\nNice to Have\nExperience with voice AI, OCR, or computer vision.\nFamiliarity with multi-agent systems, autonomous planning, or tool-using AI agents.\nContributions to open-source projects or published research in NLP, LLMs, or GenAI.\nExperience with real-time personalization, streaming data, or recommendation systems.\n\nEducation Required:\nB.Tech / M.Tech / Ph.D. in Computer Science, Artificial Intelligence, Machine Learning, or a related field from a reputed institute.\n\nWhy Join Us?\nBuild AI systems that scale and make real-world impact across industries.\nWork in a high-autonomy, innovation-driven environment using cutting-edge tools.\nJoin a team that thrives on collaboration, learning, and technical excellence.\nBe part of a mission-driven organization building ethical, secure, and transformative AI.\n\nInterested candidates please submit resumes on hr@insnapsys.com or call on 9156797671",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'LLM', 'Machine Learning', 'Python']",2025-06-12 15:10:34
"Senior Java Developer (Microservices, Emerging Technologies & Cloud)",Synechron,5 - 10 years,Not Disclosed,['Bengaluru'],"job requisition idJR1027430\n\nOverall Responsibilities:\nLead the development and implementation of projects using emerging technologies\nMentor and guide team members to ensure the successful delivery of projects\nIdentify and evaluate new technology solutions to improve business processes\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nStay up-to-date with the latest technological advancements and industry trends\n\n\nSkills:\nStrong expertise in emerging technologies such as blockchain, IoT, AI, etc.\nStrong technical knowledge of software development lifecycle\nExcellent problem-solving and critical thinking skills\nGood understanding of software architecture and design patterns\nAbility to lead and manage a team of technical experts\nExperience:\nAt least 5+ years of experience in software development and leading technology projects\nProven track record of delivering projects using emerging technologies\nExperience in mentoring and guiding junior team members\nExperience in working with cross-functional teams\nDay-to-Day Activities:\nManage the development and delivery of projects using emerging technologies\nProvide technical guidance and mentorship to junior team members\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nEvaluate and recommend new technology solutions to improve business processes\nStay up-to-date with the latest technological advancements and industry trends\nQualification:\nBachelor's or Master's degree in Computer Science, Information Technology, or related field\nRelevant certifications in emerging technologies\nSoft\n\nSkills:\nStrong communication and leadership skills\nAbility to work well under pressure and meet tight deadlines\nExcellent interpersonal and team-working skills\nAbility to effectively communicate technical information to non-technical stakeholders\nPassionate about technology and a desire to stay up-to-date with the latest advancements.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['emerging technologies', 'artificial intelligence', 'iot', 'software development life cycle', 'blockchain', 'software development', 'information technology', 'technology solutions', 'microservices', 'java', 'design patterns', 'project delivery', 'leadership development']",2025-06-12 15:10:36
CPU Full Stack Python Developer (Staff/Sr. Staff),Qualcomm,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nWe are seeking a highly skilled Full Stack Python Developer to join our dynamic team. The ideal candidate should have a strong background in tool development, data science, and automation of complex tasks. You will be responsible for developing high volume regression dashboard, parametric and power tools and contributing to both front-end and back-end development.\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 4+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 3+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 2+ years of Hardware Engineering or related work experience.\n\nTechnical\n\nSkills:\n\n\n\nPythonProficiency in Python programming, including libraries like Pandas, NumPy, and SciPy for data science.\n\n\nFull Stack DevelopmentExperience with both front-end (HTML, CSS, JavaScript, React, Vue.js) and back-end (Django, Flask) technologies.\n\n\nTool DevelopmentAbility to develop parametric and power tools, possibly using frameworks like Vue.js , PyQt or Tkinter for GUI development.\n\n\nData ScienceStrong understanding of data analysis, machine learning (using libraries like scikit-learn, TensorFlow), and data visualization (using Matplotlib, Seaborn).\n\n\nAutomationExperience in automating complex tasks using scripting and tools like Selenium, Airflow, or custom automation scripts.\n\n\nSoft\n\nSkills:\n\n\n\nProblem-SolvingAbility to tackle complex problems and develop innovative solutions.\n\n\nCommunicationStrong communication skills to effectively collaborate with team members and stakeholders.\n\n\nAdaptabilityFlexibility to adapt to new technologies and methodologies.\n\n\nExperience:\n\n\nProjectsPrevious experience in developing tools and automation solutions.\n\n\nIndustry KnowledgeFamiliarity with the specific industry or domain you're working in can be a plus.\n\n\nKey Responsibilities:\n\nDevelop and maintain parametric and power tools using Python.\n\nDesign and implement automation solutions for complex tasks.\n\nCollaborate with data scientists to analyze and visualize data.\n\nBuild and maintain web applications using Django or Flask.\n\nDevelop front-end components using HTML, CSS, JavaScript, and React.\n\nIntegrate third-party APIs and services.\n\nOptimize applications for maximum speed and scalability.\n\nWrite clean, maintainable, and efficient code.\n\nTroubleshoot and debug applications.\n\nStay updated with the latest industry trends and technologies.\n\n\nPreferred Qualifications:\n\nBachelor's degree in Computer Science, Engineering, or related field.\n\nPrevious experience in tool development and automation.\n\nFamiliarity with industry-specific tools and technologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['tool development', 'data science', 'python', 'data analysis', 'machine learning', 'css', 'hiring', 'scikit-learn', 'vue.js', 'numpy', 'staffing', 'react.js', 'tensorflow', 'seaborn', 'selenium', 'pyqt', 'html', 'data visualization', 'scipy', 'hardware engineering', 'javascript', 'pandas', 'django', 'matplotlib', 'flask']",2025-06-12 15:10:39
Principal Engineer App (React Native),Endeavour Consultancy Services And Solutions,8 - 13 years,45-65 Lacs P.A.,['Bengaluru'],"Led architecture and development of high-performance React Native apps. Proficient in native build tools like Xcode and Gradle along with state management (Redux, MobX) and advanced libraries (React Navigation, Reanimated).",Industry Type: Internet (E-Commerce),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['React Native', 'Mobile App', 'Xcode', 'Redux', 'react native apps', 'CI/CD', 'Gradle', 'MobX']",2025-06-12 15:10:41
Lead : Sales & Biz Dev (AI Tools),DNEG,5 - 10 years,Not Disclosed,['Mumbai (All Areas)( Santacruz West )'],"Role Overview:\nAs the Sales & Business Development Lead for the Media vertical, you will spearhead revenue growth by building and nurturing relationships with key stakeholders in the media ecosystem. Youll work closely with technology, product and marketing teams to bring powerful AI solutions to market, while also uncovering new business opportunities that align with evolving customer needs.\n\nKey Responsibilities:\n\nBusiness Development & Partnerships\nIdentify and cultivate new business opportunities within the media, entertainment, and publishing sectors\nBuild and maintain strategic relationships with decision-makers at broadcasters, studios, agencies, and streaming services\nLead negotiations, proposals, and contract processes for key deals and partnerships\n\nSales Strategy & Execution\nOwn the complete sales cycle from lead generation to closing across multiple media segments\nDevelop a clear go-to-market strategy tailored to the unique dynamics of the media and entertainment industries\nMaintain accurate sales forecasting and pipeline management via CRM tools\n\nIndustry Expertise & Evangelism\nRepresent the company at industry events, conferences, and client meetings\nStay ahead of media technology trends and position the company as a thought leader in AI-powered media innovation\nEducate prospects and partners on the ROI and impact of AI in creative workflows, content monetization, and audience engagement\n\nCross-functional Collaboration\nWork closely with product, marketing, and technology teams to align customer needs with platform capabilities\nProvide feedback from the field to inform product development and roadmap priorities\n\nRequirements:\n5-8 years of experience in B2B sales, business development, or partnerships ideally within the media, entertainment, or advertising sectors\nProven track record of closing complex deals with media buyers, tech vendors, or content producers\nStrong understanding of AI applications in mediasuch as generative AI, video/audio analysis, content recommendation, or digital asset management\nExcellent communication, negotiation, and relationship-building skills\nComfortable working in a fast-paced, evolving startup environment\nBachelors degree in business, marketing, communications, or a related field; MBA or technical background is a plus\n\nNice to Have:\nFamiliarity with media technology stacks (e.g., MAM, DAM, CMS, OTT platforms)\nExperience working with media/ movie agencies, ad tech firms, or global media networks\nKnowledge of AI/ML concepts (NLP, computer vision, LLMs, etc.)",Industry Type: Emerging Technologies (AI/ML),Department: Sales & Business Development,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'New Market Development', 'Sales Lead Generation']",2025-06-12 15:10:43
Engineering Lead python and Gen AI Mumabi Andheri West with startup,Intelli Search Services,12 - 18 years,40-70 Lacs P.A.,['Mumbai (All Areas)'],"Job Title: Engineering Lead\nLocation: Mumbai Andheri West\n\nJob Overview:\nAs the Engineering Lead, you will be responsible for leading the end-to-end development of our Generative AI products, ensuring scalability, performance, and innovation. You will drive engineering excellence, collaborate with cross-functional teams, and build a high-performing engineering culture. This role requires deep expertise in AI/ML, Python Coding ,distributed systems, cloud architectures, and modern software engineering practices. Needs hands on coding experience\n\nKey Responsibilities:\n1. Technical Leadership & Strategy: Define and execute the technology roadmap for Generative AI products, ensuring alignment with business goals.\n2. AI/ML Product Development: Lead the development of AI-powered products, optimizing models for performance, scalability, and real-world application.\n3. Engineering Excellence: Establish best practices in software development, DevOps, MLOps, and cloud-native architectures.\n4. Team Leadership & Scaling: Recruit, mentor, and manage a high-performing engineering team, fostering a culture of innovation and collaboration.\n5. Cross-Functional Collaboration: Work closely with Product, Data Science, and Business teams to translate AI research into real-world applications.\n6. Scalability & Performance Optimization: Architect and optimize distributed systems, ensuring efficient deployment of AI models across cloud and edge environments.\n7. Security & Compliance: Implement best practices for AI ethics, data security, and compliance with industry regulations.\n\nQualifications & Skills:\n12+ years of experience in software engineering, with at least 3 years in leadership roles within AI-driven product companies.\nStrong expertise in Generative AI, Deep Learning, NLP, Computer Vision, and model deployment.\nExperience with ML frameworks and cloud platforms (AWS, GCP, Azure).\nProven ability to scale AI/ML infrastructure and optimize models for performance and cost-efficiency.\nDeep understanding of distributed systems, cloud-native architectures, and microservices.\nHands-on experience with MLOps, CI/CD, and DevOps practices.\nStrong problem-solving, strategic thinking, and stakeholder management skills.\nAbility to attract, develop, and retain top engineering talent in a competitive market.\n\nWhy Join Us?\nLead a team at the cutting edge of Generative AI innovation.\nWork on scalable, high-impact AI products shaping the future of technology.\nOpportunity to work with a truly entrepreneurial style culture that fosters imagination, innovation and teamwork.\nOpportunity to work with a collaborative and high-calibre team.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Generative Ai', 'Artificial Intelligence', 'Machine Learning', 'Coding', 'Python', 'development', 'Devops', 'cloud', 'NLP', 'infrastructure', 'CI/CD', 'Deploying Models', 'Computer Vision']",2025-06-12 15:10:46
BCN Labs_ Associate - Front end developer,Bain,1 - 2 years,Not Disclosed,['Bengaluru'],"we're seeking an Associate - Front-End Developer to join our team and bring analytical applications to life through elegant, user-friendly interfaces. you'll work closely with data scientists, product leads, and backend developers to build front-end experiences for tools and Applications, used by clients and non-technical stakeholders.\nAs an Associate - Front-End Developer, you will:\nBuild Data-Driven Interfaces : Develop responsive, performant, and intuitive user interfaces that simplify interaction with complex data and analytical frameworks.",,,,"['Backend', 'Front end', 'Version control', 'Debugging', 'Consulting', 'Javascript', 'Information technology', 'Python', 'CSS3']",2025-06-12 15:10:49
Python Developer,Infosys,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Title\nPython Developer\n\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain.\nYou will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements.\nYou will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! Technical and Professional :\nPrimary skills:Technology-Machine Learning-Python Preferred Skills:\nTechnology-Machine Learning-Python Additional Responsibilities:\nKnowledge of design principles and fundamentals of architecture\nUnderstanding of performance engineering\nKnowledge of quality processes and estimation techniques\nBasic understanding of project domain\nAbility to translate functional / nonfunctional requirements to systems requirements\nAbility to design and code complex programs\nAbility to write test cases and scenarios based on the specifications\nGood understanding of SDLC and agile methodologies\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate Educational Bachelor of Engineering Service LineInformation Systems* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SDLC', 'software development', 'report generation', 'MIS', 'Python development', 'Python Developer', 'CI/CD']",2025-06-12 15:10:51
Applied Research Center (ARC),Infosys,5 - 10 years,Not Disclosed,['Bengaluru'],"Responsibilities\n1. Emerging Tech Trends Research - Research on emerging tech trends, ecosystem of players, use cases and their applicability and impact to client businesses. Scan & curate startups, universities and tech partnerships needed and create innovation ecosystem. Rapidly design and develop PoCs in Emerging tech areas. Share design specifications with other team members, get the components developed, integrate and test. Build reusable components and develop PoCs using relevant startups and Open-source solutions.\n2. Thought Leadership - Develop showcases that demonstrate how emerging technologies can be applied in a business context, demo scenarios for the IP. Contribute towards patents, tier-1 publications, whitepapers, blogs in the relevant emerging tech area Get certified on the emerging technology, frameworks\n3. Applied Research Center Activities - Contribute to high level design development, testing and implementation of new proof of concepts in emerging tech areas.\n4. Problem Definition, Requirements - Understand technical requirements and define detailed design. Analyze the reusable components to map the given requirement to existing implementation and identify needs for enhancements\n5. IP Development - Develop program level design, modular components to implement the proposed design. Design and develop reusable components. Ensure compliance with coding standards, secure coding, KM guidelines while developing the IP\n6. Innovation Consulting - Understand client requirements and implement first of kind solutions using emerging tech expertise. Customize and extend IP for client specific features\n7. Talent Management - Mentor the team and help them acquire the identified emerging tech skill. Participate in demo sessions, hackathons8. Emerging Tech Startup Ecosystem Work with startups in providing innovative solutions to client problems and augmenting Infosys offerings\nTechnical and Professional Requirements:\nApplied Research Center [Emerging Areas]Advanced AI [SLM, Inference Scaling, Synthetic Data, Distributed Learning, Agentic AI, ANI]New Interaction Models [Spatial computing, Mixed Reality, 3D visualizations, New Experiences]Platforms and Protocols [Architecting and engineering for Performance, Uptime, Low-latency, Scalability, Efficiency, Data, Interoperability and Low cost, Beckn, CDPI]Cybersecurity [Ethical hacking, Threat Mgmt, Supply chain security & risk, Cyber Resilience]Quantum [Quantum AI, Stack, Simulation & Optimization, Cryptography, Valued use cases]Autonomous Machines [Humanoids, Industrial Robots, Drones, Smart Products]Emerging Research [Brain, AGI, Space, Semicon ]\nPreferred Skills:\nDomain->User Experience Design->Usability Principles->HCI\nFoundational->Learning Experience Design->Learning design Management->IP Management\nTechnology->X Reality (XR)->Augmented Reality\nTechnology->X Reality (XR)->Virtual Reality\nTechnology->Blockchain->Blockchain as a Service (BaaS)->AWS Blockchain\nTechnology->Robotic Process Automation->Intelligent Process Automation\nFoundational->Cybersecurity Competency Management->Cyber Competency Strategy Planning\nFoundational ->Data privacy->Privacy by design\nTechnology->Machine Learning->Generative AI\nAdditional Responsibilities:\nTechnical Competencies\nAdvanced theoretical knowledge in specific domain\nExperimental design and methodology expertise\nData analysis and interpretation skills\nPrototype development capabilities\nResearch tool proficiency relevant to domainSoft Skills and Attributes\nCollaborative mindset for cross-disciplinary research\nCommunication skills for knowledge dissemination\nCreative problem-solving approach\nIntellectual curiosity and innovation focus\nCommercial awareness for translational research\nEducational Requirements\nPhD of Computer Science,Bachelor of Engineering\nService Line\nGlobal Delivery\n* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Research & Development,"Employment Type: Full Time, Permanent","['User Experience Design', 'Agentic AI', '3D visualizations', 'SLM', 'Distributed Learning', 'Inference Scaling', 'Spatial computing', 'ANI', 'Mixed Reality', 'Synthetic Data', 'New Experiences']",2025-06-12 15:10:53
Software Developer 3,Oracle,4 - 9 years,Not Disclosed,['Bengaluru'],"Oracle Cloud Infrastructure (OCI) delivers mission-critical applications for top tier enterprises around the world. Our cloud offers unmatched hyper-scale, multi-tenant services deployed in more than 50 regions worldwide. OCI is expanding its mission beyond the traditional boundaries of public cloud to include dedicated, hybrid and multi cloud, edge computing, and more.\nAt OCI platform organization, our mission is to provide core platform services for OCI cloud and customers. We re embarking on ambitious new initiative to scale our tier-0 services for 10x growth. We re looking for hands-on engineers with expertise and passion in solving difficult problems in distributed systems and highly available services. If this is you, at Oracle you can design and build innovative new systems from the ground up. These are exciting times in our space - we are growing fast, still at an early stage, and working on ambitious new initiatives. An engineer at any level can have significant technical and business impact\nResponsibilities\nAs a member of the software engineering division, you will apply advanced knowledge of software architecture to perform software development tasks associated with developing, debugging or designing software applications or operating systems according to provided design specifications. You will collaborate with principal engineers & architects on team to build out next gen platform dataplane\nBasic Qualifications\nBS or MS degree in Computer Science or relevant technical field involving coding or equivalent practical experience\nDemonstrated ability to write great code using Java, GoLang, or similar languages\n4+ years of software development experience.\nProven ability to deliver products and experience with the full software development lifecycle\nExperience working on large-scale, highly distributed services infrastructure\nExperience working in an operational environment with mission-critical tier-one livesite servicing\nSystematic problem-solving approach, strong communication skills, a sense of ownership, and drive\nExperience designing architectures that demonstrate deep technical depth in one area, or span many products, to enable high availability, scalability, market-leading features and flexibility to meet future business demands",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'software architecture', 'Coding', 'Debugging', 'Software development life cycle', 'Infrastructure', 'Oracle', 'Application software', 'Distribution system', 'Operations']",2025-06-12 15:10:55
Developer - L4,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"The purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n\n\nDo\n\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\nDeliver\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Continuous Integration, Deployment & Monitoring of Software 100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan 2. Quality & CSAT On-Time Delivery, Manage software, Troubleshoot queries,Customer",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Machine Learning', 'software development', 'report generation', 'MIS', 'CI/CD', 'SDLC']",2025-06-12 15:10:58
Business Analyst | Amazon Now,Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"Are you ready to embark on a thrilling journey in the realm of grocery e-commerce? Were on the lookout for a team member to work on our latest initiative, operating at the forefront of innovation in a dynamic, fast-paced environment. This role demands the agility to navigate analytics landscape across multiple functions seamlessly, the resilience to thrive in a fast paced environment, excitement to handle challenges head-on and excellence in analytical abilities.\n\nAs a Business Analyst, youll be deciphering our customers ever-evolving needs and shaping solutions that elevate their experience with Amazon.\n\nWere seeking someone who thrives on ambiguity, harnessing their first-principle problem-solving skills to drive impactful outcomes. Your ability to cultivate a customer-centric mindset, coupled with a penchant for out-of-the-box thinking, will be instrumental in navigating the complex landscape of our initiative.\n\nA successful candidate will possess:\nGood analytical and quantitative skills, leveraging data and metrics to inform strategic decisions.\nImpeccable attention to detail, adept at juggling multiple projects and priorities with finesse.\nA knack for thriving in a fast-paced, innovation-driven environment, where adaptability is key.\nClear and compelling communication skills, capable of articulating data insights to diverse stakeholders.\n\nIf youre ready to challenge the status quo, lead with innovation, and leave an indelible mark on the future of e-commerce, then we want to hear from you!\n\n\nResponsibilities:\nUnderstand the various operations across Amazon Now\nDesign and develop highly available dashboards and metrics using SQL, Quicksight, and Python\nUnderstand the requirements of stakeholders and map them with the data sources/data warehouse\nOwn the delivery and backup of periodic metrics, dashboards to the leadership team\nDraw inferences and conclusions, and create dashboards and visualizations of processed data, identify trends, anomalies\nExecute high priority (i.e. cross functional, high impact) projects to improve business performance across different verticals\nPerform business analysis and data queries using appropriate tools\nWork closely with internal stakeholders such as business teams, engineering teams, and partner teams and align them with respect to your focus area\nExecute analytical projects and understanding of analytical methods (forecasting, Machine Learning Techniques, etc.)\n\nAbout the team\nWe are building and scaling the 10 minute delivery service of Amazon Bachelors degree or equivalent\nExperience defining requirements and using data and metrics to draw business insights\nExperience with SQL or ETL\n2+ years of Excel or Tableau (data manipulation, macros, charts and pivot tables) experience\nKnowledge of Microsoft Excel at an advanced level, including: pivot tables, macros, index/match, vlookup, VBA, data links, etc.\nExperience with reporting and Data Visualization tools such as Quick Sight / Tableau / Power BI or other BI packages Experience using very large datasets",,,,"['Excel', 'Business analysis', 'VLOOKUP', 'Analytical', 'Machine learning', 'Forecasting', 'Macros', 'Analytics', 'SQL', 'Python']",2025-06-12 15:11:00
Application Architect - L1,Wipro,8 - 10 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to create exceptional and detailed architectural application design and provide thought leadership and enable delivery teams to provide exceptional client engagement and satisfaction.\n\nDo\n1. Develop architectural application for the new deals/ major change requests in existing deals\na. Creates an enterprise-wide architecture that ensures systems are scalable, reliable, and manageable.\nb. Manages application assets and directs the development efforts within an enterprise to improve solution delivery and agility\nc. Guides how to construct and assemble application components and services to support solution architecture and application development\nd. Maintains the frameworks and artefacts used in the implementation of an application, with reference to the systematic architecture of the overall application portfolio\ne. Responsible for application architecture paradigms such as service-oriented architecture (SOA) and, more specifically, microservices, ensuring business achieve agility and scalability for a faster time to market\n\nf. Provide solution of RFPs received from clients and ensure overall design assurance\nDevelop a direction to manage the portfolio of to-be-solutions including systems, shared infrastructure services, applications in order to better match business outcome objectives\nAnalyse technology environment, enterprise specifics, client requirements to set a collaboration design framework/ architecture\nDepending on the clients need with particular standards and technology stacks create complete RFPs\nProvide technical leadership to the design, development and implementation of custom solutions through thoughtful use of modern technology\nDefine and understand current state solutions and identify improvements, options & tradeoffs to define target state solutions\nClearly articulate and sell architectural targets, recommendations and reusable patterns and accordingly propose investment roadmaps\nEvaluate and recommend solutions to integrate with overall technology ecosystem\nTracks industry and application trends and relates these to planning current and future IT needs\ng. Provides technical and strategic inputs during the project planning phase in the form of technical architectural designs and recommendations\nh. Account mining to find opportunities in the existing clients\ni. Collaborates with all relevant parties in order to review the objectives and constraints of solutions and determine conformance with the Enterprise Architecture.\nj. Identifies implementation risks and potential impacts.\nk. Create new revenue streams within applications as APIs that can be leveraged by clients\nl. Bring knowledge of automation in application by embracing Agile and dev-ops principles to reduce manual part\n2.Understanding application requirements and design a standardize application\na. Creating Intellectual Property in forms of services, patterns, models and organizational approaches\nb. Designing patterns, best practices and reusable applications that can be used for future references\nc. Ensure system capabilities are consumed by system components and set criteria for evaluating technical and business value in terms of Tolerate, Invest, Migrate and Eliminate\nd. Provide platform to create standardize tools, uniform design and techniques are maintained to reduce costs of maintenance\ne. Coordinating input on risks, costs and opportunities for concepts\nf. Developing customised applications for the customers aligned with their needs\ng. Perform design and code reviews thoroughly on regular basis, keeping in mind the security measures\nh. Understanding design and production procedures and standards to create prototypes and finished products\ni. Work closely with systems analysts, software developers, data managers and other team members to ensure successful production of application software\nj. Offer viable solutions for various systems and architectures to different types of businesses\nk. Seamless integration of new and existing systems to eliminate potential problems and maintain data structure and bring value in terms of development\nl. Transforming all applications into digital form and implement and evolve around mesh app and service architecture that support new technologies like IOT, blockchain, machine learning, automation, BOTS etc\n\nm.Cloud Transformation: (Migration)\nUnderstanding non-functional requirements\nProducing artefacts such as deployment architecture, interface catalogue\nIdentify internal and external dependency, vendor and internal IT management\nSupport build and testing team\nn.Cloud Transformation: (Modernization)\nUnderstanding and Defining target architecture in Integration space\nAssessing project pipeline / demand and align to target architecture\nTechnical support of delivery team in terms and POC and technical guidance\no.Keep Up-to-date with the latest technologies in the market",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Application architecture', 'application design', 'Cloud Transformation', 'solution delivery', 'Application Architect', 'application development']",2025-06-12 15:11:03
Python Senior Developer,Infosys,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Title\nPython Senior Developer\n\nResponsibilities\nSolid development experience in Data Science Arch.\nExperience in Application Architecture & Design of Java Based Applications\nGood Knowledge of Architecture and related technologies\nExperience in Integration Technologies and Architecture\nWorking knowledge of frontend and database technologies\nExcellent Analytical and Debugging Skills\nFamiliarity with Agile & DevSecOps, Log Analytics, APM\nExperience in leading the teams technically\nExperience in requirements gathering, analysis & design and estimation\nGood communication and articulation skills Technical and Professional :\nWe are seeking a skilled Python and SQL Developer to join our dynamic team. The ideal candidate will have a strong background in Python programming and SQL database management.\nDevelop and maintain Python-based applications and scripts.\nWrite efficient SQL queries for data extraction and manipulation.\nCollaborate with cross-functional teams to gather requirements and deliver solutions.\nFamiliarity with Linux operating systems.\nBasic understanding of cloud platforms (e.g., AWS, Azure, Google Cloud).\nKnowledge of Model Quantization and Pruning\nExperience playing a Data Scientist role Preferred Skills:\nPython Technology-Open System-Open System- ALL-Python Technology-Full stack-Java Full stack-Frontend(Vue.js)+Enterprise layer(Python)+DB Additional Responsibilities:\nIn-depth knowledge of design issues and best practices\nSolid understanding of object-oriented programming\nFamiliar with various design, architectural patterns and software development process.\nExperience with both external and embedded databases\nCreating database schemas that represent and support business processes\nImplementing automated testing platforms and unit tests\nGood verbal and written communication skills\nAbility to communicate with remote teams in effective manner\nHigh flexibility to travelSoft Skills\nGood verbal & written communication skills articulate value of AI to business, project managers & other team members\nAbility to break complex problem into smaller problems and create hypothesis\nInnovation and experimentation Educational Master of Computer Science,Master Of Science,Master Of Technology,MCA,Bachelor Of Comp. Applications,Bachelor Of Computer Science,Bachelor of Engineering,Bachelor Of Technology Service LineApplication Development and Maintenance* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Enterprise layer', 'software development', 'report generation', 'MIS', 'CI/CD', 'Java Full stack-Frontend', 'SDLC']",2025-06-12 15:11:05
Asset & Wealth Management - AM FI Macro Strats - Associate,Goldman Sachs,2 - 7 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Goldman Sachs, we connect people, capital and ideas to help solve problems for our clients. We are a leading global financial services firm providing investment banking, securities and investment management services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals.\nAt Goldman Sachs, our Engineers don t just make things - we make things possible. We change the world by connecting people and capital with ideas and solve the most challenging and pressing engineering problems for our clients. Our engineering teams build scalable software and systems, architect low latency infrastructure solutions, proactively guard against cyber threats, and leverage machine learning alongside financial engineering to continuously turn data into action.\nEngineering, which is comprised of our Technology Division and global strategist groups, is at the critical center of our business. Our dynamic environment requires innovative strategic thinking. Want to push the limit of digital possibilities? Start here.\nGoldman Sachs Asset & Wealth Management\nAs one of the worlds leading asset managers, our mission is to help our clients achieve their investment goals. To best serve our clients diverse and evolving needs, we have built our business to be global, broad and deep across asset classes, geographies and solutions.\nGoldman Sachs Asset & Wealth Management is one of the worlds leading asset management institutions. AWM delivers innovative investment solutions managing close to Two Trillion US Dollars on a global, multi-product platform. In addition to traditional products (e.g. Equities, Fixed Income) our product offering also includes Hedge Funds, Private Equity, Fund of Funds, Quantitative Strategies, Fundamental Equity and a Multi-Asset Pension Solutions Business. Software is engineered in a fast-paced, dynamic environment, adapting to market and customer needs to deliver robust solutions in an ever-changing business environment. AM Data Engineering builds on top of cutting edge in-house and cloud platforms complimented with a strong focus on leveraging open source solutions.\nBusiness Overview\nThe External Investing Group ( XIG ) provides investors with investment and advisory solutions across leading private equity funds, hedge fund managers, real estate managers, public equity strategies, and fixed income strategies. XIG manages globally diversified programs, targeted sector-specific strategies, customized portfolios, and a range of advisory services. Our investors access opportunities through new fund commitments, fund-of-fund investments, strategic partnerships, secondary-market investments, co-investments, and seed-capital investments. With over 350 professionals across 11 offices around the world, XIG provides manager diligence, portfolio construction, risk management, and liquidity solutions to investors, drawing on Goldman Sachs market insights and risk management expertise. We extend these global capabilities to the world s leading sovereign wealth funds, pension plans, governments, financial institutions, endowments, foundations, and family offices, for which we invest or advise on over $300 billion of alternative investments, public equity strategies, and fixed income strategies.\nWhat We Do\nWithin Asset Management, Strategists (also known as Strats ) play important roles in research, valuation, portfolio construction, and risk management analytics. A Strategist will apply quantitative and analytical methods to come up with solutions that are accurate, robust, and scalable. Strats are innovators and problem-solvers, building novel and creative solutions for manager selection, portfolio construction, and risk management. You will develop advanced computational models, architectures, and applications to meet the challenges of a rapidly growing and evolving business.\nStrats collaborate across the business to develop solutions. These daily interactions with other team members across geographies demand an ability to communicate clearly about complex financial, business, and mathematical concepts. We look for creative collaborators who evolve, adapt to change, and thrive in a fast-paced global environment.\nBasic Qualifications\nOutstanding background in a quantitative discipline, with excellent analytical, quantitative, and problem-solving skills, and demonstrated abilities in research and data visualization\nProgramming expertise in a scripting language (e.g. Python, R, Matlab)\nStrong general and technical communication skills, with an ability to effectively articulate complex financial and mathematical concepts\nCreativity and problem-solving skills\nAbility to work independently and in a team environment\n2+ years of applicable experience\nGoldman Sachs Engineering Culture",Industry Type: Banking,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Wealth management', 'Analytical', 'Fixed income', 'Investment banking', 'Asset management', 'Investment management', 'Risk management', 'Private equity', 'Analytics', 'Financial services']",2025-06-12 15:11:08
XR Systems Architect,Qualcomm,1 - 6 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nQualcomm's XR Technology Systems team is seeking motivated engineers with who will work on driving next generation technologies & platforms enabling the future of Augmented Reality / Virtual Reality / Mixed Reality applications. This team interfaces with product management, platform architecture, technology/IP and software implementation leads.\n\n\n\nThe successful candidate will be responsible for one or more of the following roles\n\nDriving technical workstreams across key technology tracks (perception, rendering, composition, reprojection, split-processing, user input interfaces, camera & display processing). Key role here is to have forward looking perspective and identify opportunities for prototyping, demonstrating the system level trade-offs (working with respective technology team(s), helping define the feature goodness criteria from end use case perspective. Collaborating with technology experts to drive strategic direction in cross-functional AR/MR/VR areas. Working with technology, hardware, and software experts to translate use-case requirements into implementation specifications and contributing to reference design planning. Engaging on identifying and scoping new use-cases Early engagement with customers and works on aligning with product management on platform requirements. Working with technology tracks to drive the system level what-ifs/trade-offs and helping with competitive analysis.\n\nFor this multi-disciplinary role an ideal candidate has experience in AR/VR, computer vision, perception, camera technology, hardware design, SoC architecture, HW/SW partitioning, and/or system modeling (power, performance, thermal).\n\nMinimum Qualifications\n\nBachelors degree in Electrical Engineering, Information Systems, Computer Science, or related field, and project experience in architecture/micro-architecture\n\n1+ years of system engineering or related work experience\n\nExcellent problem solving and communication skills\n\n\nPreferred Qualifications\n\nMasters and/or PhD degree in Electrical Engineering, Information Systems, Computer Science\n\n1+ years experience in HW architecture/design with emphasis on areas listed above\n\nProven experience in conducting architectural trade-offs, power/performance analysis and/or SW-HW trade-offs\n\nExtensive knowledge of graphics pipeline, computer vision pipelines, machine learning methods and/or camera pipelines\n\nExperience with system level modeling (performance and/or power)\n\nC/C++ programming\n\n\nKeywords Camera, ISP, display, composition, rendering, video coding, computer vision, embedded, multimedia, image, algorithms, SOC architecture, micro-architecture\n\n\n\nEducational\n\nRequiredBachelors, Computer Engineering and/or Electrical Engineering or equivalent experience\n\nPreferredMasters / Doctorate, Computer Engineering and/or Computer Science and/or Electrical Engineering\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['graphics', 'machine learning', 'pipeline', 'computer vision', 'system engineering', 'synthesis', 'algorithms', 'asic', 'sta', 'c++', 'c', 'soc', 'verilog', 'hw', 'rtl design', 'computer science', 'product management', 'fpga', 'embedded systems', 'system architecture', 'soc design', 'rtl coding', 'system verilog']",2025-06-12 15:11:11
"QAE, Professional",Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"Do you want to develop the next generation Payments products for Indias fastest growing e-commerce company? Do you enjoy working in an entrepreneurial environment solving complex technical problems and delivering innovative solutions? If so, join us on Amazon India Payments Tech team. We are a group of talented technical professionals that are empowered and driven to build innovative world class experiences for millions of Amazon customers. The India Payments Tech in Bangalore is responsible to build product and technology solutions to build great payments products and experiences using various technologies to solve complex problems related to distributed systems, scalable architecture, machine learning, and algorithms.\n\nTo meet these challenges, we are looking for a high-energy, talented quality assurance engineer. You will work with multiple development teams and stakeholders in India and worldwide, to drive test strategy and implementation. A successful candidate will have a strong technical ability, excellent project management skills, great communication skills, and a motivation to achieve results in a fast-paced environment. Prior experience with Payment technology testing is a strong plus. The person chosen for this position will have the opportunity to contribute their creative ideas and energy to new, complex and business critical products.\n\n\n1. Coordinate with multiple teams to communicate our technical requirements, drive schedules and review and help build test plans that test end-to-end functionality spanning services owned by multiple organisations.\n2. Design, execute and automate tests of front end applications, and middle and back-end software across a variety of architectures.\n3. Test systems at the user level, both manually and with automated tools.\n4. Grey box testers rather than black-box testers, able to understand software internals, debug problems using log files, and write automated tests with scripts and/or user-level automated tools.\n5. Work with Software Development Engineers and Business Owners to understand the technical implementation of features.\n6. Work with business stakeholders, designers and customer service teams to understand customer usage models and develop test plans and suites that approximate real-world environments.\n7. Help drive the software development process towards quality-centric methodologies, always seeking to avoid defects or find them at the earliest stage possible. 2+ years of quality assurance engineering experience\nExperience in automation testing\nExperience in manual testing\nExperience in UI and API automation testing (Selenium/SOAPUI) Experience in API & Mobile testing\nExperience designing and planning test conditions, test scripts, and test data sets to ensure appropriate and adequate coverage and control",,,,"['Usage', 'Manual testing', 'Front end', 'Test scripts', 'Test strategy', 'Machine learning', 'Selenium', 'Customer service', 'Distribution system', 'Mobile testing']",2025-06-12 15:11:13
XR Systems Technology Architect (2 - 10 years),Qualcomm,1 - 6 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nOrganization overview:\n\nQualcomm is a key enabler for the XR eco-system with a dominant market share. We build custom SoCs and technologies that are at the heart of existing and emerging XR products. Qualcomm XR Systems organization is responsible for architecture definition of Qualcomms next generation XR SoCs. Our portfolio of SoC offerings serve a broad range of XR products covering Mixed Reality, Augmented Reality and AI Glass product families. The span of technologies that go into these products and SoCs include high resolution immersive displays, perception features that are bulit on computer vision and deep learning technology, highly efficient DSP processors, dedicated deep learning accelerators, graphics engines supporting high resolution and high frame rate rendering and reprojection, multimedia processing engines (audio, video, imaging,..), CPUs and SoC infrastructure that ensures efficient and secure processing, as well as very low power architecture features such as power islands and power rail isolation, sleep modes etc.\n\nWe are scaling up our operations!! We are looking for engineers with background in diverse areas including architecture and micro architecture definition, design and verification of IPs and SoCs. People who have experience in areas such as SoC architecture, networks on chip, virtual memory, on-chip and off-chip memory subsystems, security architecture, CPUs, etc, also development of IPs such as camera, video, GPU, DSPs, deep learning accelerators and neural signal processors (NSP), peripherals and interfaces such as PICe, SPI etc. Also, people with strong background on pre silicon and post silicon power estimation and optimization, performance estimation, power architecture design will be highly encouraged. We have openings at senior as well as junior job levels.\n\nWe are keenly interested in you if you are someone who has gained expertise in your specific domain which could be one or more of the areas mentioned above and are excited to take the next step in your career to become architects of the SoCs that will shape the future generations of XR products!! Apart from a rewarding career and growth prospects, the organization offers a unique opportunity to learn from a diverse set of experts working collaboratively under the same roof, towards a common goal.\n\nJob Overview\n\nQualcomm's XR Systems team is seeking system architects who will work on defining the next generation SoC architectures, enabling the future of Augmented Reality / Virtual Reality / Mixed Reality applications. Responsibilities of successful candidates may span one or more of the following areas:\n\nWorking with lead XR OEMs and QCs customer-facing teams to understand end to end use cases\n\nResearching the product family roadmap to align internal IP and SoC architecture roadmap\n\nCollaborating with colleagues in the architecture team and across technology, IP and SoC teams with diverse expertise\n\nExploring architectures for power efficient and performant mapping of use cases on future SoCs and coming up with architecture proposals\n\nDefining and optimizing use case data flows\n\nUse case power modeling, estimation and optimization\n\nWorking with SoC design, and validation teams to ensure that the use case power and performance KPIs are met.\n\n\nMinimum Qualifications\n\nBachelors degree in Electrical Engineering, Information Systems, Computer Science, or related field, and project experience in architecture/micro-architecture\n\nExperience (1 - 10 years) in areas covering at least one of the followingIP and SOC design, DV, micro architecture, architecture, camera, video, GPU, DSP, NSP, CPU, security, NOCs and DRAM controller subsystems, power architecture and power and performance estimation and optimization.\n\nExcellent problem solving and communication skills\n\n\nPreferred Qualifications\n\nMasters and/or PhD degree in Electrical Engineering, Information Systems, Computer Science\n\nExperience with Mixed & Augmented reality system design, constraints, and trade-offs\n\nDeep understanding of system architecture aspects such as NOCs, DRAM controller performance issues, power domains and sleep modes of memories, IPs and cores.\n\nProven experience in conducting architectural trade-offs, power/performance analysis and/or SW-HW trade-offs\n\nExperience with system level modeling (performance and/or power)\n\nProficiency in scripting languages such as python, perl, shell etc.\n\n\nKeywords\n\nCamera, GPU, CPU, SOC, SoC architecture, NOC, DDR subsystem, LPDDR IP, caches, security, virtual memory, development, RTL design, Computer vision, Artificial Intelligence, ML, DSP, AR, VR, MR, XR\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'system architecture', 'aspect', 'scripting languages', 'system engineering', 'synthesis', 'asic', 'cdc', 'soc', 'system design', 'rtl', 'vhdl', 'verilog', 'microservices', 'lint', 'rtl design', 'computer science', 'fpga', 'fpga design', 'soc design', 'rtl coding', 'shell scripting', 'perl', 'system verilog']",2025-06-12 15:11:16
"SDET, Alexa Audio",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Do you want to transform the way people interact with digital content? Come join the team that builds Alexa Audio platform. We are innovating and changing the way our customers interact with media services and devices! You ll be among the first ones in the space and get to define the customer experience, frameworks and processes for the years to come.\nOn the Alexa Audio Team, you will have an enormous opportunity to impact the customer experience, design, architecture, and implementation of new and exciting products that will be used every day by people you know. We re looking for people who are passionate about innovating on behalf of customers, demonstrate a high degree of product ownership, and want to have fun while they make history.\n\n\n* Lead the design and development of quality infrastructure that encourages the optimal combination of unit/component/integration and end to end tests.\n* Lead the design and development of performance testing and platform verification kit.\n* Collaborate with software engineering teams, driving continuous development, integration and deployment\n* Working with Principal Engineers and other Senior engineers to develop best practices for software quality assurance across multiple software teams\n* Actively participate in cross team design reviews\n* Assist in the career development of others, actively mentoring individuals and the community\n* Exert technical influence over multiple teams, increasing their productivity and effectiveness by sharing your deep knowledge and expertise.\n* You will have a profound impact on millions of customers.\n* Report on status of development, quality, operations, and system performance to management\n* The key requirement for this position is established skill designing and developing complex, interactive customer experiences.\n\nIf you have an entrepreneurial spirit, know how to deliver, are deeply technical, highly innovative and long for the opportunity to build pioneering solutions to challenging problems, we want to talk to you.\n\nAbout the team\nAlexa Audio: As Alexa Audio, we own the audio experiences on Alexa enabled devices. These experiences include Music, Podcast, Audio Books, Radio, and Ambient soundscapes. Our integration skill kits enable seamless integration of 1P / 3P audio content providers (e.g., Amazon Music, Spotify, Apple etc.) with Alexa devices. Audio is one of the most widely used experiences on Alexa, and is enjoyed by millions of customers on a daily basis.\n\nThere is a universe of tens of millions of distinct audio content that is available across streaming services. Using your voice, and timely proactive suggestions are the easiest ways to get to what you want. Alexa makes this magical for customers: ask any Alexa-enabled device to play your favorite song, podcast, book or a station, and Alexa will find the right content for you and play it. We also support being able to control and manage playback across multiple devices, and movement of media playback across them. It is still Day 1 for the audio experiences, and were looking for a leader to help us make it even better.\n\nAs SDET you will be responsible for building highly scaled systems that ensure Alexa continues to delight customers world wide.\n3+ years of non-internship professional software development testing experience\n3+ years of test automation frameworks and tools building experience\nExperience programming with at least one modern language such as Java, C++, or C# including object-oriented design Knowledge of overall system architecture, scalability, reliability, and performance in a database environment\nExperience with security in service-oriented architectures and web services\nBachelor s Degree in Computer Science or related technical field",,,,"['Computer science', 'Object oriented design', 'System architecture', 'Career development', 'C++', 'digital content', 'Performance testing', 'Customer experience', 'Internship', 'Software quality assurance']",2025-06-12 15:11:18
XR Systems & Software Architect XR Research Staff,Qualcomm,9 - 14 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nQualcomm XR Research India is rapidly expanding to offer state of the art XR solutions. To scale and strengthen our offering in this domain, we are seeking a systems architect who will drive the next-generation technologies and architecture, shaping the future of Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR) use cases.\n\nResponsibilities:\n\nYour responsibilities will span across technical leadership, system architecture, software architecture, implementation and compute analysis. Here are the key aspects of your role:\n\nDrive technical workstreams related to perception, reprojection, split-processing and other XR technologies.\n\nIdentify and deeply understand the use cases for AR/VR/MR applications. Collaborate with cross-functional teams to translate use case requirements into detailed implementation specifications.\n\nDefine system and software architecture, considering hardware/software tradeoffs, compute and memory constraints. Optimize compute workload distribution across different subsystems on the SoC for efficient performance and power.\n\nValidate and optimize architecture definitions through system-level use case modeling.\n\nPrototype new use cases to understand the compute and memory requirements, and influence future software/hardware features and reference device specification.\n\n\nMinimum Qualifications:\n\n9+ years of experience in systems engineering with a bachelors degree in electrical engineering, information systems, computer science, or related field.\n\nHands-on experience in defining systems architecture and software design for multi-core architectures (CPUs, GPUs, DSPs, etc.), including performance analysis on heterogeneous architectures (core, multi-level cache, memory, etc.).\n\nProficiency in documenting call flows and data flows for both software and hardware components.\n\nStrong communication skills and ability to work effectively in a team.\n\n\nPreferred Qualifications:\n\n8+ years of experience in systems engineering with masters and/or PhD degree in electrical engineering, information systems, computer science.\n\nProven expertise in AR/VR, computer vision, machine learning, perception, camera technology, graphics pipeline, hardware design, SoC architecture, HW/SW partitioning, and system modeling (power, performance).\n\nProficiency in C++, and Object-Oriented SW design\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\n\n2+ years of work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Research & Development,"Employment Type: Full Time, Permanent","['virtual reality', 'c++', 'machine learning', 'computer vision', 'call flow', 'synthesis', 'python', 'c', 'data validation', 'graphics', 'prototype', 'verilog', 'hw', 'pipeline', 'rtl design', 'java', 'computer science', 'hardware design', 'soc design', 'software engineering', 'data flow', 'system engineering']",2025-06-12 15:11:21
"Business Research Analyst - II, RBS ACCX Program",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon.com strives to be Earths most customer-centric company where people can find and discover virtually anything they want to buy online. By giving customers more of what they want low prices, vast selection, and convenience Amazon.com continues to grow and evolve as a world-class e-commerce platform. Amazons evolution from Web site to e-commerce partner to development platform is driven by the spirit of innovation that is part of the companys DNA. The worlds brightest technology minds come to Amazon.com to research and develop technology that improves the lives of shoppers and sellers around the world.\n\nOverview of the role\nThe Business Research Analyst will be responsible for Data and Machine learning part of continuous improvement projects across the Discoverability space. This will require collaboration with local and global teams. The Research Analyst should be a self-starter who is passionate about discovering and solving complicated problems, learning complex systems, working with numbers, and organizing and communicating data and reports. The Research Analyst will perform Big data analysis to identify patterns, train model to generate product to product relationship and product to brand & model relationship. The Research Analyst is also expected to continuously improve the ML/LLM solutions in terms of precision & recall, efficiency and scalability. The Research Analyst should be able to write clear and detailed functional specifications based on business requirements.\n\n\nScoping, driving and delivering complex projects across multiple teams.\nPerforms root cause analysis by understanding the data need, get data / pull the data and analyze it to form the hypothesis and validate it using data.\nBuild programs to create a culture of continuous improvement within the business unit, and foster a customer-centric focus on the quality, productivity, and scalability of our services.\nFind the scalable solution for business problem by executing pilots and build Deterministic and ML/LLM models.\nManages meetings, business and technical discussions regarding their part of the projects.\nMakes recommendations and decisions that impact development schedules and the success for a product or project.\nDrives team(s)/partners to meet program and/or product goals.\nCoordinates design effort between internal team and External team to develop optimal solutions.\nPerforms supporting research, conduct analysis of the bigger part of the projects and effectively interpret reports to identify opportunities, optimize processes, and implement changes.\nAbility to convince and interact with stakeholders at all level either to gather data and information or to execute and implement according to the plan.\nAbility to deal with ambiguity and problem solver\nCommunicate ideas effectively and with influence (both verbally and in writing), within and outside the team.\n\nKey Performance Areas:\nSolve large and complex business problems by aligning multiple teams together.\nData analytics and Data Sciences\nMachine learning\nProject/Program Management\nAutomation initiative conceptualization and implementation\nBig Data analytics\nProduct development Scoping and Testing\nDefect Elimination\nAgile Continuous Improvement\n\nAbout the team\nThe RBS group in Chennai/Bangalore is an integral part of Amazon online product lifecycle and buying operations. The team is designed to ensure Amazon remains competitive in the online retail space with the best price, wide selection and good product information. The team s primary role is to create and enhance retail selection on the worldwide Amazon online catalog. The tasks handled by this group have a direct impact on customer buying decisions and online user experience. 3+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc. experience\nExperience with data visualization using Tableau, Quicksight, or similar tools\nExperience with data modeling, warehousing and building ETL pipelines\nExperience writing complex SQL queries\nExperience in Statistical Analysis packages such as R, SAS and Matlab\nExperience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling Experience with AWS solutions such as EC2, DynamoDB, S3, and Redshift\nExperience in data mining, ETL, etc. and using databases in a business environment with large-scale, complex datasets",,,,"['Automation', 'Data analysis', 'SAS', 'Data modeling', 'Machine learning', 'Agile', 'Oracle', 'Data mining', 'MATLAB', 'Python']",2025-06-12 15:11:23
IN_Manager_Agentic Chatbot_Advisory Corporate_Advisory,PwC Service Delivery Center,8 - 13 years,Not Disclosed,['Bengaluru'],"FS XSector\nSpecialism\nOperations\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nThose in intelligent automation at PwC will focus on conducting process mining, designing next generation small and largescale automation solutions, and implementing intelligent process automation, robotic process automation and digital workflow solutions to help clients achieve operational efficiencies and reduce costs.\nSeeking an innovative Agentic Chatbot proficient in NodeJS, Python. The candidate will design and implement conversational agents to improve client interactions and automate processes.\nRole and responsibilities\nDevelop and maintain chatbot solutions using industrystandard frameworks and tools.\nIntegrate chatbots with existing service platforms using NodeJS and Python.\nImplement natural language processing (NLP) techniques for effective communication.\nCollaborate with product teams to ensure chatbot functionality aligns with business objectives.\nRequirements\nExperience in chatbot development with knowledge of bot frameworks.\nProficiency in NodeJS, Python.\nKnowledge in Generative AI, RAG, indexing, Langchain, Amazon Bedrock, gRPC tools, Cloud services (AWS, Azure).\nExperience in NLP and machine learning related to conversational AI.\nStrong troubleshooting and debugging skills.\nAbility to work collaboratively in a fastpaced environment.\nGood to have\nHandson experience in Voice and chatbots.\nHandson experience in Azure Cognitive Services, Genesys, Azure Bot framework..\nMandatory Skills Set\nProject Management\nStake holder Management\nProject planning\nPrefered Skills Set\nAgentic Chatbot\nYears of experience required\n8+ years of experience in GenAI, machine learning, and architectural design\nBachelor s or Master s degree in Computer Science, Engineering, or a related field\nEducation Qualification BE/B.Tech/MBA/ CA\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Chartered Accountant Diploma, Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nChatbots\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Methodology, Analytical Thinking, Automation Algorithms, Automation Engineering, Automation Framework Design and Development, Automation Programming, Automation Solutions, Automation Studio, Automation System Efficiency, Blue Prism, Business Analysis, Business Performance Management, Business Process Analysis, Business Process Automation (BPA), Business Transformation, Business Value Optimization, C++ Programming Language, Coaching and Feedback, Cognitive Automation, Communication, Conducting Discovery, Configuration Management (CM) {+ 41 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Mining', 'Business transformation', 'Performance management', 'Business analysis', 'Project management', 'Analytical', 'Project planning', 'Troubleshooting', 'Operations', 'Analytics']",2025-06-12 15:11:26
Chipset Architect,Qualcomm,12 - 17 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Chipset Architect, you will research, design, develop, simulate, and/or validate systems-level hardware, software, architecture, algorithms, and solutions that enables the development of cutting-edge technology. Qualcomm Systems Engineers collaborate across functional teams to meet and exceed system-level requirements and standards.\n\nMinimum Qualifications\n\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 12+ years of Systems Engineering or related work experience.ORMaster's degree in Engineering, Information Systems, Computer Science, or related field and 10+ years of Systems Engineering or related work experience.\n\nPreferred Qualifications\n\nMaster's Degree in Engineering, Information Systems, Computer Science or related field.\n\n15+ years of Systems Engineering or related work experience.\n\nExpert knowledge of interfaces (USB, PCIe, SD/eMMC, UFS, LPDDRx, CSI, DSI, SPI, I2C, I3C, PMBUS, SPMI, Slimbus etc)\n\n10+ years of experience working in PC or IoT industry;\n\nKnowledge of commercial, industrial and home automation systems\n\nExperience with full product lifecycle from requirements gathering, prototype development, production, knowledge of PCB design flow, EDA tools, electrical and thermal simulations.\n\nFamiliarity with software stack and hardware-software dependencies including HLOS, drivers, kernel, BIOS\n\nFamiliarity with certification, shock and vibration testing and EMI compliance\n\nIn depth knowledge of power delivery, PDN, component selection, tuning, PMICs, eBOM\n\nGood domain knowledge of 2 or more functional areas of SoCs such as Application Processor, Display, Graphics, Camera, Video, AI, Modem (4G/5G), WLAN, Power Management etc.\n\nExposure to 4G/5G/6G systems and associated cellular standards (e.g. 3GPP NR, LTE).\n\nWorking with a wide cross functional team comprising of various design and software teams, reliability, functional safety, business, sales and customer engineering\n\n\nPrincipal Duties and Responsibilities\n\nApplies Systems knowledge to evaluate Industrial customer asks and propose chipset solutions utilizing Qualcomm ICs.\n\nDevelops and analyzes system level design including requirements, interface definition, functional/performance definition, and implementation of a new system or modification of an existing system.\n\nEvaluates interface signaling for clock, data, sideband requirements and propose solution with internal and third-party components.\n\nEvaluates signal integrity needs and identifies architecture for high-speed interfaces, topologies (stacked boards, different form factors etc), timing needs, and signal conditioning techniques.\n\nUnderstand power requirements and propose suitable powering schemes for the entire platform.\n\nCollaborates with own team and other teams to complete project work, including implementing and testing features and verifying the accuracy of systems.\n\nPerforms functional analysis to drive requirements and specifications and to define and align with standards for hardware and software.\n\nReviews internal and customer board schematics\n\nDevelops new and innovative ideas (e.g. IDFs) for a product or feature area.\n\nDrives triage of problems at the system level to determine root cause and presents results of testing and debugging to team members.\n\nLevel of Responsibility\n\nWorks independently with minimum supervision.\n\nDecision-making may affect work beyond immediate work group.\n\nRequires verbal and written communication skills to convey information. May require basic negotiation, influence, tact, etc.\n\nHas a moderate amount of influence over key organizational decisions (e.g., is consulted by senior leadership to make key decisions).\n\nTasks require multiple steps which can be performed in various orders; some planning, problem-solving, and prioritization must occur to complete the tasks effectively.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['usb', 'sd', 'i2c', 'spi', 'pcie', 'algorithms', 'ufs', 'power system', 'simulation', '3gpp', 'pcb', 'sales', 'artificial intelligence', 'iot', '5g', 'ebom', 'computer science', 'debugging', 'lte', 'digital transformation', 'functional safety', 'system engineering', 'prototype', 'component selection', 'pc', '4g', 'csi']",2025-06-12 15:11:28
Staff HPC Software Developer,Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Area: Information Technology Group, Information Technology Group > IT Software Developer\n\nGeneral Summary:\n\nWhats in it for youQualcomm is enabling a world where everyone and everything can be intelligently connected. Qualcomm 5G and AI innovations are the power behind the connected intelligent edge. Youll find our technologies behind and inside the innovations that deliver significant value across multiple industries and to billions of people every day.Qualcomm engineering teams rely heavily on the latest High Performance Computing (HPC) technologies to design and develop new products using electronic design automation (EDA) tools. This role provides an opportunity to work on the latest HPC technologies and gain experience in building scalable and fault-tolerant software solutions that are deployed on some of the largest supercomputing infrastructures across the globe.What are we looking forEngineering Software Solutions and Data Services team (ESSDS) is looking for an experienced software developer with strong HPC background. The ESSDS team is responsible for development of software solutions enabling High Performance Compute grid and large-scale, distributed, analytical applications. They work on components and services for HPC infrastructure optimization, hardware IP management systems, petabyte-scale cloud data platforms and development of machine learning solutions and pipelines.This is an individual contributor technical role providing subject matter expertise (SME) across the portfolio of HPC software products and services being developed by ESSDS team. The ideal candidate would be a seasoned software developer who is skilled in many of the following areascluster infrastructure management, job scheduling and orchestration, parallel programming, performance tuning and optimizations, efficient algorithms and data structures, compute/storage/network architectures, cloud computing, GPU computing, and EDA workflows.What will you doThis roles responsibilities include:- Design and develop software solutions and services for HPC infrastructure running EDA workflows and AI workloads- Identify opportunities and deliver solutions for EDA workflow optimizations- Provide HPC expertise across portfolio of projects, guiding and mentoring a team of software developers as needed- Execute projects in partnership with global Engineering IT teams- Manage and track the software development process from development to production release in collaboration with other software developersWhat do we want to seeThe ideal candidate will be able to demonstrate some of the following skills:- 8+ years of hand-on experience in developing software solutions for HPC grid infrastructure- Broad knowledge of latest compute, storage and networking architectures- Experience of building HPC infrastructure in public cloud environments such as AWS, Azure or Google Cloud- Proven expertise in parallel and distributed programming, GPU computing and performance engineering- Proficiency in programming languages such as Python, C++, Java, Rust- Deep understanding of HPC job schedulers such as LSF, Slurm and PBS- Familiarity with EDA and semiconductor design process- Exposure to AI and ML workloads running on HPC infrastructure- Expertise in software lifecycle management, version control, and CI/CD best practices for quality, agility and security- Ability to explain technical concepts and analysis implications in a clear manner to a wide audience.- Bachelors or Masters in Computer Science, Computational Science or related field\n\nMinimum Qualifications:\n5+ years of IT-relevant work experience with Bachelor's degree in a technical field (e.g., Computer Engineering, Computer Science, Information Systems).\nOR\n7+ years of IT-relevant work experience without a Bachelors degree.\n\n4+ years of work experience with Full-stack Application Development (e.g., Java, Python, JavaScript, etc.).\n3+ years of work experience with Data Structures, algorithms, and data stores.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'version control', 'ci/cd', 'networking', 'performance engineering', 'algorithms', 'python', 'c++', 'microsoft azure', 'distributed architecture', 'javascript', 'application development', 'java', 'gcp', 'infrastructure', 'hpc', 'data structures', 'aws', 'parallel computing']",2025-06-12 15:11:31
Senior - TM ESW Product Owner,Volvo India,10 - 15 years,Not Disclosed,['Bengaluru'],"Transport is at the core of modern society. Imagine using your expertise to shape sustainable transport and infrastructure solutions for the future? If you seek to make a difference on a global scale, working with next-gen technologies and the sharpest collaborative teams, then we could be a perfect match.\nWhat you will do\nTitle : Thermal Management(Specialist - ESW Engineer)\nAre you ready to make electrifying connections? Help us to design sustainable transportation solutions for the future. As part of the Volvo Group Vehicle Technology team, you ll help us accelerate our journey by engineering exciting next-gen technologies with a global reach. Be part of our evolution as we strengthen our team. Bring your love of developing systems, working collaboratively, and your advanced skills to a place where you can make an impact.\nWe are looking to hire a skilled embedded software engineer to join our dynamic software team. As an embedded software engineer, you will be responsible for executing complete embedded software life cycles for company and client hardware.\nYour Future Team :\nThermal Management is a department within Vehicle Technology responsible for developing, delivering, and maintaining an optimized cab climate and vehicle cooling & heating systems for all types of propulsion installations to all truck brands within the Volvo Group.\nWe are responsible for leading the work with strategies and advanced engineering globally. We are located at Gothenburg & Bengaluru, and we have close cooperation with the sites in Greensboro and Lyon.\nWe understand the final customer needs and apply our knowledge to develop technical concepts and solutions that satisfy customer and business needs. The work is based on innovation, shared technology, common architecture, and brand uniqueness.\nWho are you?\nDo you dream big? We do too, and we are excited to grow together.\nTo be successful in this role, we believe that you are a team player, high on energy, and are interested in working with global sites.\nAs a person, you can take own initiatives and drive them forward with a business and customer mindset. You have excellent communication skills & good at networking with people. You also have a positive attitude and adapt to changing conditions.\nYou have an innovative and creative mindset to recommend engineering solutions and the willingness to learn and develop your own skills and abilities. In addition to this, you hold the below experience: -\nMandatory Skills:\nME/Mtech/BE/BTech in Electrical/Electronics/Computer Science\nMinimum 10+ years of software development experience in Automotive Embedded SW development within the distributed systems & Thermal management and/or Relevant Automotive System\nGood knowledge in Matlab, Simulink, Embedded C, ASPICE Process\nExperience in hands-on control Software development and troubleshooting on embedded targets.\nProven experience in embedded systems design with pre-emptive, multitasking real-time operating systems.\nExcellent knowledge of coding techniques, IP protocols, interfaces and hardware subsystems\nCritical thinker.\nStrong documentation and writing skills.\nAdequate knowledge:\nFamiliarity with software configuration management tools, defect tracking tools, and peer review\nAdequate knowledge of reading schematics and data sheets for components\nKnowledge of the product development life cycle and change management activities for maintenance are required for this role.\nLeading tasks independently, good planning and monitoring skills.\nExperience in future technologies (e-MOB, Fuel Cell, automation) in Software Development\nGood in compiling and presenting information verbally and in writing.\nWhat s in it for you?\nYour key responsibilities Include: -\nResponsible for embedded solutions for Cabin Climate Control software(Driver / IO / Application / Parameter calibration)\nDeveloping System Software Requirements, Supporting with timeplans/roadmaps\nTake full responsibility and independently deliver the planned tasks as well as guide junior analysts within the software team.\nDesign/Implement I/O and Application software of thermal embedded devices and systems like HVAC/Cabin Climate Systems, Cooling/Heating systems\nFunction Development of Feed forward logic and control systems.\nDriving Functional/Non functonal Diagnosis logics software systems\nAnalyzing and enhancing efficiency, stability and scalability of system resources\nIntegrate and validate new product designs.\nSupport software QA and optimize I/O performance.\nProvide post-production support.\nInterface with hardware design and development\nSupport cross functional Teams with control logic and AI/ML implementations.\nContribute with component and system engineers to define and verify the actions agreed related to virtual simulation in FMEA, to reduce the probability of failure occurrences.\nEstablish a strong network with global counterparts and cross functions.\nDevelop the strategy to integrate data analytics and machine learning in the models ultimately leading to development of thermal management digital twins in collaboration with the data analytics and machine learning team\n\nWe value your data privacy and therefore do not accept applications via mail.\nWho we are and what we believe in\n.\nApplying to this job offers you the opportunity to join Volvo Group. Every day, across the globe, our trucks, buses, engines, construction equipment, financial services, and solutions make modern life possible. We are almost 100,000 people empowered to shape the future landscape of efficient, safe and sustainable transport solutions. Fulfilling our mission creates countless career opportunities for talents with sharp minds and passion across the group s leading brands and entities.\nGroup Trucks Technology are seeking talents to help design sustainable transportation solutions for the future. As part of our team, you ll help us by engineering exciting next-gen technologies and contribute to projects that determine new, sustainable solutions. Bring your love of developing systems, working collaboratively, and your advanced skills to a place where you can make an impact. Join our design shift that leaves society in good shape for the next generation.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['HVAC', 'Automation', 'Simulation', 'Hardware design', 'thermal', 'Simulink', 'Troubleshooting', 'MATLAB', 'Automotive', 'Embedded software']",2025-06-12 15:11:33
"Training Manager II, OPTIMA",Amazon,6 - 11 years,Not Disclosed,['Bengaluru'],"The OPTIMA team is seeking a Program Manager, Training.\n\nOPTIMA is a global team and enables Amazon to deliver a superior shopping experience to customers worldwide. We aspire to provide an end-to-end data solution for the LLM lifecycle, leveraging technology alongside our operational excellence. We enable shopping feature teams deliver superior CX quality by providing them reliable and comprehensive insights and ground truth data to measure and train ML (Machine Learning) models and handle annotation and Root Cause Analysis (RCA) across 10 different languages.\n\nThe Training Manager II will be responsible for planning, coordinating, executing and delivering learning and development programs/training for OPTIMA business. The role demands thought clarity, dynamic cross-functional partnership, and strategic thinking. The ideal candidate will be comfortable influencing stakeholders and senior leaders, have strong analytical skills, a track record of using data and tools to drive business impact and be comfortable working in an ambiguous environment.\n\n\nServe as a multi-threaded leader for training and development across various Processes in Optima.\nSchedule large-scale training initiatives, tracking training completion, and reporting out on training progress. Own New Program launch and New Hire Onboarding, performance enhancement of programs\nConsult on learning strategies and effectiveness and gather feedback to improve Learning & Development programs.\nHandle a direct span of trainers, provides regular coaching and feedback to help grow individual functional skills and leadership capability.\nCollaborate with both local and global stakeholders to support Training programs and initiatives.\nEnhance existing training programs, review and supervise the designing of training content for any new process, program and feature/SOP roll out.\nGraduation or Post Graduation in related field.\n6+ years experience working in Training and People management.\nData skills and the ability to understand how learning activities and responsibilities play into the metrics that drive team success.\nThe ability to work in fast-paced ambiguous environments, adapting quickly to changing circumstances, processes and priorities.\nDemonstrated use of multiple learning methods and linking appropriate methods with learners and outcomes.\nAbility to influence stakeholders at all levels to understand their role in employee development and help build their skills.\nDetail-oriented, team-focused, and a quick problem-solver.\nFull proficiency in MS Office\nFamiliarity with online learning technology (e.g., Articulate Story line).\nProven ability to identify opportunities and launch original learning solution(s) with real impact.\nExperience in Learning Management system and Knowledge management systems.\nExperience in driving process improvement projects.\nExperience in requirement gathering and ability to write clear and detailed requirement document",,,,"['RCA', 'Root cause analysis', 'Employee development', 'Operational excellence', 'LMS', 'Management systems', 'Process improvement', 'Machine learning', 'Training and Development', 'MS Office']",2025-06-12 15:11:36
Python Developer-PAN INDIA_RA,Infosys,3 - 6 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Responsibilities\nWrite clean, maintainable, and efficient Python code for backend services or applications. Develop RESTful APIs or work on web applications using frameworks like Django or Flask. Implement data extraction, transformation, and loading (ETL) processes using Python. Collaborate with front-end developers and other team members to ensure seamless integration. Test and debug applications to ensure they meet quality and performance standards.\nParticipate in code reviews and contribute to the development of coding standards.\nKeep up-to-date with Python libraries and tools relevant to the project.\n\nTechnical and Professional Requirements:\nPrimary skills: Technology->Machine Learning->Python\n\nPreferred Skills: Technology->Machine Learning->Python\n\nAdditional Responsibilities:\nSkills Required:\nStrong proficiency in Python 3.x.\nExperience with at least one Python web framework (Django, Flask, etc.).\nKnowledge of database technologies, including SQL and ORM (e.g., SQLAlchemy).\nFamiliarity with version control systems like Git.\nBasic understanding of front-end technologies (HTML, CSS, JavaScript).\nExperience with unit testing frameworks like pytest or unittest.\nKnowledge of REST API design principles.\nEducational Requirements\nBCA/MCA/B.Tech/BE/M.Tech/ME/BSC/MSC",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Django', 'Flask']",2025-06-12 15:11:39
Python developer - Infosys @ Pan India,Infosys,2 - 7 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Responsibilities A day in the life of an Infoscion • As part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain. • You will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements. • You will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers. • You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nTechnical and Professional Requirements: • Primary skills:Technology->Machine Learning->Python\n\nPreferred Skills: Technology->Machine Learning->Python\n\nAdditional Responsibilities: • Knowledge of design principles and fundamentals of architecture • Understanding of performance engineering • Knowledge of quality processes and estimation techniques • Basic understanding of project domain • Ability to translate functional / nonfunctional requirements to systems requirements • Ability to design and code complex programs • Ability to write test cases and scenarios based on the specifications • Good understanding of SDLC and agile methodologies • Awareness of latest technologies and trends • Logical thinking and problem solving skills along with an ability to collaborate\n\nEducational RequirementsMCA,MSc,MTech,Bachelor of Engineering,BCA,BSc,BTech",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Django', 'Python Development']",2025-06-12 15:11:41
AI/ML Engineer,Ravionics,4 - 9 years,Not Disclosed,['Bengaluru'],"Making a career change is a big decision. Why consider Revionics?\nJoin a team of remarkable colleagues who are deeply committed to creating and delivering cutting-edge solutions for the global retail market. At Revionics, we are dedicated to helping you achieve and surpass your career aspirations. Youll enjoy access to industry-leading training programs, global development opportunities, and the chance to thrive within a diverse culture spanning offices in nine countries. Our inclusive culture is rooted in our Companys purpose:\nto make a difference for every colleague, every client, every day\n.\nRevionics sets the standard in retail pricing innovation by leveraging advanced artificial intelligence technologies, including predictive AI, conversational AI, generative AI, and agentic AI. These cutting-edge tools streamline the retail pricing lifecycle, driving measurable business success for our clients. Each year, our solutions enable pricing strategies for retail products that collectively generate over $3 trillion in revenue across leading grocery, health and beauty, DIY, and convenience retailers worldwide.\nWe invite you to join us in bringing innovative solutions to market as part of the worldwide leader in retail pricing.\nAptos market-leading platform drives the world s largest retailers in terms of their product pricing, promotion and merchandising decisions worldwide. Over 33,000 retail locations and $200+B in annual revenue across grocery, drug, convenience, general merchandise, discount, sporting goods stores, fashion, and eCommerce sites optimize with Aptos solutions.\nAptos acquired Revionics in September 2020. Revionics is the worldwide leader in retail pricing, with 20 years of experience delivering AI/ML-driven retail pricing and promotions SaaS solutions for some of the largest and best-known retailers in the world, affecting over $500B revenue under management across more than 50 retailers.\nThe AI team, within the Product Org, plays a central role at the company and is responsible for the GenAI (agents, conversational analytics etc.) and Predictive AI solutions (modeling, forecasting, optimization, etc) at Revionics. As an engineer on the Science team, you will be part of a skilled and diverse team while working with a mix of data scientists and engineers. You ll not only have the opportunity to learn/use state-of-art AI/GenAI and ML techniques but also implement/roll-out modern engineering frameworks and solve problems that have not been solved before.\nIf you re someone who is ready to take on a challenge, drive change, and be part of an awesome team, this is the right role for you!\nAbout the Role:\nThe engineer will be responsible for designing, building, deploying, and evolving the end-to-end AI/ML systems at Aptos (demand modeling and forecasting, optimization, GenAI agents, etc.)\nWho you are?\nYou have a Bachelors/Master s degree in computer science, engineering, or related STEM field, or equivalent work experience\nStrong algorithmic problem-solving skills and an analytical mindset\nHunger to learn new domains and complex code bases\n4+ years of development experienced with Python or another similar language\nExperience with GCP (Kubernetes, Cloud functions, Cloud Run etc.) or similar\nExperience in containerization and container orchestration (Docker, Kubernetes, etc.)\nExperience enabling CI/CD pipelines using tools such as Gitlab, or similar\nExpertise in SQL and exposure to non-relational (MongoDB or similar)\nExposure to ML frameworks such as Tensorflow, Pytorch, Scikit-Learn, Spark, would be a plus\nAble to communicate, collaborate, and work effectively in a distributed team.\nCan think about and write high quality code and can demonstrate that capability\nEnjoy tough technical challenges and are naturally intellectually curious\nSeek to drive change and influence others through clear and effective communication.\nWhat you ll do?\nCollaborate with the cross functional teams to help design, build and deliver the headless product offering\nCollaborate with the data scientists and other ML engineers to design, build and deliver the first agentic Revionics experience\nDesign, build, test and maintain end-to-end AI forecasting, optimization and modeling services\nDive deep into the underlying infra architecture to ensure we are building the right way\nWork with product, engineers, and data scientists to translate ideas into new products, services and features\nMentor junior engineers and continually improve our technical stack and processes\nWe also look for\nPassion\nInitiative and a Pioneering Spirit\nQuality orientation\nResourcefulness and application\nAre you the person we re looking for?\nBig picture thinker with laser focus. You have a unique ability to see both the forest and the trees. It s what sets you apart from the rest. You start with a good understanding of the broader strategy, zoom in to assess one particular aspect of that strategy, and then zoom back out to see how changes to that particular area will affect the broader process.\nExpert relationship cultivator. Product managers think you re a good partner -- because you are. Developers feel you respect their opinions -- because you do. You re a true people person, a natural collaborator, and a highly sought-after resource.\nQuality orientation. You have proven success at writing quality user stories and analysis deliverables through the application of established criteria like INVEST and SMART. Your work is thoughtful, timely and valuable to the team.\nResourcefulness and application. At Aptos, we have a pioneering spirit -- when we have questions, we find answers; when we re faced with challenges, we find solutions. We turn to a variety of resources, including our own colleagues, our professional network, the Internet, articles and books -- whatever helps us get the job done. But it s not just about using a variety of resources to gain knowledge -- it s also about applying that knowledge to other areas of the job or business where it might make sense\n\n\nWe offer a competitive total rewards package including a base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility.\nWe are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. By submitting an application for this job, you acknowledge that any personal data or personally identifiable information that you provide to us will be processed in accordance with our Candidate Privacy Notice .",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'orchestration', 'GCP', 'Analytical', 'Artificial Intelligence', 'MongoDB', 'Forecasting', 'Analytics', 'SQL', 'Python']",2025-06-12 15:11:44
AI Engineer,Lericon Informatics,5 - 7 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Job Summary:\nWe are seeking a passionate and skilled AI Engineer to design, develop, and deploy cutting-edge AI solutions across domains such as large language models (LLMs), computer vision, and autonomous agent workflows. You will collaborate with data scientists, researchers, and engineering teams to build intelligent systems that solve real-world problems using deep learning, transformer-based architectures, and multi-modal AI models.\n\nKey Responsibilities:\n\nDesign and implement AI/ML models, especially transformer-based LLMs (e.g., BERT, GPT, LLaMA) and vision models (e.g., ViT, YOLO, Detectron2).\nDevelop and deploy computer vision pipelines for object detection, segmentation, OCR, and image classification tasks.\nBuild and orchestrate intelligent agent workflows using prompt engineering, memory systems, retrieval-augmented generation (RAG), and multi-agent coordination.\nFine-tune and optimize pre-trained models on domain-specific datasets using frameworks like PyTorch or TensorFlow.\nCollaborate with cross-functional teams to understand problem requirements and translate them into scalable AI solutions.\nImplement inference pipelines and APIs to serve AI models efficiently using tools such as FastAPI, ONNX, or Triton Inference Server.\nConduct model evaluation, benchmarking, A/B testing, and performance tuning.\nStay updated with state-of-the-art research in deep learning, generative AI, and multi-modal learning.\nEnsure reproducibility, versioning, and documentation of all experiments and production models.\n\nQualifications:\n\nBachelors or Masters degree in Computer Science, Artificial Intelligence, Data Science, or a related field.\n35 years of hands-on experience in designing and deploying deep learning models.\nStrong knowledge of LLMs (e.g., GPT, BERT, T5), Vision Models (e.g., CNNs, Vision Transformers), and Computer Vision techniques.\nExperience building intelligent agents or using frameworks like LangChain, Haystack, AutoGPT, or similar.\nProficiency in Python, with expertise in libraries such as PyTorch, TensorFlow, Hugging Face Transformers, OpenCV, and Scikit-learn.\nFamiliarity with MLOps concepts and deployment tools (Docker, Kubernetes, MLflow).\nStrong understanding of NLP, image processing, model fine-tuning, and optimization.\nExperience with cloud platforms (AWS, GCP, Azure) and GPU environments.\nExcellent problem-solving, communication, and teamwork skills.\n\nPreferred Qualifications:\n\nExperience in building multi-modal AI systems (e.g., combining vision + language models).\nExposure to real-time inference systems and low-latency model deployment.\nContributions to open-source AI projects or research publications.\nFamiliarity with vector databases (e.g., FAISS, Pinecone, Weaviate) and RAG pipelines.\n\nLocations : Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, India",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['AI Engineering', 'Object Detection', 'Vision Models', 'LLMs', 'AI Agents', 'LangChain', 'Hugging Face', 'Deep Learning', 'PyTorch', 'NLP', 'Transformer Models', 'Model Deployment', 'RAG', 'Computer Vision', 'TensorFlow', 'OCR']",2025-06-12 15:11:46
Python Developer -ENG - Infosys@ PAN India,Infosys,3 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Responsibilities A day in the life of an Infoscion • As part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain. • You will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements. • You will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers. • You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nTechnical and Professional Requirements: • Primary skills:Technology->Machine Learning->Python\n\nPreferred Skills: Technology->Machine Learning->Python\n\nAdditional Responsibilities: • Knowledge of design principles and fundamentals of architecture • Understanding of performance engineering • Knowledge of quality processes and estimation techniques • Basic understanding of project domain • Ability to translate functional / nonfunctional requirements to systems requirements • Ability to design and code complex programs • Ability to write test cases and scenarios based on the specifications • Good understanding of SDLC and agile methodologies • Awareness of latest technologies and trends • Logical thinking and problem solving skills along with an ability to collaborate\nEducational RequirementsMCA,MSc,MTech,Bachelor of Engineering,BCA,BSc,BTech responsibilities",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Python Development', 'Python', 'Django Framework']",2025-06-12 15:11:49
Quantitative Analytics Manager,Wells Fargo,4 - 8 years,Not Disclosed,['Bengaluru'],"In this role, you will:\nManage a team responsible for the creation and implementation of low to moderate complex financial areas\nMitigate operational risk and compute capital requirements\nDetermine scope and prioritization of work in consultation with experienced management\nParticipate in the development of strategy, policies, procedures, and organizational controls with model users, developers, validators, and technology",,,,"['Quantitative Analytics', 'strategy Planning', 'marketing', 'Git', 'GitHub', 'talent development', 'credit risk analysis']",2025-06-12 15:11:51
"Business Analyst II, FIAT SEPO",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"At Amazon.com, we strive to be Earth s most customer-centric company. To support this vision, we need exceptionally talented, bright, and driven people. If you would like to help us build the place to find and buy anything online, this is your chance to work hard, have fun, and make history.\n\n\nAn ideal candidate for this role:\nWill have relevant experience in data analytics working with large data sets and to extract and transform data using various tools and technologies\nWill transform data into actionable business information, and will make it readily accessible to stakeholders worldwide\nWill use data to support ideas, drive actionable outcomes, and provide unique ways to present data and information in an easy to consume format\nWill be passionate about finding root causes, trends, and patterns and how they impact business.\nWill draw inferences and conclusions, create dashboards and visualizations of processed data\nWill have business and communication skills to be able to work with product owners to understand key business questions to build reports that enable product owners to answer those questions quickly and accurately\n. Will be very comfortable juggling competing priorities and handling ambiguity\n. Will thrive in an agile and fast-paced environment on highly visible projects and initiatives\n\nA day in the life\nYou will be responsible for modeling forecasting problems, discovering insights and identifying opportunities through the use of statistical, machine learning, algorithmic, data mining and visualization techniques. You will need to collaborate effectively with internal stakeholders and cross-functional teams to analyze forecast variances, understand and mitigate variance drivers, identify opportunities to improve operational efficiencies, and deliver successfully against high organizational standards. You should be able to apply a breadth of tools, data sources and analytical techniques to answer a wide range of high-impact business questions and present the insights in concise and effective manner. Additionally, you should be an effective communicator capable of independently driving issues to resolution and communicating insights to non-technical audiences. This is a high impact role with goals that directly impacts the bottom line of the business. Accurate forecasts drive improvements in cost and quality of our customer service on a global scale.\n\nAbout the team\nOur team strives to make Amazon the best way for Partners to reach customers locally and globally and to operate their businesses, driven by the accurate and efficient support and solutions we provide them. We are looking for a Business Analyst for its TSE (Trustworthy Shopping Experience) FIAT SEPO. The team is being grown to provide insights and provide WFM solutions to help drive operational efficiencies, uncover the hidden risks and trends, reduce investigation errors, improve customer experience and predict & recommend the optimizations for future state. 3+ years of Excel (including VBA, pivot tables, array functions, power pivots, etc.) and data visualization tools such as Tableau experience\n3+ years of tax, finance or a related analytical field experience\n3+ years of business or financial analysis experience\nExperience defining requirements and using data and metrics to draw business insights\nExperience making business recommendations and influencing stakeholders\nExperience with Excel Experience using very large datasets",,,,"['Business Analyst', 'Financial analysis', 'Analytical', 'Machine learning', 'Agile', 'Customer service', 'data visualization', 'Customer experience', 'Data mining', 'Operations']",2025-06-12 15:11:54
AI Engineer - Lead,Blend360 India,6 - 10 years,Not Disclosed,['Hyderabad'],"We are looking for someone who is ready for the next step in their career and is excited by the idea of solving problems and designing best in class. However, they also need to be aware of the practicalities of making a difference in the real world - whilst we love innovative advanced solutions, we also believe that sometimes a simple solution can have the most impact.\nOur AI Engineer is someone who feels the most comfortable around solving problems, answering questions and proposing solutions. We place a high value on the ability to communicate and translate complex analytical thinking into non-technical and commercially oriented concepts, and experience working on difficult projects and/or with demanding stakeholders is always appreciated.\nWhat can you expect from the role?\nContribute to design, develop, deploy and maintain AI solutions\nUse a variety of AI Engineering tools and methods to deliver\nOwn parts of projects end-to-end\nContributing to solutions design and proposal submissions\nSupporting the development of the AI engineering team within Blend\nMaintain in-depth knowledge of the AI ecosystems and trends\nMentor junior colleagues\n\n\nContribute to the design, development, testing, deployment, maintenance, and improvement of robust, scalable, and reliable software systems, adhering to best practices.\nApply Python programming skills fo",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Version control', 'orchestration', 'GIT', 'GCP', 'Analytical', 'System integration', 'Software development life cycle', 'Mentor', 'Monitoring', 'Python']",2025-06-12 15:11:56
"Software Engineer(.NET, Azure, C#)",Renewable Energy Equipment Manufacturing,3 - 5 years,7-12 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nDesigning and delivering Azure API and associated data platform solutions\nProvision of data in a secure and reliable manner\nTroubleshoot and resolve issues in our dev, test and production environments.\nComfort with frequent, incremental code testing and deployment\nDelivering and presenting Proofs of Concept variants to prospective customers.\nRequirements Analysis and contribute in solution architecture design.\nDocumentation of solutions (e.g. data models, configurations, and setup).\nWorking with data developers to ensure high quality access to and supply of specified data.\nEnsuring that platforms and data solutions can be deployed and operated in a highly repeatable and predictable manner through interaction and collaboration with DevOps specialists.\nDeal with other stakeholders/ end users in the software development lifecycle\nQualifications\n2 to 7 years of hands-on experience of designing and delivering distributed cloud solutions using Microsoft Azure\nVery strong, in-depth, and demonstrable hands-on experience with the large numbers of the following technologies:\nMicrosoft Azure PaaS and SaaS solution development technologies including Azure Functions, Logic Apps, .NET, JavaScript, Python etc.\nMicrosoft Azure App Service Fabric, App Service Environment, Microsoft Azure API Management platform technologies\nJSON, REST and data based APIs and high scale performant service facades\nMicrosoft Azure Identity Management and Security technologies including custom SAML 2.0 providers\nMicrosoft Visual Studio Team System\nAzure Service Bus and Azure Notifications Hub\nAzure Artificial Intelligence and Machine Learning platforms Microsoft Azure Machine Learning, Azure Cognitive Services – would be a plus to have\nMicrosoft Azure Operational and Monitoring tools\nFamiliarity with CosmosDB, Cassandra, Mongo DB or similar technologies would additionally be very useful\nFamiliarity with any of the following would be distinct advantage: Azure Data Analytics platform (Cortana Intelligence Platform) including Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Search, Azure Databricks and Open Source technologies such Apache Spark, Atlas, Hadoop, NoSQL, Kafka, Solr\nExperience with best practice design principles and approaches for a range of application styles and technologies to help guide and steer decisions.\nExperience working with structured and unstructured data including imaging & geospatial data.\nExperience of working in highly dynamic teams using agile methodologies often under demanding timescales.\nExperience of motivating and managing team performance in delivering to agreed timelines",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['C#', 'WebAPi', 'Azure Cloud', '.Net', 'Sql', 'oops concept']",2025-06-12 15:11:58
"Manager, Applied Science, RBS Tech",Amazon,5 - 10 years,Not Disclosed,['Bengaluru'],"RBS (Retail Business Services) Tech team works towards enhancing the customer experience (CX) and their trust in product data by providing technologies to find and fix Amazon CX defects at scale. Our platforms help in improving the CX in all phases of customer journey, including selection, discoverability & fulfilment, buying experience and post-buying experience (product quality and customer returns).\n\nAs a Sciences team in RBS Tech, we focus on foundational ML research and develop scalable state-of-the-art ML solutions to solve the problems covering customer experience (CX) and Selling partner experience (SPX). We work to solve problems related to multi-modal understanding (text and visual), supervised and unsupervised techniques, multi-task learning, multi-label classification, aspect and topic extraction for Customer Anecdote Mining, product similarity, using GenAI, LLMs, NLP and Computer Vision.\n\n\nAs an Applied Science Manager, you will be responsible to design and deploy scalable GenAI, NLP and Computer Vision solutions that will impact the content visible to millions of customer and solve key customer experience issues. You will Lead scientists on the team and oversee research and development projects at various stages ranging from initial exploration to deployment into production systems. You will partner with business and engineering teams to identify and solve large and significantly complex problems that require scientific innovation. You will help the team leverage your expertise, by coaching and mentoring. You will contribute to the professional development of colleagues, improving their technical knowledge and the engineering practices. You will create the environment in the team to file for patents and/or publish research work where opportunities arise. You will impact the large product strategy, identifies new business opportunities and provides strategic direction to the team. Masters degree in Computer Science, Statistics, Electrical Engineering, or Mathematics with specialization in specialization in Machine Learning, statistical modeling, or Deep learning.\n5+ years of working experience in solving machine learning problems and deploying science solutions for large-scale applications\n2+ years of experience leading a team of scientists and engineers Knowledge of programming languages such as C/C++, Python, Java\nExcellent written and verbal communication skills Experience building machine learning models or developing algorithms for business application\nExperience building complex software systems, especially involving deep learning, machine learning and computer vision, that have been successfully delivered to customers",,,,"['Computer science', 'Mining', 'Business services', 'Product quality', 'Electrical engineering', 'Computer vision', 'C++', 'Machine learning', 'Programming', 'Python']",2025-06-12 15:12:01
Senior Software Program Manager,Nvidia,8 - 11 years,Not Disclosed,['Bengaluru'],"We are looking for Senior Technical Program Manager, to join NVIDIAs Solution Engineering team. In this role, you will work on one of our key Automotive projects. Youll find the work exciting, challenging, and meaningful. You will provide the leadership for the software support team, guide the direction of the program and coordinate with other internal teams, as well as tracking and managing program deliverables. NVIDIA gives automakers, Tier 1 suppliers, automotive research institutions, and start-ups the power and flexibility to develop and deploy breakthrough artificial intelligence systems for self-driving vehicles.\nWhat youll be doing:\nFocus on a new project, developing the next generation of Vehicle Abstraction framework\nPerform release planning, manage features, bug fixes, testing and documentation\nDrive and track software releases for new vehicle platforms and software features.\nManage risks and address issues that impact release scope, schedule, and quality\nCollaborate and communicate with program and product stakeholders\nResponsible for a successful delivery of the program while working as a team with a dedicated technical PIC, who will be helping with the technical aspects.\nWhat we need to see:\nBS/MS Computer Science or related field (or equivalent experience)\n8+ years recent Program/Project Management experience driving the planning and execution of software engineering projects and releasing commercial products.\nExcellent communication and technical presentation skills\nExperience working with a multi cross-region engineering team\nExperience handling successful releases with short release cadence in a multifaceted environment.\nYou have a consistent record of leading and successfully delivering scalable programs and projects, driving process improvements.\nShown ability to evaluate and drive adoption of new and improved process workflows in scalable organizations.\nStrong project management background with good breadth and superior organization skills.\nWays to stand out from the crowd:\nPrevious experience with Embedded/Automotive system integration\nExpertise in ASPICE and ISE 26262 safety standards\nBackground with data driven operations and defining/managing operational metrics. Understanding what makes a good/bad metric and how to drive an organization using those metrics.\nExperience in optimally leading global projects across time zones\nAgile Certification/training a plus as well as PM Certification/training desired\nNVIDIA is widely considered to be one of the technology world s most desirable employers. We have some of the most hard-working and talented people in the world working for us. If youre a creative and autonomous engineer with a real passion for technology, we want to hear from you!",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Software support', 'Project management', 'Artificial Intelligence', 'System integration', 'Agile', 'Engineering projects', 'Management', 'Operations', 'Automotive']",2025-06-12 15:12:03
"Senior Manager, Software Engineering",Diligent Corporation,10 - 15 years,Not Disclosed,['Bengaluru'],"Position Overview\nYou will work closely with technology and business teams to understand requirements, design robust architectures, and influence technology choices to deliver innovative solutions. In addition to leading SaaS software development, you will drive initiatives in Data Engineering, Data Warehousing, and Artificial Intelligence (AI). You will collaborate with principal engineers and leadership, and have opportunities to cross-collaborate with inter-disciplinary teams to solve unique challenges in the GRC & ESG domain.\nKey Responsibilities\nShape the product and technical vision for the team, collaborating with product, business, and engineering leadership across the company.\nManage and mentor a team of engineers developing highly scalable, performant, maintainable, and well-tested SaaS features.\nLead the design and implementation of modern data warehouse solutions, ensuring data quality, scalability, and security.\nOversee the integration of advanced AI and machine learning models into SaaS products to deliver intelligent features and insights.\nHire, mentor, and lead a world-class group of engineers with expertise in SaaS, data engineering, and AI.\nFoster a culture of innovation, experimentation, and continuous improvement.\nEvaluate engineering requirements and design proposals, especially in the context of data-driven and AI-powered applications.\nAssess and develop the technical performance of individual contributors within the team.\nStay current with the latest frameworks and technologies in SaaS, Data Engineering, and AI, influencing technology choices for the application stack.\nFacilitate daily stand-ups, risk identification and mitigation, dependency resolution, and follow-ups for gap closure.\nPartner with Product Management and Business teams to drive the agenda, set priorities, create project plans, and deliver outstanding products.\nRequired Experience/Skills\n10 to 15 years of relevant experience in developing enterprise SaaS applications using MERN/.NET, MySQL, MS SQL, and caching technologies.\n2+ years of experience leading engineering teams building scalable platforms and architectures, including data engineering and AI initiatives.\nProven experience designing, building, and maintaining data warehouse solutions (such as Snowflake, Redshift, or BigQuery) and data pipelines (ETL/ELT).\nHands-on experience with AI/ML frameworks (such as TensorFlow, PyTorch, or Scikit-learn) and integrating AI models into production SaaS environments.\nStrong background in data modeling, data governance, and data quality best practices.\nExperience with cloud platforms (AWS/Azure), CI/CD, DevOps, scripting, and SQL/NoSQL databases.\nDemonstrated success in migrating monolithic applications to microservices and on-premises solutions to cloud environments.\nPassion for building a data-driven culture, growing talent, and making a significant impact through technology.\nStrong communication skills for engaging with end users, technical, and business teams to gather requirements and describe product features and technical designs.\nAbility to seek clarity in ambiguous situations and drive projects to completion.\nExperience in Agile development and knowledge of Scrum and Kanban methodologies.\nSelf-motivated learner and builder with a strong customer focus and a commitment to delivering high-quality solutions.",Industry Type: Design,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'MS SQL', 'NoSQL', 'Data modeling', 'MySQL', 'Machine learning', 'Scrum', 'Data quality', 'SQL', 'Recruitment']",2025-06-12 15:12:06
Staff Engineer Gen-AI,recex,8 - 12 years,Not Disclosed,['Bengaluru'],"Job Title: Staff Engineer Gen-AI\nExperience: 8.0 Year To 10.0 Year\nCTC Salary: 50.00 LPA To 65.00 LPA\nLocation: Bengaluru/Bangalore\n\nJob Description\nBuild Gen-AI native products: Architect, build, and ship platforms powered by LLMs, agents, and predictive AI.\nStay hands-on: Design systems, write code, debug, and drive product excellence.\nLead with depth: Mentor a high-caliber team of full stack engineers.\nSpeed to market: Rapidly ship and iterate on MVPs to maximize learning and feedback.\nOwn the full stack: From backend data pipelines to intuitive UIsfrom Airflow to React from BigQuery to embeddings.\nScale what works: Ensure scalability, security, and performance in multi-tenant, cloud-native environments (GCP).\nCollaborate deeply: Work closely with product, growth, and leadership to align tech with business priorities.\nWhat You Bring\n8+ years of experience building and scaling full-stack, data-driven products\nProficiency in backend (Node.js, Python) and frontend (React), with solid GCP experience\nStrong grasp of data pipelines, analytics, and real-time data processing\nFamiliarity with Gen-AI frameworks (LangChain, LlamaIndex, OpenAI APIs, vector databases)\nProven architectural leadership and technical ownership\nProduct mindset with a bias for execution and iteration\nOur Tech Stack\nCloud: Google Cloud Platform\nBackend: Node.js, Python, Airflow\nData: BigQuery, Cloud SQL\nAI/ML: TensorFlow, OpenAI APIs, custom agents\nFrontend: React.js\n\n\nInterested professional can share Resume at harshita.g@recex.co\n\nThanks & Regards\nHarshita\nRecex",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Cloud SQL', 'Google Cloud Platform', 'OpenAI APIs', 'Node.js', 'BigQuery', 'custom agents', 'React.js', 'Python', 'TensorFlow']",2025-06-12 15:12:08
Senior Generative AI Engineer - Python Programming,Zettamine Labs,7 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a Senior Generative AI Engineer who is passionate about cutting-edge AI innovation and has significant hands-on experience in building and deploying Generative AI models. In this role, you will be responsible for designing, fine-tuning, and optimizing large language models (LLMs), implementing innovative GenAI solutions, and contributing to the architecture of AI-driven platforms that deliver real business value.\n\nYou will collaborate with cross-functional teams including data scientists, machine learning engineers, product managers, and cloud infrastructure teams to build scalable, reliable, and secure AI systems. This is a high-impact position where you will directly influence the AI roadmap and innovation strategy.\n\nKey Responsibilities :\n\n- Design, develop, and fine-tune state-of-the-art Generative AI and LLM models tailored for various business use cases.\n\n- Build, integrate, and optimize solutions using transformer-based architectures (e.g., GPT, BERT, T5, LLaMA, Mistral).\n\n- Apply techniques such as fine-tuning, prompt engineering, RLHF (Reinforcement Learning from Human Feedback), and knowledge distillation to improve model performance.\n\n- Work with vector databases (e.g., FAISS, Pinecone, Weaviate) for implementing retrieval-augmented generation (RAG) pipelines.\n\n- Develop and deploy embedding models and integrate them into LLM pipelines.\n\n- Collaborate with engineering and product teams to deploy scalable AI systems using MLOps practices and CI/CD pipelines.\n\n- Leverage LangChain, Hugging Face Transformers, OpenAI APIs, and similar frameworks/tools to accelerate development.\n\n- Optimize model performance across different environments (cloud/on-premise).\n\n- Develop end-to-end pipelines, from data preprocessing to real-time inference and monitoring.\n\n- Ensure high standards of software quality, including testing, version control, code reviews, and documentation.\n\n- Stay up to date with the latest research in Generative AI and translate breakthroughs into production-ready solutions.\n\nRequired Skills & Qualifications :\n\n- Experience : 7+ years in AI/ML, data science, or software engineering; at least 3 - 4 years in Generative AI/LLMs.\n\n- Advanced Python programming skills, including familiarity with object-oriented design and software engineering best practices.\n\n- Deep expertise in PyTorch, TensorFlow, Transformers (Hugging Face), LangChain, and OpenAI or Anthropic APIs.\n\n- Experience in LLM fine-tuning, parameter-efficient tuning methods (LoRA, PEFT), RLHF, and model evaluation.\n\n- Experience with embeddings, vector stores (FAISS, Pinecone), semantic search, and RAG systems.\n\n- Hands-on experience with AWS, GCP, or Azure; knowledge of MLOps tools (SageMaker, Vertex AI, MLflow, Kubeflow) for training, deploying, and monitoring models.\n\n- Familiarity with structured/unstructured data handling and integrating AI systems with SQL/NoSQL databases.\n\n- Strong analytical thinking, problem-solving ability, and a keen interest in research and innovation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Tensorflow', 'PyTorch', 'Generative AI', 'MLOps', 'NoSQL', 'ChatGPT', 'Artificial Intelligence', 'Data Modeling', 'LLM', 'Python', 'SQL']",2025-06-12 15:12:10
"Senior Manager, Software Engineering",Diligent Corporation,10 - 15 years,Not Disclosed,['Bengaluru'],"About Us\nDiligent is the AI leader in governance, risk and compliance (GRC) SaaS solutions, helping more than 1 million users and 700,000 board members to clarify risk and elevate governance. The Diligent One Platform gives practitioners, the C-Suite and the board a consolidated view of their entire GRC practice so they can more effectively manage risk, build greater resilience and make better decisions, faster.\nAt Diligent, were building the future with people who think boldly and move fast. Whether youre designing systems that leverage large language models or part of a team reimaging workflows with AI, youll help us unlock entirely new ways of working and thinking. Curiosity is in our DNA, we look for individuals willing to ask the big questions and experiment fearlessly - those who embrace change not as a challenge, but as an opportunity. The future belongs to those who keep learning, and we are building it together. At Diligent, you re not just building the future - you re an agent of positive change, joining a global community on a mission to make an impact.\nLearn more at diligent.com or follow us on LinkedIn and Facebook\nPosition Overview\nYou will work closely with technology and business teams to understand requirements, design robust architectures, and influence technology choices to deliver innovative solutions. In addition to leading SaaS software development, you will drive initiatives in Data Engineering, Data Warehousing, and Artificial Intelligence (AI). You will collaborate with principal engineers and leadership, and have opportunities to cross-collaborate with inter-disciplinary teams to solve unique challenges in the GRC & ESG domain.\nKey Responsibilities\nShape the product and technical vision for the team, collaborating with product, business, and engineering leadership across the company.\nManage and mentor a team of engineers developing highly scalable, performant, maintainable, and well-tested SaaS features.\nLead the design and implementation of modern data warehouse solutions, ensuring data quality, scalability, and security.\nOversee the integration of advanced AI and machine learning models into SaaS products to deliver intelligent features and insights.\nHire, mentor, and lead a world-class group of engineers with expertise in SaaS, data engineering, and AI.\nFoster a culture of innovation, experimentation, and continuous improvement.\nEvaluate engineering requirements and design proposals, especially in the context of data-driven and AI-powered applications.\nAssess and develop the technical performance of individual contributors within the team.\nStay current with the latest frameworks and technologies in SaaS, Data Engineering, and AI, influencing technology choices for the application stack.\nFacilitate daily stand-ups, risk identification and mitigation, dependency resolution, and follow-ups for gap closure.\nPartner with Product Management and Business teams to drive the agenda, set priorities, create project plans, and deliver outstanding products.\nRequired Experience/Skills\n10 to 15 years of relevant experience in developing enterprise SaaS applications using MERN/.NET, MySQL, MS SQL, and caching technologies.\n2+ years of experience leading engineering teams building scalable platforms and architectures, including data engineering and AI initiatives.\nProven experience designing, building, and maintaining data warehouse solutions (such as Snowflake, Redshift, or BigQuery) and data pipelines (ETL/ELT).\nHands-on experience with AI/ML frameworks (such as TensorFlow, PyTorch, or Scikit-learn) and integrating AI models into production SaaS environments.\nStrong background in data modeling, data governance, and data quality best practices.\nExperience with cloud platforms (AWS/Azure), CI/CD, DevOps, scripting, and SQL/NoSQL databases.\nDemonstrated success in migrating monolithic applications to microservices and on-premises solutions to cloud environments.\nPassion for building a data-driven culture, growing talent, and making a significant impact through technology.\nStrong communication skills for engaging with end users, technical, and business teams to gather requirements and describe product features and technical designs.\nAbility to seek clarity in ambiguous situations and drive projects to completion.\nExperience in Agile development and knowledge of Scrum and Kanban methodologies.\nSelf-motivated learner and builder with a strong customer focus and a commitment to delivering high-quality solutions.\nWhat Diligent Offers You\nCreativity is ingrained in our culture. We are innovative collaborators by nature. We thrive in exploring how things can be differently both in our internal processes and to help our clients\nWe care about our people. Diligent offers a flexible work environment, global days of service, comprehensive health benefits, meeting free days, generous time off policy and wellness programs to name a few\nWe have teams all over the world . We may be headquartered in New York City, but we have office hubs in Washington D.C., Vancouver, London, Galway, Budapest, Munich, Bengaluru, Singapore, and Sydney.\nDiversity is important to us. Growing, maintaining and promoting a diverse team is a top priority for us. We foster and encourage diversity through our Employee Resource Groups and provide access to resources and education to support the education of our team, facilitate dialogue, and foster understanding.\nDiligent created the modern governance movement. Our world-changing idea is to empower leaders with the technology, insights and connections they need to drive greater impact and accountability - to lead with purpose. Our employees are passionate, smart, and creative people who not only want to help build the software company of the future, but who want to make the world a more sustainable, equitable and better place.\nHeadquartered in New York, Diligent has offices in Washington D.C., London, Galway, Budapest, Vancouver, Bengaluru, Munich, Singapore and Sydney. To foster strong collaboration and connection, this role will follow a hybrid work model. If you are within a commuting distance to one of our Diligent office locations, you will be expected to work onsite at least 50% of the time. We believe that in-person engagement helps drive innovation, teamwork, and a strong sense of community.\nTo all recruitment agencies: Diligent does not accept unsolicited agency resumes. Please do not forward resumes to our jobs alias, Diligent employees or any other organization location. Diligent is not responsible for any fees related to unsolicited resumes.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'MS SQL', 'NoSQL', 'Data modeling', 'MySQL', 'Machine learning', 'Scrum', 'Data quality', 'SQL', 'Recruitment']",2025-06-12 15:12:13
Senior Software Engineer,Xoom,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Summary\nWhat you need to know about the role\nAs a Software Engineer in our Risk department, you will play a critical role in developing and maintaining cutting-edge risk detection and prevention systems that protect PayPals users and merchants from financial loss. You will work closely with cross-functional teams to design, build, and deploy scalable and efficient solutions that leverage machine learning, data analytics, and automation to identify and mitigate potential risks, ensuring the integrity of our platform and driving business growth.\n\nMeet our team\nAs an engineer in Global Fraud Risk - Automation team, You will work closely with data scientists, engineering, and analytical teams, understand the requirements and drive full development lifecycle of the teams products, transforming research work to real products. We are looking for strong technologists who are passionate about technology and able to continuously deliver state of the art software solutions in scalable way.\nJob Description\nYour way to impact\nAt PayPal, Backend Software Engineers are the architects of our global payment platform. Youll design, develop, and optimize core systems that power millions of transactions daily, directly impacting our customers experiences and our companys success.\nYour day-to-day\nAs a Senior Software Engineer - Backend, youll design and implement backend solutions. Youll collaborate with cross-functional teams to deliver high-quality products.\nDesign and develop scalable backend systems.\nOptimize system performance and reliability.\nMentor junior engineers.\nWhat do you need to bring\nBachelors degree in Computer Science or related field.\n3-5 years of backend development experience.\nProficiency in at least one backend language (Python, Java, Ruby on Rails)\nAdvanced proficiency in backend development with either Java EE frameworks, including experience with Spring MVC, or Hibernate.\nExperience designing and implementing RESTful services, focusing on scalability and reliability, using Java.\nProven ability to mentor junior engineers and contribute to code reviews and design discussions.\nExperience with cloud platforms (AWS, GCP, Azure)\nExperience with databases (SQL, NoSQL)\nStrong understanding of database design, including SQL and NoSQL databases, and experience with ORM tools.\nPreferred Qualifications\nExperience with large-scale, high-performance systems.\nKnowledge of the payment processing industry and relevant regulations.\nExperience with cloud platforms (AWS, GCP, Azure).\nContributions to open-source projects .\n**We know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please dont hesitate to apply.\nPreferred Qualification\nSubsidiary\nPayPal\nTravel Percent\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https//www.paypalbenefits.com .\nWho We Are\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. .\nBelonging at PayPal\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Hibernate', 'Automation', 'Backend', 'Database design', 'Analytical', 'Machine learning', 'Open source', 'SQL', 'Python']",2025-06-12 15:12:15
Software Engineer-C#,Definitive Healthcare,3 - 5 years,Not Disclosed,['Bengaluru'],"Analytical Wizards is part of the Definitive Healthcare family. We balance innovation with an open, friendly culture and the backing of a long-established parent company, known for its ethical reputation. We guide customers from what s now to what s next by unlocking the value of their data and applications to solve their challenges, achieving outcomes that benefit both business and society. Our people are our biggest asset, they drive our innovation advantage and we strive to offer a flexible and collaborative workplace where they can thrive. We offer industry-leading benefits packages to promote a creative and inclusive culture. If driving real change gives you a sense of pride and you are passionate about powering social good, we d love to hear from you.\nRole: Software Engineer / Senior Software Engineer / Principal Software Engineer\nOffice Location : Bangalore\nWe write our code leveraging the Microsoft stack (ReactJS, .NET Core, MS SQL Server , PostgreSQL, AWS). We re relentless, curious, and doing the right thing is a tenet of the team s approach to solving business problems. We are flexible and collaborative. We all enjoy our work and helping one another is baked into our operating DNA. We believe in Agile software development principles. We are huge proponents of SOLID principles.\nThe Software Engineer at Analytical Wizards:\nHas 3-5 years of experience in a fast-paced Agile software development environment\nEnsures software is built, according to business and technical specifications, on top of an error-free and high-performing platform\nInnovates!\nPerforms code reviews and QA on team members work items\nParticipates in Agile ceremonies\nMentors more junior members of the team\nIs always thinking of better ways to do something\nIsn t afraid to fail\nCares deeply about quality\nCan deliver high-quality software with minimal supervision\nRequired Skills:\nExperience of 3-5 years experience in C#, .NET & .NET Core (5 & above)\nSolid computer science fundamentals - OOP concepts, SOLID principles & design patterns\nExperience with any relational databases (MS SQL / MySQL /Postgres)\nExperience working with public cloud like Azure/AWS/GCP (AWS is preferred)\nExperience with containerization using Docker & microservices\nExperience developing RESTful APIs\nUnderstanding of various types of testing (unit, system, integration, performance)\nGeneral familiarity with cloud computing and serverless architectures\nPreferred Skills:\nExperience in front-end development (React, JavaScript, KendoUI)\nExposure to GraphQL (HotChocloate)\nExposure to DataDog or similar logging & monitoring tool\nFamiliarity with NoSQL databases (MongoDB /DocumentDB)\nWhy we love Analytical Wizards, and why you will too!\nIndustry leading products\nWork hard, and have fun doing it\nIncredibly fast growth means limitless opportunity\nFlexible and dynamic culture\nWork alongside some of the most talented and dedicated teammates\nA collaborative and friendly culture with very high employee engagement\nAbout Company\nCompany Name: AnalyticalWizards Services Pvt. Ltd.\nProfile: A leading, high growth Analytics Software Development company developing products that touch and positively impact human lives across the globe. We are headquartered in New Jersey and have a software development and delivery center in Bangalore. Our work is mainly focused on Healthcare Industry. We develop core data science products that help our clients draw unique insights from their big data and achieve their business goals. We use advanced algorithms in the space of artificial intelligence and machine learning. Our technology-based software products are being used by the top pharma and biotechnology companies of the world. We have been recognized twice as one of fastest growing private companies of USA.\nWork Culture: Employee-friendly, collaborative, innovative, fast-paced, and conducive to learning\nCompany Address: AnalyticalWizards Services Private Limited, Fortune Summit Business Park, Ground Floor, Hosur Road, Sector 6, HSR Layout, Roopena Agrahara, Bangalore - 560068\nIndustry: Software Development and Data Science",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cloud computing', 'MS SQL', 'data science', 'Postgresql', 'MySQL', 'Javascript', 'Agile', 'Healthcare', 'microsoft', 'Analytics']",2025-06-12 15:12:18
Financial Analyst,Watson Pharama,8 - 13 years,Not Disclosed,['Bengaluru'],"Select how often (in days) to receive an alert: Select how often (in days) to receive an alert: Jun 9, 2025 Bangalore, India, 560064\nWho we are\nThe opportunity\nIn this position, you are part of the Finance GBS Team and work closely with the Internal GBS team, COE team, Global Controlling/Finance directors, Finance Leads and Supply chain team.\nHow you ll spend your day\nApply advanced statistical methods, machine learning, and data mining techniques to analyze data from various sources\nExplore data, identify patterns and trends, and create visualizations to communicate findings effectively to stakeholders\nDevelop and maintain predictive models for various business areas, such as sales forecasting, inventory management, and customer behavior\nCollaborate with stakeholders to identify business problems, opportunities, and areas for improvement\nDesign and implement analytical solutions to address business challenges, leveraging data to support decision-making\nDeploy analytical models and track their performance, as needed to ensure continued effectiveness\nIdentify new opportunities to leverage data and analytics to drive business innovation and create competitive advantages\nEnsure that analytical solutions are aligned with overall business strategy and objectives\nEffectively communicate findings and recommendations to both technical and non-technical audiences, including executive leadership and business stakeholders\nWork with other departments, such as IT, marketing, and operations, to ensure alignment and collaboration on projects\nYour experience and qualifications\nBachelors and Masters Degree in Business, Computer Science, Information Systems, Engineering, Business/Administration, Education, Technical, Finance, MBA, Information Technology\n8+ years of experience ability to work independently taking a lead role\nProficiency in relevant software and tools, such as SQL, Python, R, or Tableau\nStrong analytical and problem-solving skills\nExpertise in advanced analytics techniques, including machine learning, statistical modeling, and data mining\nExcellent communication and interpersonal skills\nAbility to manage multiple projects simultaneously and meet deadlines\nExperience in data modeling, data warehousing, and data governance\nFlexible and able to work in a changing environment\nStrong focus on improvement opportunities\nAssoc Dir Finance Operations\nAlready Working @TEVA\nThe internal career site is available from your home network as well. If you have trouble accessing your EC account, please contact your local HR/IT partner.\nTeva s Equal Employment Opportunity Commitment\nTeva Pharmaceuticals is committed to equal opportunity in employment. It is Tevas global policy that equal employment opportunity be provided without regard to age, race, creed, color, religion, sex, disability, pregnancy, medical condition, sexual orientation, gender identity or expression, ancestry, veteran status, national or ethnic origin or any other legally recognized status entitled to protection under applicable laws. We are committed to a diverse and inclusive workplace for all. If you are contacted for a job opportunity, please advise us of any accommodations needed to support you throughout the recruitment and selection process. All accommodation information provided will be treated as confidential and used only for the purpose of providing an accessible candidate experience.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Computer science', 'Data modeling', 'Pharma', 'Analytical', 'Business strategy', 'Data mining', 'Information technology', 'Recruitment', 'SQL']",2025-06-12 15:12:20
Senior Software Engineer-C#,Definitive Healthcare,5 - 10 years,Not Disclosed,['Bengaluru'],"Analytical Wizards is part of the Definitive Healthcare family. We balance innovation with an open, friendly culture and the backing of a long-established parent company, known for its ethical reputation. We guide customers from what s now to what s next by unlocking the value of their data and applications to solve their challenges, achieving outcomes that benefit both business and society. Our people are our biggest asset, they drive our innovation advantage and we strive to offer a flexible and collaborative workplace where they can thrive. We offer industry-leading benefits packages to promote a creative and inclusive culture. If driving real change gives you a sense of pride and you are passionate about powering social good, we d love to hear from you.\nRole: Senior Software Engineer / Principal Software Engineer\nOffice Location : Bangalore\nWe write our code leveraging the Microsoft stack (ReactJS, .NET Core, MS SQL Server , PostgreSQL, AWS). We re relentless, curious, and doing the right thing is a tenet of the team s approach to solving business problems. We are flexible and collaborative. We all enjoy our work and helping one another is baked into our operating DNA. We believe in Agile software development principles. We are huge proponents of SOLID principles.\nThe Software Engineer at Analytical Wizards:\nHas 5+ years of experience in a fast-paced Agile software development environment\nEnsures software is built, according to business and technical specifications, on top of an error-free and high-performing platform\nInnovates!\nPerforms code reviews and QA on team members work items\nParticipates in Agile ceremonies\nMentors more junior members of the team\nIs always thinking of better ways to do something\nIsn t afraid to fail\nCares deeply about quality\nCan deliver high-quality software with minimal supervision\nRequired Skills:\nExperience of 5+ years experience in C#, .NET & .NET Core (5 & above)\nSolid computer science fundamentals - OOP concepts, SOLID principles & design patterns\nExperience with any relational databases (MS SQL / MySQL /Postgres)\nExperience working with public cloud like Azure/AWS/GCP (AWS is preferred)\nExperience with containerization using Docker & microservices\nExperience developing RESTful APIs\nUnderstanding of various types of testing (unit, system, integration, performance)\nGeneral familiarity with cloud computing and serverless architectures\nPreferred Skills:\nExperience in front-end development (React, JavaScript, KendoUI)\nExposure to GraphQL (HotChocloate)\nExposure to DataDog or similar logging & monitoring tool\nFamiliarity with NoSQL databases (MongoDB /DocumentDB)\nWhy we love Analytical Wizards, and why you will too!\nIndustry leading products\nWork hard, and have fun doing it\nIncredibly fast growth means limitless opportunity\nFlexible and dynamic culture\nWork alongside some of the most talented and dedicated teammates\nA collaborative and friendly culture with very high employee engagement\nAbout Company\nCompany Name: AnalyticalWizards Services Pvt. Ltd.\nProfile: A leading, high growth Analytics Software Development company developing products that touch and positively impact human lives across the globe. We are headquartered in New Jersey and have a software development and delivery center in Bangalore. Our work is mainly focused on Healthcare Industry. We develop core data science products that help our clients draw unique insights from their big data and achieve their business goals. We use advanced algorithms in the space of artificial intelligence and machine learning. Our technology-based software products are being used by the top pharma and biotechnology companies of the world. We have been recognized twice as one of fastest growing private companies of USA.\nWork Culture: Employee-friendly, collaborative, innovative, fast-paced, and conducive to learning\nCompany Address: AnalyticalWizards Services Private Limited, Fortune Summit Business Park, Ground Floor, Hosur Road, Sector 6, HSR Layout, Roopena Agrahara, Bangalore - 560068\nIndustry: Software Development and Data Science",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cloud computing', 'MS SQL', 'data science', 'Postgresql', 'MySQL', 'Javascript', 'Agile', 'Healthcare', 'microsoft', 'Analytics']",2025-06-12 15:12:23
Solution Engineer - AI Infrastructure,Cisco,10 - 15 years,Not Disclosed,['Bengaluru'],"What Youll Do\nWe are seeking a Solutions Engineer - Artificial Intelligence (AI) Practitioner to join our dynamic sales team. As an SE (AI Practitioner), you will consult and drive the adoption of our AI solutions across various industries. You will identify potential clients, understand their specific needs, and provide tailored AI solutions that enhance their business operations. This role requires a deep understanding of AI technologies, real world deployment experience and expert capability to relay business and technical concepts to a diverse audience.\nWho Youll Work With\nThe Cloud and AI Infrastructure team is responsible for helping customers change their business using Cisco technology in the data center and on cloud. The team works with customers, partners and engineering teams to take customer problems and turn them into business advantage.\nWho You Are\nYoure energized by the fast-paced landscape in IT and how AI is changing the world.You love technology and thrive in solving complex problems. You are an amazing presenter and can translate complex technical scenarios into a simple easy to understand message. You inspire those around to use technology in innovative ways and use a hands-on methods to demonstrate your ideas.\nMinimum Qualifications:\n10+ years of technology consulting experience (preferably in systems, software, data, analytics and AI).\nGood understanding of programming/scripting languages\nAbility to provide detailed and consumable documentation and standard methodologies for deployment around application acceleration, automation/management efficiencies, enterprise, and AI/ML solutions.\nBe able to develop and showcase real world examples of how AI technology can help businesses thrive and solve problems.\nExcellent presentation skills ability to value-sell and deliver engaging workshops to both technical and non-technical audiences on AI and/or infrastructure topics.\nPreferred Qualifications:\nBachelor's Degree in Computer Science, Computer Engineering, Electrical Engineering, or related field. Advanced degree in Data Science is a plus.\nExperience with AI relevant infrastructure, including Networking (InfiniBand and RoCE), Storage (FC, IP and scale out) and AI accelerators (GPUs etc).\nIn-depth understanding of AI models, including but not limited to GPT, Llama, Resnet or similar.\nExperienced with data storage and management (SQL, NOSQL, Vector, BigQuery etc) and AI/ML frameworks (scikit-learn, TensorFlow, PyTorch, Jupyter, NIMS and NVAIE etc).\nExpertise in training and fine-tuning AI models on premise or in cloud environments.\nFamiliarity with containerization (k8 etc).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'BigQuery', 'PyTorch', 'scikit-learn', 'NIMS', 'Jupyter', 'NOSQL', 'Vector', 'SQL', 'TensorFlow']",2025-06-12 15:12:25
Python Developer (3-5 Years)-RA @ Infosys,Infosys,3 - 5 years,Not Disclosed,"['Chandigarh', 'Pune', 'Bengaluru']","Responsibilities\nWrite clean, maintainable, and efficient Python code for backend services or applications. Develop RESTful APIs or work on web applications using frameworks like Django or Flask. Implement data extraction, transformation, and loading (ETL) processes using Python. Collaborate with front-end developers and other team members to ensure seamless integration. Test and debug applications to ensure they meet quality and performance standards.\nParticipate in code reviews and contribute to the development of coding standards.\nKeep up-to-date with Python libraries and tools relevant to the project.\nTechnical and Professional Requirements:\nPrimary skills: Technology->Machine Learning->Python\nPreferred Skills: Technology->Machine Learning->Python\nAdditional Responsibilities:\nSkills Required:\nStrong proficiency in Python 3.x.\nExperience with at least one Python web framework (Django, Flask, etc.).\nKnowledge of database technologies, including SQL and ORM (e.g., SQLAlchemy).\nFamiliarity with version control systems like Git.\nBasic understanding of front-end technologies (HTML, CSS, JavaScript).\nExperience with unit testing frameworks like pytest or unittest.\nKnowledge of REST API design principles.\nEducational Requirements\nMCA, MTech, Bachelor of Engineering, BCA, BSc, BTech",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Django', 'Flask']",2025-06-12 15:12:27
Hiring BTech freshers - (Full stack),Guidehouse,0 - 1 years,Not Disclosed,['Thiruvananthapuram'],"Roles and Responsibilities :\nMust be a graduate in\nBE/BTech/MCA in IT/CS or a related field.\nExcellent technical and interpersonal communication skills.\nBasic understanding of programming languages such as GoLang.\nFamiliarity with web development frameworks like React, Angular, Blazor, or Vue.js.\nKnowledge of cloud platforms such as Azure, AWS, or Google Cloud Platform.\nUnderstanding of containerization technologies like Docker.\nBasic knowledge of database management systems such as MSSQL, MySQL, PostgreSQL, or MongoDB.\nGood understanding of SDLC, STLC, Agile methodologies, and business process analysis.\nFamiliarity with RESTful APIs, microservices concepts, and DevOps practices.\nBasic understanding of security best practices in software development.\nFamiliarity with version control systems like Git.\nPreferred/Good to Have\nInternship or project experience in software development, DevOps, cloud, or related fields.\nFamiliarity with business intelligence tools (e.g., Power BI, Tableau).\nKnowledge of identity and access management solutions.\nInterest or coursework in artificial intelligence, machine learning, or data science.\nExperience with scripting languages (e.g., Bash, PowerShell).\nActive participation in technical forums (e.g., Stack Overflow) or GitHub contributions.\nExposure to UI/UX design principles.\nExperience with mobile app development (Android/iOS/Flutter/React Native) is a plus.\nAwareness of CI/CD pipelines and infrastructure as code tools (e.g., Terraform, Ansible).\nExperience working with orchestration technologies like Kubernetes.\nStrong logical, analytical, and problem-solving skills; experience in hackathons, coding challenges, or open-source contributions is a plus.\nAny certifications (e.g., Microsoft, AWS, Google, Scrum) are an added advantage.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'Golang Development', 'React.Js']",2025-06-12 15:12:29
AI/ML Engineer,Aptos Retail,20 - 25 years,Not Disclosed,['Bengaluru'],"Making a career change is a big decision. Why consider Aptos?\nBecome a part of a team that is passionate about creating and delivering cutting-edge solutions for retailers worldwide. At our company, we re dedicated to supporting your career aspirations and helping you exceed your goals. You ll benefit from industry-leading training, global development opportunities, and the chance to collaborate within a diverse culture across our offices in nine countries. Our inclusive culture reflects our purpose: to make a difference for every colleague, every client, every day .\nAs a leading provider of Unified Commerce solutions for retail, our technology empowers top retail brands by optimizing product management, promotions, merchandising, and store operations. With the global shift toward our cloud-native, microservices architecture, opportunities for career growth have never been more exciting. Today, more than 100,000 retail stores in fashion, grocery, footwear, general merchandise, discount, and sporting goods rely on our solutions to generate nearly $2 trillion in annual revenue.\nWe hope you ll join us in driving innovation and delivering impactful solutions as we continue leading the Unified Commerce revolution.\nAptos market-leading platform drives the world s largest retailers in terms of their product pricing, promotion and merchandising decisions worldwide. Over 33,000 retail locations and $200+B in annual revenue across grocery, drug, convenience, general merchandise, discount, sporting goods stores, fashion, and eCommerce sites optimize with Aptos solutions.\nAptos acquired Revionics in September 2020. Revionics is the worldwide leader in retail pricing, with 20 years of experience delivering AI/ML-driven retail pricing and promotions SaaS solutions for some of the largest and best-known retailers in the world, affecting over $500B revenue under management across more than 50 retailers.\nThe AI team, within the Product Org, plays a central role at the company and is responsible for the GenAI (agents, conversational analytics etc.) and Predictive AI solutions (modeling, forecasting, optimization, etc) at Revionics. As an engineer on the Science team, you will be part of a skilled and diverse team while working with a mix of data scientists and engineers. You ll not only have the opportunity to learn/use state-of-art AI/GenAI and ML techniques but also implement/roll-out modern engineering frameworks and solve problems that have not been solved before.\nIf you re someone who is ready to take on a challenge, drive change, and be part of an awesome team, this is the right role for you!\nAbout the Role:\nThe engineer will be responsible for designing, building, deploying, and evolving the end-to-end AI/ML systems at Aptos (demand modeling and forecasting, optimization, GenAI agents, etc.)\nWho you are?\nYou have a Bachelors/Master s degree in computer science, engineering, or related STEM field, or equivalent work experience\nStrong algorithmic problem-solving skills and an analytical mindset\nHunger to learn new domains and complex code bases\n4+ years of development experienced with Python or another similar language\nExperience with GCP (Kubernetes, Cloud functions, Cloud Run etc.) or similar\nExperience in containerization and container orchestration (Docker, Kubernetes, etc.)\nExperience enabling CI/CD pipelines using tools such as Gitlab, or similar\nExpertise in SQL and exposure to non-relational (MongoDB or similar)\nExposure to ML frameworks such as Tensorflow, Pytorch, Scikit-Learn, Spark, would be a plus\nAble to communicate, collaborate, and work effectively in a distributed team.\nCan think about and write high quality code and can demonstrate that capability\nEnjoy tough technical challenges and are naturally intellectually curious\nSeek to drive change and influence others through clear and effective communication.\nWhat you ll do?\nCollaborate with the cross functional teams to help design, build and deliver the headless product offering\nCollaborate with the data scientists and other ML engineers to design, build and deliver the first agentic Revionics experience\nDesign, build, test and maintain end-to-end AI forecasting, optimization and modeling services\nDive deep into the underlying infra architecture to ensure we are building the right way\nWork with product, engineers, and data scientists to translate ideas into new products, services and features\nMentor junior engineers and continually improve our technical stack and processes\nWe also look for\nPassion\nInitiative and a Pioneering Spirit\nQuality orientation\nResourcefulness and application\nAre you the person we re looking for?\nBig picture thinker with laser focus. You have a unique ability to see both the forest and the trees. It s what sets you apart from the rest. You start with a good understanding of the broader strategy, zoom in to assess one particular aspect of that strategy, and then zoom back out to see how changes to that particular area will affect the broader process.\nExpert relationship cultivator. Product managers think you re a good partner -- because you are. Developers feel you respect their opinions -- because you do. You re a true people person, a natural collaborator, and a highly sought-after resource.\nQuality orientation. You have proven success at writing quality user stories and analysis deliverables through the application of established criteria like INVEST and SMART. Your work is thoughtful, timely and valuable to the team.\nResourcefulness and application. At Aptos, we have a pioneering spirit -- when we have questions, we find answers; when we re faced with challenges, we find solutions. We turn to a variety of resources, including our own colleagues, our professional network, the Internet, articles and books -- whatever helps us get the job done. But it s not just about using a variety of resources to gain knowledge -- it s also about applying that knowledge to other areas of the job or business where it might make sense\n\n\nWe are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. By submitting an application for this job, you acknowledge that any personal data or personally identifiable information that you provide to us will be processed in accordance with our Candidate Privacy Notice .",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'orchestration', 'GCP', 'Analytical', 'MongoDB', 'Forecasting', 'Analytics', 'SQL', 'Python']",2025-06-12 15:12:31
Simulator Engineer,Cerebras Systems,3 - 6 years,Not Disclosed,['Bengaluru'],"Cerebras Systems builds the worlds largest AI chip, 56 times larger than GPUs. Our novel wafer-scale architecture provides the AI compute power of dozens of GPUs on a single chip, with the programming simplicity of a single device. This approach allows Cerebras to deliver industry-leading training and inference speeds and empowers machine learning users to effortlessly run large-scale ML applications, without the hassle of managing hundreds of GPUs or TPUs.\nCerebras current customers include global corporations across multiple industries, national labs, and top-tier healthcare systems. In January, we announced a multi-year, multi-million-dollar partnership with Mayo Clinic, underscoring our commitment to transforming AI applications across various fields. In August, we launched Cerebras Inference, the fastest Generative AI inference solution in the world, over 10 times faster than GPU-based hyperscale cloud inference services.\nAbout The Role\nThe Simulator team is responsible for a core internal tool that is used by many teams throughout the company to ensure the success of the next generation Cerebras WSE. The WSE is composed of an array of homogenous tiles. Each tile in composed of a compute element, that runs independent code and has access to its own memory, and a router connecting the compute element to the four neighboring tiles.\nThe core of the simulator is a cycle accurate implementation of the tile. In this mode the simulator is used for design verification work, ensuring the quality of the ASIC design and the simulator implementation. The simulator also enables an array of tile to be combined into a 2D array. In this mode the simulator is used to develop kernel algorithms, where many tiles work together to implement a distributed operation, such as matrix multiplication, or an entire neural network, such as GPT-3 .\nResponsibilities\nDevelop cycle accurate software simulators using C, C++ and Python to simulate precise system behavior of the Cerebras hardware.\nEnhance the simulator to extend to multiple architecture generations of the underlying wafer scale engine.\nSimulate and validate design verification coverage stimulus to build in functional and timing correctness into both the simulator and RTL design.\nExtend the simulator using advanced distributed compute frameworks like MPI and OpenMP to scale the simulator to a cluster of machines.\nDevelop the fabric and interface solution for the architecture simulator which is used as a primary development platform for software development.\nProfile, debug and tune the simulator software for underlying micro architectures to help scale the simulations to cover the entire extent of the wafer scale engine.\nDevelop and optimize infrastructure to interface the simulator with Cerebras test infrastructure and provide API s in Python to bridge the model to the PyTorch framework.\nOptimize the threading model and algorithms inherent to the simulator.\nSkills And Qualifications\nBachelor s or masters degree in computer science or related field, or equivalent practical experience.\nProgramming in C, C++ and Python.\nData structures and algorithms.\nDemonstrated knowledge of computer architecture and microarchitecture.\nSoftware Development using Verilog or VHDL.\nVerification of microarchitecture designs using the Universal Verification Methodology (UVM) framework.\nStrong problem solving and debugging skills.\nWhy Join Cerebras\nPeople who are serious about software make their own hardware. At Cerebras we have built a breakthrough architecture that is unlocking new opportunities for the AI industry. With dozens of model releases and rapid growth, we ve reached an inflection point in our business. Members of our team tell us there are five main reasons they joined Cerebras:\nBuild a breakthrough AI platform beyond the constraints of the GPU.\nPublish and open source their cutting-edge AI research.\nWork on one of the fastest AI supercomputers in the world.\nEnjoy job stability with startup vitality.\nOur simple, non-corporate work culture that respects individual beliefs.\n.",Industry Type: Hardware & Networking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'C++', 'VHDL', 'Verilog', 'Machine learning', 'Data structures', 'Healthcare', 'Open source', 'UVM', 'Python']",2025-06-12 15:12:34
Full Stack AI Engineer (Lead),Inclusive Business Solutions,3 - 8 years,20-35 Lacs P.A.,[],"AI specialists for Full Stack, Computer Vision, and Speech Processing roles. Responsibilities include real-time data integration, emotion recognition, and speech analysis. Must have expertise in AI frameworks, ML models, and optimization techniques.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Speech Recognition', 'Natural Language Processing', 'Computer Vision', 'Machine Learning', 'Python', 'Tensorflow', 'Large Language Model', 'Artificial Intelligence', 'AWS', 'Deep Learning']",2025-06-12 15:12:36
Scientific Business Analyst (Associate) – ELN,Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nThis role involves working closely with Amgen Research partners and Technology peers to ensure that the technology/ data needs for drug discovery research are translated into technical requirements for solution implementation. The role leverages scientific domain and business process expertise to detail product requirements as epics and user stories, along with supporting artifacts like business process maps, use cases, and test plans for the software development teams. This enables the delivery team to estimate, plan, and commit to delivery with high confidence and identify test cases and scenarios to ensure the quality and performance of IT Systems.\n\nYou will join a multi-functional team of scientists and software professionals that enables technology and data capabilities to evaluate drug candidates and assess their abilities to affect the biology of drug targets. This team implements scientific software platforms such as Laboratory Information Management Systems (LIMS) that enable the capture of lab workflows & experimental data and Electronic Lab Notebooks (ELN) that act as Amgens System of Record ensuring data integrity and business continuity. You will implement and manage scientific software platforms across the research informatics ecosystem, and provide technical support, training, and infrastructure management, and ensure it meets the needs of our Amgen Research community.\nFunction as a Scientific Business Systems Analyst within a Scaled Agile Framework (SAFe) product team\nServe as a liaison between global Research Informatics functional areas and global research scientists, prioritizing their needs and expectations\nManage a suite of custom internal platforms, commercial off-the-shelf (COTS) software, and systems integrations\nLead the technology ecosystem for in vivo study data management and ensure that the platform meets their requirements for data analysis and data integrity\nTranslate complex scientific and technological needs into clear, actionable requirements for development teams\nDevelop and maintain a product roadmap that clearly outlines the planned features and enhancements, timelines, and landmarks\nIdentify and manage risks associated with the systems, including technological risks, scientific validation, and user acceptance\nDevelop documentations, communication plans and training plans for end users\nEnsure scientific data operations are scoped into building Research-wide Artificial Intelligence/Machine Learning capabilities\nEnsure operational excellence, cybersecurity and compliance.\nCollaborate with geographically dispersed teams, including those in the US and other international locations.\nFoster a culture of collaboration, innovation, and continuous improvement.\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\n\nBasic Qualifications:\nBachelors degree with 0 - 3 years of experience in Life Sciences, Computer Science, IT, Computational Chemistry/Cheminformatics, Computational Biology/Bioinformatics or related field, OR\nDiploma with 4 - 7years of experience in Life Sciences, Computer Science, IT, Computational Chemistry/Cheminformatics, Computational Biology/Bioinformatics or related field, OR\nDemonstrated expertise in a scientific domain area and related technology needs\nExcellent problem-solving skills and a passion for tackling complex challenges in drug discovery with technology and data\nExperience with writing user requirements and acceptance criteria in agile project management systems such as JIRA\nExperience with Benchling, Revvity, IDBS, or similar LIMS/ELN platforms\nPreferred Qualifications:\nExperience with Agile software development methodologies (Scrum)\nExperience performing or enabling data capture and analysis from instruments in a research laboratory or vivarium\nAbility to communicate technical or complex subject matters in business terms\nKnowledge of business analysis standard processes, DevOps, Continuous Integration, and Continuous Delivery methodology\nExperience with cloud (e.g. AWS) and on-premise infrastructure\nExperience supporting ELN/LIMS platforms in biopharma\n\n\n\nProfessional Certifications:\nSAFe for Teams certification (preferred)\n\n\n\nSoft\n\nSkills:\nAble to work under minimal supervision\nExcellent analytical and gap/fit assessment skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business Analysis', 'LIMS platforms', 'ELN platforms']",2025-06-12 15:12:38
Security Engineer II,Tekion Corp,1 - 5 years,Not Disclosed,['Bengaluru'],"About Tekion:\nPositively disrupting an industry that has not seen any innovation in over 50 years, Tekion has challenged the paradigm with the first and fastest cloud-native automotive platform that includes the revolutionary Automotive Retail Cloud (ARC) for retailers, Automotive Enterprise Cloud (AEC) for manufacturers and other large automotive enterprises and Automotive Partner Cloud (APC) for technology and industry partners. Tekion connects the entire spectrum of the automotive retail ecosystem through one seamless platform. The transformative platform uses cutting-edge technology, big data, machine learning, and AI to seamlessly bring together OEMs, retailers/dealers and consumers. With its highly configurable integration and greater customer engagement capabilities, Tekion is enabling the best automotive retail experiences ever. Tekion employs close to 3,000 people across North America, Asia and Europe.",,,,"['Patch management', 'Computer science', 'Automation', 'Coding', 'SOC', 'Machine learning', 'Vulnerability', 'Information technology', 'Automotive', 'Python']",2025-06-12 15:12:40
AI Security Engineer,Nextiva,2 - 5 years,Not Disclosed,"['Chennai', 'Bengaluru']","Redefine the future of customer experiences. One conversation at a time.\nWe re changing the game with a first-of-its-kind, conversation-centric platform that unifies team collaboration and customer experience in one place. Powered by AI, built by amazing humans.\nOur culture is forward-thinking, customer-obsessed and built on an unwavering belief that connection fuels business and life; connections to our customers with our signature Amazing Service , our products and services, and most importantly, each other. Since 2008, 100,000+ companies and 1M+ users rely on Nextiva for customer and team communication.\nIf you re ready to collaborate and create with amazing people, let your personality shine and be on the frontlines of helping businesses deliver amazing experiences, you re in the right place.\nBuild Amazing - Deliver Amazing - Live Amazing - Be Amazing\nThe AI Security and Compliance Engineer is responsible for working with development and compliance teams to ensure secure and compliant AI development throughout the product lifecycle. The engineer applies knowledge of AI and application security risks and threats to design and implement appropriate, cost-effective security controls during development, deployment, and operation of AI based applications. The engineer defines and promotes the implementation guidelines for data classification, segregation, and access controls to AI model inputs and training data to ensure data confidentiality and privacy for different data sources and user groups. The engineer performs audits and vulnerability assessments, penetration testing and supports mitigation of findings.\nKey Responsibilities:\nEnsure AI products have security and privacy by design.\nEstablish and document policies and guidelines for data classification and data used for training to prevent leaks of sensitive data.\nWork with development and compliance teams to ensure secure and compliant AI development throughout the product lifecycle to meet customer, regulatory, and contractual obligations.\nMonitor and audit AI systems and development processes for compliance with policies, regulations and contractual obligations.\nMonitor and respond to security incidents involving AI systems.\nCreate AI-specific incident management procedures to address AI related security incidents.\nEnhance the resilience of AI systems against potential threats by implementing cyber security best practices, controls, and tools to protect AI models from threats such as those in the OWASP AI Top Ten, including supply chain and model poisoning threats and attempts to access, modify, and exfiltrate confidential information via the query interface.\nEstablish policies and guidelines for access controls, limitations and guardrails on usage and prompts for AI inputs and API s.\nEnsure proper access controls on API s and processing pipelines, and segregation of data.\nCreate, update, and maintain threat models for a wide variety of software projects.\nProvide AI security training for internal development teams.\nMaintain current knowledge of AI risks, threats, and AI testing tools and techniques.\nPerform other duties to support the technical and operational security of the organization as required.\nQualifications:\nBachelor s degree in an IT related field or equivalent experience and 2-5 years of experience in working in IT security, software development, or AI development.\nDesired certifications - one or more of the following: CISSP (Certified Information Systems Security Professional), Certified Information Security Manager (CISM), SSCP (Systems Security Certified Practitioner), CCSP (Certified Cloud Security Professional) or CompTIA Security+.\nUnderstanding of Application Security and Data Security for applications and AI, such as the OWASP Top 10 and the OWASP Top 10 for Generative AI.\nProficiency in and strong working knowledge of AI technologies and models such as Llama and ChatGPT.\nExperience and understanding of threats and risks related to web applications and API s, particularly with AI based applications.\nGeneral knowledge of security implications of threats and vulnerabilities related to networks, servers, operating systems, applications, and databases.\nExperience with vulnerability management, patching, and mitigation assessment.\nExperience working within and implementing policies for a security framework such as ISO 27001 and NIST.\nFlexibility to work off-hours to support global project teams and maintenance windows.\nAbility to support 24x7 on-call for incident response on a rotating basis.\nExperience developing software, scripting and using SQL queries to automate controls, processes and reporting.\nCompetencies:\nStrong analytical problem-solving skills and attention to detail.\nOrganization, Time Management & Prioritization - Self-starter that focuses on key priorities; plans, organizes, schedules and executes on tasks and projects in an efficient and productive manner.\nAbility to form productive relationships across the organization to accomplish information security objectives.\nAbility and willingness to learn all aspects of the information security field.\nProfessional verbal and written communication skills in English.\nExpresses ideas using clear, effective and efficient language. Listens patiently and attentively. Adapts to the purpose of the communication with appropriate style, substance, detail, confidence and channel. Possess the ability to manage multiple channels of communication simultaneously; phone, email, tickets, and chat.\nAble to assess, document, and prioritize identified security flaws and vulnerabilities based on risk.\nTotal Rewards\nOur Total Rewards offerings are designed to allow our employees to take care of themselves and their families so they can be their best, in and out of the office.\nOur compensation packages are tailored to each role and candidates qualifications. We consider a wide range of factors, including skills, experience, training, and certifications, when determining compensation. We aim to offer competitive salaries or wages that reflect the value you bring to our team. Depending on the position, compensation may include base salary and/or hourly wages, incentives, or bonuses.\nMedical - Medical insurance coverage is available for employees, their spouse, and up to two dependent children with a limit of 500,000 INR, as well as their parents or in-laws for up to 300,000 INR. This comprehensive coverage ensures that essential healthcare needs are met for the entire family unit, providing peace of mind and security in times of medical necessity.\nGroup Term & Group Personal Accident Insurance - Provides insurance coverage against the risk of death / injury during the policy period sustained due to an accident caused by violent, visible & external means.\nCoverage Type - Employee Only\nSum Insured - 3 times of annual CTC with minimum cap of INR 10,00,000\nFree Cover Limit - 1.5 Crore\nWork-Life Balance - 15 days of Privilege leaves per calendar year, 6 days of Paid Sick leave per calendar year, 6 days of Casual leave per calendar year. Paid 26 weeks of Maternity leaves, 1 week of Paternity leave, a day off on your Birthday, and paid holidays\nFinancial Security - Provident Fund & Gratuity\nWellness - Employee Assistance Program and comprehensive wellness initiatives\nGrowth - Access to ongoing learning and development opportunities and career advancement\nAt Nextiva, were committed to supporting our employees health, well-being, and professional growth. Join us and build a rewarding career!\nEstablished in 2008 and headquartered in Scottsdale, Arizona, Nextiva secured $200M from Goldman Sachs in late 2021, valuing the company at $2.7B.To check out what s going on at Nextiva, check us out on Instagram , Instagram (MX) , YouTube , LinkedIn , and the Nextiva blog .\n#LI-RQ1 #LI-Hybrid",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Cism', 'Testing tools', 'Information security', 'ISO 27001', 'Healthcare', 'Incident management', 'Application security', 'Windows', 'Penetration testing']",2025-06-12 15:12:43
Software Engineer - C | Python | Linux | Platform Infrastructure,Cisco,9 - 12 years,Not Disclosed,['Bengaluru'],"you will:\nDevelop and integrate products deployed by leading service providers worldwide.\nCollaborate with a vibrant, BU-wide technical community to exchange ideas and innovate on next-generation technology.\nExplore opportunities for personal growth while mentoring colleagues and working on cutting-edge technologies.\nAs a key member of this team, you will:\nWork alongside seasoned engineers to architect, design, and develop some of routers and solutions for the world's largest service provider, web centers, and enterprises.\nContribute to the evolution of these systems to support exciting new customer business paradigms.\nInteract and collaborate with some of the finest talent in the industry, making work both fun and challenging.\nEngage with other groups such as Product Management, Marketing, Sales, Customer Support, and Advanced Services.\nWho You Are:\nYou possess:\nIn-depth knowledge of C and a solid understanding of Python.\nExtensive experience in a Unix/Linux-based development environment.\nExcellent coding, automation, and debugging skills.\nStrong teamwork and communication skills.\nFamiliarity with hardware architectures such as PCI, PCIe, DMA, I2C, SPI, NPUs/DPUs and processors like x86, AMD, and ARM. Experience with board bringup is a plus.\nExperience with emerging technologies such as AI/ML and cloud computing is a plus.\nExperience and Qualifications:\nExperience: 9 to 12 years in embedded firmware development.\nEducation: BE/B.Tech/ME/M.Tech/MS in CS/EE/IT/ECE, MCA, or similar education.\nProven ability to derive design and code based on technical standards and write comprehensive, focused design documents.\nExperience in developing software/firmware for networking equipment.\nExcellent knowledge of software architecture and system design.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'networking', 'artificial intelligence', 'sales', 'linux', 'pci', 'debugging', 'software engineering', 'i2c', 'cloud computing', 'arm', 'firmware', 'ml', 'board bringup', 'c', 'embedded firmware development', 'software development', 'system design', 'spi', 'marketing', 'x86', 'embedded c', 'dma', 'pcie', 'unix']",2025-06-12 15:12:45
Mlops Engineer,Rarr Technologies,8 - 13 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']",Key Responsibilities\nResponsible for building and maintaining robust machine learning pipelines ensuring efficient model deployment monitoring and lifecycle management within a cloud-based environment\nExtensive expertise in MLOps specifically with Google Cloud Platform GCP and Vertex AI and a deep understanding of model performance drift detection and GPU accelerators\nBuild and maintain scalable MLOps pipelines in GCP Vertex AI for endtoend machine learning workflows\nManage the full MLOps lifecycle from data preprocessing model training and deployment to model monitoring and drift detection\nImplement realtime model monitoring and drift detection to ensure optimal model performance over time\nOptimize model training and inference processes using GPU accelerators and CUDA\nCollaborate with cross functional teams to automate and streamline machine learning model deployment and monitoring\nUtilize Python 310 with libraries such as pandas NumPy and TensorFlow to handle data processing and model development\nSet up infrastructure for continuous training testing and deployment of machine learning models\nEnsure scalability security and high availability in all machine learning operations by implementing best practices in MLOps\nRequirements\n5 years of experience in MLOps and building ML pipelines 3 years of experience in GCP Vertex AI\nDeep understanding of the MLOps lifecycle and automation of ML workflows\nProficient in Python 310 and related libraries such as pandas NumPy and TensorFlow\nStrong experience in GPU accelerators and CUDA for model training and optimization\nProven experience in model monitoring drift detection and maintaining model accuracy over time\nStrong problemsolving skills with the ability to work in a fast paced environment\nKnowledge of data versioning and model version control techniques\nFamiliarity with TensorFlow Extended TFX or other ML workflow orchestration frameworks,Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['GCP', 'vertexai', 'mlops']",2025-06-12 15:12:47
Senior Engineering Manager,Product Base Company,12 - 18 years,Not Disclosed,['Bengaluru'],"About Client:\n\nOur Client is revolutionizing how the world plans, builds, and manages infrastructure projects with Masterworks, our industry-leading enterprise SaaS platform. Trusted by over 300 customers managing $300 billion in capital programs, Masterworks is setting new standards for project delivery and asset management. Recognized as one of the Top 25 AI Companies of 2024 and a Great Place to Work for three consecutive years, we are leveraging artificial intelligence to create a smarter, more connected future for customers in transportation, water and utilities, healthcare, higher education, and the government, with over 40,000 projects across North America. Our Client dont just develop softwareThey shape the future. If youre excited to join a fastgrowing company and collaborate with some of the brightest minds in the industry to solve realworld challenges, lets connect.\n\n\nJob Summary :\n\nThe Senior Engineering Manager will lead multiple engineering teams responsible for designing, developing, and scaling software products using Microsoft technologies. This role combines strong leadership capabilities with deep technical expertise, particularly in C#, ASP.NET, .NET Core, SQL Server, and IIS. The Senior Engineering Manager will set the technical direction, ensure engineering excellence, and collaborate with cross-functional teams to deliver high-quality, scalable solutions that align with business goals.\n\nKey Responsibilities\n\n1. Technical Leadership & Strategy:\n\nLead the development and implementation of scalable software solutions, with a strong focus on Microsoft technologies (C#, ASP.NET, .NET Core, SQL Server, IIS).\n\no Define the technical strategy and roadmap, ensuring alignment with overall product and business objectives.\n\no Provide hands-on technical guidance to engineering teams, including architecture, design, and code reviews, to ensure high-quality, scalable solutions.\n\n2. Engineering Excellence: o Establish best practices for software development, focusing on clean code, maintainability, performance, and security, especially for Microsoft stack-based solutions.\n\no Drive innovation in technology choices and design patterns, ensuring efficient use of C#, ASP.NET Core, SQL Server, and other key Microsoft frameworks.\n\no Collaborate with the infrastructure team to optimize the deployment and performance of applications on IIS and cloud environments like AWS or Azure.\n\n3. Project & Delivery Management:\n\no Oversee the execution of multiple software development projects, ensuring that engineering teams are aligned with the technical roadmap.\n\no Ensure effective sprint planning, task prioritization, and on-time delivery of projects using Agile methodologies, with a focus on .NET technologies",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['C#', 'Engineering Manager', 'ASP.NET', 'roadmap', '.NET Core', 'design', 'code reviews', 'SQL Server']",2025-06-12 15:12:51
AI Engineer,Shashwath Solution,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary:\nWe are seeking a highly skilled and hands-on AI Engineer with 3+ years of proven experience in building and deploying solutions using Generative AI and Large Language Models (LLMs). You will work on cutting-edge applications leveraging transformer-based architectures, fine-tuning, prompt engineering, and scalable AI deployments.\n\nThis role is ideal for engineers passionate about AI research and real-world productization of generative AI technologies.\n\nKey Responsibilities:\nDesign, develop, and deploy solutions using LLMs (e.g., GPT, LLaMA, Mistral, Claude, PaLM, etc.) for various NLP and content generation tasks.\n\nWork on fine-tuning, prompt engineering, and retrieval-augmented generation (RAG) pipelines.\n\nIntegrate LLMs into enterprise applications with APIs and orchestrate workflows using Python, LangChain, or similar frameworks.\n\nOptimize model performance, latency, and cost for production use.\n\nCollaborate with data scientists, MLOps engineers, and product managers to deliver scalable AI features.\n\nConduct experiments, analyze results, and publish internal findings or contribute to whitepapers.\n\nEnsure ethical, secure, and responsible use of AI technologies in all implementations.\n\nRequired Skills & Experience:\n3+ years of hands-on experience working with Generative AI, LLMs, and NLP technologies.\n\nStrong programming skills in Python and experience with libraries like Transformers (Hugging Face), LangChain, PyTorch, TensorFlow, etc.\n\nProven track record of fine-tuning LLMs, developing embeddings, and working with vector databases (e.g., FAISS, Pinecone, Weaviate).\n\nExperience deploying models on cloud platforms (AWS, Azure, GCP) and using ML pipelines or MLOps tools.\n\nSolid understanding of deep learning, NLP architectures, tokenization, and evaluation metrics for generative models.\n\nExperience in API development and integration of LLMs into user-facing applications.\n\nPreferred Qualifications:\nMasters or PhD in Computer Science, AI/ML, Data Science, or related field.\n\nExperience with OpenAI APIs, Anthropic, Cohere, or open-source LLMs (e.g., Mistral, Falcon, LLaMA 3).\n\nUnderstanding of RLHF (Reinforcement Learning from Human Feedback) and model alignment techniques.\n\nContributions to open-source AI projects or publications in GenAI/LLM.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Generative AI', 'LLaMA', 'GPT', 'Large Language Models', 'Azure', 'LangChain', 'Hugging Face', 'deep learning', 'OpenAI API', 'NLP', 'PyTorch', 'MLOps', 'GCP', 'PaLM', 'AWS', 'Python', 'TensorFlow']",2025-06-12 15:12:54
Staff Systems Integration Engineer,Analog Devices,6 - 11 years,Not Disclosed,['Bengaluru'],"About Analog Devices\nAnalog Devices, Inc. (NASDAQ: ADI ) is a global semiconductor leader that bridges the physical and digital worlds to enable breakthroughs at the Intelligent Edge. ADI combines analog, digital, and software technologies into solutions that help drive advancements in digitized factories, mobility, and digital healthcare, combat climate change, and reliably connect humans and the world. With revenue of more than $9 billion in FY24 and approximately 24,000 people globally, ADI ensures todays innovators stay Ahead of Whats Possible . Learn more at www.analog.com and on LinkedIn and Twitter (X) .\nAbout Us\nCome develop technology solutions that solve planetary-scale problems in the Analog Devices Battery Management business unit. The electric vehicle and green-energy revolution is here, and we are changing the world with our industry-leading technology. From the high performance and thrill that EVs deliver to the carbon-footprint reduction that makes the world a greener place, we are looking for more innovators to help make it all a reality. Whether you are a car-lover, a battery-geek, or a technologist with a passion for a cleaner and more sustainable future, the ADI Battery Management business unit is a premier career destination with a bright future.\nAnalog Devices Electrification Group is focused on developing world-class solutions for battery management for electro-mobility and stationary energy storage.\nThe Role\nVerification and Validation (V&V) is about providing objective evidence that the system/product, when in use, fulfills the requirements in the intended operating environment. Algorithms are cross-functional in nature, and the verification of these solutions is as critical as their design. This role requires excellent system understanding, system verification thinking, and close teamwork with hardware and software design, algorithm design, systems application, system architecture, and quality teams.\nResponsibilities:\nDrive the verification and validation effort of our key programs.\nWork with algorithm engineers and Embedded SW to understand requirements, specifications, and implementations to prepare and execute suitable verification and validation plans.\nDefine a long-term verification and validation roadmap of capabilities, flows, and methodologies to continuously improve productivity.\nCreatively plan to test devices in corner cases where potential marginalities and issues could arise beyond the given specifications.\nConsult on required hardware system boards and software to automate test cases to achieve the best coverage at the highest efficiency.\nQualifications\nMinimum BS in Electrical or Computer Engineering; MSEE or MSCE and 6+ years of experience preferred.\nExperience and Skills\nExperience in verifying complex algorithms such as battery state estimation and health algorithms (SoC, SoH, SoP) for automotive or industrial applications.\nUnderstanding the working theory of lithium-ion batteries and familiarity with new chemistries.\nExperience with machine learning and/or statistical analysis methods.\nProficiency in C/C++ (embedded) and Python or other programming/scripting languages.\nProficiency in MATLAB/Simulink\nBackground in Systems Engineering and/or formal testing of complex systems (e.g., automotive).\nExcellent hardware and software troubleshooting skills.\nExperience with software compiler/debug tools, such as IAR, Keil, or Segger.\nExperience with software revision control, repositories, and regression testing (e.g., Git, Bitbucket, SVN).\nAbility to work in teams and collaborate effectively with people in different functions.\nClear communicator with excellent verbal, written, and organizational skills.\nMotivated, proactive, fast learner, and hands-on.\nNice to Have\nBackground in automated software and system verification.\nExperience with requirements management tools (e.g., JAMA, DOORs).\nExperience with ARM-based microcontrollers (Cortex-Mx or Cortex-Rx series).",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['C++', 'Software design', 'Semiconductor', 'Analog', 'SOC', 'Healthcare', 'Simulink', 'MATLAB', 'Automotive', 'Python']",2025-06-12 15:12:56
Java (Backend) Engineer,Service based Top B2C/B2B MNC in Analyti...,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Job Title: Java (Backend) Engineer\nLocation: Bangalore & Chennai\nWork Mode: Hybrid (Work from Client Office)\nJob Description:\nWe are looking for a Java Backend Engineer to join our dynamic team. The ideal candidate will have solid experience in back-end development using Java, Microservices architecture, and Spring Boot framework. You will be responsible for developing robust server-side logic, building scalable APIs, and ensuring application performance and responsiveness.\nMust-Have Skills:\nStrong experience in Java programming (Back-end Server & SDK Development)\nHands-on experience with Spring Boot, Microservices, and RESTful APIs\nFamiliar with tools such as Git, Maven, and Jenkins\nSolid understanding of system reliability, availability, scalability, and performance\nBasic working knowledge of SQL\nGood to Have:\nExposure to database design and optimization\nKnowledge of front-end basics and mobile-responsive design\nRoles and Responsibilities:\nControl all technical, functional, and visual aspects of software in development\nDesign and develop robust, scalable server-side architecture\nDevelop and maintain well-functioning databases and back-end logic\nWrite, test, and maintain REST APIs\nTroubleshoot, debug, and upgrade applications\nOptimize software for performance and responsiveness\nCollaborate with frontend developers, analysts, and data scientists\nImplement security and data protection best practices\nCreate and maintain technical documentation\nEducation:\nBachelors degree in Computer Science, Software Engineering, MIS, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spring Boot', 'Microservices', 'Restfull Api', 'Msql', 'Orcale']",2025-06-12 15:12:59
Solution Design Lead & implimantation,Excellerate Global Solutions,13 - 23 years,10-20 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Solution Design & Implementation:\nLead and participate in the full project lifecycle of SAP S/4HANA Public Cloud implementations, focusing on Procurement (Sourcing & Procurement/MM), Finance (FI/CO), and Sales & Distribution (SD) modules.\nConduct in-depth business process analysis, gather requirements, and translate them into robust and scalable SAP S/4HANA Public Cloud solutions aligned with SAP best practices.\nDesign, configure, and customize SAP S/4HANA Public Cloud functionalities for Order-to-Cash (O2C), Procure-to-Pay (P2P), Record-to-Report (R2R), and other relevant cross-functional processes.\nEnsure seamless integration between Procurement, Finance, and SD modules, as well as with other SAP Cloud modules (e.g., EWM, PP) and third-party applications where applicable.\nLeverage SAP Activate methodology for project delivery, guiding clients through fit-to-standard workshops and solution design.\nFunctional Expertise:\nProcurement (MM): Expertise in Material Master, Vendor Master, Purchase Requisitions, Purchase Orders, Contracts, Sourcing, Inventory Management, Invoice Verification, and supplier collaboration.\nFinance (FI/CO): Strong knowledge of General Ledger, Accounts Payable, Accounts Receivable, Asset Accounting, Bank Accounting, Cost Center Accounting, Profit Center Accounting, Internal Orders, Product Costing, Profitability Analysis (CO-PA), and treasury functions. Understanding of the Universal Journal (ACDOCA) and its impact.\nSales & Distribution (SD): Proficiency in Sales Order Management, Pricing, Delivery Processing, Billing, Credit Management, Returns Management, and ATP (Available-to-Promise).\nTechnical Acumen (Public Cloud Specific):\nUnderstanding of SAP S/4HANA Public Cloud architecture, standard scope, extensibility options (e.g., in-app extensibility, side-by-side extensions using SAP BTP).\nFamiliarity with SAP Fiori applications and user interfaces for relevant modules.\nKnowledge of data migration strategies and tools within the Public Cloud environment (e.g., Migration Cockpit).\nExperience with SAP Cloud ALM for implementation, operations, and monitoring.\nClient Engagement & Leadership:\nAct as a trusted advisor to clients, effectively communicating complex technical and functional concepts to both business and IT stakeholders.\nLead workshops, facilitate discussions, and drive decisions throughout the project lifecycle.\nProvide expert guidance on cloud transformation strategies, change management, and user adoption.\nMentor and guide junior consultants, fostering a culture of knowledge sharing and continuous improvement.\nTesting, Training & Support:\nDevelop and execute comprehensive test plans (unit, integration, UAT) to ensure the solution meets business requirements and is defect-free.\nPrepare detailed training materials and conduct engaging training sessions for end-users.\nProvide post-implementation support, troubleshoot issues, and drive resolution in collaboration with technical teams.\nContinuous Improvement & Innovation:\nStay updated with the latest SAP S/4HANA Public Cloud releases, functionalities, and industry best practices.\nIdentify opportunities for process optimization and leverage new SAP innovations (e.g., AI, Machine Learning capabilities within S/4HANA) to enhance client value.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Sap Hana', 'Solution Design', 'SAP FICO', 'SAP SD', 'SAP MM', 'SAP Finance']",2025-06-12 15:13:01
Cloud Senior Automation Engineer,Enphase Energy,5 - 10 years,Not Disclosed,['Bengaluru'],"Description\nEnphase Energy is a global energy technology company and leading provider of solar, battery, and electric vehicle charging products. Founded in 2006, Enphase transformed the solar industry with our revolutionary microinverter technology, which turns sunlight into a safe, reliable, resilient, and scalable source of energy to power our lives. Today, the Enphase Energy System helps people make, use, save, and sell their own power. Enphase is also one of the fastest growing and innovative clean energy companies in the world, with approximately 68 million products installed across more than 145 countries.\nWe are building teams that are designing, developing, and manufacturing next-generation energy technologies and our work environment is fast-paced, fun and full of exciting new projects.\nIf you are passionate about advancing a more sustainable future, this is the perfect time to join Enphase!\nAbout the role:\nThe Enphase cloud team is looking for a Sr Quality Engineer for its software products . In this role, you will work with product managers, business owners and developers to assess the quality of software products at par with best-in-class solutions in the market.\nYou will be responsible for analyzing and making recommendations for improving the quality of our cloud services and applications across our ecosystem. You and your team will be involved in every part of the product cycle, from high-level design to development and validation.\nWhat you will do:\nOwn the complete quality ownership of the system under test .\nAble to design test cases that are of high quality which helps improving quality of the system .\nHelp team to design good test cases, review their work and give feedback .\nDesign test framework for different layers of the application.\nAdd automated tests on web services, database and UI layer.\nBe the gate keeper on the quality of the product .\nDefine, write, and execute non-functional tests and analyze results.\nDebug the issues and help developers to design the best software product.\nDrive quality metrics on system under test, be responsible for quality of the system.\nAnalyse the tests, progress of the tests and report to the management on the quality of the system on day-to-day basis .\nImprovise the test case designs, negative tests, repeatability tests, data validation, impact and risk-based testing.\nAble to work on agile, early to market without any compromise on quality .\nAble to work on continuous development and continuous testing\nHelp team to setup pipeline which executes automated test based on schedule, on-demand\nWho you are and what you bring:\nBE/BTech in Computer Science, Computer Engineering, or equivalent experience .\n5+ years of experience in software development/testing.\n2+ years of experience testing high scale applications with distributed architecture.\nExcellent programming skills and ability to debug issues at the code level, development .experience is an added advantage.\nExperience in negative testing.\nProvide out of box solution to deliver quality in a fast-paced development organization.\nStrong understanding of cloud, databases, and performance tradeoffs .\nGo-getter attitude and capable of pursuing tough engineering challenges to final solutions.\nLean / Agile attitude .\nProactive and results-oriented .\nTeam player/builder; possess a can-do attitude.\nGood at stakeholder management.\nExperience in providing estimates, plan and committed to deliverables.\nStrong technical and leadership skills: decisive, determined, strategic, and able to lead, motivate, and inspire others.\nExperience as software technical lead/architect with cloud and digital SW is a plus.\nKnowledge of data analytics and machine learning are a plus.",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data validation', 'Machine learning', 'Cloud', 'Agile', 'Test cases', 'High level design', 'Stakeholder management', 'Testing']",2025-06-12 15:13:03
Senior QA Automation Engineer,Luxoft,6 - 11 years,Not Disclosed,['Bengaluru'],"Lead the design and implementation of robust, scalable test automation frameworks.\nDevelop and maintain automated test scripts for functional, regression, and integration testing.\nCollaborate with cross-functional teams including developers, BAs, and DevOps to ensure high-quality releases.\nDrive test strategy, planning, and execution for Murex-related projects.\nMentor junior QA team members and enforce best practices in test automation and quality assurance.\nParticipate in code reviews, CI/CD pipeline integration, and test data management.\nSupport UAT and production validation efforts.\nSkills\nMust have\n6+ years of experience in software testing with at least 4+ years in automation.\nStrong hands-on experience with test automation tools such as Selenium, TestNG, Cucumber, RestAssured, or similar.\nProficiency in Java/Python or other scripting languages used in automation.\nExperience in building and maintaining custom automation frameworks.\nSolid understanding of Murex architecture, trade lifecycle, Trade insertion, E2E deal flow (FO, BO, Confo and settlements).\nStrong knowledge of SQL and database validation.\nExperience with CI/CD tools like Jenkins, Git, Maven, or similar.\nNice to have\nWorking knowledge of Xceptor for data transformation and reconciliation.\nExperience with performance testing tools (e.g., JMeter, LoadRunner) is a plus.\nExposure to cloud platforms (AWS, Azure) and using Machine Learning skills\nISTQB Advanced Level or equivalent certification.\nStrong leadership and mentoring capabilities.\nExcellent communication and stakeholder management skills.\nAnalytical mindset with a proactive approach to problem-solving.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Maven', 'Manager Quality Assurance', 'Data management', 'Testing tools', 'Performance testing', 'Selenium', 'Murex', 'Testing', 'SQL', 'Python']",2025-06-12 15:13:06
Senior Research Engineer - LLM,Trask,10 - 15 years,Not Disclosed,['Bengaluru'],"Develop, execute, implement and check methods, plans, toolsets and approaches that are appropriate and compliant to achieve digital solutions for the process intended\nIdentify technical problems and apply/integrate solutions as needed with a set based design approach\nAnalyse customer requirements and define technical solutions as input for proposals to develop proof of concepts\nWork with cross functional teams and multiple sites during the development process\nGenerate assigned project deliverables, documents, and reports according to the project milestones\nSupport and participate in technical reviews, including the creation and preparation of technical data and presentations as needed and support other engineers with peer to peer reviews\nSupport Lean culture and improvement initiatives in the organisation\nTake part in regular sprint planning meetings to plan, review and deliver outputs based on agile philosophy\nSupport in creation of training material and knowledge sharing in the relevant area of work\nSupport idea generation and CI activities\nPerform all activities independently and help other engineers within the program as required.\nBachelors in Engineering or higher, with minimum of 10 years of relevant experience Automation & Software development.\nDesign data pipelines to handle large-scale data for training, ensuring data security and compliance with aerospace and defence standards.\nExcellent experience in shop floor automation and I4.0/IOT Integration\nExcellent Understanding of Industrial Communication protocols and establishing communication between different Industrial systems.\nGood knowledge of data structure, data modelling and database architecture\nGood Knowledge of implementing business process into functional codes\nExcellent knowledge of software coding , integrated development platforms\nProficiency in programming with python, C++,C, C#, Java, .NET, VB, SQL and working knowledge in GIT\nAbility to conduct POCs and guide team members to extract valuable insights and drive data-driven decision-making.\nEvaluate and select appropriate tools and applications for tasks.\nStrong software development skills, including version control (e.g., Git), debugging, testing, and documentation. Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes) is beneficial.\nStay updated with latest developments in Automation, Software Development, NLP, ML, AI, LLM technology around the globe relevant to aerospace and defence sector and work along with team to quickly leverage, test and validate new solutions applicable to the working projects by applying cutting edge technologies.\nShould have strong problem-solving skills and ability to collaborate with cross-functional teams, including domain experts, data scientists, and engineering teams, to gather requirements and translate them into scalable applications.\nBachelors in Engineering or higher, with minimum of 10 years of relevant experience Automation & Software development.\nDesign data pipelines to handle large-scale data for training, ensuring data security and compliance with aerospace and defence standards.\nExcellent experience in shop floor automation and I4.0/IOT Integration\nExcellent Understanding of Industrial Communication protocols and establishing communication between different Industrial systems.\nGood knowledge of data structure, data modelling and database architecture\nGood Knowledge of implementing business process into functional codes\nExcellent knowledge of software coding , integrated development platforms\nProficiency in programming with python, C++,C, C#, Java, .NET, VB, SQL and working knowledge in GIT\nAbility to conduct POCs and guide team members to extract valuable insights and drive data-driven decision-making.\nEvaluate and select appropriate tools and applications for tasks.\nStrong software development skills, including version control (e.g., Git), debugging, testing, and documentation. Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes) is beneficial.\nStay updated with latest developments in Automation, Software Development, NLP, ML, AI, LLM technology around the globe relevant to aerospace and defence sector and work along with team to quickly leverage, test and validate new solutions applicable to the working projects by applying cutting edge technologies.\nShould have strong problem-solving skills and ability to collaborate with cross-functional teams, including domain experts, data scientists, and engineering teams, to gather requirements and translate them into scalable applications.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business process', 'C++', 'Automation', 'GIT', 'Coding', 'data security', 'Aerospace', 'VB', 'SQL', 'Python']",2025-06-12 15:13:08
Senior Staff Engineer I Custom Layout - Serdes,Alphawave Semi,2 - 7 years,Not Disclosed,['Bengaluru'],"The Opportunity\n\nWere looking for the Wavemakers of tomorrow.\nCustom Layout / High-Speed Analog Layout Engineer\nAlphawave IP builds industry-leading wired connectivity solutions that enable data to travel faster, more reliably, and with higher performance at lower power. Our technology is embedded in leading-edge semiconductors built to power global network and computer systems. It is an essential part of the core infrastructure enabling next generation services in data centers, artificial intelligence, 5G wireless infrastructure, data networking, autonomous vehicles, and solid-state storage.\nThe Opportunity\nThe Alphawave IP team combines technologists from different disciplines who come together with a shared passion for electronics, software, and communication technology. We look for individuals with a deep desire to build great products and we value collaboration, curiosity, and a commitment to solving hard problems.\nThe Alphawave Custom Layout team is composed of a group of highly technical, innovative, and passionate engineers, collaborating to develop the analog layouts and architectures for our world class high-speed SerDes IP s.\nWhat You ll Do\nCustom analog layout design for industry leading high speed Serdes architectures\nWorking in leading edge semiconductor nodes and cad tools including latest 3nm node\nDetailed collaboration in optimizing layouts with analog design team\nFloor-planning\nPerform physical verification (DRC,ANT,LVS,ERC, )\nDevelopment and maintenance of layout software automation capabilities\nWork with a team of world-class engineers who are willing to help when needed and are happy to receive help when offered\nWhat You ll Need\nBachelors in Electrical/Computer Engineering, EngSci, or equivalent\nFamiliarity with high-speed analog layout, electronics and CMOS transistors\nBonus points if you have worked in recent FinFet technologies (7nm, 5nm, etc)\nMinimum of 2 years of custom layout experience\nThe position is located in Vancouver\n5+ years of experience is preferred\nAbout You\nExcellent communication skills\nAble to listen to and appreciate ideas and opinions that differ from yours\nExtremely detail oriented\nSuperb analytical and problem-solving skills\nDrives for consistency\nTakes personal pride in high standard of outputs\nSelf-motivated and self-managing\n""We have a flexible work environment to support and help employees thrive in personal and professional capacities""\nAs part of our commitment to the well-being and satisfaction of our employees, we have designed a comprehensive benefits package that includes:\nCompetitive Compensation Package\nRestricted Stock Units (RSUs)\nProvisions to pursue advanced education from Premium Institute, eLearning content providers\nMedical Insurance and a cohort of Wellness Benefits\nEducational Assistance\nAdvance Loan Assistance\nOffice lunch & Snacks Facility\nEqual Employment Opportunity Statement\nAlphawave Semi is an equal opportunity employer, welcoming all applicants regardless of age, gender, race, disability, or other protected characteristics. We value diversity and provide accommodations during the recruitment process.",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Wireless', 'Automation', 'Product innovation', 'Analytical', 'Artificial Intelligence', 'CAD', 'Physical verification', 'Wellness', 'Data communication', 'Analog layout']",2025-06-12 15:13:11
Senior Engineer - Structures,Wsp Consultants,4 - 9 years,Not Disclosed,['Bengaluru'],"Role Summary\n\n\nThis role is to work as part of engineering team, focus on project delivery, production and liaison with the WSP in India Netherland team and mentoring. Role will be working under the supervision of an Principal Engineer or Associate .\n\n\n\n\nResponsibilities:\n\n\nCore Functions\n\n\n\nPrepare feasibility study reports to meet brief requirements in the agreed format and review with the Local CRC Head of Structures\n\nWork with the team to assemble a design specification compliant with the employers requirements, agree its format and content, and monitor and review its preparation ensuring delivery by the due date\n\nExpertise in Concept design to Detailed design stage for Steel and Concrete buildings\n\nAgree and monitor scope of works with the local CRC Head of Structures\n\nCarry out detailed design as per client requirements in accordance with standard codes, QA and technical review and sign off by the local CRC Head of Structures, including complex calculations and co-ordination issues\n\nReview and monitor the production of calculations including QA, technical reviews and sign off\n\nCo-ordinate project contract documents (drawings and specifications) and reviews input from team members\n\nDeal with the day to day queries from the team, ensuring that relevant information is available on time for construction activity\n\nLead the design process and encourage the rest of the team to deliver appropriate and cost effective solutions to the agreed programme.\n\nManagement of a team of engineers and BIM technicians.\n\n\n\n\n\nTechnical and Project Management\n\n\n\nRaise the level of technical competence within the teams\n\nImplement delivery and quality measurement processes\n\nPromote technical excellence in all our projects\n\nUndertake technical reviews and contribute to the concept design\n\nDevelop positive professional relationship with the WSP Netherlands team, communicating openly about project progress\n\nParticipate in team meetings, disseminate information within the team, and communicate with other teams in WSP\n\nIdentify and act on, or refer, potential risk issues and follow in full the company commercial and contracting processes\n\nManage delegated tasks to ensure that deadlines are met and flag resourcing concerns to team leader\n\nComplete timesheet accurately ahead of weekly deadlines\n\n\n\nKey Competencies / Skills\n\n\n\nThe applicant will have proven experience in the design of Building Structures, Concrete and Streel building designs, Seismic design with significant experience in a similar role or demonstration of a good track record\n\nGood presentation skills are also required\n\nMust be fully conversant with technical structural software, such as RFEM, FEM Design, ROBOT, ETABS, SAFE, RAM and STAAD Pro\n\nExperience with International design codes viz. , Eurocode, ACI etc. , is required\n\nA sound understanding of Microsoft Outlook, Word, Excel, Powerpoint is essential\n\nMust be fluent in English with an excellent understanding of technical terminology\n\nDemonstrate good management, communication and technical skills and be capable of working both within the team and independently, as dictated by work load\n\n\n\nQualifications\n\n\n\nThe candidate should possess a Bachelor s degree in Civil or Structural Engineering and possess membership to an accredited engineering body. Master s degree is preferred.\n\nIt is desirable that the candidate has obtained UK Chartered Engineer status or pursuing the same.\n\nExperience: 8 to 14 years",Industry Type: Management Consulting,Department: Construction & Site Engineering,"Employment Type: Full Time, Permanent","['Head Business Development', 'Concept design', 'CRC', 'Project management', 'MS Outlook', 'Bim', 'Resourcing', 'Staad Pro', 'Structural engineering', 'Monitoring']",2025-06-12 15:13:13
Python Developer,Coartha Technosolutions,0 - 1 years,Not Disclosed,['Hyderabad( Madhapur )'],"Seeking a passionate Python Developer (Fresher) to work on innovative projects in Python, AI, and ML. Great opportunity to build and maintain high-quality product features and grow your skills in a dynamic, collaborative environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Large Language Model', 'Pandas', 'MongoDB', 'Machine Learning', 'Deep Learning', 'Numpy', 'Elastic Search', 'Flask']",2025-06-12 15:13:16
Artificial Intelligence Architect,Ltimindtree,12 - 16 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']",We are looking for an experienced AI ML Developers experience in data science specializing in machine learning python statistical modelling and big data technologies pyspark sql.\n\nThe ideal candidate will have a strong background in developing and deploying machine learning models optimizing ML pipelines and handling largescale structured and unstructured data to drive business impact.\n\nDeep understanding of supervised and unsupervised learning including regression classification Multiclass classification clustering and NLP Proficiency in statistical analysis AB testing and causal inference techniques Experience with model deployment and MLOps in cloud environments AWS GCP \n\nKey Responsibilities\n\nDevelop and deploy machine learning models and predictive analytics solutions for business impact\nWork with largescale structured and unstructured data to extract insights and build scalable models\nDesign implement and optimize ML pipelines for realtime and batch processing\nCollaborate with engineering product and business stakeholders to translate business problems into data science solutions\nApply statistical modeling AB testing and causal inference techniques to evaluate business performance\nApply machine learning and statistical techniques for audience segmentation helping to identify patterns and optimise business strategies\nDrive research and innovation by staying updated with cuttingedge MLAI advancements and incorporating them into our solutions\nOptimize data science models for performance scalability and interpretability in production environments\nMentor junior data scientists and contribute to best practices in data science and engineering,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Architect', 'MLOps', 'Machine Learning', 'Ai Solutions', 'Aiml', 'Ml']",2025-06-12 15:13:18
Gen AI Experts,Axtria,5 - 10 years,Not Disclosed,"['Noida', 'Hyderabad', 'Bengaluru']","Axtria: -Axtria is a global provider of award-winning cloud software and data analytics to the life sciences industry. Axtria enables life sciences organizations to transform the product commercialization journey and deliver much-improved healthcare outcomes for patients worldwide. We are acutely aware that our work impacts millions of people and are incredibly passionate about the improvement we can bring to patients lives.\nOur focus is on delivering solutions that help pharmaceutical, medical device, and diagnostics companies complete the journey from data to insights to action and get superior returns from their investments. As a participant of the United Nations Global Compact, Axtria is committed to aligning strategies and operations with universal principles on human rights, labor, environment, and anti-corruption and taking actions that advance societal goals.\nOur people are our core strength, and they make us proud of our work; this has helped us grow exponentially and make tremendous strides toward developing great products. It is for the grit, thinking of an entrepreneur, and a family-like environment where each member is valued and treasured that we are growing rapidly. For more information, visit www.axtria.com.\n\n\nJob Title: - Gen AI Experts ( Open across levels – Senior Associate to Associate Director)\n\nJob Location: -Gurgaon/Bangalore/Pune/Hyderabad\n\nJob Responsibilities: -\nBe an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT\nSoftware development experience in python is needed as backend for UI based applications\nBe a part of large delivery teams working on advanced projects when expert assistance is required.\nDeliver advanced Data Science capabilities to businesses in a meaningful manner through successful proof-of-concept solutions, and later smoothly transition the proof-of-concept into production.\nCreate Technical documents, develop, test, and deploy data analytics processes using Python, SQL on Azure/AWS platforms\nCan interact with client on GenAI related capabilities and use cases\n\nQualification: -\n- B-Tech or BE in Computer Science / Computer Applications from Tier 1-2 college\nOR\n- Master’s degree in Machine Learning / Statistics / Econometrics, or related discipline from Tier 1-2 college\n\nMust have Skills: -\nRequire 3-15 years experience to develop, test, and deploy Python based applications on Azure/AWS platforms\nMust have basic knowledge on concepts of Generative AI / LLMs / GPT\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.\nFamiliarity with frontend technologies like HTML, JavaScript, REACT\nSkills that give you an edge: -\nStrong analytical skills to solve and model complex business requirements are a plus. With life sciences or pharma background.\nWe will provide– (Employee Value Proposition)\nOffer an inclusive environment that encourages diverse perspectives and ideas\nDeliver challenging and unique opportunities to contribute to the success of a transforming organization\nOpportunity to work on technical challenges that may impact across geographies\nVast opportunities for self-development: online Axtria Institute, knowledge sharing opportunities globally, learning opportunities through external certifications\nSponsored Tech Talks & Hackathons\nPossibility to relocate to any Axtria office for short and long-term projects\nBenefit package:\n-Health benefits\n-Retirement benefits\n-Paid time off\n-Flexible Benefits\n-Hybrid /FT Office/Remote\nAxtria is an equal-opportunity employer that values diversity and inclusiveness in the workplace.\nWho we are\nAxtria 14 years journey\nAxtria, Great Place to Work\nLife at Axtria\nAxtria Diversity",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'Model Building', 'Python', 'SQL', 'Modeling Tools']",2025-06-12 15:13:20
AI/ML Engineer/Architect -,Choice Consultants,6 - 10 years,20-35 Lacs P.A.,"['New Delhi', 'Bengaluru']","Automotive, Business Relationship Management, Collaborative Leadership, Communication, Computer Vision, AI, Machine Learning, Team Leadership, Technological Innovation, Proposal",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['C/C++', 'AI/ML', 'PYTHON', 'COMPUTER VISION', 'AIML', 'Medical Imaging', 'DEEP LEARING']",2025-06-12 15:13:23
Principal Engineer (Python),Hughes Systique,4 - 7 years,Not Disclosed,['Gurugram'],"Job Overview\nWe are looking for a high end Principal Engineer who will play a role of technical lead along with hands-on contributor. This role demands someone with deep expertise in Python and its frameworks, strong exposure to cloud-native development (AWS, Kubernetes), and a proven track record in driving engineering excellence across multiple domains such as backend services, infrastructure, and DevOps practices.\nYou would be responsible for designing and building scalable backend services and APIs using Python frameworks, integrating relational databases, and deploying in containerized environments on prem and cloud.\nKey Responsibilities\n. Lead technical architecture and design for scalable, resilient, and secure systems.\n\n. Design and develop RESTful backend APIs using FastAPI and Flask\n.Build server-side logic and business functionalities using Python\n.Design and integrate MySQL and PostgreSQL databases\n.Deploy and manage applications in Docker/Kubernetes environments\n.Maintain CI/CD pipelines using Git and Jenkins\n.Collaborate with DevOps and frontend teams for integration\n.Ensure code quality through peer reviews and documentation\nTechnical Stack\n.Python (FastAPI, Flask)\n.RESTful API\n.MySQL, PostgreSQL\n.Docker, Kubernetes\n.Git, Jenkins\n.Linux-based development environment\n. Strong experience working with AWS services (e.g., EC2, Lambda, EKS, S3, RDS, CloudWatch, DynamoDB).\n\nNice to Have:\n-Experience with event-driven or microservices architecture.\n-Exposure to serverless computing and IaC tools like Terraform or AWS CDK.\n-Familiarity with security and compliance in cloud-native applications.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['amazon ec2', 'Backend', 'GIT', 'Linux', 'Postgresql', 'MySQL', 'Cloud', 'jenkins', 'AWS', 'Python']",2025-06-12 15:13:25
Lead AI Engineer,Insnapsys Technologies,7 - 10 years,8.4-18 Lacs P.A.,['Nashik'],"We're hiring a Lead AI Engineer (Remote/Nashik) to build & lead AI/ML & LLM solutions. 7+ yrs exp, strong in Python, LLMs, AWS, MLOps, vector DBs. Bonus: voice AI, agents, open-source.\n\nTeam Leading Experience is Mandatory\n\nApply: hr@insnapsys.com.\n\n\nWork from home\nHouse rent allowance",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Machine Learning', 'Data Science', 'English', 'Marathi', 'Team Leading', 'Hindi', 'Deep Learning', 'Ml', 'Python']",2025-06-12 15:13:27
Python/Pyspark developer,Zensar,4 - 5 years,Not Disclosed,"['Pune', 'Bengaluru']","Job Description:\nWe are seeking a highly skilled and motivated Python/PySpark Developer to join our growing team. In this role, you will be responsible for designing, developing, and maintaining high-performance data processing pipelines using Python and the PySpark framework. You will work closely with data engineers, data scientists, and other stakeholders to deliver impactful data-driven solutions.\nResponsibilities:\n- Design, develop, and implement scalable and efficient data pipelines using PySpark.\n- Write clean, well-documented, and maintainable Python code.\n- Optimize data processing performance and resource utilization.\n- Implement ETL (Extract, Transform, Load) processes to migrate and transform data across various systems.\n- Collaborate with data scientists and analysts to understand data requirements and translate them into technical solutions.\n- Troubleshoot and debug data processing issues.\n- Stay up-to-date with the latest advancements in big data technologies and best practices.\nQualifications:\n- Bachelor's degree in Computer Science, Engineering, or a related field.\n- 3+ years of experience in Python development.\n- 2+ years of experience with PySpark and Spark ecosystem.\n- Strong understanding of data structures, algorithms, and object-oriented programming.\n- Experience with SQL and relational databases.\n- Familiarity with cloud platforms such as AWS, Azure, or GCP (preferred).\n- Excellent problem-solving and analytical skills.\n- Strong communication and teamwork skills.\nBonus Points:\n- Experience with data visualization tools (e.g., Tableau, Power BI).\n- Knowledge of machine learning and data science concepts.\n- Experience with containerization technologies (e.g., Docker, Kubernetes).\n- Contributions to open-source projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Cloud Technologies', 'SQL', 'Python']",2025-06-12 15:13:29
Principal Engineer - IT Quality & Assurance Testing,Oliver Wyman,4 - 9 years,Not Disclosed,['Mumbai'],"Company: Marsh\nDescription:\nCompany:\nMarsh McLennan\n\nDescription:\nMarsh McLennan is seeking candidates for the following position based in the India Mumbai/Pune Office\nSenior Principal Engineer - IT Quality Assurance\n\nWhat can you expect?\nMarsh McLennan offers an amazing opportunity for distinguished digital technologists to transform the company s digital capabilities while unleashing the power of our industry leading platforms and data assets to deliver innovative products and disruptive business models globally.\nAt Marsh McLennan, we are all about developing and integrating innovative digital products for our clients. This group is nimble, creative, and empowered to shape the Marsh s digital landscape.\n\nWhat is in it for you?\nIf you are looking for an opportunity to be part of an exciting team of entrepreneurs with the mission to disrupt and deliver the art of the possible in the insurance industry, look no further!\nCompetitive Benefits\nCareer Development Opportunities\n\nIn this role you will be responsible for:\nEnvision and develop a library of frontend and backend test automation frameworks to a best-in-class Marsh digital platform\nBe a change agent focusing on quality assurance, and exhibit proficiency in manual and automation testing\nHave an ability to explore, debug and investigate issues\nBelieve in bringing efficiencies through automation using new technologies, testing and automation tools\nDrive culture change in technology to become a truly Agile team which is self-organizing, DevOps and believe in everything automated\nCollaborate with Marsh technology teams on new / existing test development opportunities\nContinuously learn about new technologies and help keep the entire group abreast of industry developments and evolving best practices in quality assurance and test automation\n\nWe would like you to have:\nBachelor s Degree in Computer Science or related\n4+ years of experience with a proven track record of successfully delivering global and highly scalable customer-facing digital products and capabilities\nLeadership in thinking and ideating new and innovative solutions for QA\nExpertise in Web application Testing\nHands on API testing\nHands on with Playwright or Selenium\nHands on with agile project management tools - ADO\nHands on with SQL and RDBMS like ORacle/MSSql\nExposure to any JavaScript based testing frameworks\nExposure to public cloud technology stack in AWS, and Azure\nStrong analytical skills to be able to analyze complex problems using a number of problem solving techniques.\nExperience in delivering complex software quality assurance in an Agile environment\nProactively partner with business and technology organizations across system and organizational boundaries to identify opportunities and drive automated solutions",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Backend', 'Reinsurance', 'RDBMS', 'Javascript', 'Selenium', 'Oracle', 'Risk management', 'Software quality assurance', 'SQL']",2025-06-12 15:13:31
Snowflake - Senior Technical Lead,Sopra Steria,2 - 11 years,Not Disclosed,['Noida'],"Position: Snowflake - Senior Technical Lead\nExperience: 8-11 years\nLocation: Noida/ Bangalore\nEducation: B.E./ B.Tech./ MCA\nPrimary Skills: Snowflake, Snowpipe, SQL, Data Modelling, DV 2.0, Data Quality, AWS, Snowflake Security\nGood to have Skills: Snowpark, Data Build Tool, Finance Domain\nPreferred Skills",,,,"['Performance tuning', 'Schema', 'HIPAA', 'Javascript', 'Data quality', 'Informatica', 'Analytics', 'SQL', 'Python', 'Auditing']",2025-06-12 15:13:34
STEM / Robotics Trainer,Mindsightz Education,0 - 2 years,1.5-4 Lacs P.A.,"['Tiruppur', 'Coimbatore', 'Erode']","The position is responsible for implementing innovative, challenging, and engaging Science, Technology, Engineering and Math hands-on time tested curriculum; providing STEM training for students and participate in special Robotic events at schools\n\nRequired Candidate profile\nOral and written communication skills\nPassion for teaching\nTraining delivery and presentation skills\nThe deployment of hands-on learning technologies\nWork effectively with partner",Industry Type: Education / Training,Department: Teaching & Training,"Employment Type: Full Time, Permanent","['Training', 'Teaching', 'computer programming', 'Curriculum development', 'SCRATCH', 'STEM Robotics', 'Arduino', 'Artificial intelligence', 'Robotics', 'STEM']",2025-06-12 15:13:36
Azure & AWS Cloud Azure & AWS Cloud,Zensar,5 - 9 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Zensar Technologies is looking for Azure & AWS Cloud Azure & AWS Cloud to join our dynamic team and embark on a rewarding career journey\n\nManages deployment and operations across Azure and AWS cloud platforms\n\nEnsures high availability, security, and scalability of cloud services\n\nImplements automation for infrastructure provisioning and CI/CD pipelines\n\nMonitors performance and optimizes cost across hybrid cloud environments",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['kubernetes', 'continuous integration', 'python', 'iso', 'microsoft azure', 'networking', 'artificial intelligence', 'docker', 'ansible', 'iot', 'aws cloud', 'gcp', 'awsazure', 'devops', 'linux', 'jenkins', 'shell scripting', 'agile', 'aws', 'cloud computing', 'architecture']",2025-06-12 15:13:39
Wordpress Intern,Swadhin It Solutions,0 - 3 years,Not Disclosed,['Bhubaneswar'],"Swadhin IT Solutions is looking for Wordpress Intern to join our dynamic team and embark on a rewarding career journey\nDesign, develop, and maintain WordPress websites.\nCustomize themes and plugins to meet client requirements.\nOptimize websites for performance, security, and SEO.\nConduct testing and debugging to ensure website functionality.\nStay current with emerging technologies and best practices in WordPress development.\nProvide technical support and troubleshooting for WordPress websites.\nWrite and maintain technical documentation for website features and codebase.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['digital marketing', 'matlab', 'python', 'css', 'c++', 'software development', 'software testing', 'wordpress', 'machine learning', 'javascript', 'jquery', 'java', 'embedded c', 'linux', 'social media marketing', 'debugging', 'php', 'html', 'mysql', 'data structures', 'seo', 'communication skills']",2025-06-12 15:13:41
Gen AI Specialist,Lumen Technologies,7 - 12 years,Not Disclosed,['Bengaluru'],"About Lumen Technologies\nLumen Technologies is a global technology company that delivers innovative communication and network solutions. Our mission is to empower businesses and individuals to connect, grow, and thrive in the digital age. With a focus on customer experience and operational excellence, we strive to provide cutting-edge solutions that meet the evolving needs of our customers.\nThe Main Responsibilities\nDevelopment of Gen AI applications and conduct reviews.\nWrite clean and maintainable code, improve the system architecture\nParticipate in meetings and conferences to gather requirements, discuss architectural decisions, and collaborate on innovative solutions.\nDevelop frameworks, libraries, and tools to expedite GenAI application development.\nExplore new technologies in the Gen AI space like MCP and agent protocols, and drive adoption of these advancements to enhance our AI capabilities.\nRequired Qualifications:\nBachelor s degree in computer science or related field or the equivalent in training and experience\n7+ years of professional experience in Java/Python development\nWell-versed with development using frameworks like Spring / Spring Boot / FastAPI / Django.\nWell-versed with REST API development and security standards (OAuth2.0, OIDC etc),\nExperience using any SQL database (Oracle/PostgreSQL, etc.) and any NoSQL databases -MongoDB/Couchbase.\nExperience on GenAI Development using Python, Langchain, Vector Datastores(Azure, Pinceone etc.), Large language models like Llama, Open AI etc.\nWell-versed with prompt engineering techniques and using frameworks like Semantic Kernel, LangGraph etc.\nExperience using a container ecosystem - Kubernetes, Docker.\nExperience working on developing and deploying applications on any cloud platform (AWS / GCP / Azure)\nExperience using CI/CD pipelines for application development.\nWe are an equal opportunity employer committed to fair and ethical hiring practices. We do not charge any fees or accept any form of payment from candidates at any stage of the recruitment process. If anyone claims to offer employment opportunities in our company in exchange for money or any other benefit, please treat it as fraudulent and report it immediately.\n\n#LI-MP1",Industry Type: Telecom / ISP,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'System architecture', 'NoSQL', 'Claims', 'Architecture', 'Postgresql', 'Django', 'Application development', 'MongoDB', 'Python']",2025-06-12 15:13:43
Hiring FCT Mentor( Education Loan Team Leader)-Shiksha.com,Info Edge,3 - 7 years,Not Disclosed,['Noida'],"About Info Edge\nInfoEdges mission is to create world-class platforms that transform lives by continuously innovating. Our products and services are built keeping our customers in mind. We always delight our customers by delivering superior value through enhanced offerings on the internet and other platforms. Through our continuous investment across various businesses, especially in cutting-edge technology, machine learning and artificial intelligence (AI), we have built a robust system that constantly increases our predictive powers on customer behaviour, and optimizes and improves our systems. Our various teams tirelessly work together to solve problems, innovate, and create something to empower our customers.",,,,"['Education Loan', 'Client Management', 'Team Handling', 'sales orientation', 'account manager', 'overseas education', 'study abroad', 'loan counsellor']",2025-06-12 15:13:46
Walkin Drive II Corporate Sales B2B II 99acres_Bangalore,Info Edge,1 - 5 years,Not Disclosed,['Bengaluru'],"Walkin Drive on 13th and 14th June\nRole: Corporate Sales (B2B)\nExperience: 1- 5 years\nSkill: B2B Sales, New client acquisition\n\nAbout Info Edge:\nInfoEdges mission is to create world-class platforms that transform lives by continuously innovating. Our products and services are built keeping our customers in mind. We always delight our customers by delivering superior value through enhanced offerings on the internet and other platforms. Through our continuous investment across various businesses, especially in cutting-edge technology, machine learning and artificial intelligence (AI), we have built a robust system that constantly increases our predictive powers on customer behaviour, and optimizes and improves our systems. Our various teams tirelessly work together to solve problems, innovate, and create something to empower our customers.",,,,"['B2B Sales', 'Client Acquisition', 'Field Sales', 'Key Account Management', 'Business Development', 'Fresher Hiring']",2025-06-12 15:13:49
Principal Site Reliability Engineer,Swimlane,10 - 15 years,Not Disclosed,['Hyderabad'],"Do you have an interest and desire to work with cutting edge technologies to solve challenging problems? Do you love cyber-security? Would you like to help build a platform that helps security teams process millions of security alerts every day? Are you interested in a role where you can use the latest JavaScript technologies and frameworks, and contribute to open source?\n\nAs the most senior technical individual contributor within an entire division of Engineering at Swimlane, you will be deeply involved with and guide the reliability, availability, security, quality, and extensibility of our offerings. You must have an extreme ownership mentality. You will work very closely with other senior Engineering, Product, and Support resources to quickly advance the number and state of our offerings. You will create, test, and operate new services, as well as enhance existing ones.\n\nJob Requirements:\n8+ years experience developing in at least one common, general languages (i.e., C, C#, Java, Python, etc)\n8+ years experience as a senior or principal engineer\n15+ years experience as an engineer managing mission-critical services at scale\n6+ years experience with Amazon Web Services (AWS), Microsoft Azure, and/or Google Compute Platform (GCP)\nDeep, demonstrable experience with various Cloud-native monitoring, logging, and dashboarding platforms (e.g., New Relic, Datadog, Prometheus, etc)\nExcellent understanding of and ability to work with Hashicorp Terraform\nStrong understanding of modern continuous integration/continuous deployment (CI/CD) platforms (e.g., GitHub Actions, GitLab pipelines, AWS CodeBuild / Codedeploy / Codepipeline , etc)\nSubstantial experience with managing Kubernetes resources\nExcellent written and verbal English communication skills\nStrong ability to work in a fast-paced environment that does also have security and compliance requirements\nMust be available for on-call escalations\nMust be able to mentor resources of different levels\nAbility to manage up, down, and across the organization\nAutomate, automate, automate!\n\nDont meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Swimlane, we are dedicated to building a diverse, inclusive and authentic workplace, so if you are excited about this role but your past experience doesnot align perfectly with every qualification in the job description, we encourage you to apply anyway! You may just be the right candidate for this or other roles!\n\nWho we are, and what we offer:\nSwimlane is a rapidly growing, innovative startup that provides cloud-scale, low-code security automation for organizations of all industries and sizes. Our technology is relied upon by major security-forward companies around the globe and we are consistently rated as the #1 trusted low-code security automation platform. Our mission is to prevent breaches and enable continuous compliance via a low-code security automation platform that serves as the system of record for the entire security organization.\n\nWhat s the best thing about working at Swimlane? If you ask the team, they will tell you its the people. Swimlaners are innovative, collaborative and driven by the purpose of revolutionizing the way security teams automate and respond to alerts. Headquartered in beautiful Louisville,Colorado, directly between Denver and Boulder, Swimlanes staff spans 28 states and 16 countries!\n\nThe Perks of being a Swimlaner\nCompetitive Benefits & Compensation\nTraining & Professional Development Opportunities\nMacbook Pro\nGreat Company Culture\nWe value collaboration and innovation\nGive-back Volunteering Opportunities\n\nHere at Swimlane, our core focus is to Automate the World of Security and we strive to represent our five core values in everything we do:\nPunch above your weight class - We make the most of our circumstances and constantly surprise and impress with our ability to deliver.\nBe a happy innovator - The hard problems are the fun problems to solve, we re excited to take on difficult challenges and find creative solutions.\nAlways be leveling up - We are continuously improving, embracing change, and consuming information to better ourselves and each other.\nMove at the speed of WOW - We work with an extreme sense of urgency, but we never compromise quality.\nHave honesty and integrity in all the things - We make decisions with the best of intentions, doing what is right for as many stakeholders as possible.",Industry Type: Hardware & Networking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'github', 'cyber security', 'Compliance', 'GCP', 'Cloud', 'Javascript', 'Open source', 'Monitoring', 'Python']",2025-06-12 15:13:51
Big Data Developer,Techstar Group,7 - 10 years,Not Disclosed,['Hyderabad'],"Responsibilities of the Candidate :\n\n- Be responsible for the design and development of big data solutions. Partner with domain experts, product managers, analysts, and data scientists to develop Big Data pipelines in Hadoop\n\n- Be responsible for moving all legacy workloads to a cloud platform\n\n- Work with data scientists to build Client pipelines using heterogeneous sources and provide engineering services for data PySpark science applications\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- Define needs around maintainability, testability, performance, security, quality, and usability for the data platform\n\n- Drive implementation, consistent patterns, reusable components, and coding standards for data engineering processes\n\n- Convert SAS-based pipelines into languages like PySpark, and Scala to execute on Hadoop and non-Hadoop ecosystems\n\n- Tune Big data applications on Hadoop and non-Hadoop platforms for optimal performance\n\n- Apply an in-depth understanding of how data analytics collectively integrate within the sub-function as well as coordinate and contribute to the objectives of the entire function.\n\n- Produce a detailed analysis of issues where the best course of action is not evident from the information available, but actions must be recommended/taken.\n\n- Assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients, and assets, by driving compliance with applicable laws, rules, and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct, and business practices, and escalating, managing and reporting control issues with transparency\n\nRequirements :\n\n- 6+ years of total IT experience\n\n- 3+ years of experience with Hadoop (Cloudera)/big data technologies\n\n- Knowledge of the Hadoop ecosystem and Big Data technologies Hands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Pig, Impala, Spark, Kafka, Kudu, Solr)\n\n- Experience in designing and developing Data Pipelines for Data Ingestion or Transformation using Java Scala or Python.\n\n- Experience with Spark programming (Pyspark, Scala, or Java)\n\n- Hands-on experience with Python/Pyspark/Scala and basic libraries for machine learning is required.\n\n- Proficient in programming in Java or Python with prior Apache Beam/Spark experience a plus.\n\n- Hand on experience in CI/CD, Scheduling and Scripting\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- System level understanding - Data structures, algorithms, distributed storage & compute\n\n- Can-do attitude on solving complex business problems, good interpersonal and teamwork skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Hive', 'Data Engineering', 'Data Pipeline', 'PySpark', 'Hadoop', 'Kafka', 'HDFS', 'Spark', 'Python']",2025-06-12 15:13:54
"Sr. QA Engineer, AI",Conga,5 - 8 years,Not Disclosed,"['Pune', 'Ahmedabad', 'Bengaluru']","Job Title: Sr. QA Engineer\nLocations: Ahmedabad/ Bangalore/ Pune\nReports to: Manager, Quality Engineering\n\nA quick snapshot\n\nAs Senior QA Engineer your responsibilities will include designing and implementing tests, debugging, and defining corrective actions. You will also review system requirements and track quality assurance metrics. These tests entail other tasks such as developing and running new tests and reporting their results to stakeholders, who will collaborate to fix program bugs or problems. You will mentor juniors, collect daily updates, and circulate to managers/ higher forums making this role more important in the system.\n\nWhy its a big deal\n\nA Senior QA Engineer role has significance in the Testing Center of Excellence (TCoE) team at Conga, managing the production of test documents, the creation of test procedures, and ensuring high-quality products. Your expertise in agile methodology, and automation tools, will help in accelerating a continuous enhancement of our product features is a truly Big Deal in Conga Way. Your extensive contribution to scrum teams in the implementation of automation footprints with a Sprint/Release will bring a high-quality impact on Congas products. Your collaboration with cross-functional teams ensures the smooth running of the QA department and ultimately customer satisfaction.\n\nAre you the person were looking for?\n\nProven success in testing (Automation and Manual).Your experiences will include at least 5 years in test case planning, assessments, script development, and maintenance. You have hands-on experience with automation tools and frameworks and developing automation scripts.\n\nSelenium and API. You have expertise with automation tools such as Selenium web driver, frameworks, and developing automation scripts using Java. Strong hands-on experience with API approach using Rest Assured or any such client. Hands-on with test management software such as qTest, JIRA, Jmeter, Load Runner.\n\nAI Technology. You have experience in Large Language model, machine learning experience, AI Git knowledge for Advance Automation as well as familiar with AI Microsoft CoPilot. Candidate should be aware with attorney use cases for variety of documents\n\nAgile Methodology. You are proficient with Agile and a collaborative cross-functional approach to building awesome software. You are comfortable working with teams and collaborating on best practices across multiple Agile teams. You constantly seek opinions and solicit feedback to create the best work possible. You dont know any other way. Its a team effort and you completely appreciate that. Strong experience in software testing lifecycle (STLC) and knowledge of software development lifecycle (SDLC).\n\nEducation. A bachelors degree in engineering or equivalent.\n\nHere’s what will give you an edge\n\nStrong attention to detail. The Conga revenue lifecycle management solution showcases a wide variety of use cases, across multiple regions and languages. As a senior QA paying attention to the smallest details can help identify bugs that others might miss.\n\nStrong testing skills and logic based thinking is your forte. This is an absolute must. Your proven ability to analyze and apply logical thinking to determine the root cause of an issue is fundamental to success in this role. You can easily understand how systems interact/integrate with each other and as well as how changes in one application will affect others.\n\nInitiative. As a Senior QA, we need to own and initiate multiple things to make the quality better. Functional aspects, Non-functional aspects, Broader thinking, Integration approach, Reuse approach in Automation, Performance, Security, Database testing, and a lot more.\n\nAwareness. This role should be aware of the company vision, Goals, and Requirements, and work towards that direction to deliver quality so participation in multiple forums makes it more vital.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['copilot', 'Rest Assured', 'Playwright', 'Selenium With Java']",2025-06-12 15:13:56
Social Media Intern,The Peak View Stories,0 - 3 years,Not Disclosed,[],"The Peak View Stories is looking for Social Media Intern to join our dynamic team and embark on a rewarding career journey\nContent Creation: Assist in creating and curating content for various social media platforms, including text posts, images, videos, and infographics\nEnsure that content aligns with the brand's voice and messaging\nScheduling and Posting: Schedule and publish social media posts on platforms like Facebook, Instagram, Twitter, LinkedIn, and others\nUse social media management tools to plan content calendars\nAudience Engagement: Monitor social media channels for comments, messages, mentions, and direct interactions from followers\nEngage with the audience by responding to inquiries and comments\nAnalytics and Reporting: Track the performance of social media campaigns and posts using analytics tools\nProvide insights and data on key performance metrics, such as reach, engagement, and conversion rates\nTrend Analysis: Stay updated on social media trends, hashtags, and discussions related to the industry and brand\nUse this information to inform content creation and engagement strategies\nCompetitor Research: Research and analyze the social media presence of competitors to identify opportunities for improvement and differentiation\nHashtag Research: Identify relevant hashtags to use in posts to increase discoverability and engagement\nCreate branded hashtags when appropriate",Industry Type: Miscellaneous,Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['digital marketing', 'infographics', 'instagram', 'linkedin', 'media', 'machine learning', 'research', 'content creation', 'excel', 'marketing', 'twitter', 'online marketing', 'campaigns', 'performance metrics', 'media management', 'quality standards', 'social media marketing', 'trend analysis']",2025-06-12 15:13:59
Principal Engineer- Project Purchasing,Burns and Mc Donnells Engineering India,7 - 10 years,Not Disclosed,['Mumbai'],"About us:\nBurns & McDonnell  is a leading player in the Engineering, Procurement, and Construction (EPC) industry, delivering innovative solutions to clients  across multiple industries like Chemicals,Oil & Gas, Transmission & Distribution, Power among other verticals.We are proud to be part of a global network with our US parent company. With a track record of successful projects across various industries, we are committed to innovation, sustainability, and client satisfaction. As we continue to grow, we are seeking an experienced Purchasing  Engineers to join our team.\nPosition Overview :\nAs a Principal Purchasing Engineer at Burns & McDonnell, your role will be pivotal in the successful execution of work share projects between our consultancy firm and our US parent company.You will support US procurement manager and Buyers Purchasing on project to ensure seamless integration between the two entities, adhering to the highest standards of efficiency and quality.\nKey Responsibilities:\n1.Provide procurement support to Operations in relation to purchasing Project Procurement items like Pressure vessels, Rotary Items, E& I  items, etc. in a timely manner as assigned with supervision and support from the Business Support Manager.\n2.Responsible for RFP compilation ,Quality review of  RFP with Engineering, Floating Enquiry, Bid evaluation, Bid Tabulation, Purchase Recommendation and post order PO management.\n3.Coordiation with International supplier for bid clarification and with US counterpart to update the status of purchasing\n4.Receive and Check Supplier Invoice, Tag to proper Project , Process through Oracle OnBase application for further Projects approval and  for final processing by Finance. Complete Tracking to be followed until Invoice is processed and release to supplier.\n5.Coordinate with Procurement leadership team & project Management team  to provide Monthly status on Purchasing.\nKey Technical Deliverables:\n1.Approved Manufacturer list\n2.Prepare Request For Quotations (RFQs) and evaluate responses\n3.Negotiate with suppliers on all matters relating to terms and conditions, improved pricing of quotes received and delivery options that may be more economic and timely.\n4.Tabulate Commercial Bid Evaluation\n5.Issue Purchase Recommendation\n4.Coordinate on contractual, commercial, taxation, insurance, and legal issues with relevant internal stakeholders.\n6.Raise/Revise Purchase Orders (POs) and resolve queries as require.\n7.Prepare & Maintain Purchase reports.\n-\n1.11 to 12  years  Procurement experience, preferably in an  EPC environment in Oil and Gas, Transmission & Distribution industry.\n2. Good understanding of PO contact terms and condition, logistics, Supplier Qualification.\n3. Experience in contract formulation activities, systems and processes\n4. Good communication skills, both oral and written.\n5. Computer Knowledge and operating skills on MS office.",Industry Type: Engineering & Construction,Department: Procurement & Supply Chain,"Employment Type: Full Time, Permanent","['execution', 'project management', 'engineering purchase', 'purchase', 'purchase order', 'engineering', 'environment', 'purchase management', 'operations', 'vendor development', 'procurement', 'project procurement', 'writing', 'epc', 'construction', 'communication skills', 'ms office', 'project purchase']",2025-06-12 15:14:01
Associate ML Scientist - II,Wadhwani Ai,3 - 6 years,Not Disclosed,['Mumbai'],"SUMMARY\nAn Associate Machine Learning (ML) Scientist at Wadhwani AI will build scientifically rigorous and robustly evaluated AI solutions that will be deployed in order to bring AI to the benefit of underserved billions across the developing world.\nABOUT US - https://www.wadhwaniai.org/\nWadhwani AI is a nonprofit institute building and deploying applied AI solutions to solve critical issues in public health, agriculture, education, and urban development in underserved communities in the global south. We collaborate with governments, social sector organizations, academic and research institutions, and domain experts to identify real-world problems, and develop practical AI solutions to tackle these issues with the aim of making a substantial positive impact.\nWe have over 30+ AI projects supported by leading philanthropies such as Bill & Melinda Gates Foundation, USAID and Google.org. With a team of over 200 professionals, our expertise encompasses AI/ML research and innovation, software engineering, domain knowledge, design and user research.\nIn the Press:\nOur Founder Donors are among the Top 100 AI Influencers\nG20 India s Presidency: AI Healthcare, Agriculture, & Education Solutions Showcased Globally.\nUnlocking the potentials of AI in Public Health\nWadhwani AI Takes an Impact-First Approach to Applying Artificial Intelligence - data.org\nWinner of the H&M Foundation Global Change Award 2022\nIndian Winners of the 2019 Google AI Impact Challenge, and the first in the Asia Pacific to host Google Fellows\nROLES AND RESPONSIBILITIES\nBuild robust machine learning solutions that address problems of societal importance, under the guidance of senior ML scientists.\nAssist in the design and evaluation of automated speech recognition (ASR) solutions, both internal as well as submitted for funding to the India AI mission at the Ministry of Electronics and Information Technology (MEITy).\nTranslate challenges in the social sector into well-defined AI problems.\nDevelop and execute algorithms and models to solve the identified problems effectively.\nContribute to the successful and scalable deployment of AI solutions in real-world settings.\nDefine and apply appropriate evaluation metrics to assess the impact and effectiveness of deployed solutions.\nUnderstand user challenges and contextual factors that influence the solution s performance and relevance.\nCurate, clean, and transform datasets for use in training and validating ML models.\nConduct model training, validation, simulations, and extract meaningful insights from data.\nCollaborate with cross-functional teams including engineers, solution leads, and domain experts to develop holistic AI solutions.\nInterface with social sector organizations and stakeholders to ensure the solution meets on-ground needs.\nREQUIREMENTS\nAssociate ML scientists will have a strong academic background in a quantitative field such as B.Tech. / B.E. / B.S. / M.Tech. / M.E. / M.S. / M.Sc. or equivalent in Computer Science, Electrical Engineering, Statistics, Applied Mathematics, Physics, Economics or a relevant quantitative field with a work experience of 3-6 years.\nCandidates should preferably have experience with ASR methods.\nCandidates should have excellent communication skills and a willingness to adapt to the challenges of doing applied work for social good.\nSolid software engineering skills across one or multiple languages including Python, C++, Java.\nInterest in applying software engineering practices to ML projects.\nTrack record of project work in applied machine learning. Experience in applying AI models to concrete real-world problems is a plus.\nStrong verbal and written communication skills in English.",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'ASR', 'Electrical engineering', 'C++', 'Artificial Intelligence', 'Machine learning', 'Healthcare', 'Information technology', 'Public health', 'Python']",2025-06-12 15:14:04
Sr. Data Analyst,Icims,4 - 9 years,Not Disclosed,['Hyderabad'],"Overview\nThe Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.",,,,"['server', 'data', 'vlookup', 'market data', 'data mapping', 'dashboards', 'research', 'sql', 'analytics', 'tables', 'prep', 'pivot', 'data visualization', 'communication skills', 'python', 'data analytics', 'data analysis', 'insights', 'pivot table', 'data engineering', 'graph', 'excel', 'data quality', 'tableau', 'data governance', 'root cause']",2025-06-12 15:14:07
Product Manager -Data Science CoE,Algoleap Technologies,10 - 15 years,Not Disclosed,['Hyderabad'],"We are establishing a Data Science Center of Excellence (CoE) to drive data-driven innovation and insights across multiple global business units. Were looking for a seasoned Product Manager to lead this initiative from inception to scale. This role will act as the strategic interface between business stakeholders and the data science team to define, prioritize, and deliver impactful data products and solutions.\nKey Responsibilities:\nCollaborate with global business leaders to identify and prioritize high-impact data science opportunities.",,,,"['Product management', 'Usage', 'data science', 'Manager Program Management', 'Machine learning', 'Management', 'Stakeholder management', 'Analytics', 'Monitoring']",2025-06-12 15:14:09
Senior Data Analyst,OnlineSales.ai,2 - 7 years,Not Disclosed,['Pune'],"About OnlineSales.ai\nBuilt by ex-Amazon ad-tech experts, OnlineSales.ai offers a future-proof Retail Media Operating System - boosting Retailer s profitability by 7% of Sales! We are an Enterprise B2B SaaS startup, based out of Pune India. With OnlineSales.ais platform, retailers activate and delight 10x more Brands by offering an omni-channel media buying experience, advanced targeting, analytics & 2x better ROAS. Tier 1 Retailers and Marketplaces globally are accelerating their Monetization strategy with OnlineSales.ai and are innovating ahead of the market by at least 2 years.\n\nAbout the Role\nWe are seeking a talented and motivated individual to join our team as a Senior Data Analyst who will be responsible for extracting insights from complex datasets to drive informed decision-making and enhance business performance. You will collaborate closely with cross-functional teams to identify key metrics, develop data-driven strategies, and provide actionable recommendations. Additional responsibilities may include managing daily regulatory reporting tasks and remediation activities, as well as process improvement.\n\nWhat will you do @OnlineSales?\nData Analysis: Utilize advanced analytical techniques to explore large datasets, identify trends, patterns, and anomalies, and extract actionable insights.\nData Visualization: Create visually compelling dashboards and reports to communicate findings effectively to stakeholders, enabling them to make informed decisions.\nData Extraction: regular extraction of relevant data from internal databases using SQL queries. Design and optimize SQL queries to retrieve specific datasets required for performance analysis and reporting\nIssue Identification: Proactively identify performance-related issues by monitoring key performance indicators (KPIs), analyzing trends, and investigating anomalies reported by internal stakeholders or external clients.\nAddressing Client Exceptions and Issues: Responsively address performance-related exceptions and issues raised by clients, ensuring timely resolution and effective communication throughout the process. Collaborate with client-facing teams to understand client requirements, prioritize tasks, and deliver solutions that meet or exceed client expectations.\nRoot Cause Analysis: Dive deep into data to understand the root causes of performance issues, considering factors such as system architecture, infrastructure, code efficiency, and user behavior.\nHypothesis Testing: Apply hypothesis testing techniques to validate assumptions and identify statistically significant factors impacting performance.\nDocumentation and SOP Creation: Create clear and detailed Standard Operating Procedures (SOPs) outlining the process for diagnosing, troubleshooting, and resolving performance issues. Ensure that documentation is organized, easily accessible, and regularly updated to reflect changes in systems, processes, or configurations.\nCross-Functional Collaboration: Collaborate with teams across the organization, including business development, marketing, product development and operations, to understand their data needs and provide analytical support\n\nYou will be a great fit, if you have :\n2-4 years of relevant experience.\nBachelors or Masters degree in Computer Science, Engineering, or a related technical field.\nProficiency in SQL for data extraction and manipulation from relational databases.\nFamiliarity with programming languages such as Python for Data Analysis and Data modeling is a plus.\nStrong analytical skills with the ability to interpret complex datasets and draw meaningful insights.\nStrong problem-solving abilities with a proactive approach to troubleshooting and issue resolution.\nAdvanced proficiency in Excel and adept data manipulation skills for efficient analysis and visualization of large datasets.\nEffective communication and interpersonal skills for collaboration with cross-functional teams and stakeholders.\nUnderstanding of E-Commerce as a domain.\nExcellent documentation skills with the ability to create clear and comprehensive reports and SOPs.\nAttention to detail and commitment to data accuracy and quality. Willingness to work for a startup.\n\nWhy Online Sales.ai?\nStartup-y . We believe Startup is a mindset. It s about being scrappy, being nimble, solving tough problems with constrained resources, and more. It s about working hard and playing hard\nEnterprise SaaS . Opportunity to work with an Enterprise Product SaaS firm with aspirations of growing 10x across the globe\nAI-led Retail Tech . We are working to digitize & democratize one of the most exciting and growing verticals - Retail Tech leveraging data, machine learning, and automation (culmination of ad-tech, mar-tech, and analytics for Retail vertical)\nMeaningful work . This is not just a job. You can find a job anywhere. This is a place for the bold to get paid who make a real impact on business\nNo red tape . Say goodbye to pointless meetings or political hoops to jump through. We re scrappy, believe in autonomy, and empower our teams to do whatever it takes to do the unthinkable\nProblem Solving . We ignite the best in you. We exist not only to deliver meaningful innovation but to ignite and inspire the creative problem-solver in you\nQuirky & fun . Enjoy new skills and hobbies like being a quiz master, playing board games, trying your hands on percussion, playing Djembe, and spreading love within the org!",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Automation', 'Data modeling', 'Process improvement', 'Online sales', 'Troubleshooting', 'Analytics', 'Monitoring', 'SQL', 'Data extraction']",2025-06-12 15:14:11
Data Specialist - Research,Horizon Therapeutics,9 - 13 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgens data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leverages state-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. You will leverage domain, technical and business process expertise to provide exceptional support of Amgen s data governance framework. This role involves working closely with business stakeholders and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with the Product Owner and other Business Analysts to ensure operational support and excellence from the team.\nRoles Responsibilities:\nResponsible for the data governance and data management framework implementation for the Research domain of the biopharma lifecycle.\nResponsible for the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nDrives cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles.\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. for assigned domains.\nEnsure compliance requirements with data privacy, security, and regulatory policies for the assigned domains\nJointly with Technology teams, business functions, and enterprise teams (e. g. , MDM, Enterprise Data Fabric, etc. ) define the specifications shaping the development and implementation of data foundations .\nBuild strong relationships with key business leads and partners to ensure their needs are being met\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills with knowledge of Pharma processes with specialization in the Research domain of the biopharma lifecycle.\nIn depth knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc.\nIn depth experience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer-focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nIn depth experience of working with or supporting systems used to data governance framework. E. g. Collibra, Alation\nExcellent problem-solving skills and committed attention to detail in finding solutions\nGood-to-Have Functional Skills:\nExperience of working with data governance councils or forums\nExperience with Agile software development methodologies (Scrum)\nProficiency in data analysis and quality tools (e. g. , SQL, Excel, Python, or SAS)\nSoft Skills:\nHighly organized and able to work under minimal supervision\nExcellent analytical and assessment skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAmbitious to further develop their skills and career\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills\nHigh degree of initiative and self-motivation.\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management and customer focus.\nBasic Qualifications:\nAny Degree and 9-13 years of experience\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business process', 'Data analysis', 'Change management', 'operational support', 'SAS', 'Pharma', 'Analytical', 'Agile', 'Data quality', 'SQL']",2025-06-12 15:14:13
Data Architect,Calibo,12 - 16 years,Not Disclosed,[],"About the Role:\n\nWe are looking for a highly skilled Data Engineering Architect with strong Data Engineering pipeline implementation experience to serve as the lead Solution/Technical Architect and Subject Matter Expert for customer experience data solutions across multiple data sources. The ideal candidate will collaborate with the Enterprise Architect and the client IT team to establish and implement strategic initiatives.\n\nResponsibilities and Technical Skills:\n12+ years of relevant experience in designing and Architecting ETL, ELT, Reverse ETL, Data Management or Data Integration, Data Warehouse, Data Lake, and Data Migration.\nMust have expertise in building complex ETL pipelines and large Data Processing, Data Quality and Data security\nExperience in delivering quality work on time with multiple, competing priorities.\nExcellent troubleshooting and problem-solving skills must be able to consistently identify critical elements, variables and alternatives to develop solutions.\nExperience in identifying, analyzing and translating business requirements into conceptual, logical and physical data models in complex, multi-application environments.\nExperience with Agile and Scaled Agile Frameworks.\nExperience in identifying and documenting data integration issues, and challenges such as duplicate data, non-conformed data, and unclean data. Multiple platform development experience.\nStrong experience in performance tuning of ETL processes using Data Platforms\nMust have experience in handling Data formats like Delta Tables, Parquet files, Iceberg etc.\nExperience in Cloud technologies such as AWS/Azure or Google Cloud.\nApache Spark design and development experience using Scala, Java, Python or Data Frames with Resilient Distributed Datasets (RDDs).\nDevelopment experience in databases like Oracle, AWS Redshift, AWS RDS, Postgres Databricks and/or Snowflake.\nHands-on professional work experience with Python is highly desired.\nExperience in Hadoop ecosystem tools for real-time or batch data ingestion.\nStrong communication and teamwork skills to interface with development team members, business analysts, and project management. Excellent analytical skills.\nIdentification of data sources, internal and external, and defining a plan for data management as per business data strategy.\nCollaborating with cross-functional teams for the smooth functioning of the enterprise data system.\nManaging end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nPlanning and execution of big data solutions using Databricks, Big Data, Hadoop, Big Query, Snowflake, MongoDB, DynamoDB, PostgreSQL and SQL Server\nHands-on experience in defining and implementing various Machine Learning models for different business needs.\nIntegrating technical functionality, ensuring data accessibility, accuracy, and security.\nProgramming / Scripting Languages like Python / Java / Go, Microservices\nMachine Learning / AI tools like Scikit-learn / TensorFlow / PyTorch",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud', 'ETL', 'AWS', 'Data Handling', 'Spark']",2025-06-12 15:14:15
Data Science Architect (Full Stack),Hubnex,4 - 9 years,Not Disclosed,['Gurugram'],"Data Science Architect (Full Stack)\nLocation: Gurugram, India (On-site/Hybrid)\nType: Full-Time | 4+ Years Experience | AI, Architecture & Product Engineering\nHubnex Labs is seeking a visionary and hands-on Full Stack Data Science Architect to lead the development of scalable AI products and reusable intellectual property (IP) that power data-driven solutions across global enterprise clients. This role requires deep technical expertise in AI/ML, data architecture, backend/frontend systems, and cloud-native technologies.\nKey Responsibilities AI & Data Science Leadership\nLead design and development of end-to-end AI/ML solutions across enterprise applications\nArchitect data pipelines, model training, validation, and deployment workflows\nApply cutting-edge techniques in NLP, Computer Vision, Speech Recognition, Reinforcement Learning , etc.\nEvaluate and rank algorithms based on business impact, accuracy, and scalability\nDesign and optimize data augmentation, preprocessing, and feature engineering pipelines\nTrain, validate, and fine-tune models using state-of-the-art tools and strategies\nMonitor and improve model performance post-deployment\nFull Stack & Cloud Architecture\nDesign and implement cloud-native systems using microservices , serverless , and event-driven architectures\nBuild robust APIs and UIs for intelligent applications (using Python, Node.js, React, etc.)\nUse Docker , Kubernetes , and CI/CD pipelines for scalable deployment\nLeverage technologies like Kafka, TensorFlow, Elixir, Golang , and NoSQL/Graph DBs for high-performance ML products\nDefine infrastructure to meet latency and throughput goals for ML systems in production\nInnovation & Productization\nBuild reusable IP that can be adapted across industries and clients\nRapidly prototype AI features and user-facing applications for demos and validation\nCollaborate closely with product managers and business stakeholders to translate use cases into scalable tech\nExplore and adopt new technologies and frameworks to maintain a forward-looking tech stack\nRequired Skills & Experience\n4+ years of experience building and deploying AI/ML models and scalable software systems\nStrong understanding of ML frameworks (TensorFlow, Keras, PyTorch), data libraries (pandas, NumPy), and model tuning\nProven track record of working with large-scale data , data cleaning, and visualization\nExpertise in Python , and experience with at least one other language (Go, Java, Scala, etc.)\nExperience with front-end frameworks (React, Vue, or Angular) is a plus\nProficient in DevOps practices , CI/CD, and cloud platforms (AWS/GCP/Azure)\nFamiliarity with event-driven systems , real-time protocols (WebSockets, MQTT), and container orchestration\nHands-on experience with NoSQL databases , data lakes , or distributed data platforms\nPreferred Traits\nExperience leading agile engineering teams and mentoring junior developers\nStrong architectural thinking, with an eye on scalability, maintainability, and performance\nEntrepreneurial mindset with a focus on building reusable components and IP\nExcellent communication skills, capable of bridging business and technical conversations\nWhy Join Hubnex Labs?\nOwn and architect impactful AI products used across industries\nShape the data science foundation of a fast-scaling software consulting powerhouse\nEnjoy a creative, high-performance environment in Gurugram , with flexibility and long-term growth opportunities\nContribute to next-gen solutions in AI, cloud, and digital transformation",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'Backend', 'Product engineering', 'Prototype', 'NoSQL', 'Enterprise applications', 'Consulting', 'Agile', 'Python', 'Data architecture']",2025-06-12 15:14:17
Senior AI Scientist,Vuram,7 - 9 years,Not Disclosed,['Chennai'],"1. Generative & Agentic AI Build and deploy GenAI models for text generation and content automation. Experience on working latest AI stacks like Nvidia- Nemo, NIM Microservices, Unsloth, Pytorch, Tensorflow etc. Develop agentic AI systems with autonomous task planning and decision-making capabilities.\n2. Large Language Models (LLMs) Fine-tune and operationalize LLMs (e.g., GPT, Llama, BERT) for NLP tasks using Nemo, NIM, Unsloth etc frameworks Establish the best practices for LLMOps, including prompt engineering and monitoring.\nDevelop solution based on latest coding standards like Pep-83. Deep Learning and NLP components Experience in developing like QnA, chatbots, Image/Video/Audio processing, OCR based components like Extraction etc. Experience in designing and implementing end-to-end pipelines for Retrieval-Augmented Generation (RAG), including document indexing, retrieval mechanisms Experience in evaluating AI solutions using appropriate metrics\n\n\nQualifications\n1. Bachelors or master s in computer science, Artificial Intelligence, or a related field2.\n7+ years of experience in AI/ML, NLP, Deep Learning, Gen AI, Model fine tuning, Reinforcement learning, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'deep learning', 'Automation', 'NIM', 'Coding', 'Artificial Intelligence', 'Deployment', 'Monitoring', 'microservices']",2025-06-12 15:14:19
Data Science Professional,Algoleap Technologies,6 - 11 years,Not Disclosed,['Hyderabad'],"Job_Description"":""\nJob Title: Data Science CoE\nLocation: Hyderabad, India (Hybrid)\nExperience: 6+ years\nRole Type: Full-time\nStart Date : Immediate\nAbout the Role:\nAs we build our Data Science Center of Excellence (CoE), we are looking for an entrepreneurial and technically strong Data Science Lead who can lay the foundation for a high-performing team. You will work directly with stakeholders across multiple business units to define use cases, lead model development, and ensure successful deployment and value realization.",,,,"['customer analytics', 'Usage', 'data science', 'GCP', 'Machine learning', 'model development', 'Deployment', 'Stakeholder management', 'SQL', 'Python']",2025-06-12 15:14:22
CDnA - Data Science Manager,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will collaborate with business partners, service owners and IS peers to develop predictive models and insights across the US Commercial Organization. This position will innovate and build significant business impact through the use of sophisticated analytics techniques to help Amgen with its mission to serve patients by helping them get the therapies they need.\n\nFlexible Commuter role to Amgen India office. You will work on-site 2-3 days a week.\n\nThis position will be primarily responsible for:\nWorking collaboratively with multi-functional teams on projects and/or programs with aims to systematically derive insights that ultimately derive substantial business value for Amgen and our patients\nIdentifying business needs and proposing potential analytics approaches for solutions\nCrafting and deploying a framework to supervise the performance of various campaigns, and tactics at a granular level\nLeading measurement and tracking of various omnichannel CX enablement initiatives\nSupporting the development of data science, machine learning prototypes, proof of concepts and models for testing various omnichannel strategies\nCommunicating analysis ideas, progress and results to leadership and business partners\n\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nDoctorate degree OR\nMasters degree and 4 to 6 years of data science and/or analytics experience OR\nBachelors degree and 6 to 8 years of data science and/or analytics experience OR\nDiploma and 10 to 12 years of data science and/or analytics experience\nPreferred Qualifications:\nRelevant work experience in campaign measurement, marketing analytics and resource optimization in the pharma domain\nProgramming experience with Python, R, or SAS and experience with ML libraries like scikit-learn, MLib, or TensorFlow\nExperience working with large datasets, experience working with distributed computing tools (Spark, Hive, etc.) is a plus\nAbility to communicate analysis in a clear, detailed, and practical manner\nPassion for learning and staying on top of current developments in sophisticated analytics\nBiotech / Pharma experience",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'hive', 'python', 'tensorflow', 'r', 'scikit-learn', 'spark', 'MLib']",2025-06-12 15:14:25
Data Privacy Specialist,Horizon Therapeutics,3 - 13 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgens data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leverages state-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. You will leverage domain, technical and business process expertise to provide exceptional support of Amgen s data governance framework. This role involves working closely with business stakeholders and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with the Product Owner and other Business Analysts to ensure operational support and excellence from the team.\nRoles Responsibilities:\nResponsible for the data governance and data management framework implementation for a given domain of expertise (Research, Development, Supply Chain, etc. ).\nResponsible for the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nDrives cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles. Create and maintain privacy policies and procedures to protect sensitive data and ensure compliance.\nConduct regular privacy risk assessments and audits to identify and mitigate potential risks as required\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. for assigned domains.\nEnsure compliance requirements with data privacy, security, and regulatory policies for the assigned domains including GDPR, CCPA, and other relevant legislations.\nJointly with Technology teams, business functions, and enterprise teams (e. g. , MDM, Enterprise Data Fabric, etc. ) define the specifications shaping the development and implementation of data foundations .\nBuild strong relationships with key business leads and partners to ensure their needs are being met\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills with knowledge of Pharma processes with specialization in a domain (e. g. , Research, Clinical Trials, Commercial, etc. )\nIn depth knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc. Strong understanding of data protection laws and regulations, including GDPR, CCPA, and other relevant legislations.\nIn depth experience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer-focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nIn depth experience of working with or supporting systems used to data governance framework. E. g. Collibra, Alation\nExcellent problem-solving skills and committed attention to detail in finding solutions\nGood-to-Have Functional Skills:\nExperience of working with data governance councils or forums\nExperience with Agile software development methodologies (Scrum)\nProficiency in data analysis and quality tools (e. g. , SQL, Excel, Python, or SAS)\n3-5 years of experience in data privacy, compliance, or a related field.\nSoft Skills:\nIntegrity: Commitment to maintaining the highest ethical standards and protecting confidential information.\nAdaptability: Ability to adapt to changing regulations and emerging privacy challenges.\nProactivity: Self-motivated with a proactive approach to identifying and addressing privacy issues.\nLeadership: Strong leadership skills and the ability to influence and drive change within the organization.\nHighly organized and able to work under minimal supervision\nExcellent analytical and assessment skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAmbitious to further develop their skills and career\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills\nHigh degree of initiative and self-motivation.\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management and customer focus.\nBasic Qualifications:\nAny Degree and 9-13 years of experience\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Business process', 'Data analysis', 'Change management', 'SAS', 'Pharma', 'Analytical', 'Clinical trials', 'Data quality', 'SQL']",2025-06-12 15:14:27
Data Specialist - Development,Horizon Therapeutics,9 - 13 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgens data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leverages state-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. You will leverage domain, technical and business process expertise to provide exceptional support of Amgen s data governance framework. This role involves working closely with business stakeholders and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with the Product Owner and other Business Analysts to ensure operational support and excellence from the team.\nRoles Responsibilities:\nResponsible for the data governance and data management framework implementation for the Development domain of the biopharma lifecycle.\nResponsible for the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nDrives cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles.\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. for assigned domains.\nEnsure compliance requirements with data privacy, security, and regulatory policies for the assigned domains\nJointly with Technology teams, business functions, and enterprise teams (e. g. , MDM, Enterprise Data Fabric, etc. ) define the specifications shaping the development and implementation of data foundations .\nBuild strong relationships with key business leads and partners to ensure their needs are being met\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills with knowledge of Pharma processes with specialization in the Development domain of the biopharma lifecycle.\nIn depth knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc.\nIn depth experience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer-focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nIn depth experience of working with or supporting systems used to data governance framework. E. g. Collibra, Alation\nExcellent problem-solving skills and committed attention to detail in finding solutions\nGood-to-Have Functional Skills:\nExperience of working with data governance councils or forums\nExperience with Agile software development methodologies (Scrum)\nProficiency in data analysis and quality tools (e. g. , SQL, Excel, Python, or SAS)\nSoft Skills:\nHighly organized and able to work under minimal supervision\nExcellent analytical and assessment skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAmbitious to further develop their skills and career\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills\nHigh degree of initiative and self-motivation.\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management and customer focus.\nBasic Qualifications:\nAny Degree and 9-13 years of experience\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business process', 'Data analysis', 'Change management', 'operational support', 'SAS', 'Pharma', 'Analytical', 'Agile', 'Data quality', 'SQL']",2025-06-12 15:14:30
Technical Consultant(ETL + SQL + Data Migration),Insightsoftware,4 - 7 years,Not Disclosed,['Hyderabad'],"Insightsoftware (ISW) is a growing, dynamic computer software company that helps businesses achieve greater levels of financial intelligence across their organization with our world-class financial reporting solutions. At insightsoftware, you will learn and grow in a fast-paced, supportive environment that will take your career to the next level. The Data Conversion Specialist is a member of the insightsoftware Project Management Office (PMO) who demonstrates teamwork, results orientation, a growth mindset, disciplined execution, and a winning attitude.\nLocation: Hyderabad (Work from Office)\nWorking Hours: 5:00 PM - 2:00AM IST or 6:00 PM to 3:00 AM IS T, should be ok to work in night shift as per requirement.\nPosition Summary\nThe Consultant will integrate and map customer data from client source system(s) to our industry-leading platform. The role will include, but is not limited to:\nUsing strong technical data migration, scripting, and organizational skills to ensure the client data is converted efficiently and accurately to the insightsoftware (ISW) platform.\nPerforming extract, transform, load (ETL) activities to ensure accurate and timely data conversions.\nProviding in-depth research and analysis of complex scenarios to develop innovative solutions to meet customer needs whilst remaining within project governance.\nMapping and maintaining business requirements to the solution design using tools such as requirements traceability matrices (RTM).\nPresenting findings, requirements, and problem statements for ratification by stakeholders and working groups.\nIdentifying and documenting data gaps to allow change impact and downstream impact analysis to be conducted.\nExperience assessing data and analytic requirements to establish mapping rules from source to target systems to meet business objectives.\nExperience with real-time, batch, and ETL for complex data conversions.\nWorking knowledge of extract, transform, load (ETL) methodologies and tools such as Talend, Dell Boomi, etc.\nUtilize data mapping tools to prepare data for data loads based on target system specifications.\nWorking experience using various data applications/systems such as Oracle SQL, Excel, .csv files, etc.\nStrong SQL scripting experience.\nCommunicate with clients and/or ISW Project Manager to scope, develop, test, and implement conversion/integration\nEffectively communicate with ISW Project Managers and customers to keep project on target\nContinually drive improvements in the data migration process.\nCollaborate via phone and email with clients and/or ISW Project Manager throughout the conversion/integration process.\nDemonstrated collaboration and problem-solving skills.\nWorking knowledge of software development lifecycle (SDLC) methodologies including, but not limited to: Agile, Waterfall, and others.\nClear understanding of cloud and application integrations.\nAbility to work independently, prioritize tasks, and manage multiple tasks simultaneously.\nEnsure client s data is converted/integrated accurately and within deadlines established by ISW Project Manager.\nExperience in customer SIT, UAT, migration and go live support.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data migration', 'Data conversion', 'Financial reporting', 'Project management', 'project governance', 'Agile', 'Software development life cycle', 'data mapping', 'SDLC', 'Downstream']",2025-06-12 15:14:32
Data Architect Telecom Domain databrick BSS OSS,fast growing Data Driven IT solutions an...,10 - 20 years,45-55 Lacs P.A.,"['Noida', 'Hyderabad', 'Gurugram']","Data Architect Telecom Domain\nTo design comprehensive data architecture and technical solutions specifically for telecommunications industry challenges, leveraging TMforum frameworks and modern data platforms. To work closely with customers, and technology partners to deliver data solutions that address complex telecommunications business requirements including customer experience management, network optimization, revenue assurance, and digital transformation initiatives.\nResponsibilities:\nDesign and articulate enterprise-scale telecom data architectures incorporating TMforum standards and frameworks, including SID (Shared Information/Data Model), TAM (Telecom Application Map), and eTOM (enhanced Telecom Operations Map)\nDevelop comprehensive data models aligned with TMforum guidelines for telecommunications domains such as Customer, Product, Service, Resource, and Partner management\nCreate data architectures that support telecom-specific use cases including customer journey analytics, network performance optimization, fraud detection, and revenue assurance\nDesign solutions leveraging Microsoft Azure and Databricks for telecom data processing and analytics\nConduct technical discovery sessions with telecom clients to understand their OSS/BSS architecture, network analytics needs, customer experience requirements, and digital transformation objectives\nDesign and deliver proof of concepts (POCs) and technical demonstrations showcasing modern data platforms solving real-world telecommunications challenges\nCreate comprehensive architectural diagrams and implementation roadmaps for telecom data ecosystems spanning cloud, on-premises, and hybrid environments\nEvaluate and recommend appropriate big data technologies, cloud platforms, and processing frameworks based on telecom-specific requirements and regulatory compliance needs.\nDesign data governance frameworks compliant with telecom industry standards and regulatory requirements (GDPR, data localization, etc.)\nStay current with the latest advancements in data technologies including cloud services, data processing frameworks, and AI/ML capabilities\nContribute to the development of best practices, reference architectures, and reusable solution components for accelerating proposal development\nQualifications:\nBachelor's or Master's degree in Computer Science, Telecommunications Engineering, Data Science, or a related technical field\n10+ years of experience in data architecture, data engineering, or solution architecture roles with at least 5 years in telecommunications industry\nDeep knowledge of TMforum frameworks including SID (Shared Information/Data Model), eTOM, TAM, and their practical implementation in telecom data architectures\nDemonstrated ability to estimate project efforts, resource requirements, and implementation timelines for complex telecom data initiatives\nHands-on experience building data models and platforms aligned with TMforum standards and telecommunications business processes\nStrong understanding of telecom OSS/BSS systems, network management, customer experience management, and revenue management domains\nHands-on experience with data platforms including Databricks, and Microsoft Azure in telecommunications contexts\nExperience with modern data processing frameworks such as Apache Kafka, Spark and Airflow for real-time telecom data streaming\nProficiency in Azure cloud platform and its respective data services with an understanding of telecom-specific deployment requirements\nKnowledge of system monitoring and observability tools for telecommunications data infrastructure\nExperience implementing automated testing frameworks for telecom data platforms and pipelines\nFamiliarity with telecom data integration patterns, ETL/ELT processes, and data governance practices specific to telecommunications\nExperience designing and implementing data lakes, data warehouses, and machine learning pipelines for telecom use cases\nProficiency in programming languages commonly used in data processing (Python, Scala, SQL) with telecom domain applications\nUnderstanding of telecommunications regulatory requirements and data privacy compliance (GDPR, local data protection laws)\nExcellent communication and presentation skills with ability to explain complex technical concepts to telecom stakeholders\nStrong problem-solving skills and ability to think creatively to address telecommunications industry challenges\nGood to have TMforum certifications or telecommunications industry certifications\nRelevant data platform certifications such as Databricks, Azure Data Engineer are a plus\nWillingness to travel as required\nif you will all or most of the criteria contact bdm@intellisearchonline.net M 9341626895",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Telecom Bss', 'Data Architect', 'Telecom OSS', 'ETOM', 'Data Bricks']",2025-06-12 15:14:34
Data Specialist - G&A,Horizon Therapeutics,9 - 13 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgens data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leverages state-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. You will leverage domain, technical and business process expertise to provide exceptional support of Amgen s data governance framework. This role involves working closely with business stakeholders and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with the Product Owner and other Business Analysts to ensure operational support and excellence from the team.\nRoles Responsibilities:\nResponsible for the data governance and data management framework implementation for the General and Administrative operations (GA) domain of the biopharma lifecycle.\nResponsible for the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nDrives cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles.\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. for assigned domains.\nEnsure compliance requirements with data privacy, security, and regulatory policies for the assigned domains\nJointly with Technology teams, business functions, and enterprise teams (e. g. , MDM, Enterprise Data Fabric, etc. ) define the specifications shaping the development and implementation of data foundations .\nBuild strong relationships with key business leads and partners to ensure their needs are being met\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills with knowledge of Pharma processes with specialization in the General and Administrative operations (GA) domain of the biopharma lifecycle.\nIn depth knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc.\nIn depth experience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer-focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nIn depth experience of working with or supporting systems used to data governance framework. E. g. Collibra, Alation\nExcellent problem-solving skills and committed attention to detail in finding solutions\nGood-to-Have Functional Skills:\nExperience of working with data governance councils or forums\nExperience with Agile software development methodologies (Scrum)\nProficiency in data analysis and quality tools (e. g. , SQL, Excel, Python, or SAS)\nSoft Skills:\nHighly organized and able to work under minimal supervision\nExcellent analytical and assessment skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAmbitious to further develop their skills and career\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills\nHigh degree of initiative and self-motivation.\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management and customer focus.\nBasic Qualifications:\nAny Degree and 9-13 Years of Experience\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business process', 'Administration', 'Data analysis', 'Change management', 'operational support', 'SAS', 'Pharma', 'Analytical', 'Data quality', 'SQL']",2025-06-12 15:14:36
Data Specialist - Commercial,Horizon Therapeutics,9 - 13 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgens data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leverages state-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. You will leverage domain, technical and business process expertise to provide exceptional support of Amgen s data governance framework. This role involves working closely with business stakeholders and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with the Product Owner and other Business Analysts to ensure operational support and excellence from the team.\nRoles Responsibilities:\nResponsible for the data governance and data management framework implementation for the Commercialization domain of the biopharma lifecycle.\nResponsible for the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nDrives cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles.\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. for assigned domains.\nEnsure compliance requirements with data privacy, security, and regulatory policies for the assigned domains\nJointly with Technology teams, business functions, and enterprise teams (e. g. , MDM, Enterprise Data Fabric, etc. ) define the specifications shaping the development and implementation of data foundations .\nBuild strong relationships with key business leads and partners to ensure their needs are being met\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills with knowledge of Pharma processes with specialization in the Commercialization domain of the biopharma lifecycle.\nIn depth knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc.\nIn depth experience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer-focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nIn depth experience of working with or supporting systems used to data governance framework. E. g. Collibra, Alation\nExcellent problem-solving skills and committed attention to detail in finding solutions\nGood-to-Have Functional Skills:\nExperience of working with data governance councils or forums\nExperience with Agile software development methodologies (Scrum)\nProficiency in data analysis and quality tools (e. g. , SQL, Excel, Python, or SAS)\nSoft Skills:\nHighly organized and able to work under minimal supervision\nExcellent analytical and assessment skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAmbitious to further develop their skills and career\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills\nHigh degree of initiative and self-motivation.\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management and customer focus.\nBasic Qualifications:\nAny Degree and 9-13 years of experience\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business process', 'Data analysis', 'Change management', 'operational support', 'SAS', 'Pharma', 'Analytical', 'Agile', 'Data quality', 'SQL']",2025-06-12 15:14:38
Data Specialist - Supply Chain,Horizon Therapeutics,9 - 13 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgens data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leverages state-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. You will leverage domain, technical and business process expertise to provide exceptional support of Amgen s data governance framework. This role involves working closely with business stakeholders and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with the Product Owner and other Business Analysts to ensure operational support and excellence from the team.\nRoles Responsibilities:\nResponsible for the data governance and data management framework implementation for the Supply Chain domain of the biopharma lifecycle.\nResponsible for the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nDrives cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles.\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. for assigned domains.\nEnsure compliance requirements with data privacy, security, and regulatory policies for the assigned domains\nJointly with Technology teams, business functions, and enterprise teams (e. g. , MDM, Enterprise Data Fabric, etc. ) define the specifications shaping the development and implementation of data foundations .\nBuild strong relationships with key business leads and partners to ensure their needs are being met\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills with knowledge of Pharma processes with specialization in the Supply Chain domain of the biopharma lifecycle.\nIn depth knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc.\nIn depth experience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer-focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nIn depth experience of working with or supporting systems used to data governance framework. E. g. Collibra, Alation\nExcellent problem-solving skills and committed attention to detail in finding solutions\nGood-to-Have Functional Skills:\nExperience of working with data governance councils or forums\nExperience with Agile software development methodologies (Scrum)\nProficiency in data analysis and quality tools (e. g. , SQL, Excel, Python, or SAS)\nSoft Skills:\nHighly organized and able to work under minimal supervision\nExcellent analytical and assessment skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAmbitious to further develop their skills and career\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills\nHigh degree of initiative and self-motivation.\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management and customer focus.\nBasic Qualifications:\nAny Degree and 9-13 years of experience\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Business process', 'Data analysis', 'Change management', 'operational support', 'SAS', 'Pharma', 'Analytical', 'Data quality', 'SQL']",2025-06-12 15:14:41
Job opening For Data Warehouse + ADF + ETL,bct,3 - 6 years,Not Disclosed,['Pune'],"Greetings of the Day !!!\n\nWe have job opening for Data Warehouse + ADF + ETL with one of our Client .If you are interested for this role , kindly share update resume along with below details in this email id : shaswati.m@bct-consulting.com\n\nJob Description:\nSenior Data Engineer\nAs a Senior Data Engineer, you will support the European World Area using the Windows & Azure suite of Analytics & Data platforms. The focus of the role is on the technical aspects and implementation of data gathering, integration and database design.\nWe look forward to seeing your application!\nIn This Role, Your Responsibilities Will Be:\nData Ingestion and Integration: Collaborate with Product Owners and analysts to understand data requirements & design, develop, and maintain data pipelines for ingesting, transforming, and integrating data from various sources into Azure Data Services.\nMigration of existing ETL packages: Migrate existing SSIS packages to Synapse pipelines\nData Modelling: Assist in designing and implementing data models, data warehouses, and databases in Azure Synapse Analytics, Azure Data Lake Storage, and other Azure services.\nData Transformation: Develop ETL (Extract, Transform, Load) processes using SQL Server Integration Services (SSIS), Azure Synapse Pipelines, or other relevant tools to prepare data for analysis and reporting.\nData Quality and Governance: Implement data quality checks and data governance practices to ensure the accuracy, consistency, and security of data assets.\nMonitoring and Optimization: Monitor and optimize data pipelines and workflows for performance, scalability, and cost efficiency.\nDocumentation: Maintain comprehensive documentation of processes, including data lineage, data dictionaries, and pipeline schedules.\nCollaboration: Work closely with cross-functional teams, including data analysts, data scientists, and business stakeholders, to understand their data needs and deliver solutions accordingly.\nAzure Services: Stay updated on Azure data services and best practices to recommend and implement improvements in our data architecture and processes\nFor This Role, You Will Need:\n3-5 years of experience in Data Warehousing with On-Premises or Cloud technologies\nStrong practical experience of Synapse pipelines / ADF.\nStrong practical experience of developing ETL packages using SSIS.\nStrong practical experience with T-SQL or any variant from other RDBMS.\nGraduate degree educated in computer science or a relevant subject.\nStrong analytical and problem-solving skills.\nStrong communication skills in dealing with internal customers from a range of functional areas.\nWillingness to work flexible working hours according to project requirements.\nTechnical documentation skills.\nFluent in English.\nPreferred Qualifications that Set You Apart:\nOracle PL/SQL.\nExperience in working on Azure Services like Azure Synapse Analytics, Azure Data Lake.\nWorking experience with Azure DevOps paired with knowledge of Agile and/or Scrum methods of delivery.\nLanguages: French, Italian, or Spanish would be an advantage.\nAgile certification.\nThanks,\nShaswati",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ADF', 'ETL', 'SSIS', 'Data ware house']",2025-06-12 15:14:43
Data Governance & Data Quality Sr Associate Analyst,Amgen Inc,2 - 5 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgen's data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leveragesstate-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. This role involves working closely with business stakeholder and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with Data Product Owners, Data Stewards and technology teams to increase the trust and reuse of data across Amgen.\nRoles & Responsibilities:\nResponsible for the execution of data governance framework for a given domain of expertise (Research, Development, Supply Chain, etc.).\nContribute to the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nContribute to the cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles.\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. for assigned domains.\nPartner with business teams to identify compliance requirements with data privacy, security, and regulatory policies for the assigned domains\nJointly with Technology teams, business functions, and enterprise teams (e.g., MDM, Enterprise Data Fabric, etc.) delivers data foundations.\nBuild strong relationship with key business leads and partners to ensure their needs are met.\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills (Advanced SQL, Python etc) with knowledge of Pharma processes with specialization in a domain (e.g., Research, Clinical Trials, Commercial, etc.)\nExperience of working with or supporting systems used to data governance framework. E.g. Collibra, Alation\nGeneral knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc.\nExperience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nExcellent problem-solving skills and a committed attention to detail in finding solutions\nGood-to-Have Functional Skills:\nExperience with Agile software development methodologies (Scrum)\nSoft Skills:\nExcellent analytical skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAbility to build business relationships and understand end-to-end data use and needs.\nStrong verbal and written communication skills\nBasic Qualifications:\nExperience with 5 - 9 years of experience in Business, Engineering, IT or related field",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Governance', 'data quality', 'Collibra', 'data stewardship', 'metadata management', 'Agile software development methodologies', 'Alation', 'data protection', 'master data management', 'SQL', 'Python']",2025-06-12 15:14:45
Data Science (SSE) | FINJO I766,Omni Recruit,3 - 8 years,Not Disclosed,['Mumbai (All Areas)'],"Python Developer\nWork from Office\nLocation : Airoli , Navi Mumbai\n& :\n3.55 years of relevant experience in Python\nMinimum 3.5 years in Python programming\nAt least 1+ year in machine learning and natural language processing (NLP)\nMinimum 1.5 years with LLMs and GenAI\nAt least 2 years of experience with any database\n1+ year of experience deploying ML models/Python applications on Azure or AWS",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Machine Learning', 'Python']",2025-06-12 15:14:47
"SDE, IIoT, Decision Science and Technology (DST)",Amazon,3 - 8 years,Not Disclosed,['Hyderabad'],"Interested in creating systems and services that bring the power of Machine Learning (ML) to new application fields? With IIoT products, our organization is setting the standard for high-performance, easy-to-use, and cost-effective ML services. Our team expands this portfolio to new applications, enhancing our condition-based maintenance program, and maximizing equipment availability.\n\nAs a Software Development Engineer, you will be responsible for designing, developing, testing, and deploying distributed machine learning systems and large-scale solutions for our world-wide customer base. In this role, you will collaborate closely with a team of research and applied scientists to influence our overall strategy and define the team s roadmap. You will also drive the system architecture, spearhead best practices that enable a quality product, and help coach and develop junior engineers. A successful candidate will have an established background in engineering large-scale software systems, a strong technical ability, great communication skills, and a motivation to achieve results in a fast-paced environment.\n\nOur team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we re building an environment that celebrates knowledge sharing and mentorship. Our senior members enjoy one-on-one mentoring and thorough, but kind, code reviews. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded engineer and enable them to take on more complex tasks in the future.\n\n\nSolving difficult problems with elegant and practical code\nHelping define engineering best practices and providing technical mentorship to other members of the engineering team\nBeing thoughtful for the customer and ensuring their needs come first\nDesigning and building software for a multitude of sensors (vibration, temperature), mobile clients, and back-end cloud server systems\n\nYoure an awesome fit if you demonstrate:\n\nIndustry-leading technical abilities show-casing a breadth and depth of technical knowledge\nThe ability to build good working relationships within the team by communicating clearly both verbally and in writing\nStrong problem solving and trouble shooting skills with the ability to come up with creative solutions to seemingly impossible problems\nEffective technical leadership skills to improve technologies and infrastructure of the team\nAre curious trying new technologies, and passionate about innovating on behalf of customers\n\nAbout the team\nDST combines the expertise from talented program, product managers, engineers, and scientists to create programs and products that support such programs to drive cost optimization, and prevent events (e.g., unplanned downtime) that negatively impact customer experience. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Coding', 'Machine learning', 'Architectural design', 'Software development life cycle', 'Technical leadership', 'Sensors', 'Customer experience', 'Internship']",2025-06-12 15:14:49
"SDE, IIoT, Decision Science and Technology",Amazon,3 - 8 years,Not Disclosed,['Hyderabad'],"Interested in creating systems and services that bring the power of Machine Learning (ML) to new application fields? With IIoT products, our organization is setting the standard for high-performance, easy-to-use, and cost-effective ML services. Our team expands this portfolio to new applications, enhancing our condition-based maintenance program, and maximizing equipment availability.\n\nAs a Software Development Engineer, you will be responsible for designing, developing, testing, and deploying distributed machine learning systems and large-scale solutions for our world-wide customer base. In this role, you will collaborate closely with a team of research and applied scientists to influence our overall strategy and define the team s roadmap. You will also drive the system architecture, spearhead best practices that enable a quality product, and help coach and develop junior engineers. A successful candidate will have an established background in engineering large-scale software systems, a strong technical ability, great communication skills, and a motivation to achieve results in a fast-paced environment.\n\nOur team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we re building an environment that celebrates knowledge sharing and mentorship. Our senior members enjoy one-on-one mentoring and thorough, but kind, code reviews. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded engineer and enable them to take on more complex tasks in the future.\n\n\nSolving difficult problems with elegant and practical code\nHelping define engineering best practices and providing technical mentorship to other members of the engineering team\nBeing thoughtful for the customer and ensuring their needs come first\nDesigning and building software for a multitude of sensors (vibration, temperature), mobile clients, and back-end cloud server systems\n\nYoure an awesome fit if you demonstrate:\n\nIndustry-leading technical abilities show-casing a breadth and depth of technical knowledge\nThe ability to build good working relationships within the team by communicating clearly both verbally and in writing\nStrong problem solving and trouble shooting skills with the ability to come up with creative solutions to seemingly impossible problems\nEffective technical leadership skills to improve technologies and infrastructure of the team\nAre curious trying new technologies, and passionate about innovating on behalf of customers\n\nAbout the team\nDST combines the expertise from talented program, product managers, engineers, and scientists to create programs and products that support such programs to drive cost optimization, and prevent events (e.g., unplanned downtime) that negatively impact customer experience. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Coding', 'Machine learning', 'Architectural design', 'Software development life cycle', 'Technical leadership', 'Sensors', 'Customer experience', 'Internship']",2025-06-12 15:14:52
AI-Enabler Custom Developer,Hoffmann La Roche,2 - 4 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position\nThroughout our 125-year history, Roche has grown into one of the world s largest biotech companies and a global supplier of transformative, innovative solutions across major disease areas.\n\nWe are now entering an exciting new chapter of our digital transformation journey by embracing the power of Artificial Intelligence. In line with our Roche Group AI Strategy and our 10-Year Ambition to Transform our business with data & digital solutions, we are developing AI capabilities across all levels of the organization from EverydayAI, which enhances individual productivity, to Reshape initiatives, which reimagine business processes, to Big Ideas, which push the boundaries of what s possible in healthcare.\nWe are looking for forward-thinking professionals to join Roche Informatics and help us bring this strategy to life.\n\nPune continues to play the role of a Technology Acceleration Hub, building capabilities that drive digital innovation, including cutting-edge AI solutions that support Roche s mission to prevent, stop, or cure diseases with the highest societal burden.\n\nOur Expectations\nWe are looking for a Software Engineer eager to develop and implement AI-powered solutions within Roches\ntechnology ecosystem. The ideal candidate should have a strong foundation in software development, a willingness to\nupskill in AI and Generative AI technologies, and the ability to integrate large language models (LLMs) into applications\nand software development processes (testing, refactoring, requirements management, deployment).\n\nKey Competencies & Skills\nAI Expertise\nUnderstanding of how LLMs work, their strengths, limitations, and practical applications.\nExperience in basic prompt engineering.\nFamiliarity with direct LLM API usage (e.g., OpenAI API, SDKs).\nConceptual understanding of RAG architecture.\nHands-on experience with libraries to create basic LLM workflows (e.g., LangChain, LlamaIndex) and\nvector databases (e.g., Qdrant) is a plus.\nBasic understanding of NLP concepts such as tokens and embeddings.\nSoftware Development & Cloud Engineering:\nStrong programming skills in at least one language (e.g., Python, Go, TypeScript, Java, Kotlin) with a\ngood understanding of tooling, ecosystem, and software development best practices.\nExperience with API usage and basic understanding of cloud-based AI deployments (AWS/Azure/GCP).\nFamiliarity with AI software development tools (e.g., Github Copilot).\n\nDevOps Practices:\nUnderstanding of CI/CD pipelines and automation testing.\nFamiliarity with GitHub and GitLab.\n\nCollaboration & Knowledge Sharing:\nGood communication skills in English (B2/C1 level) to work within cross-functional teams.\nAbility to collaborate with Engineers and potentially Data Scientists on AI-driven enhancements.\nKey Responsibilities\nDesign, develop, and optimize custom applications, potentially incorporating AI-powered features within Roche s\necosystem.\nIntegrate Large Language Models (LLMs) into custom applications, leveraging APIs and prompt engineering\ntechniques, under guidance.\nUtilize cloud-based AI services (AWS, Azure, or GCP) to build basic AI-driven functionalities.\nCollaborate with internal teams to apply AI for the enhancement of custom applications.\nContribute to identifying opportunities for AI to improve application functionality.\nParticipate in knowledge sharing activities within the team.\n\nExample Projects You May Work On\nIntegrating AI features into existing custom applications to improve user experience.\nDeveloping basic AI-powered tools to aid in application development.\nAssisting in the integration of AI into application workflows.\n\nWhat We Value\nGood analytical and problem-solving skills.\nAdaptability to learn about AI and work in Agile environments.\nCuriosity and a willingness to take ownership of tasks.\nWho we are\nA healthier future drives us to innovate. Together, more than 100 000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. Our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our Diagnostics products. We empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Basic', 'github', 'Usage', 'GCP', 'Analytical', 'Artificial Intelligence', 'Agile', 'Healthcare', 'Application development', 'Python']",2025-06-12 15:14:54
AI-Enabler Custom Developer,Roche Diagnostics,2 - 4 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position\nThroughout our 125-year history, Roche has grown into one of the world s largest biotech companies and a global supplier of transformative, innovative solutions across major disease areas.\n\nWe are now entering an exciting new chapter of our digital transformation journey by embracing the power of Artificial Intelligence. In line with our Roche Group AI Strategy and our 10-Year Ambition to Transform our business with data & digital solutions, we are developing AI capabilities across all levels of the organization from EverydayAI, which enhances individual productivity, to Reshape initiatives, which reimagine business processes, to Big Ideas, which push the boundaries of what s possible in healthcare.\nWe are looking for forward-thinking professionals to join Roche Informatics and help us bring this strategy to life.\n\nPune continues to play the role of a Technology Acceleration Hub, building capabilities that drive digital innovation, including cutting-edge AI solutions that support Roche s mission to prevent, stop, or cure diseases with the highest societal burden.\n\nOur Expectations\nWe are looking for a Software Engineer eager to develop and implement AI-powered solutions within Roches\ntechnology ecosystem. The ideal candidate should have a strong foundation in software development, a willingness to\nupskill in AI and Generative AI technologies, and the ability to integrate large language models (LLMs) into applications\nand software development processes (testing, refactoring, requirements management, deployment).\n\nKey Competencies & Skills\nAI Expertise\nUnderstanding of how LLMs work, their strengths, limitations, and practical applications.\nExperience in basic prompt engineering.\nFamiliarity with direct LLM API usage (e.g., OpenAI API, SDKs).\nConceptual understanding of RAG architecture.\nHands-on experience with libraries to create basic LLM workflows (e.g., LangChain, LlamaIndex) and\nvector databases (e.g., Qdrant) is a plus.\nBasic understanding of NLP concepts such as tokens and embeddings.\nSoftware Development & Cloud Engineering:\nStrong programming skills in at least one language (e.g., Python, Go, TypeScript, Java, Kotlin) with a\ngood understanding of tooling, ecosystem, and software development best practices.\nExperience with API usage and basic understanding of cloud-based AI deployments (AWS/Azure/GCP).\nFamiliarity with AI software development tools (e.g., Github Copilot).\n\nDevOps Practices:\nUnderstanding of CI/CD pipelines and automation testing.\nFamiliarity with GitHub and GitLab.\n\nCollaboration & Knowledge Sharing:\nGood communication skills in English (B2/C1 level) to work within cross-functional teams.\nAbility to collaborate with Engineers and potentially Data Scientists on AI-driven enhancements.\nKey Responsibilities\nDesign, develop, and optimize custom applications, potentially incorporating AI-powered features within Roche s\necosystem.\nIntegrate Large Language Models (LLMs) into custom applications, leveraging APIs and prompt engineering\ntechniques, under guidance.\nUtilize cloud-based AI services (AWS, Azure, or GCP) to build basic AI-driven functionalities.\nCollaborate with internal teams to apply AI for the enhancement of custom applications.\nContribute to identifying opportunities for AI to improve application functionality.\nParticipate in knowledge sharing activities within the team.\n\nExample Projects You May Work On\nIntegrating AI features into existing custom applications to improve user experience.\nDeveloping basic AI-powered tools to aid in application development.\nAssisting in the integration of AI into application workflows.\n\nWhat We Value\nGood analytical and problem-solving skills.\nAdaptability to learn about AI and work in Agile environments.\nCuriosity and a willingness to take ownership of tasks.\nWho we are\n.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Basic', 'github', 'Usage', 'GCP', 'Analytical', 'Artificial Intelligence', 'Agile', 'Healthcare', 'Application development', 'Python']",2025-06-12 15:14:56
Sr. Technology Auditor,AMERICAN EXPRESS,2 - 4 years,13-18 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Role & responsibilities\n•       Translate business risks, controls and supporting data into analytic requirements and partners with colleagues to build effective analytics and insights\n•       Responsible for multiple simultaneous audit projects of all sizes and complexity across multiple business areas within and outside of local region, in unfamiliar areas, and for different audit leaders\n•       Link analytics and insights to ongoing strategic initiatives\n•       Apply proven/ advanced data algorithms, advanced analytic and modeling techniques to draw insights essential to driving improvement initiatives",,,,"['Natural Language Processing', 'Tableau', 'Machine Learning', 'SQL', 'Python']",2025-06-12 15:14:59
Strategic Systems Director,Ericsson,10 - 15 years,Not Disclosed,['Noida'],"About this opportunity:\nNetworks are evolving rapidly, and so is the way we manage them. Our Managed Services business is at the forefront of industry change, leveraging the capabilities of our innovative Ericsson Operations Engine, which integrates autonomous operations to enhance efficiency and effectiveness. This transformation combines our community of creative, driven individuals with digital capabilities like automation, machine learning, and autonomous operations at the core of the Ericsson Operations Engine.\nWe are spearheading a digital transformation at scale, redefining how networks are managed globally across the industry. Are you ready to become a change agent in our forward-thinking organization.\nWhat you will do:\nTechnically lead a business-critical transformation programs by collaborating with forward-thinking colleagues from diverse cultures and technology partners worldwide to reshape our platform architecture to cloud-native.\nDesign the optimal solution within the constraints of specifications, costs, products, quality, and timelines, and be ultimately accountable for its successful implementation, ensuring the final solution meets business needs.\nInspire change and improvements within and across functions/units, processes, and value flows, with a focus on incorporating autonomous operations.\nThe skills you bring:\nTotal industry experience of 10 years+\nExperience as a leader or innovator enabling technology transformation and delivering strategic direction\nExpertise in cross-domain/E2E solution architecture\nAbility to assume responsibility and coordination for solution architecture/design and lead technical issues, with a focus on autonomous operations\nStrong technical and business insight related to software delivery and operations in a cloud solution environment\nTeam leadership capability, with experience in directly leading the technical acceptance of the solution\nAbility to work and produce targeted results with minimal supervision\nSelf-starter with a strong sense of business ownership and leadership\nMotivation and commitment to meeting challenging individual and team deadlines\nIndependence and self-direction\nCapability to create order in an unstructured environment\nExperience in large and complex system integration projects\nStrong skills in system engineering and architecture design methodologies\nBusiness understanding and critical thinking capabilities to develop creative solutions to opportunities\nCommunication and presentation skills to confidently engage and influence at all levels within the company\nCultural awareness and excellent interpersonal communications and networking skills\nThought leadership in identifying short- and long-term priorities to improve business strategy, with a focus on autonomous operations",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'Automation', 'Managed services', 'Networking', 'Focus', 'System integration', 'Architectural design', 'Machine learning', 'Business strategy', 'Business understanding']",2025-06-12 15:15:01
"Technical Training Specialist, Staff",Qualcomm,6 - 11 years,Not Disclosed,['Gurugram'],"Job Area: Engineering Services Group, Engineering Services Group > Technical Training\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nJob TitleTechnical Training Specialist, Staff\n\nJob Overview:\n\nIn collaboration with subject matter experts, develop high-quality technical training materials for use in training courses geared towards engineers and business professionals, with keen attention to quality and engaging learning experience.\n\nJob Overview:\n\nThe primary responsibility of this role is to teach AI courses, with a strong emphasis on AI-related Qualcomm technologies. In collaboration with subject matter experts, you will develop and deliver high-quality technical training materials for engineers and business professionals, ensuring an engaging and effective learning experience.\n\nAdditional :\n\nKnowledge and\n\nSkills:\n\nDemonstrated proficiency in designing, developing, and delivering a variety of technical training programs, with a strong focus on AI and AI-related Qualcomm technologies, for a technical workforce encompassing engineering, business, IT, and other technical professionals.\nRobust practical knowledge of Instructional Design Methodologies and Adult Learning Theory, particularly as they apply to AI and advanced technology training.\nExcellent written and verbal communication skills in English, with the ability to convey complex AI concepts and Qualcomm technologies clearly and effectively.\nUncompromising approach towards content quality, accuracy, and effectiveness, ensuring that training materials are both informative and engaging.\nDemonstrable prior leadership experienceproven track record to successfully lead a team and drive a variety of projects to completion in a dynamic work environment.\nSuperb organizational skills, with the ability to prioritize and manage multiple simultaneous tasks in a systematic, process-oriented manner.\nAbility to work with and gain a deep understanding of highly technical content spanning multiple domains such as 5G, Artificial Intelligence, Extended Reality, etc.\nExperience in developing and managing self-paced online training using tools like Adobe Captivate and Camtasia is a plus.\n\n\nPreferred Qualifications:\nMasters degree in Educational Technology, Instructional Design, or related fields.\n6+ years of relevant experience, preferably in the technology industry.\n\n\nA degree in engineering/technology fields is a must.\n\n\nAdditional Skills & Qualifications:\nPrior experience or ability to work with a variety of AI tools applicable to learning environments, with a focus on Qualcomm technologies.\nWillingness and ability to be available for online work meetings according to US time zones.\nAbility to provide prior work samples is a plus.\n\nIf you are a self-driven leader who excels in a collaborative environment and has a consultative, customer service orientation, we want to hear from you.",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['hiring', 'artificial intelligence', 'camtasia', 'staffing', 'online tutoring', 'technical writing', 'head hunting', 'leadership hiring', 'screening', 'framemaker', 'sourcing', 'technical hiring', 'talent acquisition', 'it recruitment', 'technical recruitment', 'recruitment', 'lateral hiring']",2025-06-12 15:15:04
Spark Scala + AWS & SQL,Cognizant,10 - 12 years,Not Disclosed,['Chennai'],Job Summary\nAs a Technical Lead specializing in Generative AI and Python you will play a pivotal role in driving innovation and excellence within our team. With 10 to 12 years of experience you will leverage your expertise to develop cutting-edge solutions that align with our companys strategic goals. This office-based position offers the opportunity to work in a dynamic environment during day shifts contributing to impactful projects that enhance our technological capabilities.,,,,"['algorithms', 'python', 'data analysis', 'technical leadership', 'software development', 'workflow', 'scala', 'machine learning', 'hibernate', 'artificial intelligence', 'scalability', 'sql', 'microservices', 'spring', 'spring boot', 'security', 'java', 'spark', 'ai techniques', 'j2ee', 'agile', 'aws', 'programming', 'agile methodology']",2025-06-12 15:15:06
Generative AI- Sr. Associate,Cognizant,7 - 10 years,Not Disclosed,['Chennai'],Job Summary\nShould have worked hands-on in setting up ML services on public cloud and delivering ML based solutions.\nShould have 7 to 10 years of experience in AI ML and at least 1 year in GenAI leveraging cloud based GenAI services.\nShould have through knowledge of end-to-end deployment of GenAI solutions to customers cutting across the industry prevalence of GenAI.\nResponsibilities,,,,"['screening', 'artificial intelligence', 'research', 'sourcing', 'sql', 'cloud', 'analytics', 'talent acquisition', 'java', 'data science', 'recruitment', 'model development', 'end', 'deployment', 'architecture', 'sr', 'ml', 'interfaces', 'python', 'natural language processing', 'microsoft azure', 'aiml', 'hrsd', 'machine learning', 'r', 'aws']",2025-06-12 15:15:09
Analytics & Visualization Developer,Qualcomm,7 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Programmer Analyst\n\nGeneral Summary:\n\nQualcomms Engineering IT EDAAP team is looking for an independent contributor experienced in development and sustaining enterprise level software applications. Experience:7-10 years of experience developing dashboard with reporting tools- Tableau (Tableau API), Power BI, OBIEE. SkillsMust:\nExpert in developing visualizations/dashboards with Tableau\nStrong knowledge with SQL\nFundamentals in object-oriented design, data structures, algorithms and problem solving.\nTest, debug and performance tuning of dashboards/reports\nExperience working in an agile development environment.\nTranslate ad hoc report requests into common dashboards and application requirements\nKnowledge of different types of enterprise systems, their interaction, boundaries within an enterprise.\nUnderstanding of complex data models.\nExperience working with Oracle/MySQl/Postgres\nWILLINGNESS to multi task and work in a fast paced environment.\nMust be willing to take ownership and drive tasks to completion. Desirable:\nPython programming experience.\nExperience developing dashboards with Power BI.\nExposure to Qlikview, OBIEE and ThoughtSpot\nExperience with semiconductor industry.\nExperience working with NoSQL Databases (MongoDB) as well as relational DBs (MySQL/Oracle) Education\nBachelor's degree in technical discipline or equivalent experience required.\n\nQualifications\n5 or more years of experience in applying AI and machine learning techniques to practical and comprehensive technology solutions.\nA strong background in machine learning, deep learning, and natural language processing.\nExpertise in ML, deep learning, Py Torch, Python, NLP and Transformer architecture.\nExperience in deploying LLMs, embedding model/sentence transformers in production use cases.\nThorough knowledge in basic algorithms, object-oriented and functional design principles, and best-practice patterns\nStrong expertise in programming (Rust/Python)\nExperience in fine-tuning a large language model using custom content (documents, data, code).\nExperience in developing Generative AI applications, Agentic Systems and Retrieval Augmented Generation.\nExperience working with large-scale datasets, preprocess them, and create appropriate data representations.\nSolid understanding of statistics, linear algebra, and probability theory.\n\nPreferred Qualifications\nBachelors/masters degree in computer science, Artificial Intelligence, Data Science, or a related field.\nExperience in implementing projects involving end to end ML/NLP systems from development to deployment.\nExperience with transformer-based models (e.g., BERT, GPT, T5, Llama).\nExperience working in a distributed team.\nExperience with cloud environments (GCP/AWS).\nWorking knowledge of Rust is a plus.\n\nMinimum Qualifications:\n4+ years of work experience in programming, scripting, and/or automation or IT-relevant work experience with a Bachelor's degree.\nOR\n6+ years of work experience in programming, scripting, and/or automation or IT-relevant work experience without a Bachelors degree.\n\n2+ years experience with Database Design structures such as Mongo DB, MySQL.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'natural language processing', 'machine learning', 'deep learning', 'pytorch', 'algorithms', 'functional design', 'dashboards', 'artificial intelligence', 'sql', 'database design', 'tableau', 'data visualization', 'design principles', 'linear algebra', 'ml', 'statistics']",2025-06-12 15:15:11
AIML Security Risk Assessment Specialist - Information Security,Hdfc Bank,7 - 12 years,Not Disclosed,['Mumbai (All Areas)'],"Job Summary\nThe AIML Security Risk Assessment Specialist will play a critical role in validating reports and making final risk assessments for AIML models used in various business applications and use cases. This role will work closely with the Digital Risk Management Portfolio team to ensure the security and integrity of AIML models, use case along with applications.\nKey Responsibilities\n1. Risk Assessment: understand the business requirement, finalise the scope and perform end to end risk assessment.",,,,"['Generative Ai', 'Cyber Security', 'Information Security']",2025-06-12 15:15:13
"Program Manager, Geospatial",Amazon,3 - 8 years,Not Disclosed,['Hyderabad'],"Amazon s Geospatial team is looking for a Program Manager to join our team. Our team s our vision is to provide the best mapping solution for logistics with focus on a safe, efficient and frictionless delivery experience. We build and operate software and hardware solutions that support drivers globally. We use innovative technology, such as machine learning, computer vision and large language learning models to enrich our maps. In order to so, we work with a portfolio of map vendors to enrich our maps with attributes important to transporters, as well as hardware vendors for strategic projects.\n\nIn this role, you manage complex initiatives, delivering critical solutions, significant improvements, new mechanisms, or deprecating processes that are no longer needed. These efforts require you to work with multiple teams in and/or across organizations.\n\nThe ideal candidate has extensive Program Management experience. This role also requires demonstrated experience managing cross functional relationships, performing financial analysis, creating and implementing processes and great communication skills to influence a variety of internal and external audiences. You are a self-starter and have the business acumen to unpack complex business needs. You thrive in a fast-paced environment and are comfortable with ambiguity.\n\n\nOwn program level goals and initiatives\nWork closely with Vendor Managers, Technology teams, Product teams, Legal and Finance teams to identify opportunities for standardizing processes\nManage procurement asks for strategic initiatives including creating POs and tracking against budgets\nDevelop internal and external governance mechanisms 3+ years of program or project management experience\n3+ years of working cross functionally with tech and non-tech teams experience\n3+ years of defining and implementing process improvement initiatives using data and metrics experience\nBachelors degree\nKnowledge of Excel (Pivot Tables, VLookUps) at an advanced level and SQL\nExperience defining program requirements and using data and metrics to determine improvements 3+ years of driving end to end delivery, and communicating results to senior leadership experience\n3+ years of driving process improvements experience\nExperience in stakeholder management, dealing with multiple stakeholders at varied levels of the organization\nExperience building processes, project management, and schedules",,,,"['Procurement', 'Computer vision', 'Financial analysis', 'Project management', 'Process improvement', 'Manager Program Management', 'Machine learning', 'Stakeholder management', 'SQL', 'Logistics']",2025-06-12 15:15:16
Senior Analyst - Direct Display,Merkle B2b,2 - 8 years,Not Disclosed,['Chennai'],"The purpose of this role is to assist with the planning, reviewing and optimisation of Display campaigns whilst supporting the team in reporting and managing client accounts.\nJob Description:\nKey responsibilities:\nFocuses on day-to-day execution\nProactively reviews and manages client data to ensure optimal performance on all campaigns\nTracks and reports on campaign results, gathers data analysis and participates in weekly calls\nGenerates campaign reports and is responsible for pacing, QA and trafficking\nDevelops and maintains accurate project plans for client status updates\nLocation:\nChennai\nBrand:\nParagon\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['QA', 'Data analysis', 'Senior Analyst', 'Management']",2025-06-12 15:15:18
Senior Analyst - Direct Display,Merkle Science,1 - 6 years,Not Disclosed,['Chennai'],"The purpose of this role is to assist with the planning, reviewing and optimisation of Display campaigns whilst supporting the team in reporting and managing client accounts.\nJob Description:\nKey responsibilities:\nFocuses on day-to-day execution\nProactively reviews and manages client data to ensure optimal performance on all campaigns\nTracks and reports on campaign results, gathers data analysis and participates in weekly calls\nGenerates campaign reports and is responsible for pacing, QA and trafficking\nDevelops and maintains accurate project plans for client status updates\nLocation:\nChennai\nBrand:\nParagon\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['QA', 'Data analysis', 'Senior Analyst', 'Management']",2025-06-12 15:15:20
IN-Associate-KYC/AML - Fincrime COE-Advisory,PwC Service Delivery Center,3 - 6 years,Not Disclosed,['Gurugram'],"Not Applicable\nSpecialism\nRisk\nManagement Level\nAssociate\n& Summary\n.\n\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nAt PwC , we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n& Summary We are seeking a highly skilled KYC Analyst experience to join our dynamic team in the Financial Crime Compliance. The ideal candidate will be responsible for conducting thorough due diligence on clients by gathering and analyzing client information to verify compliance with regulatory requirements.\nResponsibilities\n1. Conduct client due diligence (CDD) to gather information such as identity verification, source of funds, and beneficial ownership for different entity types like Banks, Trust, Funds, SPV etc. 2. Perform initial checks on client documents and data to ensure completeness and accuracy. 3. Support in conducting research using various databases and sources to verify client information. 4. Evaluate based on client risk levels which includes business activities, geographic location, and other relevant factors. 5. Conduct sanction screening and adverse media screening of customers using specialized tools and databases and analyze screening results to identify matches with sanctioned individuals, entities, or countries. 6. Maintain accurate documentation for all clients, including KYC profiles and ongoing monitoring records.\nMandatory skill sets 1. Strong understanding of financial regulations, including but not limited to Bank Secrecy Act (BSA), AntiMoney Laundering (AML), and Know Your Customer (KYC) requirements. 2. Experienced analyst with a in depthknowledge of financial products, services, and industry regulations. 3. Excellent analytical skills with the ability to interpret complex financial data and identify potential risks. 4. Detailoriented with strong organizational and time management abilities\nPreferred skill sets Strong understanding of financial regulations, including but not limited to Bank Secrecy Act (BSA), AntiMoney Laundering (AML), and Know Your Customer (KYC) requirements.\nYears of experience required 36 years of experience in KYC, AML compliance, or a related role within the banking industry.\nEducation Qualification Any Grad\nEducation\nDegrees/Field of Study required Bachelor Degree\nDegrees/Field of Study preferred\nRequired Skills\nKYC Compliance\nAccepting Feedback, Accepting Feedback, Accounting and Financial Reporting Standards, Active Listening, Artificial Intelligence (AI) Platform, Auditing, Auditing Methodologies, Business Process Improvement, Communication, Compliance Auditing, Corporate Governance, Data Analysis and Interpretation, Data Ingestion, Data Modeling, Data Quality, Data Security, Data Transformation, Data Visualization, Emotional Regulation, Empathy, Financial Accounting, Financial Audit, Financial Reporting, Financial Statement Analysis, Generally Accepted Accounting Principles (GAAP) {+ 19 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Risk Management & Compliance,"Employment Type: Full Time, Permanent","['Manager Internal Audit', 'Assurance', 'Financial statements', 'Due diligence', 'Data analysis', 'Financial reporting', 'GAAP', 'Financial statement analysis', 'Risk management', 'financial auditing']",2025-06-12 15:15:24
Power BI Developer,Kellogg Brown & Root (KBR),5 - 10 years,Not Disclosed,['Chennai'],"Title:\nPower BI Developer\nCollaborate with all levels of finance organization on reporting requirements for both internal and external customers.\nWork independently and in partnership with business owners to provide innovative interactive reporting solutions to address a wide range of business needs using Power BI, Power Query, VBA, Cognos and other reporting tools.\nTransform financial data into visualization charts using Power BI and other reporting tools.\nLeverage multiple databases to merge and compile information to calculate relevant financial and business performance metrics.\nMaximize automation of routine tasks and processes using advanced toolsets (Artificial Intelligence or AI , Optical Character Recognition or OCR , Robotic Process Automation or RPA or Bots ).\nAutomate translation and migration of data between different systems (Costpoint, Cobra, EPM, EDW, OnBase).\nEnsure data quality by identifying and correcting errors, inconsistencies, and missing data to improve accuracy.\nCreate documentation and work instructions for applications and processes, ensure compliance with KBR IT standards and controls.\nBasic Qualifications:\nBachelor s Degree or equivalent in Finance, Accounting, Business Information Technology, Business Analytics, Information Systems or a related field.\nProficiency in Power BI, Data Modeling, SQL, VBA, Power Query.\nExpert understanding of Power BI functionality (reporting, publishing, security, mobile app).\nFoundational understanding of financial reporting metrics (Revenue, Cost of Goods Sold, Indirect Rate Application, EBIT, Cashflow, DSO, DPO)\nWorking knowledge of project management core concepts (contract types, cost sets, schedule, budgets).\nExperience with data analysis techniques, data integration, data modeling and data visualization.\nFamiliarity with basic software testing and implementation concepts and methods.\nPreferred Qualifications:\nWorking knowledge of Costpoint, Cobra, Hyperion (EPM, FCCS), OnBase, EDW, MSD.\nCapability with alternate programming and reporting tools (DAX, Python or R, Appian, Cognos).\nProject management Professional (PMP) or EVMS certification.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['PMP', 'Data analysis', 'Publishing', 'Data modeling', 'Cognos', 'Hyperion', 'Data quality', 'Information technology', 'SQL', 'Python']",2025-06-12 15:15:26
"Product Designer, People Technology",Assarain Group,2 - 7 years,Not Disclosed,"['Pune', 'Chennai']","Product Designer, People Technology\n\nKONE Technology and Innovation (KTI) is where the magic happens at KONE. Its where we combine the physical world - escalators and elevators - with smart and connected digital systems. We are changing and improving the way billions of people move within buildings every day. We are on a mission to shape the future of the industry with new technologies and sustainable innovations.\n\nKONE IT is part of the KONE Technology and Innovation (KTI) unit with the mission to power KONE with sustainable information flow. KONE IT also supports KONE in its digital transformation journey by introducing digital cloud-based IT services, artificial intelligence (AI) and automation to support productivity, business growth and technological disruption.\nOur Corporate Functions IT team is seeking a Product Designer for People Technology (Workday).\nYou will be responsible for designing IT solutions for HR applications, focusing on integrations and automation.\nYou will drive the vision of People Technology architecture forward, using agile and DevOps methods. As the main contact for business stakeholders, you will assess development ideas and oversee technical design and delivery in your area.\nIn this role, you collaborate closely with People Tech Product Owner, Product Architect, Functional Leads and HR and IT Operations teams, and actively support HR related projects. Current People Technology scope is based on global platforms and tools such as Workday (with wide range of capabilities), connected with SAP ERP, AWS, Microsoft Power Platform, and local solutions (e. g. payrolls, time tracking). Your job is to ensure that the solution designs meet KONEs business needs and follow market best practices.\nMain stakeholders of this role are People & Communications Functional Leads, People Tech Product Owner and Product Architect, IT and People & Comms Service Owners, local payroll teams, and business representatives from People & Communications.\n\nPosition will be based in Pune/Chennai India.\n\nIn this role, you get to:\nOwn the IT solution design for People Technology product area\nWork in close collaboration with the stakeholders to influence, collect and manage business demands and requirements for your responsibility area\nParticipate in requirement-gathering workshops to facilitate & influence the strategic direction of our projects and lead business stakeholders through solution design\nTurn requirements into specification and manage development and testing to final solution together with other IT teams and our implementation partner\nSupport solution road mapping and backlog prioritization with Product Owner\nSupport global roll-out and key user competence development of the solutions in your responsibility area\nContribute in People Tech projects\n\nSkills and Experience we`re looking for:\n5+ years of experience in People Technology, especially with Workday\nEnd-to-end People Tech process and technical solution understanding with focus on automation and integrations with global business solutions and payrolls.\nDeep understanding of Workday (Core HCM, Compensation, integration tools, Extend, Prism), MS Power Platform knowledge is considered as a plus\nPrevious experience working in IT projects and delivering end-to-end e. g. integration or automation development project\nAnalytical and conceptual thinking, innovative and ability to think big\nExperience in working in a multicultural, global organization\nExcellent communication and documentation skills in English\nWe are looking for a collaborative and proactive team player who approaches situations with an open mind, adapts quickly in a changing and agile environment and can create structure, clarity and results in a transparent way. Strong communication and interpersonal skills in English are essential for building relationships with stakeholders and peers. You naturally embrace and promote cultural diversity and inclusion within your team and organization.\nWhat do you get in return?\nPossibility to have an impact on People Technology product environment\nOpportunity to make a difference on designing more automated, improved E2E solutions and IT processes\nInternational and professional environment\nAttractive package of benefits and bonus scheme\nSocial and sport events on regular basis\nGreat place to work\nInformal culture and friendly colleagues\nKONE trainings in various areas\nPromotion on performance/competence\nWe can offer strong support in your personal growth and global opportunities for future career development at KONE. We believe in inspiring, engaging and developing our people.",Industry Type: Engineering & Construction,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['IT services', 'Career development', 'Automation', 'Payroll', 'SAP ERP', 'Analytical', 'Agile', 'microsoft', 'Business solutions', 'IT operations']",2025-06-12 15:15:29
"Financial Analyst II - AR, FinOps",Amazon,5 - 10 years,Not Disclosed,['Hyderabad'],"Are you an experienced Program Manager interested in an opportunity to help drive Amazon s flywheel and develop your A to Z business understanding? Do you enjoy learning about different Amazon business types and new subsidiaries, and thinking creatively about brand new businesses that Amazon is inventing on behalf of customers? The Global Accounts Receivable (GAR) team is seeking a creative and passionate program manager to help achieve our vision to provide a world-class Order-to-Cash (O2C) onboarding experience to our global business partners in support of Amazon s journey to become earth s most customer-centric company. We love to offer our customers unique world-class experiences, and we invite you to help Amazon make history!\n\nThe Program Manager will have global oversight of the integration of new initiatives onto O2C platforms, driving effective people, processes, and technology to achieve organizational goals and deliver results. This individual will have ownership over new business integration programs while standardizing the global implementation processes and driving efficiency. This role will require engagement and alignment with global business teams, finance teams, operational teams, system developers and product managers. Responsibilities include supporting new business initiatives through designing transactional workflows in line with the business model, defining requirements and testing of the solutions to ensure delivery is as expected and delivering and improving the customer experience. Implementation of mechanisms to monitor and measure performance is essential.\n\nThe ability to thrive in a fast-paced, ambiguous and demanding work environment is critical to success in this role. The ideal candidate will be a self-starter with knowledge of program management, experience with accounts receivable operational processes, demonstrate faster learning and adoptability, demonstrate good relationship and strategic influencing skills, experienced in large scale change management across functions and geographies, and exhibit a relentless pursuit for improvement. This individual must have a proven record of delivering results through good program management skills, problem solving skills, financial process and system knowledge, and a passion for customer experience.\n\nCore Requirements:\n5+ years of Accounts Receivable experience, with at least 2 years in a leadership role( not mandate)\nBachelors degree in Finance, Accounting, Business Administration, or related field\nAdvanced Excel skills and experience with ERP systems\nData Analytics Requirements:\n3+ years experience with data analysis and reporting tools\nProficiency in SQL for data extraction and analysis\nExperience with visualization tools (e.g., Tableau, Power BI)\nDemonstrated ability to translate data insights into actionable recommendations\n\nProgram Management Skills:\n3+ years experience managing complex projects or programs\nTrack record of process improvement initiatives\nExperience leading cross-functional teams\nGood stakeholder management abilities\nTechnical Skills:\nExperience with AR automation tools and systems\nKnowledge of financial control frameworks\nProficiency in Microsoft Office Suite\nExperience with business intelligence platforms\n\nAdditional Desired Qualifications:\nMBA or relevant masters degree\nProfessional certifications (CPA, PMP, or similar)\nExperience with machine learning or predictive analytics\nKnowledge of Python or R for advanced data analysis\n\n\nOwnership and implementation of new businesses and subsidiaries onto AR platforms\nPartner with key counterparts across geographies to launch and support initiatives globally in a scalable manner\nDevelop a solid understanding of Amazon s Finance Operations systems and processes\nDefine and implement global standards for business integration program management\nDefine and describe various business scenarios that can be relevant to New Businesses and convert them into system and operational requirements.\nTranslate complex business requirements into functional designs\nOversee comprehensive testing of systems changes and development of standard operating procedures, process documentation and performance metrics\nManage process transitions/implementations across multiple functions and geographies\nMotivate and influence business, operational and technical teams to ensure that best practices are followed and implemented\nIdentify, assess, track and mitigate risks at multiple levels\nProactively monitor program performance to identify, address and prevent potential issues\nAddress barriers through problem solving, communication and active coordination with stakeholders\nDrive effective teamwork, communication collaboration and commitment across multiple disparate groups with competing priorities\nIdentify gaps and strive constantly for re-engineering of systems and processes\nAmazon is an Equal Opportunity-Affirmative Action Employer Minority / Female / Disability /\nVeteran / Gender Identity / Sexual Orientation\n5+ years of Accounts Receivable (AR) experience 4+ years of participating in continuous improvement projects in your team to scale and improve controllership with measurable results experience\nMBA, or CPA\nKnowledge of Tableau\nExperience working with large-scale data mining and reporting tools (examples: SQL, MS Access, Essbase, Cognos) and other financial systems (examples: Oracle, SAP, Lawson, JD Edwards)",,,,"['Data analysis', 'Change management', 'PMP', 'SAP', 'MS Access', 'Process improvement', 'Oracle', 'Data mining', 'Business intelligence', 'SQL']",2025-06-12 15:15:31
Media AdTech Specialist,Capgemini,5 - 10 years,Not Disclosed,['Kolkata'],"Provide ad operations and/or AdTech operations expertise\nExecute and help implement an AdTech compliance program\nStrong understanding of Programmatic Ad Eco system and Retail Media bidding advertisement.\nYouTube, Connected TV and Video Ads advertisement and hands on ad set up experience.\nDesign and implement advertising solutions tailored for retail media needs.\nBuild operational systems that enhances the productivity of our Ad Operations team\nCollaborate with other departments and stakeholders to identify and solve complex problems\nContinuously testing and improving software solutions to ensure optimal performance and user experience\nCreate and manage strong relationships with DSPs and other relevant players in the ad tech space that can help our clients achieve their objectives.\nSpearhead initiatives to refine and expand digital media services.\nCollaborate with a diverse team of experts to drive innovation. (data scientists, developers, engineers, clients and stakeholders).\nEnsure seamless integration and service delivery.\nApply the latest industry trends and best practices to achieve our clients outcomes\nStay abreast of media regulations and trends affecting digital advertising.\nBuild highly performant AdTech platforms that will support our future growth in the Ads Space\n\nQualifications:\n5 years experience in advertising operations (AdTech ops) and/or revenue operations (Revops).\nStrong understanding of DSPs, digital advertising ecosystems, ad networks, and/or advertising exchanges.\nDemonstrated excellence in client relationship management.\nDemonstrated ability to build and work across teams.\nExperience in Technical Solutions Architecture and design leadership.\nManage multiple projects and prioritize tasks effectively\nBroad knowledge across multiple technology areas Marketing Operation, Ecommerce Domain and Retail Media.\nStrong organizational skills and attention to detail.\nAbility to work independently and as part of a team.\n\nTools:\nFacebook Ads Manager, Pinterest Ad Manager, Instagram ads, DV360, Programmatic, Campaign Manager 360, Google Ad Manager, TTD\nPower-Bi, Excel, PowerPoint will be a plus point.\nYouTube, Videos, CTV related ad platforms.",Industry Type: IT Services & Consulting,Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['DSP', 'Programmatic Buying', 'DV360', 'Facebook Ads Manager', 'Bidding', 'Display Video', 'Google Ads', 'Media Buying', 'Atl', 'Media Planning', 'Pinterest', 'Campaign Management', 'Btl']",2025-06-12 15:15:34
IN-Senior Associate-KYC/AML - Fincrime COE-Advisory,PwC Service Delivery Center,3 - 6 years,Not Disclosed,['Gurugram'],"Not Applicable\nSpecialism\nRisk\nManagement Level\nSenior Associate\n& Summary\n.\n\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nAt PwC , we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n& Summary We are seeking a highly skilled KYC Analyst experience to join our dynamic team in the Financial Crime Compliance. The ideal candidate will be responsible for conducting thorough due diligence on clients by gathering and analyzing client information to verify compliance with regulatory requirements.\nResponsibilities\n1. Conduct client due diligence (CDD) to gather information such as identity verification, source of funds, and beneficial ownership for different entity types like Banks, Trust, Funds, SPV etc. 2. Perform initial checks on client documents and data to ensure completeness and accuracy. 3. Support in conducting research using various databases and sources to verify client information. 4. Evaluate based on client risk levels which includes business activities, geographic location, and other relevant factors. 5. Conduct sanction screening and adverse media screening of customers using specialized tools and databases and analyze screening results to identify matches with sanctioned individuals, entities, or countries. 6. Maintain accurate documentation for all clients, including KYC profiles and ongoing monitoring records.\nMandatory skill sets 1. Strong understanding of financial regulations, including but not limited to Bank Secrecy Act (BSA), AntiMoney Laundering (AML), and Know Your Customer (KYC) requirements. 2. Experienced analyst with a in depthknowledge of financial products, services, and industry regulations. 3. Excellent analytical skills with the ability to interpret complex financial data and identify potential risks. 4. Detailoriented with strong organizational and time management abilities\nPreferred skill sets Strong understanding of financial regulations, including but not limited to Bank Secrecy Act (BSA), AntiMoney Laundering (AML), and Know Your Customer (KYC) requirements.\nYears of experience required 36 years of experience in KYC, AML compliance, or a related role within the banking industry.\nEducation Qualification Any Grad\nEducation\nDegrees/Field of Study required\nDegrees/Field of Study preferred\nRequired Skills\nKYC Compliance\nAccepting Feedback, Accepting Feedback, Accounting and Financial Reporting Standards, Active Listening, Analytical Thinking, Artificial Intelligence (AI) Platform, Auditing, Auditing Methodologies, Business Process Improvement, Communication, Compliance Auditing, Corporate Governance, Creativity, Data Analysis and Interpretation, Data Ingestion, Data Modeling, Data Quality, Data Security, Data Transformation, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Financial Accounting, Financial Audit {+ 24 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['Manager Internal Audit', 'Assurance', 'Financial statements', 'Due diligence', 'Data analysis', 'Financial reporting', 'Corporate governance', 'Risk management', 'financial auditing', 'Monitoring']",2025-06-12 15:15:36
"Quality Engineer, QA",XL India Business Services Pvt. Ltd,2 - 6 years,Not Disclosed,['Gurugram'],"Quality Engineer Bangalore/ Gurgaon, India AXA XL offers risk transfer and risk management solutions to clients globally\n\nWe offer worldwide capacity, flexible underwriting solutions, a wide variety of client-focused loss prevention services and a team-based account management approach\n\nAXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained advantage\n\nOur Chief Data Office is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking an Engineer for the Quality Engineering team\n\nThe Engineer sits next to our Business Partners and tests our AXIOM platform according to our stakeholders needs\n\nWhat you ll be DOING What will your essential responsibilities include? Possess excellent domain knowledge of Data warehousing technologies, SQL, Data Models to develop test strategies, approaches from Quality Engineering perspective\n\nIn close coordination with Project teams help lead all efforts from Quality Engineering perspective\n\nWork with data engineers or data scientists to collect and prepare the necessary test data sets\n\nEnsure the data adequately represents real-world scenarios and covers a diverse range of inputs\n\nExcellent domain knowledge of Data warehousing technologies, SQL, Data Models to build out test strategies and lead projects from Quality Engineering perspective\n\nWith an Automation-first mindset, work towards testing of user interfaces such as Business Intelligence solutions and validation of functionalities while constantly looking out for efficiency gains and process improvements\n\nTriage and Prioritization of stories and epics with all stakeholders to ensure optimal deliveries\n\nEngage with various stakeholders like Business Partners, Product Owners, Development and Infrastructure teams to ensure alignments with overall roadmap\n\nTrack current progress of testing activities, finding and tracking test metrics, estimating and communicating improvement actions based on the test metrics results and the experience\n\nAutomation for processes such as Data Loads, user interfaces such as Business Intelligence solutions and other validations of business KPIs\n\nAdopt and implement best practices towards Documentation of test plan, cases, results in JIRA\n\nTriage and Prioritization of defects with all stakeholders\n\nLeadership accountability for ensuring that every release to customers is fit for purpose, performant\n\nKnowledge on Scaled Agile, Scrum or Kanban methodology\n\nYou will report to Lead UAT\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: A minimum of a bachelor s or masters degree (preferred) in a relevant discipline\n\nRelevant years of excellent testing background, including knowledge/experience in automation\n\nInsurance experience in data, underwriting, claims or operations, including influencing, collaborating, and leading efforts in complex, disparate, and interrelated teams\n\nExcellent Experience with SQL Server, Azure Databricks Notebook, PowerBI, ADLS, CosmosDB, SQL DW Analytics\n\nShould have a robust background in Software development with experience in ingesting, transforming, and storing data from large datasets using Pyspark in Azure Databricks with robust knowledge of distributed computing concepts\n\nHands-on experience in designing and developing ETL Pipelines in Pyspark in Azure Databricks with robust python scripting\n\nDesired Skills and Abilities: Having experience doing UAT/System Integration testing in the insurance industry\n\nExcellent technical testing experience such as API testing, UI automation is a plus\n\nKnowledge/Experience of Testing in cloud-based systems in different data staging layers",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System integration testing', 'Test planning', 'Account management', 'Business strategy', 'Business intelligence', 'Risk management', 'JIRA', 'Analytics', 'SQL', 'Python']",2025-06-12 15:15:38
Jr.AI Engineer,Tekone It Services,1 - 3 years,1.5-6.5 Lacs P.A.,['Hyderabad'],"Position Overview\nWe are hiring five AI Engineers with 12 years of experience to join our dynamic team in Hyderabad. The ideal candidates will have a solid foundation in Large Language Models (LLMs), LangChain, and Generative AI (GenAI) frameworks. This is a great opportunity to work on innovative AI solutions, contributing to projects that integrate LLMs, prompt engineering, RAG pipelines, and cloud-based deployments.\nKey Responsibilities\nContribute to the design and development of AI-powered applications utilizing LLMs (GPT-3.5, GPT-4, Gemini).\nAssist in building LangChain-based pipelines and workflows, including LangSmith and LangGraph.\nSupport the implementation of Retrieval-Augmented Generation (RAG) frameworks using vector databases such as ChromaDB.\nApply prompt engineering techniques to optimize model responses and improve contextual accuracy.\nDevelop RESTful APIs using Flask or FastAPI to enable model consumption in production environments.\nWrite and manage data workflows using SQL, PySpark, and Spark SQL.\nDeploy and monitor models on Azure Machine Learning or AWS Bedrock platforms.\nCollaborate with cross-functional teams, including data scientists, engineers, and business stakeholders.\nRequired Skills\nProficiency in Python, SQL, PySpark, and Spark SQL\nHands-on experience with LLMs: GPT-3.5, GPT-4, Gemini\nKnowledge of LangChain, LangSmith, LangGraph\nFamiliarity with Vector Databases (e.g., ChromaDB) and embeddings\nExperience with prompt engineering and RAG-based architectures\nExposure to cloud platforms such as Azure ML or AWS Bedrock\nStrong understanding of REST APIs and version control systems (Git/GitHub)\nPreferred Qualifications\nBachelor's degree in Computer Science, Artificial Intelligence, Data Science, or a related field\nInternship or academic project experience in NLP, LLMs, or GenAI technologies\nFamiliarity with MLOps tools and practices (e.g., CI/CD, Airflow)\nStrong problem-solving abilities, attention to detail, and a collaborative mindset",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'Prompt Engineering', 'Artificial Intelligence', 'llm']",2025-06-12 15:15:40
Python Developer @ Infosys- Pan India,Infosys,4 - 9 years,Not Disclosed,"['Pune', 'Delhi / NCR', 'Mumbai (All Areas)']","Responsibilities A day in the life of an Infoscion • As part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain. • You will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements. • You will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers. • You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nTechnical and Professional Requirements: • Primary skills: Process->Testing processes->Test Automation Process, Technology->Machine Learning->Python\n\nPreferred Skills: Process->Testing processes->Test Automation Process Technology->Machine Learning->Python\n\nAdditional Responsibilities: • Knowledge of design principles and fundamentals of architecture • Understanding of performance engineering • Knowledge of quality processes and estimation techniques • Basic understanding of project domain • Ability to translate functional / nonfunctional requirements to systems requirements • Ability to design and code complex programs • Ability to write test cases and scenarios based on the specifications • Good understanding of SDLC and agile methodologies • Awareness of latest technologies and trends • Logical thinking and problem solving skills along with an ability to collaborate\n\nEducational Requirements MCA,MSc,MTech,Bachelor of Engineering,BCA,BE,BSc,BTech",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Django', 'Python Development', 'Flask']",2025-06-12 15:15:43
Applied AI Engineer,Xenonstack,3 - 5 years,Not Disclosed,['Mohali'],"XenonStack's Artificial Intelligence team is looking for a Machine Learning (ML) Engineer to help us create artificial intelligence products. Machine Learning Engineer responsibilities include creating machine learning models and retraining systems. To do this job successfully, you need exceptional skills in statistics and programming. Your goal will be to shape and build efficient self-learning applications.\n\n\nKey Responsibilities:\nStudy and transform data science prototypes\nDesign machine learning systems\nResearch and implement appropriate ML algorithms and tools\nDevelop machine learning applications according to requirements\nSelect relevant datasets and data representation methods\nRun machine learning tests and experiments\nPerform statistical analysis and fine-tuning using test results\nTrain and retrain systems when necessary\nExtend existing ML libraries and frameworks\nKeep abreast of developments in the field\n\nRequirements\nTechnical Requirement:\n\nKnowledge in Machine Learning Engineer or similar role\nUnderstanding of data structures, data modelling, and software architecture\nDeep knowledge of maths, probability, statistics, and algorithms\nAbility to write robust code in Python, Java, and R\nFamiliarity with machine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn)\n\nProfessional Attributes:\nExcellent communication skills & Attention to detail.\nAnalytical mind and problem-solving Aptitude with Strong Organizational skills & Visual Thinking.\nEducation: Technical Graduates (BCA, BSC, B. Tech), MCA, MSC, and M.Tech with strong data structure and algorithm Skills \n\nBenefits:\nDynamic and purposeful work culture in a people-oriented organization contributing to multi-million-dollar projects with guaranteed job security.\nOpen, authentic, and transparent communication fostering a warm work environment.\nRegular constructive feedback and exposure to diverse technologies.\nRecognition and rewards for exceptional performance achievements.\nAccess to certification courses & Skill Sessions to develop continually and refine your skills.\nAdditional allowances for team members assigned to specific projects.\nSpecial skill allowances to acknowledge and compensate for unique expertise.\nComprehensive medical insurance policy for your health and well-being.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Programming', 'Machine Learning', 'Tensorflow', 'Algorithm Development', 'Cnn', 'Natural Language Processing', 'Neural Networks', 'Deep Learning', 'Cuda', 'Pytorch', 'Pattern Recognition', 'Rnn', 'Image Processing', 'Keras', 'Data Processing', 'Computer Vision', 'Python']",2025-06-12 15:15:45
Python Engineer,Forbes Global 2000 MNC in Investment Ban...,3 - 8 years,20-22.5 Lacs P.A.,"['Noida', 'Gurugram']","Python/Quant Engineer\nKey Responsibilities:\nDesign, develop, and maintain scalable Python-based quantitative tools and libraries.\nCollaborate with quants and researchers to implement and optimize pricing, risk, and trading models.\nProcess and analyze large datasets (market, fundamental, alternative data) to support research and live trading.\nBuild and enhance backtesting frameworks and data pipelines.\nIntegrate models with execution systems and trading platforms.\nOptimize code for performance and reliability in low-latency environments.\nParticipate in code reviews, testing, and documentation efforts.\nRequired Qualifications:\n3-8 years of professional experience in quantitative development or similar roles.\nProficiency in Python, including libraries like NumPy, Pandas, SciPy, Scikit-learn, and experience in object-oriented programming.\nStrong understanding of data structures, algorithms, and software engineering best practices.\nExperience working with large datasets, data ingestion, and real-time processing.\nExposure to financial instruments (equities, futures, options, FX, fixed income, etc.) and financial mathematics.\nFamiliarity with backtesting, simulation, and strategy evaluation tools.\nExperience with Git, Docker, CI/CD, and modern development workflows.\nPreferred Qualifications:\nPreferred Experience with C++ for performance-critical modules.\nKnowledge of machine learning techniques and tools (e.g., TensorFlow, XGBoost).\nFamiliarity with SQL / NoSQL databases and cloud platforms (AWS, GCP).\nPrior experience in hedge funds, proprietary trading firms, investment banks, or financial data providers.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['OOPS', 'Pandas', 'Numpy', 'Python', 'Data Structures And Algorithms', 'Scipy', 'GIT', 'Docker', 'Ci/Cd', 'Scikit-Learn']",2025-06-12 15:15:48
Chat Bot Developer,Capgemini,2 - 7 years,Not Disclosed,"['Hyderabad', 'Pune', 'Chennai']","Key Responsibilities:\nDesign and develop chatbot solutions using platforms like Dialogflow, Microsoft Bot Framework, Rasa, or similar.\nIntegrate chatbots with messaging platforms (e.g., WhatsApp, Facebook Messenger, Slack, web chat).\nImplement NLP and machine learning techniques to improve chatbot understanding and responses.\nCollaborate with UX/UI designers to create engaging conversational flows.\nConnect chatbots to backend systems, APIs, and databases.\nMonitor chatbot performance and continuously improve based on analytics and user feedback.\nEnsure security, scalability, and reliability of chatbot solutions.\nRequired Skills:\nProficiency in programming languages such as Python, JavaScript, or Node.js.\nExperience with chatbot development platforms (Dialogflow, Rasa, IBM Watson, etc.).\nUnderstanding of NLP concepts and tools (spaCy, NLTK, BERT, etc.).\nFamiliarity with RESTful APIs and webhook integration.\nKnowledge of cloud platforms (AWS, Azure, GCP) is a plus.\nStrong problem-solving and communication skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Bot', 'Chatbot Development', 'Chatbot']",2025-06-12 15:15:50
Software Development Engineer,Appen,4 - 9 years,Not Disclosed,['Hyderabad'],"About Appen\n\nAppen is a leader in AI enablement for critical tasks such as model improvement, supervision, and evaluation. To do this we leverage our global crowd of over one million skilled contractors, speaking over 180 languages and dialects, representing 130 countries. In addition, we utilize the industrys most advanced AI-assisted data annotation platform to collect and label various types of data like images, text, speech, audio, and video.\n\nOur data is crucial for building and continuously improving the worlds most innovative artificial intelligence systems and Appen is already trusted by the worlds largest technology companies. Now with the explosion of interest in generative AI, Appen is helping leaders in automotive, financial services, retail, healthcare, and governments the confidence to deploy world-class AI products.\n\nAt Appen, we are purpose driven. Our fundamental role in AI is to ensure all models are helpful, honest, and harmless, so we firmly believe in unlocking the power of AI to build a better world. We have a learn-it-all culture that values perspective, growth, and innovation. We are customer-obsessed, action-oriented, and celebrate winning together.\n\nAt Appen, we are committed to creating an inclusive and diverse workplace. We are an equal opportunity employer that does not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n\nWe are looking for a self-motivated Software Development Engineer to become part of our innovative team. In this role, you will be responsible for developing high-quality software that will play a key role in shaping the future of AI and machine learning.\nKey Responsibilities:\nCollaborate with our team to design, develop, and maintain software systems and applications.\nWrite clean, maintainable, and efficient code in languages such as Python, Java etc.\nImplement and test software components and ensure that they meet design specifications.\nParticipate in code and design reviews to maintain our high development standards.\nWork closely with other team members to troubleshoot, debug, and improve existing software systems.\nContribute to all phases of the software development lifecycle, from concept to deployment.\nEngage with cross-functional teams to understand and translate business requirements into software solutions.\nQualifications:\nBachelors Degree in Computer Science, Software Engineering, or a related field.\n4+ years of experience in software development.\nProficiency in one or more programming languages such as Java, Python etc.\nWork experience in developing microservices and building RESTful APIs.\nStrong understanding of algorithms and data structures.\nWork experience in designing relational database management systems (RDBMS) or NoSQL databases.\nFamiliarity with AI and machine learning concepts is a plus.\nExcellent problem-solving skills and attention to detail.\nStrong verbal and written communication skills.\nAbility to work effectively in a fast-paced, dynamic environment.\nAppen is the global leader in data for the AI Lifecycle with more than 25 years experience in data sourcing, annotation, and model evaluation. Through our expertise, platform, and global crowd, we enable organizations to launch the world s most innovative artificial intelligence products with speed and at scale. Appen maintains the industry s most advanced AI-assisted data annotation platform and boasts a global crowd of more than 1 million contributors worldwide, speaking more than 235 languages. Our products and services make Appen a trusted partner to leaders in technology, automotive, finance, retail, healthcare, and government. Appen has customers and offices globally.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'NoSQL', 'RDBMS', 'Artificial Intelligence', 'Machine learning', 'Data structures', 'Healthcare', 'Automotive', 'Financial services', 'Python']",2025-06-12 15:15:53
Ai Ml Engineer,Compunnel,4 - 9 years,Not Disclosed,"['Noida', 'Chandigarh']","Company: Compunnel INC\nJob Location: Noida/Chandigarh\nExperience Required: 4+ years\nMode of Work: 5 days work from the office\nJob Title: AI /ML Engineer\n\nWe are seeking a talented and innovative Generative AI Engineer to join our team. The ideal candidate will have expertise in training and testing large language models (LLMs) for applications like speech-to-text and text-to-speech, with a focus on the Hindi language. This role requires proficiency in cutting-edge AI technologies, including transformers, GPU acceleration, and CUDA, along with strong Python programming skills.\n\nKey Responsibilities:\nDesign, train, and fine-tune LLMs for speech-to-text and text-to-speech applications in Hindi.\nDevelop and optimize transformer-based architectures for natural language processing (NLP) tasks.\nLeverage GPU acceleration and CUDA for efficient model training and deployment.\nPre process and manage large datasets to ensure high-quality data for model training.\nCollaborate with cross-functional teams to integrate AI models into production systems.\nConduct rigorous testing and evaluation of models to ensure accuracy, efficiency, and scalability.\nStay updated with the latest advancements in generative AI and NLP technologies.\n\nRequired Skills and Qualifications:\nProficiency in Python and experience with deep learning frameworks like TensorFlow or PyTorch.\nStrong understanding of transformer architectures (e.g., BERT, GPT, T5).\nHands-on experience with GPU acceleration and CUDA programming.\nFamiliarity with Hindi language processing, including phonetics, grammar, and linguistic nuances.\nExperience in developing and deploying speech-to-text and text-to-speech systems.\nKnowledge of data preprocessing techniques for audio and text datasets.\nStrong problem-solving skills and ability to work in a collaborative environment.\n\nPreferred Qualifications:\nExperience with tools like Hugging Face Transformers, Kaldi, or Mozilla Deep Speech.\nFamiliarity with cloud platforms (e.g., AWS, Azure, or Google Cloud) for AI model deployment.\nUnderstanding of end-to-end speech recognition and synthesis pipelines.\nA background in linguistics or computational linguistics is a plus.\n\nPlease fill in all the essential details which are given below & attach your updated resume, and send it to ralish.sharma@compunnel.com\n1. Total Experience:\n2. Relevant Experience in Python :\n3. Experience in Pytorch :\n4. Experience in Tensorflow:\n5. Experience in LLM :\n6. Experience in RAG :\n7. Experience in NLP :\n8. Experience in GPT :\n9. Experience in Bert:\n10 . Experience in CUDA :\n11. Current company :\n12. Current Designation :\n13. Highest Education :\n14. Notice Period:\n15. Current CTC:\n16. Expected CTC:\n17. Current Location:\n18. Preferred Location:\n19. Hometown:\n20. Contact No:\n21. If you have any offer from some other company, please mention the Offer amount and Offer Location:\n22. Reason for looking for change:\n\nIf the job description is suitable for you, please get in touch with me at the number below: 9910044363.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pytorch', 'Tensorflow', 'Bert', 'LLM', 'Python', 'NLP', 'RAG', 'Cuda']",2025-06-12 15:15:55
Devops Engineer,Attentive Ai,2 - 3 years,Not Disclosed,['Noida'],"About Us - Attentive.ai is a fast-growing vertical SaaS start-up, funded by Peak XV (Surge), InfoEdge, Vertex Ventures, and Tenacity Ventures that provides innovative software solutions for the landscape, paving & construction industries in the United States. Our mission is to help businesses in this space improve their operations and grow their revenue through our simple & easy-to-use software platforms.\n\nPosition Description: We are looking for a DevOps Engineer to join our engineering team and help us develop and expand various our internal pipelines and infrastructure As a DevOps Engineer at Attentive, you will be working closely with different engineering, computer vision, testing, and product teams to improve and expand their workflows and cloud resources. We offer an inspiring environment full of young people with a lot of ambition. You get the freedom to implement your own designs, solutions, and creativity\n\nRoles & Responsibilities:\nKnowledge of building and setting up new development tools and infrastructure\nSetup uptime checks, resource health monitoring, and other monitoring tools (Gcp stack-driver, ELK)\nManaging and scaling cloud-based infrastructure\nTroubleshooting and resolving infrastructure issues\nDevelop and integrate solutions for the automation of SDL processes such as automated code checks, tests, deployments, rollbacks, etc\nAutomating the build, test, and release process\nCreating and maintaining documentation for infrastructure and processes\nFollows the established processes and best practices to ensure code quality and security.\n\nRequirements\n2-3 years of work experience as a Cloud & DevOps engineer\nExcellent understanding of Python, Groovy , and bash scripting\nExperience working on Linux-based infrastructure\nExperience working on cloud services like GCP,AWS\nHands-on experience with CICD Tools\nExperienced in deploying a containerized application, static websites deployment, etc\nWorking knowledge of deploying and maintaining tools like Github, JIRA, Jenkins\nExperience in IaC tools Terraform, Ansible\n\nGood To Haves:\nExperience working with serverless application deployments.\nExperience working with source code scanning and dependency management.\nFamiliarity with data management and ML Ops (Machine Learning) process and tools.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer vision', 'Automation', 'github', 'Linux', 'Data management', 'GCP', 'Machine learning', 'Troubleshooting', 'JIRA', 'Python']",2025-06-12 15:15:58
Responsible AI Engineer,Overture Rede,3 - 7 years,Not Disclosed,['Pune'],"Job Role (20 Words)\nEnsure ethical, responsible AI system development by auditing compliance, designing mitigation strategies, and guiding cross-functional teams.\n\nJob Summary\nWe are seeking a seasoned Responsible AI Engineer to lead ethical AI development initiatives. This role focuses on auditing AI systems, advising teams, and embedding Responsible AI frameworks into enterprise solutions, ensuring alignment with regulatory and ethical standards.\n\nRequired Skills\nDeep expertise in Responsible AI and ethical governance\nStrong advisory and decision-making capabilities\nProficiency in Responsible AI frameworks and implementation\nExperience in statistical analysis and ML algorithms\nKnowledge of linear/logistic regression, decision trees, clustering\nSkilled in data preprocessing (cleaning, transformation, normalization)\nFamiliarity with visualization tools like Tableau and Power BI\nAbility to audit AI systems and enforce compliance protocols",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Logistic regression', 'tableau', 'Statistical analysis', 'Compliance', 'power bi', 'System development', 'Advisory', 'Auditing']",2025-06-12 15:16:01
AI/ML Engineer,Oak Tree Cloud Software,3 - 5 years,Not Disclosed,['Indore'],"Job Title: Python Developer AI/ML & Generative AI\nExperience: 3+ Years\nLocation: Indore (WFO)\nEmployment Type: Full-Time\nIndustry: AI / Technology / Software Development\n\nJob Description\nWe are seeking a skilled Python Developer with 3+ years of hands-on experience in Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL) and Generative AI (GenAI). The ideal candidate will be passionate about building intelligent systems and creating real-world applications using modern AI tools and frameworks.\n\nKey Responsibilities:\nDevelop and deploy AI/ML models using Python for real-world use cases including classification, regression, clustering, NLP, and computer vision.\nDesign and fine-tune Generative AI models, including transformers, large language models (LLMs), and text-to-image/audio tools.\nImplement data preprocessing pipelines, feature engineering, and model evaluation metrics.\nCollaborate with cross-functional teams (data scientists, backend developers, and product managers) to integrate models into applications or APIs.\nOptimize and scale ML/AI pipelines for performance and accuracy using techniques like model compression or distributed training.\nWork with GenAI frameworks/tools such as Hugging Face Transformers, LangChain, OpenAI API, or LLaMA.\nPerform research and experimentation with state-of-the-art models and suggest improvements for production use.\nMaintain clear documentation for models, datasets, experiments, and deployment procedures.\nRequired Skills & Qualifications:\nStrong proficiency in Python with focus on data structures, OOPs, and libraries like NumPy, Pandas, Scikit-learn, and Matplotlib.\nExperience with AI/ML frameworks such as TensorFlow, PyTorch, or Keras.\nPractical knowledge of Gen AI tools, including Hugging Face Transformers, OpenAI GPT models, or LLM fine-tuning.\nUnderstanding of ML lifecycle, from data cleaning to model deployment and monitoring.\nDevelop and implement data extraction pipelines for unstructured documents using OCR techniques and libraries (e.g., Tesseract, EasyOCR) to extract text and structured data from PDFs (PyMuPDF), scanned images, and DOCX files.\nExperience with NLP techniques, text generation, summarization, or vector embeddings.\nHands-on experience with REST APIs, FastAPI or Flask to deploy and serve models.\nFamiliarity with version control (Git), CI/CD pipelines, and containerization (Docker).\nBachelor's/Master’s in Computer Science, Artificial Intelligence, Data Science, or related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'Machine Learning', 'Deep Learning', 'Python']",2025-06-12 15:16:03
Manager - Software Development,Amway,8 - 12 years,Not Disclosed,['Hyderabad'],"Primary Responsibilities\nCloud Expertise: Familiarity or hands-on experience with AWS and Google Cloud Platform (GCP) technologies to support data transformation, data structures, metadata management, dependency tracking, and workload orchestration.\nCollaboration & Independence: Self-motivated and capable of supporting the data needs of multiple teams, systems, and products within Amways data ecosystem.\nBig Data & Distributed Systems: Strong understanding of distributed systems for large-scale data processing and analytics, with a proven track record of manipulating, processing, and deriving insights from large, complex, and disconnected datasets.",,,,"['Data Transformation', 'GCP', 'Cloud', 'AWS']",2025-06-12 15:16:06
Senior Analyst Programmer- Platform Engineering,Fidelity International,5 - 7 years,Not Disclosed,['Gurugram'],"Title Senior Analyst Programmer- Platform Engineering\nDepartment FIL India Technology - ISS Tech\nLocation Gurgaon, India\nLevel 3\nWe re proud to have been helping our clients build better financial futures for over 50 years. How have we achieved this? By working together - and supporting each other - all over the world. So, join our ISS team and feel like you re part of something bigger.\nAbout your team\nInvestment Management Technology provides systems development, implementation and support services for our global\nInvestment Management division. We support Fund Managers, Research Analysts and Traders in all of our international\nlocations, including London, Hong Kong, Ireland & Tokyo.\nAbout your role\nCRD delivery team needs highly motivated self-driven Analyst Programmer to provide Platform Support. The CRD platform consists of the Charles River product, CRD Integration Layer, PaaS and Kubernetes Services. CRD Platform is Fidelity s core trading platform, used by Portfolio Managers, Traders, Compliance and Post Trade.\nThe core elements of the role are as follows:\nPlatform Engineering - Primary objective of platform engineering is to focus on future planning and design of platform to maintain long term sustainability and supportability.\nNon-Production Incident management - Troubleshoot non-production issues and find root cause through analysis.\nNon-Production Support & Operations - Perform routine operational tasks such as critical batch monitoring, morning checks on application s readiness for business use, health check reports, maintenance etc\nProblem management & Change management - Identify and drive the changes required to bring stability on non-prod environments; Participate in Application releases, Infrastructure changes, Preventive maintenance activities like DR role swaps.\nAbout you\nSeasoned IT software delivery professional with an experience of 5+ years of relevant industry experience in supporting IT applications.\nHands on experience on Unix scripting, Oracle & SQLServer, scheduling tools - Autosys and Control-M, IBM MQ, Kubernetes and Python.\nUnderstanding of DevOps concepts, Jenkins, Urban Deploy, JIRA and Power BI.\nKnowlege of Financial Domain (Investment Banking / Wealth Management) and understanding of Fixed Income and Equity Trading, Trade flow and Fund Management and FIX connectivity and infrastructure.\nFeel rewarded",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Change management', 'Production support', 'Fixed income', 'Problem management', 'Incident management', 'Investment banking', 'Oracle', 'JIRA', 'Monitoring', 'Python']",2025-06-12 15:16:08
Responsible AI Engineer,Overture Rede,15 - 20 years,Not Disclosed,['Pune'],": 15 Years Full-Time\n\nJob Role\nEnsure ethical, responsible AI system development by auditing compliance, designing mitigation strategies, and guiding cross-functional teams.\n\nJob Summary\nWe are seeking a seasoned Responsible AI Engineer to lead ethical AI development initiatives. This role focuses on auditing AI systems, advising teams, and embedding Responsible AI frameworks into enterprise solutions, ensuring alignment with regulatory and ethical standards.\n\nRequired Skills\nDeep expertise in Responsible AI and ethical governance\nStrong advisory and decision-making capabilities\nProficiency in Responsible AI frameworks and implementation\nExperience in statistical analysis and ML algorithms\nKnowledge of linear/logistic regression, decision trees, clustering\nSkilled in data preprocessing (cleaning, transformation, normalization)\nFamiliarity with visualization tools like Tableau and Power BI\nAbility to audit AI systems and enforce compliance protocols",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Logistic regression', 'tableau', 'Statistical analysis', 'Compliance', 'power bi', 'System development', 'Advisory', 'Auditing']",2025-06-12 15:16:10
Ai Ml Engineer,Openeyes Software Solutions,8 - 10 years,Not Disclosed,['Vadodara'],"Location : Vadodara (Onsite only)\n\nJob Summary:\nWe are looking for a highly skilled and experienced AI/ML Engineer with 8 to 10 years of relevant experience in developing end-to-end machine learning solutions. The ideal candidate will be well-versed in deep learning, natural language processing (NLP), computer vision, and large language models (LLMs). You will be responsible for designing, developing, and deploying scalable AI models tailored to real-world business use cases.\n\nKey Responsibilities:\n\nDesign, train, evaluate, and deploy machine learning and deep learning models aligned with business goals.\nWork with various ML tasks such as classification, regression, and clustering.\nHandle NLP, LLMs, computer vision, and speech-to-text use cases.\nImplement and fine-tune models like LLaMA, Falcon, BERT, T5 Transformer, and Hugging Face models.\nUse libraries such as NumPy, Pandas, Matplotlib, SpaCy, Scikit-learn, TensorFlow, and PyTorch.\nWrite clean, efficient Python code for data processing and model development.\nUtilize tools like Google Colab, Jupyter Notebook, and AWS SageMaker for experimentation and deployment.\nMonitor data drift and automate model retraining pipelines as required.\nDeploy AI models on cloud infrastructure (preferably AWS) using services like SageMaker, EC2, ECS, and Kubernetes.\nCollaborate with cross-functional teams to translate complex problems into AI solutions.\nGood to Have:\n\nExperience working with LangChain or LLaMAIndex.\nFamiliarity with RAG-based (Retrieval-Augmented Generation) application development.\nHands-on experience with Google Cloud Platform (GCP) for model deployment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Natural Language Processing', 'Machine Learning', 'Deep Learning', 'Python', 'Communication Skills', 'Artificial Intelligence', 'Pandas', 'Problem Solving', 'Aws Sagemaker', 'Numpy']",2025-06-12 15:16:13
Automation Engineer,Pentair,3 - 8 years,Not Disclosed,['Noida'],"Position Title: Automation Engineer Connected Factory\n\nPosition Summary:\nPentair is currently seeking an Automation Engineer to work for Connected Factory\nSolution with Public/Private Cloud. This individual must be well-versed in the latest\ntechnologies in IoT 4.0 and digital space. This role is responsible for end-to-end design\nprovisioning and commissioning.\n\nJob Summary:\nThe Automation Engineer will play a critical role in designing, developing, and maintaining\nautomation systems, controls, and instrumentation for various industrial processes. This\nposition requires expertise in electrical engineering, automation technologies, a deep\nunderstanding of MES systems, ERP, SCADA softwares and control systems like PLC, HMI,\nVFD, CNC, IM,VMC etc.\n\nKey Responsibilities:\nAutomation System Design: Design, develop, and implement electrical automation\nsystems and controls for industrial processes, machinery, and manufacturing\nequipment.\nPLC Programming: Develop and maintain PLC (Programmable Logic Controller)\nprograms to automate and control processes efficiently. Troubleshoot PLC-related\nissues.\nHMI (Human-Machine Interface) Development: Design and create user-friendly\nHMI interfaces for operators to monitor and control automation systems.\nSensor Integration: Select, install, and calibrate various sensors, transducers, and\ninstruments to gather data and provide feedback for control systems.\nElectrical Panel Design: Design and oversee the construction of electrical control\npanels, ensuring they comply with safety and regulatory standards.\nNetworking and Communication: Establish communication protocols between\ndifferent devices, controllers, and systems, including Routing, NAT, Ethernet IP,\nModbus TCP, OPC, RS-232, RS-485 and Profinet.\nIT/OT Connectivity: Ignition Edge gateway, Ignition Cloud Edition, KepServer and\nEdge devices.\nTesting and Commissioning: Conduct testing and commissioning of automation\nsystems(FAT & SAT), ensuring they operate efficiently and meet performance\nspecifications.\nDocumentation: Create and maintain detailed documentation, including electrical\nschematics, wiring diagrams, and system manuals.\nTroubleshooting: Diagnose and resolve electrical and automation-related issues,\nboth in the design phase and during system operation.\nMaintenance and Upgrades: Perform routine maintenance and recommend system\nupgrades to improve reliability and efficiency.\nEnterprise : Experience on MES and ERP (Sepasoft,SAP,etc.) systems.\nSafety and Compliance: Ensure that all automation systems adhere to safety\nregulations and industry standards.\nCollaboration: Collaborate with cross-functional teams, including mechanical\nengineers, software developers, and project managers, to integrate automation\nsolutions into larger projects.\n\nQualifications:\nDiploma/Bachelor's degree in Electrical Engineering, Automation, or a related field.\n3-5 years of experience in electrical automation system design and\nimplementation.\nProfessional certifications related to automation (e.g., ISA Certified Automation\nProfessional).\n\nExperience:\nMust be have experience in Ignition and Sepasoft SCADA.\nExperience with PLC Programming (e.g., Siemens, Allen-Bradley, ABB,RS-Logic 500,\nRS-Logic 5000, TIA Portal, Automation Builder)\nExperience with SCADA software (Supervisory Control and Data Acquisition)\nsystems (Ignition, WinCC, FactoryTalkView, Proficy historian)\nExperience with Injection moulding, motion control systems and robotics\n(Roboshot Link-i).\nExperience in HMI programming tools and software (e.g., Siemens, Allen-Bradley,\nABB).\nExperience in SCADA programming (e.g.,Ignition, WinCC, RSView, Proficy historian)\nExperience in Machine Connectivity(Fanuc , Siemens, ABB, Robots, CNC and IM\nmachines).\nStrong understanding of ISA-95 model, electrical and control system design\nprinciples.\nExcellent problem-solving and troubleshooting skills.\nEffective communication and teamwork abilities.\nKnowledge of regulatory & safety standards and regulations (e.g., UL, CE, NFPA\n70E, UL-508).\nExperience with industrial networking and communication protocols with IT/OT\nconnectivity.\nExperience with electrical switchgear and control instruments.\nExposure on Electrical Panel Design: Design and develop electrical panel\ncomponents and select the right switchgears, power supplies, and control systems,\nadhering to industry standards and best practices.\nKnowledge of advanced control algorithms and machine learning for automation.\nKnowledge of Project management skills.\n\nSkills and Abilities Required:\nCan-do positive attitude, always looking to accelerate development.\nDriven; commit to high standards of performance and demonstrate personal\nownership for getting the job done.\nInnovative and entrepreneurial attitude; stays up to speed on all the latest\ntechnologies and industry trends; healthy curiosity to evaluate, understand and\nutilize new technologies.\nAbility to learn and adapt innovative solutions.\nMust be ready to work on multiple timezone.\nMust be able to contribute to the technology team while managing multiple tasks\nand responsibilities.\nExcellent communication and presentation skills for interactions with technology\nglobal team members, SBU stakeholders, company leadership, vendors and\ncustomers.",Industry Type: Emerging Technologies (IoT),"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['sepasoft', 'PLC', 'Ignition Scada', 'IOT', 'HMI', 'Automation Engineering']",2025-06-12 15:16:50
AI Infrastructure Engineer (DevOps/MLOps),TechVantage,5 - 10 years,Not Disclosed,['Thiruvananthapuram'],"is a next-generation technology and product engineering company at the forefront of innovation in Generative AI, Agentic AI , and autonomous intelligent systems . We build intelligent, cutting-edge solutions designed to scale and evolve with the future of artificial intelligence.\nRole Overview:\nWe are looking for a skilled and versatile AI Infrastructure Engineer (DevOps/MLOps) to build and manage the cloud infrastructure, deployment pipelines, and machine learning operations behind our AI-powered products. You will work at the intersection of software engineering, ML, and cloud architecture to ensure that our models and systems are scalable, reliable, and production-ready.\n\nWhat we are looking from an ideal candidate?\nDesign and manage CI/CD pipelines for both software applications and machine learning workflows.\nDeploy and monitor ML models in production using tools like MLflow, SageMaker, Vertex AI, or similar.\nAutomate the provisioning and configuration of infrastructure using IaC tools (Terraform, Pulumi, etc.).\nBuild robust monitoring, logging, and alerting systems for AI applications.\nManage containerized services with Docker and orchestration platforms like Kubernetes .\nCollaborate with data scientists and ML engineers to streamline model experimentation, versioning, and deployment.\nOptimize compute resources and storage costs across cloud environments (AWS, GCP, or Azure).\nEnsure system reliability, scalability, and security across all environments.\n\nPreferred Skills:\nWhat skills do you need?\n5+ years of experience in DevOps, MLOps , or infrastructure engineering roles.\nHands-on experience with cloud platforms ( AWS, GCP, or Azure ) and services related to ML workloads.\nStrong knowledge of CI/CD tools (e.g., GitHub Actions, Jenkins, GitLab CI).\nProficiency in Docker , Kubernetes , and infrastructure-as-code frameworks.\nExperience with ML pipelines , model versioning, and ML monitoring tools.\nScripting skills in Python , Bash , or similar for automation tasks.\nFamiliarity with monitoring/logging tools (Prometheus, Grafana, ELK, CloudWatch, etc.).\nUnderstanding of ML lifecycle management and reproducibility.\nPreferred Qualifications:\nExperience with Kubeflow , MLflow , DVC , or Triton Inference Server .\nExposure to data versioning , feature stores , and model registries .\nCertification in AWS/GCP DevOps or Machine Learning Engineering is a plus.\nBackground in software engineering, data engineering, or ML research is a bonus.\nWhat We Offer:\nWork on cutting-edge AI platforms and infrastructure\nCross-functional collaboration with top ML, research, and product teams\nCompetitive compensation package no constraints for the right candidate",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Product engineering', 'orchestration', 'GCP', 'devops', 'Artificial Intelligence', 'Machine learning', 'Infrastructure', 'AWS', 'Python']",2025-06-12 15:16:53
Senior High Performance Computing Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will.\nRole Description:\nThe role is responsible for the design, integration, and management of high performance computing (HPC) systems that encompass both hardware and software components into the organizations network infrastructure. This individual will be responsible for all activities related to handling and supporting the Business and platforms including system administration, as well as incorporating new technologies under the challenge of a sophisticated and constantly evolving technology landscape. This role involves ensuring that all parts of a system work together seamlessly to meet the organizations requirements.\nRoles & Responsibilities:\nImplement, and manage cloud-based infrastructure that supports HPC environments that support data science (e.g. AI/ML workflows, Image Analysis).\nCollaborate with data scientists and ML engineers to deploy scalable machine learning models into production.\nEnsure the security, scalability, and reliability of HPC systems in the cloud.\nOptimize cloud resources for cost-effective and efficient use.\nKeep abreast of the latest in cloud services and industry standard processes.\nProvide technical leadership and guidance in cloud and HPC systems management.\nDevelop and maintain CI/CD pipelines for deploying resources to multi-cloud environments.\nMonitor and fix cluster operations/applications and cloud environments.\nDocument system design and operational procedures.\nBasic Qualifications:\nMasters degree with a 4 - 6 years of experience in Computer Science, IT or related field with hands-on HPC administration OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT or related field with hands-on HPC administration OR\nDiploma with 10-12 years of experience in Computer Science, IT or related field with hands-on HPC administration\nDemonstrable experience in cloud computing (preferably AWS) and cloud architecture.\nExperience with containerization technologies (Singularity, Docker) and cloud-based HPC solutions.\nExperience with infrastructure-as-code (IaC) tools such as Terraform, CloudFormation, Packer, Ansible and Git.\nExpert with scripting (Python or Bash) and Linux/Unix system administration (preferably Red Hat or Ubuntu).\nProficiency with job scheduling and resource management tools (SLURM, PBS, LSF, etc.).\nKnowledge of storage architectures and distributed file systems (Lustre, GPFS, Ceph).\nUnderstanding of networking architecture and security best practices.\nPreferred Qualifications:\nExperience supporting research in healthcare life sciences.\nExperience with Kubernetes (EKS) and service mesh architectures.\nKnowledge of AWS Lambda and event-driven architectures.\nExposure to multi-cloud environments (Azure, GCP).\nFamiliarity with machine learning frameworks (TensorFlow, PyTorch) and data pipelines.\nCertifications in cloud architecture (AWS Certified Solutions Architect, Google Cloud Professional Cloud Architect, etc.).\nExperience in an Agile development environment.\nPrior work with distributed computing and big data technologies (Hadoop, Spark).\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nRed Hat Certified Engineer (RHCE) or Linux Professional Institute Certification (LPIC)\nAWS Certified Solutions Architect Associate or Professional\nSoft Skills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced, cloud-first environment.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud computing', 'resource management', 'Ubuntu', 'Unix system administration', 'linux', 'unix production support', 'Python']",2025-06-12 15:16:55
Senior High Performance Computing Engineer,Amgen Inc,6 - 8 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will be responsible for deploying, maintaining and supporting HPC infrastructure in a multi-cloud environment. Hands-on engineering which requires\n\ndeep technical expertise in HPC technology and standard methodologies.\nImplement, and manage cloud-based infrastructure that supports HPC environments that support data science (e.g. AI/ML workflows, Image Analysis).\nCollaborate with data scientists and ML engineers to deploy scalable machine learning models into production.\nEnsure the security, scalability, and reliability of HPC systems in the cloud.\nOptimize cloud resources for cost-effective and efficient use.\nStay ahead of with the latest in cloud services and industry standard processes.\nProvide technical leadership and guidance in cloud and HPC systems management.\nDevelop and maintain CI/CD pipelines for deploying resources to multi-cloud environments.\nMonitor and fix cluster operations/applications and cloud environments.\nDocument system design and operational procedures.\n\n\n\nMust-Have\n\nSkills:\nExpert with Linux/Unix system administration (RHEL, CentOS, Ubuntu, etc.).\nProficiency with job scheduling and resource management tools (SLURM, PBS, LSF, etc.).\nGood understanding of parallel computing, MPI, OpenMP, and GPU acceleration (CUDA, ROCm).\nKnowledge of storage architectures and distributed file systems (Lustre, GPFS, Ceph).\nExperience with containerization technologies (Singularity, Docker) and cloud-based HPC solutions.\nExpert in scripting languages (Python, Bash) and containerization technologies (Docker, Kubernetes).\nFamiliarity with automation tools (Ansible, Puppet, Chef) for system provisioning and maintenance.\nUnderstanding of networking protocols, high-speed interconnects, and security best practices.\nDemonstrable experience in cloud computing (AWS, Azure, GCP) and cloud architecture.\nExperience with infrastructure as code (IaC) tools like Terraform or CloudFormation and Git.\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. Expert knowledge in\n\nlarge Linux environments, networking, storage, and cloud related technologies. Also, the candidate will have\n\nexpertise in root-cause analysis and fix while working with a team and stakeholders.\n\nTop-level communication and documentation skills are required.\n\nExpertise in coding in\n\nPython, Bash, YAML is expected.\n\n\n\nGood-to-Have\n\nSkills:\nExperience with Kubernetes (EKS) and service mesh architectures.\nKnowledge of AWS Lambda and event-driven architectures.\nFamiliarity with AWS CDK, Ansible, or Packer for cloud automation.\nExposure to multi-cloud environments (Azure, GCP).\nBasic Qualifications:\nBachelors degree in computer science, IT, or related field with 6-8 years of hands-on HPC administration or a related field.\n\n\n\nProfessional Certifications (preferred):\nRed Hat Certified Engineer (RHCE) or Linux Professional Institute Certification (LPIC)\nAWS Certified Solutions Architect Associate or Professional\nPreferred Qualifications:\n\n\n\nSoft\n\nSkills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced, cloud-first environment.\nShift Information: This position is required to be onsite and participate in 24/5 and weekend on call in rotation fashion and may require you to work a later shift. Candidates must be willing and able to work off hours, as required based on business requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance Computing', 'python', 'cloud architecture', 'linux', 'bash', 'networking', 'linux internals', 'cloud computing', 'scripting languages']",2025-06-12 15:16:58
