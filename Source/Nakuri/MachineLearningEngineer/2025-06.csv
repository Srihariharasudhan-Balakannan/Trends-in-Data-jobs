title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Machine Learning Engineer,Draft N Craft Legal Outsourcing,2 - 3 years,10-15 Lacs P.A.,['New Delhi'],"Role Overview:\n\nAs an ML Engineer you will embark on a wonderful journey of developing and implementing various Machine Learning models that will ultimately act as an enabler in the companys growth.\nSince Draft Craft is a legal services-oriented company in which we serve clients from across the border, your work will be majorly aimed towards creating ML workflows that will serve the legal industry and help improve efficiency of the in-house teams. Draft n Craft offers you the perfect opportunity to grow and hone your ML/Data Engineering skills while contributing towards building a worthwhile product.\n\nKey Responsibilities:\nDeveloping data ingestion & data preprocessing pipelines for transforming data presented for legal requirements to extract key and actionable insights. \nData cleaning for supplying accurate, consistent & relevant content to ML models.\nExploring and experimenting with different ML models and architectures that can be used with data from the legal industry in a safe and compliant manner.\nDeveloping and deploying ML models that can function in production environments for the use-cases required by the company.\nAnalyzing key metrics for model performance and devising methods to improve efficiencies of the models.\nDocument the steps involved in data preprocessing, model development, and optimizations undertaken.\nExplore techniques for feature extraction, transformation, and selection to improve model performance.\nUnderstanding software development related terminologies to collaborate with the existing team of software engineers within the company.\nDevise methods of integration of the ML models within the company’s already developed software solutions and employee workflows.\n\nRequired Qualifications:\nDegree holder from Computer Science Engineering, Data Science or related fields.\nMinimum experience of 2 years working as an ML engineer or Data Scientist in a professional capacity.\nStrong programming skills including python and familiarity with related libraries Tensorflow, PyTorch, Pandas etc.\nDatabase Querying in terms of SQL/NoSQL.\nWorking with data extracting and ETL pipelines for pre-processing of data/documents.\nRelevant experience working in NLP, Neural Networks, and Gen AI technologies.\nFamiliarity with working or applying transfer-learning on LLM models like Llama, etc.\nSome experience in software development is preferred.\nKnowledge of deploying ML models and data pipelines to cloud services like AWS/Azure.",Industry Type: Legal,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'Retrieval Augmented Generation', 'Python', 'NoSQL', 'Large Language Model', 'Data Extraction', 'Machine Learning', 'SQL']",2025-06-12 14:56:22
Machine Learning Engineer,CompIndia,0 - 1 years,Not Disclosed,"['Tirupati', 'Chennai( Aminjikarai )']",Value-Added Skills:\nCompleted Courses in Python & ML\nCompleted Academic projects involving ML\nParticipated in Kaggle competitions and Hackathons\nBuilt ML projects with real-world datasets,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Python', 'Kaggle', 'Hackathon']",2025-06-12 14:56:25
Machine Learning Engineer,goML,3 - 8 years,Not Disclosed,[],"Looking for a culture to thrive & build a rewarding career for yourself, join the core team of young hustlers building the next generation of Machine Learning platform & services. You will develop training and deployment pipelines for machine learning, implement model compression algorithms, and productionize machine learning research solving challenging business problems.\n\nWe are Hiring Machine Learning Engineers at goML!\n\nKey Responsibilities:\nDesign and develop generative AI models using techniques like RAG, transformers, and other relevant approaches.\nFine-tune pre-trained LLMs for specific tasks and domains.\nConduct research on new techniques for improving the performance and capabilities of generative AI models.\nApply software engineering rigor and best practices to machine learning /Generative AI pipelines.\nEvaluate and analyse the performance of ML/generative AI models.\nStay up to date on the latest advancements in generative AI research.\nFacilitate the development and deployment of proof-of-concept Generative AI systems.\n\nQualifications:\nBachelors/Masters degree in Computer Science, Machine Learning, Artificial Intelligence, or a related field.\n3-8 years of experience in generative AI or related fields.\nExperience in building data pipelines, deploying ML/GenAI models in production, and monitoring and maintaining their performance.\nStrong programming skills in Python.\nFamiliarity with RAG and other techniques for building generative models.\nExtensive experience with Git, Docker and a good understanding of Linux for managing servers.\nExperience with cloud-based ecosystems, especially AWS ML/GenAI services.\nExposure to ML/GenAI frameworks and tools.\nExcellent communication and collaboration skills.\nAbility to work independently and in a team-oriented environment.\nMethodical and meticulous towards work and planning.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative AI', 'Machine Learning', 'AWS', 'Natural Language Processing', 'Computer Vision', 'Deep Learning']",2025-06-12 14:56:27
Machine Learning Engineer 2,Adobe,4 - 7 years,Not Disclosed,['Noida'],"Develop algorithms that apply deep learning and innovative methods in NLP & computer vision, combined with traditional large sophisticated solutions/codebases!\nDeveloping innovative solutions using Generative AI, Python, Machine Learning and Data Science!\nBuild experiments, algorithms and ship solution that not only yield high accuracy but are also crafted and engineered to scale.\nCollaborate across multiple research and engineering teams, making the tradeoffs required to rapidly deliver AI/ML software solutions.\nDriven, Energetic and a team player are must haves.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'deep learning', 'data science', 'Machine learning', 'Research', 'Adobe', 'Software solutions', 'Python']",2025-06-12 14:56:29
Machine Learning Engineer,CADFEM India,1 - 4 years,4-9 Lacs P.A.,['Hyderabad'],"Job Description\nDesign and develop machine learning models tailored to mechanical engineering challenges, including predictive modelling, simulation optimisation, and failure analysis.\nUtilise deep learning and other advanced ML techniques to improve the accuracy and efficiency of CAE simulations.\nPreprocess and analyse large datasets from CAE simulations, experimental tests, and manufacturing processes for modelling.\nTrain, validate, and fine-tune machine learning models using real-world engineering data.\nOptimise models for performance, scalability, and robustness in production environments.\nCollaborate with CAE engineers to integrate ML models into existing simulation workflows (e.g., FEA, CFD, structural analysis).\nAutomate repetitive simulation tasks and enable predictive analytics for design optimisation.\nWork closely with mechanical engineers, data scientists, and software developers to identify business challenges and develop data-driven solutions.\nDeploy machine learning models into production environments and monitor their performance.\nMaintain and update models to ensure reliability and continuous improvement.\nStay abreast of the latest advancements in machine learning, AI, and CAE technologies.\nApply innovative approaches to solve complex engineering problems.\nRequirements\nBachelors or Master’s degree in Mechanical Engineering, Computer Science, or a related field\nProven 2-3 years of experience in developing and deploying machine learning models, preferably in mechanical engineering or CAE domain\nHands-on experience with CAE tools such as ANSYS, Abaqus, or similar FEA/CFD software\nStrong programming skills in Python, R, or Java\nProficiency in machine learning frameworks (TensorFlow, PyTorch, scikit-learn)\nExperience with data preprocessing, feature engineering, and statistical analysis\nSolid understanding of mathematics, statistics, and problem-solving skills\nExcellent analytical thinking and ability to tackle complex engineering challenges\nStrong communication and teamwork skills to collaborate across disciplines\nPreferred: Experience with physics-informed machine learning and digital twin technologies\nPreferred: Familiarity with automation of CAE workflows and predictive modelling for product design\n\nBenefits\nChallenging job and a chance to team up with a young and dynamic professional group\nChance to build yourself as WE grow.\nRemuneration that stays competitive and attractive to retain the best.\nOpportunity to join an organization experiencing year on year growth.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ML/DL Engineer', 'AI Engineer', 'Data Scientist', 'Applied Machine Learning', 'Machine Learning Engineer', 'F1-score', 'Tensorflow', 'Data Preprocessing', 'LightGBM', 'Feature Engineering', 'RMSE', 'Numpy', 'Pytorch', 'Pandas', 'FastAPI', 'CatBoost', 'Flask', 'Python']",2025-06-12 14:56:32
Machine Learning Engineer,Adobe,12 - 15 years,Not Disclosed,[],"The engineer will be part of a team working on the development, operations and support of Adobe s AI Platform team. They will be responsible for the design, architecture and development of new features and maintenance of existing features. They will also handle all phases of development, from early specs and definition to release. They are encouraged to be hands-on problem solver and we'll conversant in analyzing, architecting and implementing Golang/python-based world class high-quality software. Prior experience on ML solutions and cloud platform services, workflow orchestrators, data pipeline solutions would be a plus.\n\nWhat you'll Do\nThis is an individual contributor position.\nHands on product/solution development knowledge are a must.\nThe position involves conceptualization of a product, design, development, debugging/triaging, deployment at scale, monitoring, analyzing, etc\nPlanning, effort estimation and risk analysis of a project.\nThe incumbent will plan, evaluate industry alternatives, design and drive new components, solutions, workflow, features, etc\nShould take the initiative to drive frugality through optimizations without compromising stability or resiliency.\n  Requirements\nbachelors / masters degree in engineering.\n12+ years of relevant industry experience.\n3+ years of experience as a lead/architect.\nA proven expertise with building large scale platforms on Kubernetes.\nProven programming skills with languages such as python and go-lang.\nExperience of the latest ML development tools.\nTrack record of delivering cloud-scale, data-driven products, and services that are widely adopted with large customer bases\nExposure to container runtime environments\nExperience in building, deploying, and managing infrastructures in public clouds (specifically AWS)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Debugging', 'Machine learning', 'Cloud', 'Conceptualization', 'Programming', 'Product design', 'Adobe', 'Monitoring', 'Python']",2025-06-12 14:56:34
Machine Learning Engineer,Whats On India Media,3 - 8 years,Not Disclosed,"['Mumbai', 'Gurugram', 'Bengaluru']","Nielsen is seeking an organized, detail oriented, team player, to join the Engineering team in the role of Software Machine Learning Engineer. Nielsen's Audience Measurement Engineering platforms support the measurement of television viewing in more than 30 countries around the world. The Software Engineer will be responsible to define, develop, test, analyze, and deliver technology solutions within Nielsen's Collections platforms.\nRequired Skills\nBachelor's degree in Computer Science or equivalent degree.\n3+ years of software experience\nExperience with Machine learning frameworks and models. Pytorch experience preferred\nStrong understanding of statistical analysis and mathematical data manipulation\nWork with web technology including Java, Python, JavaScript, React/Redux, Kotlin.\nFollow best practices for software development and deployment\nUnderstanding of relational database, big data, and experience in SQL\nProficient at using GIT, GitFlow, JIRA, Gitlab and Confluence.\nStrong analytical and problem solving skills.\nOpen-minded and passionate to learn and grow technology skills\nStrong sense of accountability\nSolution-focused and ability to drive change within the organization\nExperience in writing unit/integration tests including test automation.\nStrong testing and debugging abilities, functional, analytical and technical abilities, ability to find bugs, attention to detail, troubleshooting\nAdditional Useful Skills\nA fundamental understanding of the AWS ecosystem (EC2, S3, EMR, Lambda, etc)\nExperienced in building RESTful APIs.\nExperience in writing unit/integration tests including test automation.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Ml Algorithms', 'Python', 'Pytorch']",2025-06-12 14:56:37
"AIML - Machine Learning Engineer, Search & AI",Apple,7 - 12 years,Not Disclosed,['Bengaluru'],"Our team is responsible for delivering next-generation Search and Question Answering systems across Apple products including Siri, Safari, Spotlight, and more. This is your chance to shape how people get information by leveraging your Search and applied machine learning expertise along with robust software engineering skills.You will collaborate with outstanding Search and AI engineers on large scale machine learning to improve Query Understanding, Retrieval, and Ranking, developing fundamental building blocks needed for AI powered experiences such as fine-tuning and reinforcement learning. This involves pushing the boundaries on document retrieval and ranking, developing sophisticated machine learning models, using embeddings and deep learning to understand the quality of matches. It also includes online learning to react quickly to change and natural language processing to understand queries. You will work with petabytes of data and combine information from multiple structured and unstructured sources to provide best results and accurate answers to satisfy users information-seeking needs.\n7+ years experience in shipping Search and Q&A technologies and ML systems\nExcellent programming skills in mainstream programming languages such as C++, Python, Scala, and Go\nExperience delivering tooling and frameworks to evaluate individual components and end-to-end quality\nStrong analytical skills to systematically identify opportunities to improve search relevance and answer accuracy\nExcellent communication skills and ability to work in a collaborative research environment\nPassion for building phenomenal products and curiosity to learn\nPreferred Qualifications\nBackground in Search Relevance and Ranking, Question Answering systems, Query Understanding, Personalization or Recommendation systems, or data-informed decision-making\nHands-on experience in Retrieval Augmented Generation, including developing, evaluating and enhancing for both retrievers and generative LLMs\nMS in AI, Machine Learning, Information Retrieval, Computer Science, Statistics, or a related field",Industry Type: Consumer Electronics & Appliances,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'QA', 'Analytical skills', 'C++', 'Machine learning', 'SCALA', 'query', 'Information retrieval', 'Natural language processing', 'Python']",2025-06-12 14:56:39
Machine Learning Engineer,Panacorp Software Solutions,0 - 5 years,Not Disclosed,"['Nagercoil', 'Kanyakumari']","Research Programmer (Python/ MATLAB) Fresher & Experienced\nAbout Panacorp Software Solutions\nPanacorp Software Solutions is a research-driven organization specializing in providing technical assistance for PhD research projects. Our focus is on supporting research scholars with programming, simulations, and computational analysis in various domains, including AI, Machine Learning, and numerical computing.\n\nJob Role & Responsibilities\nAssist in research-based projects related to PhD studies.\nPerform simulations, numerical computing, and data analysis using Python, MATLAB, and Simulink.\nSupport research scholars in implementing Machine Learning (ML) and Deep Learning (DL) models.\nAutomate processes and optimize research workflows through scripting.\nDocument research methodologies, findings, and technical reports.\nWork closely with scholars to analyze and interpret computational results.\nEligibility Criteria\nQualification: BE/B.Tech/MCA\nExperience: 0 5+ years (Freshers with strong academic knowledge can apply).\nStrong understanding of research methodologies and computational tools.\nPreferred Skills\nProficiency in Python, MATLAB, and Simulink.\nKnowledge of data analysis, AI/ML techniques, and numerical simulations.\nAbility to interpret and validate research outcomes.\nStrong analytical and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Machine Learning', 'matlab', 'simulink', 'python', 'data analysis', 'research methodology', 'artificial intelligence']",2025-06-12 14:56:41
Staff Data Engineer - Machine Learning,Netradyne,5 - 8 years,22.5-35 Lacs P.A.,['Bengaluru'],"Role and Responsibilities:\n\nYou will be embedded within a team of machine learning engineers and data scientists; responsible for building and productizing generative AI and deep learning solutions. You will:\nDesign, develop and deploy production ready scalable solutions that utilizes GenAI, Traditional ML models, Data science and ETL pipelines\nCollaborate with cross-functional teams to integrate AI-driven solutions into business operations.\nBuild and enhance frameworks for automation, data processing, and model deployment.\nUtilize Gen-AI tools and workflows to improve the efficiency and effectiveness of AI solutions.\nConduct research and stay updated with the latest advancements in generative AI and related technologies.\nDeliver key product features within cloud analytics.\n\nRequirements:\n\nB. Tech, M. Tech or PhD in Computer Science, Data Science, Electrical Engineering, Statistics, Maths, Operations Research or related domain.\nStrong programming skills in Python, SQL and solid fundamentals in computer science, particularly in algorithms, data structures, and OOP.\nExperience with building end-to-end solutions on AWS cloud infra.\nGood understanding of internals and schema design for various data stores (RDBMS, Vector databases and NoSQL).\nExperience with Gen-AI tools and workflows, and large language models (LLMs).\nExperience with cloud platforms and deploying models at scale.\nStrong analytical and problem-solving skills with a keen attention to detail.\nStrong knowledge of statistics, probability, and estimation theory.\n\nDesired Skills:\n\nFamiliarity with frameworks such as PyTorch, TensorFlow and Hugging Face.\nExperience with data visualization tools like Tableau, Graphana, Plotly-Dash.\nExposure to AWS services like Kinesis, SQS, EKS, ASG, lambda etc.\nExpertise in at least one popular Python web-framework (like FastAPI, Django or Flask).\nExposure to quick prototyping using Streamlit, Gradio, Dash etc.\nExposure to Big Data processing (Snowflake, Redshift, HDFS, EMR)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'AWS', 'Generative Artificial Intelligence', 'Python', 'Big Data Technologies']",2025-06-12 14:56:43
Machine Learning Engineer,Crunchyroll,2 - 7 years,Not Disclosed,['Hyderabad'],"We are considering applicants for the location(s) of Hyderabard, India.\nWe are looking for an experienced Machine Learning Engineer with a focus on MLOps to join our dynamic team and ensure the seamless productionization, maintenance, and monitoring of machine learning and AI applications. You will support a range of applications, from traditional classification, forecasting, and prediction models to recommendation systems and LLM-powered solutions. You will collaborate with Machine Learning Engineers, Data Scientists, and Platform/Software Engineers to architect scalable, maintainable, and systems that adhere to operational excellence principles.\nCore Areas of Responsibility\nDesign, implement, and maintain MLOps pipelines for deploying, monitoring, and scaling machine learning models, including traditional models and LLM-powered applications.\nEnsure the architecture of ML systems prioritises scalability, reliability, and maintainability.\nDevelop automated workflows for model training, testing, deployment, and monitoring.\nImplement monitoring and alerting systems to track model performance, data drift, and system health in production.\nCollaborate with Machine Learning Engineers and Data Scientists to refine model integration into production environments. Work with Platform/Software Engineers to integrate ML applications with existing infrastructure and ensure compatibility with cloud or on-premises systems.\nStay up-to-date with MLOps best practices, tools, and new technologies to enhance system performance and reliability.\nAbout You\n2+ years of experience MLOps, including deploying and maintaining machine learning models in production environments.\nProficiency in Python programming and familiarity with ML frameworks such as TensorFlow, PyTorch, or equivalent.\nExperience with MLOps tools and platforms (e.g., MLflow, Kubeflow, Airflow, or similar).\nKnowledge of cloud platforms (e.g., AWS, Google Cloud, Azure) and containerization technologies (e.g., Docker, Kubernetes).\nFamiliarity with CI/CD pipelines and version control systems like Git.\nUnderstanding of operational excellence principles, including system reliability, scalability, and monitoring.",Industry Type: Film / Music / Entertainment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Version control', 'GIT', 'PDF', 'HP data protector', 'Machine learning', 'Flex', 'Healthcare', 'Forecasting', 'Monitoring', 'Python']",2025-06-12 14:56:45
Machine Learning Engineer,Tek Ninjas,8 - 13 years,Not Disclosed,['Pune'],"Skills:\n     Proficient:\nLanguages/Framework: Fast API, Azure UI Search API (React)\nCloud: Azure Cloud Basics (Azure DevOps)\nGitlab: Gitlab Pipeline\nAnsible and REX: Rex Deployment\nData Science: Prompt Engineering + Modern Testing\nData pipeline development\nUnderstanding of AI/ML algorithms and their applications\nMLOps frameworks\nKnowledge of cloud platforms (Azure ML especially)\nModel deployment process\nData pipeline monitoring\nLanguages/Framework: Azure Open AI\nData Science: Open AI GPT Family of models 4o/4/3, Embeddings + Vector Search\nDatabases and ETL: Azure Storage Account, Postgresql, Cosmos\nExperience with ML frameworks (TensorFlow, PyTorch, Scikit-learn)\nKnowledge of cloud platforms (AWS SageMaker, Google AI Platform)\nExpertise in data preprocessing, feature engineering, and model evaluation\nUnderstanding of software engineering principles (version control, CI/CD, containerization)\nFamiliarity with distributed computing and big data tools (Spark, Hadoop)\nAbility to optimize models for performance and scalability\nExperience with Azure AI Search\nDesired skills*\nAzure DevOps; MLOps frameworks; Postgresql; Cosmos",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['MLOPS', 'Aiml', 'Machine Learning', 'Azure Devops']",2025-06-12 14:56:47
Hiring Machine Learning Engineer,Motivity Labs,5 - 10 years,14-22.5 Lacs P.A.,['Hyderabad'],"Role - Machine Learning Engineer\nRequired Skills & Experience\n\n5+ years of hands-on experience in building, training, and deploying machine learning models in a professional, production-oriented setting.\nDemonstrable experience with database creation and advanced querying (e.g., SQL, NoSQL), with a strong understanding of data warehousing concepts.\nProven expertise in data blending, transformation, and feature engineering, adept at integrating and harmonizing both structured (e.g., relational databases, CSVs) and unstructured (e.g., text, logs, images) data.\nStrong practical experience with cloud platforms for machine learning development and deployment; significant experience with Google Cloud Platform (GCP) services (e.g., Vertex AI, BigQuery, Dataflow) is highly desirable.\nProficiency in programming languages commonly used in data science (e.g., Python is preferred, R).\nSolid understanding of various machine learning algorithms (e.g., regression, classification, clustering, dimensionality reduction) and experience with advanced techniques like Deep Learning, Natural Language Processing (NLP), or Computer Vision.\nExperience with machine learning libraries and frameworks (e.g., scikit-learn, TensorFlow, PyTorch).\nFamiliarity with MLOps tools and practices, including model versioning, monitoring, A/B testing, and continuous integration/continuous deployment (CI/CD) pipelines.\nExperience with containerization technologies like Docker and orchestration tools like Kubernetes for deploying ML models as REST APIs.\nProficiency with version control systems (e.g., Git, GitHub/GitLab) for collaborative development.\n\nInterested candidates share cv to dikshith.nalapatla@motivitylabs.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GCP', 'Machine Learning', 'Deep Learning', 'Python', 'BigQuery', 'MLOps', 'Git', 'Vertex AI', 'GitHub/GitLab', 'A/B testing', 'Dataflow', 'Kubernetes']",2025-06-12 14:56:49
Cloud Machine Learning LLM Serving Engineer,Qualcomm,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nJD for Cloud Machine Learning LLM Serving engineer\n\nJob Overview:\n\nThe Qualcomm Cloud Computing team is developing hardware and software for Machine Learning solutions spanning the data center, edge, infrastructure, automotive market. We are seeking ambitious, bright, and innovative engineers with experience in machine learning framework development. Job activities span the whole product life cycle from early design to commercial deployment. The environment is fast-paced and requires cross-functional interaction daily so good communication, planning and execution skills are a must.\n\nKey Responsibilities\nImprove and optimize key Deep Learning models on Qualcomm AI 100.\nBuild deep learning framework extensions for Qualcomm AI 100 in upstream open-source repositories.\nImplement Kernels for AI workloads\nCollaborate and interact with internal teams to analyze and optimize training and inference for deep learning.\nBuild software tools and ecosystem around AI SW Stack.\nWork on vLLM, Triton, ExecuTorch, Inductor, TorchDynamo to build abstraction layers for inference accelerator.\nOptimize workloads for both scale-up (multi-SoC) and scale-out (multi-card) systems.\nOptimize the entire deep learning pipeline including graph compiler integration.\nApply knowledge of software engineering best practices.\n\n\nDesirable Skills and Aptitudes\nDeep Learning experience or knowledge- LLMs, Natural Language Processing, Vision, Audio, Recommendation systems.\nKnowledge of the structure and function of different components of Pytorch, TensorFlow software stacks.\nExcellent C/C++/Python programming and software design skills, including debugging, performance analysis, and test design.\nAbility to work independently, define requirements and scope, and lead your own development effort.\nWell versed with open-source development practices.\nStrong developer with a research mindset- strives to innovate.\nAvid problem solver- should be able to find solutions to key engineering and domain problems.\n\n\nKnowledge of tiling and scheduling a Machine learning operator is a plus.\nExperience in using C++ 14 (advanced features)\nExperience of profiling software and optimization techniques\nHands on experience writing SIMD and/or multi-threaded high-performance code is a plus.\nExperience of ML compiler, Auto-code generation (using MLIR) is a plus.\nExperiences to run workloads on large scale heterogeneous clusters is a plus.\nHands-on experience with CUDA, CUDNN is a plus.\n\n\nQualifications:\nBachelor's / Masters/ PHD degree in Engineering, Machine learning/ AI, Information Systems, Computer Science, or related field.\n2+ years Software Engineering or related work experience.\n2+ years experience with Programming Language such as C++, Python.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'c++', 'c', 'software design', 'software engineering', 'cuda', 'natural language processing', 'scale', 'machine learning', 'artificial intelligence', 'deep learning', 'tensorflow', 'code generation', 'computer science', 'pytorch', 'debugging', 'machine learning algorithms', 'ml']",2025-06-12 14:56:51
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Bengaluru'],"Key Responsibilities :\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n- Experience with imbalanced datasets and techniques to improve model generalization.\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n- Strong mathematical and statistical knowledge with problem-solving skills.\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n- Experience in automating ML pipelines with MLOps practices.\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'Azure', 'MLOps', 'GCP', 'Big Data', 'Neural Networks', 'AWS', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-12 14:56:54
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Thane'],"Key Responsibilities :\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n- Experience with imbalanced datasets and techniques to improve model generalization.\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n- Strong mathematical and statistical knowledge with problem-solving skills.\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n- Experience in automating ML pipelines with MLOps practices.\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'Azure', 'MLOps', 'GCP', 'Big Data', 'Neural Networks', 'AWS', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-12 14:56:56
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Surat'],"Key Responsibilities :\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n- Experience with imbalanced datasets and techniques to improve model generalization.\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n- Strong mathematical and statistical knowledge with problem-solving skills.\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n- Experience in automating ML pipelines with MLOps practices.\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'Azure', 'MLOps', 'GCP', 'Big Data', 'Neural Networks', 'AWS', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-12 14:56:58
"Engineer, Principal/Manager - Machine Learning, AI",Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"General Summary:\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Systems Engineer, you will research, design, develop, simulate, and/or validate systems-level software, hardware, architecture, algorithms, and solutions that enables the development of cutting-edge technology. Qualcomm Systems Engineers collaborate across functional teams to meet and exceed system-level requirements and standards.\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 8+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 7+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 6+ years of Systems Engineering or related work experience.\nPrincipal Engineer Machine Learning\nWe are looking for a Principal AI/ML Engineer with expertise in model inference, optimization, debugging, and hardware acceleration. This role will focus on building efficient AI inference systems, debugging deep learning models, optimizing AI workloads for low latency, and accelerating deployment across diverse hardware platforms.\nIn addition to hands-on engineering, this role involves cutting-edge research in efficient deep learning, model compression, quantization, and AI hardware-aware optimization techniques. You will explore and implement state-of-the-art AI acceleration methods while collaborating with researchers, industry experts, and open-source communities to push the boundaries of AI performance.\nThis is an exciting opportunity for someone passionate about both applied AI development and AI research, with a strong focus on real-world deployment, model interpretability, and high-performance inference.\nEducation & Experience:\n20+ years of experience in AI/ML development, with at least 5 years in model inference, optimization, debugging, and Python-based AI deployment.\nMasters or Ph.D. in Computer Science, Machine Learning, AI\nLeadership & Collaboration\nLead a team of AI engineers in Python-based AI inference development.\nCollaborate with ML researchers, software engineers, and DevOps teams to deploy optimized AI solutions.\nDefine and enforce best practices for debugging and optimizing AI models\nKey Responsibilities\nModel Optimization & Quantization\nOptimize deep learning models using quantization (INT8, INT4, mixed precision etc), pruning, and knowledge distillation.\nImplement Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) for deployment.\nFamiliarity with TensorRT, ONNX Runtime, OpenVINO, TVM\nAI Hardware Acceleration & Deployment\nOptimize AI workloads for Qualcomm Hexagon DSP, GPUs (CUDA, Tensor Cores), TPUs, NPUs, FPGAs, Habana Gaudi, Apple Neural Engine.\nLeverage Python APIs for hardware-specific acceleration, including cuDNN, XLA, MLIR.\nBenchmark models on AI hardware architectures and debug performance issues\nAI Research & Innovation\nConduct state-of-the-art research on AI inference efficiency, model compression, low-bit precision, sparse computing, and algorithmic acceleration.\nExplore new deep learning architectures (Sparse Transformers, Mixture of Experts, Flash Attention) for better inference performance.\nContribute to open-source AI projects and publish findings in top-tier ML conferences (NeurIPS, ICML, CVPR).\nCollaborate with hardware vendors and AI research teams to optimize deep learning models for next-gen AI accelerators.\nDetails of Expertise:\nExperience optimizing LLMs, LVMs, LMMs for inference\nExperience with deep learning frameworks: TensorFlow, PyTorch, JAX, ONNX.\nAdvanced skills in model quantization, pruning, and compression.\nProficiency in CUDA programming and Python GPU acceleration using cuPy, Numba, and TensorRT.\nHands-on experience with ML inference runtimes (TensorRT, TVM, ONNX Runtime, OpenVINO)\nExperience working with RunTimes Delegates (TFLite, ONNX, Qualcomm)\nStrong expertise in Python programming, writing optimized and scalable AI code.\nExperience with debugging AI models, including examining computation graphs using Netron Viewer, TensorBoard, and ONNX Runtime Debugger.\nStrong debugging skills using profiling tools (PyTorch Profiler, TensorFlow Profiler, cProfile, Nsight Systems, perf, Py-Spy).\nExpertise in cloud-based AI inference (AWS Inferentia, Azure ML, GCP AI Platform, Habana Gaudi).\nKnowledge of hardware-aware optimizations (oneDNN, XLA, cuDNN, ROCm, MLIR, SparseML).\nContributions to open-source community\nPublications in International forums conferences journals",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'AWS Inferentia', 'Azure ML', 'AI/ML', 'ONNX Runtime', 'OpenVINO', 'GCP AI', 'TVM', 'XLA', 'MLIR', 'TensorRT', 'Python']",2025-06-12 14:57:01
Principal Machine Learning Engineer,Amgen Inc,2 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nWe are seeking a highly skilled Machine Learning Engineer with a strong MLOps background to join our team. You will play a pivotal role in building and scaling our machine learning models from development to production. Your expertise in both machine learning and operations will be essential in creating efficient and reliable ML pipelines.\nRoles & Responsibilities:\nCollaborate with data scientists to develop, train, and evaluate machine learning models.\nBuild and maintain MLOps pipelines, including data ingestion, feature engineering, model training, deployment, and monitoring.\nLeverage cloud platforms (AWS, GCP, Azure) for ML model development, training, and deployment.\nImplement DevOps/MLOps best practices to automate ML workflows and improve efficiency.\nDevelop and implement monitoring systems to track model performance and identify issues.\nConduct A/B testing and experimentation to optimize model performance.\nWork closely with data scientists, engineers, and product teams to deliver ML solutions.\nGuide and mentor junior engineers in the team\nStay updated with the latest trends and advancements\n\nBasic Qualifications:\nDoctorate degree and 2 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nMasters degree and 8 to 10 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nBachelors degree and 10 to 14 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nDiploma and 14 to 18 years of years of Computer Science, Statistics, and Data Science, Machine Learning experience\nPreferred Qualifications:\nMust-Have Skills:\nStrong foundation in machine learning algorithms and techniques\nExperience in MLOps practices and tools (e.g., MLflow, Kubeflow, Airflow); Experience in DevOps tools (e.g., Docker, Kubernetes, CI/CD)\nProficiency in Python and relevant ML libraries (e.g., TensorFlow, PyTorch, Scikit-learn)\nOutstanding analytical and problem-solving skills; Ability to learn quickly; Excellent communication and interpersonal skills\nGood-to-Have Skills:\nExperience with big data technologies (e.g., Spark), and performance tuning in query and data processing\nExperience with data engineering and pipeline development\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification\nKnowledge of NLP techniques for text analysis and sentiment analysis\nExperience in analyzing time-series data for forecasting and trend analysis\nFamiliar with AWS, Azure, or Google Cloud;\nFamiliar with Databricks platform for data analytics and MLOps\nProfessional Certifications\nCloud Computing and Databricks certificate preferred\nSoft Skills:\nExcellent analytical and fixing skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Azure', 'NLP', 'MLOps', 'Databricks', 'AWS', 'Google Cloud']",2025-06-12 14:57:03
Machine Learning Engineer (Full Stack),Hubnex,5 - 10 years,Not Disclosed,['Gurugram'],"Machine Learning Engineer (Full Stack)\nLocation: Gurugram, India (On-site/Hybrid)\nType: Full-Time | 5+ Years Experience | AI & Product Engineering\nHubnex Labs is seeking a highly skilled Machine Learning Engineer with strong capabilities in Full Stack Development to lead the development and deployment of production-grade AI systems. This role requires expertise in building end-to-end ML pipelines from data preprocessing to deployment while also contributing to the full stack of software platforms that power our solutions.\nKey Responsibilities Machine Learning & Data Science\nUnderstand business goals and translate them into ML-based solutions\nDevelop, analyze, and compare machine learning algorithms for various problem statements\nBuild robust validation strategies and design appropriate preprocessing and feature engineering pipelines\nPerform data exploration, visualization , and quality verification , including data cleaning and augmentation\nTrain models, tune hyperparameters , and interpret model performance\nAnalyze errors and design strategies to improve model robustness\nDiscover and utilize public datasets for model training and benchmarking\nDeploy models into production environments with real-world performance and latency considerations\nSoftware & System Development\nDesign and develop end-to-end production systems , including backend APIs and frontend interfaces\nMaintain full stack web applications , ensuring seamless ML model integration\nEnsure efficient use of hardware resources for training and inference\nCollaborate cross-functionally with engineering, product, and design teams\nTechnical Skills Required\n5+ years of hands-on experience in machine learning and full stack development\nProficiency in Python and ML libraries like scikit-learn , pandas , NumPy , etc.\nDeep learning experience using TensorFlow , Keras , or equivalent frameworks\nProficiency with OpenCV and image/video processing techniques\nExperience with data visualization tools and big data handling\nStrong understanding of data pipelines , feature engineering , and augmentation techniques\nProficiency in Full Stack Web Development (e.g., React, Node.js, Express, MongoDB or similar)\nExperience deploying models using REST APIs, Flask/FastAPI, Docker, etc.\nFamiliarity with Linux environments and GPU-accelerated compute systems\nUnderstanding of hardware requirements and optimization for real-time ML performance\nWhy Join Hubnex Labs?\nWork on impactful AI products deployed in real-world use cases\nBe a part of a fast-growing tech consulting and product innovation company\nCollaborate with a diverse team of engineers, data scientists, and innovators\nFlexible and collaborative culture based in Gurugram , with hybrid work options\nIdeal Candidate\nPassionate about building smart systems that go live in production\nCan operate independently and take full ownership of ML products\nBlends deep technical skill with product thinking and business awareness",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Product engineering', 'Backend', 'Linux', 'Product innovation', 'Consulting', 'Machine learning', 'Web development', 'MongoDB', 'Business awareness', 'Python']",2025-06-12 14:57:05
Data Scientist,Tesco,1 - 3 years,Not Disclosed,['Bengaluru( Whitefield )'],"Job Summary:\n\nEnable data driven decision making across the Tesco business globally by developing analytics solutions using a combination of math, tech and business knowledge\n\nRoles and Responsibilities:\n- Identifying operational improvements and finding solutions by applying CI tools and techniques\n- Responsible for completing tasks and transactions within agreed KPI's",,,,"['Data Science', 'Advanced Excel', 'Data Analytics', 'Python', 'SQL', 'Applied Mathematics', 'Machine Learning', 'Statistics']",2025-06-12 14:57:07
AI / ML Data Scientist,Sanofi,3 - 5 years,Not Disclosed,['Hyderabad'],"Transform healthcare through innovation. At Sanofi, we're not just developing treatments we're pioneering the future of healthcare by harnessing the power of data insights and responsible AI to accelerate breakthrough therapies.\n  As an AI/ML Scientist on our AI and Computational Sciences team, you'll:\nDrive innovation that directly impacts patient outcomes\nCollaborate with world-class scientists to solve complex healthcare challenges\nApply advanced AI techniques to increase drug development success rates\nShape the responsible use of AI in life-saving medical research\nBe part of a mission that matters. Help us transform data into life-changing treatments and join a team where your expertise can make a meaningful difference in patients lives.\nOur Team :\nThe AI and Computational Sciences team is a key team within R&D Digital, focused on image, omics, wearable sensor data, and clinical data analytics. This team plays a critical role in bridging the gap between general purposed digital products and specific project needs.\nWe are looking for a skilled AI/ML Data Scientist to join our elite AI and Computational Sciences team and harness cutting-edge AI to revolutionize healthcare. As a key player within R&D Digital, you'll transform complex data into life-changing medical breakthroughs.\n  Impact you'll Make\nDrive innovation across multiple high-impact domains:\nPrecision Medicine: Develop patient response prediction models that personalize treatments\nAdvanced Omics Analysis: Pioneer cell type and cell stage quantification techniques\nAdvanced Image/Video Analysis: Lead application of state-of-art computer vision methods for gaining unprecedented insights about drug efficacy from medical images/videos\nDigital Health: Design novel biomarkers from wearable sensor data\nBiological Insights: Create enzyme property prediction algorithms and conduct disease pathway analyses\nYour Growth Journey\nTechnical Mastery: Develop expertise across image analysis, time series modeling, GenAI, AI Agents, and explainable AI\nScientific Impact: Publish in top-tier AI/ML journals and secure patents that protect groundbreaking innovations\nGlobal Influence: Deploy solutions that impact patients worldwide\nYour Environment\nElite Team: Work alongside AI/ML experts and drug development experts in an agile, high-performance environment\nCutting-Edge Resources: Access Sanofis state-of-the-art cloud infrastructure and data platforms\nContinuous Learning: Receive mentorship and training opportunities to sharpen your leadership and AI/ML skills\nJoin Our AI-First Vision\nDevelop your skills through world-class mentorship and training\nChase the miracles of science to improve peoples lives\nReady to transform healthcare through the power of AI?\nMain Responsibilities :\nResearch Phase Excellence\nDesign and implement AI models for target identification and validation using multi-omics data (genomics, proteomics, transcriptomics)\nDevelop predictive algorithms to molecular design for compound selection and accelerate lead optimization\nCreate computer vision systems for high-throughput screening image analysis and cellular phenotyping\nClinical Development Innovation\nEngineer digital biomarkers from wearable sensors and mobile devices to enable objective, continuous patient monitoring\nImplement advanced time-series analysis of real-time patient data to detect early efficacy signals\nDesign AI-powe'red patient stratification models to identify responder populations and optimize trial design\nMulti-Modal Data Integration\nArchitect systems that harmonize diverse data types (imaging, omics, clinical, text, sensor) into unified analytical frameworks\nDevelop novel feature extraction techniques across modalities to enhance predictive power\nCreate visualization tools that present complex multi-modal insights to clinical teams\nScientific Impact\nCollaborate with cross-functional teams to translate AI insights into actionable drug development strategies\nPresent findings to scientific and business stakeholders with varying technical backgrounds\nPublish innovative methodologies in top-tier scientific and AI/ML journals\nContribute to patent applications to protect novel AI/ML approaches\nExperience : 3 to 5 years of experience in AI/ML and computational model development on multimodal data like omics, biomedical imaging, text and clinical trials data\n  Key Functional Requirement:\nDemonstrated track record of successful AI/ML project implementation\n3-5 years of experience in computational modeling or AI/ML algorithm development, or any other related field\nDeep understanding and proven track record of developing model training pipelines and workflows\nExcellent communication and collaboration skills\nWorking knowledge and comfort working with Agile methodologies\nTechnical Skills :\nProgramming Proficiency: Advanced Python skills with experience in ML frameworks (PyTorch, TensorFlow, JAX)\nMachine Learning: Deep expertise in supervised, unsupervised, and reinforcement learning algorithms\nDrug discovery: molecular design, docking, binding site prediction, mRNA vaccine design, ADMET property, protein structure prediction, molecular dynamics simulation\nDeep Learning: Experience designing and implementing neural network architectures (CNNs, RNNs, Transformers)\nComputer Vision: Proficiency in image processing, segmentation, and object detection techniques (SAM, ViT, Diffusion Models, MediaPipe, MMPose, MonoDepth, VoxelNet, SlowFast, C3D)\nNatural Language Processing: Experience with large language models, text mining, and information extraction (OpenAI, Claude, Llama, Qwen, Deepseek model series)\nTime Series Analysis: Expertise in analyzing temporal data from sensors and wearable devices (HAR foundation models, compliance detection models)\nOmics Analysis: Knowledge of computational methods for protein genomics, proteomics, or transcriptomics data\nCloud Computing: Experience deploying ML models on cloud platforms (AWS)\nTools and Technologies :\nData Processing: Experience with data pipelines and ETL processes\nVersion Control: Proficiency with Git and collaborative development workflows, Docker\nMLOps: Experience with model deployment, monitoring, and maintenance\nVisualization: Ability to create compelling data visualizations (Matplotlib, Seaborn, Plotly)\nExperiment Tracking: Familiarity with tools like MLflow, Weights & Biases, or similar platforms\nSoft Skills :\nStrong scientific communication abilities for technical and non-technical audiences\nCollaborative mindset for cross-functional team environments\nProblem-solving approach with ability to translate business needs into technical solutions\nSelf-motivated with capacity to work independently and drive projects forward\nEducation : PhD/MS/BE/BTech/ME/MTech in Computer Science and Engineering, AI/ML, other relevant engineering discipline, Computational Biology, Data Science, Bioinformatics or related fields (with equivalent experience)\n  Preferred : Publications or public github\nLanguages : English\n  Why Choose us\nBring the miracles of science to life alongside a supportive, future-focused team\nDiscover endless opportunities to grow your talent and drive your career, whether it s through a promotion or lateral move, at home or internationally\nEnjoy a thoughtful, well-crafted rewards package that recognizes your contribution and amplifies your impact\nTake good care of yourself and your family, with a wide range of health and wellbeing benefits including high-quality healthcare, prevention and wellness programs\nOpportunity to work in an international environment, collaborating with diverse business teams and vendors, working in a dynamic team, and fully empowe'red to propose and implement innovative ideas.",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analytical', 'Genomics', 'clinical development', 'Clinical trials', 'Healthcare', 'biomedical', 'Bioinformatics', 'Monitoring', 'clinical data']",2025-06-12 14:57:09
ML Engineer/Data Scientist,Altimetrik,6 - 8 years,15-30 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\nData Scientist /ML engineers : ML Engineer with Python, SQL, Machine Learning, Azure skills(Good to have)",Industry Type: IT Services & Consulting,,,"['Machine Learning', 'Python', 'SQL', 'Data Science', 'Ml', 'azure']",2025-06-12 14:57:11
"Data Scientist II, Regulatory Intelligence, Safety, and Compliance",Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"About Amazon Regulatory Intelligence, Safety, and Compliance (RISC).\n\nAmazon RISC s vision is to make Amazon the Earth s most trusted shopping destination for safe and compliant products. Towards this mission, we take a science-first approach to building technology, products and services, that protect customers from unsafe, illegal, controversial, or policy-violating products while offering the optimal selling partner experience.\n\nJob Summary\n\nWe are seeking an exceptional Data Scientist to join a team of experts in the field of AI/ML, and work together to tackle challenging business problems across diverse compliance domains. We leverage and train state-of-the-art multi-modal, large-language-models (LLMs), and vision language models (VLMs) to detect illegal and unsafe products across the Amazon catalog. We work on machine learning problems for generative AI, agentic system, multi-modal classification, intent detection, information retrieval, anomaly and fraud detection.\n\nThis is an exciting and challenging position to deliver scientific innovations into production systems at Amazon-scale to make immediate, meaningful customer impacts while also pursuing ambitious, long-term research. You will work in a highly collaborative environment where you can analyze and process large amounts of images, texts, documents, and tabular data. You will work on hard science problems that have not been solved before, conduct rapid prototyping to validate your hypothesis, and deploy your algorithmic ideas at scale. There will be something new to learn every day as we work in an environment with rapidly evolving regulations and adversarial actors looking to outwit your best ideas.\n\n\nDesign and evaluate state-of-the-art algorithms and approaches in generative AI, agentic system, multi-modal classification, intent detection, information retrieval, anomaly and fraud detection.\nTranslate product and CX requirements into measurable science problems and metrics.\nCollaborate with product and tech partners and customers to validate hypothesis, drive adoption, and increase business impact\nKey author in writing high quality scientific papers in internal and external peer-reviewed conferences.\n\nA day in the life\nUnderstanding customer problems, project timelines, and team/project mechanisms\nProposing science formulations and brainstorming ideas with team to solve business problems\nWriting code, and running experiments with re-usable science libraries\nReviewing labels and audit results with investigators and operations associates\nSharing science results with science, product and tech partners and customers\nWriting science papers for submission to peer-review venues, and reviewing science papers from other scientists in the team.\nContributing to team retrospectives for continuous improvements\nDriving science research collaborations and attending study groups with scientists across Amazon\n\nAbout the team\nWe are a team of scientists and engineers building AI/ML solutions to make Amazon the Earth s most trusted shopping destination for safe and compliant products. PhD, or Masters degree with 2+ years of machine learning experience, or bachelor degree with 3+ years of machine learning experience\nExperience programming in Python, Java, C++, or related language\nExperience with neural deep learning methods, LLM, and natural language processing\nExperience with conducting research in a corporate setting Experience with large scale machine learning systems such as profiling and debugging and understanding of system performance and scalability",,,,"['deep learning', 'C++', 'Debugging', 'Machine learning', 'Information retrieval', 'Natural language processing', 'Scientist II', 'Fraud detection', 'Auditing', 'Python']",2025-06-12 14:57:13
"Data Scientist II, Regulatory Intelligence, Safety, and Compliance",Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"We are seeking an exceptional Data Scientist to join a team of experts in the field of machine learning, and work together to tackle challenging problems across diverse compliance domains. We leverage risk models (including boosted trees and graph neural networks) as well as vision and large-language-models (LLMs) to detect illegal and unsafe products across the Amazon catalog. We work on machine learning problems for multi-modal classification, intent detection, information retrieval, anomaly and fraud detection, and generative AI.\n\nThis is an exciting and challenging position to deliver scientific innovations into production systems at Amazon-scale to make immediate, meaningful customer impacts while also pursuing ambitious, long-term research. You will work in a highly collaborative environment where you can analyze and process large amounts of image, text and tabular data. You will work on hard science problems that have not been solved before, conduct rapid prototyping to validate your hypothesis, and deploy your algorithmic ideas at scale. There will be something new to learn every day as we work in an environment with rapidly evolving regulations and adversarial actors looking to outwit your best ideas.\n\n\nExplore and evaluate state-of-the-art algorithms and approaches in risk modeling and vision/language models\nTranslate product and CX requirements into measurable science problems and metrics.\nCollaborate with product and tech partners and customers to validate hypothesis, drive adoption, and increase business impact\nEvaluate model performance in production and refresh/implement necessary updates to maintain optimal system performance.\n\nA day in the life\nUnderstanding customer problems, project timelines, and team/project mechanisms\nProposing science formulations and brainstorming ideas with team to solve business problems\nWriting code, and running experiments with re-usable science libraries\nReviewing labels and audit results with investigators and operations associates\nSharing science results with science, product and tech partners and customers\nContributing to team retrospectives for continuous improvements\nParticipating in science research collaborations and attending study groups with scientists across Amazon\n\nAbout the team\nWe are a team of scientists building AI/ML solutions to make Amazon Earth s most trusted shopping destination for safe and compliant products. 2+ years of data scientist experience\n3+ years of data querying languages (e.g. SQL), scripting languages (e.g. Python) or statistical/mathematical software (e.g. R, SAS, Matlab, etc.) experience\n3+ years of machine learning/statistical modeling data analysis tools and techniques, and parameters that affect their performance experience\nExperience applying theoretical models in an applied environment\nKnowledge of relevant statistical measures such as confidence intervals, significance of error measurements, development and evaluation data sets, etc. Experience in Python, Perl, or another scripting language\nExperience in a ML or data scientist role with a large technology company",,,,"['Data analysis', 'SAS', 'Neural networks', 'risk modeling', 'Machine learning', 'Information retrieval', 'Perl', 'MATLAB', 'Auditing', 'Python']",2025-06-12 14:57:16
"Data Scientist II, Regulatory Intelligence, Safety, and Compliance",Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"We are seeking an exceptional Data Scientist to join a team of experts in the field of machine learning, and work together to tackle challenging problems across diverse compliance domains. We leverage multi-modal and large-language-models (LLMs) to detect illegal and unsafe products across the Amazon catalog. We work on machine learning problems for multi-modal classification, intent detection, information retrieval, anomaly and fraud detection, and generative AI.\n\nThis is an exciting and challenging position to deliver scientific innovations into production systems at Amazon-scale to make immediate, meaningful customer impacts while also pursuing ambitious, long-term research. You will work in a highly collaborative environment where you can analyze and process large amounts of image, text and tabular data. You will work on hard science problems that have not been solved before, conduct rapid prototyping to validate your hypothesis, and deploy your algorithmic ideas at scale. There will be something new to learn every day as we work in an environment with rapidly evolving regulations and adversarial actors looking to outwit your best ideas.\n\n\nExplore and evaluate state-of-the-art algorithms and approaches in multi-modal classification, large language models (LLMs), intent detection, information retrieval, anomaly and fraud detection, and generative AI\nTranslate product and CX requirements into measurable science problems and metrics.\nCollaborate with product and tech partners and customers to validate hypothesis, drive adoption, and increase business impact\n\nA day in the life\nUnderstanding customer problems, project timelines, and team/project mechanisms\nProposing science formulations and brainstorming ideas with team to solve business problems\nWriting code, and running experiments with re-usable science libraries\nReviewing labels and audit results with investigators and operations associates\nSharing science results with science, product and tech partners and customers\nContributing to team retrospectives for continuous improvements\nParticipating in science research collaborations and attending study groups with scientists across Amazon\n\nAbout the team\nWe are a team of scientists building AI/ML solutions to make Amazon Earth s most trusted shopping destination for safe and compliant products. 2+ years of data scientist experience\n2+ years of data querying languages (e.g. SQL), scripting languages (e.g. Python) or statistical/mathematical software (e.g. R, SAS, Matlab, etc.) experience\n2+ years of machine learning/statistical modeling data analysis tools and techniques, and parameters that affect their performance experience\nExperience applying theoretical models in an applied environment\nKnowledge of relevant statistical measures such as confidence intervals, significance of error measurements, development and evaluation data sets, etc. Experience in Python, Perl, or another scripting language\nExperience in a ML or data scientist role with a large technology company",,,,"['Data analysis', 'Statistical modeling', 'SAS', 'Machine learning', 'Information retrieval', 'Perl', 'MATLAB', 'Fraud detection', 'Auditing', 'Python']",2025-06-12 14:57:18
Data Scientist,New Relic One,2 - 7 years,Not Disclosed,['Bengaluru'],"We are seeking an experienced and dynamic Senior Data Scientist to join our team. You will be primarily responsible for driving data-oriented projects and transforming ambiguous business problems into clear, actionable insights as well as productionalizing insights. The ideal candidate is adept at understanding the business needs that are often quantitatively ambiguous and using large complex data sets to find opportunities for product and process optimization.\nWhat youll do\nAnalyzing complex datasets, applying advanced statistical methods as necessary (e.g., time series forecasting, classification, linear/logistic regression).\nDesigning and deploying data-science and technology-based algorithmic solutions to address business needs.\nTranslating data findings into actionable business insights and plans.\nCollaborating effectively with internal stakeholders, understanding their needs and being able to communicate data-driven recommendations.\nPresenting information using data visualization techniques and clearly communicating complex findings and ideas to non-technical stakeholders.\nThis role requires\n2+ years of experience\nProven experience as a Data Scientist, or in a similar role.\nPhD or Masters degree in Statistics, Mathematics, Computer Science, or related quantitative field.\nStrong understanding and application of advanced statistical techniques and concepts, including but not limited to machine learning algorithms, classification, regression, and time series analysis.\nProficiency with data analysis tools and languages such as Python, SQL, etc.\nFamiliarity with data visualization tools (e.g., Looker, Tableau, PowerBI, etc.).\nStrong problem-solving abilities, business acumen, and excellent communication skills.\nAbility to work independently and with minimal supervision.Proven ability in managing and delivering on multiple, competing priorities.\nPrior experience with stakeholder management and ability to present complex data in a clear manner to non-technical audience.\nBonus points if you have\nExperience in Observability is a plus.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAN', 'Logistic regression', 'Data analysis', 'Process optimization', 'Machine learning', 'Stakeholder management', 'Forecasting', 'SQL', 'Python']",2025-06-12 14:57:20
Data Scientist,An Indian NBFC,3 - 8 years,Not Disclosed,['Chennai'],"Responsibilities:\nCollect, clean, and analyze large sets of structured and unstructured data to extract meaningful insights and trends\nDevelop and implement advanced machine learning algorithms to solve complex business problems\nSupport moving models to production, by creating high quality code modules that can be seamlessly integrated into existing systems (both on-prem and cloud)\nCommunicate complex findings to both technical and non-technical audiences through effective data visualization and storytelling.\nCollaborate with cross-functional teams to identify data-driven opportunities and translate business requirements into actionable data solutions.\nSupport the development and maintenance of data pipelines and infrastructure\nStay up-to-date with industry trends and advancements in Data Science and Machine Learning technologies.\n\nSkills Required:\nStrong foundation in statistics, and machine learning algorithms\nStrong proficiency in programming languages like Python and SQL.\nExcellent problem-solving and analytical skills.\nAbility to work independently and as part of a team.\nShould have built production models using at least 2 of the ML techniques: Clustering, Regression, Classification\nExperience in Banking & Financial Services is preferred.\nExperience working on cloud platforms (e.g., AWS, GCP) is preferred.\nA passion for data and a curiosity to explore new trends and technologies",Industry Type: NBFC,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Pipeline', 'Data Extraction', 'Model Building', 'Artificial Intelligence', 'Cloud', 'Machine Learning']",2025-06-12 14:57:23
Data Scientist,Mindpro Technologies,4 - 9 years,5-12 Lacs P.A.,"['Karur', 'Dharwad']","Greetings From Mind Pro Technologies Pvt ltd (www.mindprotech.com)\n\nJob Title : Data Scientist\nWork Location : Karur (Tamil Nadu) or Dharwad (Karnataka )\nNp : 15days or Less\n\n\nJOB DESCRIPTION:\n Must have At least 4+ Years of experience in Python with Data Science.\n Must have worked on at least one Live project.\nExperience in relevant field such as Statistics, Computer Science or Applied Math or Operational Research.\nMust have Masters in (Maths/Statistics or Applied Mathematics/Machine Learning etc.)\nHistory of successfully performing customer implementations\nStrong customer facing skills, and previous consulting experience.\nExperience of handling high frequency streaming data for real time analysis and reporting.\nFamiliarity with - Natural Language Processing, Statistical Analysis (distribution analysis, correlation, variance, deep learning.\nExperience in tools like AWS, IBM Watson is a plus.\nExperience with open source technologies is a must.\nExcellent communication\nAbility to lead & build strong teams\nAbility to work in an ambiguous environment\n\nDesired Skills and Experience\nLanguages/Tools: Python/R.\nApproaches: Machine Learning\nConcepts: Supervised ANN, Bayesian, Gaussian, Vector Quantization, Logistic Model, Statistical, Predictive Modeling, Minimum Message Length, SVM, Random Forest, Ensembles, ANOVA, Decision Trees, Hidden Markov Models\nUnsupervised ANN, ARL, Clustering Hierarchical, Cluster Analysis\nReinforcement\nGen AI, LLM, LSTM, RNN, CNN, KNN\nBig Data (Good to have): Hadoop /Kafka / Storm / Spark streaming\nOS: Linux, Windows 32/64 bits.\n\nNote:  should know supervised and unsupervised learning,   semi-supervised learning, neural networks concepts, and how ML algorithms works with training and testing data. Experience on particular data set to train, test and roll-out for production use\n\nTool sets : Python, R, MATLAB or  any AI frame work, Neural network, Gen AI, LLM\nContact Details:\n\nRecruitment Team\nMindpro Technologies Pvt Ltd (www.mindprotech.com)\n+91-04324-240904 / +91-9600672304",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Gen AI', 'Statistical Modeling', 'LLM', 'Predictive Modeling', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Machine Learning', 'Deep Learning', 'Python']",2025-06-12 14:57:25
Data Scientist,Puresoftware Technology,8 - 13 years,Not Disclosed,['Bengaluru( Whitefield )'],"Job Title: Data Scientist(5 Positions)/ Lead OR Manager -Data Scientist (3 positions)\n\nExperience: Data scientist (8-10 years) / Lead Data scientist(14+ years)\n\nJob Location: Whitefield, Bangalore\nMode of working: Hybrid\n\nInterview Process: First Round: L1-Internal interview\nSecond Round: Assessment shared by us needs to be completed in 48 hours\nThird Round: Client discussion over the submitted assessment.\nFinal Round: HR Discussion\n\nPreferred Domain: Healthcare Insurance/ Insurance agencies / Health Insurance / Any Insurance\n\nWe are looking for a talented Data Scientist to join our growing team. In this role, you will lead efforts to develop, enhance, and optimize advanced AI and machine learning models with a particular focus on Generative AI, Large Language Models (LLMs), Langchain, and Prompt Engineering. You will oversee the application of statistical modeling techniques to derive insights, build models, and lead research initiatives that push the boundaries of AI technologies.\n\nKey Responsibilities:\nLeadership & Collaboration: Lead a team of data scientists, researchers, and engineers working on high-impact projects related to generative models, NLP, and statistical modeling. Collaborate with cross-functional teams, including engineering, product management, and research, to deliver AI-powered products and solutions.\nGenerative AI Development: Spearhead the development and deployment of Generative AI models and algorithms to address complex problems in areas like content generation, conversational AI, and creative automation.\nLLM Implementation & Optimization: Develop, fine-tune, and optimize large language models (LLMs) for diverse applications, ensuring they are robust, scalable, and accurate in real-world scenarios.\nLangchain Integration: Design and integrate Langchain for managing and deploying sophisticated language models with a focus on complex workflows, multi-agent systems, and real-time applications.\nPrompt Engineering: Lead prompt engineering efforts to optimize AI models' output quality, improve interactions, and enable more effective natural language understanding across a variety of use cases.\nStatistical Modeling: Utilize advanced statistical techniques to analyze and interpret data, build predictive models, and solve business-critical challenges through data-driven insights.\nResearch & Innovation: Stay ahead of trends in AI and ML, particularly in the fields of NLP, LLMs, and generative models. Drive innovation by exploring cutting-edge techniques and methodologies in the AI space.\nMentorship & Knowledge Sharing: Mentor junior team members and promote a collaborative, learning-oriented environment. Share knowledge and foster an atmosphere of continuous improvement within the data science team.\nPerformance Optimization: Ensure model performance meets or exceeds company and client expectations by identifying areas of improvement, testing new methods, and scaling the systems accordingly.\nEthical AI Development: Advocate for and implement ethical considerations in the development and deployment of AI models, including fairness, transparency, and privacy.\n\nQualifications:\nRequired:\nEducation: Ph.D. or Masters degree in Computer Science, Data Science, Mathematics, Statistics, or related field, or equivalent practical experience.\nExperience:\n8+ years of experience in data science, with at least 2-3 years in a leadership role.\nProven expertise in Generative AI, particularly in areas like content generation, deep learning, and language modeling.\nStrong background in Large Language Models (LLMs) such as GPT, T5, BERT, or similar architectures.\nHands-on experience with Langchain for building NLP workflows, pipelines, and integrating external systems with LLMs.\nHands-on experience of Prompt Engineering, including techniques to refine and optimize outputs for various NLP tasks.\nExpertise in statistical modeling and quantitative analysis, with the ability to apply techniques to solve real-world problems.\n\nPreferred:\nExperience working with transformer models and fine-tuning LLMs for specific tasks.\nExpertise in AI model evaluation and metrics (e.g., BLEU, ROUGE, perplexity).\nBackground in developing AI-driven products from concept to deployment.\nStrong publication record in AI research, particularly in NLP and machine learning.\n\nUsed cases( Any of them)\nAutomated Underwriting.\nCustomer experience enhancement.\nFraud detection.\nPredictive analytics.\nAccelerated claims processing.\nRisk assessment and premium calculation.\nCustomer profiling.\ncustomer segmentation.\nCredit Risk Assessment.\nPersonalised marketing .\nAnti-Money Laundering (AML).\nPersonalized patient care.\nMedical training and simulations.\nMedical Data Analysis.\n\nPlease share your updated resume at renuka.rathi@puresoftware.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'data scientist', 'statistical modelling', 'Predictive Modeling', 'Customer profiling', 'Healthcare Insurance', 'Customer Segmentation', 'Automated Underwriting', 'Insurance Domain', 'Credit Risk Assessment', 'insurance agency', 'Fraud detection', 'Healthcare Domain']",2025-06-12 14:57:28
Senior AI Engineer - Machine Learning,ZS,4 - 9 years,Not Disclosed,['Bengaluru'],"Build, Refine and Use ML Engineering platforms and components.\nScaling machine learning algorithms to work on massive data sets and strict SLAs.\nBuild and orchestrate model pipelines including feature engineering, inferencing and continuous model training.\nImplement ML Ops including model KPI measurements, tracking, model drift & model feedback loop.\nCollaborate with client facing teams to understand business context at a high level and contribute in technical requirement gathering.",,,,"['Hospitality', 'Coding', 'Financial planning', 'Management consulting', 'Agile', 'Healthcare', 'Data structures', 'SAGE', 'SQL']",2025-06-12 14:57:30
"Senior Manager -Data Science, Machine Learning and GenAI Technical",Qualcomm,7 - 12 years,Not Disclosed,['Hyderabad'],"Title : Senior Manager -Data Science, Machine Learning and GenAI Technical Leader\n\nJob Area: Information Technology Group, Information Technology Group > IT Management\n\nGeneral Summary:\n\nExperience in engaging with business and technical stakeholders, understanding complex problem statements, and proposing value-driven Data Science & ML solutionsFunctional & technical leadership in developing data science roadmap and guiding the team with end-to-end delivery (design, development, maintenance, and optimization) of advanced analytical solutions with focus on process standardization and best practicesProactive in reaching out to wide variety of stakeholders for increasing the awareness of Data Science, ML and GenAI capabilities and identifying business value driven opportunitiesAnalyze the market and industry trends in the technology and proactively look for opportunities in bringing the best solutions\n\n'Strong analytical skills with the ability to gather information from several sources and identify fundamental patterns/trends in dataConduct research, design statistical studies and develop solutionsDevelop Story Telling dashboards on analytics to assist business in decision makingImplement deep learning models for structured and unstructured data setsPerform Time Series Analysis and Forecast (ARIMA, Exponential Smoothing, VAR etc.)Implementation experience with identifying, prototyping, developing and delivering GenAI solutions using Industry standard RAG models and frameworksAbility to develop capabilities in Text Analytics ranging from Basic to Advanced (Topic Modelling, NLP)Experience with AWS suite of Data Science and ML toolsDevelops predictive models using Machine Learning algorithms (SVM, Random Forest, Neural Network, Decision Tree, Logistic Regression, K-mean Clustering, Catboost etc.)\n\n'Experience with other cloud platforms (GCP and Azure)Understanding of Data Platforms and Data EngineeringExposure to ChatBot solutions and Automation technologies\n\nMinimum Qualifications:\n7+ years of IT-related work experience with a Bachelor's degree.\nOR\n9+ years of IT-related work experience without a Bachelors degree.\n\n4+ years in a leadership role in projects/programs.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['microsoft azure', 'machine learning', 'deep learning', 'gcp', 'aws', 'time series analysis', 'arima', 'algorithms', 'natural language processing', 'forecasting', 'neural networks', 'random forest', 'svm', 'text analytics', 'decision tree', 'data science', 'clustering', 'logistic regression', 'ml']",2025-06-12 14:57:32
"Applied Scientist II, SFT Machine Learning",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"This role is to solve business problems in Machine Learning for the Seller and Fulfilment Tech (SFT) org.\nThe overarching goal of the team is to enhance ML expertise and fluency within SFT and across IST, championing engineering and operational excellence in ML model development and other related parts of the ML model lifecycle. Some of the key areas which the team owns in this space area:\nSelection Recommendations, Registration improvements, Bad actor detection and prevention\nSelection economics, Inventory recommendation, Delivery Promise Predictions, Seller success.\nWithin the ML space, the scientist would have to solve intrinsically hard problems where neither problem nor solution is well defined. So, the leader should have high focus on building a deep understanding of the ML science space, experimentation methodology, as well as a high focus on embracing external trends, especially applications of GenerativeAI and LLMs.\nA large focus area for the role is to also contribute towards the science and research aspects. The ASII leader applies and extends existing scientific techniques, and invents new ones to address specific customers needs or business problems, at a project level. This should also lead to regular contributions to internal or external peer-reviewed publications that validate novelty 3+ years of building models for business application experience\nPhD, or Masters degree and 4+ years of CS, CE, ML or related field experience\nExperience in patents or publications at top-tier peer-reviewed conferences or journals\nExperience programming in Java, C++, Python or related language\nExperience in any of the following areas: algorithms and data structures, parsing, numerical optimization, data mining, parallel and distributed computing, high-performance computing Experience using Unix/Linux\nExperience in professional software development",,,,"['Unix', 'C++', 'Operational excellence', 'Linux', 'Machine learning', 'Data structures', 'model development', 'high performance computing', 'Data mining', 'Python']",2025-06-12 14:57:34
Machine Learning Senior Engineer,Qualcomm,1 - 6 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nJob Overview\n\nJoin a new and growing team at Qualcomm focused on advancing state-of-the-art in Machine Learning. The team uses Qualcomm chips extensive heterogeneous computing capabilities and engineers them to allow the running of trained neural networks on device without a need for connection to the cloud. Our inference engine is designed to help developers run neural network models trained in a variety of frameworks on Snapdragon platforms at blazing speeds while still sipping the smallest amount of power. See your work directly impact billions of mobile devices around the world & also most advanced Autonomous features for AUTO industry. In this position, you will be responsible for the development of test frameworks for Qualcomm Neural Network (QNN). You will work with neural network frameworks like TensorFlow, Pytorch and develop the validation framework to gauge functionality, performance, precision, and power of QNN. You will work with the latest and greatest DNNs emerging from the research community. You will also have to keep up with the fast pace development happening in the industry and academia to continuously enhance our benchmarking and validation infrastructure from software engineering as well as machine learning standpoint.\n\n\n\nMinimum Qualifications\nExpertise in Developing test cases, automating the tests, test case execution and troubleshooting/analyzing problems\nStrong development skills in Python.\nStrong understanding of Machine learning/Deep learning workflows..\nExperience with at least one machine learning framework like TensorFlow, Pytorch, etc.\nLive and breathe quality software development with excellent analytical and debugging skills.\nExcellent communication skills (verbal, presentation, written)\nAbility to collaborate across a globally diverse team and multiple interests 8.Excellent communication skills (verbal, presentation, written), Strong problem-solving skills, Good time management skills,, excellent analytical and debugging skills, must be an effective team player, and should be self-driven.\n\n\n\n\nPreferred Qualifications\n\nStrong exposure to software testing methodologies and reporting. Experience with CI tools like Jenkins and Data visualization tools like power BI ,Tableau Development experience in Python & C++\n\n\n\nWork Experience\n\n1 to 6 years of relevant work experience in software dev/test development\n\n\n\nEducational\n\nMasters/Bachelor's Computer Science, Computer Engineering, or Electrical Engineering\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'software testing', 'test cases', 'machine learning', 'deep learning', 'test case execution', 'automation testing', 'tensorflow', 'tableau', 'java', 'problem analysis', 'test development', 'pytorch', 'debugging', 'troubleshooting', 'testing methodologies', 'software engineering']",2025-06-12 14:57:37
Senior Machine Learning Engineer,Doublu,5 - 10 years,30-37.5 Lacs P.A.,[],"We are hiring Senior Machine Learning Engineer for an MNC\n\nJob Type : Direct , Fulltime role\n\nLocation : PAN India (Remote)\n\nSenior Machine Learning Engineer\n\nResponsibilities\n\nDevelop, implement, and maintain machine learning models using scikit-learn and SciPy.\nBuild and deploy ML models for production use cases.\nWork with regression models and optimization techniques.\nDevelop and integrate APIs using the Flask framework.\nUtilize GCP services (App Engine, Cloud Tasks, Dataflow, BigQuery, Bigtable, Vertex AI).\nOptimize CI/CD pipelines using Azure DevOps.\nCollaborate with cross-functional teams to deploy scalable ML solutions.\nQualifications\n\nStrong proficiency in Python and core ML libraries Scikit-learn and SciPy.\nHands-on experience with regression models and optimization techniques.\nExperience with Flask for API development and deployment.\nProficiency in GCP services and ML operations on cloud infrastructure.\nExperience with CI/CD tools, especially Azure DevOps.\nSolid understanding of data structures, algorithms, and software engineering practices.\nFamiliarity with Agile methodologies and technical documentation.\nStrong analytical, communication, and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Scipy', 'Bigquery', 'Vertex Ai', 'Machine Learning', 'Scikit-Learn', 'GCP', 'Ml Deployment', 'Mlops', 'Azure Devops', 'Flask']",2025-06-12 14:57:40
MDM Data Scientist,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking an accomplished and visionary Data Scientist/ GenAIdeveloper to join Amgens Enterprise Data Management team. As part of MDM team, you will be responsible for designing, developing, and deploying Generative AI and ML models to power data-driven decisions across business domains. This role is ideal for an AI practitioner who thrives in a collaborative environment and brings a strategic mindset to applying advanced AI techniques to solve real-world problems.To succeed in this role, the candidate must have strong AI/ML, Data Science, GenAI experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have AI/ML, data science and GenAI experience on technologies like (PySpark/PyTorch, TensorFlow, LLM, Autogen, Hugging FaceVectorDB,Embeddings, RAGsetc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nDevelop enterprise-level GenAI applications using LLM frameworks such as Langchain, Autogen, and Hugging Face.\nDesign and develop intelligent pipelines using PySpark, TensorFlow, and PyTorch within Databricks and AWS environments.\nImplement embedding models andmanage VectorStores for retrieval-augmented generation (RAG) solutions.\nIntegrate and leverage MDM platforms like Informatica and Reltio to supply high-quality structured data to ML systems.\nUtilize SQL and Python for data engineering, data wrangling, and pipeline automation.\nBuild scalable APIs and services to serve GenAI models in production.\nLead cross-functional collaboration with data scientists, engineers, and product teams to scope, design, and deploy AI-powered systems.\nEnsure model governance, version control, and auditability aligned with regulatory and compliance expectations.\nBasic Qualifications and Experience:\nMasters degree with 4 - 6 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 6 - 9 years of experience in Business, Engineering, IT or related field OR\nDiploma with 10 - 12 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\n6+ years of experience working in AI/ML or Data Science roles, including designing and implementing GenAI solutions.\nExtensive hands-on experience with LLM frameworks and tools such as Langchain, Autogen, Hugging Face, OpenAI APIs, and embedding models.\nStrong programming background with Python, PySpark, and experience in building scalable solutions using TensorFlow, PyTorch, and SK-Learn.\nProven track record of building and deploying AI/ML applications in cloud environments such as AWS.\nExpertise in developing APIs, automation pipelines, and serving GenAI models using frameworks like Django, FastAPI, and DataBricks.\nSolid experience integrating and managing MDM tools (Informatica/Reltio) and applying data governance best practices.\nGuide the team on development activities and lead the solution discussions\nMust have core technical capabilities in GenAI, Data Science space\nGood-to-Have Skills:\nPrior experience in Data Modeling, ETL development, and data profiling to support AI/ML workflows.\nWorking knowledge of Life Sciences or Pharma industry standards and regulatory considerations.\nProficiency in tools like JIRA and Confluence for Agile delivery and project collaboration.\nFamiliarity with MongoDB, VectorStores, and modern architecture principles for scalable GenAI applications.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nData Science and ML Certification\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['MDM', 'GenAI', 'Langchain', 'PySpark', 'VectorStores', 'Hugging Face', 'LLM', 'Data Science', 'DataBricks', 'SK-Learn', 'AI/ML', 'Autogen', 'PyTorch', 'Django', 'OpenAI APIs', 'FastAPI', 'MongoDB', 'Data Modeling', 'PySpark/PyTorch', 'TensorFlow', 'Python']",2025-06-12 14:57:42
"Senior Python Developer (Machine Learning,Data Analysis,Visualization)",Synechron,3 - 5 years,Not Disclosed,"['Pune', 'Hinjewadi']","Software Requirements\nRequired Skills:\nProficiency in Python (version 3.6+) with experience in data analysis, manipulation, and scripting\nKnowledge of SQL for data extraction, transformation, and database querying\nExperience with data visualization tools such as PowerBI, Tableau, or QlikView\nFamiliarity with AI and Machine Learning frameworks such as TensorFlow, Keras, PyTorch, or equivalent\nHands-on experience in developing, deploying, and optimizing machine learning models\nPreferred Skills:\nExperience with R for data analysis\nFamiliarity with cloud platforms like AWS, Azure, or GCP for deploying AI solutions\nKnowledge of version control systems such as Git\nOverall Responsibilities\nAnalyze, interpret, and visualize large and complex datasets to extract actionable insights\nDesign, develop, and implement machine learning and AI models for predictive and prescriptive analytics\nCollaborate with cross-functional teams to understand business requirements and translate them into data-driven solutions\nCommunicate findings, insights, and recommendations via reports, dashboards, and presentations to stakeholders\nEvaluate and refine models and algorithms to maximize accuracy, efficiency, and impact\nStay informed on emerging AI, Data Science, and analytics trends and incorporate best practices into projects\nSupport automation efforts, optimize data pipelines, and enhance existing analytical workflows\nContribute to organizational learning by sharing knowledge and mentoring team members\nStrategic objectives:\nDrive innovation through the application of AI and machine learning\nEnable data-driven decision-making across business units\nImprove operational efficiencies and business outcomes\nPerformance outcomes:\nAccurate, robust, and scalable AI models\nHigh-quality insights delivered on time and aligned with business needs\nWell-documented solutions and knowledge-sharing artifacts\nTechnical Skills (By Category)\nProgramming Languages (Essential):\nPython (required); experience with R is a plus\nSQL (required); experience with data manipulation and querying\nData Analysis & Visualization Tools (Essential):\nPowerBI, Tableau, or QlikView\nFrameworks & Libraries (Essential):\nTensorFlow, Keras, PyTorch, or similar frameworks for AI/ML development\nData Management & Databases (Essential):\nRelational databases (e.g., MySQL, PostgreSQL, Oracle)\nData extraction and transformation (ETL processes)\nCloud & Deployment (Preferred):\nExperience deploying models on cloud platforms such as AWS, Azure, GCP\nDevelopment & Version Control (Preferred):\nGit for code versioning\nOther Skills:\nStrong statistical knowledge and experience with data preprocessing, feature engineering\nFamiliarity with agile development methodologies\nExperience Requirements\n3 to 5 years of relevant experience in AI, Data Science, or Data Analytics roles\nProven track record applying machine learning techniques to real-world problems\nExperience working with large datasets and scalable data pipelines\nExperience collaborating with cross-functional teams to deliver analytics-driven solutions\nIndustry experience in finance, healthcare, retail, or similar data-rich sectors is preferred\nAlternative pathways:\nCandidates with extensive AI & ML project experience, strong programming skills, and relevant certifications can be considered with slightly varied years of experience\nDay-to-Day Activities\nCollect, clean, and explore large datasets to identify patterns and insights\nDevelop and tune machine learning models to address business problems\nCollaborate with business analysts, data engineers, and product owners to align technical solutions with organizational goals\nDocument methodologies, code, and analytical findings to ensure reproducibility and knowledge sharing\nCreate dashboards, visualizations, and reports to communicate insights effectively\nEvaluate model performance regularly and optimize models for accuracy and efficiency\nParticipate in team meetings, project planning, and review sessions\nKeep abreast of advancements in AI/ML technologies, tools, and best practices\nQualifications\nBachelors degree in Computer Science, Data Science, Statistics, or related field\nMasters degree or higher in AI, Data Science, or related disciplines is a plus\nProfessional certifications in AI/ML (e.g., TensorFlow Developer, AWS Machine Learning Specialty) are advantageous\nWilling to learn new tools and stay updated with emerging AI trends\nAbility to work independently and collaborate effectively in a dynamic environment\nProfessional Competencies\nAnalytical and problem-solving mindset with a focus on actionable insights\nExcellent verbal and written communication skills for diverse audiences\nStrong interpersonal skills and stakeholder management\nAdaptability to fast-changing technology landscapes\nGrowth mindset with continuous learning enthusiasm\nOrganizational skills to handle multiple projects and priorities simultaneously\nInnovation-driven approach and proactive problem resolution",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'PostgreSQL', 'MySQL', 'Data Analysis', 'Data Visualization', 'Oracle', 'ETL', 'Machine Learning']",2025-06-12 14:57:45
Senior Machine Learning Engineer - NLP,Avalara Technologies,5 - 8 years,Not Disclosed,[],"What You'll Do\nWe are looking for experienced Machine Learning Engineers with a background in software development and a deep enthusiasm for solving complex problems. You will lead a dynamic team dedicated to designing and implementing a large language model framework to power diverse applications across Avalara. Your responsibilities will span the entire development lifecycle, including conceptualization, prototyping and delivery of the LLM platform features.\nYou will have a blend of technical skills in the fields of AI & Machine Learning especially with LLMs and a deep-seated understanding of software development practices where you'll work with a team to ensure our systems are scalable, performant and accurate. You will be reporting to Senior Manager, AI/ML.",,,,"['Machine Learning', 'LLMs', 'Prometheus', 'Grafana', 'NLP', 'Docker', 'Terraform', 'MLFlow', 'Postgres', 'AWS', 'GitLab', 'Python', 'Kubernetes']",2025-06-12 14:57:47
Senior Machine Learning Engineer,Bebo Technologies,3 - 8 years,Not Disclosed,"['Chandigarh', 'Pune', 'Delhi / NCR']","3+ years of experience in software engineering and ML development.\nStrong proficiency in Python and ML libraries such as Scikit-learn, TensorFlow, or PyTorch.\nExperience building and evaluating models, along with data preprocessing and feature engineering.\nProficiency in REST APIs, Docker, Git, and CI/CD tools.\nSolid foundation in software engineering principles, including data structures, algorithms, and design patterns.\nHands-on experience with MLOps platforms (e.g., MLflow, TFX, Airflow, Kubeflow).\nExposure to NLP, large language models (LLMs), or computer vision projects.\nExperience with cloud platforms (AWS, GCP, Azure) and managed ML services.\nContributions to open-source ML libraries or participation in ML competitions (e.g., Kaggle, DrivenData) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Tensorflow', 'Machine Learning', 'Pytorch', 'Keras', 'Scikit-Learn', 'Python']",2025-06-12 14:57:49
Data Engineer _Technology Lead,Broadridge,6 - 10 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nAnalyzes and solve problems using technical experience, judgment and precedents\nProvides informal guidance to new team members\nExplains complex information to others in straightforward situations\n1. Data Engineering and Modelling:\nDesign & Develop Scalable Data Pipelines: Leverage AWS technologies to design, develop, and manage end-to-end data pipelines with services like .",,,,"['Star Schema', 'Snowflake', 'AWS', 'Apache Airflow']",2025-06-12 14:57:52
Data Scientist,Ltimindtree,8 - 13 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']",We are looking for an experienced AI ML Developers experience in data science specializing in machine learning python statistical modelling and big data technologies pyspark sql.\n\nThe ideal candidate will have a strong background in developing and deploying machine learning models optimizing ML pipelines and handling largescale structured and unstructured data to drive business impact.\n\nDeep understanding of supervised and unsupervised learning including regression classification Multiclass classification clustering and NLP Proficiency in statistical analysis AB testing and causal inference techniques Experience with model deployment and MLOps in cloud environments AWS GCP \n\nKey Responsibilities\n\nDevelop and deploy machine learning models and predictive analytics solutions for business impact\nWork with largescale structured and unstructured data to extract insights and build scalable models\nDesign implement and optimize ML pipelines for realtime and batch processing\nCollaborate with engineering product and business stakeholders to translate business problems into data science solutions\nApply statistical modeling AB testing and causal inference techniques to evaluate business performance\nApply machine learning and statistical techniques for audience segmentation helping to identify patterns and optimise business strategies\nDrive research and innovation by staying updated with cuttingedge MLAI advancements and incorporating them into our solutions\nOptimize data science models for performance scalability and interpretability in production environments\nMentor junior data scientists and contribute to best practices in data science and engineering,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Aiml', 'Ab Testing']",2025-06-12 14:57:54
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Bengaluru'],"Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 14:57:57
Data Scientist,Ltimindtree,7 - 12 years,Not Disclosed,['Hyderabad'],Data Scientist\n\nJob Description\n\nResponsibilities\n\nWork with team members across multiple disciplines to understand the data behind product features user behaviors the security landscape and our goals\nAnalyze data from several large sources then automate solutions using scheduled processes models and alerts\nWork with partners to design and improve metrics that guide our decisions for the product\nDetect patterns associated with fraudulent accounts and anomalous behavior\nSolve scientific problems and create new methods independently\nTranslate requirements and security questions into data insights\nSet up alerting mechanisms so our leadership is always aware of the security posture\n\nQualifications\n\nPostgraduate degree with specialization in machine learning artificial intelligence statistics or related fields or 2 years of equivalent work experience in applied machine learning and analytics\nExperience with SQL Snowflake and NoSQL databases\nProficiency in Python programming\nFamiliarity with statistics modeling and data visualization\n\nExperience\n\nExperience building statistical and machine learning models applying techniques such as regression classification clustering and anomaly detection Time series and Classical ML modeling\nFamiliarity with Snowflake SQL\nFamiliarity with cloud platforms such as AWS\nSome experience to software development or data engineering\nAnalyze business problems or research questions identify relevant data points and extract meaningful insights,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Snowflake Sql', 'AWS']",2025-06-12 14:57:59
Data Scientist,Ltimindtree,8 - 13 years,19-34 Lacs P.A.,"['Pune', 'Bengaluru', 'Mumbai (All Areas)']",10 years of experience in Data ScienceML domain\nShould have experience in Python and libraries like pandas numpy scikitlearn etc\nHave worked on building ML models and integrating it with application end to end\nHave knowledge on Recommender engines and the ML models running behind it like ALS and LightFM\nHave experience in Azure Machine Learning and Azure Services\nHave experience in deploying models in cloud environment and exposing it as an API\nGood communication and presentation skill\nAbility to deliver ML projects as an individual contributor,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Data Science', 'Ml']",2025-06-12 14:58:01
Engineer,Qualcomm,10 - 15 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nJob Overview:\n\nThe Qualcomm Cloud Computing team is developing hardware and software for Machine Learning solutions spanning the data center, edge, infrastructure, automotive market. We are seeking ambitious, bright, and innovative engineers with experience in machine learning framework development. Job activities span the whole product life cycle from early design to commercial deployment. The environment is fast-paced and requires cross-functional interaction daily so good communication, planning and execution skills are a must.\n\nWe are seeking a highly skilled and motivated Language Model Engineer to join our team. The primary role of the engineer will be to train Large Language Models (LLMs) from scratch and fine-tune existing LLMs on various datasets using state-of-the-art techniques.\n\nResponsibilities:\n\n\n\nModel Training and Fine-tuning: Train LLMs from scratch using various datasets. Fine-tune pre-trained models on specific tasks or datasets to improve performance. Implement state-of-the-art LLM training techniques such as Reinforcement Learning from Human Feedback (RLHF), ZeRO (Zero Redundancy Optimizer), Speculative Sampling, and other speculative techniques.\n\n\nData Management: Handle large datasets effectively. Ensure data quality and integrity. Implement data cleaning and preprocessing techniques. Hands-on with EDA is a plus.\n\n\nModel Evaluation: Evaluate model performance using appropriate metrics. Understand the trade-offs between different evaluation metrics.\n\n\nLLM metrics: Sound understanding of various LLM metrics like MMLU, Rouge, BLEU, Perplexity etc.\n\nAWQ: Understanding of Quantization is a plus. Knowledge on QAT will be a plus.\n\n\nResearch and Development: Stay updated with the latest research in NLP and LLMs. Implement state-of-the-art techniques and contribute to research efforts.\n\n\nCollaboration: Work closely with other teams to understand requirements and implement solutions.\n\n\nRequired Skills and Experience:\n\n\nDeep Learning Frameworks: Hands-on experience with PyTorch at a granular level. Familiarity with tensor operations, automatic differentiation, and GPU acceleration in PyTorch.\n\n\nNLP and LLMs: Strong understanding of Natural Language Processing (NLP) and experience working with LLMs.\n\n\nProgramming: Proficiency in Python and experience with software development best practices.\n\n\nData Handling: Experience working with large datasets. Familiarity with data version control tools is a plus.\n\n\nEducation: A degree in Computer Science, Machine Learning, AI, or related field. Advanced degree is a plus.\n\n\nCommunication: Excellent written and verbal communication skills.\n\n\nWork experience : Open, 2- 10 years of relevant experience.\n\n\nPreferred\n\nSkills:\n\n\n\nOptimization: Knowledge of optimization techniques for training large models.\n\n\nNeural Architecture Search (NAS): Experience with NAS techniques for optimizing model architectures is a plus. Hands-on experience with CUDA, CUDNN is a plus.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cuda', 'python', 'natural language processing', 'pytorch', 'machine learning algorithms', 'nas', 'eda', 'version control', 'sampling', 'machine learning', 'deep learning', 'data center', 'computer science', 'product life cycle', 'research and development', 'software engineering']",2025-06-12 14:58:04
Full Stack Data Scientist,Vimo Getinsured,2 - 7 years,Not Disclosed,['Gurugram( Sector 61 Gurgaon )'],"About the Role\nAs a Data Science Engineer, you will need strong technical skills in data modeling, machine learning, data engineering, and software development. You will have the ability to conduct literature reviews and critically evaluate research papers to identify applicable techniques. Additionally, you should be able to design and implement efficient and scalable data processing pipelines, perform exploratory data analysis, and collaborate with other teams to integrate data science models into production systems. Passion for conversational AI and a desire to solve some of the most complex problems in the Natural Language Processing space are essential. You will work on highly scalable, stable, and automated deployments, aiming for high performance. Taking on the challenge of building and scaling a truly remarkable AI platform to impact the lives of millions of customers will be part of your responsibilities. Working in a challenging yet enjoyable environment, where learning new things is the norm, you should think of solutions beyond boundaries. You should also drive outcomes with full ownership, deeply believe in customer obsession, and thrive in a fast-paced environment of learning and innovation.\nYou will work in a challenging, consumer-facing problem space, where you can make an immediate impact. You will get to work with the latest technologies, learn to use new tools and get the opportunity to have your say in the final product. Youll work alongside a great team in an open, collaborative environment. We are part of Vimo, a well-funded, stable mid-size company with excellent salaries, medical/dental/vision coverage, and perks. Vimo is an Equal Opportunity Employer.",,,,"['python', 'Langchain', 'Neural Networks', 'LLM', 'Linux', 'Data Structures', 'Natural Language Processing', 'Jupyter Notebook', 'Machine Learning', 'Deep Learning', 'Numpy', 'Data Science', 'pandas', 'Nltk', 'Langgraph', 'Transformers', 'BERT', 'langsmith']",2025-06-12 14:58:06
Assistant Data Scientist,Rocket Software,0 - 1 years,Not Disclosed,['Pune'],"Face to Face interview in Pune . Please apply only if you are available for a Face to Face interview .\n\nJob highlights\n\nRequired Qualifications . 0 -2 years of relevant industry experience or fresh graduates are welcome to apply.\nBasic experience or understanding in applying Data Science methodologies to extract, process, and transform data from multiple sources.\nPreferred Qualifications . Bachelors degree in Data Science , AI, Statistics ,Computer Science, Economics, or a directly related field.\n\nEssential Duties and Responsibilities\n\nAssist in developing, fine-tuning, and deploying machine learning models.\nAid in consulting with key internal and external stakeholders to understand and frame model requirements and potential applications.\nParticipate in the development of sound analytic plans based on available data sources, business partner needs, and required timelines.\nWork with software engineers in integrating trained models into end-user applications.\nHelp manage deliverables across multiple projects in a deadline-driven environment.\nPresent results, insights, and recommendations to both technical and non-technical stakeholders.\n\nRequired Qualifications\n\n0 -2 years of relevant industry experience or fresh graduates are welcome to apply.\nGood knowledge of Python and Linux, familiarity with ML frameworks, and a willingness to learn.\nDemonstrated problem-solving abilities and creative thinking.\nBasic experience or understanding in applying Data Science methodologies to extract, process, and transform data from multiple sources.\nExcellent communication and interpersonal skills.\nMust be comfortable working in a team-oriented environment.\n\nPreferred Qualifications\n\nBachelor's degree in Statistics, Computer Science, Economics, or a directly related field.\nMasters degree or current enrollment in a Masters program in Statistics, Computer Science, Mathematics, Economics, or directly related fields is a plus.\nDemonstrated passion for continued learning and innovation.\nAs a Data Science Assistant, we expect not just skills and qualifications, but also an enthusiasm for learning and growing within our team. We value those who are adaptable, innovative, and ready to take on challenges in a fast-paced work environment.\n\nDiversity, Inclusion & Equity\n\nAt Rocket we are committed to an inclusive workplace environment, where every Rocketeer can thrive by bringing their full selves to work. Being a Rocketeer means you are part of our movement to continually drive inclusivity, diversity and equity in our workforce.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'NLP', 'Natural Language Processing', 'Artificial Intelligence', 'Machine Learning', 'Python']",2025-06-12 14:58:08
Technical Specialist - Data Scientist,Fidelity International,8 - 9 years,Not Disclosed,['Gurugram'],"Application Deadline: 21 June 2025\nTitle Senior Analyst- Data Scientist\nDepartment Data Value\nLocation Gurgaon\nReports To Suman Kaur\nLevel 3\nWe re proud to have been helping our clients build better financial futures for over 50 years. How have we achieved this? By working together - and supporting each other - all over the world. So, join our Data Value team and feel like you re part of something bigger.\nAbout your team\nData Value team drives the renewed focus of extracting value from Fidelity s data for business and client insights and working as one voice with the business, technology, and data teams. The team s vision is to create measurable business impact by leveraging technology and utilising the skills to generate valuable insights and streamline engagements. The Data Science function within Data Value supports Fidelity International s Sales, Marketing, Propositions, Risk, Finance, Customer Service and HR teams across the globe. The key objectives of the function are:\nTo develop deep customer insights for our businesses helping them segment and target customers more effectively\nTo develop a fact-based understanding of sales trends and identify actionable sales growth opportunities for each of our sales channels\nTo understand customer preferences in terms of products, service attributes and marketing activity to help refine each of these\nTo help develop new services lines e.g. develop customer analytics for key IFAs, DC Clients, Individual clients etc.\nTo develop market and competitive intelligence in our key markets to help shape our business planning in those markets\nThe function works directly with business heads and other senior stakeholder s stakeholders to identify areas of analytics, define problem statements and develop key insights.\nAbout your role\nYou will be expected to take a leading role in developing the Data Science and Advanced Analytics solutions for our business. This will involve:\nEngaging with the key stakeholders to understand Fidelity s sales, marketing, client services and propositions context\nImplement advanced analytics solutions on On-Premises/Cloud platforms, develop proof-of-concepts and engage with internal and external ecosystem to progress the proof of concepts to production.\nEngaging and collaborating with different other internal teams like Data engineering, DevOps, technology team etc for development of new tools, capabilities, and solutions.\nMaximize Adoption of Cloud Based advanced analytics solutions: Build out sandbox analytics environments using Snowflake, AWS, Adobe, Salesforce.\nAbout you\nKey Responsibilities\nDeveloping and Delivering Data Science solutions for business (40%)\nPartner with internal (FIL teams) & external ecosystem to design and deliver advanced analytics enabled Data Science solutions\nCreate advanced analytics solution on quantitative and text data using Artificial Intelligence, Machine Learning and NLP techniques.\nCreate compelling visualisations that enable the smooth consumption of predictions and insights for customer benefit\n. Stakeholder Management (30%)\nWorks with channel heads/stakeholders and other sponsors understand the business problem and translate it into appropriate analytics solution.\nEngages with key stakeholders for smooth execution, delivery, and implementation of solutions\nAdoption of Cloud enabled Data Science solutions: (20%)\nMaximize Adoption of Cloud Based advanced analytics solution\nBuild out sandbox analytics environments using Snowflake, AWS, Adobe, Salesforce\nDeploy solutions in productions while adhering to best practices involving Model Explainability, MLOps, Feature Stores, Model Management, Responsible AI etc\nCollaboration and Ownership (10%)\nSharing of knowledge, best practices with the team including coaching or training in some of deep learning/machine learning methodologies. Provides mentoring, coaching, and consulting advice and guidance to staff, e.g. analytic methodologies, data recommendations\nTakes complete independent ownership of the projects and the initiatives in the team with the minimal support\nExperience and Qualifications Required\nQualifications:\nEngineer from IIT/Master s in field related to Data Science/Economics/Mathematics (Tie1 Institutions like ISI, Delhi School of Economics)/M.B.A from tier 1 institutions\nMust have Skills & Experience Required:\nOverall, 8+ years of experience in Data Science and Analytics\n5+ years of hands-on experience in - Statistical Modelling /Machine Learning Techniques/Natural Language Processing/Deep Learning\n5+ years of experience in Python/Machine Learning/Deep Learning\nExcellent problem-solving skills\nShould be able to run analytics applications such as Python, SAS and interpret statistical results\nImplementation of models with clear measurable outcomes\nGood to have Skills & Experience Required:\nAbility to engage in discussion with senior stakeholders on defining business problems, designing analyses projects, and articulating analytical insights to stakeholders.\nExperience on SPARK/Hadoop/Big Data Platforms is a plus\nExperience with unstructured data and big data\nExperience with secondary data and knowledge of primary market research is a plus.\nAbility to independently own and manage the projects with minimal support.\nExcellent analytical skills and a strong sense for structure and logic\nAbility to develop, test and validate hypotheses.\nFeel rewarded",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAS', 'Senior Analyst', 'Consulting', 'Machine learning', 'Business planning', 'Competitive intelligence', 'Customer service', 'Adobe', 'Stakeholder management', 'Salesforce']",2025-06-12 14:58:10
Graph Engineer- Data Science,HARMAN,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Description\nIntroduction: Digital Transformation Solutions (DTS)\n.\nExtensive experience in defining, developing, and implementing security software, ideally with a strong embedded firmware development background\nAbout the Role\nThis position offers an opportunity to work in a globally distributed team where you will get a unique opportunity of personal development in a multi-cultural environment. You will also get a challenging environment to develop expertise in the technologies useful in the industry.",,,,"['Computer science', 'Product quality', 'UML', 'XML', 'Relationship', 'Javascript', 'HTML', 'Oracle', 'Automotive', 'Python']",2025-06-12 14:58:12
"Associate Director, Data Science/Software Engineering",ATT Communication Services,10 - 15 years,Not Disclosed,['Bengaluru'],"Associate Director, Data Science/Software Engineering:\nAT&T is one of the leading service providers in the telecommunication sector and propelling it into the data and AI driven era is powered by CDO (Chief Data Office) . CDO is empowering AT&T, through execution, self-service, and as a data and AI center of excellence, to unlock transformative insights and actions that drive value for the company and its customers.\nEmployees in CDO imagine, innovate, and unlock data & AI driven insights and actions that create value for our customers and the enterprise. Part of the work, we govern data collection and use, mitigate for potential bias in machine learning models, and encourage an enterprise culture of responsible AI.\nAT&T s Chief Data Office (CDO) is harnessing data and making AT&T s data assets and ground-breaking AI functionality accessible to employees across the firm. In addition, our talented employees are a significant component that contributes to AT&T s place as the U.S. company with the sixth most AI-related patents. CDO also maintains academic and tech partnerships to cultivate the next generation of experts in statistics and machine learning, statistical computing, data visualization, text mining, time series modelling, data stream and database management, data quality and anomaly detection, data privacy, and more.\nWe are looking for an accomplished and visionary professional for the role of Associate Director, Data Science/Software Engineering to join our team and lead the development of cutting-edge software solutions. This is a hands-on leadership position that requires the fine balance of supervising and leading people while providing significant technical contributions to the projects you will be responsible for. As a key technical leader, you will leverage your expertise in full-stack development, DevOps best practices, Data analysis, AI/ML and Generative AI to lead your team in creating scalable, reliable, and efficient systems.\nThis role demands a strategic thinker and hands-on contributor who can work across multiple teams, drive innovation, and ensure technical excellence. You will be instrumental in shaping the technical roadmap, mentoring teams, and delivering transformative solutions that align with business objectives.\nKey Responsibilities:\nTechnical Leadership:\nDefine and drive the technical vision and architecture for scalable, resilient, and secure full-stack applications utilizing data powered insights.\nLead end-to-end software development projects from concept to deployment and maintenance.\nCollaborate with cross-functional teams to translate business requirements into technical solutions.\nServe as a mentor and technical advisor to engineering teams, fostering a culture of innovation and excellence.\nFull-Stack Development:\nDesign and implement scalable and high-performance web applications using modern front-end and back-end frameworks (e.g., React, Angular, Node.js, Python, Java).\nDevelop modular and reusable APIs (RESTful or GraphQL) with an emphasis on maintainability and performance.\nEnsure seamless integration of front-end and back-end systems while maintaining best practices for UI/UX design.\nOptimize database structures and queries for both relational (e.g., MySQL, PostgreSQL) and non-relational (e.g., MongoDB, DynamoDB) databases.\nDevOps and Automation:\nArchitect and implement CI/CD pipelines to streamline build, test, and deployment processes.\nEnsure seamless deployment and scalability of applications through containerization tools (e.g., Docker) and orchestration platforms (e.g., Kubernetes).\nLeverage infrastructure-as-code solutions (e.g., Terraform, Ansible) to automate infrastructure provisioning and management.\nMonitor application performance, troubleshoot issues, and ensure high availability through tools like Prometheus, Grafana, or New Relic.\nShell Scripting and Automation:\nDevelop and maintain shell scripts to automate routine tasks, system monitoring, and application deployments.\nDebug and troubleshoot production issues using scripting techniques to ensure minimal downtime.\nEnhance system efficiency by automating log analysis, error detection, and reporting.\nStrategic Contribution:\nCollaborate with stakeholders to align technical priorities with business goals.\nEvaluate emerging technologies and tools to recommend and implement solutions that advance the organization s technical capabilities.\nEstablish and enforce software engineering best practices, ensuring robust security, scalability, and maintainability.\nQualifications:\nEducation:\nBachelor s or Master s degree in Computer Science, Software Engineering, or a related field. A Ph.D. is a plus.\nExperience:\n13+ years of experience in software engineering, including hands-on experience with full-stack development and DevOps practices.\nProven track record of delivering large-scale, high-impact software solutions in a leadership capacity.\nTechnical Expertise:\nAdvanced proficiency in front-end frameworks (React, Angular, or Vue.js) and back-end technologies (Node.js, Python, Java, Go, etc.).\nStrong experience with DevOps tools (Jenkins, GitLab CI/CD, Docker, Kubernetes).\nDeep understanding of cloud platforms (AWS, Azure, GCP), including architecture and deployment strategies.\nSolid grasp of database technologies (SQL and NoSQL) and optimization techniques.\nProficiency in writing, debugging, and maintaining shell scripts for automation and system monitoring.\nStrong knowledge of microservices architecture, API gateways, and distributed systems.\nSoft Skills:\nExceptional problem-solving and critical-thinking abilities.\nStrong leadership and mentoring skills, with the ability to inspire and guide teams.\nExcellent communication skills, both written and verbal, to collaborate effectively with technical and non-technical stakeholders.\nStrategic mindset, capable of balancing technical depth with business impact.\nPreferred Qualifications:\nExperience with serverless computing frameworks (e.g., AWS Lambda).\nCertifications in cloud platforms (e.g., AWS Certified Solutions Architect, Azure DevOps Engineer Expert).\nKnowledge of security best practices in software development and DevOps.\n#DataEngineering\nLocation:\nIND:KA:Bengaluru / Innovator Building, Itpb, Whitefield Rd - Adm: Intl Tech Park, Innovator Bldg\nJob ID R-66889 Date posted 05/14/2025",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data analysis', 'Front end', 'Postgresql', 'MySQL', 'Shell scripting', 'Telecommunication', 'SQL', 'Python']",2025-06-12 14:58:14
Data Engineer,HARMAN,5 - 10 years,Not Disclosed,['Bengaluru'],"-Strong analytical thinking and problem-solving skills, with the ability to translate complex data into actionable insights\n-Excellent communication skills, with the ability to effectively convey complex findings to both technical and non-technical stakeholders.\nCandidate to work form SRIB Bangalore with 3 days working from office is mandatory\n  What You Will Do",,,,"['Digital media', 'CTV', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Automotive', 'Python']",2025-06-12 14:58:17
Data Scientist For DMAI,Prodapt Solutions,2 - 5 years,Not Disclosed,['Chennai'],"Overview\n\nThe Senior Data Science Engineer will leverage advanced data science techniques to solve complex business problems, guide decision-making processes, and mentor junior team members. This role requires a combination of technical expertise in data analysis, machine learning, and project management skills.\n\nResponsibilities\n\n Data Analysis and Modeling Analyze large-scale telecom datasets to extract actionable insights and build predictive models for network optimization and customer retention.\n Conduct statistical analyses  to validate models and ensure their effectiveness.\n Machine Learning Development Design and implement machine learning algorithms for fraud detection, churn prediction, and network failure analysis.\n Telecom-Specific Analytics Apply domain knowledge to improve customer experience by analyzing usage patterns, optimizing services, and predicting customer lifetime value.\n ETL Processes Develop robust pipelines for extracting, transforming, and loading telecom data from diverse sources.\n Collaboration Work closely with data scientists, software engineers, and telecom experts to deploy solutions that enhance operational efficiency.\n Data Governance :  Ensure data integrity, privacy, security and compliance with industry standards\n\n\nAdvanced degree in Data Science, Statistics, Computer Science, or a related field.\nExtensive experience in data science roles with a strong focus on machine learning and statistical modeling.\nProficiency in programming languages such as Python or R and strong SQL skills.\nFamiliarity with big data technologies (e.g., Hadoop, Spark) is advantageous.\nExpertise in cloud platforms such as AWS or Azure.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'data analysis', 'machine learning', 'sql', 'statistical modeling', 'algorithms', 'python', 'big data technologies', 'microsoft azure', 'cloud platforms', 'r', 'data science', 'spark', 'data governance', 'hadoop', 'aws', 'etl', 'machine learning algorithms', 'statistics']",2025-06-12 14:58:19
Data Scientist,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nThe Data Scientist is responsible for developing and implementing AI-driven solutions to enhance cybersecurity measures within the organization. This role involves leveraging data science techniques to analyze security data, detect threats, and automate security processes. The Data Scientist will work closely with cybersecurity teams to identify data-driven automation opportunities, strengthening the organizations security posture.\nRoles & Responsibilities:\nDevelop analytics to address security concerns, enhancements, and capabilities to improve the organization's security posture.\nCollaborate with Data Engineers to translate security-focused algorithms into effective solutions.\nWork in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.\nPerform exploratory and targeted data analyses using descriptive statistics and other methods to identify security patterns and anomalies.\nDesign and implement security-focused analytics pipelines leveraging MLOps practices.\nCollaborate with data engineers on data quality assessment, data cleansing, and the development of security-related data pipelines.\nContribute to data engineering efforts to refine data infrastructure and ensure scalable, efficient security analytics.\nGenerate reports, annotated code, and other projects artifacts to document, archive, and communicate your work and outcomes.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python) OR\nBachelors degree and 3 to 5 years of experience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python) OR\nDiploma and 7 to 9 years of experience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python)\nPreferred Qualifications:\nExperience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python)\nDemonstrated skill in the use of applied analytics, descriptive statistics, feature extraction and predictive analytics on industrial datasets\nStrong foundation in machine learning algorithms and techniques\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification\nGood-to-Have Skills:\nProficiency in Python and relevant ML libraries (e.g., TensorFlow, PyTorch, Scikit-learn)\nOutstanding analytical and problem-solving skills; Ability to learn quickly; Excellent communication and interpersonal skills\nExperience with data engineering and pipeline development\nExperience in analyzing time-series data for forecasting and trend analysis\nExperience with AWS, Azure, or Google Cloud\nExperience with Databricks platform for data analytics and MLOps\nExperience with Generative AI models (e.g., GPT, DALLE, Stable Diffusion) and their applications in cybersecurity and data analysis\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAny AWS Developer certification (preferred)\nAny Python and ML certification (preferred)\nAny SAFe Agile certification (preferred)\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'R', 'PyTorch', 'SAS', 'predictive analytics', 'Scikit-learn', 'SPSS', 'machine learning', 'data engineering', 'Python', 'TensorFlow']",2025-06-12 14:58:22
Data Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon strives to be the worlds most customer-centric company, where customers can research and purchase anything they might want online\nWe set big goals and are looking for people who can help us reach and exceed them\nThe CPT Data Engineering & Analytics (DEA) team builds and maintains critical data infrastructure that enhances seller experience and protects the privacy of Amazon business partners throughout their lifecycle\nWe are looking for a strong Data Engineer to join our team\n\nThe Data Engineer I will work with well-defined requirements to develop and maintain data pipelines that help internal teams gather required insights for business decisions timely and accurately\nYou will collaborate with a team of Data Scientists, Business Analysts and other Engineers to build solutions that reduce investigation defects and assess the health of our Operations business while ensuring data quality and regulatory compliance\n\nThe ideal candidate must be passionate about building reliable data infrastructure, detail-oriented, and driven to help protect Amazons customers and business partners\nThey will be an individual contributor who works effectively with guidance from senior team members to successfully implement data solutions\nThe candidate must be proficient in SQL and at least one scripting language (e\ng\nPython, Perl, Scala), with strong understanding of data management fundamentals and distributed systems concepts\n\n\nBuild and optimize physical data models and data pipelines for simple datasets\nWrite secure, stable, testable, maintainable code with minimal defects\nTroubleshoot existing datasets and maintain data quality\nParticipate in team design, scoping, and prioritization discussions\nDocument solutions to ensure ease of use and maintainability\nHandle data in accordance with Amazon policies and security requirements Masters degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent\n3+ years of data engineering experience\nExperience with SQL\nExperience with data modeling, warehousing and building ETL pipelines\nKnowledge of distributed systems concepts from data storage and compute perspective\nAbility to work effectively in a team environment Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nFamiliarity with big data technologies (Hadoop, Spark, etc\n)\nKnowledge of data security and privacy best practices\nStrong problem-solving and analytical skills\nExcellent written and verbal communication skills",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'data security', 'Perl', 'Data quality', 'Distribution system', 'Analytics', 'SQL', 'Python']",2025-06-12 14:58:24
Data Analyst-Having Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Analyst\n\nLocation: Bangalore\nExperience: 8 - 15 Yrs\nType: Full-time\n\nRole Overview\n\nWe are seeking a skilled Data Analyst to support our platform powering operational intelligence across airports and similar sectors. The ideal candidate will have experience working with time-series datasets and operational information to uncover trends, anomalies, and actionable insights. This role will work closely with data engineers, ML teams, and domain experts to turn raw data into meaningful intelligence for business and operations stakeholders.\n\nKey Responsibilities\n\nAnalyze time-series and sensor data from various sources\nDevelop and maintain dashboards, reports, and visualizations to communicate key metrics and trends.\nCorrelate data from multiple systems (vision, weather, flight schedules, etc) to provide holistic insights.\nCollaborate with AI/ML teams to support model validation and interpret AI-driven alerts (e.g., anomalies, intrusion detection).\nPrepare and clean datasets for analysis and modeling; ensure data quality and consistency.\nWork with stakeholders to understand reporting needs and deliver business-oriented outputs.\n\n\nQualifications & Required Skills\n\nBachelors or Masters degree in Data Science, Statistics, Computer Science, Engineering, or a related field.\n5+ years of experience in a data analyst role, ideally in a technical/industrial domain.\nStrong SQL skills and proficiency with BI/reporting tools (e.g., Power BI, Tableau, Grafana).\nHands-on experience analyzing structured and semi-structured data (JSON, CSV, time-series).\nProficiency in Python or R for data manipulation and exploratory analysis.\nUnderstanding of time-series databases or streaming data (e.g., InfluxDB, Kafka, Kinesis).\nSolid grasp of statistical analysis and anomaly detection methods.\nExperience working with data from industrial systems or large-scale physical infrastructure.\n\n\nGood-to-Have Skills\n\nDomain experience in airports, smart infrastructure, transportation, or logistics.\nFamiliarity with data platforms (Snowflake, BigQuery, Custom-built using open-source).\nExposure to tools like Airflow, Jupyter Notebooks and data quality frameworks.\nBasic understanding of AI/ML workflows and data preparation requirements.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Kafka', 'SQL', 'airports', 'InfluxDB', 'Airflow', 'structured Data', 'time-series', 'JSON', 'Tableau', 'Grafana', 'R', 'AI/ML', 'Kinesis', 'Snowflake', 'time-series databases', 'Data Preparation', 'Python', 'smart infrastructure', 'BigQuery', 'streaming data', 'Power BI', 'CSV', 'transportation', 'logistic', 'reporting tools']",2025-06-12 14:58:27
Data Scientist,Neoware Technology Solutions,3 - 7 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Scientist - Neoware Technology Solutions Private Limited\nRequirements\nDevelop predictive and prescriptive models to optimize business outcomes and drive growth.\nDesign and build Generative AI solutions to enhance business capabilities.\nWork with leading cloud platforms such as AWS, Azure, or GCP.\nProcess and analyze unstructured data using NLP and Computer Vision techniques.\nLead data-driven initiatives and collaborating with stakeholders to understand business needs and develop strategic solutions.\nConduct exploratory data analysis (EDA) to identify patterns, trends and insights in large, complex datasets.\nMentor and coach junior team members, providing technical guidance and fostering a culture of continuous learning and innovation.\nResponsibilities\nB.E. / Masters in Computer Science, Statistics, Applied Mathematics, Economics or a related quantitative field.\n3-7years of experience in data science, with a proven track record of delivering impactful business solutions.\nStrong proficiency in Python/R and SQL; experience with cloud platforms (AWS, Azure or GCP) is a plus.\nSolid understanding of machine learning techniques (classification, regression, clustering) and statistical methods.\nExcellent communication skills, with the ability to convey complex concepts to diverse audiences.\nStrong problem-solving abilities and capability to work both independently and in a team environment\nChennai / Bangalore / Mumbai\nPrincipal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Computer vision', 'Data analysis', 'GCP', 'Machine learning', 'Cloud', 'Business solutions', 'AWS', 'SQL', 'Python']",2025-06-12 14:58:29
Data Scientist,Swits Digital,5 - 12 years,Not Disclosed,['Chennai'],"Job Title: Data Scientist\nLocation: Chennai\nExperience: 5-12 Years\nJob Summary:\nWe are seeking a highly analytical and results-driven Data Scientist with a strong background in statistics , machine learning , and data science , combined with domain knowledge in mechanical engineering and cost analysis . The ideal candidate will have experience working with Google Cloud Platform (GCP) and will play a key role in transforming engineering and operational data into actionable insights to drive business decisions.\nRequired Skills & Experience:\nStrong knowledge of statistics , machine learning , and data science principles\nHands-on experience with Google Cloud Platform (GCP) , especially BigQuery , Vertex AI , and Cloud Functions\nProficiency in Python or R for data analysis and modeling\nSolid understanding of mechanical engineering concepts and their application in data analysis\nExperience with cost modeling , cost-benefit analysis , or operational performance analytics\nExcellent problem-solving , analytical thinking , and communication skills\nAbility to work with large datasets and create clear, actionable insights",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'data science', 'GCP', 'Analytical', 'Machine learning', 'Cost benefit analysis', 'Operations', 'Mechanical engineering', 'Analytics', 'Python']",2025-06-12 14:58:31
Data Engineer III,Expedia Group,5 - 10 years,Not Disclosed,['Bengaluru'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nData Engineer III\nIntroduction to the Team\nExpedia Technology teams partner with our Product teams to create innovative products, services, and tools to deliver high-quality experiences for travelers, partners, and our employees. A singular technology platform powered by data and machine learning provides secure, differentiated, and personalized experiences that drive loyalty and traveler satisfaction.\nExpedia Group is seeking a skilled and motivated Data Engineer III to join our Finance Business Intelligence team supporting the Product & Technology Finance organization. In this role, you will help drive data infrastructure and analytics solutions that support strategic financial planning, reporting, and operational decision-making across the Global Finance community. You ll work closely with Finance and Technology partners to ensure data accuracy, accessibility, and usability in support of Expedia s business objectives.\nAs a Data Engineer III, you have strong experience working with a variety of datasets, data environments, tools, and analytical techniques. You enjoy a fun, collaborative and stimulating team environment. Successful candidates should be able to own projects end-to-end, including identifying problems and solutions, building and maintain data pipelines and dashboards, distilling key insights and communicate to stakeholders.\nIn this role, you will:\nDevelop new and improve existing end to end Business Intelligence products (data pipelines, Tableau dashboards, and Machine Learning predictive forecasting models).\nDrive internal efficiencies through streamline code/documentation/Tableau development to maintain high data integrity.\nTroubleshoot and resolve production issues with the team products (automation opportunities, optimizations, back-end data issues, data reconciliations).\nProactively reach out to subject matter experts /stakeholders and collaborate to solve problems.\nRespond to ad hoc data requests and conduct analysis to provide valuable insights to stakeholders.\nCollaborate and coordinate with team members/stakeholders to translate complex data into meaningful insights, that improve the analytical capabilities of the business.\nApply knowledge of database design to support migration of data pipelines from on prem to cloud environment (including data extraction, ingestion, processing of large data sets)\nSupport dashboard development on cloud environment to enable self-service reporting.\nCommunicate clearly on current work status and design considerations\nThink broadly and comprehend the how, why, and what behind data architecture designs\nExperience & Qualifications:\nBachelor s in Computer Science, Mathematics, Statistics, Information Systems, or related field\n5+ years experience in a Data Analyst, Data Engineer or Business Analyst role\nProven expertise in SQL, with practical experience utilizing query engines including SQL Server, Starburst, Trino, Querybook and data science tools such as Python/R, SparkSQL.\nProficient visualization skills (Tableau, Looker, or similar) and excel modeling/report automation.\nExceptional understanding of relational and dimensional datasets, data warehouse and data mining and applies database design principles to solve data requirements\nExperience building robust data extract, load and transform (ELT) processes, that source data from multiple databases.\nDemonstrated record of defining and executing key analysis and solving problems with minimal supervision.\nDynamic individual contributor who consistently enhances operational playbooks to address business problems.\n3+ year working in a hybrid environment that uses both on-premise and cloud technologies is preferred.\nExperience working in an environment that manipulates large datasets on the cloud platform preferred.\nBackground in analytics, finance or a comparable reporting and analytics role preferred.\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Database design', 'Machine learning', 'Business intelligence', 'Data mining', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-12 14:58:33
Lead Data Engineer,Conduent,8 - 13 years,Not Disclosed,['Noida'],"Job Overview \n\nWe are looking for a Data Engineer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is the acquisition, transformation, loading and processing of data from a multitude of disparate data sources, including structured and unstructured data for advanced analytics and machine learning in a big data environment.\n\n\n",,,,"['sql coding', 'sql', 'configuration management', 'software engineering', 'release engineering', 'continuous integration', 'rdbms', 'sql queries', 'performance tuning', 'azure synapse', 'ci/cd', 'azure data factory', 'machine learning', 'data engineering', 'powershell', 'olap', 'etl', 'big data']",2025-06-12 14:58:36
Lead Engineer - Data Science,Sasken Technologies,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position takes ownership of a module and associated quality and delivery. Person at this position provides instructions, guidance and advice to team members to ensure quality and on time delivery.\nPerson at this position is expected to be able to instruct and review the quality of work done by technical staff.\nPerson at this position should be able to identify key issues and challenges by themselves, prioritize the tasks and deliver results with minimal direction and supervision.\nPerson at this position has the ability to investigate the root cause of the problem and come up alternatives/ solutions based on sound technical foundation gained through in-depth knowledge of technology, standards, tools and processes.\nPerson has the ability to organize and draw connections among ideas and distinguish between those which are implementable.\nPerson demonstrates a degree of flexibility in resolving problems/ issues that atleast to in-depth command of all techniques, processes, tools and standards within the relevant field of specialisation.\n\n\nRoles & Responsibilities\nResponsible for requirement analysis and feasibility study including system level work estimation while considering risk identification and mitigation.\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals.\nResponsible for traceability of the requirements from design to delivery Code optimization and coverage.\nResponsible for conducting reviews, identifying risks and ownership of quality of deliverables.\nResponsible for identifying training needs of the team.\nExpected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments.\nExpected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\nExpected to be a technical mentor for junior members.\nPerson may be given additional responsibility of managing people based on discretion of Project Manager.\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 5-8 years\n\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Unix', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Machine Learning', 'Python']",2025-06-12 14:58:38
Senior ML Compiler Engineer,Qualcomm,0 - 5 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nInterested in accelerating machine learning and artificial intelligence on mobile devices for millions of usersCome join our team. We are building software platforms that enable users of Qualcomms silicon to construct optimized neural networks and machine learning algorithms. We are looking for software engineers with a machine learning or compiler background who will help us build these software platforms. In this role, you will construct and tune machine learning frameworks, build compilers and tools, and collaborate with Qualcomm hardware and software engineers to enable efficient usage of Qualcomms silicon for machine learning applications.\n\nMinimum qualifications:\nBachelors degree in Engineering, Information Systems, Computer Science, or related field.\nProgramming in C/C++\n0 to 10 years of software engineering or related work experience\n\n\nPreferred qualifications:\nExperience in machine learning frameworks such as MxNet/NNVM/TVM, Pytorch, Tensorflow, Caffe\n\nOR experience in compilers with an interest in machine learning\nDeep knowledge of software engineering\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'software engineering', 'algorithms', 'c++', 'natural language processing', 'caffe', 'neural networks', 'mxnet', 'artificial intelligence', 'sql', 'deep learning', 'r', 'java', 'data science', 'computer vision', 'machine learning algorithms', 'ml']",2025-06-12 14:58:41
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,"['Noida', 'Gurugram']","Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 14:58:43
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Pune'],"ZSs Insights & Analytics group partners with clients to design and deliver solutions to help them tackle a broad range of business challenges. Our teams work on multiple projects simultaneously, leveraging advanced data analytics and problem-solving techniques. Our recommendations and solutions are based on rigorous research and analysis underpinned by deep expertise and thought leadership.\nWhat you'll Do\nDevelop advanced and efficient statistically effective algorithms that solve problems of high",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 14:58:45
Data Scientist,H3 Technologies,3 - 8 years,Not Disclosed,['Thiruvananthapuram'],"Position: Data Scientist\nLocation: Trivandrum\nJob Description :\nWe are urgently looking for a motivated Data Scientist with a focus on Computer Vision and Machine Learning. The candidate will have a passion for solving complex problems using deep learning, image processing, and AI-driven techniques. He shall work closely with a team of data scientists, engineers, etc and to build, optimize, and deploy machine learning models for real-world applications\nKey Responsibilities :\nDevelop, train, and optimize deep learning models for image classification, object detection, segmentation, and other computer vision tasks.\nImplement and fine-tune machine learning algorithms for structured and unstructured data analysis.\nPreprocess and augment image/video datasets to improve model accuracy and robustness.\nWork with frameworks such as YOLO, TensorFlow, PyTorch, and OpenCV to build scalable models.\nAssist in deploying models to production environments, including cloud and edge computing platforms.\nCollaborate with cross-functional teams to integrate AI solutions into existing workflows and products.\nStay up-to-date with the latest research and trends in AI, computer vision, and machine learning.\nQualifications :\nBachelors or masters degree in computer science, Data Science, AI/ML, or a related field.\nMinimum of 3 year of professional experience in Python programming and AI/ML integrations\nSolid understanding of machine learning concepts, neural networks, and deep learning architectures.\nHands-on experience in training and optimizing computer vision models.\nFamiliarity with data preprocessing techniques, image annotation tools, and model evaluation metrics.\nStrong problem-solving skills and the ability to work in a fast-paced environment.\nJoining: Immediate to less than 30 days\nBudget: 13 - 14 LPA\n"",",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Computer vision', 'deep learning', 'Data analysis', 'Image processing', 'data science', 'Neural networks', 'Machine learning', 'Budgeting', 'Python']",2025-06-12 14:58:47
Freelance Online Data Analyst - Hindi Speaker,TELUS Digital,0 - 5 years,Not Disclosed,[],Job description\nAre you a detail-oriented individual with a passion for research and a good understanding of national and local geography? This freelance opportunity allows you to work at your own pace and from the comfort of your own home.\n\n\nA Day in the Life of an Online Data Analyst:,,,,"['Artificial Intelligence', 'Data Analytics', 'Ai Solutions', 'Data Analysis', 'Information Technology']",2025-06-12 14:58:49
Data Engineer,AMERICAN EXPRESS,2 - 4 years,13-17 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Role & responsibilities\nUnderstanding business use cases and be able to convert to technical design\nPart of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers and business partners.\nYou will be designing scalable, testable and maintainable data pipelines\nIdentify areas for data governance improvements and help to resolve data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design changes",,,,"['Spark', 'SQL', 'Python', 'Hadoop', 'Big Data']",2025-06-12 14:58:52
Data Scientist,Jsg. Consulting. Pvt.Ltd.,3 - 5 years,9.6-10.8 Lacs P.A.,['Jaipur'],"Familiarity with MDM (Meter Data Management), HES, and utility billing systems.\nExposure to AMI events analysis, load curves, and customer behavior analytics.\nKnowledge of regulatory requirements, data retention, and data .",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['billing exceptions', 'load profiling', 'Machine Learning', 'Meter Data Management', 'Smart Metering', 'Hes']",2025-06-12 14:58:54
"AI/ML Engineer (Specializing in NLP/ML, Large Data Processing,",Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027361\n\nJob Summary\nSynechron seeks a highly skilled AI/ML Engineer specializing in Natural Language Processing (NLP), Large Language Models (LLMs), Foundation Models (FMs), and Generative AI (GenAI). The successful candidate will design, develop, and deploy advanced AI solutions, contributing to innovative projects that transform monolithic systems into scalable microservices integrated with leading cloud platforms such as Azure, Amazon Bedrock, and Google Gemini. This role plays a critical part in advancing Synechrons capabilities in cutting-edge AI technologies, enabling impactful business insights and product innovations.\n\nSoftware\n\nRequired Proficiency:\nPython (core librariesTensorFlow, PyTorch, Hugging Face transformers, etc.)\nCloud platformsAzure, AWS, Google Cloud (familiarity with AI/ML services)\nContainerizationDocker, Kubernetes\nVersion controlGit\nData management toolsSQL, NoSQL databases (e.g., MongoDB)\nModel deployment and MLOps toolsMLflow, CI/CD pipelines, monitoring tools\nPreferred\n\nSkills:\nExperience with cloud-native AI frameworks and SDKs\nFamiliarity with AutoML tools\nAdditional programming languages (e.g., Java, Scala)\nOverall Responsibilities\nDesign, develop, and optimize NLP models, including advanced LLMs and Foundation Models, for diverse business use cases.\nLead the development of large data pipelines for training, fine-tuning, and deploying models on big data platforms.\nArchitect, implement, and maintain scalable AI solutions in line with MLOps best practices.\nTransition legacy monolithic AI systems into modular, microservices-based architectures for scalability and maintainability.\nBuild end-to-end AI applications from scratch, including data ingestion, model training, deployment, and integration.\nImplement retrieval-augmented generation techniques for enhanced context understanding and response accuracy.\nConduct thorough testing, validation, and debugging of AI/ML models and pipelines.\nCollaborate with cross-functional teams to embed AI capabilities into customer-facing and enterprise products.\nSupport ongoing maintenance, monitoring, and scaling of deployed AI systems.\nDocument system designs, workflows, and deployment procedures for compliance and knowledge sharing.\nPerformance Outcomes:\nProduction-ready AI solutions delivering high accuracy and efficiency.\nRobust data pipelines supporting training and inference at scale.\nSeamless integration of AI models with cloud infrastructure.\nEffective collaboration leading to innovative AI product deployment.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: Python (TensorFlow, PyTorch, Hugging Face, etc.)\nPreferred: Java, Scala\nDatabases/Data Management:\nSQL (PostgreSQL, MySQL), NoSQL (MongoDB, DynamoDB)\nCloud Technologies:\nAzure AI, AWS SageMaker, Bedrock, Google Cloud Vertex AI, Gemini\nFrameworks and Libraries:\nTransformers, Keras, scikit-learn, XGBoost, Hugging Face engines\nDevelopment Tools & Methodologies:\nDocker, Kubernetes, Git, CI/CD pipelines (Jenkins, Azure DevOps)\nSecurity & Compliance:\nKnowledge of data security standards and privacy policies (GDPR, HIPAA as applicable)\nExperience\n8 to 10 years of hands-on experience in AI/ML development, especially NLP and Generative AI.\nDemonstrated expertise in designing, fine-tuning, and deploying LLMs, FMs, and GenAI solutions.\nProven ability to develop end-to-end AI applications within cloud environments.\nExperience transforming monolithic architectures into scalable microservices.\nStrong background with big data processing pipelines.\nPrior experience working with cloud-native AI tools and frameworks.\nIndustry experience in finance, healthcare, or technology sectors is advantageous.\nAlternative Experience:\nCandidates with extensive research or academic experience in AI/ML, especially in NLP and large-scale data processing, are eligible if they have practical deployment experience.\n\nDay-to-Day Activities\nDevelop and optimize sophisticated NLP/GenAI models fulfilling business requirements.\nLead data pipeline construction for training and inference workflows.\nCollaborate with data engineers, architects, and product teams to ensure scalable deployment.\nConduct model testing, validation, and performance tuning.\nImplement and monitor model deployment pipelines, troubleshoot issues, and improve system robustness.\nDocument models, pipelines, and deployment procedures for audit and knowledge sharing.\nStay updated with emerging AI/ML trends, integrating best practices into projects.\nPresent findings, progress updates, and technical guidance to stakeholders.\nQualifications\nBachelors degree in Computer Science, Data Science, or related field; Masters or PhD preferred.\nCertifications in AI/ML, Cloud (e.g., AWS, Azure, Google Cloud), or Data Engineering are a plus.\nProven professional experience with advanced NLP and Generative AI solutions.\nCommitment to continuous learning to keep pace with rapidly evolving AI technologies.\nProfessional Competencies\nStrong analytical and problem-solving capabilities.\nExcellent communication skills, capable of translating complex technical concepts.\nCollaborative team player with experience working across global teams.\nAdaptability to rapidly changing project scopes and emerging AI trends.\nInnovation-driven mindset with a focus on delivering impactful solutions.\nTime management skills to prioritize and manage multiple projects effectively.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data management', 'data processing', 'pipeline', 'big data', 'continuous integration', 'kubernetes', 'deploying models', 'natural language processing', 'ci/cd', 'fms', 'artificial intelligence', 'docker', 'sql', 'microservices', 'tensorflow', 'java', 'pytorch', 'jenkins', 'keras', 'aws']",2025-06-12 14:58:56
Data Scientist,Callaway Digital Technologies,6 - 9 years,Not Disclosed,['Hyderabad'],"JOB OVERVIEW\nThe ideal candidate will be responsible for analyzing and interpreting large data sets related to finance, sales and supply chain operations to optimize business processes, identify opportunities for improvement, and provide strategic insights to support decision-making. The Data Scientist will work closely with cross-functional teams to identify key business questions, design and implement statistical models, and develop innovative data-driven solutions.\nKey Responsibilities:",,,,"['Statistical Modeling', 'Machine Learning', 'Python', 'Data Visualization', 'Azzure', 'R Program', 'SQL']",2025-06-12 14:58:59
Data Scientist,Apcfss,2 - 6 years,Not Disclosed,"['Vijayawada', 'Guntur', 'Mangalagiri']","Location: Vijayawada, Andhra Pradesh\nExperience: 2 to 6 years\nEmployment Type: Full-Time\n\nJob Opening: Data Scientist\nWe are seeking a data-driven problem solver to join our team as a Data Scientist. You will play a key role in transforming data into actionable insights and building models that support strategic decisions across the organization. Collaborating with cross-functional teams, youll help turn complex data into clear value.\nKey Responsibilities\nAnalyze large and complex datasets to uncover trends, patterns, and insights\nBuild, validate, and deploy predictive and statistical models\nWork closely with engineering and product teams to integrate models into production systems\nCommunicate analytical findings and insights clearly to both technical and non-technical stakeholders\nRequirements\nProficiency in Python or R, and strong command of SQL\nHands-on experience with machine learning and statistical modeling\nStrong analytical and problem-solving skills\nExperience with cloud platforms such as AWS, GCP, or Azure\nNice to Have\nExperience in Natural Language Processing (NLP), deep learning, or time-series forecasting\nPrior work in [industry-specific domain, e.g., fintech, healthcare, e-commerce]",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'GCP', 'Machine Learning', 'AWS', 'Deep Learning', 'SQL']",2025-06-12 14:59:01
"MTS 1, Machine Learning Scientist",Xoom,7 - 9 years,Not Disclosed,['Bengaluru'],"Job Summary\nWhat you need to know about the role-\nPayPal Lending ML team is looking for an experienced Machine Learning Scientist to help us develop and enhance machine learning/AI capabilities and innovate to grow PP Credit products. This is an excellent opportunity to join a team of intelligent and passionate engineers and scientists and to help shape the future of the Lending Business at PayPal.\nJob Description\nMeet our team\nPayPal Lending ML team is responsible for building ML/AI solutions that drive impact across PayPal. We build innovative solutions that interact with customers throughout the lending lifecycle with a strong focus on driving PP growth and business.\nYour way to impact\nIdentify AI ML opportunities and build solutions that impact the business\nExplore and execute to get solid results\nImpact team, partners and business through PayPal values\nYour day to day\nIn your day to day role you will\nLead ML Projects and Conduct research to identify new and innovative ML techniques.\nInnovate to create efficiencies for the team and the business.\nWork closely with other engineers , analysts and leaders to implement and opt imize ML models.\nDevelop and implement best practices for ML model management, deployment, and monitoring.\nCollaborate with other teams to ensure ML models are integrated into the product and services.\nAssist with troubleshooting and resolving technical issues.\nResponsible for documentation, project tracking, and quality controls .\nWhat do you need to bring-\nThe ideal candidate will possess a degree in engineering, science, statistics or mathematics with strong technical background in machine learning.\nWe are looking for candidates who have excellent communication skills, an analytical mindset, and a passion for problem-solving.\n7+ years of hands-on experience with problem-solving using Machine Learning\nHands-on experience with Python or Java, along with relevant technologies such as Spark, Hadoop, BigQuery , SQL, is required .\nCandidates should have in-depth knowledge of machine learning algorithms, explainable AI methods, and NLP.\nExperience with Cloud frameworks such as GCP, AWS is preferred.\nExperience with Lending and Financial services is a plus.\nPreferred Qualification\nSubsidiary\nPayPal\nTravel Percent\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https//www.paypalbenefits.com .\nWho We Are\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. .\nBelonging at PayPal\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Project tracking', 'Analytical', 'Diversity and Inclusion', 'Finance', 'Machine learning', 'Wellness', 'Troubleshooting', 'Financial services', 'Monitoring', 'SQL']",2025-06-12 14:59:04
Data Scientist,Celebal Technologies,3 - 6 years,Not Disclosed,"['Navi Mumbai', 'Mumbai (All Areas)']","Job Title: Data Scientists\nLocation: Navi Mumbai\nDuration: Fulltime\nPositions: Multiple\n\nWe are looking for a highly skilled Data Scientists with deep expertise in time series forecasting, particularly in demand forecasting and customer lifecycle analytics (CLV). The ideal candidate will be proficient in Python or PySpark, have hands-on experience with tools like Prophet and ARIMA, and be comfortable working in Databricks environments. Familiarity with classic ML models and optimization techniques is a plus.",,,,"['Demand Forecasting', 'Data Bricks', 'Time Series', 'Pyspark', 'Arima', 'Customer Lifecycle', 'Forecasting', 'Machine Learning', 'Optimization', 'Data Science', 'Xgboost', 'Time Series Analysis', 'Prophet', 'Python']",2025-06-12 14:59:06
Data Scientist,Celebal Technologies,3 - 6 years,Not Disclosed,"['Mumbai', 'Navi Mumbai']","About Us: Celebal Technologies is a leading Solution Service company that provide Services the field of Data Science, Big Data, Enterprise Cloud & Automation. We are at the forefront of leveraging cuttingedge technologies to drive innovation and enhance our business processes. As part of our commitment to staying ahead in the industry, we are seeking a talented and experienced Data & AI Engineer with strong Azure cloud competencies to join our dynamic team.\n\nJob Summary: We are looking for a highly skilled Data Scientist with deep expertise in time series forecasting, particularly in demand forecasting and customer lifecycle analytics (CLV). The ideal candidate will be proficient in Python or PySpark, have hands-on experience with tools like Prophet and ARIMA, and be comfortable working in Databricks environments. Familiarity with classic ML models and optimization techniques is a plus.\n\nKey Responsibilities\n• Develop, deploy, and maintain time series forecasting models (Prophet, ARIMA, etc.) for demand forecasting and customer behavior modeling.\n• Design and implement Customer Lifetime Value (CLV) models to drive customer retention and engagement strategies.\n• Process and analyze large datasets using PySpark or Python (Pandas).\n• Partner with cross-functional teams to identify business needs and translate them into data science solutions.\n• Leverage classic ML techniques (classification, regression) and boosting algorithms (e.g., XGBoost, LightGBM) to support broader analytics use cases.\n• Use Databricks for collaborative development, data pipelines, and model orchestration.\n• Apply optimization techniques where relevant to improve forecast accuracy and business decision-making.\n• Present actionable insights and communicate model results effectively to technical and non-technical stakeholders.\n\nRequired Qualifications\n• Strong experience in Time Series Forecasting, with hands-on knowledge of Prophet, ARIMA, or equivalent Mandatory.\n• Proven track record in Demand Forecasting Highly Preferred.\n• Experience in modeling Customer Lifecycle Value (CLV) or similar customer analytics use cases Highly Preferred.\n• Proficiency in Python (Pandas) or PySpark Mandatory.\n• Experience with Databricks Mandatory.\n• Solid foundation in statistics, predictive modeling, and machine learning",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning Operations', 'Demand Forecasting', 'Data Bricks', 'Pyspark', 'Large Language Model', 'Time Series', 'Spark', 'Machine Learning', 'Python']",2025-06-12 14:59:08
Data Scientist,"Sourced Group, an Amdocs Company",4 - 9 years,Not Disclosed,['Gurugram'],"0px> Who are we?\nIn one sentence\nThis is a hands-on position for a motivated and talented innovator. The Data Scientist performs data mining and develops algorithms that provide insight from data.\nWhat will your job look like?\nYou will be responsible for and perform end-top-end data-based research.\nYou will craft data mining solutions to be implemented and executed with alignment to the planned scope and design coverage and needs/uses, demonstrating knowledge and a broad understanding of E2E business processes and requirements.\nYou will define the data analytics research plan, scope and resources required to meet the objectives of his/her area of ownership.\nYou will identify and analyze new data analytic directions and their potential business impact to determine the accurate prioritization of data analytics activities based on business needs and analytics value.\nYou will identify data sources, supervises the data collection process and crafts the data structure in collaboration with data experts (BI or big-data) and subject matter and business experts. Ensures that data used in the data analysis activities are of the highest quality.\nYou will construct data models (algorithms and formulas) for required business needs and predictions.\nYou will present results, including the preparation of patents and white papers and facilitating presentations during conferences.\nAll you need is...\nPh.D. in Computer Science, Mathematics or Statistics\n4 years experience in tasks related to data analytics\nKnowledge of telecommunications and of the subject area being investigated - advantage\nKnowledge in the product (ACC or other) application knowledge and configuration knowledge\nKnowledge in BSS, billing, Telco and the business processes\nFamiliarity in the Telco Networking - mobile, landline, cable TV, Internet\nknowledge in Oracle SQL\nWhy you will love this job:\nYou will ensure timely resolution or critical issue within the agreed SLA. This includes creating a positive customer support experience and build strong relationships through problem understanding, presenting promptly on progress, and handling customers with a professional demeanour.\nYou will be able to demonstrates an understanding of key business drivers and ensures strategic directions are followed and the organization succeeds\nWe are a dynamic, multi-cultural organization that constantly innovates and empowers our employees to grow. Our people our passionate, daring, and phenomenal teammates that stand by each other with a dedication to creating a diverse, inclusive workplace!\nWe offer a wide range of stellar benefits including health, dental, vision, and life insurance as well as paid time off, sick time, and parental leave!\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Bss', 'Networking', 'Billing', 'Data collection', 'Customer handling', 'Customer support', 'Data mining', 'Amdocs']",2025-06-12 14:59:11
WLAN Phy RTL Design- Sr lead/Staff/Sr Staff/Principal Engineer,Qualcomm,1 - 3 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nQualcomm's Bangalore WLAN PHY (Baseband) team is seeking VLSI Digital Design Engineers to lead IP development for the latest WiFi standards.\n\nOur WLAN PHY team, comprised of highly passionate and seasoned domain experts, prides itself on years of experience in taking WLAN PHY designs from concept to silicon independently.\n\nWLAN PHY team is responsible for delivering the end-to-end Tx/Rx DSP chains- all the way from antenna samples post ADC to raw bits for upper layers and on the reverse path from raw bits to DAC. The team specializes in working with challenges of practical high-speed wireless communication systems and finding innovative solutions to counter them.\n\nThe team works extensively on typical signal processing functions like filters, matrix transformations (e.g.QR, Cholesky decomposition), channel estimation, equalization (MMSE, MRC, ML), decoders/encoders (e.g.LDPC, Viterbi) , demodulators, FFT etc. on a day-to-day basis, and contributes to the development/ enhancement/ evaluation of signal processing algorithms to cater to new requirements.\n\nWe are looking for someone as passionate as us and takes pride in their work.\n\nWiFi's ubiquity in modern times is undeniable, and the IEEE 802.11 Working Group is continually developing new standards to satisfy the growing demand for high throughput and low-latency real-time applications, such as VR and AR.\n\nQualcomm is at the forefront of the WiFi revolution, aiming to become the global leader in WiFi chip solutions. The WLAN PHY team in Bangalore is instrumental in realizing this vision.\n\n\n:\n\nLooking for a candidate with 1 to 3 years of hands-on experience in micro-architecting and developing complex IPs.\n\nExpertise in digital design, VLSI concepts, and experience in creating power/area-efficient IPs across multiple clock domains are essential.\n\nProficiency in RTL coding and familiarity with RTL QA flows such as PLDRC, CDC, and CLP (optional) is expected.\n\nCandidates should be capable of proposing design alternatives to meet area/power/performance specifications and presenting these options for review.\n\nExperience in leading, guiding, or managing junior team members is advantageous.\n\nRepeated success in taking IP designs from requirements to silicon is required.\n\nWhile not mandatory, having developed IPs for wireless technologies (WLAN, LTE, NR, BT, UWB, etc.) or past HLS experience would be beneficial.\n\n\n\n\nSkills:\n\n\n\nMust have: Proficient in Verilog RTL coding, uArch, CDC check, PLDRC, Timing constraints, Python/Perl. Experience in design/debugging complex data-path/control-path IPs. Good communication, analytical & leadership skills.\n\n\nGood to have: System Verilog, Visio, Knowledge of signal processing concepts/algorithms and Wi-Fi standards (802.11a/b/g/n/ac/ax), experience with HLS.\n\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 4+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 3+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 2+ years of Hardware Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['rtl coding', 'perl', 'python', 'cdc', 'verilog', 'antenna', 'enodeb', 'gsm', 'architecting', 'node b', 'digital design', 'wireless communication', 'rtl design', '5g', '3g', 'wcdma', 'debugging', 'telecom', 'system verilog', 'lte', 'ran', 'c', 'clp', 'rnc', 'hardware engineering', 'rtl', 'baseband', 'silicon', '4g', '2g', 'visio', 'vlsi']",2025-06-12 14:59:14
"Senior Staff Engineer, Big Data Engineer",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal experience 10+ years.\nExcellent knowledge and experience in Big data engineer.\nStrong working experience with architecture and development in Apache Spark, Spark, Scala, AWS/Azure/GCP, Data Pipelines, Kafka, SQL Server/NoSQL.\nStrong expertise in Django Rest Framework, Databricks and PostgreSQL.\nHands on experience in building data pipelines and building data frameworks for unit testing, data lineage tracking, and automation.\nFamiliarity with streaming technologies (e.g., Kafka, Kinesis, Flink).\nExperience with Machine Learning and Looker.\nKnowledge of additional server-side programming languages (e.g. Golang, C#, Ruby).\nExperience with building and maintaining a cloud system.\nFamiliarity with data modeling, data warehousing, and building distributed systems.\nExpertise in Spanner for high-availability, scalable database solutions.\nKnowledge of data governance and security practices in cloud-based environments.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers\nIdentifying different solutions and being able to narrow down the best option that meets the client’s requirements\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Django Framework', 'Spark', 'Data Bricks', 'Apache Spark']",2025-06-12 14:59:16
"Senior Data Scientist (AI/ML, Data Analysis, Cloud (AWS), and Model",Synechron,8 - 13 years,Not Disclosed,['Pune'],"job requisition idJR1027352\n\nJob Summary\nSynechron is seeking an analytical and innovative Senior Data Scientist to support and advance our data-driven initiatives. The ideal candidate will have a solid understanding of data science principles, hands-on experience with AI/ML tools and techniques, and the ability to interpret complex data sets to deliver actionable insights. This role contributes to the organizations strategic decision-making and technology innovation by applying advanced analytics and machine learning models in a collaborative environment.\n\nSoftware\n\nRequired\n\nSkills:\nPython (including libraries such as pandas, scikit-learn, TensorFlow, PyTorch) proficiency in developing and deploying models\nR (optional, but preferred)\nData management tools (SQL, NoSQL databases)\nCloud platforms (preferably AWS or Azure) for data storage and ML deployment\nJupyter Notebooks or similar interactive development environments\nVersion control tools such as Git\nPreferred\n\nSkills:\nBig data technologies (Spark, Hadoop)\nModel deployment tools (MLflow, Docker, Kubernetes)\nData visualization tools (Tableau, Power BI)\nOverall Responsibilities\nAnalyze and interpret large and complex data sets to generate insights for business and technology initiatives.\nAssist in designing, developing, and implementing AI/ML models and algorithms to solve real-world problems.\nCollaborate with cross-functional teams including data engineers, software developers, and business analysts to integrate models into production systems.\nStay current with emerging trends, research, and best practices in AI/ML/Data Science and apply them to ongoing projects.\nDocument methodologies, modeling approaches, and insights clearly for technical and non-technical stakeholders.\nSupport model validation, testing, and performance monitoring to ensure accuracy and reliability.\nContribute to the development of data science workflows and standards within the organization.\nPerformance Outcomes:\nAccurate and reliable data models that support strategic decision-making.\nClear documentation and communication of findings and recommendations.\nEffective collaboration with technical teams to deploy scalable models.\nContinuous adoption of best practices in AI/ML and data management.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: Python (best practices in ML development), SQL\nPreferred: R, Java (for integration purposes)\nDatabases/Data Management:\nSQL databases, NoSQL (MongoDB, Cassandra)\nCloud data storage solutions (AWS S3, Azure Blob Storage)\nCloud Technologies:\nAWS (S3, EC2, SageMaker, Lambda)\nAzure Machine Learning (preferred)\nFrameworks & Libraries:\nTensorFlow, PyTorch, scikit-learn, Keras, XGBoost\nDevelopment Tools & Methodologies:\nJupyter Notebooks, Git, CI/CD pipelines\nAgile and Scrum processes\nSecurity Protocols:\nBest practices in data security and privacy, GDPR compliance\nExperience\n8+ years of professional experience in AI, ML, or Data Science roles.\nProven hands-on experience designing and deploying ML models in real-world scenarios.\nDemonstrated ability to analyze complex data sets and translate findings into business insights.\nPrevious experience working with cloud-based data science solutions is preferred.\nStrong portfolio showcasing data science projects, models developed, and practical impact.\nAlternative Pathways:\nCandidates with extensive research or academic experience in AI/ML can be considered, provided they demonstrate practical application of skills.\n\nDay-to-Day Activities\nConduct data exploration, cleaning, feature engineering, and model development.\nCollaborate with data engineers to prepare data pipelines for model training.\nBuild, validate, and refine machine learning models.\nPresent insights, models, and recommendations to technical and business stakeholders.\nSupport deployment of models into production environments.\nMonitor model performance and iterate to improve effectiveness.\nParticipate in team meetings, project planning, and reviewing progress.\nDocument methodologies and maintain version control of codebase.\nQualifications\nBachelors degree in Computer Science, Mathematics, Statistics, Data Science, or a related field; Masters or PhD highly desirable.\nEvidence of relevant coursework, certifications, or professional training in AI/ML.\nProfessional certifications (e.g., AWS Certified Machine Learning Specialty, Microsoft Certified Data Scientist) are a plus.\nCommitment to ongoing professional development in AI/ML methodologies.\nProfessional Competencies\nStrong analytical and critical thinking to solve complex problems.\nEffective communication skills for technical and non-technical audiences.\nDemonstrated ability to work collaboratively in diverse teams.\nAptitude for learning new tools, techniques, and technologies rapidly.\nInnovation mindset with a focus on applying emerging research.\nStrong organizational skills to manage multiple projects and priorities.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['java', 'data science', 'python', 'deploying models', 'aws', 'continuous integration', 'kubernetes', 'scikit-learn', 'ci/cd', 'artificial intelligence', 'sql', 'docker', 'tensorflow', 'spark', 'pytorch', 'keras', 'hadoop', 'big data', 'mongodb', 'microsoft azure', 'nosql', 'pandas', 'amazon ec2', 'r', 'cassandra', 'agile']",2025-06-12 14:59:18
Specialist Data Scientist,Atlasrtx,3 - 7 years,Not Disclosed,['Pune'],"So, what s the role all about\n\nNICE provides state-of-the-art enterprise level AI and analytics for all forms of business communications between speech and digital. We are a world class research team developing new algorithms and approaches to help companies with solving critical issues such as identifying their best performing agents, preventing fraud, categorizing customer issues, and determining overall customer satisfaction. If you have interacted with a major contact center in the last decade, it is very likely we have processed your call.\n\nThe research group partners with all areas of NICE s business to scale out the delivery of new technology and AI models to customers around the world that are tailored to their company, industry, and language needs.\n\n\nHow will you make an impact\n\nConduct cutting-edge research and develop advanced NLP algorithms and models.\n\nBuild and fine-tune deep learning and machine learning models, with a focus on large language models.\n\nWork closely with internal stakeholders to define model requirements and ensure alignment with business objectives.\n\nDevelop AI predictive models and perform data and model accuracy analyses.\n\nProduce and present findings, technical concepts, and model recommendations to both technical and non-technical stakeholders.\n\nDevelop and maintain scripts/tools to automate both new model production and updates to existing model packages.\n\nStay abreast of the latest advancements in data science research and contribute to the development of our knowledge base.\n\nCollaborate with developers to design automation and tool improvements for model building.\n\nMaintain documentation of processes and projects across all supported languages and environments.\n\n\nHave you got what it takes\n\nMasters degree in the field of Computer Science, Technology, Engineering, Math, or equivalent practical experience\n\nMinimum of 8 years of data science work experience, including implementing machine learning and NLP models using real-life data.\n\nExperience with Retrieval-Augmented Generation (RAG) pipelines or LLMOps.\n\nAdvanced knowledge of statistics and machine learning algorithms.\n\nProficiency in Python programming and familiarity with R.\n\nExperience with deep learning models and libraries such as PyTorch, TensorFlow, and JAX.\n\nFamiliarity with relational databases and query languages (e. g. , MSSQL) and basic SQL knowledge.\n\nHands-on experience with transformer models (BERT, FlanT5, Llama, etc. ) and GenAI frameworks (HuggingFace, LangChain, Ollama, etc. ).\n\nExperience deploying NLP models in production environments, ensuring scalability and performance using AWS/GCP/Azure\n\nStrong verbal and written communication skills, including effective presentation abilities.\n\nAbility to work independently and as part of a team, demonstrating analytical thinking and problem-solving skills.\n\n\n\nYou will have an advantage if you also have:\n\nExpertise with Big Data technologies (e. g. , PySpark).\n\nBackground in knowledge graphs, graph databases, or GraphRAG architectures.\n\nUnderstanding of multimodal models (text, audio, vision).\n\nExperience in Customer Experience domains.\n\nExperience with package development and technical writing.\n\nFamiliarity with tools like Jira, Confluence, and source control packages and methodology.\n\nKnowledge and interest in foreign languages and linguistics.\n\nExperience working on international, globe-spanning teams and with AWS.\n\nPast participation in a formal research setting.\n\nExperience as part of a software organization.\n\n\n\nWhat s in it for you\n\n\n\nEnjoy NICE-FLEX!\n\n\n\nRequisition ID : 7481\nReporting into : Tech Manager\nRole Type : Individual Contributor\n\nAbout NICE",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Technical writing', 'GCP', 'Analytical', 'Machine learning', 'Flex', 'Analytics', 'SQL', 'Python']",2025-06-12 14:59:20
Azure Cloud Data Engineering Consultant,Optum,7 - 10 years,17-27.5 Lacs P.A.,['Gurugram'],"Primary Responsibilities:\nDesign and develop applications and services running on Azure, with a strong emphasis on Azure Databricks, ensuring optimal performance, scalability, and security.\nBuild and maintain data pipelines using Azure Databricks and other Azure data integration tools.\nWrite, read, and debug Spark, Scala, and Python code to process and analyze large datasets.\nWrite extensive query in SQL and Snowflake\nImplement security and access control measures and regularly audit Azure platform and infrastructure to ensure compliance.\nCreate, understand, and validate design and estimated effort for given module/task, and be able to justify it.\nPossess solid troubleshooting skills and perform troubleshooting of issues in different technologies and environments.\nImplement and adhere to best engineering practices like design, unit testing, functional testing automation, continuous integration, and delivery.\nMaintain code quality by writing clean, maintainable, and testable code.\nMonitor performance and optimize resources to ensure cost-effectiveness and high availability.\nDefine and document best practices and strategies regarding application deployment and infrastructure maintenance.\nProvide technical support and consultation for infrastructure questions.\nHelp develop, manage, and monitor continuous integration and delivery systems.\nTake accountability and ownership of features and teamwork.\nComply with the terms and conditions of the employment contract, company policies and procedures, and any directives.\nRequired Qualifications:\nB.Tech/MCA (Minimum 16 years of formal education)\nOverall 7+ years of experience.\nMinimum of 3 years of experience in Azure (ADF), Databricks and DevOps.\n5 years of experience in writing advanced level SQL.\n2-3 years of experience in writing, reading, and debugging Spark, Scala, and Python code.\n3 or more years of experience in architecting, designing, developing, and implementing cloud solutions on Azure.\nProficiency in programming languages and scripting tools.\nUnderstanding of cloud data storage and database technologies such as SQL and NoSQL.\nProven ability to collaborate with multidisciplinary teams of business analysts, developers, data scientists, and subject-matter experts.\nFamiliarity with DevOps practices and tools, such as continuous integration and continuous deployment (CI/CD) and Teraform.\nProven proactive approach to spotting problems, areas for improvement, and performance bottlenecks.\nProven excellent communication, writing, and presentation skills.\nExperience in interacting with international customers to gather requirements and convert them into solutions using relevant skills.\nPreferred Qualifications:\nKnowledge of AI/ML or LLM (GenAI).\nKnowledge of US Healthcare domain and experience with healthcare data.\nExperience and skills with Snowflake.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'ETL', 'SQL', 'Python', 'Airflow', 'Pyspark', 'Snowflake', 'SCALA', 'Spark', 'Data Bricks']",2025-06-12 14:59:23
Senior Software Engineer- AWS Machine Learning,Cross Country Infotech,5 - 10 years,Not Disclosed,['Pune'],"[{""Salary"":null , ""Remote_Job"":false , ""Posting_Title"":""Senior Software Engineer- AWS Machine Learning"" , ""Is_Locked"":false , ""City"":""Pune City"",""Industry"":""IT Services"",""Job_Description"":""\nLeverage AWS MLplatform services and frameworks to deliver production ready models acrossmultiple internal and external applications\nBuild predictiveand generative models specific to product needs\nAnalyze large data sets and build data pipelines for model training\nCollaborate with Product Designers, Product Managers, and Software Engineers to deliver",,,,"['Computer science', 'IT services', 'Quality monitoring', 'Automation', 'Machine learning', 'Solution delivery', 'AWS', 'Enterprise software', 'Software solutions', 'Python']",2025-06-12 14:59:26
"Lead Engineer, Senior - Model Orchestration and Accuracy Tools",Qualcomm,5 - 10 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Systems Engineer, you will research, design, develop, simulate, and/or validate systems-level software, hardware, architecture, algorithms, and solutions that enables the development of cutting-edge technology. Qualcomm Systems Engineers collaborate across functional teams to meet and exceed system-level requirements and standards.\n\nThe AI SW team at Qualcomm is focused on advancing state-of-the-art in Artificial Intelligence across various business segments, including Mobile, AR & VR Technology, IoT, and Auto ADAS. The AISW stack leverages Qualcomm chips' extensive heterogeneous computing capabilities, enabling the running of trained neural networks on devices without needing a cloud connection. This allows neural network models trained in various frameworks on Snapdragon platforms to run at blazing speeds while consuming minimal power. As a Senior Lead Engineer, you will see your work directly impact billions of devices worldwide.\n\nKey Responsibilities:\n\nTo design, development, and implementation of AI/ML solutions across multiple domains.\n\nCollaborate with cross-functional teams to ensure seamless integration of AI/ML components within the broader framework\n\nAddress and resolve issues related to AI models finetuning, quantization, compression and graph level optimizations, ensuring high performance and accuracy of AI models.\n\nShould possess good analytical skills - Consistently gather, integrate, and interpret information from different sources and conduct in depth analysis to find the root causes, provide recommendations, and effectively solve moderate to highly complex problems.\n\nConduct research on industry trends and innovations in AI/ML to adopt best practices in solutions and deliverables.\n\nDevelop and optimize quantization techniques for AI/ML models, ensuring efficient execution on Qualcomm hardware\n\nManage project timelines, objectives, and goals, ensuring efficient use of resources across functional areas.\n\nMentor and coach junior engineers, providing development experiences and networking opportunities.\n\n\nMinimum Qualifications:\n\nBachelor's degree in Engineering, Computer science or a related field and 5+ years of experience of Software engineering or related work experience\n\nOR\n\nMasters degree in Engineering, Computer science or a related field and 4+ years of experience of Software engineering or related work experience:\n\nExperience with SW architecture and programming languages.\n\nExperience with tools and frameworks such as PyTorch, TensorFlow, ONNX, and others.\n\n\nPreferred Qualifications:\n\nExcellent development skills in Python / C++\n\nProficient in Data structures and algorithms\n\nHands on expertise in deep learning frameworks such as ONNX, PyTorch, etc\n\nIn depth knowledge of state-of-the-art Computer Vision, NLP, LLM, LVM and LMM.\n\nGood understanding of Quantization (8-bit, 4-bit) and Calibration algorithms\n\nGood understanding of machine learning compiler techniques and graphs optimizations\n\nExcellent analytical, development, and debugging skills\n\nGood understanding of SW design patterns and design philosophies (SOLID principles, design patterns)\n\nKnowledge of machine learning runtimes like ONNX Runtime and execuTorch.\n\nKnowledge of AI model efficiency toolkit (AIMET), Snapdragon Neural processing engine is a plus.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'c++', 'natural language processing', 'data structures', 'deep learning frameworks', 'lvm', 'sw', 'machine learning', 'artificial intelligence', 'tool engineering', 'deep learning', 'tensorflow', 'pytorch', 'debugging', 'software engineering', 'onnx', 'system engineering']",2025-06-12 14:59:29
IT Manager - Data Engineering & Analytics,ZS,12 - 15 years,Not Disclosed,['Pune'],"IT MANAGER, DATA ENGINEERING AND ANALYTICS will lead a team of data engineers and analysts responsible for designing, developing, and maintaining robust data systems and integrations. This role is critical for ensuring the smooth collection, transformation, integration and visualization of data, making it easily accessible for analytics and decision-making across the organization. The Manager will collaborate closely with analysts, developers, business leaders and other stakeholders to ensure that the data infrastructure meets business needs and is scalable, reliable, and efficient.\n",,,,"['Data modeling', 'Project management', 'Analytical', 'Financial planning', 'Management consulting', 'Data quality', 'Troubleshooting', 'Stakeholder management', 'Analytics', 'SQL']",2025-06-12 14:59:31
Machine Learning Scientist (ASR),Wadhwani Ai,6 - 11 years,Not Disclosed,['Mumbai'],"SUMMARY\nWe are looking for a Machine Learning Scientist who is passionate about applying AI and ML to solve high-impact societal challenges. The ideal candidate will have a strong research and/or applied background with 6+ years of relevant experience or a PhD, and expertise in designing and implementing machine learning models, data analysis, and simulations. Candidates with experience in Automated Speech Recognition (ASR) are preferred for this role. They should be skilled in mentoring junior researchers, working in cross-functional teams, and contributing to both practical deployments and academic research. A demonstrated commitment to social good, strong communication skills, and the ability to translate complex problems into deployable ML solutions are essential.\nABOUT US - https://www.wadhwaniai.org/\nWadhwani AI is a nonprofit institute building and deploying applied AI solutions to solve critical issues in public health, agriculture, education, and urban development in underserved communities in the global south. We collaborate with governments, social sector organizations, academic and research institutions, and domain experts to identify real-world problems, and develop practical AI solutions to tackle these issues with the aim of making a substantial positive impact.\nWe have over 30+ AI projects supported by leading philanthropies such as Bill & Melinda Gates Foundation, USAID and Google.org. With a team of over 200 professionals, our expertise encompasses AI/ML research and innovation, software engineering, domain knowledge, design and user research.\nIn the Press:\nOur Founder Donors are among the Top 100 AI Influencers\nG20 India s Presidency: AI Healthcare, Agriculture, & Education Solutions Showcased Globally.\nUnlocking the potentials of AI in Public Health\nWadhwani AI Takes an Impact-First Approach to Applying Artificial Intelligence - data.org\nWinner of the H&M Foundation Global Change Award 2022\nIndian Winners of the 2019 Google AI Impact Challenge, and the first in the Asia Pacific to host Google Fellows\nROLES AND RESPONSIBILITIES\nAs an ML Scientist, you will be responsible for building machine learning solutions to problems of societal importance, and mentoring other team members in this effort. You will participate in problem definition and the development and execution of algorithms and solutions to the problems.\nAs an ASR expert, you will also be assisting in the design of evaluations for ASR solutions submitted for funding to the India AI mission at the Ministry of Electronics and Information Technology (MEITy). You will play an advisory role in the execution of these evaluations.\nIn order to apply machine learning and related technologies for social good, you will need to understand user challenges and their context, curate and transform data, train and validate models, run simulations, and broadly derive insights from data. In doing so, you will work in cross-functional teams spanning engineering, solutions, domain experts, and designers. You will also work closely with social sector organisations, and are encouraged to collaborate with researchers across the world.\nYou will be encouraged to drive fundamental advances to the technology domains themselves as part of the efforts towards their application, present your work in technical and other forums of interest, and publish in leading conferences and journals.\nAt Wadhwani AI, excellence as an individual contributor goes hand-in-hand with good teamwork and collaboration. You will mentor junior researchers, post-docs, and interns, and participate in recruiting and hiring activities. You will also be expected to interact with external partners of Wadhwani AI when required, and to make periodic visits to the communities from where challenges are derived and where the solutions will be deployed.\nREQUIREMENTS\nM.Tech./M.E./M.S./M.Sc. or equivalent in Computer Science, Electrical Engineering, Statistics, Applied Mathematics, Physics, Economics or a relevant quantitative field with work experience of 6+ years or a PHd from a reputed institute. Experience and expertise in ASR is preferred.\nWe are looking for ML Scientists with experience applying AI, machine learning, and data science to real-world problems. Ideal candidates should have a strong research background, have experience in mentoring junior researchers through strong code practices and ideation, and be adept at a variety of data mining/analysis methods and tools, building and implementing models, visualising data, creating/using algorithms, and running simulations. Familiarity with popular deep learning algorithms is a strong plus, however demonstrated willingness and ability to learn will also be considered.\nCandidates should be comfortable working with cross-functional teams and must have excellent oral and written communication skills and a track record of driving projects to completion. Evidence of written skills through high quality research publications is a plus.\nCandidates should be passionate about using their technical skills to solve large societally important problems.",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'ASR', 'Data analysis', 'Artificial Intelligence', 'Machine learning', 'Healthcare', 'Research', 'Data mining', 'Information technology', 'Public health']",2025-06-12 14:59:33
Senior Engineer - Data Science,Sasken Technologies,2 - 5 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position has gained significant work experience to be able to apply their knowledge effectively and deliver results. Person at this position is also able to demonstrate the ability to analyse and interpret complex problems and improve change or adapt existing methods to solve the problem.\nPerson at this position regularly interacts with interfacing groups / customer on technical issue clarification and resolves the issues. Also participates actively in important project/ work related activities and contributes towards identifying important issues and risks. Reaches out for guidance and advice to ensure high quality of deliverables.\nPerson at this position consistently seek opportunities to enhance their existing skills, acquire more complex skills and work towards enhancing their proficiency level in their field of specialisation.\nWorks under limited supervision of Team Lead/ Project Manager.\n\n\nRoles & Responsibilities\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals. Responsible for adhering to guidelines and checklists for all deliverable reviews, sending status report to team lead and following relevant organizational processes. Responsible for customer collaboration and interactions and support to customer queries. Expected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments. Expected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\n\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 2-5 years\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTechnology Standard-\nNA\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Spark', 'machine learning', 'Python']",2025-06-12 14:59:35
Senior Big Data Engineer,Qualcomm,2 - 7 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\nGeneral Summary\n\nPreferred Qualifications\n\n3+ years of experience as a Data Engineer or in a similar role\n\nExperience with\n\ndata modeling, data warehousing, and building ETL pipelines\n\nSolid working experience with\n\nPython, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,)\n\nExperience with\n\nBig Data tools, platforms and architecture with solid working experience with SQL\n\nExperience working in a very large data warehousing environment,\n\nDistributed System.\n\nSolid understanding on various data exchange formats and complexities\n\nIndustry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets\n\nStrong data visualization skills\n\nBasic understanding of Machine Learning; Prior experience in ML Engineering a plus\n\nAbility to manage on-premises data and make it inter-operate with AWS based pipelines\n\nAbility to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus\n\n\nEducation\n\nBachelor's degree in computer science, engineering, mathematics, or a related technical discipline\n\nPreferred QualificationsMasters in CS/ECE with a Data Science / ML Specialization\n\n\nMinimum Qualifications:\n\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\n\nOR\n\nMaster's degree in Engineering, Information Systems, Computer Science, or related field\n\nOR\n\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n3+ years of experience with Programming Language such as C, C++, Java, Python, etc.\n\n\nDevelops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design.\n\nPrincipal Duties and Responsibilities:\n\nCompletes assigned coding tasks to specifications on time without significant errors or bugs.\n\nAdapts to changes and setbacks in order to manage pressure and meet deadlines.\n\nCollaborates with others inside project team to accomplish project objectives.\n\nCommunicates with project lead to provide status and information about impending obstacles.\n\nQuickly resolves complex software issues and bugs.\n\nGathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions.\n\nSeeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently.\n\nParticipates in technical conversations with tech leads/managers.\n\nAnticipates and communicates issues with project team to maintain open communication.\n\nMakes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks.\n\nPrioritizes project deadlines and deliverables with minimal supervision.\n\nResolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues).\n\nWrites readable code for large features or significant bug fixes to support collaboration with other engineers.\n\nDetermines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner.\n\nUnit tests own code to verify the stability and functionality of a feature.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'sql', 'software engineering', 'data visualization', 'aws', 'quicksight', 'c', 'software development', 'glue', 'aws sagemaker', 'data warehousing', 'machine learning', 'business intelligence', 'data engineering', 'java', 'data science', 'data modeling', 'athena', 'wireless', 'big data', 'etl', 'ml']",2025-06-12 14:59:38
Data Scientist (Offshore),HTC Global Services,2 - 7 years,Not Disclosed,['Chennai'],"We are seeking a Data Scientist (Offshore) with minimum experience of 3 or more years. The ideal candidate should be familiar with relational or NoSQL databases such as Oracle, Teradata, SQL Server, Hadoop and ELK etc.\nRequirements:\nMinimum 3 or more years working with languages such as R, Python or Java\nAt least 3 or more years working with advanced statistical methods such as regressions, classifiers, recommenders, anomaly detection, optimization algorithms, tree methods and neural nets etc.",,,,"['tableau', 'NoSQL', 'Hadoop', 'Agile', 'Teradata SQL', 'data visualization', 'Oracle', 'Powerpoint', 'SDLC', 'Python']",2025-06-12 14:59:40
"Lead, Engineer",Qualcomm,10 - 15 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nJob Overview:\n\nThe Qualcomm Cloud Computing team is developing hardware and software for Machine Learning solutions spanning the data center, edge, infrastructure, automotive market. We are seeking ambitious, bright, and innovative engineers with experience in machine learning framework development. Job activities span the whole product life cycle from early design to commercial deployment. The environment is fast-paced and requires cross-functional interaction daily so good communication, planning and execution skills are a must.\n\nWe are seeking a highly skilled and motivated Language Model Engineer to join our team. The primary role of the engineer will be to train Large Language Models (LLMs) from scratch and fine-tune existing LLMs on various datasets using state-of-the-art techniques.\n\nResponsibilities:\n\nModel Training and Fine-tuning:\n\nTrain LLMs from scratch using various datasets. Fine-tune pre-trained models on specific tasks or datasets to improve performance. Implement state-of-the-art LLM training techniques such as Reinforcement Learning from Human Feedback (RLHF), ZeRO (Zero Redundancy Optimizer), Speculative Sampling, and other speculative techniques.\n\n\nData Management: Handle large datasets effectively. Ensure data quality and integrity. Implement data cleaning and preprocessing techniques. Hands-on with EDA is a plus.\n\n\nModel Evaluation: Evaluate model performance using appropriate metrics. Understand the trade-offs between different evaluation metrics.\n\n\nLLM metrics: Sound understanding of various LLM metrics like MMLU, Rouge, BLEU, Perplexity etc.\n\n\nAWQ: Understanding of Quantization is a plus. Knowledge on QAT will be a plus.\n\n\nResearch and Development: Stay updated with the latest research in NLP and LLMs. Implement state-of-the-art techniques and contribute to research efforts.\n\n\nCollaboration: Work closely with other teams to understand requirements and implement solutions.\n\n\nRequired Skills and Experience:\n\n\n\nDeep Learning Frameworks: Hands-on experience with PyTorch at a granular level. Familiarity with tensor operations, automatic differentiation, and GPU acceleration in PyTorch.\n\n\nNLP and LLMs: Strong understanding of Natural Language Processing (NLP) and experience working with LLMs.\n\n\nProgramming: Proficiency in Python and experience with software development best practices.\n\n\nData Handling: Experience working with large datasets. Familiarity with data version control tools is a plus.\n\n\nEducation: A degree in Computer Science, Machine Learning, AI, or related field. Advanced degree is a plus.\n\n\nCommunication: Excellent written and verbal communication skills.\n\n\nWork experience : Open, 2- 10 years of relevant experience.\n\n\n\n\nPreferred\n\nSkills:\n\n\n\n\nOptimization: Knowledge of optimization techniques for training large models.\n\n\nNeural Architecture Search (NAS): Experience with NAS techniques for optimizing model architectures is a plus. Hands-on experience with CUDA, CUDNN is a plus.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'c++', 'natural language processing', 'pytorch', 'software engineering', 'cuda', 'nas', 'eda', 'version control', 'sampling', 'machine learning', 'deep learning', 'java', 'data center', 'computer science', 'product life cycle', 'research and development', 'machine learning algorithms']",2025-06-12 14:59:43
Lead AI/ML Engineer,Synechron,8 - 12 years,Not Disclosed,['Bengaluru'],"job requisition idJR1027508\n\nOverall Responsibilities:\nExperience with Machine Learning, Deep Learning, Computer Vision, NLP, Generative AI.\nKnowledge of algorithms, object-oriented and functional design principles, and best-practice patterns\nSolid understanding of common programming languages used in AI, such as Python, Java, C++, and R\nAdvanced knowledge of statistical and algorithmic models as well as of fundamental mathematical concepts, such as linear algebra and probability\nExperience working with large data sets and writing efficient code capable of processing large data streams at speed.\n\n\nSkills:\nMachine Learning, Deep Learning, Computer Vision, NLP\nStrong hands-on experience in AI/ML/Data Science, including machine learning algorithms, deep learning, NLP, computer vision, etc.\nKnowledge of programming languages such as Python, R, SQL, etc.\nExperience with AI/ML/Data Science tools and frameworks such as TensorFlow, PyTorch, etc.\nExcellent problem-solving skills and ability to find creative solutions to complex data science problems.\nStrong communication and interpersonal skills to effectively collaborate with cross-functional teams and clients\nExperience:\nMinimum 8-12 years of experience in AI/ML/Data Science, with at least 8+ years in a leadership role\nProven track record of successfully delivering AI/ML/Data Science projects\nExperience in mentoring and leading a team of AI/ML/Data Science professionals\nDay-to-Day Activities:\nLead AI/ML/Data Science projects, ensuring successful delivery\nMentor and guide junior team members\nCollaborate with cross-functional teams to provide subject matter expertise in AI/ML/Data Science\nStay updated with latest AI/ML/Data Science trends and technologies\nIdentify and evaluate new business opportunities in AI/ML/Data Science\nQualification:\nBachelor's or Master's degree in Computer Science, Data Science, AI, or related field\nSoft\n\nSkills:\nExcellent leadership and team management skills\nStrong interpersonal and communication skills\nAbility to work in a fast-paced and dynamic environment\nGood negotiation and stakeholder management skills\nPassionate about AI/ML/Data Science and staying updated with latest trends and technologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['team management', 'aiml', 'artificial intelligence', 'data science', 'ml', 'algorithms', 'python', 'c++', 'data analytics', 'natural language processing', 'machine learning', 'sql', 'deep learning', 'tensorflow', 'r', 'java', 'computer vision', 'pytorch', 'machine learning algorithms', 'statistics']",2025-06-12 14:59:46
Lead Software Engineering - Python Developer,JPMorgan Chase Bank,1 - 9 years,Not Disclosed,['Bengaluru'],"Are you ready to elevate your career in software engineeringJoin our dynamic team as a Python Developer, where your expertise will drive cutting-edge solutions and contribute to impactful projects. We offer unparalleled opportunities for career growth and a collaborative environment where you can thrive and make a significant impact.\nAs a Lead Software Engineer at JPMorgan Chase within the Technology and Engineering division, you will execute software solutions, design, and development, collaborating with cross-functional teams to deploy machine learning services. You will be responsible for producing architecture and design artifacts for complex applications, ensuring design constraints are met. Your role will involve contributing to the engineering community and influencing the use of leading-edge technologies.\nJob Responsibilities\nExecute software solutions, design, development, and technical troubleshooting, thinking beyond routine approaches.\nCreate secure and high-quality production code, maintaining algorithms that run synchronously with systems.\nProduce architecture and design artifacts for complex applications, ensuring design constraints are met.\nCollaborate with cross-functional teams, including Data Science partners, to design and deploy machine learning services.\nContribute to the engineering community as an advocate of firmwide frameworks, tools, and practices.\nInfluence peers and project decision-makers to consider leading-edge technologies.\nAdd to the team culture of diversity, equity, inclusion, and respect.\nRequired Qualifications, Capabilities, and Skills\nFormal training or certification on Software Engineering concepts and 5+ years applied experience.\nExperience in building complex software systems in both private and public cloud environments (AWS).\nHands-on practical experience delivering system design, application development, testing, and operational stability.\nAdvanced Python Programming Skills including Pandas, Numpy.\nProficiency with AIM algorithms.\nAdvanced knowledge of software applications and technical processes in technical disciplines (e. g. , cloud, AI, ML).\nAbility to tackle design and functionality problems independently with little oversight.\nPreferred Qualifications, Capabilities, and Skills\nAdvanced skills in additional programming languages (Java).\nFamiliarity with building services and consuming data via GraphQL, REST, or gRPC and SQL\nExperience in building and deploying machine learning models, with knowledge of the ML Lifecycle; expertise in MLOps and AIOps is an advantage.\nWorking knowledge of security best practices and compliance standards for machine learning systems.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Lead Software', 'Machine learning', 'System design', 'Deployment', 'Application development', 'Troubleshooting', 'Operations', 'Software solutions', 'SQL', 'Python']",2025-06-12 14:59:48
Machine Learning Scientist (LLM),Wadhwani Ai,6 - 11 years,Not Disclosed,['Mumbai'],"SUMMARY\nWe are looking for a Machine Learning Scientist who is passionate about aiding India s emergence into an AI superpower and about applying AI and ML to solve high-impact societal challenges, especially in the area of large language models (LLMs). The ideal candidate will have a strong research or applied background with 6+ years of relevant experience or a PhD, and expertise in designing and implementing machine learning models, including LLMs, data analysis, and simulations. They should enjoy interfacing with disparate internal and external stakeholders, mentoring junior scientists and developers, working in cross-functional teams, and contributing to ML evaluations and practical deployments. A demonstrated commitment to social good, strong communication skills, and the ability to translate complex problems into deployable ML solutions are essential.\nABOUT US - https://www.wadhwaniai.org/\nWadhwani AI is a nonprofit institute building and deploying applied AI solutions to solve critical issues in public health, agriculture, education, and urban development in underserved communities in the global south. We collaborate with governments, social sector organizations, academic and research institutions, and domain experts to identify real-world problems, and develop practical AI solutions to tackle these issues with the aim of making a substantial positive impact.\nPertinent to this role, Wadhwani AI also works closely with the Ministry of Electronics and Information Technology (MEITy), Government of India, in helping flesh out the various pillars of the India AI mission.\nWe have over 30+ AI projects supported by leading philanthropies such as Bill & Melinda Gates Foundation, USAID and Google.org. With a team of over 200 professionals, our expertise encompasses AI/ML research and innovation, software engineering, domain knowledge, design and user research.\nIn the Press:\nOur Founder Donors are among the Top 100 AI Influencers\nG20 India s Presidency: AI Healthcare, Agriculture, & Education Solutions Showcased Globally.\nUnlocking the potentials of AI in Public Health\nWadhwani AI Takes an Impact-First Approach to Applying Artificial Intelligence - data.org\nWinner of the H&M Foundation Global Change Award 2022\nIndian Winners of the 2019 Google AI Impact Challenge, and the first in the Asia Pacific to host Google Fellows\nROLES AND RESPONSIBILITIES\nAs an ML Scientist working with our team at the India AI Mission, you will be responsible for critically evaluating LLM solutions, building machine learning solutions to problems of societal importance, and mentoring other team members in this effort. You will participate in stakeholder engagements, setting critical requirements, problem definition and the development and execution of algorithms and solutions to the problems.\nYou will need to understand user challenges and their context, curate and transform data, train and validate models, run simulations, and broadly derive insights from data. In doing so, you will work in cross-functional teams spanning administration, engineering, solutions, domain experts, and designers. You will also work closely with social sector organisations and are encouraged to collaborate with researchers across the world.\nREQUIREMENTS\nThis role requires the candidate to work out of the India AI Mission office at MEITy, Government of India.\nM.Tech./M.E./M.S./M.Sc. or equivalent in Computer Science, Electrical Engineering, Statistics, Applied Mathematics, Physics, Economics or a relevant quantitative field with work experience of 6+ years or a PHd from a reputed institute.\nWe are looking for ML Scientists with experience applying AI, machine learning, and data science to real-world problems. Ideal candidates should have a strong research background, be familiar with the state of the art especially in foundational models and LLMs, have experience in mentoring junior scientists and developers through strong code practices and ideation, and be adept at a variety of data mining/analysis methods and tools, building and implementing models, visualising data, creating/using algorithms, and running simulations.\nCandidates should be completely comfortable working with cross-functional teams in an uncertain and fast-paced environment, and must have excellent oral and written communication skills and a track record of driving projects to completion. Evidence of written skills through high quality research publications is a plus.\nCandidates should be passionate about using their technical skills to solve large societally important problems.",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'data science', 'Artificial Intelligence', 'Machine learning', 'Healthcare', 'Research', 'Data mining', 'Information technology', 'Public health']",2025-06-12 14:59:51
Security and Access control-Lead Engineer,Qualcomm,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Systems Engineer, you will research, design, develop, simulate, and/or validate systems-level software, hardware, architecture, algorithms, and solutions that enables the development of cutting-edge technology. Qualcomm Systems Engineers collaborate across functional teams to meet and exceed system-level requirements and standards.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.\n\nJob Overview\n\nWork with Qualcomm's security architecture / IP and access control team on next generation SOC for smartphone, tablet, automotive and IOT product categories. is responsible for assisting product development teams throughout the company to apply secure HW design principles to individual blocks, computing cores, and at the SoC level. SW/HW co-design, HW development experience. Familiarity with debug architectures such as JTAG and ARM coresight are a plus\n\nSuccessful candidates will be able to engage with product teams independently with minimal supervision to detect and mitigate security vulnerabilities in hardware architecture and implementations, involve in access control issues at both SW and HW.\n\nMinimum Qualifications\n\n5 to 7+ years of industry or academic experience in Security are required.\n\nAdditionally,",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['vhdl', 'hypervisor', 'system verilog', 'system engineering', 'verilog', 'jtag', 'arm architecture', 'c++', 'storage domain', 'soc', 'artificial intelligence', 'power analysis', 'fpga', 'access control', 'i2c', 'ml', 'asic', 'python', 'side', 'c', 'systemc', 'cryptography', 'aiml', 'spi', 'asic design', 'embedded c', 'uart']",2025-06-12 14:59:53
Data Science & AI Engineer,Blue Altair,5 - 8 years,Not Disclosed,['Pune'],"Greetings from Blue Altair!\nJob Overview:\nWe are seeking an experienced and highly skilled Data Science and AI Engineer to join our dynamic team. The ideal candidate will have 5+ years of experience working on cutting-edge data science and AI technologies across various cloud platforms with a strong focus to work on LLMs and SLMs. The role demands a professional capable of performing in a client-facing environment, as well as mentoring and guiding junior team members.\n\nTitle: Consultant/Sr. Consultant - Data Science Engineer\nExperience: 5-8 years\nLocation: Pune/Bangalore (Hybrid)\n\nRoles and responsibilities:\nDevelop, implement, and optimize machine learning models and AI algorithms to solve complex business problems.\nDesign, build, and fine-tune AI models, particularly focusing on LLMs and SLMs, using state-of-the-art techniques and architectures.\nApply advanced techniques in prompt engineering, model fine-tuning, and optimization to tailor models for specific business needs.\nDeploy and manage machine learning models and pipelines on cloud platforms (AWS, GCP, Azure, etc.).\nWork closely with clients to understand their data and AI needs and provide tailored solutions.\nCollaborate with cross-functional teams to integrate AI solutions into broader software architectures.\nMentor junior team members and provide guidance in implementing best practices in data science and AI development.\nStay up-to-date with the latest trends and advancements in data science, AI, and cloud technologies.\nPrepare technical documentation and present insights to both technical and non-technical stakeholders.\n\nRequirement:\n5+ years of experience in data science, machine learning, and AI technologies.\nProven experience working with cloud platforms such as Google Cloud, Microsoft Azure, or AWS.\nExpertise in programming languages such as Python, R, Julia, and AI frameworks like TensorFlow, PyTorch, Scikit-learn, Hugging face Transformers.\nKnowledge of data visualization tools (e.g., Matplotlib, Seaborn, Tableau)\nSolid understanding of data engineering concepts including ETL, data pipelines, and databases (SQL, NoSQL).\nExperience with MLOps practices and deployment of models in production environments.\nFamiliarity with NLP (Natural Language Processing) tasks and working with large-scale datasets.\nHands-on experience with generative AI models like GPT, Gemini, Claude, Mistral etc.\nClient-facing experience with strong communication skills to manage and engage stakeholders.\nStrong problem-solving skills and analytical mindset.\nAbility to work independently and as part of a team and mentor and provide technical leadership to junior team members.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLMs', 'Artificial Intelligence', 'MLOps', 'RAG', 'Natural Language Processing', 'Neural Networks', 'LLM', 'Machine Learning', 'AI Models', 'Data Science', 'PyTorch', 'SLM', 'AI Automation']",2025-06-12 14:59:56
V&V Vehicle System Test Lead Engineer (Staff - AD/ADAS),Qualcomm,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Test Engineering\n\nGeneral Summary:\n\nAt Qualcomm, we are transforming the automotive industry with our Snapdragon Digital Chassis and building the next generation software defined vehicle (SDV).\n\nSnapdragon Ride is an integral pillar of our Snapdragon Digital Chassis, and since its launch it has gained momentum with a growing number of global automakers and Tier1 suppliers. Snapdragon Ride aims to address the complexity of autonomous driving and ADAS by leveraging its high-performance, power-efficient SoC, industry-leading artificial intelligence (AI) technologies and pioneering vision and drive policy stack to deliver a comprehensive, cost and energy efficient systems solution.\n\nThe Software Test and Quality infrastructure team is centrally defining, establishing, and rolling out the software test frameworks and software quality infrastructure used by multiple projects within Automated Driving.\n\nWe are looking for smart, innovative, and motivated individuals with strong vehicle testing background and experience to test the ADAS SW platform. The chosen candidate will get an opportunity to lead ADAS vehicle validation team working for a leading Indian car manufacturer.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Test Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Test Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Test Engineering or related work experience.\n\n2+ years of work or academic experience with Software Test or System Test, developing and automating test plans and/or tools (e.g., Source Code Control Systems, Continuous Integration Tools, and Bug Tracking Tools).\nPrepare test strategy based on customer requirement/Tier1 Catalogue. Review on and Test Cases.\n\nPlan and execute multiple level testing smoke test, L0/L1 testing, software in loop testing and vehicle testing for AD entry +/ Mid\n\nDesigns test plans, test cases, test scenarios, scripts, or procedures with the target to ensure the best coverage of the requirements for features.\n\nResponsible for preparing consolidated test report with test coverage, Known issues, functional/nonfunctional test results, observations, and bugs\n\nReprocess and analyze the events regression testing in application dataset from OEM project.\n\nSupport team to test in vehicle the System integration OEM specific hardware, error guessing, configurability testing, issue reproducibility, exploratory and feature combining tests for ADAS SW platform for features like:\n\no Adaptive Cruise Control\n\no Autonomous Emergency Braking\n\no Collision avoidance features (Lane Support System, Traffic Assist, Risk Mitigation Support)\n\no Road Sign Information\n\nDocuments systems-level defects, using a bug tracking system, and report defects to developers.\n\nIdentifies, analyzes, troubleshoots, and documents problems with program function, output, or content. Develops testing programs that assess effectiveness of a new system or modification of an existing system.\n\nAssure that the project is developed according to Qualcomm quality standards.\n\nPreferred Qualifications:\n\nBachelor's degree in Computer Science, Informatics or equivalent.\n\nMinimum of 7+ years of relevant work experience.\n\nKnowledge of CAN/Ethernet communication protocol experience with the associated tools form Vector (i.e. CANoe, CANalyzer, CANdela) and Star Corporation tools (i.e. Fl3X)\n\nExperience with flashing ADAS systems\n\nFamiliarity with C, CAPL programming languages\n\nExcellent analytical skills\n\nDriver Certification\n\nNice to have:\n\nAdvanced pilot passenger vehicle tests driver certification\n\nExperience in designing test cases and test automation solutions.\n\nGIT knowledge\n\nBasic C++ Programming\n\nPython scripting\n\nContinuous Integration knowledge\n\nISTQB certification\n\nAgile mindset and experience with SCRUM",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['canoe', 'test engineering', 'system testing', 'canalyzer', 'automating', 'c++', 'redhat openshift', 'hiring', 'candela', 'staffing', 'git', 'data warehouse testing', 'microsoft visual studio', 'can bus', 'python', 'c', 'software testing', 'svn', 'siebel', 'seibel', 'ethernet', 'capl', 'visio', 'siebel crm', 'sdlc', 'mqc']",2025-06-12 14:59:58
Automotive Software Lead Engineer Sr. - C/C++,Qualcomm,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAt Qualcomm, we are transforming the automotive industry with our Snapdragon Digital Chassis and building the next generation software defined vehicle (SDV).\n\nSnapdragon Ride is an integral pillar of our Snapdragon Digital Chassis, and since its launch it has gained momentum with a growing number of global automakers and Tier1 suppliers. Snapdragon Ride aims to address the complexity of autonomous driving and ADAS by leveraging its high-performance, power-efficient SoC, industry-leading artificial intelligence (AI) technologies and pioneering vision and drive policy stack to deliver a comprehensive, cost and energy efficient systems solution.\n\nEnabling safe, comfortable, and affordable autonomous driving includes solving some of the most demanding and challenging technological problems. From centimeter-level localization to multimodal sensor perception, sensor fusion, behavior prediction, maneuver planning, and trajectory planning and control, each one of these functions introduces its own unique challenges to solve, verify, test, and deploy on the road.\n\nWe are looking for smart, innovative, and motivated individuals with strong SW background and programming experience with languages such as C/C++, python, and more. Job responsibilities include design and development of SW framework and middleware. Development of sensor drivers to bring in sensors (IMU, GPS, Camera, Radar, Lidar, Ultrasonic) to our platform, sensor synchronization, and efficient techniques to share sensor across different SW modules. Work closely with different teams to implement SW optimization on Snapdragon Ride platform as well as involved in Ride SDK development. Work closely with test engineers to develop test plans and validation of SW. Will be\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n5 to 8 years of C++ development, C++11 and C++14 is a strong plus\n\nEmbedded SW design and development for safety critical systems is a strong plus\n\nExperience with Programming languages such as C++, Python, Shell, etc.\n\nExperience with multi-threaded / multi-core SW development and design\n\nKnowledge/experience on Linux and embedded platform with QNX, AGL, Safe Linux, etc.\n\nKnowledge of Linux network stack and any experience with network device drivers is a plus\n\nFamiliarity with ROS/ROS2, DDS, Adaptive AUTOSAR middleware and frameworks\n\nKnowledge / experience with safety critical software development process (Functional Safety), including ASPICE, ASIL, ISO26262, MISRA C++, AUTOSAR C++\n\nFamiliarity with static analysis tools, code coverage metrics, unit test generation\n\nExperience with source code management tools such as git, git-lfs, github/gitlab\n\nExcellent written and verbal communication skills, ability to work with a cross-functional",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'java', 'linux', 'software engineering', 'qnx', 'dd', 'sw design', 'python', 'github', 'aspice', 'c', 'embedded sw', 'device drivers', 'static analysis', 'embedded sw development', 'autosar', 'rts', 'git', 'misra', 'adas', 'shell scripting', 'multithreading', 'gitlab', 'functional safety']",2025-06-12 15:00:01
"Embedded Platform Dev- Lead Engineer, Senior",Qualcomm,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nQualcomm ADAS/Autonomy team is engaged in offering optimized solutions built on DSP, computer vision and machine learning algorithms for the Qualcomm ADAS/Autonomy SoCs. We are seeking engineers with experience in system and SoC SW level functional safety concepts. The job requires understanding and defining of the Safety Concept and Architecture, Software Safety requirements, defining and deploying safety processes and development of Safety software by following the ISO26262 software processes. Interaction with customers, architects and test/integration teams are required as part of the job. The job also involves working with the Software quality team for adherence of ISO26262 and ASPICE processes.\n\nIn this role, the candidate will work with local and global teams to understand, define and implement and productize Automotive specific features including software enablement (drivers/BSP/RTOS/AUTOSAR MCAL), security, functional safety, and power applied to Automotive products on our current and next generation SoCs. The candidate will also have the responsibility to coordinate and execute plans which will encompass validation of all the feature requirements. The Candidate will have the responsibility to identify and address any abnormal discoveries by root-causing and providing detailed corrective actions in the form of optimizations and/or fixes. When possible, the candidate is expected to prototype and pre-validate recommended fixes. Additionally, the candidate will be responsible for any automation of design under test along with validation efforts and working closely with design/production/bench IP teams.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\n 3-6 years of Embedded Software Development experience, including low level drivers, and RTOS \n\n The candidate should possess 3 to 6 years of industry experience in embedded software driver development and having expertise in one or more below areas would be preferred: \n\n Should be able to ramp up fast and must have the attitude to work with the team. \n\n Strong C and Assembly Programming with OS & Multi-Processor concepts \n\n Embedded software development in C and C++ on ARM or similar cores. \n\n Hands on experience of driver development on any RTOS, \n\n Experience in SafeRTOS/FreeRTOS based development is nice to have \n\n Experience in Autosar MCAL development is nice to have \n\n Experience in Autosar BSW integration and validation is nice to have \n\n ARM Trust-Zone & ARMv7/v8 architecture. \n\n Good debugging skills with experience on debugging with Lauterbach JTAG debuggers. \n\n Work on challenging customer requirements and issues. \n\n Basic understanding one or more of hardware blocks - Clocks, PLLs, GPIO, Interrupt Controllers (GIC), Peripherals (SPI/I2C/UART/CAN/Ethernet/Clock/etc)  \n\n Automotive SW development experience is must have \n\n Experience in ISO26262/functional safety and ASPICE is highly desirable \n\n Basic knowledge on Power Mgmt. IC is desirable \n\n Knowledge of Software/Hardware Security concepts is desirable \nClosely work with the hardware team to contribute/suggest modifications to the hardware design.\nAny past working experience on Qualcomm chips nice to have",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['embedded software development', 'assembly programming', 'c', 'sw', 'embedded software', 'dsp', 'algorithms', 'jtag', 'c++', 'aspice', 'freertos', 'rtos', 'java', 'debugging', 'software engineering', 'ic', 'i2c', 'can bus', 'arm', 'functional safety', 'python', 'spi', 'autosar', 'ethernet', 'mcal', 'uart', 'adas', 'bsp']",2025-06-12 15:00:04
Senior Data Engineer,Qualcomm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Data Engineer\n\nGeneral Summary:\n\nDeveloper will play an integral role in the PTEIT Machine Learning Data Engineering team. Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics. Design, develop and support CI/CD of data pipelines and services. - 5+ years of experience with Python or equivalent programming using OOPS, Data Structures and Algorithms - Develop new services in AWS using server-less and container-based services. - 3+ years of hands-on experience with AWS Suite of services (EC2, IAM, S3, CDK, Glue, Athena, Lambda, RedShift, Snowflake, RDS) - 3+ years of expertise in scheduling data flows using Apache Airflow - 3+ years of strong data modelling (Functional, Logical and Physical) and data architecture experience in Data Lake and/or Data Warehouse - 3+ years of experience with SQL databases - 3+ years of experience with CI/CD and DevOps using Jenkins - 3+ years of experience with Event driven architecture specially on Change Data Capture - 3+ years of Experience in Apache Spark, SQL, Redshift (or) Big Query (or) Snowflake, Databricks - Deep understanding building the efficient data pipelines with data observability, data quality, schema drift, alerting and monitoring. - Good understanding of the Data Catalogs, Data Governance, Compliance, Security, Data sharing - Experience in building the reusable services across the data processing systems. - Should have the ability to work and contribute beyond defined responsibilities - Excellent communication and inter-personal skills with deep problem-solving skills.\n\nMinimum Qualifications:\n3+ years of IT-related work experience with a Bachelor's degree in Computer Engineering, Computer Science, Information Systems or a related field.\nOR\n5+ years of IT-related work experience without a Bachelors degree.\n\n2+ years of any combination of academic or work experience with programming (e.g., Java, Python).\n1+ year of any combination of academic or work experience with SQL or NoSQL Databases.\n1+ year of any combination of academic or work experience with Data Structures and algorithms.\n5 years of Industry experience and minimum 3 years experience in Data Engineering development with highly reputed organizations- Proficiency in Python and AWS- Excellent problem-solving skills- Deep understanding of data structures and algorithms- Proven experience in building cloud native software preferably with AWS suit of services- Proven experience in design and develop data models using RDBMS (Oracle, MySQL, etc.)\n\nDesirable - Exposure or experience in other cloud platforms (Azure and GCP) - Experience working on internals of large-scale distributed systems and databases such as Hadoop, Spark - Working experience on Data Lakehouse platforms (One House, Databricks Lakehouse) - Working experience on Data Lakehouse File Formats (Delta Lake, Iceberg, Hudi)\n\nBachelor's or Master's degree in Computer Science, Software Engineering, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data quality', 'data structures', 'aws', 'schema', 'continuous integration', 'glue', 'amazon redshift', 'event driven architecture', 'ci/cd', 'data engineering', 'sql', 'alerts', 'java', 'data modeling', 'spark', 'devops', 'data flow', 'nosql databases', 'sql database']",2025-06-12 15:00:06
Display System Performance - Lead Engineer,Qualcomm,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nJob Summary:\n\nWe are seeking a highly motivated and skilled Performance and Power Analysis Engineer to join our Display Systems team in Bengaluru. In this critical role, you will be responsible for the analysis, modeling, and optimization of performance and power consumption across various stages of our cutting-edge chip development process. You will take the lead and collaborate closely with architecture, design, Software and verification teams to ensure our products meet stringent performance targets and power efficiency requirements. As an independent collaborator, contribute with cross functional teams, SoC performance and SW/HW teams to enhance or optimize the process. This is an exciting opportunity to contribute to the development of next-generation semiconductor technology.\n\nResponsibilities:\nDevelop and maintain architectural-level and/or cycle-accurate models for performance and power estimation.\nAnalyze trade-offs between performance, power, and area (PPA) at the architecture and microarchitecture levels.\nDrive performance and power analysis early in the design cycle to influence architecture and design decisions.\nCollaborate with architecture and design teams to explore and evaluate different design options and trade-offs to optimize performance and power.\nConduct detailed analysis to identify performance bottlenecks and power inefficiencies in chip architectures and microarchitectures.\nPerform power profiling and characterization of designs under various operating conditions and workloads.\nDevelop and implement power reduction techniques at different design stages (e.g., clock gating, power gating, voltage scaling).\nAnalyze and debug performance and power-related issues during simulation, emulation, and silicon bring-up.\nGenerate comprehensive reports and presentations summarizing analysis results and providing actionable recommendations to the design teams, cross-functional teams and senior leadership.\nStay abreast of the latest industry trends, tools, and methodologies in performance and power analysis.\nContribute to the development and improvement of internal tools and flows for performance and power analysis.\nCollaborate with verification teams to define and execute performance and power validation plans.\nValidate model accuracy through correlation with RTL simulations, emulation, and silicon measurements.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.\n\nQualifications:\nBachelor's or Master's degree in Electrical Engineering, Computer Engineering, or a related field.\n5 to 8+ years of experience in performance and power analysis for ASIC or SoC designs.\nStrong understanding of computer architecture, microarchitecture, and digital design principles.\nStrong experience in developing and utilizing performance and power models using languages such as SystemC, Python, C++, or custom in-house frameworks.\nProficiency in using industry-standard performance and power analysis tools (e.g., Synopsys PrimeTime PX)\nSolid understanding of power management techniques and low-power design methodologies.\nExperience with simulation and emulation environments.\nStrong analytical and problem-solving skills with the ability to interpret complex data and draw meaningful conclusions.\nExcellent communication and interpersonal skills with the ability to collaborate effectively with cross-functional teams.\nFamiliarity with silicon bring-up and post-silicon power/performance characterization is a plus.\nExperience with machine learning techniques for power/performance prediction is a plus.\nExperience with IOS and Xcode profiling/development is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['digital design', 'microservices', 'computer architecture', 'analysis tools', 'design principles', 'asic', 'python', 'c++', 'systemc', 'xcode', 'ios', 'machine learning', 'power analysis', 'silicon', 'power management', 'soc design', 'machine learning algorithms', 'system engineering']",2025-06-12 15:00:09
Big Data Developer/Data Engineer,Grid Dynamics,5 - 10 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nExperience: 5 - 8 years\nEmployment Type: Full-Time\n\nJob Summary:\nWe are looking for a highly skilled Scala and Spark Developer to join our data engineering team. The ideal candidate will have strong experience in building scalable data processing solutions using Apache Spark and writing robust, high-performance applications in Scala. You will work closely with data scientists, data analysts, and product teams to design, develop, and optimize large-scale data pipelines and ETL workflows.\n\nKey Responsibilities:\nDevelop and maintain scalable data processing pipelines using Apache Spark and Scala.\nWork on batch and real-time data processing using Spark (RDD/DataFrame/Dataset).\nWrite efficient and maintainable code following best practices and coding standards.\nCollaborate with cross-functional teams to understand data requirements and implement solutions.\nOptimize performance of Spark jobs and troubleshoot data-related issues.\nIntegrate data from multiple sources and ensure data quality and consistency.\nParticipate in design reviews, code reviews, and provide technical leadership when needed.\nContribute to data modeling, schema design, and architecture discussions.\nRequired Skills:\nStrong programming skills in Scala.\nExpertise in Apache Spark (Core, SQL, Streaming).\nHands-on experience with distributed computing and large-scale data processing.\nExperience with data formats like Parquet, Avro, ORC, and JSON.\nGood understanding of functional programming concepts.\nFamiliarity with data ingestion tools (Kafka, Flume, Sqoop, etc.).\nExperience working with Hadoop ecosystem (HDFS, Hive, YARN, etc.) is a plus.\nStrong SQL skills and experience working with relational and NoSQL databases.\nExperience with version control tools like Git.\nPreferred Qualifications:\nBachelor's or Masters degree in Computer Science, Engineering, or related field.\nExperience with cloud platforms like AWS, Azure, or GCP (especially EMR, Databricks, etc.).\nKnowledge of containerization (Docker, Kubernetes) is a plus.\nFamiliarity with CI/CD tools and DevOps practices.ndidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Pyspark', 'Spark']",2025-06-12 15:00:11
Senior Data Engineer - AWS,Blend360 India,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nQualifications\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 15:00:14
Data Engineer,Grid Dynamics,4 - 9 years,Not Disclosed,['Bengaluru'],"Required Qualifications:\n4+ years of professional experience in data engineering and data analysis roles.\nStrong proficiency in SQL and experience with database management systems such as MySQL, PostgreSQL, Oracle, and MongoDB.\nHands-on experience with big data tools like Hadoop and Apache Spark.\nProficient in Python programming.\nExperience with data visualization tools such as Tableau, Power BI, and Jupyter Notebooks.\nProven ability to design, build, and maintain scalable ETL pipelines using tools like Apache Airflow, DBT, Composer (GCP), Control-M, Cron, and Luigi.\nFamiliarity with data engineering tools including Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nExperience working with cloud data warehouses and services (Snowflake, Redshift, BigQuery, AWS Glue, GCP Dataflow, Azure Data Factory).\nUnderstanding of data modeling concepts and data lake/data warehouse architectures.\nExperience supporting CI/CD practices with Git, Docker, Terraform, and DevOps workflows.\nKnowledge of both relational and NoSQL databases, including PostgreSQL, BigQuery, MongoDB, and DynamoDB.\nExposure to Agile and DevOps methodologies.\nExperience with at least one cloud platform:\nGoogle Cloud Platform (BigQuery, Dataflow, Composer, Cloud Storage, Pub/Sub)\nAmazon Web Services (S3, Glue, Redshift, Lambda, Athena)\nMicrosoft Azure (Data Factory, Synapse Analytics, Blob Storage)\nEssential functions\nKey Responsibilities:\nDesign, develop, and maintain robust, scalable ETL pipelines using Apache Airflow, DBT, Composer (GCP), Control-M, Cron, Luigi, and similar tools.\nBuild and optimize data architectures including data lakes and data warehouses.\nIntegrate data from multiple sources ensuring data quality and consistency.\nCollaborate with data scientists, analysts, and stakeholders to translate business requirements into technical solutions.\nAnalyze complex datasets to identify trends, generate actionable insights, and support decision-making.\nDevelop and maintain dashboards and reports using Tableau, Power BI, and Jupyter Notebooks for visualization and pipeline validation.\nManage and optimize relational and NoSQL databases such as MySQL, PostgreSQL, Oracle, MongoDB, and DynamoDB.\nWork with big data tools and frameworks including Hadoop, Spark, Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nUtilize cloud data services and warehouses like AWS Glue, GCP Dataflow, Azure Data Factory, Snowflake, Redshift, and BigQuery.\nSupport CI/CD pipelines and DevOps workflows using Git, Docker, Terraform, and related tools.\nEnsure data governance, security, and compliance standards are met.\nParticipate in Agile and DevOps processes to enhance data engineering workflows.\nQualifications\nData Engineer with experience in MySQL or SQL or PL/SQL and any cloud experience like GCP or AWS or Azure\nWould be a plus\nPreferred Skills:\nStrong problem-solving and communication skills.\nAbility to work independently and collaboratively in a team environment.\nExperience with service development, REST APIs, and automation testing is a plus.\nFamiliarity with version control systems and workflow automation.\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Automation', 'Data modeling', 'MySQL', 'Workflow', 'Informatica', 'Oracle', 'Apache', 'SSIS', 'Analytics']",2025-06-12 15:00:16
Software Development Engineer 1,Meesho,0 - 1 years,20-25 Lacs P.A.,['Bengaluru'],"Join us for an exciting SDE Traineeship at Meesho.\nBased on the performance at Meesho assessment, successful candidates will be considered for a full-time opportunity (FTE).\n\nAPPLY HERE: https://p.hck.re/6TcJ\n\nAbout the role:\nAs an SDE Trainee , we expect you to be motivated in solving real-life complex problems and creating compelling experiences for our resellers. Being a small company we have a culture of creative problem- solving, intellectual design, fast-paced development, and passionate product delivery. The pace of our growth is incredible. If you want to tackle hard, interesting and UNIQUE problems, and create an impact within an entrepreneurial environment, JOIN US!\nKey Responsibilities:\nCollaborate with teams to develop new features for Meesho customers and suppliers\nLeverage state-of-the-art technologies and write highly performant code\nTake end-to-end ownership of features, from ideation to production\n\nTechnical Requirements:\n0-1 years of experience\nStrong problem-solving skills\nExcellent understanding of data structures and algorithms, and their space & time complexities\nStrong hands-on and practical working experience with at least one programming language: Java/Python/Javascript\nExcellent coding skills should be able to convert design into code fluently.",Industry Type: Internet (E-Commerce),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Generative Ai', 'Gen AI', 'java', 'Artificial Intelligence', 'Data Structures', 'Javascript', 'Python']",2025-06-12 15:00:18
"Data Analytics Fresher , Data Analyst Fresher",Ablycon Global Angalore,0 - 1 years,4.25-6.5 Lacs P.A.,"['Hyderabad', 'Bengaluru', 'Delhi / NCR']","NOTE- Please do not call. Apply through Naukri or email your resume at ankit@ablyconglobal.com. or whatsapp on 9821833955 - Don't CALL Please .\n\n\nJob Title: Data Analytics Fresher\nEmployment Type: Full-Time\nExperience: 0 - 1 Year\nQualification- ANY UG , ANY PG\n\n\nAbout the Role:\nWe are seeking a highly motivated and detail-oriented individual to join our Data Analytics team as a Data Analyst Fresher. This position offers a launchpad into the world of data analytics. Youll work on structured and unstructured datasets, assist in building dashboards and models, and get practical exposure to tools like SQL, Python, and BI platforms. Ideal for someone with a strong analytical foundation and a hunger to grow into a full-stack data professional.\n\nKey Responsibilities:\n\nCollect, organize, and analyze large datasets from various internal and external sources.\nAssist in preparing dashboards, reports, and visualizations to present insights and findings.\nSupport the team in identifying trends, anomalies, and patterns that impact business performance.\nWork with different departments (marketing, sales, operations, etc.) to understand data requirements.\nPerform exploratory data analysis (EDA) to help refine business strategies.\nMaintain and ensure data integrity and consistency across databases and reporting tools.\nSupport the automation of repetitive reporting processes using scripting or BI tools.\n\nRequired Skills & Qualifications:\n\nStrong analytical and problem-solving skills.\nProficiency in Excel and a basic understanding of SQL.\nFamiliarity with data visualization tools (e.g., Tableau, Power BI) is a plus.\nKnowledge of programming languages such as Python or R is an advantage.\nStrong communication skills to explain technical results to non-technical audiences.\nAttention to detail and a strong sense of responsibility.\nEagerness to learn new tools, technologies, and business domains.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Analytics', 'Business Analytics', 'Power Bi', 'Artificial Intelligence', 'Data Interpretation', 'Data Management', 'Data Extraction', 'Tableau', 'Machine Learning', 'Statistics', 'data analyst', 'SQL', 'Data Science', 'Excel', 'MySQL', 'Data Analysis', 'Data Visualization', 'Data Processing', 'Python']",2025-06-12 15:00:20
"AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING",Zensar,2 - 7 years,Not Disclosed,['Pune'],"Zensar Technologies is looking for AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING, ANALYTICS AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING, ANALYTICS to join our dynamic team and embark on a rewarding career journey\n\nDevelops and executes test plans for AI and machine learning models\n\nValidates model accuracy, fairness, performance, and edge-case behavior\n\nImplements automation tools and creates synthetic test datasets\n\nEnsures compliance with model validation protocols and documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Design engineering', 'deep learning', 'Technology consulting', 'Focus', 'Agile', 'Conceptualization', 'Management', 'Data mining', 'Analytics', 'Testing']",2025-06-12 15:00:23
Bangalore Non-Agent Requirement,Startek,0 - 3 years,Not Disclosed,['Bengaluru'],"STARTEK is looking for Bangalore Non-Agent Requirement to join our dynamic team and embark on a rewarding career journey\n\nSupports backend or operational roles not directly customer-facing\n\nHandles documentation, data entry, process execution, and reporting\n\nCollaborates with internal departments for workflow accuracy\n\nEnsures compliance with operational guidelines and SLAs",Industry Type: Oil & Gas,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Wireless', 'Business transformation', 'orchestration', 'Artificial Intelligence', 'Customer support', 'Customer experience', 'Management', 'Troubleshooting', 'Customer engagement', 'digital transformation']",2025-06-12 15:00:25
Senior Data Scientist,Epsilon,6 - 9 years,Not Disclosed,['Bengaluru'],"Responsibilities: -\nContribute and build an internal product library that is focused on solving business problems related to prediction & recommendation.\nResearch unfamiliar methodologies, techniques to fine tune existing models in the product suite and, recommend better solutions and/or technologies.\nImprove features of the product to include newer machine learning algorithms in the likes of product recommendation, real time predictions, fraud detection, offer personalization etc\nCollaborate with client teams to on-board data, build models and score predictions.\nParticipate in building automations and standalone applications around machine learning algorithms to enable a One Click solution to getting predictions and recommendations.\nAnalyze large datasets, perform data wrangling operations, apply statistical treatments to filter and fine tune input data, engineer new features and eventually aid the process of building machine learning models.\nRun test cases to tune existing models for performance, check criteria and define thresholds for success by scaling the input data to multifold.\nDemonstrate a basic understanding of different machine learning concepts such as Regression, Classification, Matrix Factorization, K-fold Validations and different algorithms such as Decision Trees, Random Forrest, K-means clustering.\nDemonstrate working knowledge and contribute to building models using deep learning techniques, ensuring robust, scalable and high-performance solutions\nMinimum Qualifications:\nEducation: Master's or PhD in a quantitative discipline (Statistics, Economics, Mathematics, Computer Science) is highly preferred.\nDeep Learning Mastery: Extensive experience with deep learning frameworks (TensorFlow, PyTorch, or Keras) and advanced deep learning projects across various domains, with a focus on multimodal data applications.\nGenerative AI Expertise: Proven experience with generative AI models and techniques, such as RAG, VAEs, Transformers, and applications at scale in content creation or data augmentation.\nProgramming and Big Data: Expert-level proficiency in Python and big data/cloud technologies (Databricks and Spark) with a minimum of 4-5 years of experience.\nRecommender Systems and Real-time Predictions: Expertise in developing sophisticated recommender systems, including the application of real-time prediction frameworks.\nMachine Learning Algorithms: In-depth experience with complex algorithms such as logistic regression, random forest, XGBoost, advanced neural networks, and ensemble methods.\nExperienced with machine learning algorithms such as logistic regression, random forest, XG boost, KNN, SVM, neural network, linear regression, lasso regression and k-means.\nDesirable Qualifications:\nGenerative AI Tools Knowledge: Proficiency with tools and platforms for generative AI (such as OpenAI, Hugging Face Transformers).\nDatabricks and Unity Catalog: Experience leveraging Databricks and Unity Catalog for robust data management, model deployment, and tracking.\nWorking experience in CI/CD tools such as GIT & BitBucket",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Engineering', 'Pyspark', 'Azure Aws', 'Generative AI', 'Big Data', 'AWS', 'Data Bricks', 'Deep Learning', 'Python', 'SQL']",2025-06-12 15:00:28
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-12 15:00:30
Data Scientist - Immediate Joiners Only,Reyika,5 - 10 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","Role: Data Scientist\nExperience: 5+ years\nLocation: Any - Hybrid (Bangalore, Hyderabad, Pune, Chennai and Gurgaon)\nJob Summary:\nWe're seeking a highly skilled NLP Engineer with expertise in Large Language Models (LLMs) and text summarization to join our team. The ideal candidate will have hands-on experience with Amazon Bedrock, OpenAI, or Hugging Face transformers and a strong background in Python programming. This role involves working with unstructured audio-to-text data, such as call transcripts, and developing innovative solutions using LLMs.\n\nRequirements:\nStrong expertise in NLP, text summarization, semantic search, and LLM APIs.\nPractical experience with Amazon Bedrock, OpenAI, or Hugging Face transformers.\nFamiliar with prompt tuning and few-shot learning.\nPython (pandas, langchain, boto3, NumPy, etc.)\nExperience working with unstructured audio-to-text data (e.g., call transcripts).\n\nKey Responsibilities:\nDesign and Development: Design, develop, and deploy LLM-based solutions for text summarization, semantic search, and other NLP tasks\nLLM APIs: Integrate LLM APIs from Amazon Bedrock, OpenAI, or Hugging Face transformers into existing applications\nPrompt Tuning and Few-Shot Learning: Implement prompt tuning and few-shot learning techniques to improve LLM performance\nUnstructured Audio-to-Text Data: Work with unstructured audio-to-text data, such as call transcripts, to develop accurate and efficient NLP models\nPython Programming: Utilize Python libraries like pandas, LangChain, boto3, and NumPy for data processing and model development.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Large Language Model', 'Natural Language Processing', 'Python']",2025-06-12 15:00:32
Principal Engineer,Wells Fargo,7 - 12 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Principal Engineer\n\nIn this role, you will:\nAct as an advisor to leadership to develop or influence applications, network, information security, database, operating systems, or web technologies for highly complex business and technical needs across multiple groups",,,,"['Underwriting', 'VSAM', 'JCL', 'Git', 'DB2', 'REXX', 'COBOL', 'SonarQube', 'debugging', 'IMS']",2025-06-12 15:00:35
Principal Engineer,Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 8+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 7+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 6+ years of Software Engineering or related work experience.\n\n4+ years of work experience with Programming Language such as C, C++, Java, Python, etc.\n\nAdditional\n\nPreferred requirements:\n15+ or more overall years of relevant experience in software design, including debugging, performance analysis.\nWorking knowledge of operating systems and hypervisors like Linux, QNX and other RTOSs\nSystem SW development experience including kernels, device drivers and BSP.\nUnderstanding of OS internals, storage, peripherals, and interfaces e.g., UFS/EMMC, PCIe, SPI/UART/I2C, USB, Ethernet etc.\nUnderstanding of secure and safe automotive SW architecture design and development involving safety subsystems and monitors,\nSystem level boot, power, performance, and latency optimizations.\nExposure to automotive SW development processes and standards (e.g., ASPCE, ISO26262 and ISO21434).\n\n\nPrincipal Duties and Responsibilities:\nThe idle candidate might have demonstrated ability to work with engineers/partners/customers across different geographies and contribute to large-scale SoC SW product development and customer support.\nHands-on technical lead/engineer who is not hesitant to dig into the details where needed to get first-hand knowledge of the issues and play an active role in steering team success.\nWork with management team on roadmap and strategy planning\nWorking with Automotive T1/OEMs and commercialization of Automotive HW/SW platforms is a plus.\n\nLeverages advanced Software knowledge and experience to design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs.\nDesign, develop, code, test software modules\nGather customer requirements, distill requirements to software architecture, create software architecture documents.\nAnalyzes user needs, software requirements, and time and cost constraints to design and customize software for optimal operational efficiency.\nDesigns and implements software modules for large-scale products and systems.\nParticipates in and leads design, coding, unit testing, debugging, and integration efforts to ensure projects are completed to specifications and schedules.\nPerforms complex code reviews and regression tests as well as triages and fixes issues to ensure the quality of code.\nCollaborates with individuals outside the software function (e.g., Hardware, Systems, and Test engineers) to ensure solutions work with other components of a specific project.\nWrites detailed technical documentation for complex Software projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'software design', 'linux', 'debugging', 'software engineering', 'usb', 'soc', 'unit testing', 'device drivers', 'hw', 'test engineering', 'java', 'computer science', 'product development', 'voip', 'sip', 'python', 'c', 'sw', 'spi', 'ethernet', 'cucm', 'uart', 'qnx', 'technical documentation', 'h323', 't1', 'bsp']",2025-06-12 15:00:37
Senior Data Scientist - AI/ML,Inumellas Consultancy Services,9 - 14 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Role - Senior Data Scientist / Senior Gen AI Engineer\nExp Range - 8 to 18 yrs\nPosition - Permanent Fulltime\nCompany - Data Analytics & AIML MNC\nLocation - Hyderabad, Pune, Bangalore (Relocation accepted)\nAbout the Role:\n\nWe are seeking a Software Engineer with expertise in Generative AI and Microsoft technologies to design, develop, and deploy AI-powered solutions using the Microsoft ecosystem. You will work with cross-functional teams to build scalable applications leveraging generative AI models and Azure services.\n\nSkills Required:\n\nExperience with Large Language Models (LLMs) like GPT, LLaMA, Claude, etc.\nProficiency in Python for building and fine-tuning AI/ML models\nFamiliarity with LangChain, LLMOps, or RAG (Retrieval-Augmented Generation) pipelines\nExperience with Vector Databases (e.g. FAISS, Pinecone, Weaviate)\nKnowledge of Prompt Engineering and model evaluation techniques\nExposure to cloud platforms (Azure, AWS or GCP) for deploying GenAI solutions\n\nPreferred Skills:\n\nExperience with Azure OpenAI, Databricks or Microsoft Fabric\nHands-on with Hugging Face Transformers, OpenAI APIs or custom model training",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Deep Learning', 'Prompt Engineering', 'Large Language Model', 'Vector Database', 'Retrieval Augmented Generation', 'GenAI', 'Langchain', 'Artificial Intelligence', 'LLMOps', 'LLaMa', 'GPT', 'Azure OpenAI', 'Machine Learning', 'ML Models', 'Model Evaluation', 'Huggingface', 'Aiml', 'OpenAI', 'Azure Machine Learning', 'Python']",2025-06-12 15:00:40
Principal Engineer - GCP,Wells Fargo,7 - 12 years,Not Disclosed,['Bengaluru'],"In this role, you will:\nAct as an advisor to leadership to develop or influence applications, network, information security, database, operating systems, or web technologies for highly complex business and technical needs across multiple groups\nLead the strategy and resolution of highly complex and unique challenges requiring in-depth evaluation across multiple areas or the enterprise, delivering solutions that are long-term, large-scale and require vision, creativity, innovation, advanced analytical and inductive thinking",,,,"['GCP', 'architecture roadmap', 'architects engineering', 'ETL Tools', 'enterprise principles', 'Google Cloud Platform', 'Informatica', 'Abinitio']",2025-06-12 15:00:43
Senior Data Scientist,Toast,6 - 11 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist - S&A\nNow, more than ever, the Toast team is committed to our customers. We re taking steps to help restaurants navigate these unprecedented times with technology, resources, and community. We focus on building the restaurant platform that helps restaurants adapt, take control, and get back to what they do best: building the businesses they love. And because our technology is purpose-built for restaurants, by restaurant people, restaurants can trust that we ll deliver on their needs for today while investing in experiences that will power their restaurant of the future.\nBready*\nto make a change?\n\nAs the Senior Data Scientist in our Bangalore Data Science team, you will contribute to building machine learning algorithms using our huge reservoir of point of sale transaction data. You will work with architects, engineers and product managers to solve business and customer problems and turn machine learning models into business impact across product lines, including financial processing and fraud.\nAbout this\nRoll*\n:\nDesign, build, train and evaluate machine learning models to drive business value for Toast and our restaurant customers\nCollaborate closely with internal and external product stakeholders, both technical and non-technical and help translate deep machine learning knowledge to product applications\nBreak down larger ML initiatives into smaller problems that enables data science to deliver incremental business value and lead the team to execute on them\nWork closely with Production Engineering and Data Platform teams to deploy models to production and regularly monitor for efficiency, key KPIs and enhance them as needed\nWork with incident response and problem resolution teams to check and resolve any problems/challenges as and when identified in the model deployed in the production\nEffectively document all steps and manage code repositories for easy scalability and knowledge sharing with the the team\nIncorporate up-to-date ML technology and DS approach as best practice for the team\nHelp in continuing to build out and expand the Data Science and ML Engineering teams\nWork effectively in a dynamic, changing environment while focusing on key goals and objectives\nDo you have the right\ningredients*\n?\nAdvanced degree in Data Science, Statistics, Applied Math, Computer Science, Engineering or other equivalent quantitative disciplines\n6 + years of industry experience in the field of Data Science and Machine Learning\nExperience in time series modelling. Familiarity with ARIMA, SARIMA, ETS, VAR models. Familiarity with forecasting tools like Facebook Prophet, GluonTS, or NeuralProphet.\nStrong proficiency in Python and SQL; experience with some of the following languages, tools, and frameworks: R, Spark, Scala, scikit-learn, Tensorflow, PyTorch, etc.\nFamiliarity with standard software engineering practices and tools including object-oriented programming, test-driven development, CI/CD, git, shell scripting, task orchestration (Airflow, Luigi, etc.) and preferably AWS tooling (Sagemaker, DynamoDB, ECS, etc.)\nStrong knowledge of underlying mathematical foundations of statistics and machine learning\nPrior success deploying machine learning solutions in large-scale production environments\nExperience collaborating with cross-functional teams and stakeholders to evaluate new Machine Learning opportunities\nProblem solver who loves to dig into different kinds of data and can communicate their findings to cross-functional stakeholders\nBonus\ningredients*\n:\nPassion for research and curiosity that calls you to go beyond good enough to create something innovative and exciting\nDiversity, Equity, and Inclusion is Baked into our Recipe for Success\nAt Toast, our employees are our secret ingredient when they thrive, we thrive. The restaurant industry is one of the most diverse, and we embrace that diversity with authenticity, inclusivity, respect, and humility. By embedding these principles into our culture and design, we create equitable opportunities for all and raise the bar in delivering exceptional experiences.\nWe Thrive Together\nWe embrace a hybrid work model that fosters in-person collaboration while valuing individual needs. Our goal is to build a strong culture of connection as we work together to empower the restaurant community. To learn more about how we work globally and regionally, check out: https: / / careers.toasttab.com / locations-toast .\nApply today!\nToast is committed to creating an accessible and inclusive hiring process. As part of this commitment, we strive to provide reasonable accommodations for persons with disabilities to enable them to access the hiring process. If you need an accommodation to access the job application or interview process, please contact .\n------\nFor roles in the United States, It is unlawful in Massachusetts to require or administer a lie detector test as a condition of employment or continued employment. An employer who violates this law shall be subject to criminal penalties and civil liability.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'orchestration', 'data science', 'Production engineering', 'Machine learning', 'Shell scripting', 'test driven development', 'Forecasting', 'SQL', 'Python']",2025-06-12 15:00:45
Principal Engineer - .Net Full Stack,Wells Fargo,7 - 9 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Principal Engineer\n\nIn this role, you will:\nAct as an advisor to leadership to develop or influence applications, network, information security, database, operating systems, or web technologies for highly complex business and technical needs across multiple groups",,,,"['.Net', 'Java', 'Gen AI', 'DevOps', 'CI/CD', 'micro services architecture', 'Full Stack', 'ETL', 'SDLC', 'Python']",2025-06-12 15:00:48
Senior Data Scientist,Hindustan Unilever (HUL),2 - 5 years,Not Disclosed,['Bengaluru'],"Job Title: Senior Data Scientist\nLocation: Bangalore\nJob Title: Assistant Manager - Security Engineering\nLocation: UniOps Bangalore\nABOUT UNILEVER:\nEvery individual here can bring their purpose to life through their work. Join us and you ll be surrounded by inspiring leaders and supportive peers. Among them, you ll channel your purpose, bring fresh ideas to the table, and simply be you. As you work to make a real impact on the business and the world, we ll work to help you become a better you.\nABOUT UNIOPS:\nUnilever Operations (UniOps) is the global technology and operations engine of Unilever offering business services, technology, and enterprise solutions. UniOps serves over 190 locations and through a network of specialized service lines and partners delivers insights and innovations, user experiences and end-to-end seamless delivery making Unilever Purpose Led and Future Fit\nBackground\nFor Unilever to remain competitive in the future, the business needs to continue on the path to become data intelligent. The Data Analytics team will persevere to make Unilever Data Intelligent, powering key decisions with data, insights, advanced analytics and AI. Our ambition is to enable democratization of data, information and insights as a completely agile organization that builds fantastic careers for our people and is accountable for delivering great work that maximizes impact and delivers growth.\nThis Data Analytics function endeavours to create clear accountability for all aspects of Data Strategy, Data Management, Information Management, Analytics, and Insights. We are accountable for impact of solutions, maintaining market relevance and minimising unnecessary overlaps in analytics products, ensuring simplicity and that our solutions better meet the needs of our users. We partner with the Digital and Data Legal Counsel to ensure that our Data Defence (Privacy, Governance, Quality, etc) is well structured and sufficiently robust to use data and AI correctly throughout the enterprise. We democratize information across the business, while supporting the culture shift required for data driven decision making.\nOur vision is to make Unilever data intelligent, partnering with the business to power key decisions with data, advanced analytics and AI to accelerate growth. Our 5 strategies to achieve this are:\nAccelerate simplify access to relevant data, information and insights Build in-house, leading-edge data, information, insights analytics capability Lead the data insights culture and careers to empower employees across Unilever Rapidly embed analytics products, solutions and services to drive growth Advance Information Automation at Scale\nThe Senior Data Scientist is an exciting role in the Data Foundation. This team builds state of the art machine learning algorithms, maximising the impact of analytic solutions in driving enterprise performance. Typical initiatives include optimizing trade promotion investments, accurately forecasting customer demand, using NLP to glean insight on consumer trends from search data, and making individual assortment recommendations for each of the millions of stores that sell Unilever products.\nMain Purpose of the Job:\nThe Senior Data Scientist improves business performance in the functional area of Unilever they serve, through the application of world class data science capability. They own delivery of data science on moderate projects or specific modules of a major global initiative.\nKey accountabilities:\nInteract with relevant teams to identify business challenges where data science can help\nApply comprehensive data science knowledge to propose optimal techniques for key business challenges\nCreate detailed data science proposals and project plans, flagging any limitations of proposed solution\nDesign and prototype experimental solutions, particularly machine learning models\nDesign scaled solutions and ensure high quality and timely delivery\nFacilitate industrialization and ongoing operation of solutions through well organised code, clear documentation and collaboration with ML Ops resources\nGovern the work of 3rd party vendors where needed to support delivery, while maximising creation of Unilever IP\nRepresent Data Science in cross-functional governance of projects, engaging with stakeholders up to Director level\nHighlight recent developments in data science capability which could solve additional challenges\nLead a team of up 1-2 data scientists / interns, providing career mentorship and line management\nProvide technical guidance to data scientists across DA, particularly on the projects you lead\nSupport the growth of DA s data science capability by contributing to activities such as tool and vendor selection, best practice definition, recruitment, and creation of training materials\nBuild the reputation of DA s data science capability within Unilever and externally, through activities such as community engagement (e. g. Yammer), publications or blogs\nProvide ad-hoc immediate support to the business when needed (for example Covid-19 crisis support)\nDepending on the specific project, the Senior Data Scientist can expect 60-90% of their work to be hands-on prototyping solutions, with the remainder spent planning and designing, overseeing and reviewing work of project staff, interfacing with stakeholders and managing team members.\nExperience and qualifications required:\nStandards of Leadership Required in This Role\nPersonal Mastery (Data-science and advanced analytics)\nAgility\nBusiness acumen\nPassion for High Performance\nKey Skills Required\nProfessional Skills\nMachine learning - Expert\nStatistical modelling - Expert\nForecasting - Expert\nOptimisation techniques and tools - Fully Operational\nPython coding - Fully Operational\nData science platform tools e. g. MS Azure, Databricks - Fully Operational\nDeep learning (and applications to NLP Computer Vision) - Fully Operational\nCollaborative development using Git repos - Fully Operational\nAutomated Machine Learning platforms - Foundational knowledge\nWhile a broad data science technical background is required, the role will benefit from deeper skills (for example graduate studies or prior work experience) in one of the following areas, optimization, simulation, forecasting, natural language processing, computer vision or geospatial analysis.\nGeneral Skills\nProject Management - Expert\nCommunication / presentation skills - Expert\n3rd party resource management - Expert\nCPG Industry analytics - Expert\nStrong communication and stakeholder engagement skills are essential, including the ability to influence peers and senior business stakeholders across Unilever.\nRelevant Experience:\nMinimum of B. E. in a relevant technical field (e. g. Computer Science, Engineering, Statistics, Operations Research); preferably a postgraduate (Masters or Doctorate) degree\nAt least 4 years building data science solutions to solve business problems, preferably in the CPG industry (less experience may be acceptable if balanced by strong post-grad qualifications)\nExperience with open source languages (eg. Python) and preferably with distributed computing (PySpark)\nExperience deploying solutions in a modern cloud-based architecture\nExperience managing the work of team members and 3rd party resource vendors\nExperience presenting insights and influencing decisions of senior non-technical stakeholders\nKey interfaces\nInternal\nUnilever operational, marketing, customer development, supply chain, product finance teams\nInternal DA teams (Engagement teams; Data CoE; Solution Factory; BDL Factory; Information Factory; Tech Transformation)\nWider Unilever analytics and data science professionals\nExternal\n3rd party Data Science vendors\nUniversities\nIndustry bodies",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Operations research', 'Automation', 'data science', 'Data management', 'Project management', 'Information management', 'Resource management', 'Forecasting', 'Recruitment']",2025-06-12 15:00:50
Senior Data Scientist with GCP,TVS Next,5 - 7 years,Not Disclosed,['Bengaluru'],"What you’ll do:\nUtilize advanced mathematical, statistical, and analytical expertise to research, collect, analyze, and interpret large datasets from internal and external sources to provide insight and develop data driven solutions across the company\nBuild and test predictive models including but not limited to credit risk, fraud, response, and offer acceptance propensity\nResponsible for the development, testing, validation, tracking, and performance enhancement of statistical models and other BI reporting tools leading to new innovative origination strategies within marketing, sales, finance, and underwriting",,,,"['analytical', 'scikit-learn', 'searching', 'bi', 'pyspark', 'numpy', 'sql', 'analytics', 'apache', 'automation', 'data science', 'spark', 'gcp', 'bigquery', 'data visualization', 'xgboost', 'programming', 'reporting', 'ml', 'advanced analytics', 'python', 'data processing', 'predictive', 'jupyter notebook', 'bert', 'pandas', 'matplotlib', 'statistics']",2025-06-12 15:00:53
Data Engineer - SAS Migration,Crisil,2 - 4 years,Not Disclosed,['Mumbai'],"The SAS to Databricks Migration Developer will be responsible for migrating existing SAS code, data processes, and workflows to the Databricks platform\n\nThis role requires expertise in both SAS and Databricks, with a focus on converting SAS logic into scalable PySpark and Python code\n\nThe developer will design, implement, and optimize data pipelines, ensuring seamless integration and functionality within the Databricks environment\n\nCollaboration with various teams is essential to understand data requirements and deliver solutions that meet business needs",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'scala', 'pyspark', 'data warehousing', 'data migration', 'azure data factory', 'sql', 'sql azure', 'java', 'spark', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'sas', 'microsoft azure', 'power bi', 'machine learning', 'sql server', 'data bricks', 'migration', 'sqoop', 'aws', 'ssis']",2025-06-12 15:00:55
Design Verification WLAN - Principal Engr / Mgr,Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Hardware Engineer, you will plan, design, optimize, verify, and test electronic systems, bring-up yield, circuits, mechanical systems, Digital/Analog/RF/optical systems, equipment and packaging, test systems, FPGA, and/or DSP systems that launch cutting-edge, world class products. Qualcomm Hardware Engineers collaborate with cross-functional teams to develop solutions and meet performance requirements.\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 8+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 7+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 6+ years of Hardware Engineering or related work experience.\n\n\nAs a design verification engineer you will work with a fast paced Integrated Wireless Technology (IEEE 802.11) team, with various wireless technologies embedded into an ARM based SOC infrastructure.\n\n\nYou will be responsible for developing HW blocks (IP design), conduct High/Mid/Low level Design review and delivery IP to Subsystem team for making complex SoCs.\n\nYou will be a critical part of the WLAN subsystem, contribute to IP design, sign-off the core to the SOC design team.\n\nSkills/Experience:\n\n- 6-15 years experience in Digital Design with a leading chipset company\n\n- Decent knowledge in Wireless connectivity technologiesIEEE 802.11 a/b/g/n/ac/ax/be\n\n- Knowledge in SoC architecture, including CPUs (preferably ARM), communications peripherals, multi-domain clocking, bus & interconnect structures, and power management\n\n- Strong fundamentals in one or few of these domain areas - Wireless and Mobile communications, Information theory, Coding theory, Signal processing\n\n- Strong knowledge on fixed-point implementation Truncation/Rounding/Saturation concepts\n\n- Strong knowledge on Digital communication engines viz., Demodulator, Deinterleaver, Viterbi/Turbo Decoders, Sigma-Delta modulation, Base band filters, FFT etc.",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['signal processing', 'digital design', 'mobile communication', 'digital communication', 'wireless', 'soc', 'gts', 'hardware engineering', 'system testing', 'packaging', 'design management', 'vhdl', 'verilog', 'electricals', 'rtl design', 'fpga', 'soc design', 'rtl coding', 'design review', 'arm']",2025-06-12 15:00:57
Data Scientist,Grid Dynamics,10 - 20 years,Not Disclosed,['Hyderabad'],"Role & responsibilitiMes\n\nCandiate needs to be 8+ Years of Experience\n\nDetails on tech stack\nPython\nPrompt engineering\nBest practices for prompt engineering\nHow LLM can be used in applications for a variety of tasks\nNLP\nUnderstanding of typical NLP problems: classification, NER, summarization, question answering, sentiment analysis, etc.\nTheoretical intuitive understanding of how Transformers work (tokenization, attention, etc).\nWord and sentence embeddings\nVector search\nVector databases, performance tuning\nDocument chunking techniques\nLLM applications development\nLangChain, LlamaIndex\nChain of Thoughts, DSP, and other techniques\nAgents and tools\nGoogle cloud (GCP)\nNice to have requirements to the candidate\nPreferable, the engineers are expected to have IT services/consulting experience.\nProficient in developing LLM-powered systems using advanced prompt engineering techniques, RAG and agentic design patterns. Experienced with frameworks like LangChain, LlamaIndex, and DSPy.\nFamiliar with evaluation approaches and metrics for different types of LLM-based systems.\nExperienced with keyword and vector search methods, including understanding of their underlying algorithms. Familiar with popular vector search engines.\nCompetent in various document understanding models and techniques to parse complex documents and implement effective chunking strategies for RAG systems.\nFamiliar with LLM and embedding models fine-tuning techniques.\nCompetent in using joint vision-language and generative models to solve various problems related to image generation, visual question answering, and multi-modal search. Familiar with diffusion models and associated techniques like LoRA, Dreambooth, and ControlNet.\nUnderstanding of the challenges and risks associated with the development of Generative AI systems and how to mitigate them.\nFamiliar with various architecture design patterns for different types of LLM-based applications such as chatbots, text2sql, document understanding, etc. Familiar with various approaches to scalability and cost reduction in Generative AI systems.\nAbility to stay updated with the latest advancements in Generative AI and integrate emerging technologies to drive innovation and improve the performance of AI systems.\nFamiliar with Responsible AI principles and Human-AI interaction design best practices.\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Lora', 'Natural Language Processing', 'Deep Learning', 'Python']",2025-06-12 15:00:59
Principal Engineer - Post Si Validation,Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Hardware Engineer, you will plan, design, optimize, verify, and test electronic systems, bring-up yield, circuits, mechanical systems, Digital/Analog/RF/optical systems, equipment and packaging, test systems, FPGA, and/or DSP systems that launch cutting-edge, world class products. Qualcomm Hardware Engineers collaborate with cross-functional teams to develop solutions and meet performance requirements.\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 8+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 7+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 6+ years of Hardware Engineering or related work experience.\n\nSilicon Validation Lead - Graphics Silicon Team, Bangalore\n\nThe Qualcomm Graphics Silicon Team in Bangalore is seeking a Silicon Validation Lead. Our power-efficient GPU solutions are fundamental to enabling exciting new markets such as Virtual Reality (VR), Internet of Things (IoT), Artificial Intelligence (AI), drones, and autonomous driving. We are looking for a talented Silicon Lead to deliver power-optimized, high-quality, high-performance graphics and computing solutions. The Graphics Silicon team in Bangalore is part of a global team responsible for developing and delivering GPU solutions that set industry benchmarks. Qualcomm boasts a strong portfolio of GPU cores, providing engineers with the opportunity to work with a world-class engineering team that leads the industry through innovation and disciplined execution.\n\nRoles and Responsibilities\n\nAs a GPU Silicon Validation Engineer, you will be part of the GPU Silicon Team and drive:\n\nThe new feature, use case enablement, and their validation.\n\nCollaboration with GPU design and verification teams to develop GPU bring-up and validation test plans.\n\nPreparation for GPU bring-up through pre-work on emulation and FPGA platforms.\n\nCoordination with SoC bring-up teams and software teams to plan GPU bring-up.\n\nTriage and debugging of failures on silicon.\n\nDevelopment of test contents and testing strategies to assist in the validation of GPU on silicon.\n\nWorking with GPU verification teams to reproduce silicon failures on emulators and FPGAs.\n\nCollaboration with the design team to suggest and architect new debug features to improve future GPU bring-ups.\n\nPower and performance characterization of GPU.\n\nPlanning and implementation of new efficiency improvement methodologies in GPU.\n\n\nQualifications\n\nThe ideal candidate should possess deep knowledge of scripting and software languages, including PERL/TCL, Linux/Unix shell, and C.\n\nMinimum Qualifications\n\nBachelor's or Masters degree in Electrical or Electronic Engineering from a reputed institution.\n\nOver 14 years of experience in silicon validation and bring-up.\n\n\nMinimum\n\nStrong understanding of microprocessor architecture.\n\nStrong understanding of power management.\n\nExperience in silicon bring-up and validation of GPU features.\n\nExperience in debugging functional, power, performance, and/or physical design issues in silicon.\n\nExperience in GPU silicon validation and debug basics.\n\nExperience in test development for validation of GPU features on silicon.\n\nExperience in developing test vectors for tester bring-up.\n\nImplementation of assembly, C, and Python language programming.\n\nExperience with HW tools like JTAG, Kratos, LA, Emulation platforms, DMM, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['python', 'hardware engineering', 'silicon validation', 'power management', 'perl', 'jtag', 'c', 'soc', 'system testing', 'spi', 'artificial intelligence', 'emulators', 'hw', 'silicon', 'fpga', 'fpga platforms', 'test development', 'linux', 'debugging', 'shell scripting', 'tcl', 'electronics engineering', 'fpgas']",2025-06-12 15:01:02
CPU Micro-architect/RTL Designer -Principal Engineer,Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nGeneral Summary Qualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in. We are hiring talented engineers for CPU RTL development targeted for high performance, low power devices. As a CPU Micro-architecture and RTL Design Engineer, you will work with chip architects to conceive of the micro-architecture, and also help with architecture/product definition through early involvement in the product life-cycle.\n\nRoles And Responsibilities\n\nPerformance exploration. Explore high performance strategies working with the CPU modeling team.\n\nMicroarchitecture development and specification. From early high-level architectural exploration, through micro architectural research and arriving at a detailed specification.\n\nRTL ownership. Development, assessment and refinement of RTL design to target power, performance, area and timing goals.\n\nFunctional verification support. Help the design verification team execute on the functional verification strategy.\n\nPerformance verification support. Help verify that the RTL design meets the performance goals.\n\nDesign delivery. Work with multi-functional engineering team to implement and validate physical design on the aspects of timing, area, reliability, testability and po\n\n\nPreferred Qualifications\n\nThorough knowledge of microprocessor architecture including expertise in one or more of the following areasinstruction fetch and decode, branch prediction, instruction scheduling and register renaming, out-of-order execution, integer and floating point execution, load/store execution, prefetching, cache and memory subsystems\n\nKnowledge of Verilog and/or VHDL. Experience with simulators and waveform debugging tools\n\nKnowledge of logic design principles along with timing and power implications\n\nUnderstanding of low power microarchitecture techniques\n\nUnderstanding of high performance techniques and trade-offs in a CPU microarchitecture\n\nExperience using a scripting language such as Perl or Python\n\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 8+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 7+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 6+ years of Hardware Engineering or related work experience.\n\nPreferred Qualifications:\n\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field.\n\n15+ years of Hardware Engineering or related work experience.\n4+ years of experience with circuit/logic design/validation (e.g., digital, analog, RF).\n\n4+ years of experience utilizing schematic capture and circuit stimulation software.\n\n4+ years of experience with hardware design and measurement instruments such as oscilloscopes, spectrum analyzers, RF tools, etc.\n\n4+ years in a technical leadership role with or without direct reports.\n\nPrincipal Duties and Responsibilities:\n\nLeverages expert Hardware knowledge and experience to plan, optimize, verify, and test highly critical electronic systems, bring-up yield, circuits, mechanical systems, Digital/Analog/RF/optical systems, equipment and packaging, test systems, FPGA, and/or DSP systems.\n\nDrives the development of design rules and processes for electronic hardware, equipment, and/or integrated circuitry.\n\nServes as an expert resource for conducting highly complex simulations and analyses of designs as well as for the implementation of designs with the best power, performance, and area.\n\nCollaborates with high-level representatives across functions (e.g., design, verification, validation, software and systems engineering, architecture development teams, etc.) to implement and drive new requirements and the latest test solutions in the production program to improve the yield, test time, and quality.\n\nEvaluates, characterizes, and develops the novel manufacturing solutions for leading edge products in highly advanced processes and bring-up product to meet customer expectations and schedules.\n\nServes as an expert resource for the evaluation of reliability for highly critical materials, properties, and techniques and brings innovation, automation, and optimization to maximize productivity.\n\nAdvises multiple teams of engineers in the development of complex hardware designs, evaluating various design features to identify potential flaws or issues.\n\nWrites detailed technical documentation for highly complex Hardware projects; reviews technical documentation for experienced engineers.\n\nLevel of Responsibility:\n\nProvides supervision to direct reports.\n\nDecision-making is critical in nature and highly impacts program, product, or project success.\n\nRequires verbal and written communication skills to convey highly complex and/or detailed information. May require strong negotiation and influence with large groups or high-level constituents.\n\nWorks within the prescribed budgetary objectives of the department.\n\nHas a great degree of influence over key organizational decisions.\n\nTasks often require multiple steps which can be performed in various orders; extensive planning, problem-solving, and prioritization must occur to complete the tasks effectively.",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['circuit', 'hardware engineering', 'order execution', 'hardware design', 'logic design', 'python', 'physical design', 'data validation', 'simulation', 'design verification', 'rtl', 'vhdl', 'uvm', 'verilog', 'schematic capture', 'rtl design', 'instruments', 'axi', 'fpga', 'debugging', 'perl', 'system verilog']",2025-06-12 15:01:04
"Associate Scientist, Data Sourcing & Solutions",XL India Business Services Pvt. Ltd,1 - 5 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Associate Scientist - Data Sourcing & Solutions Gurgaon/Bangalore, India AXA XL recognises data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XLs executive leadership team to maximise benefits and facilitate sustained enterprise advantage\n\nOur Innovation, Data, and Analytics Office (IDA) is focused on driving innovation by optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward a greater focus on the use of data and data-driven insights, we are seeking an Associate Scientist for our Data Sourcing & Solutions team\n\nThe role sits across the IDA Department to make sure customer requirements are properly captured and transformed into actionable data specifications\n\nSuccess in the role will require a focus on proactive management of the sourcing and management of data from source through usage\n\nWhat you ll be DOING What will your essential responsibilities include? Accountable for documenting data requirements (Business and Function Requirements) and assessing the reusability of Axiom assets\n\nBuild processes to simplify and expedite data sourcing to focus on delivering data to AXA XL business stakeholders frequently\n\nDevelops and operationalizes strategic data products, and answers and proactively manages the sourcing and management of data from source through usage (reusable Policy and Claim Domain data assets)\n\nData Validation Testing of the data products in partnership with the AXA XL business to ensure the accuracy of the data and validation of the requirements\n\nAssesses all data required as part of the Data Ecosystem to make sure data has a single version of the truth\n\nRespond to ad-hoc data requests to support AXA XLs business\n\nInstill a customer-first attitude, prioritizing service for our business stakeholders above all else\n\nInternalize and execute IDA and company-wide goals to become a data-driven organization\n\nContribute to best practices and standards to make sure there is a consistent and efficient approach to capturing business requirements and translating them into functional, non-functional, and semantic specifications\n\nDevelop a comprehensive understanding of the data and our customers\n\nDrive root cause analysis for identified data deficiencies within reusable data assets delivered via IDA\n\nIdentify solution options to improve the consistency, accuracy, and quality of data when captured at its source\n\nYou will report to the Team Lead - Data Sourcing & Solutions\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Experience in a data role (business analyst, data analyst, analytics) preferably in the Insurance industry and within a data division\n\nA minimum of a bachelor s or masters degree (preferred) in a relevant discipline\n\nRobust SQL knowledge and technical ability to query AXA XL data sources to understand our data\n\nExcellent presentation, communication (oral & written), and relationship-building skills, across all levels of management and customer interaction\n\nInsurance experience in data, underwriting, claims, and/or operations, including influencing, collaborating, and leading efforts in complex, disparate, and interrelated teams with competing priorities\n\nPassion for data and experience working within a data-driven organization\n\nWork together internal data with external industry data to deliver holistic answers\n\nWork with unstructured data to unlock information needed by the business to create unique products for the insurance industry\n\nPossesses robust exploratory analysis skills and high intellectual curiosity\n\nDisplays exceptional organizational skills and is detail-oriented\n\nThe robust conceptual thinker who connects dots, and has critical thinking, and analytical skills\n\nDesired Skills and Abilities: Ability to work with team members across the globe and departments\n\nAbility to take ownership, work under pressure, and meet deadlines\n\nBuilds trust and rapport within and across groups\n\nApplies in-depth knowledge of business and specialized areas to solve business problems and understand integration challenges and long-term impact creatively and strategically\n\nAbility to manage data needs of an individual project(s) while being able to understand the broader enterprise data perspective\n\nExpected to recommend innovation and improvement to policies, and procedures, deploying resources, and performing core activities\n\nExperience with SQL Server, Azure Databricks Notebook, Qlikview, PowerBI, and Jira/Confluence a plus",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data validation', 'Claims', 'Underwriting', 'Agile', 'QlikView', 'Business strategy', 'JIRA', 'Analytics', 'SQL', 'Customer interaction']",2025-06-12 15:01:07
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-12 15:01:09
Data Scientist,Xoom,2 - 4 years,Not Disclosed,['Bengaluru'],"Job Summary\nWhat you need to know about the role\n\nEach Data Scientist on this team has full ownership of a portfolio of a product and is responsible for end-to-end management of loss and decline rates. Day-to-day duties include data analysis, monitoring and forecasting, creating the logic for and implementing risk rules and strategies, providing requirements to data scientists and technology teams on attribute, model and platform requirements, and communicating with global stakeholders to ensure we deliver the best possible customer experience while meeting loss rate targets.\n\nMeet our team\n\nPayPals Global Fraud Protection team is responsible for partnering with global business units to manage a variety of risk of various types, including identity fraud, account takeover, stolen financial fraud, and credit issues. This is an exciting department that plays an important role in contributing PayPals bottom line financial savings, ensuring safe and secure global business growth, and delivering the best customer experience.\n\nThis open opportunity is within the Large Merchant and Markets Fraud Risk team. This portfolio is comprised of PayPal s newest leading-edge payments solutions, such as Risk-as-Service, Fastlane, PayPal Complete Payments, etc. as well as customized experiences developed for the company s highest-priority strategic Markets and Partnerships.\nJob Description\nYour way to impact\nYou will be the Data Scientist in the Fraud Risk team , where you will work on leading new projects to build and improve the Risk strategies to prevent fraud using the Risk tooled and custom data & AL/ML models. In this position, you will be partnering with the corresponding Business Units to align with and influence their strategic priorities, educate business partners about Risk management principles, and collaboratively optimize the Risk treatments and experiences for these unique products and partners.\nYour day to day\nIn your day to day role you will -\nIn this role you will have full ownership of a portfolio of merchants and is responsible for end-to-end management of loss and decline rates.\nCollaborate with different teams to develop strategies for fraud prevention, loss savings, and optimize transaction declines or improve customer friction.\nYou will work together with cross-functional teams to deliver solutions and providing Risk analytics on frustration trend/ KPIs monitoring or alerting for fraud events.\nThese solutions will adapt PayPal s advanced proprietary fraud prevention tools enabling business growth.\nWhat do you need to bring-\n2-4 years of relevant experience working with large-scale complex dataset.\nStrong analytical mindset, ability to decompose business requirements into an analytical plan, and execute the plan to answer those business questions\nExcellent communication skills, equally adept at working with engineers as well as business leaders\nWant to build new solutions and invent new approaches to big, ambiguous, critical problems\nStrong working knowledge of Excel, SQL and Python/R\nTechnical Proficiency Exploratory Data Analysis and expertise in preparing a clean and structured data for model development. Experience in applying AI/ML techniques for business decisioning including supervised and unsupervised learning (e.g., regression, classification, clustering, decision trees, anomaly detection, etc.). Knowledge of model evaluation techniques such as Precision, Recall, ROC-AUC Curve, etc. along with basic statistical concepts.\nPreferred Qualification\nSubsidiary\nPayPal\nTravel Percent\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https//www.paypalbenefits.com .\nWho We Are\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. .\nBelonging at PayPal\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Risk analytics', 'Analytical', 'Diversity and Inclusion', 'ROC', 'Wellness', 'Risk management', 'Forecasting', 'Monitoring', 'SQL']",2025-06-12 15:01:11
Data ML Program Manager - Product Operations,Apple,3 - 8 years,Not Disclosed,['Bengaluru'],"Apple is where individual imaginations gather together, committing to the values that lead to great work\nEvery new product we build, service we create, or Apple Store experience we deliver is the result of us making each other s ideas stronger\nThat happens because every one of us shares a belief that we can make something wonderful and share it with the world, changing lives for the better\nIt s the diversity of our people and their thinking that inspires the innovation that runs through everything we do\nWhen we bring everybody in, we can do the best work of our lives\nHere, you ll do more than join something you ll add something\nJoin Apple, and help us leave the world better than we found it\nProduct Operations group is looking to add a Data and Machine Learning Program Manager to support data systems, automation, machine learning, tools that enhance manufacturing of Apples products\nThis team is made up of creative innovators and problem solvers who tackle unique challenges to ideate, create POCs and deliver new solutions in the data space\nIt takes deeply dedicated, intelligent and hard-working individuals to maintain and exceed the high expectations for the exciting products at Apple\nThe Product Operations Data Team is looking for an extraordinary Program manager to join our team\nYou will craft, design and implement our machine learning strategy to the massive supply chain and help build the future of our manufacturing systems\nDo you love using your creative left brain and structured, tactical right brain to drive projects to build systems, innovate with ML/analytics, and find operations efficiencies\nDescription\nIn this role you will be responsible for planning and managing deployment of various business solutions for the Product Operations organization\nWe lead and participate in efforts for process standardization, ideation and development of tools, and implementation through successful completion\n- Lead the ideation, prototyping, and business justification/validation of solutions for Product Ops- Work closely with engineering teams on technical development and deployment of high-quality solutions- Lead the definition and execution of scalable solutions to support core internal customer needs and to improve process efficiencies- Process re-engineering or build out enhancements- Identify inefficient process and help enable better decisions leveraging data, AI, Machine Learning or all 3 together- Prototype solutions before providing recommendations on directions\ne\ng\ncreate dashboard mock-ups before investing in development effort- Program management of machine learning initiatives - scoping, prioritization, resourcing, and implementation- Comfortable and flexible in serving both supporting and leadership roles for various projects\n3+ years of project management experience in highly technical field\nBachelors or Masters in Computer Science, Engineering or similar field and hands on experience in technical roles\nPreferred Qualifications\nKnowledge of emerging concepts like machine learning and predictive analytics (Deep Learning, Python, R, Neural Networks, applied Data Science) a strong plus\nStrong Excel skills, including pivots, vlookups, conditional formatting, large record sets\nFamiliarity with databases, large data sets, reporting Tableau and SQL a plus\nDomestic and international travel up to 25% and work hour flexibility required in this dynamic, global position\nAble to work independently when managing multiple priorities in an unstructured, global and virtual environment is essential\nHighly desirable to have high tech manufacturing, Supply Chain Management, NPI or Manufacturing Operations related education / experience\nMBA is considered a plus",Industry Type: Consumer Electronics & Appliances,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Supply chain management', 'Prototype', 'Project management', 'Machine learning', 'Resourcing', 'Business solutions', 'SQL', 'Python']",2025-06-12 15:01:13
Data ML Program Manager - Product Operations,Apple,7 - 12 years,Not Disclosed,['Bengaluru'],"In this role you will be responsible for planning and managing deployment of various business solutions for the Product Operations organization\nWe lead and participate in efforts for process standardization, ideation and development of tools, and implementation through successful completion\n- Lead the ideation, prototyping, and business justification/validation of solutions for Product Ops- Work closely with engineering teams on technical development and deployment of high-quality solutions- Lead the definition and execution of scalable solutions to support core internal customer needs and to improve process efficiencies- Process re-engineering or build out enhancements- Identify inefficient process and help enable better decisions leveraging data, AI, Machine Learning or all 3 together- Prototype solutions before providing recommendations on directions\ne\ng\ncreate dashboard mock-ups before investing in development effort- Program management of machine learning initiatives - scoping, prioritization, resourcing, and implementation- Comfortable and flexible in serving both supporting and leadership roles for various projects\n7+ years of project management experience in highly technical field\nBachelors or Masters in Computer Science, Engineering, or similar field or hands on experience in technical roles\nPreferred Qualifications\nKnowledge of emerging concepts like machine learning and predictive analytics (Deep Learning, Python, R, Neural Networks, applied Data Science) a strong plus\nStrong Excel skills, including pivots, vlookups, conditional formatting, large record sets\nFamiliarity with databases, large data sets, reporting Tableau and SQL a plus\nDomestic and international travel up to 25% and work hour flexibility required in this dynamic, global position\nAble to work independently when managing multiple priorities in an unstructured, global and virtual environment is essential\nHighly desirable to have high tech manufacturing, Supply Chain Management, NPI or Manufacturing Operations related education / experience\nMBA is considered a plus",Industry Type: Consumer Electronics & Appliances,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Supply chain management', 'Prototype', 'Project management', 'Machine learning', 'Resourcing', 'Business solutions', 'SQL', 'Python']",2025-06-12 15:01:16
Lead Data Scientist,Bizopp Management Consultants,11 - 18 years,25-35 Lacs P.A.,['Chennai'],"• Proficiency in Python and SQL for data extraction, manipulation, and analysis\n\n• Exploratory data analysis (EDA), developing, and deploying machine learning models\n\n• Expertise in deploying ML models on cloud platforms such as AWS or Azure",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['EDA', 'Data Scientist', 'Python', 'SQL', 'Azure', 'Exploratory data analysis', 'Machine Learning', 'AWS', 'ML']",2025-06-12 15:01:18
Lead Data Scientist,Trion Consultancy Services,10 - 18 years,20-35 Lacs P.A.,['Chennai'],"LD Scientist with 12 yrs of industry exp, including at least 5 yrs of hands-on exp in data science & a proven track record of delivering impactful data science solutions.\nData Analysis &Exploration\nTime Series Analysis\nModel Deployment & Integration\n\nRequired Candidate profile\n12+ yrs/including 5+ yrs in data science\nExp in Python and SQL for data extraction, manipulation & analysis\nDS & Model Development: Demonstrated exp in performing exploratory data analysis (EDA)",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Lead Data science', 'Machine Learning', 'Python']",2025-06-12 15:01:20
Data Engineering Lead,Yotta Techports,10 - 15 years,30-35 Lacs P.A.,['Hyderabad'],"Responsibilities:\nLead and manage an offshore team of data engineers, providing strategic guidance, mentorship, and support to ensure the successful delivery of projects and the development of team members.\nCollaborate closely with onshore stakeholders to understand project requirements, allocate resources efficiently, and ensure alignment with client expectations and project timelines.\nDrive the technical design, implementation, and optimization of data pipelines, ETL processes, and data warehouses, ensuring scalability, performance, and reliability.\nDefine and enforce engineering best practices, coding standards, and data quality standards to maintain high-quality deliverables and mitigate project risks.\nStay abreast of emerging technologies and industry trends in data engineering, and provide recommendations for tooling, process improvements, and skill development.\nAssume a data architect role as needed, leading the design and implementation of data architecture solutions, data modeling, and optimization strategies.\nDemonstrate proficiency in AWS services such as:\nExpertise in cloud data services, including AWS services like Amazon Redshift, Amazon EMR, and AWS Glue, to design and implement scalable data solutions.\nExperience with cloud infrastructure services such as AWS EC2, AWS S3, to optimize data processing and storage.\nKnowledge of cloud security best practices, IAM roles, and encryption mechanisms to ensure data privacy and compliance.\nProficiency in managing or implementing cloud data warehouse solutions, including data modeling, schema design, performance tuning, and optimization techniques.\nDemonstrate proficiency in modern data platforms such as Snowflake and Databricks, including:\nDeep understanding of Snowflake's architecture, capabilities, and best practices for designing and implementing data warehouse solutions.\nHands-on experience with Databricks for data engineering, data processing, and machine learning tasks, leveraging Spark clusters for scalable data processing.\nAbility to optimize Snowflake and Databricks configurations for performance, scalability, and cost-effectiveness.\nManage the offshore team's performance, including resource allocation, performance evaluations, and professional development, to maximize team productivity and morale.\n\nQualifications:\nBachelor's degree in Computer Science, Engineering, or a related field; advanced degree preferred.\n10+ years of experience in data engineering, with a proven track record of leadership and technical expertise in managing complex data projects.\nProficiency in programming languages such as Python, Java, or Scala, as well as expertise in SQL and relational databases (e.g., PostgreSQL, MySQL).\nStrong understanding of distributed computing, cloud technologies (e.g., AWS), and big data frameworks (e.g., Hadoop, Spark).\nExperience with data architecture design, data modeling, and optimization techniques.\nExcellent communication, collaboration, and leadership skills, with the ability to effectively manage remote teams and engage with onshore stakeholders.\nProven ability to adapt to evolving project requirements and effectively prioritize tasks in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team Handling', 'Snowflake', 'Data Services', 'Cloud Infrastructure', 'Data Bricks']",2025-06-12 15:01:22
Senior Manager-Decision Scientist,Tesco Plc,3 - 8 years,Not Disclosed,['Bengaluru'],"Senior Manager-Decision Scientist\nBack to job search results\nTesco India Bengaluru, Karnataka, India Hybrid Full-Time Permanent Apply by 19-Jun-2025\nAbout the role\nPlease refer to you are Responsible for :-\nWhat is in it for you\nAt Tesco, we are committed to providing the best for you.\nAs a result, our colleagues enjoy a unique, differentiated, market- competitive reward package, based on the current industry practices, for all the work they put into serving our customers, communities and planet a little better every day.\nOur Tesco Rewards framework consists of pillars - Fixed Pay, Incentives, and Benefits.\nTotal Rewards offered at Tesco is determined by four principles - simple, fair, competitive, and sustainable.\nSalary - Your fixed pay is the guaranteed pay as per your contract of employment.\nPerformance Bonus - Opportunity to earn additional compensation bonus based on performance, paid annually\nLeave & Time-off - Colleagues are entitled to 30 days of leave (18 days of Earned Leave, 12 days of Casual/Sick Leave) and 10 national and festival holidays, as per the company s policy.\nMaking Retirement Tension-FreeSalary - In addition to Statutory retirement beneets, Tesco enables colleagues to participate in voluntary programmes like NPS and VPF.\nHealth is Wealth - Tesco promotes programmes that support a culture of health and wellness including insurance for colleagues and their family. Our medical insurance provides coverage for dependents including parents or in-laws.\nMental Wellbeing - We offer mental health support through self-help tools, community groups, ally networks, face-to-face counselling, and more for both colleagues and dependents.\nFinancial Wellbeing - Through our financial literacy partner, we offer one-to-one financial coaching at discounted rates, as well as salary advances on earned wages upon request.\nSave As You Earn (SAYE) - Our SAYE programme allows colleagues to transition from being employees to Tesco shareholders through a structured 3-year savings plan.\nPhysical Wellbeing - Our green campus promotes physical wellbeing with facilities that include a cricket pitch, football field, badminton and volleyball courts, along with indoor games, encouraging a healthier lifestyle.\nYou will be responsible for\nDeveloping and leading a high performing team, creating an environment for success by setting direction and coaching them to succeed through inspiring conversations every day. (Refer to the expectations of a manager at Tesco- the minimum standards)\n- Promoting a culture of CI within their teams to drive operational improvements\n- Accountable for achieving teams objectives, stakeholder management and escalation management\n- Provides inputs that impact the functions plans, policies, influences the budget and resources in their scope.\nAccountable to EA and market leaderships for building the analytics road-map and improve analytical maturity of partnering functions with in depth understanding of key priorities & outcome.\n- Accountable to shape & own the analytics workplan, proactively spot size able opportunities and deliver programs successfully that will result in disproportionate returns\n- Thought leadership in scoping the business problems, solutions and bringing disruptive / depth oriented solutions to complex problems and institutionalize robust ways of working with business partners\n- Partner with TBS and markets finance team to measure the value delivered through analytics initiatives\n- Build impact driven teams by creating an environment for success by setting direction, objectives and mentor managers, and guide teams to craft analytical assets which will deliver value in sustainable manner\n- Be the voice and represent Enterprise Analytics on internal and external forums\n- Provides inputs that impact the functions plans, policies, influences the budget and resources in their scope\n- Developing managers and colleagues to succeed through inspiring conversations every day\nYou will need\nUnderstanding of machine learning techniques, Linear & Logistics regression, Decision Trees, Random Forest, XGBoost and Neural Network\n- Knowledge of Python, SQL, Hive and Visualization tools (e.g. Tableau )\n- Retail Expertise, Partnership management, Analytics\n- Conceptual application to larger business context, Storyboarding, Managing managers\nAbout us\nTesco in Bengaluru is a multi-disciplinary team serving our customers, communities, and planet a little better every day across markets. Our goal is to create a sustainable competitive advantage for Tesco by standardising processes, delivering cost savings, enabling agility through technological solutions, and empowering our colleagues to do even more for our customers. With cross-functional expertise, a wide network of teams, and strong governance, we reduce complexity, thereby offering high-quality services for our customers.\nTesco in Bengaluru, established in 2004 to enable standardisation and build centralised capabilities and competencies, makes the experience better for our millions of customers worldwide and simpler for over 3,30,000 colleagues.\nTesco Business Solutions:\nEstablished in 2017, Tesco Business Solutions (TBS) has evolved from a single entity traditional shared services in Bengaluru, India (from 2004) to a global, purpose-driven solutions-focused organisation. TBS is committed to driving scale at speed and delivering value to the Tesco Group through the power of decision science. With over 4,400 highly skilled colleagues globally, TBS supports markets and business units across four locations in the UK, India, Hungary, and the Republic of Ireland. The organisation underpins everything that the Tesco Group does, bringing innovation, a solutions mindset, and agility to its operations and support functions, building winning partnerships across the business. TBSs focus is on adding value and creating impactful outcomes that shape the future of the business. TBS creates a sustainable competitive advantage for the Tesco Group by becoming the partner of choice for talent, transformation, and value creation.\nApply",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analytical', 'Machine learning', 'Medical insurance', 'Business solutions', 'Stakeholder management', 'Operations', 'Analytics', 'SQL', 'Python', 'Logistics']",2025-06-12 15:01:25
MLOps Engineer,Affine Analytics,4 - 8 years,Not Disclosed,['Bengaluru'],"Machine Learning & Data Pipelines\nStrong understanding of Machine Learning principles, lifecycle, and deployment practices\nExperience in designing and building ML pipelines\nKnowledge of deploying ML models on AWS Lambda, EKS, or other relevant services\nWorking knowledge of Apache Airflow for orchestration of data workflows\nProficiency in Python for scripting, automation, and ML model development with Data Scientists",,,,"['Machine Learning', 'S3', 'AWS Lambda', 'MLOps', 'SQS', 'AWS Glue', 'SNS', 'EKS', 'Lambda', 'Python']",2025-06-12 15:01:27
Data Engineer - Databricks,Inorg,2 - 5 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']",InOrg Global is looking for Data Engineer - Databricks to join our dynamic team and embark on a rewarding career journey.\n\nLiaising with coworkers and clients to elucidate the requirements for each task.\nConceptualizing and generating infrastructure that allows big data to be accessed and analyzed.\nReformulating existing frameworks to optimize their functioning.\nTesting such structures to ensure that they are fit for use.\nPreparing raw data for manipulation by data scientists.\nDetecting and correcting errors in your work.\nEnsuring that your work remains backed up and readily accessible to relevant coworkers.\nRemaining up - to - date with industry standards and technological advancements that will improve the quality of your outputs.,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent",['Data Engineer - Databricks'],2025-06-12 15:01:30
Data Engineer - Databricks,KPI Partners,3 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About KPI Partners.\nKPI Partners is a leading provider of data analytics solutions, dedicated to helping organizations transform data into actionable insights. Our innovative approach combines advanced technology with expert consulting, allowing businesses to leverage their data for improved performance and decision-making.\n\nJob Description.\nWe are seeking a skilled and motivated Data Engineer with experience in Databricks to join our dynamic team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and data processing solutions that support our analytics initiatives. You will collaborate closely with data scientists, analysts, and other engineers to ensure the consistent flow of high-quality data across our platforms.",,,,"['python', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'data pipeline', 'data architecture', 'data engineering', 'sql', 'data bricks', 'cloud', 'analytics', 'data quality', 'data modeling', 'gcp', 'teamwork', 'integration', 'aws', 'etl', 'programming', 'communication skills', 'etl scripts']",2025-06-12 15:01:33
Data Engineer,Aqilea Softech,5 - 9 years,13-20 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Job Title: Data Engineer\nCompany : Aqilea India(Client : H&M India)\nEmployment Type: Full Time\nLocation: Bangalore(Hybrid)\nExperience: 4.5 to 9 years\nClient : H&M India\n\nAt H&M, we welcome you to be yourself and feel like you truly belong. Help us reimagine the future of an entire industry by making everyone look, feel, and do good. We take pride in our history of making fashion accessible to everyone and led by our values we strive to build a more welcoming, inclusive, and sustainable industry. We are privileged to have more than 120,000 colleagues, in over 75 countries across the world. Thats 120 000 individuals with unique experiences, skills, and passions. At H&M, we believe everyone can make an impact, we believe in giving people responsibility and a strong sense of ownership. Our business is your business, and when you grow, we grow.\nWebsite : https://career.hm.com/\n\nWe are seeking a skilled and forward-thinking Data Engineer to join our Emerging Tech team. This role is designed for someone passionate about working with cutting-edge technologies such as AI, machine learning, IoT, and big data to turn complex data sets into actionable insights.\nAs the Data Engineer in Emerging Tech, you will be responsible for designing, implementing, and optimizing data architectures and processes that support the integration of next-generation technologies. Your role will involve working with large-scale datasets, building predictive models, and utilizing emerging tools to enable data-driven decision-making across the business. You ll collaborate with technical and business teams to uncover insights, streamline data pipelines, and ensure the best use of advanced analytics technologies.\n\nKey Responsibilities:\nDesign and build scalable data architectures and pipelines that support machine learning, analytics, and IoT initiatives.\nDevelop and optimize data models and algorithms to process and analyse large-scale, complex data sets.\nImplement data governance, security, and compliance measures to ensure high-quality\nCollaborate with cross-functional teams (engineering, product, and business) to translate business requirements into data-driven solutions.\nEvaluate, integrate, and optimize new data technologies to enhance analytics capabilities and drive business outcomes.\nApply statistical methods, machine learning models, and data visualization techniques to deliver actionable insights.\nEstablish best practices for data management, including data quality, consistency, and scalability.\nConduct analysis to identify trends, patterns, and correlations within data to support strategic business initiatives.\nStay updated on the latest trends and innovations in data technologies and emerging data management practices.\n\nSkills Required :\nBachelors or masters degree in data science, Computer Science, Engineering, Statistics, or a related field.\n4.5-9 years of experience in data engineering, data science, or a similar analytical role, with a focus on emerging technologies.\nProficiency with big data frameworks (e.g., Hadoop, Spark, Kafka) and experience with modern cloud platforms (AWS, Azure, or GCP).\nSolid skills in Python, SQL, and optionally R, along with experience using machine learning libraries such as Scikit-learn, TensorFlow, or PyTorch.\nExperience with data visualization tools (e.g., Tableau or Power BI or D3.js) to communicate insights effectively.\nFamiliarity with IoT and edge computing data architectures is a plus.\nUnderstanding of data governance, compliance, and privacy standards.\nAbility to work with both structured and unstructured data.\nExcellent problem-solving, communication, and collaboration skills, with the ability to work in a fast-paced, cross-functional team environment.\nA passion for emerging technologies and a continuous desire to learn and innovate.\nInterested Candidates can share your Resumes to mail id karthik.prakadish@aqilea.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Powerbi', 'Hadoop', 'Kafka', 'Tableau', 'Azure', 'GCP', 'Data Engineer', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-12 15:01:35
Data Engineer,Talent Aspire,2 - 7 years,Not Disclosed,"['Chandigarh', 'Bengaluru']","As the Data Engineer, you will play a pivotal role in shaping our data infrastructure and\nexecuting against our strategy. You will ideate alongside engineering, data and our clients to\ndeploy data products with an innovative and meaningful impact to clients. You will design, build, and maintain scalable data pipelines and workflows on AWS. Additionally, your expertise in AI and machine learning will enhance our ability to deliver smarter, more predictive solutions.\n\nKey Responsibilities\nCollaborate with other engineers, customers to brainstorm and develop impactful data\nproducts tailored to our clients.\nLeverage AI and machine learning techniques to integrate intelligent features into our\nofferings.\nDevelop, and optimize end-to-end data pipelines on AWS\nFollow best practices in software architecture and development.\nImplement effective cost management and performance optimization strategies.\nDevelop and maintain systems using Python, SQL, PySpark, and Django for front-end\ndevelopment.\nWork directly with clients and end-users and address their data needs\nUtilize databases and tools including and not limited to, Postgres, Redshift, Airflow, and\nMongoDB to support our data ecosystem.\nLeverage AI frameworks and libraries to integrate advanced analytics into our solutions.\nQualifications\n\nExperience:\nMinimum of 3 years of experience in data engineering, software development, or\nrelated roles.\nProven track record in designing and deploying AWS cloud infrastructure\nsolutions\nAt least 2 years in data analysis and mining techniques to aid in descriptive and\ndiagnostic insights\nExtensive hands-on experience with Postgres, Redshift, Airflow, MongoDB, and\nreal-time data workflows.\n\nTechnical Skills:\nExpertise in Python, SQL, and PySpark\nStrong background in software architecture and scalable development practices.\nTableau, Metabase or similar viz tools experience\nWorking knowledge of AI frameworks and libraries is a plus.\nLeadership & Communication:\nDemonstrates ownership and accountability for delivery with a strong\ncommitment to quality.\nExcellent communication skills with a history of effective client and end-user\nengagement.\nStartup & Fintech Mindset:\nAdaptability and agility to thrive in a fast-paced, early-stage startup environment.\nPassion for fintech innovation and a strong desire to make a meaningful impact\non the future of finance.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'performance optimization strategies', 'PySpark', 'Django', 'cost management', 'AWS', 'AI frameworks', 'Python', 'SQL']",2025-06-12 15:01:37
Data Science,Global Banking Organization,5 - 10 years,Not Disclosed,['Bengaluru'],"Key Skills: Machine Learning, Data Science, Azure, Python, Hadoop.\nRoles and Responsibilities:\nStrong understanding of Math, Statistics, and the theoretical foundations of Statistical & Machine Learning, including Parametric and Non-parametric models.\nApply advanced data mining techniques to curate, process, and transform raw data into reliable datasets.\nUse various statistical techniques and ML methods to perform predictive modeling/classification for problems related to clients, distribution, sales, client profiles, and segmentation, and provide actionable insights for business decision-making.\nDemonstrate expertise in the full Machine Learning lifecycle--feature engineering, training, validation, scaling, deployment, scoring, monitoring, and feedback loops.\nProficiency in Python visualization libraries such as matplotlib and seaborn.\nExperience with cloud computing infrastructure like Azure, including Machine Learning Studio, Azure Data Factory, Synapse, Python, and PySpark.\nAbility to develop, test, and deploy models on cloud/web platforms.\nExcellent knowledge of Deep Learning Architectures, including Convolutional Neural Networks and Transformer/LLM Foundation Models.\nStrong expertise in supervised and adversarial learning techniques.\nRobust working knowledge of deep learning frameworks such as TensorFlow, Keras, and PyTorch.\nExcellent Python coding skills.\nExperience with version control tools (Git, GitHub/GitLab) and data version control.\nExperience in end-to-end model deployment and productionization.\nDemonstrated proficiency in deploying, scaling, and optimizing ML models in production environments with low latency, high availability, and cost efficiency.\nSkilled in model interpretability and CI/CD for ML using tools like MLflow and Kubeflow, with the ability to implement automated monitoring, logging, and retraining strategies.\nExperience Requirement:\n5-12 years of experience in designing and deploying deep learning and machine learning solutions.\nProven track record of delivering AI/ML solutions in real-world business applications at scale.\nHands-on experience working in cross-functional teams including data engineers, product managers, and business stakeholders.\nExperience mentoring junior data scientists and providing technical leadership within a data science team.\nExperience working with big data tools and environments such as Hadoop, Spark, or Databricks is a plus.\nPrior experience in managing model lifecycle in enterprise production environments including drift detection and retraining pipelines.\nEducation: B.Tech.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure', 'Hadoop.', 'Machine Learning', 'Python']",2025-06-12 15:01:40
Lead Data Engineer,Prolegion,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Lead Data Engineer/Associate Architect to lead the design, implementation, and optimization of scalable data architectures. The ideal candidate will have a deep understanding of data modeling, ETL processes, cloud data solutions, and big data technologies. You will work closely with cross-functional teams to build robust, high-performance data pipelines and infrastructure to enable data-driven decision-making.\n\nExperience: 8 - 12+ years\nWork Location: Hyderabad (Hybrid)\nMandatory skills: Python, SQL, Snowflake\n\nResponsibilities:\nDesign and Develop scalable and resilient data architectures that support business needs, analytics, and AI/ML workloads.\nData Pipeline Development: Design and implement robust ETL/ELT processes to ensure efficient data ingestion, transformation, and storage.\nBig Data & Cloud Solutions: Architect data solutions using cloud platforms like AWS, Azure, or GCP, leveraging services such as Snowflake, Redshift, BigQuery, and Databricks.\nDatabase Optimization: Ensure performance tuning, indexing strategies, and query optimization for relational and NoSQL databases.\nData Governance & Security: Implement best practices for data quality, metadata management, compliance (GDPR, CCPA), and security.\nCollaboration & Leadership: Work closely with data engineers, analysts, and business stakeholders to translate business requirements into scalable solutions.\nTechnology Evaluation: Stay updated with emerging trends, assess new tools and frameworks, and drive innovation in data engineering.\n\nRequired Skills:\nEducation: Bachelors or Masters degree in Computer Science, Data Engineering, or a related field.\nExperience: 8 - 12+ years of experience in data engineering\nCloud Platforms: Strong expertise in AWS data services.\nBig Data Technologies: Experience with Hadoop, Spark, Kafka, and related frameworks.\nDatabases: Hands-on experience with SQL, NoSQL, and columnar databases such as PostgreSQL, MongoDB, Cassandra, and Snowflake.\nProgramming: Proficiency in Python, Scala, or Java for data processing and automation.\nETL Tools: Experience with tools like Apache Airflow, Talend, DBT, or Informatica.\nMachine Learning & AI Integration (Preferred): Understanding of how to architect data solutions for AI/ML applications\n\n,",Industry Type: Defence & Aerospace,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'Data modeling', 'Postgresql', 'Informatica', 'Apache', 'Analytics', 'SQL', 'Python']",2025-06-12 15:01:42
Gcp Data Engineer,Saama Technologies,3 - 8 years,Not Disclosed,"['Pune', 'Chennai', 'Coimbatore']","We are looking for immediate joiners only.\nPosition: GCP Data Engineer\nWe are seeking a skilled and experienced GCP Data Engineer to join our dynamic team. The ideal candidate will have a strong background in Google Cloud Platform (GCP), BigQuery, Dataform, and data warehouse concepts. Experience with Airflow/Cloud Composer and cloud computing knowledge will be a significant advantage.\nResponsibilities:\n- Designing, developing, and maintaining data pipelines and workflows on the Google Cloud Platform.",,,,"['Pyspark', 'GCP', 'Python', 'SQL', 'Google Cloud Platforms']",2025-06-12 15:01:44
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Gurugram'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nBengaluru\nSenior Flexera Data Analyst\nData Science\nIndia\nChennai\nData Scientist\nData Science\nIndia\nBengaluru\nGurugram, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Gurugram\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-12 15:01:46
Senior Data Engineer -Bangalore,Happiest Minds Technologies,6 - 10 years,Not Disclosed,['Bengaluru'],"Job Overview:\nThe primary purpose of this role is to translate business requirements and functional specifications into logical program designs and to deliver dashboards, schema, data pipelines, and software solutions. This includes developing, configuring, or modifying data components within various complex business and/or enterprise application solutions in various computing environments. You will partner closely with multiple Business partners, Product Owners, Data Strategy, Data Platform, Data Science and Machine Learning (MLOps) teams to drive innovative data products for end users. Additionally, you will help shape overall solution & data products, develop scalable solutions through best-in-class engineering practices.",,,,"['NoSQL', 'big data systems', 'Data Pipeline', 'MongoDB', 'SQL', 'Hive', 'GIT', 'Hadoop', 'Kafka', 'Agile', 'MQL', 'Ci/Cd']",2025-06-12 15:01:49
Azure Data Engineer ( Azure Databricks),Apex One,4 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Job Summary\nWe are seeking a skilled Azure Data Engineer with 4 years of overall experience, including at least 2 years of hands-on experience with Azure Databricks (Must). The ideal candidate will have strong expertise in building and maintaining scalable data pipelines and working across cloud-based data platforms.\nKey Responsibilities\nDesign, develop, and optimize large-scale data pipelines using Azure Data Factory, Azure Databricks, and Azure Synapse.\nImplement data lake solutions and work with structured and unstructured datasets in Azure Data Lake Storage (ADLS).\nCollaborate with data scientists, analysts, and engineering teams to design and deliver end-to-end data solutions.\nDevelop ETL/ELT processes and integrate data from multiple sources.\nMonitor, debug, and optimize workflows for performance and cost-efficiency.\nEnsure data governance, quality, and security best practices are maintained.\nMust-Have Skills\n4+ years of total experience in data engineering.\n2+ years of experience with Azure Databricks (PySpark, Notebooks, Delta Lake).\nStrong experience with Azure Data Factory, Azure SQL, and ADLS.\nProficient in writing SQL queries and Python/Scala scripting.\nUnderstanding of CI/CD pipelines and version control systems (e.g., Git).\nSolid grasp of data modeling and warehousing concepts.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Azure Data Factory', 'SQL queries', 'PySpark', 'Delta Lake', 'Azure Databricks', 'Notebooks', 'Azure SQL']",2025-06-12 15:01:51
It Recruiter,IonIdea,0 - 3 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nTalent Sourcing: Utilize various channels such as job boards, social media, LinkedIn, networking events, and internal databases to source and attract high-quality candidates for a variety of technical positions (software developers, systems engineers, data scientists, etc.).\nCandidate Screening: Review resumes, conduct initial phone screenings, and assess candidates technical skills, experience, and cultural fit.\nInterview Coordination: Schedule and facilitate interviews with hiring managers, ensuring a smooth and efficient process for all parties involved.\nCandidate Engagement: Build relationships with both active and passive candidates to maintain a strong pipeline of qualified talent. Keep candidates informed throughout the hiring process.\nOffer Management: Work with HR and hiring managers to present offers, negotiate terms, and ensure a positive candidate experience during the offer process.\n\nQualifications:\nExperience: Fresher-3years\n\nTechnical Knowledge: A solid understanding of IT roles, including knowledge of programming languages, software development frameworks, network infrastructure, cloud technologies, and emerging IT trends.\nRecruitment Tools: Proficient in using Applicant Tracking Systems (ATS), job boards (e.g., LinkedIn, Indeed), and social media platforms for sourcing candidates.\nCommunication Skills: Excellent written and verbal communication skills with the ability to engage with both technical and non-technical stakeholders.",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['IT Recruitment', 'C2H', 'Contract Hiring']",2025-06-12 15:01:53
Sr Engineer/Sr. Lead - Generative AI,Qualcomm,3 - 8 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\n\nLocation - Hyderabad\n\n\nExperience - 3-8 Years\n\n\nWe are seeking an experienced Machine Learning Engineers specializing in Generative AI to join our core AI team.\n\nThe ideal candidate will be responsible for designing, developing, and deploying cutting-edge generative AI solutions, with a focus on Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Intelligent agent systems.\n\nKey Responsibilities:\n\nDesign and implement RAG-based solutions to enhance LLM capabilities with external knowledge sources\n\nDevelop and optimize LLM fine-tuning strategies for specific use cases and domain adaptation\n\nCreate robust evaluation frameworks for measuring and improving model performance\n\nBuild and maintain agentic workflows for autonomous AI systems\n\nCollaborate with cross-functional teams to identify opportunities and implement AI solutions\n\n\nRequired Qualifications:\n\nBachelor's or Master's degree in Computer Science, or related technical field\n\n3+ years of experience in Machine Learning/AI engineering\n\nStrong programming skills in Python and experience with ML frameworks (PyTorch, TensorFlow)\n\nPractical experience with LLM deployments and fine-tuning\n\nExperience with vector databases and embedding models\n\nFamiliarity with modern AI/ML infrastructure and cloud platforms (AWS, GCP, Azure)\n\nStrong understanding of RAG architectures and implementation\n\n\nPreferred Qualifications:\n\nExperience with popular LLM frameworks (Langchain, LlamaIndex, Transformers)\n\nKnowledge of prompt engineering and chain-of-thought techniques\n\nExperience with containerization and microservices architecture\n\nBackground in NLP and deep learning\n\nBackground in Reinforcement Learning\n\nContributions to open-source AI projects\n\nExperience with ML ops and model deployment pipelines\n\n\nSkills and Competencies:\n\nStrong problem-solving and analytical skills\n\nExcellent communication and collaboration abilities\n\nExperience with agile development methodologies\n\nAbility to balance multiple projects and priorities\n\nStrong focus on code quality and best practices\n\nUnderstanding of AI ethics and responsible AI development",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'software engineering', 'c++', 'c', 'natural language processing', 'microsoft azure', 'artificial intelligence', 'microservices', 'reinforcement learning', 'deep learning', 'java', 'computer science', 'gcp', 'agile', 'aws', 'ml']",2025-06-12 15:01:56
AI/ML framework Staff Engineer,Qualcomm,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nLooking for ""ML framework and AI compiler Engineer"" responsible for\nDesigning, implementing, and deploying machine learning models using PyTorch\nFocusing on backend infrastructure and system architecture.\nResponsibilities often include developing framework, integrating with other AI tools, and ensuring scalability and reliability.\n\nHere's a more detailed breakdown of what you might see in such a job description:\n\nKey Responsibilities:\n\n\nModel Development and DeploymentDesigning, building, and deploying AI models, particularly those leveraging PyTorch for deep learning.\n\n\nBackend InfrastructureDeveloping and maintaining the backend systems that power AI applications, including data ingestion, processing, and storage.\n\n\nSystem ArchitectureDesigning scalable and high-performance backend architectures to handle AI workloads.\n\n\nModel OptimizationOptimizing model performance for speed, accuracy, and resource efficiency.\n\n\nIntegrationIntegrating AI models with other systems and applications.\n\n\nAPI DevelopmentCreating and maintaining APIs for communication between frontend and backend components.\n\n\nData HandlingManaging data ingestion, preprocessing, and storage for AI training and inference.\n\n\nCollaborationWorking with data scientists, product managers, and other engineers to bring AI solutions to life.\n\nTools, Technologies, Skills and Programming:\n\n\nC, C++: Strong programming capability using advanced techniques to design and develop AI compilers and backends.\n\n\nScripting: Strong expertise in Python with design, develop, release and maintain projects.\n\n\nAI Frameworks: Familiarity with other AI frameworks like PyTorch, TensorFlow, Hugging Face, etc.\n\n\nMachine Learning Knowledge: Understanding of machine learning principles and algorithms starting Computer vision to large language models and continuously update to new trends.\nExpertise to deep learning accelerator programming (GPU, NPU). Any parallel programming experience (Like CUDA, OpenCL, MKLDNN ..etc) is a plus.\nExperience with deep leaning compilers like Glow, TVM ""etc is a plus.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'system engineering', 'c#', 'cuda', 'algorithms', 'c++', 'parallel programming', 'artificial intelligence', 'opencl', 'deep learning', 'java', 'product management', 'computer vision', 'asp.net', 'multithreading', 'mvc', 'ml']",2025-06-12 15:01:58
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-12 15:02:01
AI Engineer - Lead,Blend360 India,6 - 10 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for someone who is ready for the next step in their career and is excited by the idea of solving problems and designing best in class. However, they also need to be aware of the practicalities of making a difference in the real world - whilst we love innovative advanced solutions, we also believe that sometimes a simple solution can have the most impact.\nOur AI Engineer is someone who feels the most comfortable around solving problems, answering questions and proposing solutions. We place a high value on the ability to communicate and translate complex analytical thinking into non-technical and commercially oriented concepts, and experience working on difficult projects and/or with demanding stakeholders is always appreciated.\nWhat can you expect from the role?\nContribute to design, develop, deploy and maintain AI solutions\nUse a variety of AI Engineering tools and methods to deliver\nOwn parts of projects end-to-end\nContributing to solutions design and proposal submissions\nSupporting the development of the AI engineering team within Blend\nMaintain in-depth knowledge of the AI ecosystems and trends\nMentor junior colleagues\n\n\nQualifications\nContribute to the design, development, testing, deployment, maintenance, and improvement of robust, scalable, and reliable software systems, adhering to best practices.",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Version control', 'orchestration', 'GIT', 'GCP', 'Analytical', 'System integration', 'Software development life cycle', 'Mentor', 'Monitoring', 'Python']",2025-06-12 15:02:03
Data Engineering Manager,NOVARTIS,6 - 8 years,Not Disclosed,['Hyderabad'],"Summary\nWe are seeking a highly skilled and motivated GCP Data Engineering Manager to join our dynamic team. As a Data Engineering manager specializing in Google Cloud Platform (GCP), you will play a crucial role in designing, implementing, and maintaining scalable data pipelines and\nsystems. You will leverage your expertise in Google Big Query, SQL, Python, and analytical skills to drive data-driven decision-making processes and support various business functions.\nAbout the Role\nKey Responsibilities:\nData Pipeline Development: Design, develop, and maintain robust data pipelines using GCP services like Dataflow, Dataproc, ensuring high performance and scalability.\nGoogle Big Query Expertise: Utilize your hands-on experience with Google Big Query to manage and optimize data storage, retrieval, and processing.\nSQL Proficiency: Write and optimize complex SQL queries to transform and analyze large datasets, ensuring data accuracy and integrity.\nPython Programming: Develop and maintain Python scripts for data processing, automation, and integration with other systems and tools.\nData Integration: Collaborate with data analysts, and other stakeholders to integrate data from various sources, ensuring seamless data flow and consistency.\nData Quality and Governance: Implement data quality checks, validation processes, and governance frameworks to maintain high data standards.\nPerformance Tuning: Monitor and optimize the performance of data pipelines, queries, and storage solutions to ensure efficient data processing.\nDocumentation: Create comprehensive documentation for data pipelines, processes, and best practices to facilitate knowledge sharing and team collaboration.\nMinimum Qualifications:\nProven experience (minimum 6 - 8 yrs) in Data Engineer, with significant hands-on experience in Google Cloud Platform (GCP) and Google Big Query.\nProficiency in SQL for data transformation, analysis and performance optimization.\nStrong programming skills in Python, with experience in developing data processing scripts and automation.\nProven analytical skills with the ability to interpret complex data and provide actionable insights.\nExcellent problem-solving abilities and attention to detail.\nStrong communication and collaboration skills, with the ability to work effectively in a team enviro\nDesired Skills :\nExperience with Google Analytics data and understanding of digital marketing data.\nFamiliarity with other GCP services such as Cloud Storage, Dataflow, Pub/Sub, and Dataproc.\nKnowledge of data visualization tools such as Looker, Tableau, or Data Studio.\nExperience with machine learning frameworks and libraries.\nWhy Novartis: Helping people with disease and their families takes more than innovative science. It takes a community of smart, passionate people like you. Collaborating, supporting and inspiring each other. Combining to achieve breakthroughs that change patients lives. Ready to create a brighter future together? https://www. novartis. com / about / strategy / people-and-culture\nJoin our Novartis Network: Not the right Novartis role for you? Sign up to our talent community to stay connected and learn about suitable career opportunities as soon as they come up: https://talentnetwork. novartis. com/network\nBenefits and Rewards: Read our handbook to learn about all the ways we ll help you thrive personally and professionally:",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'Google Analytics', 'Machine learning', 'Data processing', 'Data quality', 'data visualization', 'Digital marketing', 'SQL', 'Python']",2025-06-12 15:02:06
GCP Data Engineer,TVS Next,3 - 5 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['kubernetes', 'pyspark', 'data pipeline', 'sql', 'docker', 'cloud', 'tensorflow', 'java', 'spark', 'gcp', 'pytorch', 'bigquery', 'programming', 'ml', 'cloud sql', 'cd', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'application engine', 'machine learning', 'apache flink', 'data engineering', 'dataproc', 'kafka', 'cloud storage', 'terraform', 'bigtable']",2025-06-12 15:02:08
Data Engineer,Luxoft,5 - 8 years,Not Disclosed,['Pune'],"Help Group Enterprise Architecture team to develop our suite of EA tools and workbenches\nWork in the development team to support the development of portfolio health insights\nBuild data applications from cloud infrastructure to visualization layer\nProduce clear and commented code\nProduce clear and comprehensive documentation\nPlay an active role with technology support teams and ensure deliverables are completed or escalated on time\nProvide support on any related presentations, communications, and trainings\nBe a team player, working across the organization with skills to indirectly manage and influence\nBe a self-starter willing to inform and educate others\nSkills\nMust have\nB.Sc./M.Sc. degree in computing or similar\n5-8+ years experience as a Data Engineer, ideally in a large corporate environment\nIn-depth knowledge of SQL and data modelling/data processing\nStrong experience working with Microsoft Azure\nExperience with visualisation tools like PowerBI (or Tableau, QlikView or similar)\nExperience working with Git, JIRA, GitLab\nStrong flair for data analytics\nStrong flair for IT architecture and IT architecture metrics\nExcellent stakeholder interaction and communication skills\nUnderstanding of performance implications when making design decisions to deliver performant and maintainable software.\nExcellent end-to-end SDLC process understanding.\nProven track record of delivering complex data apps on tight timelines\nFluent in English both written and spoken.\nPassionate about development with focus on data and cloud\nAnalytical and logical, with strong problem solving skills\nA team player, comfortable with taking the lead on complex tasks\nAn excellent communicator who is adept in, handling ambiguity and communicating with both technical and non-technical audiences\nComfortable with working in cross-functional global teams to effect change\nPassionate about learning and developing your hard and soft professional skills\nNice to have\nExperience working in the financial industry\nExperience in complex metrics design and reporting\nExperience in using artificial intelligence for data analytics\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Power BI Developer\nBI Engineering\nIndia\nBengaluru\nSenior Power BI Developer\nBI Engineering\nIndia\nChennai\nSenior Power BI Developer\nBI Engineering\nIndia\nGurugram\nPune, India\nReq. VR-114797\nBI Engineering\nBCM Industry\n02/06/2025\nReq. VR-114797\nApply for Data Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Enterprise architecture', 'Analytical', 'Artificial Intelligence', 'Data processing', 'Data analytics', 'QlikView', 'JIRA', 'SDLC', 'SQL']",2025-06-12 15:02:11
Data Engineer II - Marketplace (Experimentation Track),Booking Holdings,5 - 10 years,Not Disclosed,['Bengaluru'],"We are looking for a Data Engineer to join our team and help us to improve the platform that supports one of the best experimentation tools in the world.\nYou will work side by side with other data engineers and site reliability engineers to improve the reliability, scalability, maintenance and operations of all the data products that are part of the experimentation tool at Booking.com.\nYour day to day work includes but is not limited to: maintenance and operations of data pipelines and products that handles data at big scale; the development of capabilities for monitoring, alerting, testing and troubleshooting of the data ecosystem of the experiment platform; and the delivery of data products that produce metrics for experimentation at scale. You will collaborate with colleagues in Amsterdam to achieve results the right way. This will include engineering managers, product managers, engineers and data scientists.\nKey Responsibilities and Duties\nTake ownership of multiple data pipelines and products and provide innovative solutions to reduce the operational workload required to maintain them\nRapidly developing next-generation scalable, flexible, and high-performance data pipelines.\nContribute to the development of data platform capabilities such as testing, monitoring, debugging and alerting to improve the development environment of data products\nSolve issues with data and data pipelines, prioritizing based on customer impact.\nEnd-to-end ownership of data quality in complex datasets and data pipelines.\nExperiment with new tools and technologies, driving innovative engineering solutions to meet business requirements regarding performance, scaling, and data quality.\nProvide self-organizing tools that help the analytics community discover data, assess quality, explore usage, and find peers with relevant expertise.\nServe as the main point of contact for technical and business stakeholders regarding data engineering issues, such as pipeline failures and data quality concerns\nRole requirements\nMinimum 5 years of hands-on experience in data engineering as a Data Engineer or as a Software Engineer developing data pipelines and products.\nBachelors degree in Computer Science, Computer or Electrical Engineering, Mathematics, or a related field or 5 years of progressively responsible experience in the specialty as equivalent\nSolid experience in at least one programming language. We use Java and Python\nExperience building production data pipelines in the cloud, setting up data-lakes and server-less solutions\nHands-on experience with schema design and data modeling\nExperience designing systems E2E and knowledge of basic concepts (lb, db, caching, NoSQL, etc)\nKnowledge of Flink, CDC, Kafka, Airflow, Snowflake, DBT or equivalent tools\nPractical experience building data platform capabilities like testing, alerting, monitoring, debugging, security\nExperience working with big data.\nExperience working with teams located in different timezones is a plus\nExperience with experimentation, statistics and A/B testing is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Airflow', 'Java', 'CDC', 'NoSQL', 'Snowflake', 'DBT', 'Kafka', 'Python']",2025-06-12 15:02:13
Microsoft Fabrics Data Engineer,Swits Digital,5 - 10 years,Not Disclosed,['Bengaluru'],"Job TItle: Microsoft Fabric Data Engineer\nLocation: Bangalore\nJob Type: Conract (24 Months)\nJob Description:\nWe are seeking a highly skilled and experienced Microsoft Fabric Data Engineer/Architect to design, develop, and maintain robust, scalable, and secure data solutions within the Microsoft Fabric ecosystem. This role will leverage the full suite of Microsoft Azure data services, including Azure Data Bricks, Azure Data Factory, and Azure Data Lake, to build end-to-end data pipelines, data warehouses, and data lakehouses that enable advanced analytics and business intelligence.\nRequired Skills & Qualifications:\nBachelors degree in Computer Science, Engineering, or a related field.\n5+ years of experience in data architecture and engineering, with a strong focus on Microsoft Azure data platforms.\nProven hands-on expertise with Microsoft Fabric and its components, including:\nOneLake\nData Factory (Pipelines, Dataflows Gen2)\nSynapse Analytics (Data Warehousing, SQL analytics endpoint)\nLakehouses and Warehouses\nNotebooks (PySpark)\nExtensive experience with Azure Data Bricks, including Spark development (PySpark, Scala, SQL).\nStrong proficiency in Azure Data Factory for building and orchestrating ETL/ELT pipelines.\nDeep understanding and experience with Azure Data Lake Storage Gen2.\nProficiency in SQL (T-SQL, Spark SQL), Python, and/or other relevant scripting languages.\nSolid understanding of data warehousing concepts, dimensional modeling, and data lakehouse architectures.\nExperience with data governance principles and tools (e.g., Microsoft Purview).\nFamiliarity with CI/CD practices, version control (Git), and DevOps for data pipelines.\nExcellent problem-solving, analytical, and communication skills.\nAbility to work independently and collaboratively in a fast-paced, agile environment.\nPreferred Qualifications:\nMicrosoft certifications in Azure Data Engineering (e.g., DP-203, DP-600: Microsoft Fabric Analytics Engineer Associate).\nExperience with Power BI for data visualization and reporting.\nFamiliarity with real-time analytics and streaming data processing.\nExposure to machine learning workflows and integrating ML models with data solutions",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Analytical', 'microsoft azure', 'data visualization', 'microsoft', 'Business intelligence', 'Data warehousing', 'Analytics', 'Data architecture', 'Python']",2025-06-12 15:02:16
Sr Data Engineer,Lowes Services India Private limited,5 - 10 years,Not Disclosed,['Bengaluru'],"We are seeking a seasoned Senior Data Engineer to join our Marketing Data Platform team. This role is pivotal in designing, building, and optimizing scalable data pipelines and infrastructure that support our marketing analytics and customer engagement strategies. The ideal candidate will have extensive experience with big data technologies, cloud platforms, and a strong understanding of marketing data dynamics.\n\nData Pipeline Development & Optimization\nDesign, develop, and maintain robust ETL/ELT pipelines using Apache PySpark on GCP services like Dataproc and Cloud Composer.\nEnsure data pipelines are scalable, efficient, and reliable to handle large volumes of marketing data.\nData Warehousing & Modeling\nImplement and manage data warehousing solutions using BigQuery, ensuring optimal performance and cost-efficiency.\nDevelop and maintain data models that support marketing analytics and reporting needs.\nCollaboration & Stakeholder Engagement\nWork closely with marketing analysts, data scientists, and cross-functional teams to understand data requirements and deliver solutions that drive business insights.\nTranslate complex business requirements into technical specifications and data architecture.\nData Quality & Governance\nImplement data quality checks and monitoring to ensure the accuracy and integrity of marketing data.\nAdhere to data governance policies and ensure compliance with data privacy regulations.\nContinuous Improvement & Innovation\nStay abreast of emerging technologies and industry trends in data engineering and marketing analytics.\nPropose and implement improvements to existing data processes and infrastructure\n  Years of Experience\n5 Years in Data Engineer space\n  Education Qualification & Certifications\nB.Tech or MCA\n  Experience\nProven experience with Apache PySpark, GCP (including Dataproc, BigQuery, Cloud Composer), and data pipeline orchestration.\nTechnical Skills\nProficiency in SQL and Python.\nExperience with data modeling, ETL/ELT processes, and data warehousing concepts.",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['orchestration', 'Data modeling', 'data governance', 'Data quality', 'Apache', 'Continuous improvement', 'Monitoring', 'SQL', 'Python', 'Data architecture']",2025-06-12 15:02:18
Associate Advanced Services Engineer (Oracle Apps/Cloud),Oracle,3 - 6 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","The Customer Success Services (CSS) is a unit within Oracle s Customer Service Organization that enables long term advanced support relationships with many of Oracles largest customers.\nGlobal Expertise Center Engineers (GEC) provide support in the continuous operational improvement of Oracle environments.\nGEC Engineers do this by leveraging Oracle s support-based intellectual property and customers experiences throughout their involvement with Oracle s technologies.\nOur goal is for every customer to gain ever-more value from their Oracle Solutions by helping them make well informed decisions regarding the implementation; management and use of Oracle technologies.\nCareer Level - IC1\nB.E./B.Tech. with 3 to 12 months of relevant experience.\nDemonstrates good communication, customer management, customer engagement and project management skills.\nTechnical aptitude: Has a basic understanding of application, middleware, hardware and/or Cloud technologies, and curiosity in cloud technology concepts such as Artificial Intelligence, Blockchain, Machine Learning, DevOps, Security and Oracle Cloud infrastructure.\nFluency in English.\nWork involves some problem solving with assistance and guidance in understanding and applying Oracle policies and procedures.\nAble to demonstrate time management.\nGoal oriented individual.\nAble to complete individual goals as well as work in a team environment.\nDemonstrated ability to communicate using technical concepts. Working knowledge of Oracle products a plus but not necessary.\nProfessional demeanor.\nAvailability to work in scheduled out of hours operations when required",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle Apps', 'Time management', 'Artificial Intelligence', 'Project management', 'Machine learning', 'Intellectual property', 'Customer service', 'Oracle', 'Customer engagement', 'Middleware']",2025-06-12 15:02:20
Associate Advanced Services Engineer (Oracle Apps/Cloud),Oracle,3 - 6 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","The Customer Success Services (CSS) is a unit within Oracle s Customer Service Organization that enables long term advanced support relationships with many of Oracles largest customers.\nGlobal Expertise Center Engineers (GEC) provide support in the continuous operational improvement of Oracle environments.\nGEC Engineers do this by leveraging Oracle s support-based intellectual property and customers experiences throughout their involvement with Oracle s technologies.\nOur goal is for every customer to gain ever-more value from their Oracle Solutions by helping them make well informed decisions regarding the implementation; management and use of Oracle technologies.\nCareer Level - IC1\nB.E./B.Tech. with 3 to 12 months of relevant experience.\nDemonstrates good communication, customer management, customer engagement and project management skills.\nTechnical aptitude: Has a basic understanding of application, middleware, hardware and/or Cloud technologies, and curiosity in cloud technology concepts such as Artificial Intelligence, Blockchain, Machine Learning, DevOps, Security and Oracle Cloud infrastructure.\nFluency in English.\nWork involves some problem solving with assistance and guidance in understanding and applying Oracle policies and procedures.\nAble to demonstrate time management.\nGoal oriented individual.\nAble to complete individual goals as well as work in a team environment.\nDemonstrated ability to communicate using technical concepts. Working knowledge of Oracle products a plus but not necessary.\nProfessional demeanor.\nAvailability to work in scheduled out of hours operations when required",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle Apps', 'Time management', 'Artificial Intelligence', 'Project management', 'Machine learning', 'Intellectual property', 'Customer service', 'Oracle', 'Customer engagement', 'Middleware']",2025-06-12 15:02:24
Senior Data Scientist,Straive,5 - 10 years,Not Disclosed,"['Hyderabad', 'Gurugram', 'Bengaluru']","Role & responsibilities\nRequires 5-8 years of proven experience in banking/payments/other domains\nStrong experience in developing Machine Learning models, Python & SQL\nExperience working with pre-trained models, awareness of state-of-art in embeddings and applicability for use cases\nDetailed oriented with a proactive mindset towards problem-solving\nExcellent communication and presentation skills with the ability to convey complex information clearly and concisely",,,,"['Machine Learning', 'Python', 'SQL', 'Xgboost', 'Neural Networks', 'Random Forest']",2025-06-12 15:02:26
Senior Data Engineer,Eurofins Digital Testing,7 - 10 years,Not Disclosed,['Bengaluru'],"Job Description\nSenior Data Engineer - Quality Engineering\nExperience Range: 7-10 Years\nLocation: Bangalore (Hybrid Mode)\nResillion, a leading quality engineering company with offices around the world, is seeking a talented Data Engineer to join our growing India team. In this role, you will play a critical part in building and maintaining the data infrastructure that supports our AI-powered testing tools and analytics solutions. You will have technical responsibility for the entire data lifecycle, from data acquisition and ingestion to storage, processing, and analysis.\nResponsibilities:\nCollaborate with GenAI engineers, Software Engineers, Test automation engineers and other stakeholders within Resillion to understand data needs and translate them into technical solutions, including designing data pipelines for training & deploying models and data pre-processing for AI, including generative AI applications.\nDesign, implement, and advise on data migration testing strategies and data quality assurance strategies for Resillion customers, ensuring a smooth transition of data to new customer systems.\nDesign, develop, and implement scalable data pipelines using cloud-based data engineering tools and technologies, with a focus on both Microsoft Azure solutions (e.g., Azure Data Factory, Azure Databricks) and Google Cloud Platform (GCP) solutions (e.g., Google Cloud Dataflow, Google Cloud Dataproc).\nWrite efficient and maintainable code to extract, transform, and load data from various sources, leveraging your expertise in Azure Data Lake Storage and other relevant Azure services, as well as Google Cloud Storage and other relevant GCP services.\nBuild and manage data warehouses and data lakes for quality engineering data, utilizing your knowledge of Azure Synapse Analytics or similar technologies, and Google BigQuery or similar technologies.\nDevelop and implement data quality checks and monitoring procedures to ensure data integrity using Azure Data Catalog or other appropriate tools, as well as Google Cloud Data Catalog or other appropriate tools.\nAutomate data engineering tasks and workflows using Azure automation tools and GCP automation tools (e.g., Cloud Functions, Cloud Composer).\nSet up quality intelligence dashboards for quality assurance data using Microsoft Power BI to provide stakeholders with clear and actionable insights.\nStay up-to-date with the latest data engineering tools and technologies, including advancements in AI, Machine Learning (MLOps), and generative AI for data processing in both the Azure and GCP environments.\nAdvise on and implement test data generation strategies and solutions for various testing needs.\n\n\nQualifications\nMinimum 7+ years of experience in data engineering or a related field\nProven experience in designing, developing, and deploying data pipelines",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Engineering services', 'Manager Quality Assurance', 'GCP', 'Quality engineering', 'Machine learning', 'Quality Engineer', 'Monitoring', 'Analytics', 'SQL', 'Python']",2025-06-12 15:02:28
Senior Data Engineer,Talentien Global Solutions,4 - 8 years,12-18 Lacs P.A.,"['Hyderabad', 'Chennai', 'Coimbatore']","We are seeking a skilled and motivated Data Engineer to join our dynamic team. The ideal candidate will have experience in designing, developing, and maintaining scalable data pipelines and architectures using Hadoop, PySpark, ETL processes, and Cloud technologies.\n\nResponsibilities:\nDesign, develop, and maintain data pipelines for processing large-scale datasets.\nBuild efficient ETL workflows to transform and integrate data from multiple sources.\nDevelop and optimize Hadoop and PySpark applications for data processing.\nEnsure data quality, governance, and security standards are met across systems.\nImplement and manage Cloud-based data solutions (AWS, Azure, or GCP).\nCollaborate with data scientists and analysts to support business intelligence initiatives.\nTroubleshoot performance issues and optimize query executions in big data environments.\nStay updated with industry trends and advancements in big data and cloud technologies.\nRequired Skills:\nStrong programming skills in Python, Scala, or Java.\nHands-on experience with Hadoop ecosystem (HDFS, Hive, Spark, etc.).\nExpertise in PySpark for distributed data processing.\nProficiency in ETL tools and workflows (SSIS, Apache Nifi, or custom pipelines).\nExperience with Cloud platforms (AWS, Azure, GCP) and their data-related services.\nKnowledge of SQL and NoSQL databases.\nFamiliarity with data warehousing concepts and data modeling techniques.\nStrong analytical and problem-solving skills.\n\nInterested can reach us at +91 7305206696/ saranyadevib@talentien.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Hadoop', 'Spark', 'ETL', 'Airflow', 'Etl Pipelines', 'Big Data', 'EMR', 'Gcp Cloud', 'Data Bricks', 'Azure Cloud', 'Data Pipeline', 'SCALA', 'Snowflake', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'AWS', 'Python']",2025-06-12 15:02:30
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Bengaluru'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nGurugram\nSenior Flexera Data Analyst\nData Science\nIndia\nChennai\nBusiness Analyst\nData Science\nPoland\nRemote Poland\nBengaluru, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Bengaluru\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-12 15:02:33
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Chennai'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nGurugram\nSenior Flexera Data Analyst\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nChennai, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Chennai\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-12 15:02:35
GStreamer multimedia framework Lead Engineer Senior,Qualcomm,3 - 8 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nWe are seeking a skilled Engineer with extensive experience in the GStreamer multimedia framework. The ideal candidate will be responsible for designing, developing, and optimizing multimedia applications and systems. This role requires a deep understanding of multimedia processing, pipeline architecture, and the ability to work on complex projects.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\nExperience with majority in Multimedia framework & Gstreamer plugins development. Strong programming skills in C and C++ for embedded systems Good knowledge about AI/ML applications developements Exposure to developing solutions on Linux is must Strong in multi-threaded programming, synchronization and IPCs Strong Software design skills and ability to guide team of engineers Good knowledge on software development processes Need very good Communication skills and ability to work with cross functional teams Exposure to other media frameworks such as ffmpeg, directshow, stagefright is a plus Good knowledge on V4L2, Pulseaudio, Alsa, OpenGLES is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'c', 'embedded systems', 'linux', 'gstreamer', 'python', 'software development', 'software design', 'plugins', 'pipeline architecture', 'artificial intelligence', 'multimedia', 'stagefright', 'multimedia framework', 'java', 'synchronization', 'multithreading', 'software engineering', 'ffmpeg', 'ml']",2025-06-12 15:02:38
Sr Manager of Software Engineering,JPMorgan Chase Bank,14 - 20 years,Not Disclosed,['Bengaluru'],"When you mentor and advise multiple technical teams and move financial technologies forward, it s a big challenge with big impact. You were made for this.\n\n\nAs a Senior Manager of Software Engineering at JPMorgan Chase within the Consumer and Community Banking Technology Team, you serve in a leadership role by providing technical coaching and advisory for multiple technical teams, as well as anticipate the needs and potential dependencies of other functions within the firm. As an expert in your field, your insights influence budget and technical considerations to advance operational efficiencies and functionalities.\n\nJob responsibilities\n\n\n\nProvide direction, oversight, and coaching for a team of entry-level to mid-level software engineers working on basic to moderately complex tasks.\n\nBe accountable for decisions affecting team resources, budget, tactical operations, and the execution and implementation of processes and procedures.\n\nLead the design, development, testing, and implementation of data visualization projects to support business objectives.\n\nCollaborate with data analysts, data scientists, and business stakeholders to understand data requirements and translate them into effective visual solutions.\n\nWork in an Agile development environment with team members, including Product Managers, UX Designers, QA Engineers, and other Software Engineers.\n\nValidate the technical feasibility of UI/UX designs and provide regular technical guidance to support business and technical teams, contractors, and vendors.\n\nDevelop secure, high-quality production code, review and debug code written by others, and drive decisions influencing product design, application functionality, and technical operations.\n\nServe as a subject matter expert in one or more areas of focus and actively contribute to the engineering community as an advocate of firmwide frameworks, tools, and practices of the Software Development Life Cycle.\n\nInfluence peers and project decision-makers to consider the use and application of leading-edge technologies.\n\nDevelop and maintain dashboards, reports, and interactive visualizations using tools such as Tableau, ensuring data accuracy and integrity by implementing best practices in data visualization and management.\n\nStay current with industry trends and emerging technologies in data visualization and analytics, communicate complex data insights clearly to various audiences, including senior management, and manage a team of data visualization associates, providing guidance and mentorship to junior team members.\n\n\n\nRequired qualifications, capabilities, and skills\n\n\n\nFormal training or certification in software engineering concepts and 5+ years of applied experience.\n\n5+ Years of experience as a Web/UI Lead Architect\n\nProficiency in Javascript, Typescript, HTML, CSS\n\nExpert knowledge in ReactJs, Redux, React hooks.\n\nStrong understanding of front-end coding and development technologies\n\nHands-on practical experience delivering system design, application development, testing, and operational stability\n\nAdvanced knowledge of software applications and technical processes with considerable in-depth knowledge in UI and Web Technologies\n\nAbility to tackle design and functionality problems independently with little to no oversight\n\nPractical cloud native experience\n\nExperience in Computer Science, Computer Engineering, Mathematics, or a related technical field\n\n\n\nPreferred qualifications, capabilities, and skills\n\n\n\nFull stack development with Node/. NET/Java\n\nFamiliarity with working in event driven environments\n\nA good understanding of cross-browser compatibility issues and their solutions along with Typescript\n\nExperience working with Databases and ability to write SQL queries along with experience with messaging platforms\n\nBachelor s degree in data science, Computer Science, Information Systems, Statistics, or a related field.\n\nProblem solver and solution oriented. Strong written and verbal communication skills. Jira and Agile practices\n\nExperience with big data technologies and machine learning is a plus.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Front end', 'Coding', 'Javascript', 'Agile', 'System design', 'HTML', 'Application development', 'JIRA', 'Analytics']",2025-06-12 15:02:40
"Software Engineer III - Java, Spring, AWS",JPMorgan Chase Bank,3 - 10 years,Not Disclosed,['Bengaluru'],"As a Software Engineer III at JPMorgan Chase within the Consumer Community Banking Team, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm s business objectives.\nJob responsibilities\nExecutes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems\nCreates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems\nProduces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems\nProactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture\nContributes to software engineering communities of practice and events that explore new and emerging technologies\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 3+ years applied experience in Java, Springboot, Terraforms, AWS\nHands-on practical experience in system design, application development, testing, and operational stability.\nHands-on experience working with AWS stack/services, Java and Spring\nExperience of AWS RDS/Aurora Database/AWS EKS/ECS/Lambda\nKnowledge of AWS SQS/SNS, Terraform for AWS resource/service provisioning\nExperience building and debugging large-scale web services, and microservices based, Kubernetes-orchestrated applications.\nExperience of software applications and technical processes within AWS public cloud\nStrong communication skills and stakeholder management is must.\nExperience in providing technical leadership and guidance to the team.\nSolid understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security.\nDemonstrated knowledge of software applications and technical processes within a technical discipline (e. g. , cloud, artificial intelligence, machine learning, mobile\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies, web services is a plus",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System architecture', 'Front end', 'Coding', 'Debugging', 'Machine learning', 'Technical leadership', 'Agile', 'System design', 'Application development', 'Troubleshooting']",2025-06-12 15:02:42
"Software Engineer III - Java, AWS, Terraforms",JPMorgan Chase Bank,3 - 10 years,Not Disclosed,['Bengaluru'],"As a Software Engineer III at JPMorgan Chase within the Consumer Community Banking Team, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm s business objectives.\nJob responsibilities\nExecutes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems\nCreates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems\nProduces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems\nProactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture\nContributes to software engineering communities of practice and events that explore new and emerging technologies\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 3+ years applied experience in Java, AWS, Terraforms\nHands-on practical experience in system design, application development, testing, and operational stability\nProficient in coding in one or more languages\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nOverall knowledge of the Software Development Life Cycle\nSolid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\nDemonstrated knowledge of software applications and technical processes within a technical discipline (e. g. , cloud, artificial intelligence, machine learning, mobile, etc. )\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System architecture', 'Front end', 'Coding', 'Artificial Intelligence', 'Debugging', 'Machine learning', 'Agile', 'System design', 'Application development', 'Troubleshooting']",2025-06-12 15:02:45
Software Engineer II - Java AWS Terraforms,JPMorgan Chase Bank,3 - 10 years,Not Disclosed,['Bengaluru'],"You re ready to gain the skills and experience needed to grow within your role and advance your career and we have the perfect software engineering opportunity for you.\nAs a Software Engineer II at JPMorgan Chase within the Consumer Community Banking Team, you are part of an agile team that works to enhance, design, and deliver the software components of the firm s state-of-the-art technology products in a secure, stable, and scalable way. As an emerging member of a software engineering team, you execute software solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role.\nJob responsibilities\nExecutes standard software solutions, design, development, and technical troubleshooting\nWrites secure and high-quality code using the syntax of at least one programming language with limited guidance\nDesigns, develops, codes, and troubleshoots with consideration of upstream and downstream systems and technical implications\nApplies knowledge of tools within the Software Development Life Cycle toolchain to improve the value realized by automation\nApplies technical troubleshooting to break down solutions and solve technical problems of basic complexity\nGathers, analyzes, and draws conclusions from large, diverse data sets to identify problems and contribute to decision-making in service of secure, stable application development\nLearns and applies system processes, methodologies, and skills for the development of secure, stable code and systems\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 2+ years applied experience in Java, AWS, Terraforms\nHands-on practical experience in system design, application development, testing, and operational stability, with hands-on experience in developing and deploying applications on AWS\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nDemonstrable ability to code in one or more languages\nExperience across the whole Software Development Life Cycle\nExposure to agile methodologies such as CI/CD, Application Resiliency, and Security\nEmerging knowledge of software applications and technical processes within a technical discipline (e. g. , cloud, artificial intelligence, machine learning, mobile, etc. )\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Artificial Intelligence', 'Debugging', 'Machine learning', 'Agile', 'System design', 'Application development', 'Troubleshooting', 'Downstream']",2025-06-12 15:02:47
Software Engineer II,JPMorgan Chase Bank,3 - 10 years,Not Disclosed,['Bengaluru'],"You re ready to gain the skills and experience needed to grow within your role and advance your career and we have the perfect software engineering opportunity for you.\nAs a Software Engineer II at JPMorgan Chase within the Consumer Community Banking Team, you are part of an agile team that works to enhance, design, and deliver the software components of the firm s state-of-the-art technology products in a secure, stable, and scalable way. As an emerging member of a software engineering team, you execute software solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role.\nJob responsibilities\nExecutes standard software solutions, design, development, and technical troubleshooting\nWrites secure and high-quality code using the syntax of at least one programming language with limited guidance\nDesigns, develops, codes, and troubleshoots with consideration of upstream and downstream systems and technical implications\nApplies knowledge of tools within the Software Development Life Cycle toolchain to improve the value realized by automation\nApplies technical troubleshooting to break down solutions and solve technical problems of basic complexity\nGathers, analyzes, and draws conclusions from large, diverse data sets to identify problems and contribute to decision-making in service of secure, stable application development\nLearns and applies system processes, methodologies, and skills for the development of secure, stable code and systems\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 2+ years applied experience in Java, AWS, Terraforms\nHands-on practical experience in system design, application development, testing, and operational stability\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nDemonstrable ability to code in one or more languages\nExperience across the whole Software Development Life Cycle\nExposure to agile methodologies such as CI/CD, Application Resiliency, and Security\nEmerging knowledge of software applications and technical processes within a technical discipline (e. g. , cloud, artificial intelligence, machine learning, mobile, etc. )\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Artificial Intelligence', 'Debugging', 'Machine learning', 'Agile', 'System design', 'Application development', 'Troubleshooting', 'Downstream']",2025-06-12 15:02:50
SOC Thermal engineer,Qualcomm,1 - 3 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Hardware Engineer, you will plan, design, optimize, verify, and test electronic systems, bring-up yield, circuits, mechanical systems, Digital/Analog/RF/optical systems, equipment and packaging, test systems, FPGA, and/or DSP systems that launch cutting-edge, world class products. Qualcomm Hardware Engineers collaborate with cross-functional teams to develop solutions and meet performance requirements.\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field.\n\nJob Function:\n\nSOC Thermal analysis methodology developmentencompassing SoC, package and systems, for various Qualcomm products going into smartphones, laptops, automotives and virtual reality headsets.\n\nDeveloping strategies to predict and improve thermally constrained scores (AnTuTu/Geekbench) and also measure/tune it on post-Si platforms, including infrared imaging.\n\nThermal measurement The candidate will be responsible for improving thermal mitigation strategies through power and thermal management to achieve optimal system thermal performance.\n\n\n\n\nSkills:\n\n\nShould possess strong analytical skills and mathematical modeling abilities.\n\nKnowledge of SOC stack up is must\n\nGood knowledge of scripting in PERL/Python required; some exposure to Machine Learning algorithms/frameworks will be a plus.\n\nNeed to have thorough knowledge of heat transfer mechanisms and CFD.\n\nProficiency with state-of-the-art thermal analysis tools- Ansys Icepak/Flotherm\n\nThermal Measurement will be preferred skill\n\n\nExperience: BE/ME graduates with 1 to 3 years of experience.",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['python', 'soc', 'machine learning', 'mathematical modeling', 'perl', 'dsp', 'data analytics', 'natural language processing', 'hardware engineering', 'sql', 'deep learning', 'r', 'fpga', 'data science', 'predictive modeling', 'machine learning algorithms', 'logistic regression', 'statistics']",2025-06-12 15:02:52
Auto AI Systems Engineer,Qualcomm,1 - 6 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nSummary - We are seeking experts with a robust background in the field of deep learning (DL) to design state-of-the-art low-level perception (LLP) as well as end-to-end AD models, with a focus on achieving accuracy-latency Pareto optimality. This role involves comprehending state-of-the-art research in this field and deploying networks on the Qualcomm Ride platform for L2/L3 Advanced Driver Assistance Systems (ADAS) and autonomous driving.\n\nThe ideal candidate must be well-versed in recent advancements in Vision Transformers (Cross-attention, Self-attention), lifting 2D features to Bird's Eye View (BEV) space, and their applications to multi-modal fusion. This position offers extensive opportunities to collaborate with advanced R&D teams of leading automotive Original Equipment Manufacturers (OEMs) as well as Qualcomm's internal stack teams. The team is responsible for enhancing the speed, accuracy, power consumption, and latency of deep networks running on Snapdragon Ride AI accelerators.\n\nA thorough understanding of machine learning algorithms, particularly those related to automotive use cases (autonomous driving, vision, and LiDAR processing ML algorithms), is essential. Research experience in the development of efficient networks, various Neural Architecture Search (NAS) techniques, network quantization, and pruning is highly desirable.\n\nStrong communication and interpersonal skills are required, and the candidate must be able to work effectively with various horizontal AI teams.\n\nMinimum Qualifications:\n\nBachelor's degree in Computer Science, Engineering, Information Systems, or related field and 1+ years of Hardware Engineering, Software Engineering, Systems Engineering, or related work experience.ORMaster's degree in Computer Science, Engineering, Information Systems, or related field and 1+ year of Hardware Engineering, Software Engineering, Systems Engineering, or related work experience.ORPhD in Computer Science, Engineering, Information Systems, or related field.\n\nPreferred Qualifications:\nGood at software development with excellent analytical, development, and problem-solving skills.\nStrong understanding of Machine Learning fundamentals\nHands-on experience with deep learning network design and implementation. Ability to define network from scratch in PyTorch, ability to add new loss function, modify network with torch.fx. Adept at version control system like GIT.\nExperience in neural network quantization, compression, pruning algorithms.\nExperience in deep learning kernel/compiler optimization\nStrong communication skills\n\n\nPrincipal Duties and Responsibilities:\n\nApplies Machine Learning knowledge to extend training or runtime frameworks or model efficiency software tools with new features and optimizations.\n\nModels, architects, and develops machine learning hardware (co-designed with machine learning software) for inference or training solutions.\n\nDevelops optimized software to enable AI models deployed on hardware (e.g., machine learning kernels, compiler tools, or model efficiency tools, etc.) to allow specific hardware features; collaborates with team members for joint design and development.\n\nAssists with the development and application of machine learning techniques into products and/or AI solutions to enable customers to do the same.\n\nDevelops, adapts, or prototypes complex machine learning algorithms, models, or frameworks aligned with and motivated by product proposals or roadmaps with minimal guidance from more experienced engineers.\n\nConducts complex experiments to train and evaluate machine learning models and/or software independently.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hardware engineering', 'machine learning', 'software engineering', 'machine learning algorithms', 'system engineering', 'switching', 'pruning', 'algorithms', 'eigrp', 'kernel', 'rstp', 'networking', 'vrrp', 'ospf', 'deep learning', 'routing', 'git', 'computer science', 'vlan', 'adas', 'hsrp', 'control system']",2025-06-12 15:02:54
XR Perception Systems Engineer,Qualcomm,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nAt Qualcomm XR Perception, were a passionate team of engineers who want to change the world through virtual and augmented reality products and technologies. We develop state-of-the-art, power efficient XR perception algorithms optimized for Qualcomm hardware. The perception algorithms enable XR systems to map the users environment and track the user within the environment.\n\nTo scale and strengthen our offering, Qualcomm XR Perception team in Bengaluru is rapidly expanding and seeking candidates to investigate and develop the fundamental perception algorithm that enables self-contained XR platforms. We are looking for innovators who will push the boundaries of mobile perception technology to offer a robust platform to our customers.\n\nJob Function/General Responsibilities\n\nIn this job the candidate would\n\nBe part of a world-class XR team researching and developing XR perception\n\nWork with building blocks of Simultaneous Localization and Mapping (SLAM)\n\no Multi-view geometry\n\no Sensor Fusion\n\no Pose estimation\n\no Non-linear optimization\n\nApply ML/ DL techniques to enhance accuracy and robustness of traditional SLAM systems\n\nImplement efficient and optimized algorithms in C++\n\nUnderstand and analyze requirements for perception systems for XR platforms\n\nHave excellent verbal and written communication and presentation skills\n\n\n\nCandidate that fits this role should be well versed in the basics of\n\nLinear algebra\n\nProbability\n\nMulti-view geometry\n\nSensor fusion\n\nEstimation techniques\n\nNon-linear optimization\n\nMachine learning techniques\n\nObject oriented programming in C++\n\n\n\nKeywords\n\nVirtual reality, Augmented reality, Computer vision, Machine learning, Perception, VIO, SLAM\n\n\n\nEducational requirements:\n\nBachelor's degree in Engineering, Applied Mathematics, Computer Science, or related field\n\n+3 years of work experience OR Master's degree in Engineering, Applied Mathematics, Computer Science, or related field\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['presentation skills', 'fusion', 'geometry', 'sensor', 'linear algebra', 'algorithms', 'c++', 'python', 'machine learning', 'estimation', 'slam', 'deep learning', 'tensorflow', 'computer science', 'computer vision', 'keras', 'object oriented programming', 'vio', 'system engineering']",2025-06-12 15:02:57
Software Development Engineer,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Own feature enhancements end to end, collaborate with product and science , design, develop and launch in production.\nParticipate in design reviews/code reviews and contribute to the teams deliverables\nA Software Development Engineer in Alexa Sensitive Content Intelligence will develop the design for complex, cross functional, multimodal and multi locale applications. The engineer in this role will be the owner of the technical areas supporting Generative AI Large Language Model (LLM) based services and customer facing applications. An Software Development Engineer will be forging alliance with upstream and downstream engineering teams to deliver the services and create mechanisms for end-to-end integration. You will build low-latency services that are optimized for performance to meet the classic Alexa and LLM based customer interaction expectations.\nAn Software Development Engineer in Alexa Sensitive Content Intelligence will own full stack software development and contribute to all aspects of an agile software development lifecycle, including design, architecture, development, documentation, testing and operations. You will work with Scientists, Engineers and Product managers and influence the design of the customer facing features and products.",,,,"['Coding', 'Machine learning', 'Architectural design', 'Agile', 'Software development life cycle', 'Customer experience', 'Internship', 'Downstream', 'Customer interaction']",2025-06-12 15:02:59
Engineering - L2 - Associate-Software Engineering,Goldman Sachs,2 - 5 years,Not Disclosed,['Bengaluru'],"The Entitlements Platform team sits at the very heart of Goldman Sachs, responsible for protecting the firm against the risks associated with both over-privilege and under-privilege. We are the guardians of access, ensuring that only the right people have access to the right data at the right time. Our team builds the critical security controls that facilitate the firms mandatory adherence to regulatory and compliance policies. We are an elite team of full-stack developers that provide hosted applications, standalone libraries, and customizable workflow tooling to empower the firms authorization needs for applications and their associated data.\nWe are seeking passionate software engineers who crave more than just coding. We need individuals who are driven to build robust control solutions and understand the power of platforms to drive enterprise-wide leverage. We are looking for individuals who are not only technically skilled, but also possess the leadership qualities, strategic thinking, and communication skills to drive change and make a real impact on the firm.\n  As a Software Engineer on the team, you will:\nDesign, develop, and test cross-platform software solutions that are secure, scalable, and maintainable\nBuild applications, libraries, and services to manage the lifecycle of application privileges\nProvide externalized authorization capabilities and allow secure, auditable access to the production environment\nCollaborate with colleagues across diverse technology teams to understand their needs and build solutions that meet their requirements\nBe a proactive problem solver, identifying potential issues and developing mitigation plans\nContinuously learn and stay current with the latest technologies and industry best practices\nTake ownership of your work and be accountable for its success\nRequired Qualifications:\nStrong core and server-side Java software development experience\nExperience implementing RESTful APIs and microservice architecture\nFamiliarity with SQL and/or NoSQL databases, database design principles and object-relational mapping frameworks\nThe ability to communicate technical concepts effectively, both in writing and orally, as we'll as the interpersonal skills required to collaborate effectively with colleagues across diverse technology teams\nProficiency in designing, developing, and testing cross-platform software\nSolid experience of version control, continuous integration, deployment, and configuration management tools\nThe ability to understand and effectively debug both new and existing software\nSelf-motivated and ability to work in a high-paced environment with tight deadlines\nAn understanding of customer-first and data driven design\nPreferred Qualifications:\nA grounding in identity and privilege management concepts, including relationships between employee and privilege lifecycle, authorization vs. authentication, and segregation of duties\nExpertise in Web display technologies, particularly modern JavaScript libraries and frameworks (eg, React/Redux), CSS (including experience with SASS, LESS, and/or PostCSS), and HTML\nExperience with Kubernetes and container management\nProficiency in Apache Kafka and its core principles",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Coding', 'Database design', 'Configuration management', 'Machine learning', 'Javascript', 'HTML', 'Investment banking', 'Apache', 'SQL']",2025-06-12 15:03:02
Sr Engineer,Qualcomm,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nCPU architecture team is driving the core math libraries needed for ML/AI acceleration. This position/s will expose you to Qualcomms cutting edge SoC and ML/AI platforms in the industry. Participate in Optimizing the core ML kernels using the latest advancements like SME, SVE of the ARM CPU architecture and enhance the performance of the ML models on the CPU of the QCOM Soc\n\nRequired Skills==\n3+ experince with Understanding of ARM CPU architecture fundamentals and ARM Arch64 ISA\nOptimizing kernels for vector Processors\nUnderstanding of the basic linear algebra functions used in AI/ML\nAlgorithm design (logic, critical thinking)\nPerformance Evaluation and Optimization of the applications for ARM architecture\nInferencing of the ML models written in Pytorch/TensorFlow/Keras\nUnderstanding of the typical Open-Source Library framework design\n\n\n\nPreferred Skills===\nStrong Programming skills and deep understanding of the ARM ISA\nunderstanding of the algorithms suitable for Vector and matrix accelerators\nStrong Analytical and debugging skills\nGood understanding of Optimizing the Linear Algebra algorithms\nPerformance evaluation using QEMU, Simulators, Emulators and on Real Hardware\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cpu', 'isa', 'debugging', 'software engineering', 'arm', 'algorithms', 'python', 'c++', 'c', 'natural language processing', 'soc', 'neural networks', 'machine learning', 'artificial intelligence', 'deep learning', 'tensorflow', 'java', 'computer science', 'computer vision', 'pytorch', 'keras', 'dhcp']",2025-06-12 15:03:05
"Software Development Engineer II, International Emerging Stores",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Shaping the Future of Global E-commerce!!\n\nAt Amazons International Emerging Stores (IES), were reinventing how millions of customers discover and shop online. Our team is architecting foundational platforms and mechanisms that will power Amazons next generation of shopping experiences globally. Were tackling intrinsically hard problems at the intersection of AI, personalization, and scalable systems challenges that require innovative solutions while maintaining our commitment to operational excellence. Our charter extends beyond traditional e-commerce boundaries, focusing on creating competitive advantages through technical innovation. Were building solutions that not only serve immediate business needs but establish new patterns and practices that can be adopted across Amazon. Our work demands deep technical judgment, cross-organizational collaboration, and the ability to influence at the highest levels of the organization.\n\nThe Opportunity: Software Development Engineer\n\nAt IES, were building the future of retail, and were looking for a talented Software Development Engineer to join our innovative team. As an SDE, youll be instrumental in revolutionizing how customers make purchase decisions across our retail platform by developing next-generation shopping experiences powered by artificial intelligence and adaptive technologies.\n\nIn this role, youll design and implement intelligent systems that deliver personalized shopping experiences, working with cutting-edge generative AI and ML models to create innovative visual experiences. Youll develop sophisticated algorithms for adaptive layout optimization, product visualization features, theme-based recommendation systems, and customer behavior analysis. Your work will span multiple customer touchpoints, requiring you to write high-quality, scalable code while collaborating with product managers, designers, and data scientists to drive technical solutions.\n\nWere seeking someone with strong programming skills and software design expertise, particularly in distributed systems and scalable architectures. Knowledge of AI/ML technologies and their practical applications is essential, as is the ability to translate complex business requirements into technical solutions. Youll participate in architecture discussions, technical design reviews, and contribute to the continuous improvement of customer experience metrics while debugging complex production issues and optimizing system performance.\n\nThis position offers an exciting opportunity to work on cutting-edge technologies while solving complex engineering challenges that impact millions of customers globally. Youll be part of a team that values technical excellence and innovation, with the chance to shape the future of e-commerce through technological advancement.\n\n3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'Software design', 'Operational excellence', 'Software Development Engineer II', 'Coding', 'Artificial Intelligence', 'Debugging', 'Continuous improvement', 'Internship', 'Distribution system']",2025-06-12 15:03:07
AutoIT Solutioning Engineer-Staff,Qualcomm,3 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nAs a Site Reliability Engineer (SRE), youll be part of a highly collaborative team focused on provisioning and maintaining infrastructure and services with stability, sustainability, and security always on your mind. You will work in a self-guided, cross-functional team responsible for everything from modernizing traditional services and applications to deploying new technology. You'll collaborate closely with software engineers, data scientists, and product managers to maintain and optimize our systems. If you're passionate about automotive technology, software reliability, and continuous improvement, this role is perfect for you.\n\nYour Guiding Principles:\n\n\nAutomationYou understand the power of automation and ""infrastructure as code"" concepts. Automation is your primary consideration in problem-solving.\n\n\nCollaboration: You share a common language with fellow engineers, understand their needs, and thrive working in a high trust collaborate culture in which people are rewarded for taking risks.\n\n\nData-drivenYou understand why decisions are supported by facts and not opinions. You have experience applying logical approach to decision making. Skilled at metric collection and using that data to drive change.\n\n\nDebuggingYou understand debugging principles and are adept at applying them routinely and successfully.\n\n\nDevSecOps: You understand that DevSecOps is a culture which needs to be cultivated and you can help nurture those philosophies.\n\n\nSecurityYou know how to layer appropriate security within solutions across the lifecycle. You understand the security implications and consequences of any deployment.\n\n\nSelf-Driven: You understand how to prioritize work and time allocation at a personal and team level.\n\n\nStability: You know what it means to deliver a service with a high degree of reliability and are intimately familiar with how disruptions impact consumers.\n\n\nSustainability: You avoid one off solutions which are challenging to support. Instead, your solutions are aligned with team goals and strategic vision. You routinely dedicate cycles to reducing technical debt.\n\n\nWhat you have:\nExtensive Linux experience with servers and workstations. You can easily navigate the CLI, knowledgeable with typical Linux troubleshooting tools, and have a broad understanding of Ubuntu and RedHat.\nThe ability to automate through scripting languages such as Python, Bash, Go, etc.\nThe skill to provide sufficient automated test coverage of various implementations.\nYou have familiarity with Jenkins, Puppet, Splunk, JIRA, Vault, Docker, AWS, Cloud services, etc.\nAbility to respond rapidly to changing landscapes while providing stable, reliable, and secure services to customers.\nYou have a passion for continuous learning and leverage the scientific method to ensure nothing is taken for granted.\n\n\nResponsibilities:\nSystem Monitoring and Incident Response:\nMonitor system health, detect anomalies, and respond promptly to incidents.\nInvestigate and troubleshoot issues related to services.\nImplement proactive measures to prevent service disruptions.\nInfrastructure Automation:\nDevelop and maintain infrastructure-as-code (IaC) scripts for deployment and scaling.\nAutomate routine tasks to improve efficiency and reduce manual intervention.\nPerformance Optimization:\nCollaborate with development teams to optimize software performance.\nIdentify bottlenecks and implement solutions to enhance system speed and reliability.\nCapacity Planning:\nForecast resource requirements based on traffic patterns and business growth.\nScale infrastructure to accommodate increasing demand.\nSecurity and Compliance:\nEnsure compliance with industry standards and best practices.\nImplement security controls and participate in security audits.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['docker', 'linux', 'python', 'puppet', 'aws', 'kubernetes', 'owasp', 'golang', 'redhat linux', 'vulnerability assessment', 'ansible', 'microservices', 'java', 'devops', 'jenkins', 'debugging', 'penetration testing', 'vault', 'jira', 'cloud services', 'ubuntu', 'microsoft azure', 'splunk', 'bash', 'devsecops', 'terraform']",2025-06-12 15:03:10
Senior Software Engineer - AI/ML,Qualcomm,5 - 10 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nThe AI SW team at Qualcomm is focused on advancing state-of-the-art in Artificial Intelligence across various business segments, including Mobile, AR & VR Technology, IoT, and Auto ADAS. The AISW stack leverages Qualcomm chips' extensive heterogeneous computing capabilities, enabling the running of trained neural networks on devices without needing a cloud connection. This allows neural network models trained in various frameworks on Snapdragon platforms to run at blazing speeds while consuming minimal power. As a Senior Lead Engineer, you will see your work directly impact billions of devices worldwide.\n\nKey Responsibilities:\n\nDesign, develop, and maintain high-quality software solutions using Python for running machine learning models on Qualcomm devices.\n\nContribute to the development and optimization of AI models, using popular frameworks like Pytorch.\n\nBuild tools and infrastructure for onboarding, debugging and analysis of AI models.\n\nParticipate in code reviews and ensure adherence to best practices and coding standards.\n\nDebug accuracy and performance on devices, addressing any challenges that arise.\n\nCollaborate with cross-functional teams to define, design, and ship new features.\n\nOwn end-to-end development and release of features and lead and mentor a sub-team of engineers.\n\n\nMinimum Qualifications:\n\nBachelors degree in engineering, Computer science or a related field and 5+ years of professional experience in software engineering or related work experience\n\nOR\n\nMasters degree in engineering, Computer science or a related field and 4+ years of experience of Software engineering or related work experience\n\nSolid understanding of fundamental computer science concepts, general programming principles and practices.\n\n4+ years of hands-on professional experience in programming with Python (preferred) / Java / C++.\n\nStrong problem-solving skills and the ability to work independently and as part of a team.\n\nBasic knowledge of AI concepts and techniques\n\nExcellent communication skills and the ability to articulate complex technical concepts.\n\nWillingness to learn advanced concepts in AI and machine learning and keep updated with latest industry trends\n\n\nPreferred Qualifications:\n\nExperience with machine learning frameworks and tools such as PyTorch, ONNX.\n\nFamiliarity with Large Language Models (LLMs) and Transformers\n\nFamiliarity working with Linux systems and hardware devices\n\nExperience with mobile development frameworks and tools (e.g., Android SDK, Kotlin).\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'computer science', 'software engineering', 'system engineering', 'algorithms', 'c++', 'natural language processing', 'android', 'kotlin', 'c+', 'aiml', 'artificial intelligence', 'iot', 'deep learning', 'tensorflow', 'java', 'linux', 'computer vision', 'pytorch', 'onnx', 'ml']",2025-06-12 15:03:12
Sr Staff Video Codec Systems Engineer,Qualcomm,15 - 20 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nResponsibilities:\n\nPart of video IP systems team and will be responsible for video technology roadmap ; interaction with different teams including design, verification, system, firmware, software, SOC and power teams; video algorithms, image quality analysis; video processing and compression trends with standardization committees.\n\nQualcomm is the innovation leader in the area of integrated chipsets that power advanced mobile devices, XR/IoT/Automotive & compute platforms. We are building on and expanding our reputation as the industry powerhouse for innovation in both wireless technologies and enabling advanced multimedia capabilities. We are seeking experienced system engineers for our cutting-edge efforts in the architecture and design of our video codec hardware. The video Systems group provides video solutions on all of Qualcomms Snapdragon mobile processors. The teams scope includes video processing algorithms and IP architecture design for video compression, visual signal processing and analytics, with power and performance optimization.\n\nThe selected candidate, along with his/her colleagues and other team members, will have responsibilities in one or more of the following areas:\n\nDesigning and evaluating video algorithms to be implemented in hardware video encoders and decoders .\n\nDefine systems architecture for video solutions including data flow, task partition, interface and systems interoperation.\n\nImplement models to accurately model the HW (functional, performance), and supporting HW verification & SW development via behavioral model vectors .\n\nCollaborate with systems, software, hardware teams at various stages of chipset life in design/validation/commercialization.\n\nResearch and develop video algorithms for mobile, automotive, compute and VR/AR applications with performance and power efficiency.\n\n\nMinimum Qualifications:\n\nMasters degree in Electrical/Electronics Engineering, Computer Science, or related field and 15+ years of systems engineering experience\n\nPhD in Electrical/Electronics Engineering, Communications - Signal Processing, Computer Science, or related field and 12+ years of systems engineering experience\n\nKnowledge & Experience in video coding standards such as VVC, AV1, HEVC, H.264/AVC, VP9.\n\nHands on Knowledge & Experience in Video Codec Design and implementation with in-depth understanding of codec algorithms and flow\n\nSolid C/C++ programming, Python scripting skills.\n\nStrong communication skills\n\nGood analytical and problem solving skills.\n\n\nPreferred Qualifications:\n\nHW C modeling experience\n\nImage quality evaluation and metric comparisons\n\nSignal / Image processing basicsComputer Vision and Machine Learning algorithms for Video Compression and Video/Image processing.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 8+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 7+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 6+ years of Systems Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'c++', 'system engineering', 'python', 'c', 'dsp', 'cuda', 'vod', 'dvb', 'embedded systems', 'linux', 'hevc', 'data structures', 'multithreading', 'ffmpeg', 'stb', 'image processing', 'matlab', 'video codecs', 'machine learning', 'ott', 'av', 'mpeg', 'computer vision', 'embedded c', 'h264', 'vp']",2025-06-12 15:03:15
Wealth Management-Bengaluru-Associate-Software Engineering,Goldman Sachs,3 - 5 years,Not Disclosed,['Bengaluru'],"Associate GenAI Developer\nWe are seeking a highly skilled GenAI Developer to join our dynamic, global team. The ideal candidate will have a strong background in applied generative AI. This role will involve developing and implementing AI solutions, working with various technologies, and collaborating with cross-functional teams to drive innovation. The GenAI Developer will play a crucial role in advancing our GenAI capabilities and contributing to the success of our Wealth Management division.\nKey Responsibilities:\nWork with stakeholders to understand requirements and deliver AI solutions across several domains in Wealth Management.\nStay updated with the latest advancements in AI and machine learning technologies.\nConduct research and experiments to improve AI capabilities within the division.\nRequired Competencies:\nRetrieval-Augmented Generation (RAG): Experience in developing and implementing RAG models to enhance information retrieval and generation tasks.\nVector Stores: Knowledge of Vector Stores for efficient data storage and retrieval.\nPrompt Engineering: Skills in designing and optimizing prompts for AI models to improve accuracy and relevance.\nLarge Language Model APIs (LLM APIs): Understanding of different LLMs, both commercial and open source, and their capabilities (e.g., OpenAI, Gemini, Llama, Claude).\nProgramming Languages: Proficiency in Python, Java, or other relevant programming languages.\nData Analysis: Strong analytical skills and experience with data analysis tools.\nProblem-Solving: Excellent problem-solving abilities and attention to detail.\nCommunication: Strong verbal and written communication skills.\nPreferred Competencies:\nGraph RAG: Proficiency in using Graph RAG for complex data relationships and insights.\nKnowledge Graphs: Expertise in building and managing Knowledge Graphs to represent and query complex data structures.\nMachine Learning Frameworks: Experience with TensorFlow, PyTorch, or similar frameworks.\nExperience with cloud platforms such as AWS, Google Cloud, or Azure.\nFamiliarity with natural language processing (NLP) and computer vision technologies.\nPrevious experience in a similar role or industry.\nMaster s or Ph.D. in Computer Science, Data Science, or a related field.\nYears of experience: 3-5",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Computer vision', 'Data analysis', 'Wealth management', 'Machine learning', 'Data structures', 'HTML', 'Investment banking', 'Open source', 'Python']",2025-06-12 15:03:18
Wealth Management - Vice President-Software Engineering,Goldman Sachs,6 - 10 years,Not Disclosed,['Bengaluru'],"We are seeking a highly skilled GenAI Developer to join our dynamic, global team. The ideal candidate will have a strong background in applied generative AI. This role will involve developing and implementing AI solutions, working with various technologies, and collaborating with cross-functional teams to drive innovation. The GenAI Developer will play a crucial role in advancing our GenAI capabilities and contributing to the success of our Wealth Management division.\n  Key Responsibilities:\nWork with stakeholders to understand requirements and deliver AI solutions across several domains in Wealth Management.\nStay updated with the latest advancements in AI and machine learning technologies.\nConduct research and experiments to improve AI capabilities within the division.\nRequired Competencies:\nRetrieval-Augmented Generation (RAG): Experience in developing and implementing RAG models to enhance information retrieval and generation tasks.\nVector Stores: Knowledge of Vector Stores for efficient data storage and retrieval.\nPrompt Engineering: Skills in designing and optimizing prompts for AI models to improve accuracy and relevance.\nLarge Language Model APIs (LLM APIs): Understanding of different LLMs, both commercial and open source, and their capabilities (eg, OpenAI, Gemini, Llama, Claude).\nProgramming Languages: Proficiency in Python, Java, or other relevant programming languages.\nData Analysis: Strong analytical skills and experience with data analysis tools.\nProblem-Solving: Excellent problem-solving abilities and attention to detail.\nCommunication: Strong verbal and written communication skills.\nPreferred Competencies:\nGraph RAG: Proficiency in using Graph RAG for complex data relationships and insights.\nKnowledge Graphs: Expertise in building and managing Knowledge Graphs to represent and query complex data structures.\nMachine Learning Frameworks: Experience with TensorFlow, PyTorch, or similar frameworks.\nExperience with cloud platforms such as AWS, Google Cloud, or Azure.\nFamiliarity with natural language processing (NLP) and computer vision technologies.\nPrevious experience in a similar role or industry.\nmasters or Ph.D. in Computer Science, Data Science, or a related field.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer vision', 'Data analysis', 'Wealth management', 'Machine learning', 'Data structures', 'HTML', 'Investment banking', 'Open source', 'Python']",2025-06-12 15:03:20
Automotive cybersecurity engineer Sr/Staff,Qualcomm,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Hardware Engineer, you will plan, design, optimize, verify, and test electronic systems, bring-up yield, circuits, mechanical systems, Digital/Analog/RF/optical systems, equipment and packaging, test systems, FPGA, and/or DSP systems that launch cutting-edge, world class products. Qualcomm Hardware Engineers collaborate with cross-functional teams to develop solutions and meet performance requirements.\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 4+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 3+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 2+ years of Hardware Engineering or related work experience.\n\n\nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in. systems architecture.ADAS/Autonomy/Infotainment Qualcomm is utilizing its traditional strengths in digital wireless technologies to play a central role in the evolution of automotive infotainment and autonomous driving. We are investing in several supporting technologies including 4G, 5G, ADAS, and Deep Learning. The Qualcomm Automotive Security team is engaged in offering optimized solutions built on Safety CPU cores, DSP, computer vision and machine learning algorithms for the Qualcomm\n\nWe are seeking engineers with experience in system and SoC level automotive cybersecurity concepts and implementations and knowledge of the ISO21434 cyber security standards and process. This position will be a hands-on role in in analyzing and improving the current cybersecurity hardware development process to meet ISO 21434. Candidate must have thorough knowledge on ISO 21434 and ISO 26262 standards to independently perform functional safety and cybersecurity audits and assessments.\n\nResponsibilities shall include the following\n\nDevelopment/ enhancement of hardware development process and related work products meeting cyber security standards.\n\nProvide training on ISO 21434/ ISO 26262 with respect to application of Qualcomms best practices.\n\nFacilitate TARA and VARA and review them\n\nIndependently carry out cybersecurity audits and assessments, guiding the hardware design and development teams in the process. Provide feedback and identify process improvements and follow up with internal teams.\n\nWork closely with SoC Design and IP teams, 3rd party IP vendors, Software team, the functional safety and cybersecurity manager(s), Quality team as well as customers to ensure the defined process is deployed.\n\nParticipate in external and customer cybersecurity and functional safety audits and assessments and assist in corrective actions.\n\nMaintain a strong knowledge of Automotive Industry cybersecurity best practices. Influence internal stakeholders with actionable data to ensure gaps to expectations are closed in a timely fashion.\n\nEstablish and maintain healthy relationships with influential/decision making on Cybersecurity, Functional Safety, and ongoing Quality assessments throughout the organization.\n\nFacilitate in qualitative and quantitative safety analyses and review of them.\n\nMinimum Qualifications\n\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field.\n\n8+ years Systems Engineering / VLSI design or related work experience.\n\nHas knowledge of product development flow and validation.\n\nHas led audits of different development processes and is comfortable with assessments and reporting Quality metrics.\n\nHands-on experience with automotive quality processesISO 21434, ISO 26262, TARA, VARA, DFMEA, FTA, FMEDA, 8D, quality agreements, PCNs / EOLs, customer audits, AEC- Q100 requirements, ISO9001, IATF 16949, etc.\n\nStructured problem-solving capability and ability to work with teams on root cause analyses.\n\nExcellent verbal/written communication, interpersonal skills, and presentation skills",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['vlsi design', 'presentation skills', 'hardware engineering', 'dfmea', 'system engineering', 'matlab', 'c++', 'python', 'c', 'hiring', 'networking', 'vhdl', 'verilog', 'staffing', 'talent acquisition', 'technical support', 'recruitment', 'desktop engineering', 'embedded systems', 'desktop support']",2025-06-12 15:03:23
Automotive Platform - Engineer Sr.,Qualcomm,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nQualcomm ADAS/Autonomy team is engaged in offering optimized solutions built on DSP, computer vision and machine learning algorithms for the Qualcomm ADAS/Autonomy SoCs. We are seeking engineers with experience in system and SoC SW level functional safety concepts. The job requires understanding and defining of the Safety Concept and Architecture, Software Safety requirements, defining and deploying safety processes and development of Safety software by following the ISO26262 software processes. Interaction with customers, architects and test/integration teams are required as part of the job. The job also involves working with the Software quality team for adherence of ISO26262 and ASPICE processes.\n\nIn this role, the candidate will work with local and global teams to understand, define and implement and productize Automotive specific features including software enablement (drivers/BSP/RTOS/AUTOSAR MCAL), security, functional safety, and power applied to Automotive products on our current and next generation SoCs. The candidate will also have the responsibility to coordinate and execute plans which will encompass validation of all the feature requirements. The Candidate will have the responsibility to identify and address any abnormal discoveries by root-causing and providing detailed corrective actions in the form of optimizations and/or fixes. When possible, the candidate is expected to prototype and pre-validate recommended fixes. Additionally, the candidate will be responsible for any automation of design under test along with validation efforts and working closely with design/production/bench IP teams.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\n 3-6 years of Embedded Software Development experience, including low level drivers, and RTOS \n\n The candidate should possess 3 to 6 years of industry experience in embedded software driver development and having expertise in one or more below areas would be preferred: \n\n Should be able to ramp up fast and must have the attitude to work with the team. \n\n Strong C and Assembly Programming with OS & Multi-Processor concepts \n\n Embedded software development in C and C++ on ARM or similar cores. \n\n Hands on experience of driver development on any RTOS, \n\n Experience in SafeRTOS/FreeRTOS based development is nice to have \n\n Experience in Autosar MCAL development is nice to have \n\n Experience in Autosar BSW integration and validation is nice to have \n\n ARM Trust-Zone & ARMv7/v8 architecture. \n\n Good debugging skills with experience on debugging with Lauterbach JTAG debuggers. \n\n Work on challenging customer requirements and issues. \n\n Basic understanding one or more of hardware blocks - Clocks, PLLs, GPIO, Interrupt Controllers (GIC), Peripherals (SPI/I2C/UART/CAN/Ethernet/Clock/etc)  \n\n Automotive SW development experience is must have \n\n Experience in ISO26262/functional safety and ASPICE is highly desirable \n\n Basic knowledge on Power Mgmt. IC is desirable \n\n Knowledge of Software/Hardware Security concepts is desirable \nClosely work with the hardware team to contribute/suggest modifications to the hardware design.\nAny past working experience on Qualcomm chips nice to have",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['assembly programming', 'c', 'sw', 'embedded software development', 'embedded software', 'aspice', 'python', 'c++', 'canoe', 'spi', 'freertos', 'autosar', 'mcal', 'rtos', 'java', 'embedded systems', 'embedded c', 'uart', 'debugging', 'software engineering', 'ic', 'i2c', 'can bus', 'functional safety']",2025-06-12 15:03:25
Senior Staff Engineer - AI-IOT Product,Infineon,6 - 8 years,Not Disclosed,['Bengaluru'],"We are seeking an experienced AI-IOT Product Engineer to design and develop AI and IoT products, with a focus on technical product management and product design. This role requires a strong technical background, excellent problem-solving skills, and the ability to collaborate with cross-functional teams to deliver innovative solutions.\n\nJob Description\nProduct Design: Design and develop AI and IoT product concepts, including hardware, firmware, and software components. This includes:\nDeveloping product requirements and technical specifications.\nCollaborating with engineering teams to design and develop product architectures.\nEnsuring product designs meet technical and business requirements.\nTechnical Product Management: Manage the technical aspects of AI and IoT products, including:\nDefining and prioritizing product features and requirements.\nCollaborating with engineering teams to develop and deploy products.\nEnsuring products meet quality and reliability standards.\nSensor Algorithm Development: Collaborate with engineering teams to develop and implement sensor algorithms that enable accurate and efficient data collection and processing. This includes:\nProviding technical guidance on sensor fusion algorithms and machine learning models.\nCollaborating with engineering teams to integrate sensor algorithms with IoT devices and platforms.\nLLM RAG Systems: Collaborate with engineering teams to design and develop Large Language Model (LLM) Retrieval-Augmented Generation (RAG) systems that enable efficient and effective natural language processing. This includes:\nProviding technical guidance on LLM models and RAG systems.\nCollaborating with engineering teams to integrate LLM RAG systems with IoT devices and platforms.\nApplied Deep Learning: Collaborate with engineering teams to develop and implement deep learning models that enable accurate and efficient data analysis and processing. This includes:\nProviding technical guidance on deep learning models and architectures.\nCollaborating with engineering teams to integrate deep learning models with IoT devices and platforms.\nTechnical Road mapping: Develop and maintain technical roadmaps for AI and IoT products, including:\nIdentifying and prioritizing technical requirements and features.\nCollaborating with engineering teams to develop and deploy products.\nYou are best equipped for this task if you have:\n6-8 years of experience in AI, IoT, or related fields.\nBachelors or Masters degree in Computer Science, Electrical/Electronics Engineering, or a related field.\nExcellent problem-solving skills and attention to detail.\nExperience with AI and IoT technologies, such as AWS or Azure.\nStrong technical knowledge of sensor algorithms, LLM RAG systems, and applied deep learning.\nExperience with technical product management and product design.\nExperience with agile development methodologies.\nKnowledge of IoT protocols and technologies (eg, MQTT, CoAP, BLE, Zigbee, LoRa, NB-IoT).\nExperience with data analytics and machine learning.\nCertifications in AI or IoT technologies (eg, AWS Certified IoT Architect, Microsoft Certified Azure AI Engineer).\nExperience with DevOps tools and practices.",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Semiconductor', 'Data analysis', 'Machine learning', 'Data collection', 'Product design', 'Natural language processing', 'Firmware', 'microsoft']",2025-06-12 15:03:28
Staff System Test Engineer,Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Test Engineering\n\nGeneral Summary:\n\n\n\nWe are seeking a Senior Staff AI System-Level Test Engineer to lead end-to-end testing of Retrieval-Augmented Generation (RAG) AI systems for Hybrid, Edge-AI Inference solutions. This role will focus on designing, developing, and executing comprehensive test strategies for evaluating the reliability, accuracy, usability and scalability of large-scale AI models integrated with external knowledge retrieval systems.\n\nThe ideal candidate needs to have deep expertise in AI testing methodologies, experience with large language models (LLMs), expertise in building test solutions for AI Inference stacks, RAG, search/retrieval architecture, and a strong background in automation frameworks, performance validation, and building E2E automation architecture.\n\nExperience testing large-scale generative AI applications, familiarity with LangChain, LlamaIndex, or other RAG-specific frameworks, and knowledge of adversarial testing techniques for AI robustness are preferred qualifications\n\nKey Responsibilities:\n\nTest Strategy & Planning\nDefine end-to-end test strategies for RAG, retrieval, generation, response coherence, and knowledge correctness\nDevelop test plans & automation frameworks to validate system performance across real-world scenarios.\nHands-on experience in benchmarking and optimizing Deep Learning Models on AI Accelerators/GPUs\nImplement E2E solutions to integrate Inference systems with customer software workflows\nIdentify and implement metrics to measure retrieval accuracy, LLM response quality\n\n\nTest Automation\nBuild automated pipelines for regression, integration, and adversarial testing of RAG workflows.\nValidate search relevance, document ranking, and context injection into LLMs using rigorous test cases.\nCollaborate with ML engineers and data scientists to debug model failures and identify areas for improvement.\nConduct scalability and latency tests for retrieval-heavy applications. Analyze failure patterns, drift detection, and robustness against hallucinations and misinformation.\n\n\nCollaboration\nWork closely with AI research, engineering teams & customer teams to align testing with business requirements.\nGenerate test reports, dashboards, and insights to drive model improvements.\nStay up to date with the latest AI testing frameworks, LLM evaluation benchmarks, and retrieval models.\n\n\nRequired Qualifications:\n8+ years of experience in AI/ML system testing, software quality engineering, or related fields.\nBachelors or masters degree in computer science engineering/ data science / AI/ML\nHands-on experience with test automation frameworks (e.g., PyTest, Robot Framework, JMeter).\nProficiency in Python, SQL, API testing, vector databases (e.g., FAISS, Weaviate, Pinecone) and retrieval pipelines.\nExperience with ML model validation metrics (e.g., BLEU, ROUGE, MRR, NDCG).\nExpertise in CI/CD pipelines, cloud platforms (AWS/GCP/Azure), and containerization (Docker, Kubernetes).\n\n\nWhy Join Us\nWork on cutting-edge AI retrieval-augmented generation technologies\nCollaborate with world-class AI researchers and engineers.\n\nIf you are passionate about AI system testing and ensuring the reliability of next-generation generative models, apply now!\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 6+ years of Systems Test Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 5+ years of Systems Test Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 4+ years of Systems Test Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['automation framework', 'continuous integration', 'python', 'sql', 'ci cd pipeline', 'kubernetes', 'ci/cd', 'cloud platforms', 'software quality', 'artificial intelligence', 'docker', 'test engineering', 'quality engineering', 'e2e', 'testing methodologies', 'vector', 'aws', 'api testing']",2025-06-12 15:03:30
AI Solutions Engineer,Edwisely,0 - 3 years,Not Disclosed,['Hyderabad'],"AI Solutions Engineer at Edwisely | Jobs at Edwisely\nExperience: 0-3 Years\nRole Overview\nWe re seeking an AI Solutions Engineer to lead the integration of GenAI tools such as ChatGPT or Claude into our core platform. You ll design, prototype, and deploy features that elevate student outcomes, make teaching exciting, and enrich dashboards with intelligent insights, all while upholding privacy and governance standards.\nKey Responsibilities\nDesign and build GenAI-powered features like guided study assistants, automated remediation engine, and intelligent feedback tools.\nDevelop end-to-end pipelines (prompt design model integration CI/CD deployment) that align with Edwisely s Intelligent Learning Infrastructure.\nCollaborate with product teams, faculty, and UX designers to deploy AI features in classrooms and dashboards.\nEnsure all AI implementations meet ISO 27001 security and CERT-In data protection guidelines.\nMeasure adoption and impact via analytics dashboards support evidence-based learning outcomes.\nWhat You ll Bring\nStrong in prompt engineering, RAG, embeddings, or QA systems using frameworks like OpenAI API, LangChain, Hugging Face Transformers.\nFamiliarity with ML deployment FastAPI, Docker, AWS/GCP within production-grade applications.\nExperience with education data, knowledge graphs, student modeling, or assessment systems is a big plus.\nResults-driven: ability to turn prototypes into scalable features.\nPassionate about improving higher education with impactful, AI-driven solutions.\nWhy Edwisely?\nBe part of a high-impact EdTech startup, transforming learning journeys of engineering students nationwide.\nWork with a product that uses AI + analytics to personalize learning, support faculty, and guide institutional decisions .\nCareer growth: define your ownership architecture, implementation, or research-led product development.\nCompetitive package, fast-paced work culture, and freedom to drive real-world impact.\nApplication Process\nSubmit your CV + GitHub or project links (especially showcasing GenAI integrations or NLP projects). for Edwisely",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['QA systems', 'remediation', 'github', 'Prototype', 'Architecture', 'HP data protector', 'GCP', 'ISO 27001', 'Deployment', 'Analytics']",2025-06-12 15:03:32
Data & Gen AI Specialist,Altimetrik,1 - 4 years,Not Disclosed,['Bengaluru'],"Job Title: Data & GenAI AWS Specialist\nExperience: 1-4 Years\nLocation: Bangalore\nMandatory Qualification: B.E./ B.Tech/ M.Tech/ MS from IIT or IISc ONLY\nJob Overview:\nWe are seeking a seasoned Data & GenAI Specialist with deep expertise in AWS Managed Services (PaaS) to join our innovative team. The ideal candidate will have extensive experience in designing sophisticated, scalable architectures for data pipelines and Generative AI (GenAI) solutions leveraging cloud services.",,,,"['Generative Ai', 'Cloud', 'Data Science', 'Open Source', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Snowflake', 'Machine Learning', 'AWS']",2025-06-12 15:03:34
Data Annotation hiring For Fresher || Excellent communication skills,Multinational Company,0 - 4 years,1-3 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Lead data annotation and collection projects.\nDevelop and implement data annotation guidelines and processes.\nTrain and manage data annotation teams.\nCollaborate with data scientists and engineers to understand data requirements.\n\nHR - 63980 09438\n\nRequired Candidate profile\nQualification - Graduate\nSalary :-\nCTC\n25,000 / experience\n20,000 / fresher\nExperience - Data Annotation only\nTransport:- Both Side\n\n5 Day working / Rotation shift / 2 day Rotation week off",Industry Type: BPM / BPO,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Object Detection', 'Data Annotation', 'Business Intelligence', 'Digital Image Processing', 'Data Management', 'Image Recognition', 'Image Analysis', 'Annotation', 'Deep Learning', 'Pattern Recognition', 'Image Processing', 'Imaging', 'Content Moderation', 'Data Warehousing', 'Data Analytics']",2025-06-12 15:03:36
Trainee Solution Engineer,Qualitykiosk Technologies,0 - 1 years,Not Disclosed,"['Mumbai', 'Navi Mumbai']","Key ResponsibilitiesLearning and Development: Engage in training sessions and workshops to enhance your skills and knowledge relevant to your role\nAssisting Senior Staff: Support senior team members in their tasks, which may include research, data analysis, and project management\nProject Participation: Contribute to team projects, providing fresh perspectives and innovative ideas to foster collaboration and creativity\nProblem-Solving: Participate in problem-solving activities, making decisions based on available information and seeking guidance when necessary\nCollaboration: Work with cross-functional teams to gain a holistic understanding of the company s operations and contribute to multidisciplinary projects",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['project management', 'python', 'data analysis', 'data analytics', 'data management', 'c', 'mis reporting', 'business analysis', 'pivot table', 'vlookup', 'machine learning', 'presales', 'javascript', 'sql', 'excel', 'react.js', 'tableau', 'node.js', 'advanced excel', 'data visualization', 'pega', 'powerpoint']",2025-06-12 15:03:38
"Business Intelligence Engineer, RBS ARTS",Amazon,5 - 10 years,Not Disclosed,['Chennai'],"An candidate will be a self-starter who is passionate about discovering and solving complicated problems, learning complex systems, working with numbers, and organizing and communicating data and reports. You will be detail-oriented and organized, capable of handling multiple projects at once, and capable of dealing with ambiguity and rapidly changing priorities. You will have expertise in process optimizations and systems thinking and will be required to engage directly with multiple internal teams to drive business projects/automation for the RBS team. Candidates must be successful both as individual contributors and in a team environment, and must be customer-centric. Our environment is fast-paced and requires someone who is flexible, detail-oriented, and comfortable working in a deadline-driven work environment. Responsibilities Include Works across team(s) and Ops organization at country, regional and/or cross regional level to drive improvements and enables to implement solutions for customer, cost savings in process workflow, systems configuration and performance metrics.\n\nBasic Qualifications\nBachelors degree in Computer Science, Information Technology, or a related field\nProficiency in automation using Python\nExcellent oral and written communication skills\nExperience with SQL, ETL processes, or data transformation\n\nPreferred Qualifications\nExperience with scripting and automation tools\nFamiliarity with Infrastructure as Code (IaC) tools such as AWS CDK\nKnowledge of AWS services such as SQS, SNS, CloudWatch and DynamoDB\nUnderstanding of DevOps practices, including CI/CD pipelines and monitoring solutions\nUnderstanding of cloud services, serverless architecture, and systems integration\n\n\nAs a Business Intelligence Engineer in the team, you will collaborate closely with business partners, architect, design, implement, and BI projects & Automations.\n\nResponsibilities:\n\nDesign, development and ongoing operations of scalable, performant data warehouse (Redshift) tables, data pipelines, reports and dashboards.\nDevelopment of moderately to highly complex data processing jobs using appropriate technologies (eg SQL, Python, Spark, AWS Lambda, etc)\nDevelopment of dashboards and reports.\nCollaborating with stakeholders to understand business domains, requirements, and expectations. Additionally, working with owners of data source systems to understand capabilities and limitations.\nDeliver minimally to moderately complex data analysis; collaborating as needed with Data Science as complexity increases.\nActively manage the timeline and deliverables of projects, anticipate risks and resolve issues.\nAdopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\nInternal job description\n\nRetail Business Service, ARTS is a growing team that supports the Retail Efficiency and Paid Services business and tech teams. There is ample growth opportunity in this role for someone who exhibits Ownership and Insist on the Highest Standards, and has strong engineering and operational best practices experience.\n\nBasic qualifications:\n\n5+ years of relevant professional experience in business intelligence, analytics, statistics, data engineering, data science or related field.\nExperience with Data modeling, SQL, ETL, Data Warehousing and Data Lakes.\nStrong experience with engineering and operations best practices (version control, data quality/testing, monitoring, etc)\nExpert-level SQL.\nProficiency with one or more general purpose programming languages (eg Python, Java, Scala, etc)\nKnowledge of AWS products such as Redshift, Quicksight, and Lambda.\nExcellent verbal/written communication & data presentation skills, including ability to succinctly summarize key findings and effectively communicate with both business and technical teams.\n\nPreferred qualifications:\n\nExperience with data-specific programming languages/packages such as R or Python Pandas.\nExperience with AWS solutions such as EC2, DynamoDB, S3, and EMR.\nKnowledge of machine learning techniques and concepts. 3+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc experience\nExperience with data visualization using Tableau, Quicksight, or similar tools\nExperience with data modeling, warehousing and building ETL pipelines\nExperience in Statistical Analysis packages such as R, SAS and Matlab\nExperience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling Experience with AWS solutions such as EC2, DynamoDB, S3, and Redshift\nExperience in data mining, ETL, etc and using databases in a business environment with large-scale, complex datasets",,,,"['SAS', 'Data modeling', 'Oracle', 'Business intelligence', 'MATLAB', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-12 15:03:41
Software Development Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"As a Software Development Engineer, you will get the opportunity to own problems end-to-end and work with some of the best minds in Amazon. This role is for a full-stack developer with an emphasis on designing highly scalable and extensible applications. You will design flexible and scalable solutions, and work on some of the most complex challenges in computing by utilizing your skills in data structures, algorithms, and principle programming. You will have a broad range of responsibilities from design, development, testing, deployment and operations. You would have easy access to Sr SDE, and Principal Engineers to bounce off your ideas and discuss tech solutions.\n\n\nN/A\n\nA day in the life\nN/A 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'Software Development Engineer II', 'development testing', 'Coding', 'Architectural design', 'Software development life cycle', 'Design development', 'Programming', 'Data structures', 'Internship']",2025-06-12 15:03:43
Artificial Engineer,Care Allianz,1 - 4 years,Not Disclosed,['Pune'],Care Allianz is looking for Artificial Engineer to join our dynamic team and embark on a rewarding career journey\n\nDevelops and maintains AI/ML models for automation and analytics\n\nTrains data pipelines and manages model lifecycle\n\nCollaborates on product features using artificial intelligence\n\nMonitors model performance and ensures quality outcomes,Industry Type: Software Product,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['project management', 'python', 'mechanical engineering', 'software testing', 'production', 'plc', 'manual testing', 'engineering', 'autocad', 'catia', 'kaizen', 'electricals', 'manpower handling', 'automation', 'java', 'selenium', 'computer science', 'quality assurance', 'design', 'production engineering']",2025-06-12 15:03:46
Data Science Manager,ZS,10 - 15 years,Not Disclosed,"['Pune', 'Bengaluru']","A key enabler of our services is leveraging data in delivering client solutions. The data available about customers is getting richer and the problems that our customers are trying to answer continue to evolve. In our endeavor to stay ahead in providing solutions to these evolving complex problems, ZS has set up an Advanced Data Science which has three major focus areas:\nResearch the evolving datasets and advanced analytical techniques to develop new offerings/solutions\nDeliver client impact by collaboratively implementing these solutions",,,,"['Team management', 'data science', 'Pharma', 'Analytical', 'Management consulting', 'Financial planning', 'Healthcare', 'Project planning', 'Predictive modeling', 'Financial services']",2025-06-12 15:03:48
Apprentice - Operations,Startek,0 - 1 years,Not Disclosed,['Kolkata'],"STARTEK is looking for Apprentice - Operations to join our dynamic team and embark on a rewarding career journey\n\nSupport daily operations and gain exposure to process workflows\n\nAssist with documentation, reporting, and coordination tasks\n\nLearn industry standards and company-specific systems\n\nParticipate in training and mentorship activities",Industry Type: Oil & Gas,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Wireless', 'Business transformation', 'orchestration', 'Artificial Intelligence', 'Customer support', 'Customer experience', 'Management', 'Troubleshooting', 'Customer engagement', 'digital transformation']",2025-06-12 15:03:50
Executive - Operations,Startek,0 - 6 years,Not Disclosed,['Kolkata'],"STARTEK is looking for Executive - Operations to join our dynamic team and embark on a rewarding career journey An Executive - Operations is responsible for overseeing the day-to-day operations of a company to ensure efficiency, profitability, and growth\n\nSome of the key responsibilities for this role include:\n\nDeveloping and implementing operational strategies\n\n\n\nMonitoring and analyzing operational metrics to identify areas for improvement and implementing continuous improvement initiatives\n\n\n\nExpress their operations strategies & objectives to make sure that the company which they are working for reaches its target and operates effectively\n\n\n\nStrong leadership and problem-solving skills, as well as the ability to analyze data and make informed decisions, are essential for success in this role",Industry Type: Oil & Gas,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Wireless', 'Business transformation', 'orchestration', 'Artificial Intelligence', 'Customer support', 'Operation Executive', 'Customer experience', 'Troubleshooting', 'Customer engagement', 'digital transformation']",2025-06-12 15:03:53
Senior Applied AI Scientist,ZS,4 - 9 years,Not Disclosed,['Bengaluru'],"Write complex SQL queries for data extraction, perform exploratory data analysis (EDA) to uncover insights.\nStrong proficiency in Python and Py Spark for scalable data processing and analytics.\nCreate, transform, and optimize features to enhance model performance.\nTrain, evaluate, and maintain machine learning models in production.\nWrite efficient, maintainable, and version-controlled code that handles large datasets.\nRegularly update internal teams and clients on project progress, results, and insights.\nConduct hypothesis testing and experiment analysis to drive data-driven decisions using AB testing.",,,,"['Data analysis', 'data security', 'Financial planning', 'Management consulting', 'Machine learning', 'Data processing', 'Analytics', 'Data extraction', 'Python']",2025-06-12 15:03:55
"Catalog Specialist I, Amazon Business Catalog Taxonomy",Amazon,0 - 7 years,Not Disclosed,['Hyderabad'],"Amazon Business Catalog Taxonomy Operations team is focused on building solutions that enable B2B customers to find, research, and buy products and services from a vast selection, across multiple devices, marketplaces and regions. The team ensures that our selection is classified for AB customers to perform business specific functions such as approval routing, spend analysis, create procurement policies for compliance, and do forecasting and reporting.\n\n\nWe re looking for people who have the ability to follow given guidelines and make decisions in ambiguous situations, as they work with a team focused on assigning global classifications to our Amazon catalog selection for business customers. They ll use internal tools to manage workload and should feel confident to actively contribute to process improvement initiatives. We d love to speak to candidates who already have proficiency in Microsoft Office, with an emphasis on basic Excel competencies.\n\nA day in the life\nYou will 1) Create Machine Learning rules / Classify ASINs for global classification standards (e.g. UNSPSC) 2) Use tools to create and manage classification mappings between internal catalog and external taxonomy 3) actively troubleshoot and respond to issues that are caused by incorrect classification, mappings or rationales.\n\nAbout the team\nWe are a global and multicultural team who interacts daily with teammates across other regions (EU, JP) and global stakeholders.\nThe teams vision is to have the product catalog perfectly classified for our AB customers.\nWe classify millions of items daily for 10 marketplaces which helps our business customers` ordering experience more smooth. Bachelors degree\nSpeak, write, and read fluently in English\nExperience with Microsoft Office products and applications\nExperience with Excel Catalog knowledge\nSQL Query knowledge",,,,"['Procurement', 'Compliance', 'Process improvement', 'Machine learning', 'Spend analysis', 'MS Office', 'Troubleshooting', 'Forecasting', 'SQL', 'Recruitment']",2025-06-12 15:03:57
Software Development Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Want to join a team that protects and improves the buyer experience of millions Amazon customers and builds earths most customer-centric sellers daily using innovative technology including machine learning, data mining and big data analytics, cloud computing services, and highly available/scalable distributed systems that support hundreds of millions of transactions across the globe?\n\nWe have an exciting opportunity with the Regulatory Intelligence, Safety, and Compliance (RISC) engineering team, to architect and build next-generation engineering systems to quickly and accurately identify and mitigate product safety issues and potential risks to the customer experience.\n\nAs a Software Development Engineer, you will work with your team of highly skilled software, data, and ML engineers to invent, design, build and manage highly scalable distributed systems that provide availability, scalability and latency guarantees. You will work with your internal customers to balance customer requirements with team requirements and help your team and business evolve, by working with LLMs and large data sets. You will be using the latest AI, AWS and industry technologies to deliver a one-stop risk identification and remediation ecosystem for Amazon, keeping our customers safe and products compliant, building the software creating the world s most trustworthy data set on everything companies and customers need to know related to the safety and compliance of products and chains.\n\nEach and every person buying, selling, or handling Amazon products will be your customer.\n\nAs a member of this growing team, you ll be able to build the groundwork and influence its direction for the years to come. Our work cuts across various disciplines from delivering an awesome user experience via great UI/UX, to building massively scalable backend systems to support the most high-traffic pages on Amazon.com, to analytical and feedback systems which give us data-driven customer insights, to using machine learning and AI to influence recommendations and marketing. If you have a passion for consumer-facing applications, and are obsessed with customer experience, we want you!\n\nIf you d like to make a real-world difference by working hard, having fun, and making history, this is the team for you!\n\n\nIn this role you will:\nHelp define the system architecture, own and implement specific components, and help shape the overall experience\nCollaborate closely with product managers, UX designers, and other SDE team members to help define the scope of the product\nTake responsibility for technical problem solving, creatively meeting product objectives, and developing best practices\nDemonstrate cross-functional resource interaction to accomplish your goals\nWrite high-quality, efficient, testable code in Java and other object-oriented languages\nDesign Amazon-scale tools to facilitate internal business\nBuild highly available, secure, and low-latency systems\nMentor other developers\nFind out what it takes to engineer systems for ""Amazon Scale""\nDesign and build microservices\nOwn and operate the systems that you build based on real-time customer data and demanding service-level agreements\nContribute to planning, design, implementation, testing, operations, and process improvement\n\nA day in the life\nHigh-level designs, cross-team alignment, long-term architectural roadmap and technical strategy, understanding the business domain and proposing solutions to address customer and business problems, helping scope and analyze product requirements, mentorship, reviewing CRs, writing high-quality code to be an example for the team. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\n3+ years of Video Games Industry (supporting title Development, Release, or Live Ops) experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Cloud computing', 'Backend', 'Coding', 'Analytical', 'Machine learning', 'Data mining', 'Internship', 'Distribution system']",2025-06-12 15:03:59
Senior/Lead MLops Engineer,Tiger Analytics,7 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","JOB DESCRIPTION\n\nSenior MLE / Architect MLE (ML Ops) Chennai / Bangalore / Hyderabad (Hybrid)\n\nWho we are Tiger Analytics is a global leader in AI and analytics, helping Fortune 1000 companies solve their toughest challenges. We offer fullstack AI and analytics services & solutions to empower businesses to achieve real outcomes and value at scale. We are on a mission to push the boundaries of what AI and analytics can do to help enterprises navigate uncertainty and move forward decisively. Our purpose is to provide certainty to shape a better tomorrow. Our team of 4000+ technologists and consultants are based in the US, Canada, the UK, India, Singapore and Australia, working closely with clients across CPG, Retail, Insurance, BFS, Manufacturing, Life Sciences, and Healthcare. Many of our team leaders rank in Top 10 and 40 Under 40 lists, exemplifying our dedication to innovation and excellence. We are a Great Place to Work-Certified (2022-24), recognized by analyst firms such as Forrester, Gartner, HFS, Everest, ISG and others. We have been ranked among the Best and Fastest Growing analytics firms lists by Inc., Financial Times, Economic Times and Analytics India Magazine.",,,,"['MLops', 'Azure', 'Snowflake', 'Deployment', 'Ci/Cd', 'Machine Learning']",2025-06-12 15:04:02
Data Science Analyst (Lead),Infogain,8 - 11 years,Not Disclosed,['Gurugram'],"- Strong understanding of ML algorithms (regression, classification, clustering) with the ability to independently develop and scale models using Python.\n- Experience in commercial analytics with a knack for translating business problems into analytical solutions and strategic recommendations.\n- Proficient in Power BI to build intuitive dashboards and deliver insights in a clear, actionable format.\n- Strong storytelling and communication skills to convey complex analytical findings to both technical and non-technical audiences.\n- Demonstrates ownership and leadership, driving projects from idea to impact with minimal supervision.\n  SKILLS\nPrimary Skill: Data Science\nSub Skill(s): Data Science\nAdditional Skill(s): Python, Data Science",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analyst', 'Translation', 'data science', 'Analytical', 'power bi', 'Analytics', 'Supervision', 'Python']",2025-06-12 15:04:04
"Software Development Engineer II, RISC",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Want to join a team that protects and improves the buyer experience of millions Amazon customers and builds earths most customer-centric sellers daily using innovative technology including machine learning, data mining and big data analytics, cloud computing services, and highly available/scalable distributed systems that support hundreds of millions of transactions across the globe?\n\nWe have an exciting opportunity with the Regulatory Intelligence, Safety, and Compliance (RISC) engineering team, to architect and build next-generation engineering systems to quickly and accurately identify and mitigate product safety issues and potential risks to the customer experience.\n\nAs a Software Development Engineer, you will work with your team of highly skilled software, data, and ML engineers to invent, design, build and manage highly scalable distributed systems that provide availability, scalability and latency guarantees. You will work with your internal customers to balance customer requirements with team requirements and help your team and business evolve, by working with LLMs and large data sets. You will be using the latest AI, AWS and industry technologies to deliver a one-stop risk identification and remediation ecosystem for Amazon, keeping our customers safe and products compliant, building the software creating the world s most trustworthy data set on everything companies and customers need to know related to the safety and compliance of products and chains.\n\nEach and every person buying, selling, or handling Amazon products will be your customer.\n\nAs a member of this growing team, you ll be able to build the groundwork and influence its direction for the years to come. Our work cuts across various disciplines from delivering an awesome user experience via great UI/UX, to building massively scalable backend systems to support the most high-traffic pages on Amazon.com, to analytical and feedback systems which give us data-driven customer insights, to using machine learning and AI to influence recommendations and marketing. If you have a passion for consumer-facing applications, and are obsessed with customer experience, we want you!\n\nIf you d like to make a real-world difference by working hard, having fun, and making history, this is the team for you!\n\n\nIn this role you will:\nHelp define the system architecture, own and implement specific components, and help shape the overall experience\nCollaborate closely with product managers, UX designers, and other SDE team members to help define the scope of the product\nTake responsibility for technical problem solving, creatively meeting product objectives, and developing best practices\nDemonstrate cross-functional resource interaction to accomplish your goals\nWrite high-quality, efficient, testable code in Java and other object-oriented languages\nDesign Amazon-scale tools to facilitate internal business\nBuild highly available, secure, and low-latency systems\nMentor other developers\nFind out what it takes to engineer systems for ""Amazon Scale""\nDesign and build microservices\nOwn and operate the systems that you build based on real-time customer data and demanding service-level agreements\nContribute to planning, design, implementation, testing, operations, and process improvement\n\nA day in the life\nHigh-level designs, cross-team alignment, long-term architectural roadmap and technical strategy, understanding the business domain and proposing solutions to address customer and business problems, helping scope and analyze product requirements, mentorship, reviewing CRs, writing high-quality code to be an example for the team. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\n3+ years of Video Games Industry (supporting title Development, Release, or Live Ops) experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Cloud computing', 'Backend', 'Coding', 'Analytical', 'Machine learning', 'Data mining', 'Internship', 'Distribution system']",2025-06-12 15:04:07
Process Associate- Invoice To Cash,Genpact,0 - 3 years,Not Disclosed,['Jodhpur'],"Ready to shape the future of work?\nAt Genpact, we don't just adapt to changewe drive it. AI and digital innovation are redefining industries and wereleading the charge. Genpact’s AI Gigafactory, our industry-first accelerator,is an example of how advanced technology solutions were scaling to help globalenterprises work smarter, grow faster, and transform at scale. From large-scalemodels to agentic AI, our breakthrough solutions tackle companies’ most complexchallenges.\nIf you thrive in a fast-moving, tech-drivenenvironment, love solving real-world problems, and want to be part of a teamthat’s shaping the future, this is your moment",,,,"['accounts receivable', 'law', 'verbal communication', 'interpersonal skills', 'pay', 'hrsd', 'accounting', 'order to cash', 'cash applications', 'reconciliation', 'technology solutions', 'artificial intelligence', 'operations', 'automation', 'recruitment', 'writing', 'english', 'operational excellence']",2025-06-12 15:04:09
Senior Verification Engineer - GPU Fullchip,Nvidia,4 - 10 years,Not Disclosed,['Bengaluru'],"NVIDIA has continuously reinvented itself. Our invention of the GPU sparked the growth of the PC gaming market, redefined modern computer graphics, and revolutionized parallel computing. Today, research in artificial intelligence is booming worldwide, which calls for highly scalable and massively parallel computation horsepower that NVIDIA GPUs excel. NVIDIA is a learning machine that constantly evolves by adapting to new opportunities that are hard to solve, that only we can address, and that matter to the world. This is our life s work , to amplify human creativity and intelligence. As an NVIDIAN, you ll be immersed in a diverse, supportive environment where everyone is inspired to do their best work. Come join our diverse team and see how you can make a lasting impact on the world!\nThe GPU started out as an engine for simulating human imagination, conjuring up the amazing virtual worlds of video games and Hollywood films. Today, NVIDIA s GPU simulates human intelligence, running deep learning algorithms and acting as the brain of computers, robots, and self-driving cars that can perceive and understand the world. NVIDIA is increasingly known as the AI computing company.\nWhat you ll be doing:\nAs a key member of our ASIC Verification team, you will verify the design and implementation of the industrys leading GPUs.\nResponsible for verification of the ASIC design, architecture, golden models and micro-architecture of the GPUs and full-chip level using advanced verification tools and methodologies.\nBuild reusable bus functional models, traffic generators, monitors, checkers and scoreboards following coverage driven verification methodology.\nYou are expected to understand the design and implementation, define the verification scope, develop the verification infrastructure and verify the correctness of the design.\nWorking with architects, designers, and pre and post silicon verification teams to accomplish your tasks.\nWhat we need to see:\nB. Tech. / M. Tech or equivalent experience with 4+ years of relevant experience\nProficiency in Verilog and C\nFamiliarity with OOPs concepts\nHands-on experience in verification at Unit/Sub-system/SOC level\nGood understanding of computer architecture concepts\nStrong technical fundamentals with superior analytical skills and problem-solving skills\nKnowledge of SystemVerilog or similar HVL is highly desirable\nExperience in verification methodologies like UVM/VMM is highly desirable\nExposure to industry standard verification tools for simulation and debug\nWays to stand out from the crowd:\nPrior experience in 3D graphics processing or processor verification\nC/C++ programming language experience\nExperience in scripting and tool development using Perl and Python\nExcellent debugging skills\nGood interpersonal and communication skills & dream to work as a great teammate\n#LI-Hybrid",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Graphics', 'Simulation', 'SOC', 'Artificial Intelligence', 'Verilog', 'VMM', 'Perl', 'Silicon', 'UVM', 'Python']",2025-06-12 15:04:12
Senior SOC Design Engineer,Nvidia,5 - 8 years,Not Disclosed,['Bengaluru'],"NVIDIA has continuously reinvented itself. Our invention of the GPU sparked the growth of the PC gaming market, redefined modern computer graphics, and revolutionized parallel computing. Today, research in artificial intelligence is booming worldwide, which calls for highly scalable and massively parallel computation horsepower that NVIDIA GPUs excel. NVIDIA is a learning machine that constantly evolves by adapting to new opportunities that are hard to solve, that only we can address, and that matter to the world. This is our life s work , to amplify human creativity and intelligence. As an NVIDIAN, you ll be immersed in a diverse, supportive environment where everyone is inspired to do their best work. Come join our diverse team and see how you can make a lasting impact on the world!\nNVIDIA System-On-Chip (SOC) group is hiring for a Senior SOC Design Engineer! The complexity of the chips we build has increased manifold over the years. We are now packing tens of billions of transistors in a chip to meet the growing computing demand. We are looking for a star candidate with strong inclination in RTL integration and chip level front-end design, including padring, pinmuxing, SOC Assembly process, retiming etc. You must have a real passion for methodologies and automation solutions that enable SOC creation in the most optimized way. In this position, you will get the chance to build sophisticated Tegra SOCs, work closely with chip management to set ASIC execution timelines goals while directly interacting with System Architecture, unit-level ASIC, Physical Design, CAD, Package Design, DFT and other teams. Additionally, you will be involved in defining and crafting methodologies that build more efficient and flexible SOCs in future.\nWhat youll be doing:\nDrive SOC Assembly and design chip level functions for Tegra SOCs.\nResponsible for front-end design quality/correctness checks, reviews and driving those with multi-functional teams.\nDrive SOC execution across chip milestones working with all multi-functional teams to help define, track and drive complex dependencies.\nDefine and develop system-level methodologies, tools, and IPs to build SOCs in an efficient and scalable manner.\nIdentify difficulties and inefficiencies in the front-end chip implementation process and propose and implement ideas to solve them.\nWhat we need to see:\nB. Tech or M. Tech in Electronics Engineering.\n5+ years of proven experience in chip design, specializing in SOC integration and design automation. Padring and fuse/floorsweep design experience is a bonus.\nExcellent analytical and problem-solving skills.\nExperience in RTL design (Verilog), System-On-Chip design/implementation flow.\nStrong coding skills in Perl, Python, or other industry-standard scripting languages.\nExposure to various Chip Design Functions to be able to collaborate and solve complex multi-functional problems.\nExcellent interpersonal skills to work with multiple teams to drive consensus.\nGood teamwork spirit and collaboration skills with team members.\nBackground in SOC Verification, Synthesis, Physical design and DFT is a bonus.\nExperience in RTL Build flows and Makefiles is a plus.\n#LI-Hybrid",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['System architecture', 'Automation', 'ASIC', 'DFT', 'Coding', 'Verilog', 'CAD', 'Perl', 'Python', 'Physical design']",2025-06-12 15:04:14
Lead Engineer - App F/W&MW - Linux,Sasken Technologies,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position takes ownership of a module and associated quality and delivery. Person at this position provides instructions, guidance and advice to team members to ensure quality and on time delivery.\nPerson at this position is expected to be able to instruct and review the quality of work done by technical staff.\nPerson at this position should be able to identify key issues and challenges by themselves, prioritize the tasks and deliver results with minimal direction and supervision.\nPerson at this position has the ability to investigate the root cause of the problem and come up alternatives/ solutions based on sound technical foundation gained through in-depth knowledge of technology, standards, tools and processes.\nPerson has the ability to organize and draw connections among ideas and distinguish between those which are implementable.\nPerson demonstrates a degree of flexibility in resolving problems/ issues that atleast to in-depth command of all techniques, processes, tools and standards within the relevant field of specialisation.\n\n\nRoles & Responsibilities\nResponsible for requirement analysis and feasibility study including system level work estimation while considering risk identification and mitigation.\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals.\nResponsible for traceability of the requirements from design to delivery Code optimization and coverage.\nResponsible for conducting reviews, identifying risks and ownership of quality of deliverables.\nResponsible for identifying training needs of the team.\nExpected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments.\nExpected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\nExpected to be a technical mentor for junior members.\nPerson may be given additional responsibility of managing people based on discretion of Project Manager.\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 5-8 years\n\nCompetencies DescriptionApplication Protocol & Engines - Linux engineer is one:\nwho has done one or more of the following on Embedded Linux\ndesign, development/customization, bug fixing/sustenance\nwho has experience in one or more of the following domains\nMultimedia\nTelephony\nConnectivity\nSensor\nSecurity\nPlatforms-\nMandatory to have worked on one or more of the following:\nEmbedded Linux\nTools-\nMandatory to have worked on one or more of the following;\ngdb/ddd; linux editors; top; ps; meminfo\nLanguages-\nMandatory to have worked on one or more of the following;\nC; C++\nSpecialization-\nMULTIMEDIA, CONNECTIVITY, TELEPHONY, CARRIER GRADE PLATFORM, GENERIC FRAMEWORK",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['LINUX', 'Unix', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Machine Learning', 'Python']",2025-06-12 15:04:16
STEM Innovation Engineer,Stemrobo,0 - 4 years,2.5-3.5 Lacs P.A.,"['Kottayam', 'Delhi / NCR', 'Mumbai (All Areas)']","1. Develop strong expertise in the field of Robotics, IOT, AI, Drone & Coding.\n2. Regular visits to schools and provide mentorships to K-12 students.\n3. Organize, Develop and coordinate special STEM Engineering and technology based events/activities\n\nRequired Candidate profile\nPractical knowledge of Arduino, Raspberry Pi and other microcontrollers is required.\nSound knowledge in C and Python Language.\nGood verbal and written communication.\nFlexible with working hours.",Industry Type: Education / Training,Department: Teaching & Training,"Employment Type: Full Time, Permanent","['Arduino', 'IOT', 'Mechatronics', 'C', 'Coding', 'Artificial Intelligence', 'Electronics', 'Drone', 'Robotics', 'Python']",2025-06-12 15:04:18
Senior Associate Data Scientist,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will identify trends, root causes, and potential improvements in our products and processes, ensuring that patient voices are heard and addressed with utmost precision.\nAs the Sr Associate Data Scientist at Amgen, you will be responsible for developing and deploying basic machine learning, operational research, semantic analysis, and statistical methods to uncover structure in large data sets. This role involves creating analytics solutions to address customer needs and opportunities.\nCollect, clean, and manage large datasets related to product performance and patient complaints.\nEnsure data integrity, accuracy, and accessibility for further analysis.\nDevelop and maintain databases and data systems for storing patient complaints and product feedback.\nAnalyze data to identify patterns, trends, and correlations in patient complaints and product issues.\nUse advanced statistical methods and machine learning techniques to uncover insights and root causes.\nDevelop analytics or predictive models to foresee potential product issues and patient concerns to address customer needs and opportunities.\nPrepare comprehensive reports and visualizations to communicate findings to key collaborators.\nPresent insights and recommendations to cross-functional teams, including product development, quality assurance, and customer service.\nCollaborate with regulatory and compliance teams to ensure adherence to healthcare standards and regulations.\nFind opportunities for product enhancements and process improvements based on data analysis.\nWork with product complaint teams to implement changes and monitor their impact.\nStay abreast of industry trends, emerging technologies, and standard methodologies in data science and healthcare analytics.\nEvaluate data to support product complaints.\nWork alongside software developers and software engineers to translate algorithms into commercially viable products and services.\nWork in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.\nPerform exploratory and targeted data analyses using descriptive statistics and other methods.\nWork with data engineers on data quality assessment, data cleansing and data analytics\nGenerate reports, annotated code, and other projects artifacts to document, archive, and communicate your work and outcomes.\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience OR\nBachelors degree and 3 to 5 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience OR\nDiploma and 7 to 9 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience\nPreferred Qualifications:\nDemonstrated skill in the use of applied analytics, descriptive statistics, feature extraction and predictive analytics on industrial datasets.\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification.\nExperience in analyzing time-series data for forecasting and trend analysis.\nExperience with Data Bricks platform for data analytics.\nExperience working with healthcare data, including patient complaints, product feedback, and regulatory requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'data bricks', 'hypothesis testing', 'predictive analytics', 'data visualization', 'machine learning', 'statistics']",2025-06-12 15:04:20
Artificial Intelligence Engineer,Wissen Technology,4 - 9 years,Not Disclosed,"['Bengaluru', 'Mumbai (All Areas)']","Roles and Responsibilities\nDesign, develop, test, and deploy AI models using Python and deep learning frameworks such as TensorFlow, PyTorch, or scikit-learn.\nCollaborate with cross-functional teams to integrate AI solutions into existing systems and applications.\nDevelop REST APIs using Flask or Django for data processing and integration purposes.\nTroubleshoot issues related to model performance, data quality, and system architecture.\nStay up-to-date with industry trends and advancements in artificial intelligence research.\nDesired Candidate Profile\n4-9 years of experience in developing AI models using Python programming language.\nStrong proficiency in at least two deep learning frameworks (TensorFlow, PyTorch, or scikit-learn).\nExperience working on projects involving natural language processing (NLP), computer vision, or other areas of machine learning.\nBachelor's degree in Any Specialization (B.Tech/B.E.).\nHands-on experience with Pandas library for data manipulation and analysis.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Rest Api Development', 'Tensorflow', 'Pytorch', 'Pandas', 'Scikit-Learn']",2025-06-12 15:04:22
Data Scientist,Big Oh Tech,4 - 6 years,Not Disclosed,['Noida'],"Key Responsibilities:\n\nDesign, build, and maintain robust and scalable data pipelines to support analytics and reporting needs.\nManage and optimize data lake architectures, with a focus on Apache Atlas for metadata management, data lineage, and governance.\nIntegrate and curate data from multiple structured and unstructured sources to enable advanced analytics.\nCollaborate with data scientists and business analysts to ensure availability of clean, well-structured data.\nImplement data quality, validation, and monitoring processes across data pipelines.\nDevelop and manage Power BI datasets and data models, supporting dashboard and report creation.\nSupport data cataloging and classification using Apache Atlas for enterprise-wide discoverability and compliance.\nEnsure adherence to data security, privacy, and compliance policies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advanced analytics', 'metadata', 'Compliance', 'Business Analyst', 'data security', 'power bi', 'Data quality', 'Management', 'Apache', 'Monitoring']",2025-06-12 15:04:24
Artificial Intelligence Engineer,Indovation Lab,5 - 10 years,Not Disclosed,"['Bengaluru', 'Delhi / NCR']","The Opportunity:\nWe are seeking a talented and experienced Senior AI Engineer to play a pivotal role in developing and enhancing our conversational AI capabilities across multiple modalities (text, voice/telephony) and building intelligent agents to drive workflow automation. You will work closely with our Director of AI Engineering, product managers, and other engineering teams to bring our AI vision to life, primarily focusing on fine-tuning off-the-shelf Large Language Models (LLMs) and developing agentic systems for end-users (members, CBO staff, healthcare providers).\nThis is a unique opportunity to apply your AI expertise to solve meaningful problems in the social and healthcare space, directly impacting people's lives.\nWhat You'll Do:\nConversational AI Development & Fine-Tuning (Text & Voice):\nLead the fine-tuning, evaluation, and deployment of pre-trained LLMs (e.g., Gemini, GPT series, open-source models) to create natural, empathetic, and effective conversational experiences for various user interactions via text-based channels (chat, SMS) and voice-based telephonic systems (IVR chatbots).\nDevelop and implement strategies for data collection, preparation, and augmentation to support model fine-tuning and continuous improvement for both text and voice modalities.\nDesign and implement robust evaluation frameworks to measure conversational AI performance, including metrics for accuracy, fluency, empathy, task completion, and call handling efficiency for telephonic agents.\nWork on prompt engineering, context management, and dialogue flow design to optimize conversational AI interactions across channels.\nIntegrate and manage speech-to-text (STT) and text-to-speech (TTS) services for telephonic AI solutions.\nAgentic System Development:\nDesign, build, and deploy AI agents that can reason, make decisions, and take actions to automate and optimize key workflows within the platform (e.g., intelligent referral initiation, proactive follow-ups, task management assistance).\nDevelop agents capable of interacting with internal platform APIs, external data sources, and potentially third-party tools to achieve their goals.\nExplore and implement techniques for agent planning, tool usage, and multi-step reasoning.\nCollaboration & Technical Leadership:\nCollaborate closely with the Director of AI Engineering to define AI strategy, architecture, and technical roadmap for conversational AI and agentic systems.\nPartner with Product Managers to understand user needs and translate them into technical requirements for AI features.\nWork with platform and application engineers to integrate AI models and agents into the broader ecosystem.\nMentor junior engineers and contribute to building a strong AI engineering culture.\nStay up-to-date with the latest advancements in LLMs, conversational AI (text and voice), agent-based systems, and MLOps.\nMLOps & Productionization:\nContribute to the development and maintenance of our MLOps infrastructure for training, deploying, monitoring, and iterating on AI models and agents in production.\nEnsure AI systems are scalable, reliable, and maintainable.\nBack-end Development:\nSolid understanding of back-end development principles and experience building or integrating with APIs (e.g., RESTful services) to connect AI models and agents with broader application systems.\nFamiliarity with database technologies (SQL and/or NoSQL) and practical experience in how AI systems interact with data storage and retrieval for training, inference, and logging.\nWhat You'll Bring:\nEducation: Bachelor's or Master's degree in Computer Science, Artificial Intelligence, Machine Learning, or a related field.\nExperience:\n5+ years of hands-on experience in AI/ML engineering, with a significant focus on Natural Language Processing (NLP) and conversational AI.\nDemonstrable experience developing and deploying AI-powered telephonic chatbots or Interactive Voice Response (IVR) systems, including integration with STT/TTS technologies.\nProven experience fine-tuning and deploying Large Language Models (LLMs) for specific tasks and domains. Strong understanding of model architectures, training techniques, and evaluation metrics.\nDemonstrable experience designing and building AI agents or systems that exhibit autonomous behavior, decision-making, and/or tool usage.\nProficiency in Python and common AI/ML frameworks (e.g., TensorFlow, PyTorch, Hugging Face Transformers, LangChain, LlamaIndex).\nExperience with cloud platforms (GCP preferred, AWS/Azure acceptable) and their AI/ML services (e.g., Vertex AI, SageMaker, cloud telephony APIs).\nSolid understanding of MLOps principles and experience with tools for model deployment, monitoring, and CI/CD for ML.\nSkills:\nStrong analytical and problem-solving skills.\nExcellent communication and collaboration abilities.\nAbility to translate complex technical concepts to non-technical stakeholders.\nProactive, self-starter with a passion for building impactful AI solutions.\nNice to Haves:\nExperience working in the healthcare or social care domain.\nFamiliarity with data privacy and security considerations in regulated environments (e.g., HIPAA).\nExperience with specific telephony platforms or APIs (e.g., Twilio, Vonage, Google Dialogflow CX).\nContributions to open-source AI/ML projects.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Conversational Ai', 'Python', 'TTS', 'NLP', 'GCP', 'SST', 'Artificial Intelligence', 'Natural Language Processing', 'Chatbot Development', 'Dialogflow']",2025-06-12 15:04:26
Test Automation Software Engineer - CDOT,National Institute for Smart Government (NISG),3 - 6 years,8-12 Lacs P.A.,['Bengaluru'],"Job Description\nName of the Post: Test Automation Software Engineer\nNo of Posts: 1\nLocation: Bangalore\nRoles and Responsibilities:\nDesign, Develop and Build Test Automation frameworks\nDesign and Development of Test suites for the 4G and 5G system.\nWrite, design, and execute automated tests by creating scripts that run testing functions automatically\nMaximize test coverage for the most critical features of the system\nDetermine the priority for test scenarios and create execution plans to implement these scenarios\n\nSkills and Abilities:\n\nComprehensive knowledge in the field of software development.\nKnowledge of Artificial Intelligence/Machine Language\nKnowledge on Cloud computing, CI/CD tools like Jekins\nUnderstanding the Industry Standards: 3gpp standards, Tec standards\nModerate Knowledge on simulators: 2G/3G/4G node Simulators, UE simulators\nModerate knowledge on Software testing tools: Wire shark, Scripting languages.\nLinux administration and Linux networking\nProgramming expertise: High level languages C,C++, Perl, Java and Python\nModerate knowledge on Databases: SQL, ORACLE etc.\nKnowledge on Android OS to Support Mobile-Telephone Automations\nFull stack developers for Test Automation of Validation Test Cases Execution, Configuration, monitoring and maintaining the Test Environment\n\nMinimum Qualifications:\n\nB.E/B.Tech in Computer Science & Engineering or equivalent degree from a recognized college/university\nMinimum 3 years of professional work experience post qualification.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Software Development', '4G', '3G', 'Artificial Intelligence', 'Machine Learning', '2G']",2025-06-12 15:04:28
Manager Data Science,Optum,12 - 17 years,Not Disclosed,['Noida'],"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start  Caring. Connecting. Growing together. \n\nAt Optum AI, we leverage data and resources to make a significant impact on the healthcare system. Our solutions have the potential to improve healthcare for everyone. We work on cutting-edge projects involving ML, NLP, and LLM techniques, continuously developing and improving generative AI methods for structured and unstructured healthcare data. Our team collaborates with world-class experts and top universities to develop innovative AI/ML solutions, often leading to patents and published papers.\n\n Primary Responsibilities \nDevelop and implement AI and machine learning strategies for several healthcare domains\nCollaborate with cross-functional teams to identify and prioritize AI and machine learning initiatives\nManage the development and deployment of AI and machine learning solutions\nDevelop and run pipelines for data ingress and model output egress\nDevelop and run scripts for ML model inference\nDesign, implement, and maintain CI/CD pipelines for MLOps and DevOps functions\nIdentify technical problems and develop software updates and fixes\nDevelop scripts or tools to automate repetitive tasks\nAutomate the provisioning and configuration of infrastructure resources\nProvide guidance on the best use of specific tools or technologies to achieve desired results\nCreate documentation for infrastructure design and deployment procedures\nUtilize AI/ML frameworks and tools such as MLFlow, TensorFlow, PyTorch, Keras, Scikit-learn, etc.\nLead and manage AI/ML teams and projects from ideation to delivery and evaluation\nApply expertise in various AI/ML techniques, including deep learning, NLP, computer vision, recommender systems, reinforcement learning, and large language models\nCommunicate complex AI/ML concepts and results to technical and non-technical audiences effectively\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\n Required Qualifications: \nBachelors/master degree in computer science, engineering, mathematics, statistics, or a related discipline\n12+ years of experience in Software Engineering, Data Science, or Analytics with 8+ years of experience in AI/ML engineering or related fields\nExperience with cloud platforms and services, such as AWS, Azure, GCP, etc.\nExperience in developing solutions in the NLP space and relevant projects\nHands on Experience in AI and drive the development of innovative AI and machine learning solutions\nDemonstrated experience in leading and managing AI/ML teams and projects, from ideation to delivery and evaluation\nExperience with Azure development environments\nKnowledge of NLP literature, thrust areas, conference venues, and code repositories\nFamiliarity with both open-source and OpenAI LLMs and RAG architecture\nFamiliarity with UI tools like Streamlit, Flask, FAST APIs, Rest APIs, Docker containers\nUnderstanding of common NLP tasks such as text classification, entity recognition, entity extraction, and question answering\nProficient in Python and one of PySpark or Scala. Familiarity with python tools for data processing\nProficiency in multiple machine learning and AI techniques such as supervised, unsupervised, reinforcement learning, deep learning, and NLP\nProficiency in Python, R, or other programming languages for data analysis and AI/ML development\nProficiency in libraries such as Hugging Face and OpenAI API\nProven ability to develop and deploy data pipelines, machine learning models, or applications on cloud platforms (Azure, Databricks, AzureML)\nProven excellent communication, presentation, and interpersonal skills, with the ability to explain complex AI/ML concepts and results to technical and non-technical audiences\nProven solid analytical, problem-solving, and decision-making skills, with the ability to balance innovation and pragmatism\nProeven passion for learning and staying updated with the latest AI/ML trends and research",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'machine learning', 'artificial intelligence', 'r', 'continuous integration', 'data analysis', 'scala', 'scikit-learn', 'presentation skills', 'ci/cd', 'microsoft azure', 'docker', 'tensorflow', 'data science', 'ai techniques', 'devops', 'pytorch', 'keras', 'software engineering', 'aws']",2025-06-12 15:04:31
"Principal Engineer/Manager/Director, CAD tools & Methodology",Qualcomm,8 - 13 years,Not Disclosed,['Noida'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Hardware Engineer, you will plan, design, optimize, verify, and test electronic systems, bring-up yield, circuits, mechanical systems, Digital/Analog/RF/optical systems, equipment and packaging, test systems, FPGA, and/or DSP systems that launch cutting-edge, world class products. Qualcomm Hardware Engineers collaborate with cross-functional teams to develop solutions and meet performance requirements.\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 8+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 7+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 6+ years of Hardware Engineering or related work experience.\nPosition- Principal Engineer/Manager, CAD tools & Methodology\n\nLocationNoida\n\n:\nWe are looking for a senior leader to lead a 20+ CAD team in Noida.\nThe Noida CAD team delivers tools/flows/methodologies to enable Qualcomm to build its most complex SoCs in cutting edge process nodes.\nThe person will be responsible for:\nManaging all CAD functions in Noida- including front-end and RTL2GDS tools.\nDrive tools, flows, methodologies globally as part of world-wide CAD organization.\nDrive local EDA vendor eco-system.\nBe the interface to Qualcomm execution teams in Noida.\nExperience:\nAtleast 15 years experience in development of tools/flows/methodologies in either RTL, DV, synthesis, PnR or Signoff.\nShould have a proven record of driving new innovative tool/flow/methodology solutions.\nShould have managed a medium sized team.\nEducational Qualification:\nPreferred- Masters in VLSI or Computer Science\nMinimum- Bachelors in Electronics/Electrical Engineering/Computer Science",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['synthesis', 'cad', 'hardware engineering', 'rtl', 'pnr', 'catia v5', 'system testing', 'cad tools', 'electrical engineering', 'design engineering', 'autocad', 'vhdl', 'catia', 'verilog', 'electricals', 'rest assured', 'solid works', 'creo', 'pro-e', 'electronics engineering', 'digital transformation', 'gd']",2025-06-12 15:04:33
Full Stack Engineer- Manager,Axtria,5 - 10 years,20-35 Lacs P.A.,"['Noida', 'Pune', 'Bengaluru']","Job description:\nAxtria: -Axtria is a global provider of award-winning cloud software and data analytics to the life sciences industry. Axtria enables life sciences organizations to transform the product commercialization journey and deliver much-improved healthcare outcomes for patients worldwide. We are acutely aware that our work impacts millions of people and are incredibly passionate about the improvement we can bring to patients lives.\nOur focus is on delivering solutions that help pharmaceutical, medical device, and diagnostics companies complete the journey from data to insights to action and get superior returns from their investments. As a participant of the United Nations Global Compact, Axtria is committed to aligning strategies and operations with universal principles on human rights, labor, environment, and anti-corruption and taking actions that advance societal goals.\nOur people are our core strength, and they make us proud of our work; this has helped us grow exponentially and make tremendous strides toward developing great products. It is for the grit, thinking of an entrepreneur, and a family-like environment where each member is valued and treasured that we are growing rapidly.\nFor more information, visit www.axtria.com.\n\nJob Title: - Full Stack Experts ( Open across levels Senior Associate to Associate Director)\nJob Location: -Gurgaon/Bangalore/Pune/Hyderabad\n\nJob Responsibilities: -\nBe an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on UI\nSoftware development experience in REACT, JavaScript/TypeScript, python is needed as frontend and backend for UI based applications\nBe a part of large delivery teams working on advanced projects when expert assistance is required.\nDeliver advanced Data Science capabilities to businesses in a meaningful manner through successful proof-of-concept solutions, and later smoothly transition the proof-of-concept into production.\nCreate Technical documents, develop, test, and deploy data analytics processes using REACT, Python, SQL on Azure/AWS platforms\nCan interact with client on GenAI related capabilities and use cases\n\nQualification: -\n- B-Tech or BE in Computer Science / Computer Applications from Tier 1-2 college\nOR\n- Masters degree in Machine Learning / Statistics / Econometrics, or related discipline from Tier 1-2 college\n\nMust have Skills: -\nRequire 3-15 years of experience to develop, test, and deploy React, JavaScript, Python based applications on Azure/AWS platforms\nMust have experience in building production grade application with frontend & backend technologies like HTML, JavaScript, REACT, Python etc.\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.\nBasic knowledge on concepts of Generative AI / LLMs / GPT will be beneficial\nSkills that give you an edge: -\nStrong analytical skills to solve and model complex business requirements are a plus. With life sciences or pharma background.\nWe will provide (Employee Value Proposition)\nOffer an inclusive environment that encourages diverse perspectives and ideas\nDeliver challenging and unique opportunities to contribute to the success of a transforming organization\nOpportunity to work on technical challenges that may impact across geographies\nVast opportunities for self-development: online Axtria Institute, knowledge sharing opportunities globally, learning opportunities through external certifications\nSponsored Tech Talks & Hackathons\nPossibility to relocate to any Axtria office for short and long-term projects\nBenefit package:\n-Health benefits\n-Retirement benefits\n-Paid time off\n-Flexible Benefits\n-Hybrid /FT Office/Remote\nAxtria is an equal-opportunity employer that values diversity and inclusiveness in the workplace.\nWho we are\nAxtria 14 years journey\nAxtria, Great Place to Work\nLife at Axtria\nAxtria Diversity",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Javascript', 'React.Js', 'Python', 'AWS', 'SQL']",2025-06-12 15:04:35
MDM Data Scientist,Amgen Inc,4 - 9 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking an accomplished and visionary Data Scientist/ GenAIdeveloper to join Amgens Enterprise Data Management team. As part of MDM team, you will be responsible for designing, developing, and deploying Generative AI and ML models to power data-driven decisions across business domains. This role is ideal for an AI practitioner who thrives in a collaborative environment and brings a strategic mindset to applying advanced AI techniques to solve real-world problems.\nTo succeed in this role, the candidate must have strong AI/ML, Data Science, GenAI experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have AI/ML, data science and GenAI experience on technologies like (PySpark/PyTorch, TensorFlow, LLM, Autogen, Hugging FaceVectorDB,Embeddings, RAGsetc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nDevelop enterprise-level GenAI applications using LLM frameworks such as Langchain, Autogen, and Hugging Face.\nDesign and develop intelligent pipelines using PySpark, TensorFlow, and PyTorch within Databricks and AWS environments.\nImplement embedding models andmanage VectorStores for retrieval-augmented generation (RAG) solutions.\nIntegrate and leverage MDM platforms like Informatica and Reltio to supply high-quality structured data to ML systems.\nUtilize SQL and Python for data engineering, data wrangling, and pipeline automation.\nBuild scalable APIs and services to serve GenAI models in production.\nLead cross-functional collaboration with data scientists, engineers, and product teams to scope, design, and deploy AI-powered systems.\nEnsure model governance, version control, and auditability aligned with regulatory and compliance expectations.\nBasic Qualifications and Experience:\nMasters degree with 4 - 6 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 6 - 9 years of experience in Business, Engineering, IT or related field OR\nDiploma with 10 - 12 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\n6+ years of experience working in AI/ML or Data Science roles, including designing and implementing GenAI solutions.\nExtensive hands-on experience with LLM frameworks and tools such as Langchain, Autogen, Hugging Face, OpenAI APIs, and embedding models.\nStrong programming background with Python, PySpark, and experience in building scalable solutions using TensorFlow, PyTorch, and SK-Learn.\nProven track record of building and deploying AI/ML applications in cloud environments such as AWS.\nExpertise in developing APIs, automation pipelines, and serving GenAI models using frameworks like Django, FastAPI, and DataBricks.\nSolid experience integrating and managing MDM tools (Informatica/Reltio) and applying data governance best practices.\nGuide the team on development activities and lead the solution discussions\nMust have core technical capabilities in GenAI, Data Science space\nGood-to-Have Skills:\nPrior experience in Data Modeling, ETL development, and data profiling to support AI/ML workflows.\nWorking knowledge of Life Sciences or Pharma industry standards and regulatory considerations.\nProficiency in tools like JIRA and Confluence for Agile delivery and project collaboration.\nFamiliarity with MongoDB, VectorStores, and modern architecture principles for scalable GenAI applications.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nData Science and ML Certification\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'GenAI', 'Langchain', 'PySpark', 'Hugging Face', 'OpenAI API', 'Autogen', 'PyTorch', 'Django', 'MDM', 'FastAPI', 'Data Modeling', 'ETL', 'TensorFlow', 'Python']",2025-06-12 15:04:38
Full Stack Engineer,Axtria,4 - 9 years,Not Disclosed,"['Noida', 'Hyderabad', 'Bengaluru']","Axtria: -Axtria is a global provider of award-winning cloud software and data analytics to the life sciences industry. Axtria enables life sciences organizations to transform the product commercialization journey and deliver much-improved healthcare outcomes for patients worldwide. We are acutely aware that our work impacts millions of people and are incredibly passionate about the improvement we can bring to patients lives.\nOur focus is on delivering solutions that help pharmaceutical, medical device, and diagnostics companies complete the journey from data to insights to action and get superior returns from their investments. As a participant of the United Nations Global Compact, Axtria is committed to aligning strategies and operations with universal principles on human rights, labor, environment, and anti-corruption and taking actions that advance societal goals.\nOur people are our core strength, and they make us proud of our work; this has helped us grow exponentially and make tremendous strides toward developing great products. It is for the grit, thinking of an entrepreneur, and a family-like environment where each member is valued and treasured that we are growing rapidly.\nFor more information, visit www.axtria.com.\nJob Title: - Full Stack Experts ( Open across levels – Senior Associate to Associate Director)\n\nJob Location: -Gurgaon/Bangalore/Pune/Hyderabad\nJob Responsibilities: -\nBe an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on UI\nSoftware development experience in REACT, JavaScript/TypeScript, python is needed as frontend and backend for UI based applications\nBe a part of large delivery teams working on advanced projects when expert assistance is required.\nDeliver advanced Data Science capabilities to businesses in a meaningful manner through successful proof-of-concept solutions, and later smoothly transition the proof-of-concept into production.\nCreate Technical documents, develop, test, and deploy data analytics processes using REACT, Python, SQL on Azure/AWS platforms\nCan interact with client on GenAI related capabilities and use cases\nQualification: -\n- B-Tech or BE in Computer Science / Computer Applications from Tier 1-2 college\nOR\n- Master’s degree in Machine Learning / Statistics / Econometrics, or related discipline from Tier 1-2 college\nMust have Skills: -\nRequire 3-15 years of experience to develop, test, and deploy React, JavaScript, Python based applications on Azure/AWS platforms\nMust have experience in building production grade application with frontend & backend technologies like HTML, JavaScript, REACT, Python etc.\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.\nBasic knowledge on concepts of Generative AI / LLMs / GPT will be beneficial\nSkills that give you an edge: -\nStrong analytical skills to solve and model complex business requirements are a plus. With life sciences or pharma background.\nWe will provide– (Employee Value Proposition)\nOffer an inclusive environment that encourages diverse perspectives and ideas\nDeliver challenging and unique opportunities to contribute to the success of a transforming organization\nOpportunity to work on technical challenges that may impact across geographies\nVast opportunities for self-development: online Axtria Institute, knowledge sharing opportunities globally, learning opportunities through external certifications\nSponsored Tech Talks & Hackathons\nPossibility to relocate to any Axtria office for short and long-term projects\nBenefit package:\n-Health benefits\n-Retirement benefits\n-Paid time off\n-Flexible Benefits\n-Hybrid /FT Office/Remote\nAxtria is an equal-opportunity employer that values diversity and inclusiveness in the workplace.\nWho we are\nAxtria 14 years journey\nAxtria, Great Place to Work\nLife at Axtria\nAxtria Diversity",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['React.Js', 'Python', 'AWS']",2025-06-12 15:04:40
Principal Engineer Software Developer,Indian / Global Engineering & Manufactur...,9 - 14 years,Not Disclosed,['Manesar'],"Key Skills: Embedded Software Development, C++, Microcontroller, Embedded C, Automotive embedded, LIN, CAN, CANalyzer\nRoles and Responsibilities:\nResponsible for Automotive Embedded Technology Product like ECU, TCU, Controllers, On/Off board charger based Embedded Electronics like Analog, Digital, MCU, Sensors, Power Supplies and Power Electronics\nSoftware requirement understanding and product architecture.\nSoftware flaw less launch of product/Product life cycle management, align with group Goals.\nDevelops new function/module, contribute for new process development/tailoring existing process.\nResponsible for software development, design documents and test setup.\nAlign hardware test activities to meet Product Development Process schedules using best practices and tools.\nTesting automation and maintaining manual documentation regression suites for Part components for Software releases.\nParticipating in project team discussions on product design and presenting test results to development teams and management.\nContributing in a meaningful way to team goals and initiatives to increase quality and efficiency of software test processes.\nSkills Required:\nTechnical/Functional Competencies Embedded Software/Hardware:\nHands-on experience in application software and embedded software in automotive application.\nExperience in digital controls and interfaces like PID/ PWM timers/LCD/EEPROM/interface of sensors for volt, current, and temperature etc.\nwork experience in Embedded C/C++.\nMust have worked on 8 bit, 16bit, 32bit, Renesas, Cypress, Fujitsu, ARM M0/M1/M2/M3/M4 microcontrollers.\nPreferable if candidate has worked on Renesas microcontrollers.\nWork experience on PWM Timer and controls, LCD interface, UART, ADC, DAC, PGA, DMA, GPIO, Interrupts handling, Exception handling, WatchDog Timer, Software Timers/UART etc.\nMust have hands-on experience in communication protocols like LIN, CAN, I2C, SPI, UART, RS232, RS485.\nFirmware debugging experience with JTAG, Single wire debug interface, RS232, UART.\nUnderstanding of IVN Network, UDS, KWP2000, IO vehicle test.\nHands on Toll like CANalyser, CANoe, CAPL scripting etc.\nShould have ability to create test cases for Embedded C code and design documents.\nTechnical/Functional Competencies Hardware:\nExperience in writing/Software debugging, Software Compliance Standard\nInterpret test cases as per the OEM test case with relevant test standards.\nGood organizational and communication skills.\nAbility to work effectively with cross-functional teams and suppliers.\nA knowledge of Controller Area Network CAN and LIN communications protocols\nEducation: Bachelor's or Master's degree in Computer Science, Engineering, or a related technical field",Industry Type: Electronics Manufacturing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Embedded C', 'C++', 'Microcontroller', 'Automotive embedded', 'Embedded Software Development', 'LIN', 'CAN', 'CANalyzer']",2025-06-12 15:04:42
WLAN Test- Staff/ Sr Staff/ Principal Engineer,Qualcomm,6 - 11 years,Not Disclosed,['Chennai'],"Job Area: Engineering Group, Engineering Group > Systems Test Engineering\n\nGeneral Summary:\n\nYou will join the System Test team that is responsible for defining and implementing the overall testing strategy for WiFi & Networking access points. This involves development of test plans, tools and automation framework for validating and qualifying the WiFi routers. You will work closely with systems team, development, and architecture team to understand the features and define test plans and solutions needed to deliver production grade software/firmware to the end customer.\n\nYou will be collaborating with a variety of internal teams in Qualcomm covering multiple engineering disciplines including software, systems, and hardware. The successful applicant should have a diverse skill set and a strong background in continuous testing and automation strategies\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 6+ years of Systems Test Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 5+ years of Systems Test Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 4+ years of Systems Test Engineering or related work experience.\n\nResponsibilities:\n\nMust be responsible for analyzing new feature and develop/create new test plans and adding new test cases\n\nManage Infrastructure, develop test topologies and prepare use cases for validation\n\nWork with cross functional teams for supporting end-to-end release\n\nDirects small team of engineers on gathering, integrating, and interpreting information from a variety of sources in-order to troubleshoot issues and find solutions.\n\nDevelop the right skill and train the team. Serves as a mentor to Engineers and Senior Engineers and teaches them about complex features, systems, testing strategies, and automation.\n\nConduct log analysis with team members to identify where an issue has occurred and makes recommendations for how to address the issue.\n\nNetworks with colleagues within own domain to gain insight, ideas, and connections. Shares information with peers and junior engineers.\n\nCollaborates with functional and lab teams, IO teams, network operators, field teams, and product management teams to ensure that the testing plan is accurate for addressing feature issues.\n\nMinimum Qualifications\n\nBachelor's or Masters degree in Engineering, Information Systems, Computer Science, Electronics & communications or related field.\n\n15+ years in WiFi/Networking Test, automation or Software Engineering\n\nExperience with Programming Language such as Python, Shell Script (optional)\n\nRequired Skills and Aptitudes\n\nShould possess strong knowledge in WLAN/networking and manual testing of networking products\n\nMust have good experience in testing of layer-2 to layer 7 protocols\n\nPossess high Debugging capability\n\nStrong problem-solving skills\n\nAbility to prioritize and execute tasks across multiple projects with tight deadlines and aggressive goals.\n\nExperience in scripting languages like python (optional)\n\nExperience in shell scripting and windows batch commands (optioinal)\n\nExcellent English communication (written and verbal) and interpersonal skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['software testing', 'manual testing', 'networking', 'networking products', 'layer 2', 'python', 'regression testing', 'automation testing', 'functional testing', 'test engineering', 'wlan testing', 'linux', 'test planning', 'debugging', 'software engineering', 'shell scripting']",2025-06-12 15:04:45
Engineer/Senior Engineer/Lead Engineer/Principal Engineer - Wastewater,Aecom,2 - 7 years,Not Disclosed,['Bengaluru'],"Engineer/Senior Engineer/Lead Engineer/Principal Engineer - Wastewater Modelling.\nProvide technical mentorship to junior modellers, review their work, and deliver training sessions on modelling concepts.\nCapable for leading the project, coordinating with Lead Offices, mentoring juniors and ensuring quality checks\nComplete design/hydraulic modellingactivities as per specified standards for different regional projects.\nAssist in establishing processes for working with ANZ/US/UK&I/ME offices.\nIndependently manage project tasks, ensuring on-time delivery while adhering to AECOM's quality standards.\nLead project execution, liaise with the Lead Office, and effectively manage project resources.\nConduct comprehensive QA/QC reviews on models to ensure compliance with client standards and industry best practices.\nIndependently troubleshoot and enhance hydraulic models for improved accuracy and performance.\nCollaborate with multidisciplinary teams, including engineers, GIS specialists, and project managers, to drive seamless project execution.\nSupport bid preparation and program management, contributing to business growth and project acquisition.\n\n\nQualifications\nMaster of Engineering degree (or equivalent education) in an appropriate engineering discipline from an accredited college or university.\nChartered Engineer (CEng), or Professional Engineer (PE) license or equivalent in the relevant field from any global organization (e.g., Institution of Civil Engineers, UK) oractively working towards achieving chartership.\nHaving 2 to 12 years of working experience on projects from UK & Ireland, US, Middle East, and ANZ regions.\nWork on model build and calibration, flood mitigation schemes, CSO spill reduction program etc. using InfoWorks ICM for wastewater modelling projects across UK & Ireland, US, Middle East, and ANZ regions. Proficiency in 2D and water quality modelling is an added advantage.\nProficiency on hydraulic modelling & design of foul sewer, storm sewer, urban drainage networks using Infoworks ICM, SewerGEMS, PCSWMM etc.\nHighly motivated, hardworking, interpersonal, and enthusiastic team player that is willing to learn and adapt to change.\nUS/UK/Canada Experience will be added advantage\nGood communication skills, and ability to work well independently at times.\nAble to see the bigger picture and take a birds-eye view of projects\nConfident, with the ability to work either independently or as part of a team.\nAbility to work to deadlines and under pressure.\nAccountability for assigned work.\nAccuracy & precision of work.\nWillingness to learn and develop.\nExcellent written and verbal communication skills\nStrong problem-solving skills\nEnthusiastic and Self-motivated.\nWork well within a multidisciplinary team",Industry Type: Building Material (Cement),Department: Construction & Site Engineering,"Employment Type: Full Time, Permanent","['hydraulic modelling', 'wastewater modelling', 'project management', 'program management', 'water quality modelling']",2025-06-12 15:04:47
"Machine Learning, Technical Lead - NLP / LLM",Avalara Technologies,6 - 10 years,Not Disclosed,[],"What You'll Do\nWe are looking for experienced Machine Learning Engineers with a background in software development and a deep enthusiasm for solving complex problems. You will lead a dynamic team dedicated to designing and implementing a large language model framework to power diverse applications across Avalara.\nYour responsibilities will span the entire development lifecycle, including conceptualization, prototyping and delivery of the LLM platform features. You will build core agent infrastructureA2A orchestration and MCP-driven tool discoveryso teams can launch secure, scalable agent workflows. You will be reporting to Senior Manager, Machine Learning",,,,"['Machine Learning', 'NLP', 'Docker', 'Terraform', 'MLFlow', 'Prometheus', 'LLM', 'AWS', 'Grafana', 'GitLab', 'Kubernetes']",2025-06-12 15:04:50
WiFi Connectivity - Sr Staff/ Principal Engineer,Qualcomm,15 - 20 years,Not Disclosed,['Chennai'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\nMinimum 15 yrs in SW Engineering life cycle.\nMinimum 10 yrs in SW engineering roles that covers SW development and test.\nStrong technology focus and experience in networking technology areas that include WLAN, Ethernet, Bridging and Routing.\nStrong understanding SW architecture and real time embedded system design with Linux Operating System\nExperience in detailed planning, reporting, defining and managing engineering / technology metrics\nDecisive and ability to quickly identify problems & make decisions to solve them.\nStrong interpersonal and communication skills\nOwnership of commercialization of the key Wi-Fi access point products\nTechnical product owner of the releases responsible of driving all technology areas to deliver world class AP and Wi-Fi Routers\nPrimary technology and commercialization interface with product management and engineering teams\nEnd to End ownership to drive programs from start to the post launch in the field.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 6+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 5+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Engineering or related work experience.\n\n3+ years of work experience with Programming Language such as C, C++, Java, Python, etc.\nExperience in managing programs & meets required program specifications with required quality, content & cost\nGlobal program management experience across geos\nWorking with cross geo teams in US and China\nCustomer interactions and Product Marketing interfacing experience",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'linux', 'software engineering', 'sw', 'embedded system design', 'switching', 'eigrp', 'networking', 'routing', 'java', 'rtos', 'computer science', 'embedded systems', 'lan', 'pcb designing', 'arm', 'tcp', 'matlab', 'python', 'c', 'software testing', 'ospf', 'ethernet', 'embedded c', 'microsoft windows', 'ccna']",2025-06-12 15:04:52
Data Scientist IV - Python / LLM,Sadup Soft,6 - 8 years,Not Disclosed,['Hyderabad'],"Must have skills :\n\n- 6+ Years of Experience.\n\n- Statics, SQL, Big query, LLM, AI, Python\n\n- Work experience in the payments, ecommerce, or financial services industry is a plus .\n\nResponsibilities :\n\n- At least 6 years of experience analyzing large, multi-dimensional data sets and synthesizing insights into actionable solutions\n\n- Bachelor's/Master's degree in a quantitative field (such as Analytics, Statistics, Mathematics, Economics or Engineering) or equivalent field experience\n\n- Advanced SQL experience, preferable with Big Query analytics (Google Cloud) on Jupyter Notebooks and experience analyzing very large, complex, multi-dimensional data sets.\n\n- Understanding of statistics (e.g hypothesis testing, statistical inference, regression) and experience designing and evaluating complex experiments-\n\n- Ability to solve problems analytically and create actionable recommendations\n\n- Advanced ability to use reporting tools like Tableau and/or Excel to share analysis\n\n- Strong written and verbal communication skills with the ability to translate complex problems into simpler terms, expertise in stitching together findings to convey coherent insights and effectively influence both peers and senior leadership\n\n- Prior work experience in a product analytics space would be highly valued\n\n- A passion for problem-solving and comfort with ambiguity\n\n- Work experience in the payments, ecommerce, or financial services industry is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'Data Science', 'BigQuery', 'Data Management', 'Jupyter', 'LLM', 'Statistics']",2025-06-12 15:04:54
RTL Design - Sr Staff/ Principal Engineer,Qualcomm,15 - 20 years,Not Disclosed,['Chennai'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n15+ years of experience in SoC design\nShould have knowledge of AMBA protocols - AXI, AHB, APB, SoC clocking/reset/debug architecture and peripherals like USB, PCIE and SDCC.\nUnderstanding of Memory controller designs and microprocessors is an added advantage\nHands on experience in constraint development and timing closure\nWork closely with the SoC verification and validation teams for pre/post Silicon debug\nHands on experience in Low power SoC design is required\nExperience in Synthesis / Understanding of timing concepts for ASIC is required.\nHands on experience in Multi Clock designs, Asynchronous interface is a must.\nExperience in using the tools in ASIC development such as Lint, CDC, Design compiler and Primetime is required\n\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 6+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 5+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 4+ years of Hardware Engineering or related work experience.\n12+ years of experience with a Bachelor's/ Masters degree in Electrical/ Electronics engineering",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['asynchronous', 'soc design', 'asic development', 'hardware engineering', 'lint', 'soc verification', 'usb', 'soc', 'amba', 'apex', 'salesforce', 'design compiler', 'apb', 'rtl design', 'axi', 'ahb', 'microprocessors', 'protocols', 'synthesis', 'asic', 'cdc', 'primetime', 'rtl', 'verilog', 'silicon', 'timing closure', 'pcie']",2025-06-12 15:04:57
Senior Data Analyst-Azure Data Factory,Lumen Technologies,8 - 12 years,Not Disclosed,['Bengaluru'],"Were looking for a Senior Data Analyst with a strong foundation in Azure-based data engineering and Machine Learning to design, develop, and optimize robust data pipelines, applications, and analytics infrastructure. This role demands deep technical expertise, cross-functional collaboration, and the ability to align data solutions with dynamic business needs.\nKey Responsibilities:\nData Pipeline Development:\nDesign and implement efficient data pipelines using Azure Databricks with PySpark to transform and process large datasets.\nOptimize data workflows for scalability, reliability, and performance.\nApplication Integration:\nCollaborate with cross-functional teams to develop APIs using the .NET Framework for Azure Web Application integration.\nEnsure smooth data exchange between applications and downstream systems.\nData Warehousing and Analytics:\nBuild and manage data warehousing solutions using Synapse Analytics and Azure Data Factory (ADF).\nDevelop and maintain reusable and scalable data models to support business intelligence needs.\nAutomation and Orchestration:\nUtilize Azure Logic Apps, Function Apps, and Azure DevOps to automate workflows and streamline deployments.\nImplement CI/CD pipelines for efficient code deployment and testing.\nInfrastructure Management:\nOversee Azure infrastructure management and maintenance, ensuring a secure and optimized environment.\nProvide support for performance tuning and capacity planning.\nBusiness Alignment:\nGain a deep understanding of AMO data sources and their business implications.\nWork closely with stakeholders to provide customized solutions aligning with business needs.\nBAU Support:\nMonitor and support data engineering workflows and application functionality in BAU mode.\nTroubleshoot and resolve production issues promptly to ensure business continuity.\nTechnical Expertise:\nProficiency in Microsoft SQL for complex data queries and database management.\nAdvanced knowledge of Azure Databricks and PySpark for data engineering and ETL processes.\nExperience with Azure Data Factory (ADF) for orchestrating data workflows.\nExpertise in Azure Synapse Analytics for data integration and analytics.\nProficiency in .NET Framework for API development and integration.\nCloud and DevOps Skills:\nStrong experience in Azure Infrastructure Management and optimization.\nHands-on knowledge of Azure Logic Apps, Function Apps, and Azure DevOps for CI/CD automation.\n""We are an equal opportunity employer committed to fair and ethical hiring practices. We do not charge any fees or accept any form of payment from candidates at any stage of the recruitment process. If anyone claims to offer employment opportunities in our company in exchange for money or any other benefit, please treat it as fraudulent and report it immediately.""\n#LI-BS1",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'orchestration', 'Infrastructure management', 'Machine learning', 'Business intelligence', 'Business continuity', 'Analytics', 'Downstream', 'Capacity planning']",2025-06-12 15:05:00
Sr. Principal Software Engineer,Morningstar,12 - 18 years,Not Disclosed,"['Thane', 'Navi Mumbai', 'Mumbai (All Areas)']","Position Title: Senior Principal Software Engineer\nThe Area: Morningstar Data for equities provides comprehensive coverage of global stock markets from as early as 1975. We are continuously broadening our coverage and creating new products to help clients prepare for regulatory changes and other industry shifts. Our data features proprietary statistics and is developed using stringent quality screens to ensure accuracy. From APIs to data feeds, our solutions are delivered quickly to help institutions meet a broad range of functions. The Role: In this role, you will collaborate with technology manager, scrum Master, functional experts, and developers to build technology solutions for Morningstar`s Equity applications. The team is looking for forward-thinking problem solvers who thrive in a fastpaced environment. The Lead is responsible in mentoring the team members, provide guidance and opportunities for the team members to expand their capabilities and skills. They will have to coordinate and work with the members in a global team. We are looking for an individual that can apply discipline, create solid software products.\n\nResponsibilities:\n• Design & develop web and enterprise solutions to be flexible, scalable & extensible.\n• Improve complex data flow, data structures and db design to move to next platform.\n• Enforce good agile practices like test driven development, Continuous Integration.\n• Hands-on development will be an integral part of the responsibilities.\n• Develop areas of continuous and automated deployment.\n• Introduce and follow good development practices, innovative frameworks and technology solutions that help business move faster.\n• Follow best practices like estimation, planning, reporting and improvement brought to processes in every day work.\n• Analyses and reviews system requirements. Uses requirement and other design documents to gain overall understanding of the functionality of the new or enhanced application.\n• Participate actively in the design, architecture and build phases, to aim at producing high quality deliverables.\n• Provide recommendations on product and development environment improvements.\n\nRequirements:\nThese are the most important skills, qualities, etc. that we’d like for this role.\n• Minimum 12 Years of experience\n• Bachelor of Science in Computer Science, Engineering, or equivalent.\n• At least 2+ years as a software Lead/Architect\n• Demonstrated familiarity with AI-powered assistants (e.g., GitHub Copilot, ChatGPT) for code generation, debugging, and/or other technical tasks.\n• Hands-on in Java 8, Adv Java, Spring Framework\n• Strong knowledge and hands-on on micro-services based architecture.\n• Very Strong knowledge of databases and hands on MS SQL/MySQL/PostgreSQL or NoSQL DB DynamoDB/MongoDB.\n• Experience with building REST based APIs.\n• Experience in analysis, design, coding and implementation of large-scale, n-tier Java based platforms.\n• Knowledge of any JavaScript framework like Vue, Angular JS (version >2)/ NodeJS etc.\n• Be aware of activity in the open source world. Contributing back to open source is a big plus.\n• Distributed computing, with experience in cloud computing (Amazon Web Services platform and associated technologies)\n• Experience on agile practices\n• Experience with modern development practices in areas of Product design, Requirement Analysis, Test Driven Development, Automation & Unit Testing, in a product development environment.\n• Excellent listening, written and verbal communication skills. Good to Have: • Machine Learning knowledge.\n• Exposure to Capital Market domain preferred (Indexes, Equity etc.) Morningstar is an equal opportunity employer",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java Fullstack', 'Java', 'Vue.Js', 'Java Spring Boot', 'Spring Boot', 'React.Js', 'AWS', 'Angular']",2025-06-12 15:05:02
"Applied Scientist, Amazon Autos",Amazon,3 - 8 years,Not Disclosed,['Gurugram'],"Interested in building something new? Join the Amazon Autos team on an exhilarating journey to redefine the vehicle shopping experience.\nThis is an opportunity to be part of the Amazons new business ventures. Our goal is to create innovative automotive discovery and shopping experiences on Amazon, providing customers with greater convenience and a wider selection.\nYoull work in a creative, fast-paced, and entrepreneurial environment at the center of Amazons innovation. As a key member, youll play a pivotal role in helping us achieve our mission. We are looking for a highly accomplished Applied Science professional drive our science strategy, foster a culture of data-driven decision-making, and drive impactful business outcomes through advanced state-of-the-art science methodologies.\nIf youre enthusiastic about innovating and delivering exceptional shopping experiences to customers, thrive on new challenges, and excel at solving complex problems using top-notch ML models, LLM and GenAI techniques, then youre the perfect candidate for this role. Strong business acumen and interpersonal skills are a must, as youll work closely with business owners to understand customer needs and design scalable solutions.\nJoin us on this exhilarating journey and be part of redefining the vehicle shopping experience.\n\n\nAs an Applied Scientist in Amazon Autos, you will:\n\nShape the roadmap and strategy for applying science to solve customer problems in the Amazon AutoStore domain.\nDrive big picture innovations with clear roadmaps for intermediate delivery.\nApply your skills in areas such as deep learning and reinforcement learning while building scalable solutions for business problems.\nProduce and deliver models that help build best-in-class customer experiences and build systems that allow us to deploy these models to production with low latency and high throughput.\nUtilize your Generative AI, time series and predictive modeling skills, and creative problem-solving skills to drive new projects from ideation to implementation.\nInterface with business customers, gathering requirements and delivering science solutions.\nCollaborate with cross-functional teams, including software engineers, data scientists, and product managers, to define project requirements, establish success metrics, and deliver high-quality solutions.\nEffectively communicate complicated machine learning concepts to multiple partners.\nResearch new and innovative machine learning approaches.\n\nA day in the life\nIn this role, you will be part of a multidisciplinary team working on one of Amazons newest business ventures. As a key member, you will collaborate closely with engineering, product, design, operations, and business development to bring innovative solutions to our customers.\nYour science expertise will be leveraged to research and deliver novel solutions to existing problems, explore emerging problem spaces, and create new knowledge. You will invent and apply state-of-the-art technologies, such as large language models, machine learning, natural language processing, and computer vision, to build next-generation solutions for Amazon.\nYoull publish papers, file patents, and work closely with engineers to bring your ideas to production.\n\nAbout the team\nThis is a critical role for Amazon Autos team with a vision to create innovative automotive discovery and shopping experiences on Amazon, providing customers better convenience and more selection. We re collaborating with other experienced teams at Amazon to define the future of how customers research and shop for cars online. 3+ years of building models for business application experience\nPhD, or Masters degree and 4+ years of CS, CE, ML or related field experience\nExperience in patents or publications at top-tier peer-reviewed conferences or journals\nExperience programming in Java, C++, Python or related language\nExperience in any of the following areas: algorithms and data structures, parsing, numerical optimization, data mining, parallel and distributed computing, high-performance computing Experience using Unix/Linux\nExperience in professional software development\nExperience building complex software systems, especially involving deep learning, machine learning and computer vision, that have been successfully delivered to customers",,,,"['Unix', 'Computer vision', 'C++', 'Linux', 'Machine learning', 'Data structures', 'Product design', 'Data mining', 'Automotive', 'Python']",2025-06-12 15:05:05
ML Data Associate-II,Amazon,2 - 7 years,Not Disclosed,['Chennai'],"AI is the most transformational technology of our time, capable of tackling some of humanity s most challenging problems. Amazon is investing in generative AI and the responsible development and deployment of large language models (LLMs) across all of our businesses. Come build the future of human-technology interaction with us.\n\nWe are looking for those candidates who just don t think out of the box, but make the box they are in Bigger . The future is now, do you want to be a part of it? Then read on!\n\n\nMaintain and follow strict confidentiality as customer privacy is our most important tenet\nWork with a range of different types of data including, but not limited to: text, speech, audio, image, and video\nDeliver high-quality labelled data, using guidelines provided to meet our KPIs and using in-house tools and software, as part of Amazons commitment to developing and deploying AI responsibly.\nDemonstrate proficiency in generating high quality human insight data across a range of modalities, inclusive of text, image video and audio.\nCapable of making sound judgments and logical decisions when faced with ambiguous or incomplete information while performing tasks.\nEye for detail and ability to pivot from one category of requirement to another instantaneously.\nDemonstrate support on daily operational deliverables for multiple task types assigned to you and the team\nAnalyze root causes, identify error patterns, and propose solutions to enhance the quality of labeling tasks and their outputs.\nResponsible for identifying day-to-day process and operational issues in Standard Operating Procedure, tools and suggest changes to unblock operations\nDemonstrate ownership in floor support to clarify internal queries during execution on need basis\n\nA day in the life\nWe are looking for a ML Data Associate (MLDA) to undertake the task of foundational labeling functions, such as dialogue evaluation on speech, text, audio, video data.\n\nYour ability to concentrate, multi-task and your high attention to detail helps you deliver high-quality work as well as maintaining strict confidentiality and follow all applicable Amazon policies for securing confidential information. You will be a part of a diverse team with the shared vision of improving customers lives with practical, useful generative AI innovations. An inner drive, individuality, and a creative mind are extremely beneficial.\nAn Associate s Degree or related work experience\nStrong business writing skills with ability to create reports, proposals, and professional correspondence\nAdvanced reading comprehension with ability to analyze complex business documents\nDeveloped analytical thinking and structured problem-solving capabilities\nStrong ability to interpret and implement detailed instructions across various projects\nProficient research skills with experience gathering and synthesizing information from multiple sources\nProven attention to detail in managing complex tasks and documents\nExperience managing stakeholder relationships across departments\nAdvanced proficiency in Microsoft Office Suite and common business applications.\nBachelor s degree in a relevant field May vary in other locations like India\nC1+ or equivalent fluency in English language\n2+ years of professional work experience with demonstrated task execution ability\nProven capacity to leverage open-source resources effectively for comprehensive research purposes\nAbility to adapt well to fast-paced environments with changing circumstances, direction, and strategy\n1+ years project coordination or management experience (for support functions teams)\nExperience managing stakeholder relationships across departments\nAdvanced proficiency in Microsoft Office Suite and common business applications.",,,,"['Business writing', 'Analytical', 'Project coordination', 'Associate II', 'Research', 'Open source', 'Business applications', 'MS Office', 'Operations', 'Data Associate']",2025-06-12 15:05:07
Deep Learning Research Engineer (Vision & Audio Focus),HIREXpert,4 - 7 years,15-20 Lacs P.A.,"['Bhubaneswar', 'Bengaluru']","Strong programming skills in Python, with experience in PyTorch or TensorFlow.\nHands-on experience with CNNs\nSolid knowledge of Vision Transformers, including recent architectures (e.g., Swin, DeiT).\nYOLO, SSD, Faster R-CNN, RetinaNet, DETR\n\nRequired Candidate profile\nExperience in video analysis and temporal modeling.\nStrong grasp of audio classification workflows and features.\nlarge-scale datasets and designing data pipelines.",Industry Type: Medical Services / Hospital (Diagnostics),Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Pytorch', 'Deep Learning', 'Deep Learning Frameworks', 'Research And Development', 'Machine Learning', 'Python']",2025-06-12 15:05:09
Data & Analytics Specialist,Hoffmann La Roche,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\n.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\nA healthier future drives us to innovate. Together, more than 100 000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. Our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our Diagnostics products. We empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-12 15:05:11
Data & Analytics Specialist,Roche Diagnostics,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\nAt Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we ve become one of the world s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\n.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-12 15:05:14
Data Engineer,Xenonstack,2 - 5 years,Not Disclosed,['Mohali( Phase 8B Mohali )'],"At XenonStack, We committed to become the Most Value Driven Cloud Native, Platform Engineering and Decision Driven Analytics Company. Our Consulting Services and Solutions towards the Neural Company and its Key Drivers.\nXenonStacks DataOps team is looking for a Data Engineer who will be responsible for employing techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field.\nYou should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. The ideal candidate should be highly skilled in all aspects of Python, Java/Scala, SQL and analytical skills.\nJob Responsibilities:\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with co-workers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analysed quickly by BI & AI Teams.\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant co-workers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nRequirements:\nTechnical Requirements\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch, Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores.\nProfessional Attributes:\nExcellent communication skills & Attention to detail.\nAnalytical mind and problem-solving Aptitude with Strong Organizational skills & Visual Thinking.\nBenefits:\nDiscover the benefits of joining our team:\nDynamic and purposeful work culture in a people-oriented organization contributing to multi-million-dollar projects with guaranteed job security.\nOpen, authentic, and transparent communication fostering a warm work environment.\nRegular constructive feedback and exposure to diverse technologies.\nRecognition and rewards for exceptional performance achievements.\nAccess to certification courses & Skill Sessions to develop continually and refine your skills.\nAdditional allowances for team members assigned to specific projects.\nSpecial skill allowances to acknowledge and compensate for unique expertise.\nComprehensive medical insurance policy for your health and well-being.\nTo Learn more about the company -\nWebsite - http://www.xenonstack.com/",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Spark', 'ETL', 'Python', 'SQL', 'Java', 'Data Processing', 'Machine Learning']",2025-06-12 15:05:16
"Data Engineer, AVP",NatWest Markets,16 - 18 years,Not Disclosed,['Gurugram'],"Join us as a Data Engineer\nWe re looking for someone to build effortless, digital first customer experiences to help simplify our organisation and keep our data safe and secure\nDay-to-day, you ll develop innovative, data-driven solutions through data pipelines, modelling and ETL design while inspiring to be commercially successful through insights\nIf you re ready for a new challenge, and want to bring a competitive edge to your career profile by delivering streaming data ingestions, this could be the role for you\nWere offering this role at assistant vice president level\nWhat you ll do\nYour daily responsibilities will include you developing a comprehensive knowledge of our data structures and metrics, advocating for change when needed for product development. You ll also provide transformation solutions and carry out complex data extractions.\nWe ll expect you to develop a clear understanding of data platform cost levels to build cost-effective and strategic solutions. You ll also source new data by using the most appropriate tooling before integrating it into the overall solution to deliver it to our customers.\nYou ll also be responsible for:\nDriving customer value by understanding complex business problems and requirements to correctly apply the most appropriate and reusable tools to build data solutions\nParticipating in the data engineering community to deliver opportunities to support our strategic direction\nCarrying out complex data engineering tasks to build a scalable data architecture and the transformation of data to make it usable to analysts and data scientists\nBuilding advanced automation of data engineering pipelines through the removal of manual stages\nLeading on the planning and design of complex products and providing guidance to colleagues and the wider team when required\nThe skills you ll need\nTo be successful in this role, you ll have an understanding of data usage and dependencies with wider teams and the end customer. You ll also have experience of extracting value and features from large scale data.\nWe ll expect you to have experience of ETL technical design, data quality testing, cleansing and monitoring, data sourcing, exploration and analysis, and data warehousing and data modelling capabilities.\nYou ll also need:\nExperience of using programming languages alongside knowledge of data and software engineering fundamentals\nGood knowledge of modern code development practices\nGreat communication skills with the ability to proactively engage with a range of stakeholders\nHours\n45\nJob Posting Closing Date:\n16/06/2025",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Usage', 'Technical design', 'Programming', 'Data structures', 'Data quality', 'Assistant Vice President', 'Data warehousing', 'Monitoring', 'Data architecture']",2025-06-12 15:05:18
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nAs part of the cybersecurity organization, In this vital role you will be responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The role sits at the intersection of data infrastructure and business insight delivery, requiring the Data Engineer to design and build robust data pipelines while also translating data into meaningful visualizations for stakeholders across the organization. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nBuild data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nDevelop and maintain interactive dashboards and reports using tools like Tableau, ensuring data accuracy and usability\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\n\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, GitLab, LucidChart, etc.\nHands-on experience with data visualization and dashboarding toolsTableau, Power BI, or similar is a plus\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\n\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\n\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\n\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data engineering', 'data analysis', 'data modeling', 'analysis tools', 'data warehousing', 'troubleshooting', 'data architecture', 'data integration', 'etl process']",2025-06-12 15:05:20
AI Python Data Science Engineer,Probeseven,4 - 5 years,Not Disclosed,['Coimbatore'],"AI Python Data Science Engineer\nHot Openings\nAs a data science and analytics engineer, you will be involved in developing computer visions and data algorithms. Artificial intelligence development and deep machine learning implementations will be part of your development and deployment to the cloud.\n\nExperience for senior positions: 4 - 5+ years\nExperience for junior positions: 2 - 3 years\n\nRequired Tech Skills\nExperience in Python (must have) and/or R language.\nExperience with computer vision algorithms and data science.\nExperience in deep machine learning models.\nWell-versed in data visualization techniques.\nTroubleshoot and resolve code issues.\nCollaborate with data engineers to design and integrate the data sources.\nExperience in handling multiple priorities with Agile development.\nExperience with Git and working in a collaborative and distributive team environment.\nRequired Soft Skills\nExcellent listening, verbal, and written communication skills.\nStrong interpersonal & customer relationship skills.\nStrong analytical, problem solving, and decision-making skills.\nDocumentation skills.\nApply now\nHot Openings\nPHP + Node.js Developers\nFull Time\nTech Development\nExperience 4 - 6+ years",Industry Type: Film / Music / Entertainment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'GIT', 'data science', 'Analytical', 'Artificial Intelligence', 'Machine learning', 'PHP', 'Customer relationship', 'Analytics', 'Python']",2025-06-12 15:05:23
Associate Specialist Data Science,Merck Sharp & Dohme (MSD),2 - 7 years,Not Disclosed,['Pune'],"Primary Responsibilities\nSupport in establishing frameworks to standardize, productize and scale existing and new capabilities / analytical solutions\nImplement the vision, roadmap, and best practices for the Data Science Center of Excellence ( CoE ) to align with business goals\nSupport establishing governance frameworks to measure the value of products, standardize data science methodologies, coding practices, and project workflows\nWork with senior CoE members in development and maintenance of best practices for model and algorithm development and design, deployment, and monitoring across the enterprise functions\nCollaborate with product team on product development incorporating Agile framework and latest industry best practices and norms\nSupport in development of MLOps and ModelOps frameworks to streamline the development-to-deployment product pipeline\nDrive innovation by identifying, evaluating, and implementing cutting-edge data science methodologies based on latest published literature\n\nQualifications\nEducation & Work Experience Requiremen ts:\nMaster s degree (relevant field like Economics, Statistics, Mathematics, Operational Research) with 2+ years work experience.\nBachelor s degree (in Engineering or related field, such as Computer Science, Data Science, Statistics, Business, etc.) with at least 3 + years relevant experience\nPrior experience in research publications in reputed journal is a plus\nSkillset:\nCandidates must have -\nStrong programming skills in languages such as Python or R, and SQL with experience in data manipulation and analysis libraries (e.g., pandas, NumPy, scikit-learn, stats models)\nExperience with data science principles, machine learning (supervised and unsupervised) and GenAI algorithms, test-control analysis, propensity score matching etc.\nExposure to product roadmaps, Agile methodologies and backlog management, ensuring iterative and incremental product improvements\nStrong problem solving, business analysis and quantitative skills\nAbility to effectively communicate proposals to key stakeholders\nCandidates are desired but not mandatory to have -\nExperience and familiarity with underlying concepts such as Patient analytics, MMx etc.\nUnderstanding of Pharma commercial landscape will be a plus\nExperience working with healthcare, financial, or enterprise SaaS products\n  Search Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nNot Applicable\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness Intelligence (BI), Database Design, Data Engineering, Data Modeling, Data Science, Data Visualization, Machine Learning, Software Development, Stakeholder Relationship Management, Waterfall Model",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Relationship management', 'Business analysis', 'Coding', 'Pharma', 'Analytical', 'Healthcare', 'Business intelligence', 'Analytics', 'Monitoring', 'SQL']",2025-06-12 15:05:25
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nAs part of the cybersecurity organization, the Data Engineer is responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nCreate data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, Gitlab, LucidChart,etc.\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data engineering', 'data security', 'Agile', 'cloud data platforms', 'Databricks', 'data governance frameworks', 'ETL', 'AWS', 'SQL', 'Python']",2025-06-12 15:05:28
Data Engineering Specialist,Sanofi,5 - 10 years,Not Disclosed,['Hyderabad'],"We are seeking an experienced Data Engineering Specialist interested in challenging the status quo to ensure the seamless creation and operation of the data pipelines that are needed by Sanofi s advanced analytic, AI and ML initiatives for the betterment of our global patients and customers.\nSanofi has recently embarked into a vast and ambitious digital transformation program. A cornerstone of this roadmap is the acceleration of its data transformation and of the adoption of artificial intelligence (AI) and machine learning (ML) solutions, to accelerate R&D, manufacturing and commercial performance and bring better drugs and vaccines to patients faster, to improve health and save lives\nMain Responsibilities:\nEstablish technical designs to meet Sanofi requirements aligned with the architectural and Data standards\nOwnership of the entire back end of the application, including the design, implementation, test, and troubleshooting of the core application logic, databases, data ingestion and transformation, data processing and orchestration of pipelines, APIs, CI/CD integration and other processes\nFine-tune and optimize queries using Snowflake platform and database techniques\nOptimize ETL/data pipelines to balance performance, functionality, and other operational requirements.\nAssess and resolve data pipeline issues to ensure performance and timeliness of execution\nAssist with technical solution discovery to ensure technical feasibility.\nAssist in setting up and managing CI/CD pipelines and development of automated tests\nDeveloping and managing microservices using python\nConduct peer reviews for quality, consistency, and rigor for production level solution\nDesign application architecture for efficient concurrent user handling, ensuring optimal performance during high usage periods\nOwn all areas of the product lifecycle: design, development, test, deployment, operation, and support\nQualifications:\n5+ years of relevant experience developing backend, integration, data pipelining, and infrastructure\nExpertise in database optimization and performance improvement\nExpertise in Python, PySpark, and Snowpark\nExperience data warehousing and object-relational database (Snowflake and PostgreSQL) and writing efficient SQL queries\nExperience in cloud-based data platforms (Snowflake, AWS)\nProficiency in developing robust, reliable APIs using Python and FastAPI Framework\nExpert in ELT and ETL & experience working with large data sets and performance and query optimization. IICS is a plus\nUnderstanding of data structures and algorithms\nUnderstanding of DBT is a plus\nExperience in modern testing framework (SonarQube, K6 is a plus)\nStrong collaboration skills, willingness to work with others to ensure seamless integration of the server-side and client-side\nKnowledge of DevOps best practices and associated tools is a plus, especially in the setup, configuration, maintenance, and troubleshooting of associated tools:\nContainers and containerization technologies (Kubernetes, Argo, Red Hat OpenShift)\nInfrastructure as code (Terraform)\nMonitoring and Logging (CloudWatch, Grafana)\nCI/CD Pipelines (JFrog Artifactory)\nScripting and automation (Python, GitHub, Github actions)\nExperience with JIRA & Confluence\nWorkflow orchestration (Airflow)\nMessage brokers (RabbitMQ)\nEducation: bachelors degree in computer science, engineering, or similar quantitative field of study\nWhy choose us\nBring the miracles of science to life alongside a supportive, future-focused team.\nDiscover endless opportunities to grow your talent and drive your career, whether it s through a promotion or lateral move, at home or internationally.\nEnjoy a thoughtful, we'll-crafted rewards package that recognizes your contribution and amplifies your impact.\nTake good care of yourself and your family, with a wide range of health and we'llbeing benefits including high-quality healthcare, prevention and we'llness programs and at least 14 weeks gender-neutral parental leave.\nOpportunity to work in an international environment, collaborating with diverse business teams and vendors, working in a dynamic team, and fully empowe'red to propose and implement innovative ideas.",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'github', 'Postgresql', 'Data structures', 'Healthcare', 'Troubleshooting', 'JIRA', 'Monitoring', 'Python']",2025-06-12 15:05:30
Data Engineer,Atyeti,2 - 4 years,Not Disclosed,['Pune'],"Role & responsibilities\n\nDevelop and Maintain Data Pipelines: Design, develop, and manage scalable ETL pipelines to process large datasets using PySpark, Databricks, and other big data technologies.\nData Integration and Transformation: Work with various structured and unstructured data sources to build efficient data workflows and integrate them into a central data warehouse.\nCollaborate with Data Scientists & Analysts: Work closely with the data science and business intelligence teams to ensure the right data is available for advanced analytics, machine learning, and reporting.",,,,"['Azure Synapse', 'Pyspark', 'ETL', 'Python']",2025-06-12 15:05:33
Enterprise Data Operations Manager,Pepsico,12 - 17 years,Not Disclosed,['Hyderabad'],"Overview\n\nDeputy Director - Data Engineering\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCos global business scale to enable business insights, advanced analytics, and new product development. PepsiCos Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\nIncrease awareness about available data and democratize access to it across the company.\nAs a data engineering lead, you will be the key technical expert overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be empowered to create & lead a strong team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\nResponsibilities\n\nData engineering lead role for D&Ai data modernization (MDIP)\n\nIdeally Candidate must be flexible to work an alternative schedule either on tradition work week from Monday to Friday; or Tuesday to Saturday or Sunday to Thursday depending upon coverage requirements of the job. The candidate can work with immediate supervisor to change the work schedule on rotational basis depending on the product and project requirements.\nResponsibilities\nManage a team of data engineers and data analysts by delegating project responsibilities and managing their flow of work as well as empowering them to realize their full potential.\nDesign, structure and store data into unified data models and link them together to make the data reusable for downstream products.\nManage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\nCreate reusable accelerators and solutions to migrate data from legacy data warehouse platforms such as Teradata to Azure Databricks and Azure SQL.\nEnable and accelerate standards-based development prioritizing reuse of code, adopt test-driven development, unit testing and test automation with end-to-end observability of data\nBuild and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality, performance and cost.\nCollaborate with internal clients (product teams, sector leads, data science teams) and external partners (SI partners/data providers) to drive solutioning and clarify solution requirements.\nEvolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects to build and support the right domain architecture for each application following well-architected design standards.\nDefine and manage SLAs for data products and processes running in production.\nCreate documentation for learnings and knowledge transfer to internal associates.\nQualifications\n\n12+ years of engineering and data management experience\n\nQualifications\n12+ years of overall technology experience that includes at least 5+ years of hands-on software development, data engineering, and systems architecture.\n8+ years of experience with Data Lakehouse, Data Warehousing, and Data Analytics tools.\n6+ years of experience in SQL optimization and performance tuning on MS SQL Server, Azure SQL or any other popular RDBMS\n6+ years of experience in Python/Pyspark/Scala programming on big data platforms like Databricks\n4+ years in cloud data engineering experience in Azure or AWS.\nFluent with Azure cloud services. Azure Data Engineering certification is a plus.\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Great Expectations.\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one business intelligence tool such as Power BI or Tableau\nExperience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.\nExperience with version control systems like ADO, Github and CI/CD tools for DevOps automation and deployments.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nUnderstanding of metadata management, data lineage, and data glossaries is a plus.\nBA/BS in Computer Science, Math, Physics, or other technical fields.\nCandidate must be flexible to work an alternative work schedule either on tradition work week from Monday to Friday; or Tuesday to Saturday or Sunday to Thursday depending upon product and project coverage requirements of the job.\nCandidates are expected to be in the office at the assigned location at least 3 days a week and the days at work needs to be coordinated with immediate supervisor\nSkills, Abilities, Knowledge:\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\nProven track record of leading, mentoring data teams.\nStrong change manager. Comfortable with change, especially that which arises through company growth.\nAbility to understand and translate business requirements into data and technical requirements.\nHigh degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\nPositive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\nStrong leadership, organizational and interpersonal skills; comfortable managing trade-offs.\nFoster a team culture of accountability, communication, and self-management.\nProactively drives impact and engagement while bringing others along.\nConsistently attain/exceed individual and team goals.\nAbility to lead others without direct authority in a matrixed environment.\nComfortable working in a hybrid environment with teams consisting of contractors as well as FTEs spread across multiple PepsiCo locations.\nDomain Knowledge in CPG industry with Supply chain/GTM background is preferred.",Industry Type: Beverage,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Pyspark', 'Azure', 'Power BI', 'Github', 'Azure Databricks', 'Tableau', 'ADO', 'Scala programming', 'SQL', 'Azure Data Factory', 'Azure Machine learning', 'Data Lakehouse', 'Azure Data Engineering', 'CI/CD', 'Data Warehousing', 'Data Analytics', 'AWS', 'Python']",2025-06-12 15:05:36
"4 To 8 years of exp. as a Data Analyst @ Banglore, Hyderabad , Chennai",A Client of Career Focus Consultancy,4 - 8 years,5-10 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Strong proficiency in Advanced SQL with experience in writing optimized queries for large datasets.\nMandatory skill : Data Analyst, Python ,SQL, Power BI\n\n\nExposure in, including predictive modeling and machine learning techniques.\n\nRequired Candidate profile\nHands-on experience with Python, R, or similar analytical tools is a plus.\nFamiliarity with cloud platforms such as AWS, Azure, or GCP for data processing and analytics.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['R', 'Power BI', 'Data Analyst', 'Python', 'SQL', 'Azure', 'GCP', 'AWS']",2025-06-12 15:05:38
"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon",One of the largest insurance providers.,5 - 10 years,Not Disclosed,['Gurugram'],"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon\n\nSummary: An excellent opportunity for someone having a minimum of five years of experience with expertise in building data pipelines. A person must have experience in Python, Pyspark and AWS.\n\nLocation- Gurgaon (Hybrid)\n\nYour Future Employer- One of the largest insurance providers.\n\nResponsibilities-\nTo design, develop, and maintain large-scale data pipelines that can handle large datasets from multiple sources.\nReal-time data replication and batch processing of data using distributed computing platforms like Spark, Kafka, etc.\nTo optimize the performance of data processing jobs and ensure system scalability and reliability.\nTo collaborate with DevOps teams to manage infrastructure, including cloud environments like AWS.\nTo collaborate with data scientists, analysts, and business stakeholders to develop tools and platforms that enable advanced analytics and reporting.\n\nRequirements-\nHands-on experience with AWS services such as S3, DMS, Lambda, EMR, Glue, Redshift, RDS (Postgres) Athena, Kinesics, etc.\nExpertise in data modeling and knowledge of modern file and table formats.\nProficiency in programming languages such as Python, PySpark, and SQL/PLSQL for implementing data pipelines and ETL processes.\nExperience data architecting or deploying Cloud/Virtualization solutions (Like Data Lake, EDW, Mart ) in the enterprise.\nCloud/hybrid cloud (preferably AWS) solution for data strategy for Data lake, BI and Analytics.\nWhat is in for you-\nA stimulating working environment with equal employment opportunities.\nGrowing of skills while working with industry leaders and top brands.\nA meritocratic culture with great career progression.\n\nReach us- If you feel that you are the right fit for the role please share your updated CV at randhawa.harmeen@crescendogroup.in\n\nDisclaimer- Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Pipeline', 'AWS', 'Data Ingestion', 'Data Engineering', 'Data Processing']",2025-06-12 15:05:40
Data Architect with Azure Databricks + Power BI,Tech Mahindra,15 - 19 years,Not Disclosed,['Pune'],"Skill Name - Data Architect with Azure & Databricks + Power BIExperience: 15 - 19 years Responsibilities: Architect and design end-to-end data solutions on Cloud Platform, focusing on data warehousing and big data platforms. Collaborate with clients, developers, and architecture teams to understand requirements and translate them into effective data solutions.Develop high-level and detailed data architecture and design documentation. Implement data management and data governance strategies, ensuring compliance with industry standards. Architect both batch and real-time data solutions, leveraging cloud native services and technologies. Design and manage data pipeline processes for historic data migration and data integration. Collaborate with business analysts to understand domain data requirements and incorporate them into the design deliverables. Drive innovation in data analytics by leveraging cutting-edge technologies and methodologies. Demonstrate excellent verbal and written communication skills to communicate complex ideas and concepts effectively. Stay updated on the latest advancements in Data Analytics, data architecture, and data management techniques. Requirements Minimum of 5 years of experience in a Data Architect role, supporting warehouse and Cloud data platforms/environments. Extensive Experience with common Azure services such as ADLS, Synapse, Databricks, Azure SQL etc. Experience on Azure services such as ADF, Polybase, Azure Stream Analytics Proven expertise in Databricks architecture, Delta Lake, Delta sharing, Unity Catalog, data pipelines, and Spark tuning. Strong knowledge of Power BI architecture, DAX, and dashboard optimization. In-depth experience with SQL, Python, and/or PySpark. Hands-on knowledge of data governance, lineage, and cataloging tools such as Azure Purview and Unity Catalog. Experience in implementing CI/CD pipelines for data and BI components (e.g., using DevOps or GitHub). Experience on building symantec modeling in Power BI. Strong knowledge of Power BI architecture, DAX, and dashboard optimization. Strong expertise in data exploration using SQL and a deep understanding of data relationships. Extensive knowledge and implementation experience in data management, governance, and security frameworks. Proven experience in creating high-level and detailed data architecture and design documentation. Strong aptitude for business analysis to understand domain data requirements. Proficiency in Data Modelling using any Modelling tool for Conceptual, Logical, and Physical models is preferred Hands-on experience with architecting end-to-end data solutions for both batch and real-time designs. Ability to collaborate effectively with clients, developers, and architecture teams to implement enterprise-level data solutions. Familiarity with Data Fabric and Data Mesh architecture is a plus. Excellent verbal and written communication skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Symantec', 'Data migration', 'github', 'Data management', 'Business analysis', 'Data Architect', 'data governance', 'SQL', 'Python', 'Data architecture']",2025-06-12 15:05:43
Data Engineer 4,Comcast,5 - 11 years,Not Disclosed,['Chennai'],".\nResponsible for designing, building and overseeing the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs. Work with data modelers/analysts to understand the business problems they are trying to solve then create or augment data assets to feed their analysis. Integrates knowledge of business and functional priorities. Acts as a key contributor in a complex and crucial environment. May lead teams or projects and shares expertise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBachelors Degree\nWhile possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.\n7-10 Years\nComcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.",,,,"['Engineering services', 'Assurance', 'Process optimization', 'MySQL', 'Machine learning', 'Data structures', 'Data quality', 'Troubleshooting', 'Downstream', 'Python']",2025-06-12 15:05:45
Data Engineer,Databeat,3 - 7 years,Not Disclosed,['Hyderabad( Rai Durg )'],"Experience Required: 3+ years\n\nTechnical knowledge: AWS, Python, SQL, S3, EC2, Glue, Athena, Lambda, DynamoDB, RedShift, Step Functions, Cloud Formation, CI/CD Pipelines, Github, EMR, RDS,AWS Lake Formation, GitLab, Jenkins and AWS CodePipeline.\n\n\n\nRole Summary: As a Senior Data Engineer,with over 3 years of expertise in Python, PySpark, SQL to design, develop and optimize complex data pipelines, support data modeling, and contribute to the architecture that supports big data processing and analytics to cutting-edge cloud solutions that drive business growth. You will lead the design and implementation of scalable, high-performance data solutions on AWS and mentor junior team members.This role demands a deep understanding of AWS services, big data tools, and complex architectures to support large-scale data processing and advanced analytics.\nKey Responsibilities:\nDesign and develop robust, scalable data pipelines using AWS services, Python, PySpark, and SQL that integrate seamlessly with the broader data and product ecosystem.\nLead the migration of legacy data warehouses and data marts to AWS cloud-based data lake and data warehouse solutions.\nOptimize data processing and storage for performance and cost.\nImplement data security and compliance best practices, in collaboration with the IT security team.\nBuild flexible and scalable systems to handle the growing demands of real-time analytics and big data processing.\nWork closely with data scientists and analysts to support their data needs and assist in building complex queries and data analysis pipelines.\nCollaborate with cross-functional teams to understand their data needs and translate them into technical requirements.\nContinuously evaluate new technologies and AWS services to enhance data capabilities and performance.\nCreate and maintain comprehensive documentation of data pipelines, architectures, and workflows.\nParticipate in code reviews and ensure that all solutions are aligned to pre-defined architectural specifications.\nPresent findings to executive leadership and recommend data-driven strategies for business growth.\nCommunicate effectively with different levels of management to gather use cases/requirements and provide designs that cater to those stakeholders.\nHandle clients in multiple industries at the same time, balancing their unique needs.\nProvide mentoring and guidance to junior data engineers and team members.\n\n\n\nRequirements:\n3+ years of experience in a data engineering role with a strong focus on AWS, Python, PySpark, Hive, and SQL.\nProven experience in designing and delivering large-scale data warehousing and data processing solutions.\nLead the design and implementation of complex, scalable data pipelines using AWS services such as S3, EC2, EMR, RDS, Redshift, Glue, Lambda, Athena, and AWS Lake Formation.\nBachelor's or Masters degree in Computer Science, Engineering, or a related technical field.\nDeep knowledge of big data technologies and ETL tools, such as Apache Spark, PySpark, Hadoop, Kafka, and Spark Streaming.\nImplement data architecture patterns, including event-driven pipelines, Lambda architectures, and data lakes.\nIncorporate modern tools like Databricks, Airflow, and Terraform for orchestration and infrastructure as code.\nImplement CI/CD using GitLab, Jenkins, and AWS CodePipeline.\nEnsure data security, governance, and compliance by leveraging tools such as IAM, KMS, and AWS CloudTrail.\nMentor junior engineers, fostering a culture of continuous learning and improvement.\nExcellent problem-solving and analytical skills, with a strategic mindset.\nStrong communication and leadership skills, with the ability to influence stakeholders at all levels.\nAbility to work independently as well as part of a team in a fast-paced environment.\nAdvanced data visualization skills and the ability to present complex data in a clear and concise manner.\nExcellent communication skills, both written and verbal, to collaborate effectively across teams and levels.\n\nPreferred Skills:\nExperience with Databricks, Snowflake, and machine learning pipelines.\nExposure to real-time data streaming technologies and architectures.\nFamiliarity with containerization and serverless computing (Docker, Kubernetes, AWS Lambda).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'SQL', 'Data Pipeline', 'Python', 'Amazon Ec2', 'Data Engineering', 'Data Bricks', 'Aws Lambda', 'Amazon Redshift', 'Azure Cloud', 'Data Lake', 'Data Modeling', 'Athena']",2025-06-12 15:05:47
Data Engineer IV - Big Data / Spark,Sadup Soft,5 - 7 years,Not Disclosed,['Chennai'],"Must have skills :\n\n- Minimum of 5-7 years of experience in software development, with a focus on Java and infrastructure tools.\n\n- Min 6+ years of experience as a Data Engineer.\n\n- Good Experience in handling Big Data Spark, Hive SQL, BigQuery, SQL.\n\n- Candidate worked on cloud platforms and GCP would be an added advantage.\n\n- Good understanding of Hadoop based ecosystem including hard sequel, HDFS would be very essential.\n\n- Very good professional knowledge of PySpark or using Scala\n\nResponsibilities :\n\n- Collaborate with cross-functional teams such as Data Scientists, Product Partners and Partner Team Developers to identify opportunities for Big Data, Query ( Spark, Hive SQL, BigQuery, SQL ) tuning opportunities that can be solved using machine learning and generative AI.\n\n- Write clean, high-performance, high-quality, maintainable code.\n\n- Design and develop Big Data Engineering Solutions Applications for above ensuring scalability, efficiency, and maintainability of such solutions.\n\nRequirements :\n\n- A Bachelor or Master's degree in Computer Science or a related field.\n\n- Proven experience working as a Big Data & MLOps Engineer, with a focus on Spark, Scala Spark or PySpark, Spark SQL, BigQuery, Python, Google Cloud,.\n\n- Deep understanding and experience in tuning Dataproc, BigQuery, Spark Applications.\n\n- Solid knowledge of software engineering best practices, including version control systems (e.g Git), code reviews, and testing methodologies.\n\n- Strong communication skills to effectively collaborate and present findings to both technical and non-technical stakeholders.\n\n- Proven ability to adapt and learn new technologies and frameworks quickly.\n\n- A proactive mindset with a passion for continuous learning and research.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Data Engineering', 'BigQuery', 'GCP', 'Spark', 'Machine Learning', 'Python', 'SQL']",2025-06-12 15:05:50
Artificial Intelligence Intern,Kumaran Systems,0 - 1 years,4.5-5 Lacs P.A.,['Chennai( Siruseri Sipcot IT Park )'],"We are looking for a passionate and motivated AI Developer Fresher to join our growing AI team. This role will focus on Generative AI (GenAI) technologies such as large language models (LLMs), diffusion models, and other cutting-edge machine learning techniques.\n\nAs a fresher, youll work closely with senior AI engineers and data scientists to build and fine-tune generative models, contribute to prompt engineering, and support model integration into real-world applications.",,,,"['Data Science', 'Mechine Learning', 'Artificial Intelligence', 'GEN AI', 'Python']",2025-06-12 15:05:52
Ai Ml Engineer,Optum,5 - 10 years,Not Disclosed,['Noida'],"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health optimization on a global scale. Join us to start Caring. Connecting. Growing together.  \nAI Engineer is tasked with the design, development, and deployment of advanced generative AI models and systems. This position requires close collaboration with data scientists, product managers, and other stakeholders to integrate generative AI solutions into existing products and develop new innovative features. Proficiency in the Agentic AI framework is vital for coordinating multiple autonomous AI agents to accomplish complex tasks.\n\nPrimary Responsibilities:\nImplement Generative AI Models: Develop sophisticated generative AI algorithms and models to create new data samples, patterns, or content based on existing data or inputs\nData Processing: Collaborate with stakeholders to preprocess, analyze, and interpret extensive datasets\nModel Deployment: Deploy generative AI models into production environments, ensuring scalability and robustness\nOptimization: Conduct model testing, validation, and optimization to enhance performance\nIntegration: Work with cross-functional teams to seamlessly integrate generative AI solutions into products\nResearch: Stay current with the latest advancements in generative AI technologies and practices\nAgentic AI Framework: Utilize the Agentic AI framework to coordinate multiple AI agents for the completion of complex tasks\nMentorship: Provide mentorship to junior team members and offer technical guidance\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or a related field\n5+ years of experience in software engineering with a focus on AI/ML\nExperience with data preprocessing and analysis\nKnowledge of the Agentic AI framework and its application in AI systems\nProficiency in machine learning frameworks such as TensorFlow and PyTorch\nSolid programming skills in Python, Java, or C++\nFamiliarity with cloud platforms (e.g., AWS, Google Cloud, Azure)\nProven excellent problem-solving abilities and algorithmic thinking\nProven solid communication and teamwork skills\n\nPreferred Qualifications:\nExperience with data processing\nKnowledge of version control systems like Git\nUnderstanding of Generative AI, associated technologies and frameworks like RAG, agents etc.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Agentic Ai', 'Gen AI', 'Cloud', 'RAG', 'LLM']",2025-06-12 15:05:54
Data Engineer,Infoobjects Inc.,3 - 6 years,Not Disclosed,['Jaipur'],"Role & responsibilities:\nDesign, develop, and maintain robust ETL/ELT pipelines to ingest and process data from multiple sources.\nBuild and maintain scalable and reliable data warehouses, data lakes, and data marts.\nCollaborate with data scientists, analysts, and business stakeholders to understand data needs and deliver solutions.\nEnsure data quality, integrity, and security across all data systems.\nOptimize data pipeline performance and troubleshoot issues in a timely manner.\nImplement data governance and best practices in data management.\nAutomate data validation, monitoring, and reporting processes.\n\n\n\nPreferred candidate profile:\nBachelor's or Masters degree in Computer Science, Engineering, Information Systems, or related field.\nProven experience (X+ years) as a Data Engineer or similar role.\nStrong programming skills in Python, Java, or Scala.\nProficiency with SQL and working knowledge of relational databases (e.g., PostgreSQL, MySQL).\nHands-on experience with big data technologies (e.g., Spark, Hadoop).\nFamiliarity with cloud platforms such as AWS, GCP, or Azure (e.g., S3, Redshift, BigQuery, Data Factory).\nExperience with orchestration tools like Airflow or Prefect.\nKnowledge of data modeling, warehousing, and architecture design principles.\nStrong problem-solving skills and attention to detail.\n\nPerks and benefits\nFree Meals\nPF and Gratuity\nMedical and Term Insurance",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Kafka', 'AWS', 'Python', 'Pyspark', 'Java', 'Postgresql', 'Hadoop', 'Spark', 'ETL', 'SQL']",2025-06-12 15:05:57
Senior ML Compiler Engineer,Qualcomm,2 - 4 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nInterested in accelerating machine learning and artificial intelligence on mobile devices for millions of usersCome join our team. We are building software platforms that enable users of Qualcomms silicon to construct optimized neural networks and machine learning algorithms. We are looking for software engineers with a machine learning or compiler background who will help us build these software platforms. In this role, you will construct and tune machine learning frameworks, build compilers and tools, and collaborate with Qualcomm hardware and software engineers to enable efficient usage of Qualcomms silicon for machine learning applications.\n\nMinimum qualifications:\nBachelors degree in Engineering, Information Systems, Computer Science, or related field.\nProgramming in C/C++\n2 to 4 years of software engineering or related work experience\n\n\nPreferred qualifications:\nExperience in machine learning frameworks such as MxNet/NNVM/TVM, Pytorch, Tensorflow, Caffe\n\nOR experience in compilers with an interest in machine learning\nDeep knowledge of software engineering\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'software engineering', 'algorithms', 'c++', 'natural language processing', 'caffe', 'neural networks', 'mxnet', 'artificial intelligence', 'sql', 'deep learning', 'r', 'java', 'data science', 'computer vision', 'machine learning algorithms', 'ml']",2025-06-12 15:06:00
Speech Engineer,BUSINESSNEXT,2 - 5 years,Not Disclosed,['Noida'],"What would you do?\nSystem Design: Architect and design end-to-end speech processing pipelines, from data acquisition to model deployment. Ensure systems are scalable, efficient, and maintainable.\nAdvanced Modeling: Develop and implement advanced machine learning models for speech recognition, speaker diarization, and related tasks. Utilize state-of-the-art techniques such as deep learning, transfer learning, and ensemble methods.\nResearch and Development: Conduct research to explore new methodologies and tools in the field of speech processing. Publish findings and present at industry conferences.",,,,"['Tensorflow', 'Pytorch', 'Stt', 'Speech Recognition', 'transfer learning', 'deep speech', 'Natural Language Processing', 'model deployment', 'Data Acquisition', 'Machine Learning', 'Deep Learning', 'Sts', 'speech processing', 'whisper', 'text to speech']",2025-06-12 15:06:02
AI Engineer,HCLTech,10 - 14 years,Not Disclosed,['Noida'],"Seniority: Senior\nDescription & Requirements\nPosition Summary\nThe Senior AI Engineer with GenAI expertise is responsible for developing advanced technical solutions, integrating cutting-edge generative AI technologies. This role requires a deep understanding of modern technical and cloud-native practices, AI, DevOps, and machine learning technologies, particularly in generative models. You will support a wide range of customers through the Ideation to MVP journey, showcasing leadership and decision-making abilities while tackling complex challenges.",,,,"['AI engineering', 'VMware', 'Java', 'Azure', 'Data engineering', 'AI models', 'Node.js', 'NLP', 'Azure AKS', 'Machine Learning Operations', 'AWS', 'Kubernetes', 'Python']",2025-06-12 15:06:05
Data Science Lead,Protiviti India,9 - 14 years,25-40 Lacs P.A.,['Mumbai (All Areas)'],"Role & responsibilities\n8+ year bachelors or master’s degree from reputed University with concentration on finance, economics or other quantitative field such as statistics or engineering.\nManage multiple client engagements in Financial Services locally in India\nActively drive pre-sales, sales activities primarily for FS clients locally in Data Science Domain\nUnderstand client requirements in detail and create technical & commercial proposal\nDrive client conversations specifically for business development activities",,,,"['Data Science', 'Natural Language Processing', 'Presales', 'Machine Learning', 'AWS', 'GCP', 'Cloud Platform', 'Python']",2025-06-12 15:06:07
Business Analyst/ Data Scientist - SAS & SQL,Khushboo,3 - 8 years,10-20 Lacs P.A.,['Hyderabad'],"hands on SQL/ SAS programming experience & handling complex/large data\nMust have experience inTableau/Power BI\nExperience in campaign performance measurement, customer targeting framework\nProven ability to design and lead strategic projects\n\nRequired Candidate profile\nMust - SAS , SQL, Python\nGood in Statistical model , Predictive model, Logistic regression, Linear regression\nBFSI Mandatory - Credit risk, Credit Card, Retail Banking",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Decision Tree', 'sas', 'sql', 'Advanced Analytics', 'Strategy Building', 'Predictive Modeling', 'python', 'Logistic Regression', 'Segmentation', 'Random Forest', 'Linear Regression', 'Classification', 'Statistical Modeling', 'Credit Risk']",2025-06-12 15:06:09
Data Science_ Lead,Rishabh Software,8 - 13 years,Not Disclosed,"['Ahmedabad', 'Bengaluru', 'Vadodara']","Job Description\n\nWith excellent analytical and problem-solving skills, you should understand business problems of the customers, translate them into scope of work and technical specifications for developing into Data Science projects. Efficiently utilize cutting edge technologies in AI, Generative AI areas and implement solutions for business problems. Good exposure technology platforms for Data Science, AI, Gen AI, cloud with implementation experience. Ability to provide end to end technical solutions leveraging latest AI, Gen AI tools, frameworks for the business problems. This Job requires the following:",,,,"['Data Science', 'gen ai', 'Computer Vision', 'Machine Learning', 'Deep Learning', 'Tensorflow', 'NLP', 'Artificial Intelligence', 'Dl', 'Python']",2025-06-12 15:06:12
"Data Engineer Openings at Advantum Health, Hyderabad",Advantum Health,3 - 5 years,Not Disclosed,['Hyderabad'],"Data Engineer openings at Advantum Health Pvt Ltd, Hyderabad.\nOverview:\nWe are looking for a Data Engineer to build and optimize robust data pipelines that support AI and RCM analytics. This role involves integrating structured and unstructured data from diverse healthcare systems into scalable, AI-ready datasets.\nKey Responsibilities:\nDesign, implement, and optimize data pipelines for ingesting and transforming healthcare and RCM data.\nBuild data marts and warehouses to support analytics and machine learning.\nEnsure data quality, lineage, and governance across AI use cases.\nIntegrate data from EMRs, billing platforms, claims databases, and third-party APIs.\nSupport data infrastructure in a HIPAA-compliant cloud environment.\nQualifications:\nBachelors in Computer Science, Data Engineering, or related field.\n3+ years of experience with ETL/ELT pipelines using tools like Apache Airflow, dbt, or Azure Data Factory.\nStrong SQL and Python skills.\nExperience with healthcare data standards (HL7, FHIR, X12) preferred.\nFamiliarity with data lake house architectures and AI integration best practices\nPh: 9177078628\nEmail id: jobs@advantumhealth.com\nAddress: Advantum Health Private Limited, Cyber gateway, Block C, 4th floor Hitech City, Hyderabad.\nDo follow us on LinkedIn, Facebook, Instagram, YouTube and Threads\nAdvantum Health LinkedIn Page:\nhttps://lnkd.in/gVcQAXK3\n\nAdvantum Health Facebook Page:\nhttps://lnkd.in/g7ARQ378\n\nAdvantum Health Instagram Page:\nhttps://lnkd.in/gtQnB_Gc\n\nAdvantum Health India YouTube link:\nhttps://lnkd.in/g_AxPaPp\n\nAdvantum Health Threads link:\nhttps://lnkd.in/gyq73iQ6",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'SQL', 'Python', 'Airflow', 'ETL', 'Elt']",2025-06-12 15:06:14
Walk In Drive II (June 13 Friday) II B2B Tele Sales II Noida,Info Edge,0 - 4 years,1-5 Lacs P.A.,['Noida'],"About Info Edge\nInfoEdges mission is to create world-class platforms that transform lives by continuously innovating. Our products and services are built keeping our customers in mind. We always delight our customers by delivering superior value through enhanced offerings on the internet and other platforms. Through our continuous investment across various businesses, especially in cutting-edge technology, machine learning and artificial intelligence (AI), we have built a robust system that constantly increases our predictive powers on customer behaviour, and optimizes and improves our systems. Our various teams tirelessly work together to solve problems, innovate, and create something to empower our customers.",,,,"['B2B Sales', 'Virtual Sales', 'Sales Process', 'Direct Sales', 'Crm Tool', 'Client Engagement', 'Business Development', 'Salesforce CRM', 'sales', 'Salesforce']",2025-06-12 15:06:16
HIH - Data Science Lead Analyst - Evernorth,ManipalCigna Health Insurance,5 - 8 years,Not Disclosed,['Hyderabad'],"Internal Title: Data Science Lead Analyst\nExternal Title: Data Science Lead Analyst\nRole Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\n\nThe role will support the development and maintenance of machine learning models, with a focus on ensuring that models meet Cigna s requirements for governance and legal compliance. The role will require collaboration with other data scientists and involve work across many lines of business.\nKey Responsibilities:\nAnalyze model performance of new models with specific regards to requirements for legal compliance and governance standards around accuracy and bias;\nPerform periodic analyses of performance of existing models to ensure continued compliance with internal and external standards for accuracy and bias;\nConduct research (i.e. literature review) to understand when bias may be biologically or medically justifiable, and to what degree, for example: finding evidence from literature that heart disease is more prevalent among older populations\nUsing machine learning development tools to mitigate model bias when this is determined to be necessary\nCollaborating with data scientists, business stakeholders, and governance/compliance teams to ensure models meet compliance and governance standards\nQualifications:\nBachelors or Masters/PhD (preferred) in statistics or computer science or equivalent field with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR)\nAbility to promote best coding practices, championing a culture of documentation/logging\nThorough understanding of ML lifecycle, including necessary tradeoffs and associated risks\nLeadership in Data Science\nCan own a project end-to-end e.g., scoping, business value estimation, ideation, dev, prod, timeline\nCollaborates and guides junior team members in completion of projects and career development\nWorks cross functionally with technical (e.g., Data Science, Data Engineering) and business (e.g., clinical, marketing, pricing, business analysts) to implement solutions with measurable value\nScope and Impact\nIndependently delivers clear and well-developed presentations for both technical and business audiences\nCreates data science specific project goals associated with project deliverables\nArticulates timeline changes, rationale, and goals to meet deadlines moving forward\nValues diversity, growth mindset, and improving health outcomes of our customers\n\nLevel of Influence\nCommunicate with stakeholders to identify opportunities and possible solutions based on business need\nDraft project charter, timeline, and features/stories\nInfluence matrix-partner leadership\nAbout Evernorth Health Services",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Version control', 'Claims', 'data science', 'Legal compliance', 'Coding', 'Pharmacy', 'Machine learning', 'SQL', 'Python']",2025-06-12 15:06:19
HIH - Data Science Lead Analyst - Evernorth,ManipalCigna Health Insurance,5 - 8 years,Not Disclosed,['Hyderabad'],"Internal Title: Data Science Lead Analyst\nExternal Title: Data Science Lead Analyst\nRole Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\nThe role will support the development and maintenance of proprietary advanced neural network ( AI ) foundation models in support of Cigna s business operations.\nKey Responsibilities:\nWrite code using PyTorch and/or Tensorflow to implement, test, and operationalize deep learning models\nCollaborate with data scientists and engineers to improve deep learning models and implement business-facing solutions built on top of those models\nTake responsibility for improving code performance and quality\nFollow developments in deep learning technology to identify opportunities to improve models\nQualifications:\nBachelors or Masters(preferred) in computer science or statistics or any other equivalent discipline with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR), with emphasis on Tensorflow and Pytorch\nKnows and follows best coding and software engineering practices\nSuccessfully completes technical project components with limited guidance\nFamiliarity with deploying machine learning and predictive models to production and cloud environments\nLeadership in Data Science\nUnderstands how assigned work is related to purpose of the overall project\nIndependently identifies project roadblocks, and solutions\nSeeks to understand the health insurance domain\nScope and Impact\nDocuments the business considerations, methodology, process, code, and results associated with their work\nCollaborates to deliver clear and well developed presentations for both technical and business audiences\nConsistently communicates decisions, considerations, and needs for support\nReceives and responds to feedback in a professional and appropriate manner\nLevel of Influence\nPresent technical topics and results to non-technical stakeholders\nCommunicate and gather domain knowledge from non-technical stakeholders\nAbout Evernorth Health Services",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Version control', 'GIT', 'Claims', 'Coding', 'Machine learning', 'Lead Analyst', 'SQL', 'Python', 'Business operations']",2025-06-12 15:06:21
Walk In || Corporate Sales Role || Naukri.com || Ahmedabad,Info Edge,0 - 2 years,Not Disclosed,['Ahmedabad'],"About Info Edge India Ltd\nInfo Edge is Indias leading consumer internet company known for its strong brands in recruitment (naukri.com, naukrigulf.com, iimjobs.com, firstnaukri.com), real estate (99acres.com), matrimony (Jeevansathi.com) and education (shiksha.com).\nStarting with a classified recruitment online business, naukri.com, the Company has grown and diversified rapidly, setting benchmarks as a pioneer for others to follow either through setting up of in-house brands or through the route of strategic investments and acquisitions. Zomato.com, policybazaar.com & Happily Unmarried Marketing Private Limited are our investee companies to name a few out of many. With years of experience in the domain, strong cash flow generation, and a diversified business portfolio, Info Edge is one of the very few profitable pure play internet companies in the country.",,,,"['Sales', 'B2B Sales', 'Lead Generation', 'Client Acquisition', 'Corporate Sales']",2025-06-12 15:06:24
Corporate Sales Executive,Info Edge,0 - 2 years,Not Disclosed,['Ahmedabad'],"About Info Edge India Ltd\nInfo Edge is Indias leading consumer internet company known for its strong brands in recruitment (naukri.com, naukrigulf.com, iimjobs.com, firstnaukri.com), real estate (99acres.com), matrimony (Jeevansathi.com) and education (shiksha.com).Starting with a classified recruitment online business, naukri.com, the Company has grown and diversified rapidly, setting benchmarks as a pioneer for others to follow either through setting up of in-house brands or through the route of strategic investments and acquisitions. Zomato.com, policybazaar.com & Happily Unmarried Marketing Private Limited are our investee companies to name a few out of many. With years of experience in the domain, strong cash flow generation, and a diversified business portfolio, Info Edge is one of the very few profitable pure play internet companies in the country.These are exciting times for Info Edge as we continue to grow in our businesses and scale newer heights. We are investing across various businesses, especially in cutting-edge technology, machine learning, and artificial intelligence (AI) to increase our predictive powers on customer behavior and continuously optimize and improve our systems.At Info Edge, people are our core competitive advantage and we will continue doing all that is needed to attract and retain the best available talent. Driven by innovation, an experienced and talented leadership team, and a strong entrepreneurial orientation, we pride ourselves on having a culture that promotes meritocracy. Our numerous milestones can largely be credited to an incredibly smart team working in an environment that encourages creativity and going the extra mile to develop products that people love to use and add value to our clients.About BU: Naukri.comNaukri.com, online recruitment classifieds, is a significant player and a market leader in Indias well-established business space. The recruitment space provides all the job seekers with advisory services and caters to different elements of the job listing, employer branding, resume short-listing, career site management, and campus recruitment. With over 67 Million resumes searches daily, Naukri.com has 5 Million job listings, 59 Thousand+ more unique clients and 4.9 Million recruiters connect with the job seekers via emails.The platform, in the online recruitment space, continues to reinforce its established leadership position in India which has given it a competitive edge in the market.",,,,"['B2B Corporate Sales', 'New Client Acquisition', 'Corporate Sales']",2025-06-12 15:06:26
Manager Data Engineer – Research Data and Analytics,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will create and develop data lake solutions for scientific data that drive business decisions for Research. You will build scalable and high-performance data engineering solutions for large scientific datasets and collaborate with Research collaborators. You will also provide technical leadership to junior team members. The ideal candidate possesses experience in the pharmaceutical or biotech industry, demonstrates deep technical skills, is proficient with big data technologies, and has a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nLead, manage, and mentor a high-performing team of data engineers\nDesign, develop, and implement data pipelines, ETL processes, and data integration solutions\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks\nDevelop and maintain data models for biopharma scientific data, data dictionaries, and other documentation to ensure data accuracy and consistency\nOptimize large datasets for query performance\nCollaborate with global multi-functional teams including research scientists to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nCollaborate with Data Architects, Business SMEs, Software Engineers and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve data-related challenges\nAdhere to best practices for coding, testing, and designing reusable code/component\nExplore new tools and technologies that will help to improve ETL platform performance\nParticipate in sprint planning meetings and provide estimations on technical implementation\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nDoctorate Degree OR\nMasters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications:\n3+ years of experience in implementing and supporting biopharma scientific research data analytics (software platforms)\n\n\nFunctional Skills:\nMust-Have Skills:\nProficiency in SQL and Python for data engineering, test automation frameworks (pytest), and scripting tasks\nHands on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing\nExcellent problem-solving skills and the ability to work with large, complex datasets\nAble to engage with business collaborators and mentor team to develop data pipelines and data models\n\n\nGood-to-Have Skills:\nA passion for tackling complex challenges in drug discovery with technology and data\nGood understanding of data modeling, data warehousing, and data integration concepts\nGood experience using RDBMS (e.g. Oracle, MySQL, SQL server, PostgreSQL)\nKnowledge of cloud data platforms (AWS preferred)\nExperience with data visualization tools (e.g. Dash, Plotly, Spotfire)\nExperience with diagramming and collaboration tools such as Miro, Lucidchart or similar tools for process mapping and brainstorming\nExperience writing and maintaining technical documentation in Confluence\nUnderstanding of data governance frameworks, tools, and best practices\n\n\nProfessional Certifications:\nDatabricks Certified Data Engineer Professional preferred\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Spotfire', 'PySpark', 'PostgreSQL', 'Plotly', 'SparkSQL', 'SQL server', 'SQL', 'process mapping', 'Dash', 'MySQL', 'ETL', 'Oracle', 'data governance frameworks', 'Python']",2025-06-12 15:06:29
Senior Data Engineer : 7+ Years,Jayam Solutions Pvt Ltd - CMMI Level III Company,5 - 9 years,Not Disclosed,['Hyderabad( Madhapur )'],"Job Description:\nPosition: Sr.Data Engineer\nExperience: Minimum 7 years\nLocation: Hyderabad\nJob Summary:\n\nWhat Youll Do\n\nDesign and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.\nParticipate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.\nIndependently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.\nMonitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.\nMaintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.\nAcquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.\nEngage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.\nDrive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.\n\nWhat Youll Bring\n\nBachelors degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.\n7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.\nExtensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.\nCloud - AWS\nApache Flink/Spark , Redis\nDatabase platform- Databricks.\nProficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.\nExpertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.\nExperience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).\nAbility to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.\nStrong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.\nExceptional attention to detail and professionalism in report and dashboard creation.\nExcellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.\nOutstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.\n\nPlease feel free to contact us: 9440806850\nEmail ID : careers@jayamsolutions.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Flink', 'Redis', 'Spark', 'Python', 'SCALA', 'Ci/Cd', 'Devops', 'AWS']",2025-06-12 15:06:31
Senior Azure Data Engineer,Cloud Angles Digital Transformation,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Data Engineer with expertise in leveraging Data Lake architecture and the Azure cloud platform to develop, deploy, and optimise data-driven solutions. . You will play a pivotal role in transforming raw data into actionable insights, supporting strategic decision-making across the organisation.\nResponsibilities\nDesign and implement scalable data science solutions using Azure Data Lake, Azure Data Bricks, Azure Data Factory and related Azure services.\nDevelop, train, and deploy machine learning models to address business challenges.\nCollaborate with data engineering teams to optimise data pipelines and ensure seamless data integration within Azure cloud infrastructure.\nConduct exploratory data analysis (EDA) to identify trends, patterns, and insights.\nBuild predictive and prescriptive models to support decision-making processes.\nExpertise in developing end-to-end Machine learning lifecycle utilizing crisp-DM which includes of data collection, cleansing, visualization, preprocessing, model development, model validation and model retraining\nProficient in building and implementing RAG systems that enhance the accuracy and relevance of model outputs by integrating retrieval mechanisms with generative models.\nEnsure data security, compliance, and governance within the Azure cloud ecosystem.\nMonitor and optimise model performance and scalability in production environments.\nPrepare clear and concise documentation for developed models and workflows.\nSkills Required:\nGood experience in using Pyspark, Python, MLops (Optional), ML flow (Optional), Azure Data Lake Storage. Unity Catalog\nWorked and utilized data from various RDBMS like MYSQL, SQL Server, Postgres and NoSQL databases like MongoDB, Cassandra, Redis and graph DB like Neo4j, Grakn.\nProven experience as a Data Engineer with a strong focus on Azure cloud platform and Data Lake architecture.\nProficiency in Python, Pyspark,\nHands-on experience with Azure services such as Azure Data Lake, Azure Synapse Analytics, Azure Machine Learning, Azure Databricks, and Azure Functions.\nStrong knowledge of SQL and experience in querying large datasets from Data Lakes.\nFamiliarity with data engineering tools and frameworks for data ingestion and transformation in Azure.\nExperience with version control systems (e.g., Git) and CI/CD pipelines for machine learning projects.\nExcellent problem-solving skills and the ability to work collaboratively in a team environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Data Engineering', 'Azure Databricks', 'Pyspark', 'Azure Data Lake', 'Python']",2025-06-12 15:06:34
Senior Data Engineer - AWS,Blend360 India,6 - 10 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data engineering , with at lea",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 15:06:36
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nRole Description:\nWe are seeking a Senior Data Engineer with expertise in Graph Data technologies to join our data engineering team and contribute to the development of scalable, high-performance data pipelines and advanced data models that power next-generation applications and analytics. This role combines core data engineering skills with specialized knowledge in graph data structures, graph databases, and relationship-centric data modeling, enabling the organization to leverage connected data for deep insights, pattern detection, and advanced analytics use cases. The ideal candidate will have a strong background in data architecture, big data processing, and Graph technologies and will work closely with data scientists, analysts, architects, and business stakeholders to design and deliver graph-based data engineering solutions.\nRoles & Responsibilities:\nDesign, build, and maintain robust data pipelines using Databricks (Spark, Delta Lake, PySpark) for complex graph data processing workflows.\nOwn the implementation of graph-based data models, capturing complex relationships and hierarchies across domains.\nBuild and optimize Graph Databases such as Stardog, Neo4j, Marklogic or similar to support query performance, scalability, and reliability.\nImplement graph query logic using SPARQL, Cypher, Gremlin, or GSQL, depending on platform requirements.\nCollaborate with data architects to integrate graph data with existing data lakes, warehouses, and lakehouse architectures.\nWork closely with data scientists and analysts to enable graph analytics, link analysis, recommendation systems, and fraud detection use cases.\nDevelop metadata-driven pipelines and lineage tracking for graph and relational data processing.\nEnsure data quality, governance, and security standards are met across all graph data initiatives.\nMentor junior engineers and contribute to data engineering best practices, especially around graph-centric patterns and technologies.\nStay up to date with the latest developments in graph technology, graph ML, and network analytics.\nWhat we expect of you\nMust-Have Skills:\nHands-on experience in Databricks, including PySpark, Delta Lake, and notebook-based development.\nHands-on experience with graph database platforms such as Stardog, Neo4j, Marklogic etc.\nStrong understanding of graph theory, graph modeling, and traversal algorithms\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies with strong problem-solving and analytical skills\nExcellent collaboration and communication skills, with experience working with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SPARQL', 'Maven', 'PySpark', 'GSQL', 'Subversion', 'AWS services', 'Stardog', 'Cypher', 'SAFe', 'Jenkins', 'DevOps', 'Git', 'Neo4j', 'Delta Lake', 'Graph Databases', 'Spark', 'Marklogic', 'Gremlin']",2025-06-12 15:06:38
Software Engineer II ( Java Fullstack),JPMorgan Chase Bank,3 - 10 years,Not Disclosed,['Hyderabad'],"You re ready to gain the skills and experience needed to grow within your role and advance your career and we have the perfect software engineering opportunity for you.\nAs a Software Engineer II at JPMorgan Chase within the Consumer anc Community Banking , you are part of an agile team that works to enhance, design, and deliver the software components of the firm s state-of-the-art technology products in a secure, stable, and scalable way. As an emerging member of a software engineering team, you execute software solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role.\nJob responsibilities\nExecutes standard software solutions, design, development, and technical troubleshooting\nWrites secure and high-quality code using the syntax of at least one programming language with limited guidance\nDesigns, develops, codes, and troubleshoots with consideration of upstream and downstream systems and technical implications\nApplies knowledge of tools within the Software Development Life Cycle toolchain to improve the value realized by automation\nApplies technical troubleshooting to break down solutions and solve technical problems of basic complexity\nGathers, analyzes, and draws conclusions from large, diverse data sets to identify problems and contribute to decision-making in service of secure, stable application development\nLearns and applies system processes, methodologies, and skills for the development of secure, stable code and systems\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 2+ years applied experience\nHands-on practical experience in system design, application development, testing, and operational stability\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nDemonstrable ability to code in one or more languages\nExperience across the whole Software Development Life Cycle\nExpereince in Java, J2EE, Fullstack, React JS, AWS.\nExposure to agile methodologies such as CI/CD, Application Resiliency, and Security\nEmerging knowledge of software applications and technical processes within a technical discipline (e. g. , cloud, artificial intelligence, machine learning, mobile, etc. )\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Artificial Intelligence', 'Debugging', 'Machine learning', 'Agile', 'System design', 'Application development', 'Troubleshooting', 'Downstream']",2025-06-12 15:06:41
"Engineer, Staff",Qualcomm,4 - 9 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nJob description:\n\nSkill set\n\nlooking for Python programming, Machine Learning concepts and Automation Testing ( Python framework) Mandatory\n\nPrincipal Duties and Responsibilities:\n\nApplies Software knowledge to assist and support the design, development, creation, modification, and validation of embedded and cloud edge software, applications, and/or specialized utility programs.\n\nAnalyzes user needs and software requirements.\n\nDesigns and implements small software features for products and systems.\n\nParticipates in the design, coding for small features, unit testing, minor debugging fixes, and integration efforts to ensure projects are completed on schedule.\n\nAssists in performing code reviews and regression tests as well as the triaging of issues to ensure the quality of code.\n\nCollaborates with others inside project team to accomplish project objectives.\n\nWrites technical documentation for Software projects.\n\nLevel of Responsibility:\n\nWorks under supervision.\n\nDecision-making affects direct area of work and/or work group.\n\nRequires verbal and written communication skills to convey basic, routine factual information.\n\nTasks require multiple steps which can be performed in various orders; some planning, problem-solving, and prioritization must occur to complete the tasks effectively.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\n\n2+ years of work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['software engineering', 'python', 'automation testing', 'machine learning', 'python framework', 'css', 'jquery', 'sql', 'react.js', 'java', 'git', 'selenium', 'debugging', 'html', 'mysql', 'data structures', 'rest', 'c', 'python development', 'javascript', 'django framework', 'node.js', 'django', 'php', 'agile', 'aws']",2025-06-12 15:06:43
Staff Engineer - Camera Systems,Qualcomm,3 - 8 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Systems Engineer, you will research, design, develop, simulate, and/or validate systems-level software, hardware, architecture, algorithms, and solutions that enables the development of cutting-edge technology. Qualcomm Systems Engineers collaborate across functional teams to meet and exceed system-level requirements and standards.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.\nCandidate should have 10+ years of experience\n\nExperience in C/C++, Computer vision/ Image processing is must\n\nExperience in camera technology, ML/DL is good to have\n\nExperience in Embedded/arm programming is good to have but not necessary\n\nResponsibilities\n\nThe job responsibilities may include a subset of the following\n\nDesigning computer vision /image processing for mobile devices\nDesigning and evaluating algorithms to be implemented in hardware on software prototypes\nDeveloping or Optimizing image processing and computer vision algorithms for HW acceleration\nSupport product teams for commercialization, such as solution optimization, performance profiling and benchmarking.\nTest regression and release support\n\nPreferred Qualifications:\n\nExposure or working experience in Vision or Multimedia accelerators\nWorking experience with image processing algorithms.\nKnowledge/working experience in computer vision algorithms\nStrong knowledge in data structures and working experience with C/C++ programming\nSoftware optimizations experience in various SIMD and multi-threading",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['image processing', 'c++', 'c', 'computer vision', 'data structures', 'python', 'natural language processing', 'scikit-learn', 'dl', 'machine learning', 'artificial intelligence', 'deep learning', 'tensorflow', 'data science', 'embedded systems', 'keras', 'multithreading', 'arm', 'system engineering', 'ml']",2025-06-12 15:06:46
Software Engineer II,Chegg,3 - 8 years,Not Disclosed,['New Delhi'],"About the Team\nChegg's engineering team is a group of passionate engineers who, in close collaboration with data scientists, product managers, designers, and other backend developers, build the future of the online education industry. We develop our products to scale and to last, we dont take shortcuts (hello unit tests and documentation), and we take pride in delivering high-quality solutions on time. We are cloud native.\nRole\nWe are looking for software engineers passionate about solving real-world problems for students in online education using technology. The ideal candidate can think outside the box, is passionate about technology, is adaptable, thinks big, and is passionate about making an impact. Chegg is evolving very fast, and we are constantly redefining our offerings to match the requirements of our student community; the candidate should have the appetite to pivot fast and be interested in continuous improvement and learning. Chegg has a very open and vibrant engineering culture where the candidate will get the opportunity to work with the best in the industry; the role demands ideating and sharing creative ideas as you never know the next big thing Chegg works on can come from you !! If you have dreamt of leveraging your skills and knowledge to impact something big enough to matter, Chegg provides those opportunities, and the candidate should make the best use of them.\nResponsibilities\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, solution development, and proposed solutions;\nCross-team collaboration in driving the end-to-end delivery of SDN on Edge;\nParticipating in the code reviews and design discussions of other engineers;\nHave a strong sense of end-to-end ownership;\nAdhere to key principles: Code and design for best performance, scalability, and resiliency;\nParticipate in daily SCRUM meetings;\nParticipates in the testing process through test review and analysis, test witnessing, and certification of software;\nBe a self-starter, capable of solving ambiguous and challenging technical problems with wide scope;\nFull stack development of new features/tools, including design, documentation, implementation, and testing;\nWork alongside other engineers on the team to elevate technology and consistently apply best practices.\nSkills and Qualifications [Must Have]\nB.E., B.Tech, . degree in Computer Science or a related technical field\n3+ years of product lifecycle experience (from customer requirements -> functional spec -> design -> development/testing -> deployment and monitoring);\nStrong interpersonal and communication skills;\nStrong hands-on development/scripting experience with Python and shell.\nUse tools and methodologies to create representations of workflows, user interfaces, data schemas, etc;\nSolid understanding of software design and development;\nExperience with third-party libraries and APIs;\nExcellent design and problem-solving skills.\nStrong experience with Cloud technologies such as AWS\nExperience with Unit testing frameworks for TDD (Test Driven Development) methodology\nSkills and Qualifications [Good To Have]\nSolid understanding of Agile methodologies and experience working in Agile teams.\nHands-on experience with CI/CD pipelines, preferably using GitLab.\nDevelopment knowledge of mobile apps (android/iOS)",Industry Type: E-Learning / EdTech,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'schema', 'continuous integration', 'software testing', 'software design', 'unit testing', 'android', 'ci/cd', 'solution development', 'ios', 'cloud technologies', 'tdd', 'full stack', 'scrum', 'gitlab', 'shell scripting', 'software engineering', 'code review', 'agile', 'api', 'agile methodology']",2025-06-12 15:06:49
Staff Engineer- Compiler and library development,Qualcomm,4 - 9 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Systems Engineer, you will research, design, develop, simulate, and/or validate systems-level software, hardware, architecture, algorithms, and solutions that enables the development of cutting-edge technology. Qualcomm Systems Engineers collaborate across functional teams to meet and exceed system-level requirements and standards.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nBelow is the JD\n\nInterested in enabling open source developers to build software for millions of devicesInterested in leading optimization solutions for AI on the EdgeCome join our team! Our team builds open source compiler toolsets for Qualcomm silicon. This includes compilers, assemblers, linkers, libraries, debuggers, profilers, and other developer tools. The toolsets enable internal and external developers to build software ecosystems on Qualcomm hardware. We are looking for engineers who will work actively in open source communities to establish and augment compiler and system software toolsets. In this role, you will add and enhance support for Qualcomm hardware in open source projects. You will collaborate with Qualcomm hardware and software engineers to enable efficient usage of Qualcomms silicon for a broad set of applications including machine learning. You will work with the team on the entire compilation stack including optimizing code generation, improving performance, and programmer usability.Responsibilities:Work in the GCC, LLVM, glibc, and related open source communities to add features and improve performance for Qualcomm processorsIdentify areas for improvement in compiler toolsets via benchmarking and code analysisCollaborate with hardware teams to plan, identify, and contribute support in open source projects for hardware features in Qualcomm siliconIdentify areas for improvement in tool usability via interaction with users.Explore new optimization frameworks for leveraging advance CPU features.Design, develop and contribute features to open source ML frameworks.Minimum qualifications:Knowledge and/or experience in compiler frameworks such as GCC or LLVMExperience in working with open source communitiesProgramming in C/C++Bachelors degree in Engineering, Information Systems, Computer Science, or related field.Preferred qualifications:Masters degree or PhD. in Engineering, Information Systems, Computer Science, or related field.Established record of contributions to open source compiler project.Strong background in computer architecture",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['gcc', 'machine learning', 'computer architecture', 'system engineering', 'ml', 'c#', 'algorithms', 'rest', 'developer tools', 'simulation', 'system software', 'javascript', 'sql server', 'sql', 'visual studio', 'open source', 'silicon', 'java', 'computer science', 'asp.net', 'html', 'digital transformation']",2025-06-12 15:06:51
Software tools development Engineer,Qualcomm,2 - 7 years,Not Disclosed,['Chennai'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nQualcomm's Corporate Engineering division in Chennai is looking for software tools development engineer. The candidate will work in a development role to put together software for tool development and test automation across various technologies that are part of Access points, mobile platform, RF, Machine learning platforms. The candidate is expected to have full proficiency on C++ or C# or Python and have experience on developing applications, APIs, software automation using a combination of commercial test equipment and custom hardware designs.\n\nThe ideal candidate will be responsible for implementing novel test plans and supporting those test plans from the R&D lab environment through manufacturing. Candidate will also be responsible for evaluating new complex hardware designs and providing feedback regarding design for testability. Candidate will be responsible to own the test infrastructure, build automation framework and enable other developers towards achieving deployable, scalable test frameworks. Candidate will be responsible for implementing automated test solutions for those hardware designs using a combination of custom test software/hardware and commercial test equipment.\n\nThe candidate will interface with internal staff and outside partners in the fast-paced execution of a variety of multi-disciplined projects. The candidate will have an opportunity to influence and help adopt new test, tool development methodologies and enhance existing processes. International travel might be required. All Qualcomm employees are expected to actively support diversity on their teams, and in the Company.\n\nMinimum Qualifications:\n\nB.E/B.Tech. with industry experience in the following areas:\n\n2+ years of programming experience across C++ / C# / Python\n\nStrong lab skills and experience with standard lab equipment is required\n\nStrong experience in various software technologies, methodologies and applied software engineering practices/standards such as Object-Oriented Design (OOD), cloud and embedded software test automation\n\nPreferred Qualifications:\n\nStrong programming skills in C++/C#\n\nExperience with embedded software and device drivers\n\nApplication UI design Winforms/WPF\n\nExperience with hardware debug equipment such as JTAG and scope\n\nExperience with scripting languages (Perl, Python etc.)\n\nFamiliarity with AI frameworks models performance, quantization, and accuracy metrics\n\nGood analytical, debug and problem-solving abilities\n\nGood communication skills and ability to work in a cross-functional team environment\n\nEffectively delegates tasks to other team members, multitasks and meets aggressive schedules in a dynamic environment.\n\nFPGA/CPLD design, JTAG/boundary scan\n\nExperience with RF test equipment measurements such as signal generator and spectrum analyzer and HW/SW issue troubleshooting\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field.\nEducation requirements:\n\nRequiredB.E. or B.Tech. in Electronics and Communication or Electrical engineering or Computer Science or equivalent. PreferredMasters",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c#', 'c++', 'python', 'software engineering', 'perl', 'rest', 'software development', 'ood', 'device drivers', 'wpf', 'machine learning', 'artificial intelligence', 'software programming', 'winforms', 'embedded software', 'computer science', 'debugging', 'troubleshooting', 'api', 'scripting languages']",2025-06-12 15:06:54
AI Model System Software Performance Optimization Engineer,Qualcomm,1 - 6 years,Not Disclosed,['Hyderabad'],"Title : AI Model System Software Performance Optimization Engineer / Senior Engineer / Lead Engineer / Staff\n\nJob Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nWe are seeking candidates with strong optimized software development knowledge and hands-on experience AI. You will be working in a team responsible for comprehensiveness and enhancement of Performance Optimization tools of state-of-the-art machine learning solutions on Snapdragon platform.You will be working on technical initiatives to continuously benchmark the AI optimization workflow that will serve as relevant, reference case studies for application developers for Windows on Snapdragon. You will drive improvements into the SW stack including SDK, Tools, and documentation that will directly impact the ease of use and performance realization by Windows Application Developers on Snapdragon. You will work closely with development leads, software and hardware architects, customer engineers, and application developers.\n\nResponsibilities:\nUnderstand trends in ML model design, and workflow through application developer engagements and latest academic research\nContinuously measure KPIs for AI development tools on Windows on Snapdragon in terms of level of automation, ease of use, and resulting performance and accuracy preservation\nCompetitive benchmarking of tools and workflow on competitive platforms on state-of-the-art models\nEnhancement of AI performance debug, analysis, and optimization tools for AI application development for Windows on Snapdragon so that Application Developers have nil to very low barrier to entry for Windows on Snapdragon\nInterface with 3rd party application developers and other cross-site and cross-functional teams to arrive at best-in-class performant tools, and documentation that are directly leveraged by 3rd party app developers for Windows on Snapdragon\nContribute new features and designs to the Qualcomm AI toolkit to enhance the workflow experience of Application Developers\n\nSkills and Experience:\n1-10 years experience in AI application development\nExperience in building LLM applications using AI/ML tools/workflow preferably on Windows on CPU, GPU, NPU\nAbility to code in C, C++, and Python\nExperience with performance optimization of AI on GPU, NPU, CPU a plus\nStrong communication skills (written and verbal)\nDemonstrated ability to learn, think and adapt in a fast-changing environment\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['software development', 'c', 'application development', 'java', 'software engineering', 'rest', 'algorithms', 'python', 'c++', 'system software', 'cpu', 'gps', 'machine learning', 'artificial intelligence', 'sql server', 'sql', 'computer science', 'microsoft windows', 'oops', '.net', 'data structures', 'sdk', 'ml']",2025-06-12 15:06:56
Senior AI Camera Systems Engineer,Qualcomm,2 - 4 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n2-4 years of experiences in image processing/computer vision/camera domain.\nWorking experience with machine learning framework/packages (e.g, PyTorch, TensorFlow, Keras etc.)\nStrong hands on experience on developing object detection, tracking or face detection algorithms.\nStrong background in image and signal processing, statistics, and data analysis.\nDeveloping machine learning algorithms for advanced imaging features\nStrong programming skills and working experience in C/C++\\ assembly programming skills, multithreading and RTOS/OS concepts\\fundamentals and Python.\nStrong debugging skills to debug complex system level issues.\nCollaborate with cross-functional teams to design, implement and debug camera\\multimedia features for mobiles.\nGood analytical and problem-solving skills.\n\n\nResponsibilities:\nDevelopment and productize camera essential features on Qualcomm chipsets for mobile\nInfluence camera HW architecture in Qualcomm chipsets\nCollaborate with systems, software, hardware teams at various stages of chipset life in design/validation/commercialization.\nCustomer interaction to commercialize Qualcomm camera solutions.\nIndividual contributions and working with cross functional teams on camera essential features design/planning/execution/commercialization for future Snapdragon chipsets\n\n\nEducation requirements:\nRequiredBachelor's/Masters/PHd Computer Engineering and/or Electrical / Electronic Engineering\nPreferred Masters\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['algorithms', 'data analysis', 'signal processing', 'debugging', 'statistics', 'image processing', 'python', 'c++', 'c', 'object detection', 'machine learning', 'imaging', 'mac', 'hw', 'tensorflow', 'rtos', 'computer science', 'computer vision', 'pytorch', 'keras', 'multithreading', 'system engineering']",2025-06-12 15:06:59
Senior Engineer I,AMERICAN EXPRESS,10 - 15 years,Not Disclosed,['Gurugram'],"Here, your voice and ideas matter, your work makes an impact, and together, you will help us define the future of American Express.\nHow will you make an impact in this role\nAmerican Express is embarking on an exciting transformation driven by an energetic new team of high performers. This group is nimble and creative with the power to shape our technology and product roadmap. If you have the talent and desire to deliver innovative digital and servicing products at a rapid pace, serving our customers seamlessly across physical, digital, mobile, and social media, join our transformation team! You will be part of a fast-paced, entrepreneurial team responsible for delivering projects platform supporting our global customer base. Our Engineers that join our Technologies team will be assigned to one of several exciting teams that are responsible for development and management of business-critical platforms.",,,,"['Computer science', 'Automation', 'Social media', 'Analytical', 'Machine learning', 'Agile', 'Workflow', 'Scrum', 'application architecture', 'Testing']",2025-06-12 15:07:02
Senior ETL Engineer/Consultant Specialist,Hsbc,3 - 6 years,Not Disclosed,['Hyderabad'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Consultant Specialist\nIn this role you will be\nDesign and Develop ETL Processes: Lead the design and implementation of ETL processes using all kinds of batch/streaming tools to extract, transform, and load data from various sources into GCP.\nCollaborate with stakeholders to gather requirements and ensure that ETL solutions meet business needs.\nData Pipeline Optimization: Optimize data pipelines for performance, scalability, and reliability, ensuring efficient data processing workflows.\nMonitor and troubleshoot ETL processes, proactively addressing issues and bottlenecks.\nData Integration and Management: I ntegrate data from diverse sources, including databases, APIs, and flat files, ensuring data quality and consistency.\nManage and maintain data storage solutions in GCP (e. g. , BigQuery, Cloud Storage) to support analytics and reporting.\nGCP Dataflow Development: Write Apache Beam based Dataflow Job for data extraction, transformation, and analysis, ensuring optimal performance and accuracy.\nCollaborate with data analysts and data scientists to prepare data for analysis and reporting.\nAutomation and Monitoring: Implement automation for ETL workflows using tools like Apache Airflow or Cloud Composer, enhancing efficiency and reducing manual intervention.\nSet up monitoring and alerting mechanisms to ensure the health of data pipelines and compliance with SLAs.\nData Governance and Security: Apply best practices for data governance, ensuring compliance with industry regulations (e. g. , GDPR, HIPAA) and internal policies.\nCollaborate with security teams to implement data protection measures and address vulnerabilities.\nDocumentation and Knowledge Sharing: Document ETL processes, data models, and architecture to facilitate knowledge sharing and onboarding of new team members.\nConduct training sessions and workshops to share expertise and promote best practices within the team.\n\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nEducation: Bachelor s degree in Computer Science, Information Systems, or a related field.\nExperience: Minimum of 5 years of industry experience in data engineering or ETL development, with a strong focus on Data Stage and GCP.\nProven experience in designing and managing ETL solutions, including data modeling, data warehousing, and SQL development.\nTechnical Skills: Strong knowledge of GCP services (e. g. , BigQuery, Dataflow, Cloud Storage, Pub/Sub) and their application in data engineering.\nExperience of cloud-based solutions, especially in GCP, cloud certified candidate is preferred.\nExperience and knowledge of Bigdata data processing in batch mode and streaming mode, proficient in Bigdata eco systems, e. g. Hadoop, HBase, Hive, MapReduce, Kafka, Flink, Spark, etc.\nFamiliarity with Java Python for data manipulation on Cloud/Bigdata platform.\nAnalytical Skills: Strong problem-solving skills with a keen attention to detail.\nAbility to analyze complex data sets and derive meaningful insights.\nBenefits: Competitive salary and comprehensive benefits package.\nOpportunity to work in a dynamic and collaborative environment on cutting-edge data projects.\nProfessional development opportunities to enhance your skills and advance your career.\nIf you are a passionate data engineer with expertise in ETL processes and a desire to make a significant impact within our organization, we encourage you to apply for this exciting opportunity!",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data modeling', 'HIPAA', 'Data quality', 'Apache', 'Monitoring', 'Analytics', 'Financial services', 'Python']",2025-06-12 15:07:04
Cloud Support Engineer / Senior Software Engineer,Hsbc,2 - 5 years,Not Disclosed,['Pune'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Senior Software Engineer\nIn this role, you will:\nProvide three tier (L1, L2, L3) support to all applications and provide assistance to all end users.\nProactively identify any issues in production via automated monitoring, history of production issues and trends.\nMaintain schedule jobs and perform troubleshoot on processes.\nAnalyze all vendor applications and provide operational support.\nDocument all production applications and resolve all application issues and answer all requests.\nMonitor all performance metrics for various production systems and identify root cause for all technical issues and recommend solutions.\nAnalyze all applications and recommend necessary upgrades and patches and perform troubleshoot on all issues.\nMaintain effective relationships with various system administrators and development teams.\nParticipate in periodic meetings and maintain all applications for productions and plan appropriate various strategies.\nPublishing GCP cost Dashboards, Alerting and monitoring\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nKnowledge of Incident Management Problem Management is mandatory.\nProduction support ticketing knowledge is an advantage (Remedy, Jira, SQL Assistant, Blade logic, Splunk, MuleSoft, App Dynamics, GitHUB Knowledge/ Websphere etc. )\nExcellent communication skills in both Oral and Written communication.\nExcellent understanding of machine learning setup in Google architecture and google Analytics products\nShould have experience working in agile and devops environment using team collaboration tools such as Confluence, JIRA.\nProgramming skills and hands-on experience in Python desirable\nProficiency in working with cloud based native data stores/databases\nKnowledge on design patterns for GCP third party tools setup and native tools usage\nExperience in publishing GCP cost Dashboards, Alerting and monitoring",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Ticketing', 'operational support', 'Publishing', 'Google Analytics', 'Production support', 'Problem management', 'Incident management', 'Financial services', 'SQL', 'Remedy']",2025-06-12 15:07:08
Automation Engineer,Synechron,3 - 8 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027505\n\nOverall Responsibilities:\nLead the development and implementation of projects using emerging technologies\nMentor and guide team members to ensure the successful delivery of projects\nIdentify and evaluate new technology solutions to improve business processes\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nStay up-to-date with the latest technological advancements and industry trends\n\n\nSkills:\nStrong expertise in emerging technologies such as blockchain, IoT, AI, etc.\nStrong technical knowledge of software development lifecycle\nExcellent problem-solving and critical thinking skills\nGood understanding of software architecture and design patterns\nAbility to lead and manage a team of technical experts\nExperience:\nAt least 3+ years of experience in software development and leading technology projects\nProven track record of delivering projects using emerging technologies\nExperience in mentoring and guiding junior team members\nExperience in working with cross-functional teams\nDay-to-Day Activities:\nManage the development and delivery of projects using emerging technologies\nProvide technical guidance and mentorship to junior team members\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nEvaluate and recommend new technology solutions to improve business processes\nStay up-to-date with the latest technological advancements and industry trends\nQualification:\nBachelor's or Master's degree in Computer Science, Information Technology, or related field\nRelevant certifications in emerging technologies\nSoft\n\nSkills:\nStrong communication and leadership skills\nAbility to work well under pressure and meet tight deadlines\nExcellent interpersonal and team-working skills\nAbility to effectively communicate technical information to non-technical stakeholders\nPassionate about technology and a desire to stay up-to-date with the latest advancements.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['emerging technologies', 'artificial intelligence', 'iot', 'software development life cycle', 'blockchain', 'project management', 'python', 'software development', 'information technology', 'business analysis', 'machine learning', 'java', 'automation engineering', 'design patterns', 'agile']",2025-06-12 15:07:10
Senior Engineer - Network Stack Development with AI,Qualcomm,2 - 7 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\nTitleSenior EngineerJob FunctionNetwork Stack Development with AISkills/Experience:\n""ƒ""ƒ3-5 years of proficiency in C/C++, Python programming languages and Linux operating systems\n""ƒ""ƒStrong understanding of Networking concepts, particularly with L3/L4 (Layer 3/Layer 4) experience\n""ƒ""ƒKnowledge of AI/ML conceptsResponsibilities:\n""ƒ""ƒContribute to the design and implementation of AI modules for network stack components\n""ƒ""ƒPerform thorough testing to ensure the reliability and performance of the developed componentsEducation :\n""ƒ""ƒBE/MTech/MS in a relevant field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'python', 'c', 'networking concepts', 'linux', 'network expansion', 'channel sales', 'networking', 'network development', 'dealer development', 'business development', 'artificial intelligence', 'sales', 'channel development', 'marketing', 'java', 'software engineering', 'dealer management']",2025-06-12 15:07:12
Senior Engineer - Fingerprint SW,Qualcomm,2 - 7 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\n\nJob Overview\n\nAs a member of the Fingerprint SW team, the developer shall design, modify, and implement Fingerprint SW in Middleware Layer for Qualcomm Fingerprint Solution\n\n\n\nSW design and development on embedded platforms SW Stack development in Middleware layer. Debug and resolve issues in SW reported by internal test teams as well as by customers.Minimum Qualifications 3 to 5 years of experience with embedded systems Must be proficient in C and Database Concepts. Understanding of Linux User and Kernel space development. Good analytical and problem solving skills Strong understanding of basic real-time/embedded programming concepts & real time operating systems concepts Preferred Qualifications Good understanding of microprocessor, multiprocessor architecture. Good to have exposure with ARM based processor and Trustzone awareness. Good to have some basic understanding of Machine Learning and Deep learning techniques.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c', 'database creation', 'embedded systems', 'software engineering', 'embedded programming', 'rest', 'python', 'c++', 'kernel', 'machine learning', 'deep learning', 'test engineering', 'java', 'computer science', 'linux', 'debugging', 'arm', 'digital transformation', 'middleware']",2025-06-12 15:07:15
Senior AIML Engineer,Synechron,6 - 11 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027526\n\nOverall Responsibilities:\nLead the development and implementation of projects using emerging technologies\nMentor and guide team members to ensure the successful delivery of projects\nIdentify and evaluate new technology solutions to improve business processes\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nStay up-to-date with the latest technological advancements and industry trends\n\n\nSkills:\nStrong expertise in emerging technologies such as blockchain, IoT, AI, etc.\nStrong technical knowledge of software development lifecycle\nExcellent problem-solving and critical thinking skills\nGood understanding of software architecture and design patterns\nAbility to lead and manage a team of technical experts\nExperience:\nAt least 6+ years of experience in software development and leading technology projects\nProven track record of delivering projects using emerging technologies\nExperience in mentoring and guiding junior team members\nExperience in working with cross-functional teams\nDay-to-Day Activities:\nManage the development and delivery of projects using emerging technologies\nProvide technical guidance and mentorship to junior team members\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nEvaluate and recommend new technology solutions to improve business processes\nStay up-to-date with the latest technological advancements and industry trends\nQualification:\nBachelor's or Master's degree in Computer Science, Information Technology, or related field\nRelevant certifications in emerging technologies\nSoft\n\nSkills:\nStrong communication and leadership skills\nAbility to work well under pressure and meet tight deadlines\nExcellent interpersonal and team-working skills\nAbility to effectively communicate technical information to non-technical stakeholders\nPassionate about technology and a desire to stay up-to-date with the latest advancements.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['emerging technologies', 'artificial intelligence', 'iot', 'software development life cycle', 'blockchain', 'soft skills', 'project management', 'software development', 'information technology', 'program management', 'strategy consulting', 'design patterns', 'it strategy', 'agile']",2025-06-12 15:07:17
L2 Support Engineer,Synechron,5 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027519\n\nOverall Responsibilities:\nLead the development and implementation of projects using emerging technologies\nMentor and guide team members to ensure the successful delivery of projects\nIdentify and evaluate new technology solutions to improve business processes\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nStay up-to-date with the latest technological advancements and industry trends\n\n\nSkills:\nStrong expertise in emerging technologies such as blockchain, IoT, AI, etc.\nStrong technical knowledge of software development lifecycle\nExcellent problem-solving and critical thinking skills\nGood understanding of software architecture and design patterns\nAbility to lead and manage a team of technical experts\nExperience:\nAt least 5+ years of experience in software development and leading technology projects\nProven track record of delivering projects using emerging technologies\nExperience in mentoring and guiding junior team members\nExperience in working with cross-functional teams\nDay-to-Day Activities:\nManage the development and delivery of projects using emerging technologies\nProvide technical guidance and mentorship to junior team members\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nEvaluate and recommend new technology solutions to improve business processes\nStay up-to-date with the latest technological advancements and industry trends\nQualification:\nBachelor's or Master's degree in Computer Science, Information Technology, or related field\nRelevant certifications in emerging technologies\nSoft\n\nSkills:\nStrong communication and leadership skills\nAbility to work well under pressure and meet tight deadlines\nExcellent interpersonal and team-working skills\nAbility to effectively communicate technical information to non-technical stakeholders\nPassionate about technology and a desire to stay up-to-date with the latest advancements.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['emerging technologies', 'artificial intelligence', 'iot', 'software development life cycle', 'blockchain', 'project management', 'software development', 'engineering support', 'information technology', 'sql', 'production support', 'design patterns', 'application support', 'agile']",2025-06-12 15:07:20
Power Architecture -Staff Engineer,Qualcomm,8 - 13 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nJob Overview:\n\nWe are looking for software engineers who can define software architectures while viewing software as part of a larger system comprising both software and hardware. Previous architecture experience is not necessary as long as you have good software engineering skills and are willing to approach problems at the system level.In this role you will have two related areas of responsibilities:1) Participating in the definition of next generation architectures for future Qualcomm SoCs, and2) Driving the software design to realize the architecture on each SoC. This role is focused on power, thermal and limits management but you must also consider other important metrics such as performance and cost.Qualcomm SoCs serve many product categories including smartphones, tablets, wearables, IoT, servers, AR/VR and automotive (telemetry, IVI and ADAS). One challenge in this role is to drive commonality in the architecture across these diverse product categories.\n\nJob function / Responsibilities\nWork with engineers across a range of disciplines (e.g. hardware, software and systems) and technologies (e.g. advanced CPUs, Hexagon DSPs, Adreno GPUs, AR/VR, ML/AI, 5G modems, Wireless LAN, and GPS)\nParticipate in defining and communicating next generation architectures for Qualcomm SoCs with a focus on power, thermal and limits management\nDrive the process of converting the power, thermal and limits management architecture into a software design and software requirements for each SoC\nWork with software teams to provide guidance on the architecture and design, and to help resolve issues\nDesign tools to identify and debug power consumption issues on development platforms and commercial devices\n\n\nPreferred skills/experience\n\n8+ yrs of experience in software development for SoCs and platforms in wireless, automotive and/or IOT\n\nStrong analytical skills and the ability to approach problems at a system level\n\nOne or more of the following:\nDevice driver or board support package (BSP) knowledge or development experience\nExperience with one or more RTOSs\nExperience with ADAS or in vehicle infotainment systems\nUnderstanding of ARM processor architectures\nExperience in power, thermal and/or limits management at the system or device driver level\nExperience with virtualization and hypervisors\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\n\n2+ years of work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'software development', 'iot', 'java', 'software engineering', 'board support package', 'python', 'virtual reality', 'c', 'software design', 'vxworks', 'device drivers', 'spi', 'artificial intelligence', 'rtos', 'computer science', 'embedded systems', 'embedded c', 'linux', 'information systems', 'i2c']",2025-06-12 15:07:22
Engineer staff -Gstreamer Plugin development,Qualcomm,7 - 12 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nWe are seeking a skilled Engineer with extensive experience in the GStreamer multimedia framework. The ideal candidate will be responsible for designing, developing, and optimizing multimedia applications and systems. This role requires a deep understanding of multimedia processing, pipeline architecture, and the ability to work on complex projects.Minimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 7+ years of Software Engineering or related work experience. ORMaster's degree in Engineering, Information Systems, Computer Science, or related field and 5+ years of Software Engineering or related work experience. ORPhD in Engineering, Information Systems, Computer Science, or related field and 2+ year of Software Engineering or related work experience.\n7+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.Experience with majority in Multimedia framework & Gstreamer plugins development.Strong programming skills in C and C++ for embedded systemsGood knowledge about AI/ML applications developementsExposure to developing solutions on Linux is mustStrong in multi-threaded programming, synchronization and IPCsStrong Software design skills and ability to guide team of engineersGood knowledge on software development processesNeed very good Communication skills and ability to work with cross functional teamsExposure to other media frameworks such as ffmpeg, directshow, stagefright is a plusGood knowledge on V4L2, Pulseaudio, Alsa, OpenGLES is a plus.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\n\n2+ years of work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'c', 'multimedia framework', 'linux', 'gstreamer', 'python', 'software development', 'software design', 'plugins', 'opengl es', 'pipeline architecture', 'artificial intelligence', 'multimedia', 'stagefright', 'java', 'alsa', 'computer science', 'multithreading', 'software engineering', 'ffmpeg']",2025-06-12 15:07:25
Senior Consultanr - AI Cloud Engineer,AstraZeneca India Pvt. Ltd,5 - 10 years,Not Disclosed,['Chennai'],"Job Title: Senior Consultant - AI Cloud Engineer Career Level: D2 Introduction to role:\nAre you ready to tackle some of the most exciting machine-learning challenges in drug discovery? We are seeking a Senior AI Platform Engineer to join our innovative AI platform team, IGNITE. With your expertise in AWS cloud environments, youll design and deploy large-scale production infrastructure that will redefine healthcare and improve the lives of millions worldwide. As part of a close-knit team of technical specialists, youll create tools that support major AI initiatives, from clinical trial data analysis to imaging and Omics. Your role will be pivotal in providing frameworks for data scientists to develop scalable machine learning models safely and robustly. Are you prepared to bridge the gap between science and engineering with your deep expertise?\nAccountabilities:\nDesign, implement, and manage cloud infrastructure on AWS using Infrastructure as Code (IaC) tools such as Terraform or AWS CloudFormation.\nMaintain and enhance CI/CD pipelines using tools like GitHub Actions, AWS CodePipeline, Jenkins, or ArgoCD.\nEnsure platform reliability, scalability, and high availability across development, staging, and production environments.\nAutomate operational tasks, environment provisioning, and deployments using scripting languages such as Python, Bash, or PowerShell.\nEnable and maintain Amazon SageMaker environments for scalable ML model training, hosting, and pipelines.\nIntegrate AWS Bedrock to provide foundation model access for generative AI applications, ensuring security and cost control.\nLead and publish curated infrastructure templates through AWS Service Catalogue to enable consistent and compliant provisioning.\nCollaborate with security and compliance teams to implement best practices around IAM, encryption, logging, monitoring, and cost optimization.\nImplement and manage observability tools like Amazon CloudWatch, Prometheus/Grafana, or ELK for monitoring and alerting.\nSupport container orchestration environments using EKS (Kubernetes), ECS, or Fargate.\nContribute to incident response, post-mortems, and continuous improvement of the platform s operational excellence.\nEssential Skills/Experience:\nBachelor s degree in Computer Science, Engineering, or related field (or equivalent experience).\n5+ years of hands-on experience with AWS cloud services.\nStrong experience with Terraform, AWS CDK, or CloudFormation.\nProficiency in Linux system administration and networking fundamentals.\nSolid understanding of IAM policies, VPC design, security groups, and encryption.\nExperience with Docker and container orchestration using Kubernetes (EKS preferred).\nHands-on experience with CI/CD tools and version control (Git).\nExperience with monitoring, logging, and alerting systems.\nStrong solving skills and ability to work independently or in a team.\nDesirable Skills/Experience:\nAWS Certification (e.g., AWS Certified DevOps Engineer, Solutions Architect - Associate/Professional).\nExperience with serverless technologies like AWS Lambda, Step Functions, and EventBridge.\nExperience supporting machine learning or big data workloads on AWS.\nExperience with SAFe agile principles and practices.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Version control', 'Networking', 'Machine learning', 'Agile', 'Healthcare', 'Monitoring', 'Python', 'Recruitment']",2025-06-12 15:07:28
Security Engineer,Indian / Global Digital Organization,6 - 8 years,Not Disclosed,['Gurugram'],"Key Skills: Cloud Security, Cyber Security, AI Artificial intelligence.\nRoles and Responsibilities:\nSecurity Engineering:\nBuild and integrate security solutions such as firewalls, encryption tools, and intrusion detection systems to safeguard critical infrastructure and data.\nCollaborate with development teams to embed security measures throughout the software development lifecycle (SDLC).\nAutomate security workflows, vulnerability management, and incident response processes to enhance efficiency and response time.\nLead security initiatives to address emerging threats and ensure systems are resilient to evolving cyber risks.\nCloud Security:\nDesign, implement, and manage secure cloud architectures across AWS, Azure, and Google Cloud platforms.\nLeverage AI/ML-driven security tools for enhanced cloud monitoring, threat detection, and automated incident response.\nAutomate cloud security configurations and utilize AI to conduct predictive vulnerability assessments.\nWork closely with DevOps and infrastructure teams to implement robust, automated security controls in cloud environments.\nData Protection Controls:\nDesign and manage comprehensive data protection strategies including encryption, tokenization, and data masking practices to ensure data confidentiality and integrity.\nExperience Requirement:\n6-8 years of experience in building and integrating security solutions across diverse environments.\nStrong knowledge of cloud platforms such as AWS, Azure, and Google Cloud, including cloud security frameworks and best practices.\nHands-on experience with AI/ML-powered security tools for monitoring, detection, and response.\nProficiency in automating security operations and infrastructure configurations.\nExperience working in cross-functional teams, including DevOps and software engineering, to enforce security standards throughout the development lifecycle.\nFamiliarity with industry-standard protocols, tools, and methodologies in vulnerability assessment and incident management.\nEducation: B.Tech M.Tech (Dual), B.Tech.",Industry Type: Beauty & Personal Care,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Cloud Security', 'Cyber Security', 'AI Artificial intelligence.']",2025-06-12 15:07:30
Staff Engineer,Qualcomm,8 - 13 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\n\n\nIf youre interested in advancing and applying mathematics, programming languages theory, and advanced algorithms to program optimization for cutting-edge machine learning accelerators, then you really want to be talking to us!\n\n\n\nWe are looking to hire ML Compiler engineers to join our team. We work tactically on improving existing ML compilers and strategically on developing new and innovative ML compilers.\n\n\n\nOur technical approach to compilers emphasizes powerful representations for precisely and compactly modeling programs and the optimization challenges and using advanced mathematics and algorithms for performing optimizations.\n\n\n\nWe are also solid in using ""old school"" compiler technologies as they apply to contemporary ML challenges, and in meticulous software engineering to produce beautiful compilers. We are also keen about seeing our compilers used and having large impacts on Qualcomms business.\n\n\n\nMapping ML algorithms to ML accelerators is currently one of the most interesting and challenging problems for compilers. Our compiler targets include the Qualcomm Neural Signal Processor, Adreno GPUs, low-power ML accelerators, and CPU accelerators.\n\nThis job description spans multiple levels, from entry to experienced. Our team is a good home for compiler developers with advanced degrees, and we have solid mentoring and give substantial responsibility quickly for entry level engineers.\n\n\n\nResponsibilities Work on a wide range of ML compilers\n\nImprove ML compiler optimization capabilities through benchmark analysis and profiling\n\nInnovate new ML compiler and optimization algorithms\n\nUpstream compiler algorithms to open-source compiler projects Author research publications and represent the company in conferences and industry forums\n\n\n\nRequired\n\nExperience with compiler development and computer architecture\n\nML experience\n\nA degree in the field of computer science or applied mathematics\n\nExperience with software engineering\n\nSolid intellectual ability, motivation, and a strong history of achievementExcellent oral and written communication skills\n\nDesired Experience with MLIR, MLIR Dialects (LinAlg, Affine), Pytorch 2.0, TVM, Triton, and/or LLVM\n\nSYCL experience\n\nML applications and ML optimization experience\n\nML architecture experience\n\nHigh performance computing experience\n\nPolyhedral compiler optimization experience\n\nLoop transformation and vectorization experience\n\nGPU programming, parallel programming experience\n\nGeneral optimization experience\n\n8+ years of relevant work experience\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\n\n2+ years of work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'python', 'gps', 'java', 'software engineering', 'algorithms', 'database management system', 'c', 'software development', 'dbms', 'sql', 'spring', 'rtos', 'computer architecture', 'design patterns', 'embedded systems', 'linux', 'oops', 'embedded c', 'multithreading', 'data structures', 'html']",2025-06-12 15:07:33
J PED Engineer,Tata Technologies,2 - 5 years,Not Disclosed,"['Pune', 'Bengaluru']","Role Summary:\nThe Performance, Efficiency & Drivability (PED) Attributes Engineer is responsible for supporting the PED Lead Engineer with data analysis, summaries and judgement to support the delivery of Propulsion Efficiency Performance and Drivability Attributes and Features across various vehicle architectures and powertrains. A PED Engineer s scope of work spans the entire vehicle development cycle, contributing to targets definition, competitors benchmark analysis, management of attribute inputs and trades, vehicle development and sign off to brand DNA.\nWe are looking for a junior engineer to add to our dynamic and growing team. The successful candidate will work on the delivery of our new lineup of Battery Electric Vehicles, supporting the Lead Engineers in the delivery of Propulsion Efficiency.",,,,"['J PED Engineering', 'benchmark analysis', 'D&R', 'UK Driving License', 'vehicle development cycle', 'PED tools', 'FMA process', '8D', 'Quality processes']",2025-06-12 15:07:36
Technical Lead,Ericsson,10 - 15 years,Not Disclosed,['Bengaluru'],"About this opportunity\nWe are seeking a Tech Lead FPGA Designer to join the Ericsson Silicon organization. In this pivotal role, you will provide technical leadership to a group of dedicated engineers committed to developing world-class Radio and RAN Compute products.\nYou will lead the FPGA team in designing, integrating, and optimizing complex systems for high-efficiency data transfer and processing with embedded subsystems. As part of our global organization, youll collaborate with talented teams across our various sites.\nWe are committed to Agile principles, fostering a collaborative and innovative work environment that encourages creativity, teamwork, and strategic thinking.",,,,"['VHDL', 'Communication protocols', 'Hardware design', 'Verilog', 'Ethernet', 'Machine learning', 'Shell scripting', 'PCIE', 'SPI', 'Python']",2025-06-12 15:07:38
Python Developer Lead {ENG - Infosys @ Pan India - G },Infosys,4 - 9 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Responsibilities A day in the life of an Infoscion • As part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain. • You will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements. • You will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers. • You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nTechnical and Professional Requirements: • Primary skills:Process->Testing processes->Test Automation Process,Technology->Machine Learning->Python\n\nPreferred Skills: Process->Testing processes->Test Automation Process\nTechnology->Machine Learning->Python\n\nAdditional Responsibilities: • Knowledge of design principles and fundamentals of architecture • Understanding of performance engineering • Knowledge of quality processes and estimation techniques • Basic understanding of project domain • Ability to translate functional / nonfunctional requirements to systems requirements • Ability to design and code complex programs • Ability to write test cases and scenarios based on the specifications • Good understanding of SDLC and agile methodologies • Awareness of latest technologies and trends • Logical thinking and problem solving skills along with an ability to collaborate\n\nEducational RequirementsMCA,MSc,MTech,Bachelor of Engineering,BCA,BE,BSc,BTech Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Django Framework', 'Python Development', 'Python']",2025-06-12 15:07:40
Technical Lead FPGA,Ericsson,10 - 15 years,Not Disclosed,['Bengaluru'],"About this opportunity\nWe are seeking a Tech Lead FPGA Designer to join the Ericsson Silicon organization. In this pivotal role, you will provide technical leadership to a group of dedicated engineers committed to developing world-class Radio and RAN Compute products.\nYou will lead the FPGA team in designing, integrating, and optimizing complex systems for high-efficiency data transfer and processing with embedded subsystems. As part of our global organization, youll collaborate with talented teams across our various sites.\nWe are committed to Agile principles, fostering a collaborative and innovative work environment that encourages creativity, teamwork, and strategic thinking.",,,,"['VHDL', 'Communication protocols', 'Hardware design', 'Verilog', 'Ethernet', 'Machine learning', 'Shell scripting', 'PCIE', 'SPI', 'Python']",2025-06-12 15:07:42
BCN Labs_ Project leader - Full Stack,Bain,3 - 8 years,Not Disclosed,['Bengaluru'],"We're seeking a Project Leader, who is a self-starter , and brings a unique mix of data engineering expertise and analytical problem-solving ability to play a key role in the delivery of cutting-edge analytical solutions at BCN Labs. This role sits at the intersection of robust data platform engineering, software development and client-oriented delivery, requiring both hands-on implementation skills and the knack for strategic thinking to solve real world business problems.\nAs a PL, you will drive the end-to-end data pipeline lifecycle - from designing robust architectures to deploying production-grade analytical solutions. you'll also work closely with analysts, data scientists and business stakeholders to frame problems, validate solutions, and lead teams in client delivery.\n  A PL will be responsible to:\nArchitect and Deliver Scalable Data Pipelines : Build, optimize, and maintain batch and streaming pipelines using modern data engineering tools and frameworks (eg, PySpark, Airflow, Snowflake etc).\nEnd-to-End Project Leadership: Own the full delivery cycle from data ingestion and transformation to application deployment and monitoring in the cloud.\nAnalytical Framing : Work with project teams to understand business needs and help shape technical solutions that are analytically sound and measurable in terms of business value.\nMentorship and Team Leadership : Lead a team of engineers and analysts, providing technical guidance, code reviews, and project oversight to ensure quality and impact. Help build the next layer of people with full-stack capabilities at BCN Labs.\nHands-on Development : Write and review clean, modular, production-ready code. Ensure scalability, reusability, and maintainability of solutions.\nClient & Stakeholder Engagement : Communicate complex technical concepts and insights clearly and persuasively to non-technical audiences, both internally and externally.\nData Infrastructure Innovation: Contribute to internal tooling, frameworks, and automation efforts to accelerate the Labs data engineering capabilities.\nCollaborate on Analytical Solutions : Work with data scientists by enabling high-performance, we'll-governed data environments and workflows.\nEducation & Experience:\nbachelors or masters degree in Computer Science, Information Technology, Engineering, or a related field.\n5+ years (Masters + 3+ years) of proven experience in data engineering, software development, and building scalable data pipelines in a production environment.\nDemonstrated expertise in driving end-to-end analytical solution delivery from data ingestion and transformation to cloud deployment and performance optimization.\nYou will fit into our team-oriented structure with a college/hostel-style way of working, having the comfort of reaching out to anyone for support that can enable our clients better\nCore Technical Skills:\nExpertise in Python with solid experience in writing efficient, maintainable, and testable code for data pipelines and services.\nStrong skills in SQL (and NoSQL DB) for data transformation, analysis, and performance tuning.\nProficiency with HTML, CSS, JavaScript, AJAX to build data-driven UIs or Web Apps.\nExperience in developing, integrating and consuming RESTful APIs and working with microservices architecture.\nFrameworks & Platforms:\nHands-on experience with Python-based frameworks like FastAPI, Django and/or Streamlit for building data apps or APIs.\nSolid frontend development skills, with experience using modern JavaScript frameworks such as React and/or Vue.js to build interactive, data-driven UIs and Web Apps.\nFamiliarity with Docker, Git, CI/CD pipelines, and modern software delivery practices.\nExperience deploying and managing data solutions on AWS or Azure (eg, Lambda, EC2, S3, Data Factory).\nStrong preference for candidates with real-world experience in Apache Airflow, PySpark, and Snowflake.\nKnowledge of container orchestration (Eg: Kubernetes) is a plus",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'Consulting', 'HTML', 'Apache', 'Information technology', 'Monitoring', 'SQL', 'Ajax', 'Python']",2025-06-12 15:07:45
Associate- Referral - Decision Science / Data Science,Axtria,3 - 5 years,Not Disclosed,['Gurugram'],"Position Summary \n\nThis Requisition is for the Employee Referral Campaign.\n\nWe are seeking high-energy, driven, and innovative Data Scientists to join our Data Science Practice to develop new, specialized capabilities for Axtria, and to accelerate the company’s growth by supporting our clients’ commercial & clinical strategies.\n\n Job Responsibilities \n\nBe an Individual Contributor tothe Data Science team and solve real-world problems using cutting-edge capabilities and emerging technologies.\n\nHelp clients translate the business use cases they are trying to crack into data science solutions. Provide genuine assistance to users by advising them on how to leverage Dataiku DSS to implement data science projects, from design to production.\n\nData Source Configuration, Maintenance, Document and maintain work-instructions.\n\nDeep working onmachine learning frameworks such as TensorFlow, Caffe, Keras, SparkML\n\nExpert knowledge in Statistical and Probabilistic methods such as SVM, Decision-Trees, Clustering\n\nExpert knowledge of python data-science and math packages such as NumPy , Pandas, Sklearn\n\nProficiency in object-oriented languages (Java and/or Kotlin),Python and common machine learning frameworks(TensorFlow, NLTK, Stanford NLP, Ling Pipe etc\n\n\n Education \n\nBachelor Equivalent - Engineering\nMaster's Equivalent - Engineering\n\n Work Experience \n\nData Scientist 3-5 years of relevant experience in advanced statistical and mathematical models and predictive modeling using Python. Experience in the data science space prior relevant experience in Artificial intelligence and machine Learning algorithms for developing scalable models supervised and unsupervised techniques likeNLP and deep Learning Algorithms. Ability to build scalable models using Python, R-Studio, R Shiny, PySpark, Keras, and TensorFlow. Experience in delivering data science projects leveraging cloud infrastructure. Familiarity with cloud technology such as AWS / Azure and knowledge of AWS tools such as S3, EMR, EC2, Redshift, and Glue; viz tools like Tableau and Power BI. Relevant experience in Feature Engineering, Feature Selection, and Model Validation on Big Data. Knowledge of self-service analytics platforms such as Dataiku/ KNIME/ Alteryx will be an added advantage.\n\nML Ops Engineering 3-5 years of experience with MLOps Frameworks like Kubeflow, MLFlow, Data Robot, Airflow, etc., experience with Docker and Kubernetes, OpenShift. Prior experience in end-to-end automated ecosystems including, but not limited to, building data pipelines, developing & deploying scalable models, orchestration, scheduling, automation, and ML operations. Ability to design and implement cloud solutions and ability to build MLOps pipelines on cloud solutions (AWS, MS Azure, or GCP). Programming languages like Python, Go, Ruby, or Bash, a good understanding of Linux, knowledge of frameworks such as Keras, PyTorch, TensorFlow, etc. Ability to understand tools used by data scientists and experience with software development and test automation. Good understanding of advanced AI/ML algorithms & their applications.\n\nGen AI :Minimum of 4-6 years develop, test, and deploy Python based applications on Azure/AWS platforms.Must have basic knowledge on concepts of Generative AI / LLMs / GPT.Deep understanding of architecture and work experience on Web Technologies.Python, SQL hands-on experience.Expertise in any popular python web frameworks e.g. flask, Django etc. Familiarity with frontend technologies like HTML, JavaScript, REACT.Be an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT.Can interact with client on GenAI related capabilities and use cases.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'gpm', 'machine learning', 'python data', 'statistics', 'kubernetes', 'microsoft azure', 'numpy', 'javascript', 'sql', 'docker', 'pandas', 'tensorflow', 'java', 'django', 'predictive modeling', 'python web framework', 'mathematical modeling', 'pytorch', 'keras', 'aws', 'flask', 'advanced statistical']",2025-06-12 15:07:48
Director - Data Science,Axtria,12 - 17 years,Not Disclosed,['Noida'],"Minimum 12+ years of relevant experience in building software applications in data and analytics field\nEnhance the go-to-market strategy by designing new and relevant solution frameworks to accelerate our clients’ journeys for impacting patient outcomes. Pitch for these opportunities and craft winning proposals to grow the Data Science Practice.\nBuild and lead a team of data scientists and analysts, fostering a collaborative and innovative environment.\nOversee the design and delivery of the models, ensuring projects are completed on time and meet business objectives.\nEngaging in consultative selling with clients to grow/deliver business.\nDevelop and operationalize scalable processes to deliver on large & complex client engagements.\nExtensive hands-on experience with Python, R, or Julia, focusing on data science and generative AI frameworks.\nExpertise in working with generative models such as GPT, DALL-E, Stable Diffusion, Codex, and MidJourney for various applications.\nProficiency in fine-tuning and deploying generative models using libraries like Hugging Face Transformers, Diffusers, or PyTorch Lightning.\nStrong understanding of generative techniques, including GANs, VAEs, diffusion models, and autoregressive models.\nExperience in prompt engineering, zero-shot, and few-shot learning for optimizing generative AI outputs across different use cases.\nExpertise in managing generative AI data pipelines, including preprocessing large-scale multimodal datasets for text, image, or code generation.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['application software', 'python', 'artificial intelligence', 'r', 'julia', 'hive', 'natural language processing', 'neural networks', 'predictive analytics', 'machine learning', 'sql', 'deep learning', 'java', 'data science', 'spark', 'predictive modeling', 'pytorch', 'hadoop', 'statistics']",2025-06-12 15:07:50
GEN AI DevOps Engineer,Care Allianz,2 - 5 years,Not Disclosed,['Pune'],"Care Allianz is looking for GEN AI DevOps Engineer to join our dynamic team and embark on a rewarding career journey\nCollaborating with coworkers to conceptualize, develop, and release software\n\nConducting quality assurance to ensure that the software meets prescribed guidelines\n\nRolling out fixes and upgrades to software, as needed\n\nSecuring software to prevent security breaches and other vulnerabilities\n\nCollecting and reviewing customers' feedback to enhance user experience\n\nSuggesting alterations to workflow in order to improve efficiency and success\n\nPitching ideas for projects based on gaps in the market and technological advancements",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['continuous integration', 'kubernetes', 'cd', 'nexus', 'github', 'configuration', 'nagios', 'sonarqube', 'maven', 'ci/cd', 'sonar', 'docker', 'ansible', 'continuous delivery', 'puppet', 'git', 'devops', 'linux', 'jenkins', 'terraform', 'shell scripting', 'aws']",2025-06-12 15:07:53
"Platform Dev & Support Engineer (Python, NoSQL, Devops)",Qualcomm,3 - 7 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Software Developer\n\nGeneral Summary:\n\nQualcomm EDAAP (Engineering Solutions and AIML) team is seeking an experienced develop and support scalable Machine learning platform. The ideal candidate will have a strong background in building and operating distributed systems, with expertise in Rust, Python, Kubernetes, and Linux. You will play a critical role in developing, supporting and debugging our Generative AI platforms.Experience:3 to 7 years of experience strong knowledge of Python or Rust, NoSQL (Mongo/Redis), working experience of developing/supporting large scale end user facing applications.Responsibilities\nDevelop, Debug and support end to end components of large-scale Generative AI platform.\nSet up and operate Kubernetes clusters for efficient deployment and management of containerized applications\nImplement distributed microservices architecture to enable scalable and fault-tolerant inference pipelines\nEnsure optimal performance, security, and reliability of inference platforms, leveraging expertise in Linux, networking, servers, and data centers\nDevelop and maintain scripts and tools for automating deployment, monitoring, and maintenance tasks\nTroubleshoot issues and optimize system performance, using knowledge of data structures and algorithms\nWork closely with users to debug issues and address performance and scalability issues.\nParticipate in code reviews, contributing to the improvement of the overall code quality and best practices/Skills\n3 to 7 years of experience in software development, with a focus on building scalable and distributed systems\nProficiency in Rust and Python programming languages, with experience in developing high-performance applications\nExperience setting up and operating Kubernetes clusters, including deployment, scaling, and management of containerized applications\nStrong understanding of distributed microservices architecture and its application in large-scale systems\nExcellent knowledge of Linux, including shell scripting, package management, and system administration\nGood understanding of networking fundamentals, including protocols, architectures, and network security\nFamiliarity with data structures and algorithms, including trade-offs and optimization techniques\nExperience debugging complex production issues in large scale application platforms.\nExperience working with cloud-native technologies, such as containers, orchestration, and service meshes\nStrong problem-solving skills, with the ability to debug complex issues and optimize system performance\nExcellent communication and collaboration skills, with experience working with cross-functional teams and customers\n\nMinimum Qualifications:\n3+ years of IT-relevant work experience with a Bachelor's degree in a technical field (e.g., Computer Engineering, Computer Science, Information Systems).\nOR\n5+ years of IT-relevant work experience without a Bachelors degree.\n\n3+ years of any combination of academic or work experience with Full-stack Application Development (e.g., Java, Python, JavaScript, etc.)\n1+ year of any combination of academic or work experience with Data Structures, algorithms, and data stores.\nDevelop, Debug and support end to end components of large-scale Generative AI platform.\nSet up and operate Kubernetes clusters for efficient deployment and management of containerized applications\nImplement distributed microservices architecture to enable scalable and fault-tolerant inference pipelines\nEnsure optimal performance, security, and reliability of inference platforms, leveraging expertise in Linux, networking, servers, and data centers\nDevelop and maintain scripts and tools for automating deployment, monitoring, and maintenance tasks\nTroubleshoot issues and optimize system performance, using knowledge of data structures and algorithms\nWork closely with users to debug issues and address performance and scalability issues.\nParticipate in code reviews, contributing to the improvement of the overall code quality and best practices\n\n3 to 7 years of experience in software development, with a focus on building scalable and distributed systems\nProficiency in Rust and Python programming languages, with experience in developing high-performance applications\nExperience setting up and operating Kubernetes clusters, including deployment, scaling, and management of containerized applications\nStrong understanding of distributed microservices architecture and its application in large-scale systems\nExcellent knowledge of Linux, including shell scripting, package management, and system administration\nGood understanding of networking fundamentals, including protocols, architectures, and network security\nFamiliarity with data structures and algorithms, including trade-offs and optimization techniques\nExperience debugging complex production issues in large scale application platforms.\nExperience working with cloud-native technologies, such as containers, orchestration, and service meshes\nStrong problem-solving skills, with the ability to debug complex issues and optimize system performance\nExcellent communication and collaboration skills, with experience working with cross-functional teams and customers\n\nBachelors (Engineering) or Masters",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['kubernetes', 'python', 'networking', 'linux', 'shell scripting', 'container', 'algorithms', 'css', 'software development', 'javascript', 'redis', 'microservices', 'nosql', 'application development', 'docker', 'ansible', 'rust', 'system administration', 'java', 'devops', 'data structures', 'html', 'aws', 'mongodb']",2025-06-12 15:07:55
Associate Director - Data Science,Axtria,10 - 15 years,Not Disclosed,['Noida'],"Minimum of 10+ years in development, testing and deployment of React, JavaScript, Python based applications on Azure/AWS platforms\nExtensive experience with frontend and backend technologies to develop AI/GenAI applications.\nSoftware development experience in REACT, JavaScript/TypeScript, Python, FastAPI/Flask/Django is needed for UI based applications.\nMust have experience in building production grade application with frontend & backend technologies like HTML, JavaScript, REACT, Python etc.\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'production', 'javascript', 'react.js', 'python web framework', 'software testing', 'natural language processing', 'neural networks', 'microsoft azure', 'aws stack', 'machine learning', 'sql', 'deep learning', 'django', 'data science', 'html', 'typescript', 'flask', 'aws']",2025-06-12 15:07:58
Python Engineer - ML/Big Query - Hyd/Chennai/Bangalore,People staffing Solutions,5 - 10 years,12-20 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Key Responsibilities:\nDesign, develop, and maintain scalable and optimized ETL pipelines using Python and SQL.\nWork with Google BigQuery and other cloud-based platforms to build data warehousing solutions.\nDevelop and deploy ML models; collaborate with Data Scientists for productionizing models.\nWrite efficient and optimized SQL queries for large-scale data processing.\nBuild APIs using Flask/Django for machine learning and data applications.\nWork with both SQL and NoSQL databases including Elasticsearch.\nImplement data ingestion using batch and streaming technologies.\nEnsure data quality, integrity, and governance across the data lifecycle.\nAutomate and optimize CI/CD pipelines for data solutions.\nCollaborate with cross-functional teams to gather data requirements and deliver solutions.\nTroubleshoot and monitor data pipelines for seamless operations.\nRequired Skills & Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or related field.\n5+ years of experience with Python in a data engineering and/or ML context.\nStrong hands-on experience with SQL, BigQuery, and cloud data platforms (preferably GCP).\nPractical knowledge of ML concepts and experience developing ML models.\nProficiency in frameworks such as Flask and Django.\nExperience with NoSQL databases and data streaming technologies.\nSolid understanding of data modeling, warehousing, and ETL frameworks.\nFamiliarity with CI/CD tools and automation best practices.\nExcellent communication, problem-solving, and collaboration skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Machine Learning', 'Python', 'SQL', 'Pandas', 'Numpy', 'Ml', 'Flask']",2025-06-12 15:08:00
AI Engineer - Manager,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for someone who is ready for the next step in their career and is excited by the idea of solving problems and designing best in class. However, they also need to be aware of the practicalities of making a difference in the real world - whilst we love innovative advanced solutions, we also believe that sometimes a simple solution can have the most impact.\nOur AI Engineer is someone who feels the most comfortable around solving problems, answering questions and proposing solutions. We place a high value on the ability to communicate and translate complex analytical thinking into non-technical and commercially oriented concepts, and experience working on difficult projects and/or with demanding stakeholders is always appreciated.\n\nWhat can you expect from the role?\nOwn tasks end-to-end and lead on project delivery and project governance\nManagement of AI Engineer(s)\nPreparing and presenting data driven solutions to stakeholders\nDesign, develop, deploy and maintain AI solutions.\nUse a variety of AI Engineering tools and methods to deliver\nContributing to solutions design and proposal submissions\nSupporting the development of the AI engineering team within Blend\nMaintain in-depth knowledge of AI ecosystems and trends\nMentor junior colleagues\nContributing to proposal submissions and business development initiatives under the direction of the Leadership team\n\n\nQualifications\nProven ability to design, develop, test, deploy, maintain, and improve robust, scalable, and reliable software systems following best practices.\nExpertise in Python pro",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Version control', 'orchestration', 'GCP', 'Analytical', 'System integration', 'Software development life cycle', 'Project delivery', 'Operations', 'Monitoring', 'Python']",2025-06-12 15:08:02
Senior Associate - Data Science,Axtria,3 - 8 years,Not Disclosed,['Noida'],"Job Summary-\nData Scientist with good hands-on experience of 3+ years in developing state of the art and scalable Machine Learning models and their operationalization, leveraging off-the-shelf workbench production.\n\nJob Responsibilities-\n\n1. Hands on experience in Python data-science and math packages such as NumPy, Pandas, Sklearn, Seaborn, PyCaret, Matplotlib\n2. Proficiency in Python and common Machine Learning frameworks (TensorFlow, NLTK, Stanford NLP, PyTorch, Ling Pipe, Caffe, Keras, SparkML and OpenAI etc.)\n3. Experience of working in large teams and using collaboration tools like GIT, Jira and Confluence\n4. Good understanding of any of the cloud platform - AWS, Azure or GCP\n5. Understanding of Commercial Pharma landscape and Patient Data / Analytics would be a huge plus\n6. Should have an attitude of willingness to learn, accepting the challenging environment and confidence in delivering the results within timelines. Should be inclined towards self motivation and self-driven to find solutions for problems.\n7. Should be able to mentor and guide mid to large sized teams under him/her\n\nJob -\n1. Strong experience on Spark with Scala/Python/Java\n2. Strong proficiency in building/training/evaluating state of the art machine learning models and its deployment\n3. Proficiency in Statistical and Probabilistic methods such as SVM, Decision-Trees, Bagging and Boosting Techniques, Clustering\n4. Proficiency in Core NLP techniques like Text Classification, Named Entity Recognition (NER), Topic Modeling, Sentiment Analysis, etc. Understanding of Generative AI / Large Language Models / Transformers would be a plus",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'java', 'spark', 'machine learning algorithms', 'python', 'confluence', 'scikit-learn', 'nltk', 'training', 'numpy', 'tensorflow', 'git', 'seaborn', 'gcp', 'pytorch', 'keras', 'spark mllib', 'jira', 'sentiment analysis', 'lingpipe', 'caffe', 'microsoft azure', 'pandas', 'matplotlib', 'aws', 'statistics']",2025-06-12 15:08:05
GCP Adobe Engineer II,AMERICAN EXPRESS,6 - 9 years,Not Disclosed,['Chennai'],"We are looking for energetic, high-performing and highly skilled Engineers to help shape our technology and product roadmap. You will be part of the fast-paced, entrepreneurial Enterprise Personalization Portfolio focused on delivering the next generation of digital global marketing capabilities. Under Personalization portfolio, GCT team is responsible for Global Campaign Tracking of new accounts acquisition and bounty payments and leverages transformational technologies, such as Adobe Analytics, Google Analytics, SQL, GCP, Big Query, Spark & Java.\n  Focus",,,,"['Unix', 'Data analysis', 'Google Analytics', 'Coding', 'Finance', 'Data processing', 'Data mining', 'Digital marketing', 'Monitoring', 'SQL']",2025-06-12 15:08:07
Senior Associate - Data Science,Axtria,2 - 5 years,Not Disclosed,['Noida'],"Be an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT\nSoftware development experience in python is needed as backend for UI based applications\nBe a part of large delivery teams working on advanced projects when expert assistance is required.\nDeliver advanced Data Science capabilities to businesses in a meaningful manner through successful proof-of-concept solutions, and later smoothly transition the proof-of-concept into production.\nCreate Technical documents, develop, test, and deploy data analytics processes using Python, SQL on Azure/AWS platforms\nCan interact with client on GenAI related capabilities and use cases\n\n\nMust have\n\nSkills:\n\n\nMinimum of 3-5years develop, test, and deploy Python based applications on Azure/AWS platforms\nMust have basic knowledge on concepts of Generative AI / LLMs / GPT\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.\nFamiliarity with frontend technologies like HTML, JavaScript, REACT",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'software testing', 'gpm', 'microsoft azure', 'python web framework', 'data analytics', 'neural networks', 'aws stack', 'machine learning', 'javascript', 'artificial intelligence', 'sql', 'react.js', 'deep learning', 'django', 'data science', 'html', 'flask', 'aws']",2025-06-12 15:08:10
Manager - Data Science,Axtria,6 - 11 years,Not Disclosed,['Noida'],"Job Summary-\n\nData Scientist with good hands-on experience of 6+ years in developing state of the art and scalable Machine Learning models and their operationalization, leveraging off-the-shelf workbench production.\n\nJob Responsibilities-\n\nHands on experience in Python data-science and math packages such as NumPy, Pandas, Sklearn, Seaborn, PyCaret, Matplotlib\nProficiency in Python and common Machine Learning frameworks (TensorFlow, NLTK, Stanford NLP, PyTorch, Ling Pipe, Caffe, Keras, SparkML and OpenAI etc.)\nExperience of working in large teams and using collaboration tools like GIT, Jira and Confluence\nGood understanding of any of the cloud platform – AWS, Azure or GCP\nUnderstanding of Commercial Pharma landscape and Patient Data / Analytics would be a huge plus\nShould have an attitude of willingness to learn, accepting the challenging environment and confidence in delivering the results within timelines. Should be inclined towards self motivation and self-driven to find solutions for problems.\nShould be able to mentor and guide mid to large sized teams under him/her\n\n\nJob -\nStrong experience on Spark with Scala/Python/Java\nStrong proficiency in building/training/evaluating state of the art machine learning models and its deployment\nProficiency in Statistical and Probabilistic methods such as SVM, Decision-Trees, Bagging and Boosting Techniques, Clustering\nProficiency in Core NLP techniques like Text Classification, Named Entity Recognition (NER), Topic Modeling, Sentiment Analysis, etc. Understanding of Generative AI / Large Language Models / Transformers would be a plus",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'java', 'spark', 'machine learning algorithms', 'python', 'confluence', 'scikit-learn', 'nltk', 'training', 'numpy', 'tensorflow', 'git', 'seaborn', 'gcp', 'pytorch', 'keras', 'spark mllib', 'jira', 'sentiment analysis', 'lingpipe', 'caffe', 'microsoft azure', 'pandas', 'matplotlib', 'aws', 'statistics']",2025-06-12 15:08:12
Senior Associate - Data Science,Axtria,4 - 6 years,Not Disclosed,['Noida'],"o Minimum of 4-6 years' experience in developing, testing, and deploying Python based applications on Azure/AWS platforms\no Must have basic knowledge on concepts of Generative AI / LLMs / GPT\no Deep understanding of architecture and work experience on Web Technologies\no Python, SQL hands-on experience\no Expertise in any popular python web frameworks e.g. flask, Django etc.\no Familiarity with frontend technologies like HTML, JavaScript, REACT",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'gpm', 'django', 'python web framework', 'flask', 'natural language processing', 'neural networks', 'microsoft azure', 'object detection', 'machine learning', 'javascript', 'artificial intelligence', 'sql', 'react.js', 'deep learning', 'data science', 'computer vision', 'html', 'aws']",2025-06-12 15:08:14
Senior Lead business execution consultant,Wells Fargo,7 - 12 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Senior Lead business execution consultant\n\nIn this role, you will:\nAct as a Business Execution advisor to leadership to drive performance and initiatives, and develop and implement information delivery or presentations to key stakeholders and senior management",,,,"['Business execution', 'Business Implementation', 'Data Engineering', 'NLP', 'generative AI', 'Data Mining', 'machine learning', 'Strategic Planning', 'agentic AI']",2025-06-12 15:08:17
Lead ML Ops Engineer with GCP,TVS Next,8 - 10 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['hive', 'kubernetes', 'data pipeline', 'sql', 'docker', 'tensorflow', 'java', 'product management', 'gcp', 'spark', 'pytorch', 'bigquery', 'hadoop', 'big data', 'programming', 'hbase', 'ml', 'cloud sql', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'machine learning', 'data engineering', 'ops', 'mapreduce', 'kafka', 'cloud storage', 'hdfs', 'bigtable', 'aws']",2025-06-12 15:08:19
Artificial Intelligence Architect,Emerson,10 - 20 years,Not Disclosed,['Pune'],"Role & responsibilities\nDesign robust and scalable AI/ML architectures that support the development and deployment of machine learning models and AI solutions.\nDevelop and guide the implementation of end-to-end AI/ML solutions, including model development, data processing, and system integration.\nEvaluate and recommend the latest AI/ML technologies, frameworks, and tools to enhance system capabilities and performance.\nCollaborate with software engineers and other development teams to integrate AI/ML solutions into existing systems and applications. Ensure seamless operation and performance.\nWork with cross-functional teams, including developers, data scientists, machine learning engineers, and business stakeholders, to understand requirements and design solutions that align with business objectives.\n\nPreferred candidate profile\nBachelors degree in computer science, Data Science, Statistics, or a related field or a master's degree or higher is preferred.\nMore than 3 years of experience in designing and implementing AI/ML architectures, with a proven track record of successful projects.\nExtensive experience with machine learning frameworks (e.g., Go, TensorFlow, PyTorch), programming languages C#, .Net, NodeJS and data processing tools.\nStrong understanding of system architecture principles, including distributed systems, microservices, and cloud computing.\nExperience with Microsoft Azure cloud services and their AI/ML offerings\nExperience with event-handling systems such as Kafka\nExperience with big data technologies and data engineering practices.\nExcellent verbal and written communication skills, with the ability to convey complex technical concepts to non-technical stakeholders.",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Ml', 'Python', 'Tensorflow', 'Pytorch', 'Architecture', 'Artificial Intelligence', '.Net', 'Machine Learning', 'Scikit-Learn']",2025-06-12 15:08:22
Freelance AI Agent Assistant,Mindrift,0 - 4 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","At Mindrift , innovation meets opportunity. We believe in using the power of collective intelligence to ethically shape the future of AI.\nWhat we do\nThe Mindrift platform connects specialists with AI projects from major tech innovators. Our mission is to unlock the potential of Generative AI by tapping into real-world expertise from across the globe.\nAbout the Role\nIf you re a professional who works with AI Data Annotation and friendly user of LLMs, Mindrift offers a unique opportunity to apply your editing, annotating, fact-checking and creative skills to an AI training project.\nThis is a freelance role for a project, and your typical tasks may include:\nConduct high-quality web searches to verify facts, gather supporting data, and cross-check AI responses.\nPerform fact-checking and intent verification to ensure AI responses align with the users goals.\nCarefully review and flag any inaccuracies, inconsistencies, or irrelevant answers.\nProvide structured feedback on AI-generated content to help improve model performance.\nWork effectively with large language models (LLMs), understanding their capabilities and limitations, and applying best practices when interacting with them.\nPrompt generation with a purpose to receive the best quality result of LLMs.\nHow to get started\nSimply apply to this post, qualify, and get the chance to contribute to projects aligned with your skills, on your own schedule. From creating training prompts to refining model responses, you ll help shape the future of AI while ensuring technology benefits everyone.\n\n\nYou are currently enrolled in or completed a Bachelors degree or higher.\nYou have professional and/or educational experience in data annotation, demonstrate a deeper-than-user-level interest in AI, and possess intellectual b",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Training', 'Web technologies', 'Manager Technology', 'Fact']",2025-06-12 15:08:24
Python and Machine Learning Programmer,Panacorp Software Solutions,1 - 3 years,Not Disclosed,['Nagercoil'],"Job Overview:\nWe are looking for a skilled Python and Data Science Programmer to develop and implement data-driven solutions. The ideal candidate should have strong expertise in Python, machine learning, data analysis, and statistical modeling.\n\nKey Responsibilities:\nData Analysis & Processing: Collect, clean, and preprocess large datasets for analysis.\nMachine Learning: Build, train, and optimize machine learning models for predictive analytics.\nAlgorithm Development: Implement data science algorithms and statistical models for problem-solving.\nAutomation & Scripting: Develop Python scripts and automation tools for data processing and reporting.\nData Visualization: Create dashboards and visual reports using Matplotlib, Seaborn, Plotly, or Power BI/Tableau.\nDatabase Management: Work with SQL and NoSQL databases for data retrieval and storage.\nCollaboration: Work with cross-functional teams, including data engineers, business analysts, and software developers.\nResearch & Innovation: Stay updated with the latest trends in AI, ML, and data science to improve existing models.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'NoSQL', 'Power BI', 'Database Management', 'Plotly', 'Data Analysis', 'Seaborn', 'Tableau', 'SQL']",2025-06-12 15:08:27
Python and Machine Learning Programmer,Panacorp Software Solutions,1 - 3 years,Not Disclosed,['Nagercoil'],"Job Overview:\nWe are looking for a skilled Python and Data Science Programmer to develop and implement data-driven solutions. The ideal candidate should have strong expertise in Python, machine learning, data analysis, and statistical modeling.\n\nKey Responsibilities:\nData Analysis & Processing: Collect, clean, and preprocess large datasets for analysis.\nMachine Learning: Build, train, and optimize machine learning models for predictive analytics.\nAlgorithm Development: Implement data science algorithms and statistical models for problem-solving.\nAutomation & Scripting: Develop Python scripts and automation tools for data processing and reporting.\nData Visualization: Create dashboards and visual reports using Matplotlib, Seaborn, Plotly, or Power BI/Tableau.\nDatabase Management: Work with SQL and NoSQL databases for data retrieval and storage.\nCollaboration: Work with cross-functional teams, including data engineers, business analysts, and software developers.\nResearch & Innovation: Stay updated with the latest trends in AI, ML, and data science to improve existing models.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'tableau', 'data analysis', 'data science', 'predictive analytics', 'statistical modeling', 'machine learning', 'sql', 'nosql']",2025-06-12 15:08:29
Artificial Intelligence Engineer,Infosys,5 - 10 years,5-10 Lacs P.A.,['Pune'],"Our client INFOSYS is looking for Artificial Intelligence Engineer position with 5+ years of experience in Pune location. CONTRACT TO HIRE AND WORK FROM OFFICE\n\nJob Description :\nMandatory Skills : Python + LLMs + AI + Azure Certified.\nole Definition:\nThis is a specialized role for an AI Software Engineers design, build, and deploy scalable AI models and systems. They work with machine learning frameworks, cloud platforms, and data engineering tools to create and optimize AI solutions.\nSkills:\nProficient:\nLanguages/Framework: Fast API, Azure UI Search API (React)\nCloud: Azure Cloud Basics (Azure DevOps)\nGitlab: Gitlab Pipeline\nAnsible and REX: Rex Deployment\nData Science: Prompt Engineering + Modern Testing\nData pipeline development\nUnderstanding of AI/ML algorithms and their applications\nMLOps frameworks\nKnowledge of cloud platforms (Azure ML especially)\nModel deployment process\no             Data pipeline monitoring\n              Expert: (in addition to proficient skills)\no             Languages/Framework: Azure Open AI\no             Data Science: Open AI GPT Family of models 4o/4/3, Embeddings + Vector Search\no             Databases and ETL: Azure Storage Account, Postgresql, Cosmos\no             Experience with ML frameworks (TensorFlow, PyTorch, Scikit-learn)\no             Knowledge of cloud platforms (AWS SageMaker, Google AI Platform)\no             Expertise in data preprocessing, feature engineering, and model evaluation\no             Understanding of software engineering principles (version control, CI/CD, containerization)\no             Familiarity with distributed computing and big data tools (Spark, Hadoop)\no             Ability to optimize models for performance and scalability\no             Experience with Azure AI Search",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Temporary/Contractual","['llm', 'Python', 'Artificial Intelligence']",2025-06-12 15:08:31
Data Science Analyst (Standard),Infogain,3 - 5 years,Not Disclosed,['Gurugram'],"- Strong understanding of ML algorithms (regression, classification, clustering) with the ability to independently develop and scale models using Python with minimal supervision\n- Experience in commercial analytics with a knack for translating business problems into analytical solutions and strategic recommendations.\n- Proficient in Power BI to build intuitive dashboards and deliver insights in a clear, actionable format.\n  SKILLS\nPrimary Skill: Data Science\nSub Skill(s): Data Science\nAdditional Skill(s): Python, Data Science",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analyst', 'Translation', 'data science', 'Analytical', 'power bi', 'Analytics', 'Supervision', 'Python']",2025-06-12 15:08:34
AI Test Lead,Naukri,8 - 13 years,20-32.5 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nKey Responsibilities:\nAI Testing Strategy and Planning\nCollaborate with cross-functional teams to develop comprehensive AI testing strategies and plans for AI-powered applications.\nWork closely with product managers, data scientists, and developers to understand AI model requirements, use cases, and project goals.\nDefine the scope and objectives of AI testing efforts, including performance, accuracy, bias detection, and robustness of AI models. Test Execution for AI Models and Algorithms\nDesign, develop, and execute test cases for AI systems and models (including machine learning and deep learning algorithms).\nTest and validate AI solutions across various stages of the development lifecycle, including model training, testing, and deployment.\nEnsure that AI models meet business requirements and perform accurately under various real-world conditions.\nEvaluate the performance of AI models by assessing speed, efficiency, scalability, and resource utilization.\nPerform manual and automated testing on AI-based applications, platforms, and solutions.\nAI Model Accuracy and Validation\nTest AI models for accuracy, precision, recall, F1 score, and other performance metrics.\nEnsure AI models' fairness by conducting tests for potential bias in decisionmaking processes, especially in clinical or medical applications.\nValidate AI model predictions against real-world data, ensuring that results are consistent, reliable, and actionable. Also, need to look the test results from a business perspective and help evaluate the balance between risks and benefits.\nCollaboration and Knowledge Sharing\nWork with data scientists, AI engineers and Test Manager to improve testing methodologies and continuously optimize AI model testing processes.\nProvide feedback on AI models, pointing out any potential improvements in testing coverage or areas for model retraining.\nCommunicate findings, bugs, and issues related to AI models to technical teams, ensuring prompt resolution.\nHelp the team set up AI Testing standards, make informed decisions, and build knowledge across projects\nHelp the team in decision-making processes, such as whether to continue or stop investments based on testing results. Test Automation for AI Projects\nDevelop and implement automated testing scripts and frameworks specifically designed for AI applications.\nUtilize AI testing tools and frameworks (RAGAS etc.) to automate the validation of AI models and algorithms.\nIntegrate automated AI testing within continuous integration and continuous deployment (CI/CD) pipelines.\nCompliance and Regulatory Testing\nEnsure that AI applications comply with industry-specific regulations, especially in the pharma and healthcare sectors (e.g., FDA regulations, HIPAA compliance).\nVerify that all AI-driven processes adhere to ethical standards and data privacy laws.\nContinuous Improvement and Research\nStay up-to-date with the latest trends, tools, and techniques in AI testing and apply these advancements to optimize the testing process.\nParticipate in AI testing forums and workshops, contributing insights to improve best practices within the team. Reporting and Documentation\nDocument test results, methodologies, and issues clearly, providing insights into test coverage, risk analysis, and performance benchmarks.\nPrepare detailed reports for both technical and non-technical stakeholders, summarizing testing outcomes and potential risks associated with AI implementations.\nAssist in the creation and maintenance of knowledge-sharing platforms related to AI testing best practices.\nKey Skills and Qualifications:\nTechnical Expertise\nStrong knowledge of AI/ML testing methodologies and best practices.\nExperience with any AI development frameworks and libraries such as TensorFlow, Keras, PyTorch, scikit-learn, RAGAS and MLlib.\nExperience in testing tools and environments for AI-based systems (e.g., Jupyter Notebooks, Apache Spark, and DataRobot).\nExperience with performance testing tools like Grafana K6 and JMeter for AI solutions.\nKnowledge of Python (Must to have), R, JavaScript or other programming languages frequently used in AI/ML.\nKnowledge of cloud technologies like Microsoft Azure / AWS.\nUnderstanding of test automation frameworks and experience in tools like Cypress, Playwright and Pytest for automating AI tests. AI Model Evaluation\nSolid understanding of machine learning and deep learning models, including supervised and unsupervised learning techniques.\nFamiliarity with evaluating AI models on metrics such as accuracy, precision, recall, F1 score, confusion matrices, and AUC.\nAbility to identify and test for model biases, fairness, and ethical implications, especially in sensitive applications like healthcare and pharma. Analytical and Problem-Solving Skills\nStrong problem-solving abilities and keen attention to detail, with a systematic approach to diagnosing and resolving AI-related issues.\nAbility to perform root cause analysis of issues in AI algorithms and suggest actionable fixes.\nCollaboration and Communication\nExcellent teamwork and communication skills, with the ability to collaborate with cross-functional teams, including data scientists, engineers, and product managers.\nStrong verbal and written communication skills to convey technical information clearly and concisely to both technical and non-technical stakeholders.\nExperience\nMinimum of 8+ years experience in software testing, with at least 2 years focused on testing AI/ML models or AI-based applications.\nProven experience in testing AI/ML algorithms in production or staging environments.\nExperience in testing Visual AI Assistant Applications is good to have.\nExperience working in a regulated industry (such as pharmaceuticals or healthcare) is a plus.\nPreferred Qualifications:\nExperience with cloud platforms (e.g., AWS, Azure) for deploying AI applications and models. Certification in AWS/Azure will be good to have.\nFamiliarity with DevOps practices and integrating AI testing into CI/CD pipelines.\nCertification in AI/ML or related testing frameworks (e.g. ISTQB AI Tester)\nThis AI Tester role is a unique opportunity to shape the future of AI in the pharmaceutical industry. If youre passionate about AI, testing, and making a difference in healthcare, we encourage you to apply.\n\nPreferred candidate profile",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation Testing', 'AI/ ML', 'Python', 'Performance Testing', 'Automation Strategy', 'AI Framework', 'AI testing']",2025-06-12 15:08:36
Data Science Analyst (Senior),Infogain,6 - 8 years,Not Disclosed,['Gurugram'],"- Strong understanding of ML algorithms (regression, classification, clustering) with the ability to independently develop and scale models using Python.\n- Experience in commercial analytics with a knack for translating business problems into analytical solutions and strategic recommendations.\n- Proficient in Power BI to build intuitive dashboards and deliver insights in a clear, actionable format.\n- Strong storytelling and communication skills to convey complex analytical findings to both technical and non-technical audiences.\n- Demonstrates ownership and leadership, driving projects from idea to impact with minimal supervision.\nSKILLS\nPrimary Skill: Data Science\nSub Skill(s): Data Science\nAdditional Skill(s): Python, Data Science",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analyst', 'Translation', 'data science', 'Analytical', 'power bi', 'Analytics', 'Supervision', 'Python']",2025-06-12 15:08:38
Information Security Engineer,BMC Software,4 - 9 years,Not Disclosed,['Pune'],"In this role, your primary responsibilities include implementing, configuring, and supporting application security and identity access management technology solutions including generate reports and threat identification\nThe candidate needs to have experience in application security and identity management area.\n\n\nHere is how, through this exciting role, YOU will contribute to BMC's and your own success:\n\nResponsible for developing and maintaining application security and identify access management technology solutions including Sailpoint/IIQ, Okta Single Sign On, Azure AD, AWS SSO, Cloudflare Web application firewall, penetration testing, developing and maintaining internally developed Python tools and utilities.\nIdentify and develop integration opportunities between security solutions and automation but not exclusively.\nWork with virtual team/management to collect and prioritize system requirements, develop delivery plans and meet aggressive deadlines, develop code, perform unit as well as system integration testing, participate in architecture of new capabilities and debug, troubleshoot production support.\nCoordinate quality assurance and testing with users of the new functionalities/capabilities.\nGenerate reports for capability implementation\nReview report data and identify threats to discuss with management for mitigation\nEnsure that project issues are communicated in a timely and effective manner\nOther duties as assigned.\n\nTo ensure you re set up for success, you will bring the following skillset & experience:\n\nExperiences in Sailpoint IIQ, Python and Java development (automation, integration, etc) and application security are must-have.\nFamiliar with security tools in software development lifecycle as well as Azure AD, AWS APIs/CLI, containers experiences are nice to have.\nKnowledge of Artificial Intelligence learning model\nAbility to work with little supervision as well as being a team player\nExcellent verbal, written, and interpersonal communication skills\nExperience working with remote teams\n4 years + experience\nShould be willing to work in 12.30 PM to 9.30 PM shift",Industry Type: Software Product,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Information Security', 'python', 'java', 'application security', 'sailpoint', 'java development', 'aws', 'artificial intelligence']",2025-06-12 15:08:40
Principal Architect (Data and Cloud),Neoware Technology Solutions,10 - 15 years,Not Disclosed,"['Chennai', 'Bengaluru']","Principal Architect (Data and Cloud) - Neoware Technology Solutions Private Limited Principal Architect (Data and Cloud)\nRequirements\nMore than 10 years of experience in Technical, Solutioning, and Analytical roles.\n5+ years of experience in building and managing Data Lakes, Data Warehouse, Data Integration, Data Migration and Business Intelligence/Artificial Intelligence solutions on Cloud (GCP/AWS/Azure).\nAbility to understand business requirements, translate them into functional and non-functional areas, define non-functional boundaries in terms of Availability, Scalability, Performance, Security, Resilience etc.\nExperience in architecting, designing, and implementing end to end data pipelines and data integration solutions for varied structured and unstructured data sources and targets.\nExperience of having worked in distributed computing and enterprise environments like Hadoop, GCP/AWS/Azure Cloud.\nWell versed with various Data Integration, and ETL technologies on Cloud like Spark, Pyspark/Scala, Dataflow, DataProc, EMR, etc. on various Cloud.\nExperience of having worked with traditional ETL tools like Informatica / DataStage / OWB / Talend , etc.\nDeep knowledge of one or more Cloud and On-Premise Databases like Cloud SQL, Cloud Spanner, Big Table, RDS, Aurora, DynamoDB, Oracle, Teradata, MySQL, DB2, SQL Server, etc.\nExposure to any of the No-SQL databases like Mongo dB, CouchDB, Cassandra, Graph dB, etc.\nExperience in architecting and designing scalable data warehouse solutions on cloud on Big Query or Redshift.\nExperience in having worked on one or more data integration, storage, and data pipeline tool sets like S3, Cloud Storage, Athena, Glue, Sqoop, Flume, Hive, Kafka, Pub-Sub, Kinesis, Dataflow, DataProc, Airflow, Composer, Spark SQL, Presto, EMRFS, etc.\nPreferred experience of having worked on Machine Learning Frameworks like TensorFlow, Pytorch, etc.\nGood understanding of Cloud solutions for Iaas, PaaS, SaaS, Containers and Microservices Architecture and Design.\nAbility to compare products and tools across technology stacks on Google, AWS, and Azure Cloud.\nGood understanding of BI Reporting and Dashboarding and one or more tool sets associated with it like Looker, Tableau, Power BI, SAP BO, Cognos, Superset, etc.\nUnderstanding of Security features and Policies in one or more Cloud environments like GCP/AWS/Azure.\nExperience of having worked in business transformation projects for movement of On-Premise data solutions to Clouds like GCP/AWS/Azure.\nBe a trusted technical advisor to customers and solutions for complex Cloud & Data related technical challenges.\nBe a thought leader in architecture design and development of cloud data analytics solutions.\nLiaison with internal and external stakeholders to design optimized data analytics solutions.\nPartner with SMEs and Solutions Architects from leading cloud providers to present solutions to customers.\nSupport Sales and GTM teams from a technical perspective in building proposals and SOWs.\nLead discovery and design workshops with potential customers across the globe.\nDesign and deliver thought leadership webinars and tech talks alongside customers and partners.\nResponsibilities\nLead multiple data engagements on GCP Cloud for data lakes, data engineering, data migration, data warehouse, and business intelligence.\nInterface with multiple stakeholders within IT and business to understand the data requirements.\nTake complete responsibility for the successful delivery of all allocated projects on the parameters of Schedule, Quality, and Customer Satisfaction.\nResponsible for design and development of distributed, high volume multi-thread batch, real-time, and event processing systems.\nImplement processes and systems to validate data, monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.\nWork with the Pre-Sales team on RFP, RFIs and help them by creating solutions for data.\nMentor young Talent within the Team, Define and track their growth parameters.\nContribute to building Assets and Accelerators.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Db2', 'Cognos', 'MySQL', 'Datastage', 'Presales', 'Informatica', 'Oracle', 'Teradata', 'Business intelligence', 'SQL']",2025-06-12 15:08:43
Senior Software Engineer,Dynamic Yield,5 - 10 years,Not Disclosed,"['Pune', 'Gurugram']","Our Purpose\nTitle and Summary\nSenior Software Engineer\nWhat is Mastercard?\n\nMastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.\nOur decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.\n\nOur Team:\n\nThe AI & DPE team is responsible for product decisioning management and innovative product development under services business unit to address the evolving risk and security needs of all of Mastercard s various customer segments. AI & DPE team focuses on defining the strategic direction for underlying platforms to enable the successful implementation of real-time, data-driven innovative products and services focused on network, security, fraud, digital identity and authentication. The team is responsible to look across all the products/services to drive efficiency, re-usability and increase speed to market for our products and services.\n\nThe candidate for this position will focus on driving actionable insights and build innovative solutions out of multiple data sources using analytics, machine learning and reporting capabilities.\n\nThe Role:\n\nWe are seeking a Senior Software Engineer who will:\nPartner with various teams (i.e., Product Manager, Data Science, Platform Strategy, Technology) on requirement gathering in order to deliver analytics solutions that generate business value\n\nPerform data preparation by ingestion, aggregation, processing to drive and enable relevant insights from available data sets\n\nIdentify and code the best suited data algorithm model for the relevant insights\n\nManipulate and analyze complex, high-volume, high-dimensionality data from varying sources using a variety of tools and data analysis techniques.\n\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions\n\nCollect and synthesize feedback from clients, development, product and sales teams for new solutions or product enhancements\n\nAll about You\n\nStrong SQL knowledge for data preparation and mining\nStrong knowledge of writing data/machine learning algorithm in Python or R\nExperience in doing data analysis and extraction on Hadoop\nExperience of working on at least one reporting tool - Tableau and PowerBI is a plus\nExperience in data modeling, programming, querying, data mining and report development using large volumes of granular data to deliver business intelligence and custom reporting solutions\nExposure to collecting and/or working with data including standardizing, summarizing, offering initial observations and highlighting inconsistencies\nStrong understanding of the application of analytical methods and data visualization to support business decisions\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor\nAbility to easily move between business, analytical, and technical teams and articulate solution requirements for each group",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Data modeling', 'Information security', 'Analytical', 'Network security', 'Data mining', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 15:08:46
Senior Software Engineer,Mastercard,10 - 15 years,Not Disclosed,"['Pune', 'Gurugram']","The AI & DPE team is responsible for product decisioning management and innovative product development under services business unit to address the evolving risk and security needs of all of Mastercard s various customer segments. AI & DPE team focuses on defining the strategic direction for underlying platforms to enable the successful implementation of real-time, data-driven innovative products and services focused on network, security, fraud, digital identity and authentication. The team is responsible to look across all the products/services to drive efficiency, re-usability and increase speed to market for our products and services.\n\nThe candidate for this position will focus on driving actionable insights and build innovative solutions out of multiple data sources using analytics, machine learning and reporting capabilities.\n\nThe Role:\n\nWe are seeking a Senior Software Engineer who will:\nPartner with various teams (ie, Product Manager, Data Science, Platform Strategy, Technology) on requirement gathering in order to deliver analytics solutions that generate business value\n\nPerform data preparation by ingestion, aggregation, processing to drive and enable relevant insights from available data sets\n\nIdentify and code the best suited data algorithm model for the relevant insights\n\nManipulate and analyze complex, high-volume, high-dimensionality data from varying sources using a variety of tools and data analysis techniques.\n\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions\n\nCollect and synthesize feedback from clients, development, product and sales teams for new solutions or product enhancements\n\nAll about You\n\nStrong SQL knowledge for data preparation and mining\nStrong knowledge of writing data/machine learning algorithm in Python or R\nExperience in doing data analysis and extraction on Hadoop\nExperience of working on at least one reporting tool - Tableau and PowerBI is a plus\nExperience in data modeling, programming, querying, data mining and report development using large volumes of granular data to deliver business intelligence and custom reporting solutions\nExposure to collecting and/or working with data including standardizing, summarizing, offering initial observations and highlighting inconsistencies\nStrong understanding of the application of analytical methods and data visualization to support business decisions\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor\nAbility to easily move between business, analytical, and technical teams and articulate solution requirements for each group\n",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Data modeling', 'Information security', 'Analytical', 'Network security', 'Data mining', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 15:08:49
Senior Software Engineer - Python Developer,FactSet,5 - 10 years,Not Disclosed,['Hyderabad'],"FactSet creates flexible, open data and software solutions for over 200,000 investment professionals worldwide, providing instant access to financial data and analytics that investors use to make crucial decisions.\nAt FactSet, our values are the foundation of everything we do. They express how we act and operate , serve as a compass in our decision-making, and play a big role in how we treat each other, our clients, and our communities. We believe that the best ideas can come from anyone, anywhere, at any time, and that curiosity is the key to anticipating our clients needs and exceeding their expectations.",,,,"['Computer science', 'C++', 'Data analysis', 'GCP', 'Analytical', 'Machine learning', 'Technical leadership', 'Monitoring', 'SQL', 'Python']",2025-06-12 15:08:52
Manager - Data Science,Axtria,6 - 11 years,Not Disclosed,['Gurugram'],"JOB OBJECTIVE Manager with good hands-on experience of 6+ years in developing state of the art and scalable Machine Learning models and their operationalization, leveraging off-the-shelf workbench production.\n\nKEY RESPONSIBILITIES\n\n\nNecessary Skills –\n6+ years of experience of model development using Python/PySpark libraries. Development on Databricks or Dataiku DSS (Data Science Studio) environment would be a plus\nStrong experience on Spark with Scala/Python/Java\nStrong proficiency in building/training/evaluating state of the art machine learning models and its deployment\nProficiency in Statistical and Probabilistic methods such as SVM, Decision-Trees, Bagging and Boosting Techniques, Clustering\nProficiency in Core NLP techniques like Text Classification, Named Entity Recognition (NER), Topic Modeling, Sentiment Analysis, etc. Understanding of Generative AI / Large Language Models / Transformers would be a plus\nHands on experience in Python data-science and math packages such as NumPy, Pandas, Sklearn, Seaborn, PyCaret, Matplotlib\nProficiency in Python and common Machine Learning frameworks (TensorFlow, NLTK, Stanford NLP, PyTorch, Ling Pipe, Caffe, Keras, SparkML and OpenAI etc.)\nExperience of working in large teams and using collaboration tools like GIT, Jira and Confluence\nGood understanding of any of the cloud platform – AWS, Azure or GCP\nUnderstanding of Commercial Pharma landscape and Patient Data / Analytics would be a huge plus\nShould have an attitude of willingness to learn, accepting the challenging environment and confidence in delivering the results within timelines. Should be inclined towards self motivation and self-driven to find solutions for problems.\nShould be able to mentor and guide mid to large sized teams under him/her",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'java', 'spark', 'machine learning algorithms', 'python', 'confluence', 'scikit-learn', 'nltk', 'pyspark', 'training', 'numpy', 'tensorflow', 'git', 'seaborn', 'gcp', 'pytorch', 'model development', 'keras', 'jira', 'sentiment analysis', 'caffe', 'microsoft azure', 'pandas', 'matplotlib', 'aws', 'statistics']",2025-06-12 15:08:54
Artificial Intelligence Engineer,Sightspectrum,6 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai']","AI Engineer/ Artificial Intaligence Engineer\n\nShould Experticed as AI Engineer/ Artificial Intaligence Engineer Previously\n\nExperience in :Python, API\n\nWork From Office\n\nJob Location : Chennai/Hyderabad\n\nyears of Experience : 6 to 8 years,\n\nShould Experticed as AI Engineer Previously",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'API', 'Python']",2025-06-12 15:08:56
Senior Software Engineer,Dynamic Yield,5 - 8 years,Not Disclosed,['Pune'],"Our Purpose\nTitle and Summary\nSenior Software Engineer\nWho is Mastercard?\nMastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.\nOur decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.\n\nOur Team:\n\nThe AI & DPE team is responsible for product management and innovative product development of products and services to address the evolving risk and security needs of all of Mastercard s various customer segments. Platform Services team in AI & DPE focuses on defining the strategic direction for underlying platforms to enable the successful implementation of real-time, data-driven innovative products and services focused on network, security, fraud, digital identity and authentication. The team is responsible to look across all the products/services across AI & DPE and drive efficiency, re-usability and increase speed to market for our products and services.\n\nThe candidate for this position will focus on Data Unification across different data assets, enabling a single unified view of data from multiple sources and support the development of new innovative data driven cyber products, services and actionable insights.\n\nThe Role:\nWe are seeking a Senior Software Engineer who will:\n\nPerform data ingestion, aggregation, processing to drive and enable relevant insights from available data sets.\nPartner with various teams (i.e., Product Manager, Data Science, Platform Strategy, Technology) on data needs/requirements in order to deliver data solutions that generate business value.\nManipulate and analyze complex, high-volume, high-dimensionality data from varying sources using a variety of tools and data analysis techniques.\nIdentify innovative ideas and deliver proof of concepts, prototypes to deliver against the existing and future needs and propose new products, services and enhancements.\nIntegrate & Unify new data assets which increase the value proposition for our customers and enhance our existing solutions and services.\nAnalyse large volumes of transaction and product data to generate insights and actionable recommendations to drive business growth\nCollect and synthesize feedback from clients, development, product and sales teams for new solutions or product enhancements.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nMinimum 5-8 years of relevant experience.\nGood understanding of programming language preferably PySpark and Big Data technologies.\nExperience with Enterprise Business Intelligence Platform/Data platform.\nStrong SQL and higher-level programming languages with solid knowledge of data mining, machine learning algorithms and tools\nExperience with data integration tools - ETL/ELT tools (i.e. Apache NiFi, Azure Data Factory, Databricks)\nExposure to collecting and/or working with data including standardizing, summarizing, offering initial observations and highlighting inconsistencies.\nStrong understanding of the application of analytical methods and data visualization to support business decisions.\nAbility to understand complex operational systems and analytics/business intelligence tools for the delivery of information products and analytical offerings to a large, global user base.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor\nAbility to easily move between business, analytical, and technical teams and articulate solution requirements for each group",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Data analysis', 'Information security', 'Analytical', 'Network security', 'Data mining', 'Business intelligence', 'Operations', 'Analytics', 'SQL']",2025-06-12 15:09:00
Senior Software Engineer,Mastercard,5 - 8 years,Not Disclosed,['Pune'],"Senior Software Engineer\n?\n\n\n\n\nThe AI & DPE team is responsible for product management and innovative product development of products and services to address the evolving risk and security needs of all of Mastercard s various customer segments. Platform Services team in AI & DPE focuses on defining the strategic direction for underlying platforms to enable the successful implementation of real-time, data-driven innovative products and services focused on network, security, fraud, digital identity and authentication. The team is responsible to look across all the products/services across AI & DPE and drive efficiency, re-usability and increase speed to market for our products and services.\n\nThe candidate for this position will focus on Data Unification across different data assets, enabling a single unified view of data from multiple sources and support the development of new innovative data driven cyber products, services and actionable insights.\n\nThe Role:\nWe are seeking a Senior Software Engineer who will:\n\nPerform data ingestion, aggregation, processing to drive and enable relevant insights from available data sets.\nPartner with various teams (i.e., Product Manager, Data Science, Platform Strategy, Technology) on data needs/requirements in order to deliver data solutions that generate business value.\nManipulate and analyze complex, high-volume, high-dimensionality data from varying sources using a variety of tools and data analysis techniques.\nIdentify innovative ideas and deliver proof of concepts, prototypes to deliver against the existing and future needs and propose new products, services and enhancements.\nIntegrate & Unify new data assets which increase the value proposition for our customers and enhance our existing solutions and services.\nAnalyse large volumes of transaction and product data to generate insights and actionable recommendations to drive business growth\nCollect and synthesize feedback from clients, development, product and sales teams for new solutions or product enhancements.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nMinimum 5-8 years of relevant experience.\nGood understanding of programming language preferably PySpark and Big Data technologies.\nExperience with Enterprise Business Intelligence Platform/Data platform.\nStrong SQL and higher-level programming languages with solid knowledge of data mining, machine learning algorithms and tools\nExperience with data integration tools - ETL/ELT tools (i.e. Apache NiFi, Azure Data Factory, Databricks)\nExposure to collecting and/or working with data including standardizing, summarizing, offering initial observations and highlighting inconsistencies.\nStrong understanding of the application of analytical methods and data visualization to support business decisions.\nAbility to understand complex operational systems and analytics/business intelligence tools for the delivery of information products and analytical offerings to a large, global user base.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor\nAbility to easily move between business, analytical, and technical teams and articulate solution requirements for each group",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Data analysis', 'Information security', 'Analytical', 'Network security', 'Data mining', 'Business intelligence', 'Operations', 'Analytics', 'SQL']",2025-06-12 15:09:03
Manager - Data Science,Axtria,4 - 6 years,Not Disclosed,['Gurugram'],"Be an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on UI\nSoftware development experience in REACT, JavaScript/TypeScript, python is needed as frontend and backend for UI based applications\nBe a part of large delivery teams working on advanced projects when expert assistance is required.\nDeliver advanced Data Science capabilities to businesses in a meaningful manner through successful proof-of-concept solutions, and later smoothly transition the proof-of-concept into production.\nCreate Technical documents, develop, test, and deploy data analytics processes using REACT, Python, SQL on Azure/AWS platforms\nCan interact with client on GenAI related capabilities and use cases\n\n\nMust have\n\nSkills:\n\n\nMinimum of 4-6 years develop, test, and deploy React, JavaScript, Python based applications on Azure/AWS platforms\nMust have experience in building production grade application with frontend & backend technologies like HTML, JavaScript, REACT, Python etc.\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.\nBasic knowledge on concepts of Generative AI / LLMs / GPT will be beneficial",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'production', 'javascript', 'react.js', 'python web framework', 'data analytics', 'software testing', 'natural language processing', 'gpm', 'neural networks', 'microsoft azure', 'aws stack', 'machine learning', 'sql', 'deep learning', 'django', 'data science', 'html', 'typescript', 'flask', 'aws']",2025-06-12 15:09:06
"Principal Engineer, Performance Modelling",Renesas Electronics India Pvt. Ltd.,7 - 11 years,Not Disclosed,['Noida'],"We are seeking a highly skilled and experienced Engineers to join our team. The ideal candidate will be responsible for performance verification of System-on-Chip (SoC) designs using Platform Architect and Emulation Platform. This role involves working closely with SoC architects and cross-functional teams to optimize SoC performance and ensure the successful delivery of high-quality products.\nKey Responsibilities:\nDevelop and maintain performance models for SoC designs using Synopsys Platform Architect or Emulation Platform\nCollaborate with architecture, design, software and verification teams to define performance requirements and ensure alignment with overall system goals.\nAnalyze and optimize system performance, including DDR, CPU, GPU, Interconnects and high-speed interface like PCIe, UCIe etc\nIdentify performance bottlenecks and propose solutions to improve system efficiency.\nConduct performance simulations and provide detailed analysis and reports.\nMentor and guide junior engineers in performance modelling and analysis techniques and best practices.\nStay updated with the latest advancements in SoC performance modelling and industry solutions.\n\nQualifications\n\nBachelors or Masters degree in Electrical Engineering, Computer Engineering, or a related field.\n15+ years of experience in SoC performance modelling and analysis.\nProficiency in using Platform Architect or Emulation platform for performance modelling and analysis\nStrong understanding of SoC architecture, including CPU, GPU, DDR and interconnect subsystems. Any knowledge of NPUs and AI accelerators would be an added advantage.\nExperience with performance simulation tools and methodologies.\nExcellent problem-solving skills and attention to detail.\nStrong communication and collaboration skills.\nAbility to work independently and as part of a team.\n\nCompany Description\n\nRenesas is one of the top global semiconductor companies in the world. We strive to develop a safer, healthier, greener, and smarter world, and our goal is to make every endpoint intelligent by offering product solutions in the automotive, industrial, infrastructure and IoT markets. Our robust product portfolio includes world leading MCUs, SoCs, Analog and power products, plus Winning Combination solutions that curate these complementary products. We are a key supplier to the world s leading manufacturers of electronics you rely on every day; you may not see our products, but they are all around you.\n\n\n\nRenesas employs roughly 21, 000 people in more than 30 countries worldwide. As a global team, our employees actively embody the Renesas Culture, our guiding principles based on five key elements: Transparent, Agile, Global, Innovative, and Entrepreneurial. Renesas believes in, and has a commitment to, diversity and inclusion, with initiatives and a leadership team dedicated to its resources and values. At Renesas, we want to build a sustainable future where technology helps make our lives easier. Join us and build your future by being part of what s next in electronics and the world.",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Architect', 'Semiconductor', 'Simulation', 'Analog', 'SOC', 'Diversity and Inclusion', 'Agile', 'PCIE', 'Emulators', 'Automotive']",2025-06-12 15:09:08
Senior Cloud Engineer - AWS,S&P Global Market Intelligence,5 - 10 years,Not Disclosed,"['Hyderabad', 'Gurugram']","Grade Level (for internal use):\n10\nS&P Global Commodity Insights\nThe Role: Senior Cloud Engineer\nThe Location: Hyderabad, Gurgaon\nThe Team: The Cloud Engineering Team is responsible for designing, implementing, and maintaining cloud infrastructure that supports various applications and services within the S&P Global Commodity Insights organization. This team collaborates closely with data science, application development, and security teams to ensure the reliability, security, and scalability of our cloud solutions.\nThe Impact: As a Cloud Engineer, you will play a vital role in deploying and managing cloud infrastructure that supports our strategic initiatives. Your expertise in AWS and cloud technologies will help streamline operations, enhance service delivery, and ensure the security and compliance of our environments.\nWhats in it for you: This position offers the opportunity to work on cutting-edge cloud technologies and collaborate with various teams across the organization. You will gain exposure to multiple S&P Commodity Insights Divisions and contribute to projects that have a significant impact on the business. This role opens doors for tremendous career opportunities within S&P Global.\nResponsibilities:\nDesign and deploy cloud infrastructure using core AWS services such as EC2, S3, RDS, IAM, VPC, and CloudFront, ensuring high availability and fault tolerance.\nDeploy, manage, and scale Kubernetes clusters using Amazon EKS, ensuring high availability, secure networking, and efficient resource utilization.\nDevelop secure, compliant AWS environments by configuring IAM roles/policies, KMS encryption, security groups, and VPC endpoints.\nConfigure logging, monitoring, and alerting with CloudWatch, CloudTrail, and GuardDuty to support observability and incident response.\nEnforce security and compliance controls via IAM policy audits, patching schedules, and automated backup strategies.\nMonitor infrastructure health, respond to incidents, and maintain SLAs through proactive alerting and runbook execution.\nCollaborate with data science teams to deploy machine learning models using Amazon SageMaker, managing model training, hosting, and monitoring.\nAutomate and schedule data processing workflows using AWS Glue, Step Functions, Lambda, and EventBridge to support ML pipelines.\nOptimize infrastructure for cost and performance using AWS Compute Optimizer, CloudWatch metrics, auto-scaling, and Reserved Instances/Savings Plans.\nWrite and maintain Infrastructure as Code (IaC) using Terraform or AWS CloudFormation for repeatable, automated infrastructure deployments.\nImplement disaster recovery, backups, and versioned deployments using S3 versioning, RDS snapshots, and CloudFormation change sets.\nSet up and manage CI/CD pipelines using AWS services like CodePipeline, CodeBuild, and CodeDeploy to support application and model deployments.\nManage and optimize real-time inference pipelines using SageMaker Endpoints, Amazon Bedrock, and Lambda with API Gateway to ensure reliable, scalable model serving.\nSupport containerized AI workloads using Amazon ECS or EKS, including model serving and microservices for AI-based features.\nCollaborate with SecOps and SRE teams to uphold security baselines, manage change control, and conduct root cause analysis for outages.\nParticipate in code reviews, design discussions, and architectural planning to ensure scalable and maintainable cloud infrastructure.\nMaintain accurate and up-to-date infrastructure documentation, including architecture diagrams, access control policies, and deployment processes.\nCollaborate cross-functionally with application, data, and security teams to align cloud solutions with business and technical goals.\nStay current with AWS and AI/ML advancements, suggesting improvements or new service adoption where applicable.\nWhat Were Looking For:\nStrong understanding of cloud infrastructure, particularly AWS services and Kubernetes.\nProven experience in deploying and managing cloud solutions in a collaborative Agile environment.\nAbility to present technical concepts to both business and technical audiences.\nExcellent multi-tasking skills and the ability to manage multiple projects under tight deadlines.\nBasic Qualifications:\nBA/BS in computer science, information technology, or a related field.\n5+ years of experience in cloud engineering or related roles, specifically with AWS.\nExperience with Infrastructure as Code (IaC) tools such as Terraform or AWS CloudFormation.\nKnowledge of container orchestration and microservices architecture.\nFamiliarity with security best practices in cloud environments.\nPreferred Qualifications:\nExtensive Hands-on Experience with AWS Services.\nExcellent problem-solving skills and the ability to work independently as well as part of a team.\nStrong communication skills and the ability to influence stakeholders at all levels.\nExperience with greenfield projects and building cloud infrastructure from scratch.\nBenefits:\n\nHealth & Wellness: Health care coverage designed for the mind and body.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS', 'Terraform', 'AWS CloudFormation', 'Cloud', 'cloud infrastructure']",2025-06-12 15:09:11
"Data Science Specialist - R/Python, Statistical Analysis, AI/Ml",Cisco,4 - 8 years,Not Disclosed,['Bengaluru'],"Responsibilities:\nAnalysis of cross-customer and customer specific data.\nAnalysis for diagnosis of product and customers specific problems and also to demonstrate value of our data to customers.\nSupport sales and product adoption for data related use-cases (occupancy, captive portal, behavioral metrics, BMS integrations etc)\nHelp design monitoring tools to detect product and customer relative issues around product\nCustomer demonstrations of more sophisticated data products like Firehose. Engineering/Product linkages\nCollaborate with specialist teams to help deliver solutions. (Webex, Meraki etc)\nLeverage on ML based approaches for fault detection tools, for trends and also customer/category analysis\n\nQualifications:\nAdvanced degree or equivalent experience in Engineering, Computer Science, Maths or a related technical field\nProficiency in programming and scripting languagesRand/orPython\nExperience using relational database -SQL\nBasic proficiency withMachine Learning methods and applications\n\nSkills:\nPassion for problem solving.\nHighly driven and customer oriented.\nExcellent communication.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'rest', 'python', 'data analysis', 'natural language processing', 'machine learning', 'relational databases', 'artificial intelligence', 'javascript', 'sql', 'spring', 'r', 'tableau', 'java', 'computer science', 'html', 'mysql', 'data structures', 'data visualization', 'ml', 'statistics']",2025-06-12 15:09:14
AI Lead - L1,Wipro,5 - 8 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of this role is to develop minimum viable product (MVP) and comprehensive AI solutions that meet and exceed clients expectations and add value to business.\n\nDo\nManage the product/ solution development using the desired AI techniques\nLead development and implementation of custom solutions through thoughtful use of modern AI technology\nReview and evaluate the use cases and decide whether a product can be developed to add business value\nCreate the overall product development strategy and integrating with the larger interfaces\nCreate AI models and framework and implement them to cater to a business problem\nDraft the desired user Interface and create AI models as per business problem\nAnalyze technology environment and client requirements to define product solutions using AI framework/ architecture\nImplement the necessary security features as per products requirements\nReview the used case and see the latest AI that can be used in products development\nIdentify problem areas and perform root cause analysis and provide relevant solutions to the problem\nTracks industry and application trends and relates these to planning current and future AI needs\nCreate and delegate work plans to the programming team for product development\nInteract with Holmes advisory board for knowledge sharing and best practices\nResponsible for developing and maintaining client relationships with the key strategic partners and decision makers\nDrive discussions and provide consultation around product design as per customer needs\nParticipate in client interactions and gather insights regarding product development\nInteract with vertical delivery and business teams and provide and correct responses to RFP/ client requirements\nAssist in products demonstration and receive feedback from the client\nDesign presentations for seminars, meetings and enclave primarily focused over product\nTeam Management\nResourcing\nForecast talent requirements as per the current and future business needs\nHire adequate and right resources for the team\nTalent Management\nEnsure adequate onboarding and training for the team members to enhance capability & effectiveness\nBuild an internal talent pool and ensure their career progression within the organization\nManage team attrition\nDrive diversity in leadership positions\nPerformance Management\nSet goals for the team, conduct timely performance reviews and provide constructive feedback to own direct reports\nEnsure that the Performance Nxt is followed for the entire team\nEmployee Satisfaction and Engagement\nLead and drive engagement initiatives for the team\nTrack team satisfaction scores and identify initiatives to build engagement within the team\nDeliver\n\nNo.Performance ParameterMeasure1.Continuous technical project management & deliveryAdoption of new technologies, IP creation, MVP creation, Number of patents filed, Research papers created2.Client CentricityNo. of automation done, On-Time Delivery, cost of delivery, optimal resource allocation3.Capability Building & Team Management% trained on new age skills, Team attrition %, Number of webinars conducted (internal/external)\n\nMandatory Skills: Generative AI. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Generative AI', 'project management', 'resource allocation', 'team management', 'performance management', 'artificial intelligence']",2025-06-12 15:09:16
Senior Data Manager/ Lead,Codeforce 360,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description:\nWe are looking for a highly experienced and dynamic Senior Data Manager / Lead to oversee a team of Data Engineers and Data Scientists. This role demands a strong background in data platforms such as Snowflake and proficiency in Python, combined with excellent people management and project leadership skills. While hands-on experience in the technologies is beneficial, the primary focus of this role is on team leadership, strategic planning, and project delivery .\n\nJob Title : Senior Data Manager / Lead\nLocation: Hyderabad (Work From Office)\nShift Timing: 10AM-7PM\nKey Responsibilities:\nLead, mentor, and manage a team of Data Engineers and Data Scientists.\nOversee the design and implementation of data pipelines and analytics solutions using Snowflake and Python.\nCollaborate with cross-functional teams (product, business, engineering) to align data solutions with business goals.\nEnsure timely delivery of projects, with high quality and performance.\nConduct performance reviews, training plans, and support career development for the team.\nSet priorities, allocate resources, and manage workloads within the data team.\nDrive adoption of best practices in data management, governance, and documentation.\nEvaluate new tools and technologies relevant to data engineering and data science.\n\nRequired Skills & Qualifications:\n6+ years of experience in data-related roles, with at least 23 years in a leadership or management position.\nStrong understanding of Snowflake architecture, performance tuning, data sharing, security, etc.\nSolid knowledge of Python for data engineering or data science tasks.\nExperience in leading data migration, ETL/ELT, and analytics projects.\nAbility to translate business requirements into technical solutions.\nExcellent leadership, communication, and stakeholder management skills.\nExposure to tools like Databricks, Dataiku, Airflow, or similar platforms is a plus.\nBachelors or Master’s degree in Computer Science, Engineering, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Bricks', 'Python', 'Airflow', 'Data Migration', 'Dataiku', 'Data Warehousing', 'ETL', 'ELT', 'SQL']",2025-06-12 15:09:18
Conversational AI Technical Lead,Qualcomm,4 - 9 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Programmer Analyst\n\nGeneral Summary:\n\nQualcomm IT is seeking a Lead Conversational AI Developer for Intelligent Automation Center (IAC)Responsibilities include:\nExperience on designing and implementing Conversational AI solutions using Microsoft Azure and Copilot Stack in combination with GenAI\nHands-on experience with Microsoft Copilot Studio, Microsoft Bot Framework, NLP, Azure AI Search and Azure OpenAI\nExtensive hands-on experience in implementing end-to-end projects utilizing Generative AI using Retrieval-Augmented Generation (RAG) or Agentic AI architecture\nStrong expertise in Python for building bot solutions\nExperience with Azure Cognitive Services (LUIS/CLU, QnA Maker/CQA, Spell Check,Speech API) for advanced NLP features.\nKnowledge of Power Automate, Azure Logic Apps, and APIs for extending Copilot Agent and bot functionalities.\nExperience in software development with a focus on Conversational AI and Machine Learning.\nProficiency with tools and Frameworks such as LangChain, LlamaIndex, and Streamlit.\nKnowledge and implementation experience of chatbot technologies using Microsoft Azure Services and Power Platform.\nEnsure quality of coded components by performing thorough unit testing and develop reusable test cases\nWork collaboratively with test teams for supporting Product testing and UAT\nReport status, issues and risks to tech leads on a regular basis\nImprove skills in automation products through certifications\nTrain and coach team members on Conversational AI related technologies\nWork independently with minimal supervision and good team management skills\nExcellent communication and collaboration skills\nProvide timely status on assignments, planned activities, issues, and dependencies\n\nGood knowledge on Conversational AI on Microsoft Stack (Copilot Studio, Azure AI Foundry, Azure AI Search, Azure OpenAI)\nGood understanding of Generative AI concepts and Frameworks like Langchain\nHands-on programming experience on Python and any frontend technology like Angular\n\nMinimum Qualifications:\n4+ years of work experience in programming, scripting, and/or automation or IT-relevant work experience with a Bachelor's degree.\nOR\n6+ years of work experience in programming, scripting, and/or automation or IT-relevant work experience without a Bachelors degree.\n\n2+ years experience with Database Design structures such as Mongo DB, MySQL.\n\nGood understanding of conversational AI and Intelligent Automation methodologies and associated tools & technologies\nKnowledge of Process Mining concepts and implementation expereience using Celonis inclusing data models and dasboards\nExperience in business process diagrams and process flow charts with Automation Anywhere\nCertification in Industry Leading Robotic Automation products is a plus.\nExperience in identifying the right processes for Automation and providing estimates for implementations\nProgramming concepts and coding background in Python\nUnderstanding of RDBMS concepts and writing SQL queries\nExpereience in Cloud (preferrably AWS) and certifications is a plus\nExperience in Agile development using standard tools like Jira\n\nBachelor's degree and 5+ years IT-relevant work experience",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'microsoft azure', 'sql', 'database design', 'aws', 'luis', 'rdbms', 'microsoft bot framework', 'software development', 'natural language processing', 'data mining', 'azure cognitive services', 'machine learning', 'angular', 'cqa', 'qna maker', 'speech', 'mysql', 'agile', 'api', 'mongodb', 'jira']",2025-06-12 15:09:20
Immediate Opening For Data Science,Happiest Minds Technologies,8 - 13 years,Not Disclosed,['Bengaluru( Madiwala )'],"Machine Learning, Deep Learning models, Data Science. (Important);-R / python programming (mandatory) ;- Fast API development ;- deployment of models experience ; - any cloud Azure (good to have - for this requirement); - basics of Generative AI , NLP (optional - Good to have)\n\nGIS data, Geospatial data, Google Maps, ArcGIS, Demand pattern analysis\n\n5 to 15 Yrs",,,,"['Data Science', 'Machine Learning', 'Deep Learning', 'Python', 'GenAi', 'Natural Language Processing']",2025-06-12 15:09:23
Data Science Lead,Algoleap Technologies,10 - 15 years,Not Disclosed,['Hyderabad'],"As we build our Data Science Center of Excellence (CoE), we are looking for an entrepreneurial and technically strong Data Science Lead who can lay the foundation for a high-performing team. You will work directly with stakeholders across multiple business units to define use cases, lead model development, and ensure successful deployment and value realization.\n\nKey Responsibilities:\nServe as the technical and strategic lead for the Data Science CoE.",,,,"['customer analytics', 'Usage', 'data science', 'GCP', 'Machine learning', 'model development', 'Deployment', 'Stakeholder management', 'SQL', 'Python']",2025-06-12 15:09:25
Team Leader - Operations,Startek,4 - 10 years,Not Disclosed,['Noida'],STARTEK is looking for Team Leader - Operations to join our dynamic team and embark on a rewarding career journey\n\nManage and lead operations team\n\nMonitor performance and implement improvements\n\nEnsure operational targets are met\n\nCoordinate with management for strategic goals,Industry Type: Oil & Gas,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Wireless', 'Business transformation', 'orchestration', 'Artificial Intelligence', 'Team Leader Operations', 'Customer support', 'Customer experience', 'Troubleshooting', 'Customer engagement', 'digital transformation']",2025-06-12 15:09:28
HIH - Data Science Lead Analyst - Evernorth,Cigna Medical Group,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\nThe role will support the development and maintenance of proprietary advanced neural network (AI) foundation models in support of Cigna business operations.\nKey Responsibilities:\nWrite code using PyTorch and/or Tensorflow to implement, test, and operationalize deep learning models\nCollaborate with data scientists and engineers to improve deep learning models and implement business-facing solutions built on top of those models\nTake responsibility for improving code performance and quality\nFollow developments in deep learning technology to identify opportunities to improve models\nQualifications:\nBachelors or Masters(preferred) in computer science or statistics or any other equivalent discipline with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR), with emphasis on Tensorflow and Pytorch\nKnows and follows best coding and software engineering practices\nSuccessfully completes technical project components with limited guidance\nFamiliarity with deploying machine learning and predictive models to production and cloud environments\nLeadership in Data Science\nUnderstands how assigned work is related to purpose of the overall project\nIndependently identifies project roadblocks, and solutions\nSeeks to understand the health insurance domain\nScope and Impact\nDocuments the business considerations, methodology, process, code, and results associated with their work\nCollaborates to deliver clear and well developed presentations for both technical and business audiences\nConsistently communicates decisions, considerations, and needs for support\nReceives and responds to feedback in a professional and appropriate manner\nLevel of Influence\nPresent technical topics and results to non-technical stakeholders\nCommunicate and gather domain knowledge from non-technical stakeholders",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Health insurance', 'Version control', 'GIT', 'Claims', 'Coding', 'Machine learning', 'SQL', 'Python', 'Business operations']",2025-06-12 15:09:30
Data Quality Lead by Domain,Horizon Therapeutics,3 - 13 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgens data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leverages state-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. You will leverage domain, technical and business process expertise to provide exceptional support of Amgen s data governance framework. This role involves working closely with business stakeholders and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with the Product Owner and other Business Analysts to ensure operational support and excellence from the team.\nRoles Responsibilities:\nResponsible for the data governance and data management framework implementation for a given domain of expertise (Research, Development, Supply Chain, etc. ).\nResponsible for the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nDrives cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles. Develop and implement data quality frameworks. Design and enforce standards for data quality and governance to ensure consistent, accurate, and reliable data.\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. data quality best practices and tools for assigned domains.\nEnsure compliance requirements with data privacy, security, and regulatory policies for the assigned domains. Collaborate with stakeholders and work closely with data stewards, analysts, IT, and business units to understand data requirements and address quality concerns.\nJointly with Technology teams, business functions, and enterprise teams (e. g. , MDM, Enterprise Data Fabric, etc. ) define the specifications shaping the development and implementation of data foundations .\nBuild strong relationships with key business leaders and partners to ensure their needs are being met. Lead data quality initiatives aimed at improving data quality, including data cleansing, enrichment, and validation processes.\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills with knowledge of Pharma processes with specialization in a domain (e. g. , Research, Clinical Trials, Commercial, etc. ).\nIn depth knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc.\nIn depth experience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer-focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nIn depth experience of working with or supporting systems used to data governance framework. E. g. Collibra, Alation\nExcellent problem-solving skills and committed attention to detail in finding solutions. Proficiency in data analysis and quality tools (e. g. , SQL, Excel, Python, or SAS)\nGood-to-Have Functional Skills:\nExperience of working with data governance councils or forums\nExperience with Agile software development methodologies (Scrum)\n3-5 years of experience in data quality management, data governance, or related roles.\nSoft Skills:\nHighly organized and able to work under minimal supervision\nExcellent analytical and assessment skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAmbitious to further develop their skills and career\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills\nHigh degree of initiative and self-motivation.\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management and customer focus.\nBasic Qualifications:\nAny Degree and 9-13 years of experience\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Business process', 'Data analysis', 'Change management', 'SAS', 'Pharma', 'Analytical', 'Clinical trials', 'Data quality', 'SQL']",2025-06-12 15:09:32
Senior Principal Engineer,Marvell Technology,7 - 11 years,Not Disclosed,"['Pune', 'Bengaluru']","About Marvell\n.\nYour Team, Your Impact\nThe Data Centre Engineering Group develops Custom Silicon products tailored for the Data Centre market, focusing on cutting-edge Accelerated Infrastructure solutions for Networking, Switching, Connectivity, and Compute. The team works on high-performance and scalable architectures, ensuring optimized performance, power efficiency, and reliability to meet evolving data center demands. By collaborating across multiple teams, the group delivers best-in-class silicon solutions that drive innovation in next-generation data center applications.\nWhat You Can Expect\nLead the DV execution and sign-off for the entire SoC\nDefine and drive improvements in DV processes for efficient and high-quality execution\nCollaborate with IP, Subsystem, and SoC teams on test plan creation, testbench architecture, and milestone reviews\nWork closely with Design and DV teams across IP, Subsystem, and SoC levels for test plan development, execution, debug, coverage closure, and gate-level simulations\nCoordinate with cross-functional teams including Architecture, Chip Lead, Emulation, and Program Management to drive SoC-level DV execution\nPartner with Silicon bring-up and Firmware teams to support post-silicon validation and bring-up activities\nOwn and debug simulation failures to identify and resolve root causes\nArchitect and implement simulation testbenches using UVM\nDevelop and execute test plans to verify design correctness and performance\nCollaborate with logic designers for thorough verification coverage and closure\nWhat Were Looking For\nBachelor s degree in CS/EE with 22+ years of relevant experience or Master s degree in CS/EE with 20+ years of relevant experience\nStrong background in IP and SoC verification methodologies and testbench development using Verilog, SystemVerilog, UVM, and C/C++\nDeep understanding of verification techniques including object-oriented programming, white-box/black-box testing, directed/random testing, formal verification, coverage analysis, and gate-level simulations\nProven experience in DV sign-off for Functional, Power, Performance, and Security metrics\nStrong knowledge of Unix/Linux environments; scripting experience in Shell, Perl, or Python is a plus\nDemonstrated analytical and problem-solving capabilities\nAbility to manage multiple tasks in a fast-paced, dynamic environment\nExcellent interpersonal, teamwork, and communication skills\nProven ability to interface across all levels of internal and external stakeholders\nExperience leading DV execution and sign-off for complex SoCs\nHands-on involvement in 10+ successful tape-outs and post-silicon bring-up\nAdditional Compensation and Benefit Elements\nWith competitive compensation and great benefits, you will enjoy our workstyle within an environment of shared collaboration, transparency, and inclusivity. We re dedicated to giving our people the tools and resources they need to succeed in doing work that matters, and to grow and develop with us. For additional information on what it s like to work at Marvell, visit our Careers page.\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status.\n#LI-KP1",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Unix', 'C++', 'Linux', 'Networking', 'Verilog', 'Test planning', 'Perl', 'Firmware', 'Automotive', 'Python']",2025-06-12 15:09:35
HIH - Data Science Lead Analyst - Evernorth,Cigna Medical Group,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\n\nThe role will support the development and maintenance of machine learning models, with a focus on ensuring that models meet Cigna requirements for governance and legal compliance. The role will require collaboration with other data scientists and involve work across many lines of business.\nKey Responsibilities:\nAnalyze model performance of new models with specific regards to requirements for legal compliance and governance standards around accuracy and bias;\nPerform periodic analyses of performance of existing models to ensure continued compliance with internal and external standards for accuracy and bias;\nConduct research (i.e. literature review) to understand when bias may be biologically or medically justifiable, and to what degree, for example: finding evidence from literature that heart disease is more prevalent among older populations\nUsing machine learning development tools to mitigate model bias when this is determined to be necessary\nCollaborating with data scientists, business stakeholders, and governance/compliance teams to ensure models meet compliance and governance standards\nQualifications:\nBachelors or Masters/PhD (preferred) in statistics or computer science or equivalent field with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR)\nAbility to promote best coding practices, championing a culture of documentation/logging\nThorough understanding of ML lifecycle, including necessary tradeoffs and associated risks\nLeadership in Data Science\nCan own a project end-to-end e.g., scoping, business value estimation, ideation, dev, prod, timeline\nCollaborates and guides junior team members in completion of projects and career development\nWorks cross functionally with technical (e.g., Data Science, Data Engineering) and business (e.g., clinical, marketing, pricing, business analysts) to implement solutions with measurable value\nScope and Impact\nIndependently delivers clear and well-developed presentations for both technical and business audiences\nCreates data science specific project goals associated with project deliverables\nArticulates timeline changes, rationale, and goals to meet deadlines moving forward\nValues diversity, growth mindset, and improving health outcomes of our customers\n\nLevel of Influence\nCommunicate with stakeholders to identify opportunities and possible solutions based on business need\nDraft project charter, timeline, and features/stories\nInfluence matrix-partner leadership",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Version control', 'GIT', 'Claims', 'data science', 'Legal compliance', 'Coding', 'Machine learning', 'SQL', 'Python']",2025-06-12 15:09:37
"Principal Engineer, Emulation",Marvell Technology,6 - 11 years,Not Disclosed,['Bengaluru'],"About Marvell\n.\nYour Team, Your Impact\nCustom Compute and Storage (CCS) Business Unit closely collaborates with strategic customers in the development of advanced and highly complex SoCs, from architecture and design all the way through layout, packaging, prototype validation and production ramp up. The Emulation Center of Excellence (CoE) team is key part of this group, with global ownership and responsibility for delivering emulation infrastructure, validating the design on emulation and drive left shift of SW and post-silicon readiness for all of CCS products.\n\nAs part of the Emulation CoE leadership, you will drive the emulation strategy, vendor platform enablement, testplan execution for a high-quality design tape-out.\nWhat You Can Expect\nAs part of a strong technical team of emulation experts, define the emulation strategy and platform requirements, develop emulation testplan, and drive execution of the emulation verification for large CCS products on emulation platform such as Veloce, Zebu and Palladium.\nWork with project lead and various stakeholders to define the emulation HW requirements for CCS products, including platforms, hardware/software collaterals, transactors, speed-bridges etc.\nWork closely with emulation hardware vendor application engineers (AEs) to keep the emulation hardware, software ecosystem updated, drive debug and resolution of issues with the vendor and design team.\nDefine and develop new capabilities HW/SW tools to enable acceleration of RTL and improve emulation/FPGA model usability for pre-Silicon and post-Silicon functional validation as well as SW development/validation\nInterface with and provide guidance to pre-silicon Validation teams for optimizing pre-Si validation environments, test suites and methodologies for emulation efficiency\nDevelop and apply automation aids, flows and scripts in support of emulation ease of use and improvement of equipment utilization.\nWhat Were Looking For\nBachelor s degree in Computer Science, Electrical Engineering or related fields and 15+ years of related professional experience or Master s degree and/or PhD in Computer Science, Electrical Engineering or related fields with 10+ years of experience.\nSubstantial knowledge of emulation platforms offerings from various vendors such as Synopsys, Cadence, Seimens including extensive experience in building complex SOC emulation models\nWorking knowledge in one or more of the following: Processor architecture, SOC components, SOC inter-connect buses, IO protocols (PCIe, CXL, Ethernet) and memory technologies interfaces (DDR, HBM)\nStrong understanding of product development process of large SOCs and verification/debug experience in emulation platforms.\nStrong experience in coding in scripting languages like Perl, Python, Tcl & UNIX Shell etc\nAdditional Compensation and Benefit Elements\nWith competitive compensation and great benefits, you will enjoy our workstyle within an environment of shared collaboration, transparency, and inclusivity. We re dedicated to giving our people the tools and resources they need to succeed in doing work that matters, and to grow and develop with us. For additional information on what it s like to work at Marvell, visit our Careers page.\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status.\n#LI-CP1",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Automation', 'Semiconductor', 'Prototype', 'FPGA', 'Coding', 'Packaging', 'Perl', 'Automotive', 'Python']",2025-06-12 15:09:40
Data Architect - AWS,Happiest Minds Technologies,10 - 15 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Roles and responsibilities\nWork closely with the Product Owners and stake holders to design the Technical Architecture for data platform to meet the requirements of the proposed solution.\nWork with the leadership to set the standards for software engineering practices within the machine learning engineering team and support across other disciplines\nPlay an active role in leading team meetings and workshops with clients.\nChoose and use the right analytical libraries, programming languages, and frameworks for each task.",,,,"['SQL', 'data architect', 'Python', 'Pyspark', 'Apache Airflow', 'GLUE', 'Kinesis', 'Amazon Redshift', 'Data Architecture Principles', 'Data Modeling', 'Data Warehousing', 'Athena', 'Lambda', 'AWS']",2025-06-12 15:09:42
Data Analyst - Python/Hadoop,Sadup Soft,3 - 6 years,Not Disclosed,['Bengaluru'],"- Minimum of 3 years of hands-on experience.\n\n- Python/ML, Hadoop, Spark : Minimum of 2 years of experience.\n\n- At least 3 years of prior experience as a Data Analyst.\n\n- Detail-oriented with a structured thinking and analytical mindset.\n\n- Proven analytic skills, including data analysis, data validation, and technical writing.\n\n- Strong proficiency in SQL and Excel.\n\n- Experience with Big Query is mandatory.\n\n- Knowledge of Python and machine learning algorithms is a plus.\n\n- Excellent communication skills with the ability to be precise and clear.\n\n- Learning Ability : Ability to quickly learn and adapt to new analytic tools and technologies.\n\nKey Responsibilities :\n\nData Analysis :\n\n- Perform comprehensive data analysis using SQL, Excel, and Big Query.\n\n- Validate data integrity and ensure accuracy across datasets.\n\n- Develop detailed reports and dashboards that provide actionable insights.\n\n- Create and deliver presentations to stakeholders with clear and concise findings.\n\n- Document queries, reports, and analytical processes clearly and accurately.\n\n- Leverage Python/ML for advanced data analysis and model development.\n\n- Utilize Hadoop and Spark for handling and processing large datasets.\n\n- Work closely with cross-functional teams to understand data requirements and provide analytical support.\n\n- Communicate findings effectively and offer recommendations based on data analysis.\n\nEducation : Bachelor's degree in Computer Science, Data Science, Statistics, or a related field.\n\nExperience : Minimum of 3 years of experience as a Data Analyst with a strong focus on SQL, Excel, and Big Query.\n\nTechnical Skills : Proficiency in SQL, Excel, and Big Query; experience with Python, ML, Hadoop, and Spark is preferred.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Data Validation', 'Big Query', 'Data Integrity', 'Hadoop', 'Spark', 'Python', 'SQL']",2025-06-12 15:09:45
Azure Data Bricks (4-15 Yrs) - Bangalore,Happiest Minds Technologies,4 - 9 years,Not Disclosed,['Bengaluru'],"Hi,\n\nGreetings from Happiest Minds Technologies\n\nCurrently we are hiring for below positions and looking for immediate joiners.\n1. Azure Databricks Bangalore 5 to 10 Yrs - Bangalore\nAs a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools. Proficiency in Python PySpark, Pandas, NumPy, SciPy, Spark SQL, DataFrames, RDDs, Delta Lake, Databricks Notebooks, and MLflow is required, along with hands-on experience in Azure Data Lake, Blob Storage, and Synapse Analytics.",,,,"['Pyspark', 'Azure', 'Data Bricks', 'sql', 'ETL']",2025-06-12 15:09:47
Principal Engineer - STA,Alphawave Semi,14 - 19 years,Not Disclosed,['Bengaluru'],"The Opportunity\n\nWere looking for the Wavemakers of tomorrow.\nWhat youll need:\nGood understanding of overall design Flow RTL to GDS.\nMust have 14+ years of experience on signing off the full chip synthesis/STA for tape outs\nHands on Experience on Both Block level and Full chip timing Constraints Development and Management for hierarchical designs.\nDeep Understanding of DFT Constraints.\nHand on Synthesis & STA Experience on Lower node Technologies with Synopsys/Cadence Tools.\ngood understanding of overall ASIC Physical Design/DFT, Tools and implication on Timing Convergence\nMust have in-depth understanding of relevant areas of Library / Memory / Other collaterals and dependencies on STA\nMust understand Ultra Submicron issues, Variation aware/Aging Aware Design Sign-off Must understand CTS/Other clock Distribution methodologies well.\nGood knowledge on Timing Budgets.\nKnowledge on Perl / TCL / Python scripting language\nExperience on multi voltage designs using CPF/UPF.\nGood understanding on timing/area/power/complexity tradeoffs on complex interface design\nHands on experience on power analysis using PTPX\nGood understanding of VHDL / Verilog Constructs.\nFamiliarity with IP level verification and strong RTL debugging capabilities is an added bonus\nA, enthusiastic team player who enjoys working with others\nExperience troubleshooting issues with users Experience communicating updates and resolutions to customers and other partners complex technical concepts to other design peers in verbal and written form\nWhat Youll Do:\nWe are looking for experienced STA engineer to lead the timing convergence of the SoCs.Responsibilities include\nSTA setup, convergence, reviews and sign-off for Multi-Mode and Multi-corner Multi voltage domain designs.\nConstraint Generation & Maintenance for Block / SOC for complex hierarchical Designs for all the Modes\nTiming analysis, and timing closure at Full chip level while supporting the PD team on Block/SS level timing convergence.\nInteraction with Design, DFT, IP&PD teams for Timing Convergence & Resolving Constraint Conflicts.\nSupport Verification team to enable GLS.\nGuide the CTS strategies and provide feedback to Implementation Team.\nManage the timing ECO generation and strategize the implementation methodology.\nDevelop Automation scripts with-in STA tools for Methodology development.\nYou will be reporting to Director - ASIC engineering.\nWe have a flexible work environment to support and help employees thrive in personal and professional capacities""\nAs part of our commitment to the well-being and satisfaction of our employees, we have designed a comprehensive benefits package that includes:\nCompetitive Compensation Package\nRestricted Stock Units (RSUs)\nProvisions to pursue advanced education from Premium Institute, eLearning content providers\nMedical Insurance and a cohort of Wellness Benefits\nEducational Assistance\nAdvance Loan Assistance\nOffice lunch & Snacks Facility\nEqual Employment Opportunity Statement\nAlphawave Semi is an equal opportunity employer, welcoming all applicants regardless of age, gender, race, disability, or other protected characteristics. We value diversity and provide accommodations during the recruitment process.",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Wireless', 'Automation', 'VHDL', 'ASIC', 'DFT', 'Verilog', 'Perl', 'Troubleshooting', 'Physical design', 'Python']",2025-06-12 15:09:49
Design Verification Principal Engineer,Marvell Technology,10 - 15 years,Not Disclosed,"['Pune', 'Bengaluru']","About Marvell\n.\nYour Team, Your Impact\nThe Data Centre Engineering Group develops Custom Silicon products tailored for the Data Centre market, focusing on cutting-edge Accelerated Infrastructure solutions for Networking, Switching, Connectivity, and Compute. The team works on high-performance and scalable architectures, ensuring optimized performance, power efficiency, and reliability to meet evolving data center demands. By collaborating across multiple teams, the group delivers best-in-class silicon solutions that drive innovation in next-generation data center applications.\nWhat You Can Expect\nArchitect and implement simulation test bench in UVM.\nDevelop and execute test-plans for verifying correctness and performance of the design.\nOwn and debug failures in simulation to root-cause problems\nClosely work with logic designers of the block being verified for test plan development, execution, debug, coverage closure and gate level simulations\nWhat Were Looking For\nBachelor s degree in CS/EE with 13-15 years of relevant experience, or Master s degree in CS/EE with 10-12 years of relevant experience\nStrong background in IP and SoC verification, including methodology and testbench development\nProficient in hardware verification languages such as Verilog, SystemVerilog, UVM, and C/C++\nSolid understanding of verification methodologies: object-oriented programming, white-box/black-box testing, directed/random testing, coverage analysis, and gate-level simulations\nExperience in Unix/Linux environments; scripting skills in Shell, Perl, or Python are a plus\nStrong analytical and problem-solving skills\nAbility to manage multiple tasks in a fast-paced environment\nExcellent communication, interpersonal, and teamwork skills\nCapable of interfacing effectively at all levels within and outside the organization\nProactive in participating in problem-solving and quality improvement initiatives\nAdditional Compensation and Benefit Elements\nWith competitive compensation and great benefits, you will enjoy our workstyle within an environment of shared collaboration, transparency, and inclusivity. We re dedicated to giving our people the tools and resources they need to succeed in doing work that matters, and to grow and develop with us. For additional information on what it s like to work at Marvell, visit our Careers page.\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status.\n#LI-KP1",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Unix', 'C++', 'Linux', 'Networking', 'Verilog', 'Test planning', 'Perl', 'Automotive', 'Python', 'White box']",2025-06-12 15:09:51
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Gurugram'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nData Engineer with Neo4j\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nGurugram, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Gurugram\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-12 15:09:54
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Chennai'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nGurugram\nData Engineer with Neo4j\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nChennai, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Chennai\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-12 15:09:56
"Principal Engineer, Bridges & Civils, REC",Ramboll,6 - 10 years,Not Disclosed,['Gurugram'],"Principal Engineer, Bridges & Civils, REC\nCompany Description\nAbout Ramboll\nFounded in Denmark, Ramboll is a foundation-owned people company. We have more than 18,000 experts working across our global operations in 35 countries. Our experts are leaders in their fields, developing and delivering innovative solutions in diverse markets including Buildings, Transport, Planning & Urban Design, Water, Environment & Health, Energy, and Management Consulting. We invite you to contribute to a more sustainable future working in an open, collaborative, and empowering company. Combining local experience with global knowledge, we together shape the societies of tomorrow.\nEquality, diversity, and inclusion are at the heart of what we do\nWe believe in the strength of diversity and know that unique experiences and perspectives are vital for creating truly sustainable societies. Therefore, we are committed to providing an inclusive and supportive work environment where everyone can flourish and reach their potential. We welcome applications from candidates of all backgrounds and encourage you to contact our recruitment team to discuss any accommodations you need during the application process.\nJob Description\nPrincipal Engineer, Bridges & Civils, REC \nRamboll in Middle East and Asia Pacific \nAt Ramboll, our 15,000 consulting engineers and scientists; designers and management consultants are based in more than 300 offices in 35 countries across the globe. In the Middle East and Asia Pacific region, we have more than 1,500 experts working across 15 offices present in India, Malaysia, Singapore, China, Hong Kong, Australia, New Zealand, Qatar, and the United Arab Emirates. Our experts are applying their passion to deliver innovative solutions in markets as diverse as Buildings, Transport, Planning & Urban Design, Water, Environment & Health, and Energy. Founded in Denmark, Ramboll is a people company. We invite you to contribute to a sustainable future working in an open, collaborative, and empowering culture. \nJob Description \nWe invite you to bring your strong knowledge on bridge design and Lusas/Sofistik into play as you would be key player in the technical delivery of the project and would carry out the design and would also be responsible of the delivery of design/drawings. To succeed in this role, you must have Knowledge of design codes like Euro code/DMRB/AASHTO/any other international standards and M. Tech degree in Structural Engineering with more than 7 years of experience. Are you our new Principal Engineer - Bridges & Civils? Click the apply-button to send your application. \nInviting bright minds \nDo you want to push the boundaries of your profession and develop your excellence in an open, collaborative, and empowering culture? We work to create a sustainable future, and our inspiring projects and innovative solutions strive to set the standard among our peers. You will join a global company that has been growing successfully since its founding in 1945. Together, we lead and leave a positive impact on societies, companies, and people around the world. \nYou will join our RECdepartment \nAs our new Principal Engineer - Bridges & Civils you will be part of a world class, innovation driven engineering design center owned by an independent trust and its employees. REC is a highly sophisticated center of engineering excellence and based in our India head office in Gurgaon. Working in partnership with all our established offices globally, the Ramboll Engineering Centre (REC) is a center for excellence in design by offering optimized solutions to the rest of the organization. \nYour key tasks and responsibilities will be: \nCarrying out the design and review of the work of Asst Engineer / Design engineers and modelers in the team and maintaining the quality of deliverables. \nParticipate in the design and delivery of Bridge Projects, coordinate with other team members of the drafting/modelling team in accomplishing complex tasks. \nMentoring and supervising asst. Engineers/ Design Engineers and provide inputs to Team Lead for the continued development of the staff in the team \nIs responsible for technical correctness and timely delivery of the design documents and 3D models corresponding to the design. \nAs a REC Project Manager for global engineering Bridge Projects, coordinated with the design team for project planning/preparation of schedule/WBS and delivering projects to time and budget. \nPerform complex analysis using computer modelling and increase efficiencies in the processes and technical design. \nParticipate in skill enhancements of the Team. \nExercises self-discipline and work ethic, respect and follow company policies and procedures.  \nYour starting point for constant growth \nFrom the moment you join Ramboll, we will support your personal and professional development so that you grow with the company. For this role, we believe your starting point is: \nWe are looking for self-motivated team members who meet the following requirements: \nME/ M. Tech degree in Structural Engineering from an institute of repute. \nShould have more than M. Tech + 7 years of experience in Bridge design preferably on existing bridges and structures (strengthening and assessments). Knowledge of design codes like Euro code is mandatory DMRB/AASHTO would be an added advantage. \nGood Knowledge of detailed design of concrete Bridges and steel composite using Lusas/Sofistik/MIDAS with Eurocodes. \nHave hands-on experience in using any of the bridge design software (LUSAS/Sofistik/MIDAS/STAAD Pro etc), \nShould be a good team member and should coordinate with other team members and the project manager for timely delivery of project \nSelf-motivated, team player and able to work independently with minimum supervision. \nFlexible attitude, in an environment with frequently changing deadlines can be relied on to meet deadlines. \nWelcome to our Transport division \nRamboll is a global transportation consultancy, and we work on some of the biggest and most innovative infrastructure projects in the world. We are close to 3,000 bright minds working within Transport worldwide, creating practical, sustainable, and economic solutions for national transport authorities, private contractors, and municipalities alike. \nRamboll in India \nRamboll has more than 1,000 experts working across five offices in India applying their passion to deliver innovative solutions in markets as diverse as Buildings, Transport, Planning & Urban Design, Water, Environment & Health, and Energy. Founded in Denmark, Ramboll is a people company. We invite you to contribute to a sustainable future working in an open, collaborative, and empowering culture. \n  Qualification\nExperience – ME/M. Tech with 7+ years of experience (preferred from IITs/NITs) \nSkills Required – Hands-on experience in detailed design of RCC & prestressed concrete and steel composite bridges based on Eurocodes of practice (Eurocode is mandatory). Exposure to working in an international environment would be preferred. Experience working with Danish bridges/Danish Annexes would be an added advantage. \nSoftware Skills - Sofistik, Midas, Lusas, basics of BIM software, computational design would be an added advantage. \nAdditional Information\nRamboll globally \nRamboll is a leading engineering, architecture, and consultancy company. Working at one of our offices in 35 countries you will join more than 16,000 fellow bright minds in creating innovative and sustainable solutions within Buildings, Transport, Energy, Environment and Health, Architecture, Landscape and Urbanism, Water and Management Consulting. Combining local experience with global knowledge, we help shape the society of tomorrow. \nAlle your information will be kept confidential according to EEO guidelines.  \nWhat we can offer you\nInvestment in your development\nLeaders you can count on, guided by our Leadership Principles\nBe valued for the unique person you are.\nNever be short of inspiration from colleagues, clients, and projects.\nThe long-term thinking of a foundation-owned company\nWe offer:\nA challenging and interesting workday characterized by continuous learning, in an environment where you have many to spar with and learn from.\nOpportunity to work with varied work tasks, across the organization.\nOpportunity to develop and influence your own area of responsibility.\nWork at the heart of sustainable change\nRamboll is a global architecture, engineering, and consultancy company. We believe that the purpose of sustainable change is to create a thriving world for both nature and people. So, that’s where we start – and how we work. At Ramboll, our core strength is our people, and our history is rooted in a clear vision of how a responsible company should act. Being open and curious is a cornerstone of our culture. We embrace an inclusive mindset that looks for fresh, diverse, and innovative perspectives. We respect, embrace, and invite diversity in all forms to actively cultivate an environment where everyone can flourish and realize their full potential.\nReady to join us?\nPlease submit your application. Be sure to include all relevant documents including your CV, cover letter, etc.\nThank you for taking the time to apply! We look forward to receiving your application.",Industry Type: IT Services & Consulting,Department: Construction & Site Engineering,"Employment Type: Full Time, Permanent","['concrete', 'danish', 'structural engineering', 'project management', 'software', 'etabs', 'bridge design', 'bim', 'staad', 'staad pro', 'midas', 'bridges', 'prestressed concrete', 'environment', 'rcc', 'java', 'lusas', 'civil engineering', 'code writing', 'design', 'international', 'structural design', 'bridge engineering']",2025-06-12 15:09:59
AI/ML,Larsen & Toubro (L&T),2 - 4 years,Not Disclosed,"['Chennai', 'Bengaluru']","Experience Required\n\n2 to 4 years of experience in AI/ML model development, deployment, and optimization. Hands-on experience in building machine learning pipelines and working with large datasets\n\nDomain Experience (Functional)\nExperience in domains such as natural language processing (NLP), computer vision, predictive analytics, or recommendation systems. Exposure to industry-specific AI applications (e.g., healthcare, finance, retail, manufacturing) is a plus.\n\nQualification\nBachelors or Masters degree in Computer Science, Artificial Intelligence, Data Science, Mathematics, or a related field\n\nRoles & Responsibilities\nDesign, develop, and deploy machine learning and deep learning models.\nCollaborate with data engineers and domain experts to collect, clean, and preprocess data.\nConduct experiments, evaluate model performance, and iterate for improvement.\nIntegrate AI models into production systems and monitor their performance.\nStay updated with the latest research and advancements in AI/ML.\nDocument model development processes and contribute to knowledge sharing.\n\nTechnical Skills\n\nProficient in Python and core ML libraries: TensorFlow, PyTorch, Scikit-learn.\nStrong with Pandas, NumPy for data handling.\nSolid grasp of ML algorithms, statistics, and model evaluation.\nFamiliar with cloud platforms (AWS/Azure/GCP).\nExperience with Git and basic CI/CD for model deployment",Industry Type: Engineering & Construction,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Npl', 'Aiml', 'Tensorflow', 'Ci/Cd', 'Machine Learning', 'Deep Learning', 'Scikit-Learn', 'Numpy', 'Pytorch', 'GCP', 'Pandas', 'Microsoft Azure', 'AWS', 'Python']",2025-06-12 15:10:01
IT Software Developer,Qualcomm,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Information Technology Group, Information Technology Group > IT Software Developer\n\nGeneral Summary:\n\nWhats in it for you""Qualcomm is enabling a world where everyone and everything can be intelligently connected. Qualcomm 5G and AI innovations are the power behind the connected intelligent edge. You will find our technologies behind and inside the innovations that deliver significant value across multiple industries and to billions of people every day.""Qualcomm engineering teams rely heavily on the latest High-Performance Computing (HPC) technologies to design and develop new products using electronic design automation (EDA) tools. This role provides an opportunity to work on the latest HPC technologies and gain experience in building scalable and fault-tolerant software solutions that are deployed on some of the largest supercomputing infrastructures across the globe.""What are we looking for""Engineering Data Analytics and Applications team (EDAAP) is looking for an experienced developer with a strong programming background. The EDAAP team is responsible for the development of software solutions enabling High Performance Compute grid and large-scale, distributed, analytical applications. They work on components and services for HPC infrastructure optimization, hardware IP management systems, petabyte-scale cloud data platforms and development of machine learning solutions and pipelines.""This role involves designing and developing high-quality software solutions to manage compute environments, including compute grids, EDA licenses, storage, data synchronization, and IT infrastructure. The ideal candidate is an experienced software developer skilled in multiple programming languages and frameworks, parallel programming, efficient algorithms and data structures, design patterns, task automation through tools and scripting.What will you do""This roles responsibilities include:"". Design and develop high-quality software using suitable programming languages and frameworks for HPC infrastructure.. Create reusable components and libraries for future use.. Continuously monitor and upgrade existing applications to ensure they meet current standards and requirements.. Develop and maintain technical documentation to guide future software development projects.. Perform comprehensive code reviews to ensure quality, maintainability, and adherence to best pr. actices.. Collaborate with internal teams to resolve issues by leveraging expertise from various departments.What do we want to see""The ideal candidate will be able to demonstrate some of the following skills:"". Over 2 years of hands-on experience in full stack development.. Proficient in programming languages and frameworks such as Python, Java/J2EE and Angular.. Expertise in using relational databases (PostgreSQL, MySQL) and familiarity with anyone of NoSQL databases (Redis, MongoDB).. Expertise in software lifecycle management, version control, coding, and CI/CD best practices to ensure quality, agility, and security.. Exposure to AI and ML technologies is a plus.. Ability to clearly explain technical concepts and analysis implications to a diverse audience.. Team-oriented, with a strong inclination to work collaboratively.. Bachelors or Masters degree in Computer Science, Computational Science, or a related field.\n\nMinimum Qualifications:\nBachelor's degree in a technical field (e.g., Computer Engineering, Computer Science, Information Systems).\nOR\nBachelors degree in a non-technical field (e.g., Business, Humanities, Marketing) and 1+ year of IT-relevant experience.\nOR\nHigh School Diploma or equivalent and 3+ years of IT-relevant experience.\n\n2+ years of any combination of academic or work experience with programming (e.g., Java, Python, etc.).\n1+ year of any combination of academic or work experience with Data Structures, algorithms, and data stores.\nBachelors / Masters or equivalent degree in computer engineering or in equivalent stream",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'version control', 'ci/cd', 'relational databases', 'data structures', 'algorithms', 'fullstack development', 'python', 'software development', 'redis', 'artificial intelligence', 'nosql', 'angular', 'java', 'postgresql', 'computer science', 'j2ee', 'mysql', 'mongodb', 'ml']",2025-06-12 15:10:03
Python Developer Lead @ Infosys- PAN INDIA,Infosys,3 - 8 years,Not Disclosed,"['Kolkata', 'Pune', 'Delhi / NCR']","Responsibilities A day in the life of an Infoscion • As part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain. • You will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements. • You will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers. • You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nTechnical and Professional Requirements: • Primary skills:Process->Testing processes->Test Automation Process,Technology->Machine Learning->Python\n\nPreferred Skills: Process->Testing processes->Test Automation Process Technology->Machine Learning->Python\n\nAdditional Responsibilities: • Knowledge of design principles and fundamentals of architecture • Understanding of performance engineering • Knowledge of quality processes and estimation techniques • Basic understanding of project domain • Ability to translate functional / nonfunctional requirements to systems requirements • Ability to design and code complex programs • Ability to write test cases and scenarios based on the specifications • Good understanding of SDLC and agile methodologies • Awareness of latest technologies and trends • Logical thinking and problem solving skills along with an ability to collaborate\n\nEducational RequirementsMCA,MSc,MTech,Bachelor of Engineering,BCA,BE,BSc,BTech",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Django', 'Python Development', 'Flask']",2025-06-12 15:10:05
Early Career Professional-E,Conduent,2 - 6 years,Not Disclosed,['Bengaluru'],"Join Us \nIf you are seeking an opportunity to make a real impact in a company that appreciates ideas and new ways of thinking, come join us and grow with a team of people who will challenge and inspire you to be the best!\nAbout the Role\nThis role is designed as an entry-level position for applicants with strong skill sets in programming (logical reasoning, analytical skills), systems configuration and testing related to systems that support different business functions.\n : \nBachelor\\u2019s degree in computer science, Information Technology, or a related field from a reputable institution with 7 CGPA and Above. Strong foundational knowledge of computer science principles and programming concepts.\nProficiency in at least one programming language, such as Java, Python, .Net, C, JavaScript or SQL.\nFamiliarity with software development lifecycle (SDLC) methodologies and best practices. Ability to quickly learn and adapt to new technologies and tools.\nGood understanding of data structures, algorithms, and object-oriented design principles. Excellent problem-solving skills with keen attention to detail.\nAbility to work effectively in a fast-paced, collaborative environment.\nDemonstrated ability to work independently and take initiative to complete tasks and solve problems.\nCommitment to continuous learning and professional development.\nGood to Have:\nFamiliarity with Oracle technologies, Like SQL, PLSQL, Reports, Shell Scripts Knowledge of database concepts and experience with SQL. Familiarity of REST, SOAP etc.\nUnderstanding of fundamental of cloud computing concepts Exposure to DevOps practices and tools, such as Git, Jenkins, Docker, and Kubernetes.\nUnderstanding of Artificial Intelligence & Machine Learning concepts\nParticipation in relevant internships, co-op programs, or personal projects demonstrating practical experience and initiative.\nGREAT\n OPPORTUNITY FOR FRESHERS \n Technical\n\nSkills:\n \nKnowledge of Software Development Life Cycle (SDLC) principles/concepts.\nKnowledge in Simple & Complex SQL Queries\nWrite intermediate SQL Queries\nCommunication and Excellence:\nExcellent logical and communication skills (Oral, written and listening ability)\nStrong communication and interpersonal skills, with the ability to effectively communicate.\n  \nConduent delivers mission-critical services and solutions on behalf of businesses and governments creating exceptional outcomes for its clients and the millions of people who count on them. Through process, technology, and our diverse and dedicated associates, Conduent solutions and services automate workflows, improve efficiencies, reduce costs, and enable revenue growth. It\\u2019s why most Fortune 100 companies and over 500 government entities depend on Conduent every day to manage their essential interactions and move their operations forward.\nConduent\\u2019s differentiated services and solutions improve experiences for millions of people every day, including 3 out of every 4 U.S. insured patients, 10 million employees who use its HR Services, and nearly 18 million benefits recipients. Conduent\\u2019s solutions deliver exceptional outcomes for its clients including :1c billion in savings from medical bill review of workers compensation claims, up to 40% efficiency increase in HR operations, up to 27% reduction in government benefits costs, up to 40% improvement in finance, accounting and procurement expense, and improved customer service interaction times by up to 20% with higher end-user satisfaction. Learn more at https://www.conduent.com",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'javascript', 'sql', 'java', 'computer science', 'rest', 'algorithms', 'kubernetes', 'oracle', 'c', 'sql queries', 'machine learning', 'artificial intelligence', 'docker', 'plsql', 'software development life cycle', 'git', 'devops', 'jenkins', '.net', 'data structures', 'shell scripting', 'sdlc', 'soap']",2025-06-12 15:10:08
Architect (AI and Cloud),Rakuten Symphony,10 - 18 years,Not Disclosed,['Bengaluru( Kadubeesanahalli )'],"Why should you choose us? Rakuten Symphony is reimagining telecom, changing supply chain norms and disrupting outmoded thinking that threatens the industrys pursuit of rapid innovation and growth. Based on proven modern infrastructure practices, its open interface platforms make it possible to launch and operate advanced mobile services in a fraction of the time and cost of conventional approaches, with no compromise to network quality or security. Rakuten Symphony has operations in Japan, the United States, Singapore, India, South Korea, Europe, and the Middle East Africa region. For more information, visit: https://symphony.rakuten.com\n\nBuilding on the technology Rakuten used to launch Japans newest mobile network, we are taking our mobile offering global. To support our ambitions to provide an innovative cloud-native telco platform for our customers, Rakuten Symphony is looking to recruit and develop top talent from around the globe. We are looking for individuals to join our team across all functional areas of our business from sales to engineering, support functions to product development. Lets build the future of mobile telecommunications together!\n\nAbout Rakuten Rakuten Group, Inc. (TSE: 4755) is a global leader in internet services that empower individuals, communities, businesses and society. Founded in Tokyo in 1997 as an online marketplace, Rakuten has expanded to offer services in ecommerce, fintech, digital content and communications to approximately 1.5 billion members around the world. The Rakuten Group has over 27,000 employees, and operations in 30 countries and regions. For more information visit https://global.rakuten.com/corp/\n\nJob Summary:\nThe AI Architect is a senior technical leader responsible for designing and implementing the overall AI infrastructure and architecture for the organization. This role will define the technical vision for AI initiatives, select appropriate technologies and platforms, and ensure that AI systems are scalable, reliable, secure, and aligned with business requirements. The AI Architect will work closely with CTO Office, product manager, engineering manager, data scientists, machine learning engineers, and other stakeholders to build a robust and efficient AI ecosystem.\n\nMandatory Skills:\nCloud Computing Platforms (AWS, Azure, GCP).\nAI/ML Frameworks (TensorFlow, PyTorch, scikit-learn) .\nData Engineering Tools (Spark, Hadoop, Kafka).\nMicroservices Architecture.\nAI/ML as a service Deployment.\nDevOps Principles (CI/CD/CT).\nStrong understanding of AI/ML algorithms and techniques\n\nRoles & Responsibilities:\nDefine the overall AI architecture and infrastructure strategy for the organization.\nSelect appropriate technologies and platforms for AI development and deployment. • Design scalable, reliable, and secure AI systems.\nDevelop and maintain architectural blueprints and documentation.\nProvide technical leadership and guidance to tech lead, engineering manager, data scientists, machine learning engineers, and other stakeholders.\nEnsure that AI systems are aligned with business requirements and industry best practices. Evaluate new AI technologies and trends.\nCollaborate with security and compliance teams to ensure that AI systems meet regulatory requirements.\nCollaborate with CTO Office to ensure the AI strategy implemented aligned with overall business unit strategy.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Architect', 'Artificial Intelligence', 'Cloud', 'Microservice Based Architecture', 'Tensorflow', 'Pytorch', 'Ai', 'Machine Learning', 'Solution Architect', 'Scikit-Learn', 'Technical Architecture']",2025-06-12 15:10:10
Analytics Manager,Flipkart,8 - 12 years,Not Disclosed,['Bengaluru'],"Skills Required :\nStrong background in statistical modelling and experience with machine learning / data mining tools such as R, Python, SQL, Spark, SAS, Excel. High expertise in implementing machine learning and AI models\nEducation/Qualification :\nBachelors in Engineering, Computer Science, Math, Statistics, or related discipline from a reputed institute or an MBA from a reputed institute\nDesirable Skills :",,,,"['Analytics', 'R', 'Excel', 'Power BI', 'SAS', 'Qlikview', 'Tableau', 'Datastudio', 'Spark', 'Python', 'SQL']",2025-06-12 15:10:13
Development Manager AI Center of Excellence,IBM,12 - 15 years,Not Disclosed,['Bengaluru'],"Your Role & Responsibilities:\nLooking to make a significant impact\nThis is your chance to become a key part of a dynamic team of talented professionals, leading the development and deployment of innovative, industry-leading, cloud-based AI services.\nWe are seeking an experienced AI & Cloud Software Engineering Manager to join us. This leadership role focuses on guiding, mentoring, and managing a team of engineers in designing, developing, and deploying AI-based services. You will be instrumental in problem-solving, automating wide ranges of tasks, and interfacing with other teams, offering managers, and customers to solve complex problems.\nYou will build a strong, agile, and modern team culture aimed at creating world-class development and deployment environments. Your efforts will ensure industry-leading user experiences for our customers. As an essential part of the leadership team, you will contribute to the cloud services architecture and design while mentoring the next generation of cloud engineers.\nResponsibilities:\nLead, mentor, and manage a team of AI and cloud software engineers.\nOversee the planning, execution, and delivery of AI-based services and cloud solutions.\nProvide technical direction and architectural oversight for AI and cloud projects.\nInterface with other teams, managers, and customers to align project goals and deliverables.\nAddress and resolve technical challenges and roadblocks faced by the team.\nMonitor and evaluate team performance, providing feedback and development opportunities.\nStay updated with the latest trends in AI and cloud technologies to encourage continuous improvement.\nEnsure the delivery of industry-leading user experiences for customers.\nAllocate resources effectively to meet project and organizational goals.\nMaintain comprehensive documentation of projects and processes, and report progress to senior management.\nRequired education\nBachelor's Degree\nRequired technical and professional expertise\nMinimum 12-15 years of experience as Full Stack Developer with a focus on AI projects\nExperience with AI and machine learning frameworks such as scikit-learn, TensorFlow, PyTorch, LLMs, Generative AI.\nFamiliarity with AI model deployment and integration.\nSolid understanding of backend technologies, including server-side languages (Node.js, Python, Java, etc.) and databases (Cassandra, PostgreSQL, etc.).\nUnderstanding and experience with RESTful APIs, Java/J2EE, Kafka & GitHub.\nStrong experience with Cloud Technologies, Kubernetes based microservices architecture, Kafka, Object Storage, Cassandra database and docker container technologies. Knowledge on IBM Cloud Technologies will be an added advantage.\nAt least 6 years of hands-on development experience building applications with one or more of the following: Java, Spring, Liberty, Node.js, Express.js, Golang, NoSQL DB, Redis, distributed caches, containers etc.,\nAt least 3 years of experience in building and operating highly secured, distributed cloud services with one or more of the following: IBM Cloud, AWS, Azure, SRE, CI/CD, Docker, Container orchestration, performance testing, etc.,\nAt least 3 years of experience in web technologies: HTTP, REST, JSON, HTML, Ajax, JavaScript etc.,\nSolid understanding of the micro-services architecture and modern cloud programming practices. Strong ability to design a clean, developer-friendly API.\nPassionate about constant, continuous learning and applying new technologies as well as mentoring others.\nKeen troubleshooting skills and strong verbal/written communication skills.\nPreferred technical and professional experience\nPreferred Skills:\nExperience in using messaging brokers like RabbitMQ, Kafka etc.\nOperating Systems (such as Red Hat, Ubuntu, etc.)\nKnowledge of network protocols such as TCP/IP, HTTP, etc.\nExperience and working knowledge of version Control systems like Github and build tools like Maven/Gradle\nAbility to learn and apply new technologies quickly\nExperience in working on a SaaS application with high industry standard CI/CD, and development cycle processes\nStrong sense of ownership of deliverables\nUI test automation skills - Selenium and/or Puppeteer\nBeyond the requirements, candidates should be passionate about in the role:\nContinuous learning and ability to adapt to change\nWorking across global teams and collaborating across teams and organization boundaries\nFinding innovative ways to solve complex problems with cutting edge technologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Artificial intelligence', 'RESTful API', 'Java', 'Generative AI', 'scikit-learn', 'Kafka', 'J2EE', 'Node.js', 'LLM', 'machine learning', 'PyTorch', 'TensorFlow', 'Python']",2025-06-12 15:10:16
IN Senior Associate AWS AI/ML/GenAI Developer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\n& Summary We are looking for a seasoned AWS\nAI/ML/GenAI Developer\nResponsibilities\nDesign and implement AI/ML/GenAI models using AWS services such as AWS Bedrock, SageMaker, Comprehend, Rekognition, and others.\nStrong programming skills in Python, R etc\nExperience with machine learning frameworks such as TensorFlow, PyTorch, or Scikitlearn.\nKnowledge of data preprocessing, feature engineering, and model evaluation techniques.\nDevelop and deploy generative AI solutions to solve complex business problems and improve operational efficiency.\nCollaborate with data scientists, engineers, and product teams to understand requirements and translate them into technical solutions.\nOptimize and finetune machine learning models for performance and scalability. Ensure the security, reliability, and scalability of AI/ML solutions by adhering to best practices.\nMaintain and update existing AI/ML models to ensure they meet evolving business needs.\nStay uptodate with the latest advancements in AI/ML and GenAI technologies and integrate relevant innovations into our solutions.\nProvide technical guidance and mentorship to junior developers and team members.\nExcellent problemsolving skills and ability to work in a fastpaced, collaborative environment.\nGood to have AWS Certified Machine Learning Specialty or other relevant AWS certifications.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired 48 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Bachelor of Engineering, Bachelor of Technology\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'GCP', 'Data modeling', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Corporate advisory', 'Operations', 'AWS']",2025-06-12 15:10:18
IN Senior Associate AWS AI/ML/GenAI Developer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\n& Summary We are looking for a seasoned AWS\nAI/ML/GenAI Developer\nResponsibilities\nDesign and implement AI/ML/GenAI models using AWS services such as AWS Bedrock, SageMaker, Comprehend, Rekognition, and others.\nStrong programming skills in Python, R etc\nExperience with machine learning frameworks such as TensorFlow, PyTorch, or Scikitlearn.\nKnowledge of data preprocessing, feature engineering, and model evaluation techniques.\nDevelop and deploy generative AI solutions to solve complex business problems and improve operational efficiency.\nCollaborate with data scientists, engineers, and product teams to understand requirements and translate them into technical solutions.\nOptimize and finetune machine learning models for performance and scalability. Ensure the security, reliability, and scalability of AI/ML solutions by adhering to best practices.\nMaintain and update existing AI/ML models to ensure they meet evolving business needs.\nStay uptodate with the latest advancements in AI/ML and GenAI technologies and integrate relevant innovations into our solutions.\nProvide technical guidance and mentorship to junior developers and team members.\nExcellent problemsolving skills and ability to work in a fastpaced, collaborative environment.\nGood to have AWS Certified Machine Learning Specialty or other relevant AWS certifications.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired 48 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'GCP', 'Data modeling', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Corporate advisory', 'Operations', 'AWS']",2025-06-12 15:10:20
Senior AI/ML Manager,IQVIA,9 - 14 years,Not Disclosed,"['Kochi', 'Pune', 'Bengaluru']","Project Role: Senior AI/ML Manager\nWork Experience: 8 to 14 Years\nWork location: Bengaluru/Kochi\nMust Have Skills: Machine Learning, NLP, Deep Learning, AI, Python, GenAI.\nJob Description\nDevelop/leverage fit for purpose AIML models/algorithms/foundation models/processes to address pharma/healthcare applications and innovative products upon completion of prototypes followed by the building of production grade algorithms/automation engines for client deliverables. Test for viability to deliver final products to clients. Able to bring newly researched ideas to reality quickly and on a large scale. Design, build, test, and deliver products from post-prototype to client delivery.\n\nEssential Functions\nAssists with the ongoing development and implementation of an enterprise architecture. May devise and present business cases and program release plans to senior management with priority recommendations to maintain and evolve this architecture.\nBuilds effective business relationships with business line managers and provides technical and system expertise as input to product concepts.\nMay assist product development management to define IT strategic direction and assists in the mapping of projects to that strategic direction whilst ensuring product capabilities and process improvements are delivered over time within the framework of the IQVIA enterprise architecture.\nParticipates in cross-functional product development teams, may also act as a consultant to provide system and technical advice.\nKeeps up to date with technology changes and identifies opportunities for implementation in future systems.\nParticipates in R&D projects and may run those projects in compliance with standard project management practices.\nMay mentor and assist lower-level architects and business analysts.\n\nQualifications\n10+ years of experience in engineering roles with team management experience\n6-8 relevant years experience on NLP,GenAI/LLM, Machine Learning and Deep learning\n6-8 relevant years of experience on Python\nExtensively work on NLP applications, ability to work on Machine learning model, Deep learning development\nExtensive knowledge in leveraging GenAI/LLM for developing innovative solutions to complex business problems.\nSolid understanding of LLMs, including fine-tuning methodologies and deployment strategies.\nExperience on text to transform natural language into useful features.\nFind and implement the right algorithms and tools for NLP tasks.\nExtend ML libraries and frameworks to apply in NLP tasks.\nDemonstrated experience in writing effective, scalable, and maintainable code\nExperience on Writing on effective and scalable code\nExperience on Unit Testing using Pytest or equivalent framework.\nA deep understanding and multi-process architecture and the threading limitations of Python.\nMust have experience working with REST APIs, and frameworks like Flask API and Fast API. Experience with Django or Pyramid framework is a good to have\nKnowledge of NER, Knowledge graphs is good to have.\n\nEducation Qualification\nMaster's Degree masters degree in Machine Learning, Statistics, Computer Science, Physics, Math, or related field",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Natural Language Processing', 'Aiml', 'Machine Learning', 'Python', 'Agentic AI', 'Genrative Ai', 'Deep Learning']",2025-06-12 15:10:23
AI Technical Architect,Care Allianz,7 - 11 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Care Allianz is looking for AI Technical Architect_ to join our dynamic team and embark on a rewarding career journey\n\nDesigns AI-based system architectures for scalable solutions\n\nCollaborates with data scientists and engineers for model integration\n\nEnsures performance, scalability, and security of AI platforms\n\nGuides development teams in implementing AI strategies",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Technical Architect', 'Manager Technology']",2025-06-12 15:10:25
RAG Architect,Qualcomm,13 - 18 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Test Engineering\n\nGeneral Summary:\n\nJob description\n\nWe are seeking an experienced AI Architect to design, develop, and deploy Retrieval-Augmented Generation (RAG) solutions for Qualcomm Cloud AI Platforms.\n\nRoles and Responsibilities\nLead the design and development of applications for RAG AI models and provide APIs for frontend consumption. Manage the interaction between retrieval-augmented techniques and generative models.\nBuild services that connect AI models (e.g., transformers, embeddings, and vector search) to handle tasks such as query retrieval, model inference, and generating responses. Leverage frameworks like Flask, FastAPI, or Django for API development.\nDesign pipelines to preprocess, clean, and prepare data for AI model training, as well as for serving the models in production environments. Optimize these pipelines to support both batch and real-time data processing. Implement RESTful APIs or GraphQL endpoints for seamless frontend-backend interaction.\nImplement cloud solutions to host Python-based services, ensuring that AI models are scalable and that the infrastructure can handle high traffic. Leverage containerization (Docker) and orchestration (Kubernetes) for model deployment and management.\nSet up monitoring, logging, and alerting for Python backend services, ensuring smooth operation of AI features. Use tools like Prometheus, Grafana, and ELK stack for real-time performance tracking.\nContinuously optimize model performance by fine-tuning and adapting Python-based AI models for real-time use cases. Manage trade-offs between computation load, response time, and quality of generated content.\nPartner with data scientists, machine learning engineers, and mobile/web developers to ensure tight integration between AI models, mobile/web front-end, and backend infrastructure.\n\n- Experience:\n13+ years of overall SW development experience\n10+ years Strong experience in working with technologies (e.g., React, React Native, Flutter, Django, Flask, FastAPI).\n5+ years of experience in building AI applications with a focus on NLP, machine learning, generative models, and retrieval-augmented systems.\nProven experience in designing and deploying AI systems that integrate retrieval-based techniques (e.g., FAISS, Weaviate) and generative models (e.g., GPT, BERT). - Expertise in cloud platforms (e.g., AWS, GCP, Azure) and deployment of Python-based microservices.\nBuilding RESTful APIs or GraphQL services (using frameworks like Flask, FastAPI, or Django).\nHandling AI model inference and data processing (using libraries like NumPy, Pandas, TensorFlow, PyTorch, and Hugging Face Transformers).\nIntegrating vector search solutions (e.g., FAISS, Pinecone, Weaviate) with the AI models for efficient retrieval-augmented generation. - Experience with containerization (Docker) and Kubernetes for deploying scalable Python-based services.\nProficient in cloud infrastructure management, with a focus on managing Python services in the cloud.\nExperience in End-to-End product development and Software Lifecycle\n\n\nKey\n\nSkills:\n\nAdvanced proficiency in Python for building backend services and data processing pipelines. Familiarity with frameworks like Flask, Django, and FastAPI. Experience with AI libraries and frameworks (TensorFlow, PyTorch, Hugging Face Transformers).\nFamiliarity with vector databases (e.g., Pinecone, FAISS, Weaviate) and integration with retrieval-augmented systems.\nStrong knowledge of RESTful API design, GraphQL, and API security best practices (e.g., OAuth, JWT).\nExcellent problem-solving abilities and a strong focus on creating highly scalable and performant solutions.\nStrong communication skills, with the ability to collaborate across different teams and geography\nAbility to mentor junior team members and lead technical discussions.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 6+ years of Software Test Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 5+ years of Software Test Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Test Engineering or related work experience.\n\n2+ year of work experience with Software Test or System Test, developing and automating test plans, and/or tools (e.g., Source Code Control Systems, Continuous Integration Tools, and Bug Tracking Tools).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'cloud platforms', 'api', 'graphql', 'natural language processing', 's w development', 'rest api design', 'system testing', 'react native', 'machine learning', 'pipeline', 'react.js', 'flutter', 'test engineering', 'django', 'cloud infrastructure management', 'flask']",2025-06-12 15:10:27
"Sr Software Eng: Generative AI, Go/Python, AWS, Kubernetes 7-12 Yrs",Cisco,7 - 12 years,Not Disclosed,['Bengaluru'],"Meet The Team\nThe Cisco AI Software & Platform Group drives the development of groundbreaking generative AI applications. We empower Cisco's diverse product portfolio, spanning networking and security, with intelligent assistants and agents. We work on pioneering technologies that proactively defend against threats, safeguard critical business assets, and simplify security operations. Fueled by a passion for AI/ML, we strive to create a secure future for businesses. Our collaborative and passionate team thrives with tackling sophisticated challenges and delivering innovative solutions.",,,,"['Golang', 'Generative Ai', 'AWS', 'Python', 'Kubernetes', 'Java']",2025-06-12 15:10:30
Senior AI/ML Architect- Iqvia,IQVIA,10 - 18 years,Not Disclosed,"['Kochi', 'Noida', 'Bengaluru']","Job Description\nDevelop fit for purpose AIML models/algorithms/processes to address pharma/healthcare applications and innovative products upon completion of prototypes followed by the building of production grade algorithms/automation engines for client deliverables. Test for viability in order to deliver final products to clients. Able to bring newly researched ideas to reality quickly and on a large scale. Design, build, test, and deliver products from post-prototype to client delivery.\nEssential Functions\nAssists with the ongoing development and implementation of an enterprise architecture.\nMay devise and present business cases and program release plans to senior management with priority recommendations to maintain and evolve this architecture. Builds effective business relationships with business line managers and provides technical and system expertise as input to product concepts.\nMay assist product development management to define IT strategic direction and assists in the mapping of projects to that strategic direction whilst ensuring product capabilities and process improvements are delivered over time within the framework of the IQVIA enterprise architecture.\nParticipates in cross-functional product development teams, may also act as a consultant to provide system and technical advice.\nKeeps up to date with technology changes and identifies opportunities for implementation in future systems.\nParticipates in R&D projects and may run those projects in compliance with standard project management practices.\nMay mentor and assist lower level architects and business analysts\nQualifications\n10+ years of experience in engineering roles with team management experience\n5-7 relevant years experience on NLP, Machine Learning and Deep learning\n5-7 relevant years of experience on Python\nExtensively work on NLP applications, ability to work on Machine learning model, Deep learning development\nKnowledge of LLMs, fine tuning and deployment\nExperience annotated datasets for Supervised Learning methods and correction\nExperience on text to transform natural language into useful features\nFind and implement the right algorithms and tools for NLP tasks\nPerform statistical analysis of results and refine models\nExtend ML libraries and frameworks to apply in NLP tasks\nExperience on ORM, SQL Alchemy, Alembic is a must\nExperience on Writing on effective and scalable code\nExperience on Unit Testing using Pytest or equivalent framework\nA deep understanding and multi-process architecture and the threading limitations of Python.\nFamiliarity with server-side templating languages including Jinja 2 and Mako is good to have\nExperience on REST API, Flask API, Fast API is must, Django or Pyramid framework is good to have\nKnowledge of NER, Knowledge graphs is good to have\n\nEducation Qualification\nMaster's Degree Masters Degree in Machine Learning, Statistics, Computer Science, Physics, Math, or related field",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['AI', 'Machine Learning', 'Python', 'GenAI', 'NLP', 'Natural Language Processing', 'Ml']",2025-06-12 15:10:32
Lead AI Engineer,Insnapsys Technologies,4 - 8 years,Not Disclosed,[],"Lead AI Engineer: Nashik/Remote\nExperience: 7+ years in Software Engineering and 3+years in AI/ML development\n\nWhat you will do:\nLead the end-to-end design, development, and deployment of AI/ML systems and generative AI applications.\nBuild & fine-tune custom AI agents, chatbots, and automation tools.\nArchitect scalable pipelines for LLMs optimized for performance and cost-efficiency.\nImplement advanced techniques like RAG, vector search, and hybrid architectures.\nCollaborate with cross-functional Product, Engineering, and Data teams.\nEstablish MLOps infrastructure (monitoring, CI/CD, versioning, A/B testing).\nMentor and inspire a high-performing AI team.\nChampion ethical, scalable, and secure AI development practices\n\n\nWhat you Bring\n7+ years in software engineering or data science, with 3+ years in AI/ML development.\n3+ years leading AI teams and managing end-to-end projects.\nHands-on experience with LLMs like GPT, Claude, Mistral, Falcon including fine-tuning, RAG, and agent orchestration.\nStrong command over Python, PyTorch/TensorFlow, HuggingFace.\nExperience on AWS: SageMaker, Bedrock, Lambda, EC2, S3.\nKnowledge of vector databases: Pinecone, FAISS, or Weaviate.\nSystems-thinking mindset with great communication & leadership skills.\n\n\nNice to Have\nExperience with voice AI, OCR, or computer vision.\nFamiliarity with multi-agent systems, autonomous planning, or tool-using AI agents.\nContributions to open-source projects or published research in NLP, LLMs, or GenAI.\nExperience with real-time personalization, streaming data, or recommendation systems.\n\nEducation Required:\nB.Tech / M.Tech / Ph.D. in Computer Science, Artificial Intelligence, Machine Learning, or a related field from a reputed institute.\n\nWhy Join Us?\nBuild AI systems that scale and make real-world impact across industries.\nWork in a high-autonomy, innovation-driven environment using cutting-edge tools.\nJoin a team that thrives on collaboration, learning, and technical excellence.\nBe part of a mission-driven organization building ethical, secure, and transformative AI.\n\nInterested candidates please submit resumes on hr@insnapsys.com or call on 9156797671",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'LLM', 'Machine Learning', 'Python']",2025-06-12 15:10:34
"Senior Java Developer (Microservices, Emerging Technologies & Cloud)",Synechron,5 - 10 years,Not Disclosed,['Bengaluru'],"job requisition idJR1027430\n\nOverall Responsibilities:\nLead the development and implementation of projects using emerging technologies\nMentor and guide team members to ensure the successful delivery of projects\nIdentify and evaluate new technology solutions to improve business processes\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nStay up-to-date with the latest technological advancements and industry trends\n\n\nSkills:\nStrong expertise in emerging technologies such as blockchain, IoT, AI, etc.\nStrong technical knowledge of software development lifecycle\nExcellent problem-solving and critical thinking skills\nGood understanding of software architecture and design patterns\nAbility to lead and manage a team of technical experts\nExperience:\nAt least 5+ years of experience in software development and leading technology projects\nProven track record of delivering projects using emerging technologies\nExperience in mentoring and guiding junior team members\nExperience in working with cross-functional teams\nDay-to-Day Activities:\nManage the development and delivery of projects using emerging technologies\nProvide technical guidance and mentorship to junior team members\nCollaborate with cross-functional teams to ensure alignment with the organization's overall strategy\nEvaluate and recommend new technology solutions to improve business processes\nStay up-to-date with the latest technological advancements and industry trends\nQualification:\nBachelor's or Master's degree in Computer Science, Information Technology, or related field\nRelevant certifications in emerging technologies\nSoft\n\nSkills:\nStrong communication and leadership skills\nAbility to work well under pressure and meet tight deadlines\nExcellent interpersonal and team-working skills\nAbility to effectively communicate technical information to non-technical stakeholders\nPassionate about technology and a desire to stay up-to-date with the latest advancements.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['emerging technologies', 'artificial intelligence', 'iot', 'software development life cycle', 'blockchain', 'software development', 'information technology', 'technology solutions', 'microservices', 'java', 'design patterns', 'project delivery', 'leadership development']",2025-06-12 15:10:36
CPU Full Stack Python Developer (Staff/Sr. Staff),Qualcomm,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nWe are seeking a highly skilled Full Stack Python Developer to join our dynamic team. The ideal candidate should have a strong background in tool development, data science, and automation of complex tasks. You will be responsible for developing high volume regression dashboard, parametric and power tools and contributing to both front-end and back-end development.\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 4+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 3+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 2+ years of Hardware Engineering or related work experience.\n\nTechnical\n\nSkills:\n\n\n\nPythonProficiency in Python programming, including libraries like Pandas, NumPy, and SciPy for data science.\n\n\nFull Stack DevelopmentExperience with both front-end (HTML, CSS, JavaScript, React, Vue.js) and back-end (Django, Flask) technologies.\n\n\nTool DevelopmentAbility to develop parametric and power tools, possibly using frameworks like Vue.js , PyQt or Tkinter for GUI development.\n\n\nData ScienceStrong understanding of data analysis, machine learning (using libraries like scikit-learn, TensorFlow), and data visualization (using Matplotlib, Seaborn).\n\n\nAutomationExperience in automating complex tasks using scripting and tools like Selenium, Airflow, or custom automation scripts.\n\n\nSoft\n\nSkills:\n\n\n\nProblem-SolvingAbility to tackle complex problems and develop innovative solutions.\n\n\nCommunicationStrong communication skills to effectively collaborate with team members and stakeholders.\n\n\nAdaptabilityFlexibility to adapt to new technologies and methodologies.\n\n\nExperience:\n\n\nProjectsPrevious experience in developing tools and automation solutions.\n\n\nIndustry KnowledgeFamiliarity with the specific industry or domain you're working in can be a plus.\n\n\nKey Responsibilities:\n\nDevelop and maintain parametric and power tools using Python.\n\nDesign and implement automation solutions for complex tasks.\n\nCollaborate with data scientists to analyze and visualize data.\n\nBuild and maintain web applications using Django or Flask.\n\nDevelop front-end components using HTML, CSS, JavaScript, and React.\n\nIntegrate third-party APIs and services.\n\nOptimize applications for maximum speed and scalability.\n\nWrite clean, maintainable, and efficient code.\n\nTroubleshoot and debug applications.\n\nStay updated with the latest industry trends and technologies.\n\n\nPreferred Qualifications:\n\nBachelor's degree in Computer Science, Engineering, or related field.\n\nPrevious experience in tool development and automation.\n\nFamiliarity with industry-specific tools and technologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['tool development', 'data science', 'python', 'data analysis', 'machine learning', 'css', 'hiring', 'scikit-learn', 'vue.js', 'numpy', 'staffing', 'react.js', 'tensorflow', 'seaborn', 'selenium', 'pyqt', 'html', 'data visualization', 'scipy', 'hardware engineering', 'javascript', 'pandas', 'django', 'matplotlib', 'flask']",2025-06-12 15:10:39
Principal Engineer App (React Native),Endeavour Consultancy Services And Solutions,8 - 13 years,45-65 Lacs P.A.,['Bengaluru'],"Led architecture and development of high-performance React Native apps. Proficient in native build tools like Xcode and Gradle along with state management (Redux, MobX) and advanced libraries (React Navigation, Reanimated).",Industry Type: Internet (E-Commerce),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['React Native', 'Mobile App', 'Xcode', 'Redux', 'react native apps', 'CI/CD', 'Gradle', 'MobX']",2025-06-12 15:10:41
Lead : Sales & Biz Dev (AI Tools),DNEG,5 - 10 years,Not Disclosed,['Mumbai (All Areas)( Santacruz West )'],"Role Overview:\nAs the Sales & Business Development Lead for the Media vertical, you will spearhead revenue growth by building and nurturing relationships with key stakeholders in the media ecosystem. Youll work closely with technology, product and marketing teams to bring powerful AI solutions to market, while also uncovering new business opportunities that align with evolving customer needs.\n\nKey Responsibilities:\n\nBusiness Development & Partnerships\nIdentify and cultivate new business opportunities within the media, entertainment, and publishing sectors\nBuild and maintain strategic relationships with decision-makers at broadcasters, studios, agencies, and streaming services\nLead negotiations, proposals, and contract processes for key deals and partnerships\n\nSales Strategy & Execution\nOwn the complete sales cycle from lead generation to closing across multiple media segments\nDevelop a clear go-to-market strategy tailored to the unique dynamics of the media and entertainment industries\nMaintain accurate sales forecasting and pipeline management via CRM tools\n\nIndustry Expertise & Evangelism\nRepresent the company at industry events, conferences, and client meetings\nStay ahead of media technology trends and position the company as a thought leader in AI-powered media innovation\nEducate prospects and partners on the ROI and impact of AI in creative workflows, content monetization, and audience engagement\n\nCross-functional Collaboration\nWork closely with product, marketing, and technology teams to align customer needs with platform capabilities\nProvide feedback from the field to inform product development and roadmap priorities\n\nRequirements:\n5-8 years of experience in B2B sales, business development, or partnerships ideally within the media, entertainment, or advertising sectors\nProven track record of closing complex deals with media buyers, tech vendors, or content producers\nStrong understanding of AI applications in mediasuch as generative AI, video/audio analysis, content recommendation, or digital asset management\nExcellent communication, negotiation, and relationship-building skills\nComfortable working in a fast-paced, evolving startup environment\nBachelors degree in business, marketing, communications, or a related field; MBA or technical background is a plus\n\nNice to Have:\nFamiliarity with media technology stacks (e.g., MAM, DAM, CMS, OTT platforms)\nExperience working with media/ movie agencies, ad tech firms, or global media networks\nKnowledge of AI/ML concepts (NLP, computer vision, LLMs, etc.)",Industry Type: Emerging Technologies (AI/ML),Department: Sales & Business Development,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'New Market Development', 'Sales Lead Generation']",2025-06-12 15:10:43
Engineering Lead python and Gen AI Mumabi Andheri West with startup,Intelli Search Services,12 - 18 years,40-70 Lacs P.A.,['Mumbai (All Areas)'],"Job Title: Engineering Lead\nLocation: Mumbai Andheri West\n\nJob Overview:\nAs the Engineering Lead, you will be responsible for leading the end-to-end development of our Generative AI products, ensuring scalability, performance, and innovation. You will drive engineering excellence, collaborate with cross-functional teams, and build a high-performing engineering culture. This role requires deep expertise in AI/ML, Python Coding ,distributed systems, cloud architectures, and modern software engineering practices. Needs hands on coding experience\n\nKey Responsibilities:\n1. Technical Leadership & Strategy: Define and execute the technology roadmap for Generative AI products, ensuring alignment with business goals.\n2. AI/ML Product Development: Lead the development of AI-powered products, optimizing models for performance, scalability, and real-world application.\n3. Engineering Excellence: Establish best practices in software development, DevOps, MLOps, and cloud-native architectures.\n4. Team Leadership & Scaling: Recruit, mentor, and manage a high-performing engineering team, fostering a culture of innovation and collaboration.\n5. Cross-Functional Collaboration: Work closely with Product, Data Science, and Business teams to translate AI research into real-world applications.\n6. Scalability & Performance Optimization: Architect and optimize distributed systems, ensuring efficient deployment of AI models across cloud and edge environments.\n7. Security & Compliance: Implement best practices for AI ethics, data security, and compliance with industry regulations.\n\nQualifications & Skills:\n12+ years of experience in software engineering, with at least 3 years in leadership roles within AI-driven product companies.\nStrong expertise in Generative AI, Deep Learning, NLP, Computer Vision, and model deployment.\nExperience with ML frameworks and cloud platforms (AWS, GCP, Azure).\nProven ability to scale AI/ML infrastructure and optimize models for performance and cost-efficiency.\nDeep understanding of distributed systems, cloud-native architectures, and microservices.\nHands-on experience with MLOps, CI/CD, and DevOps practices.\nStrong problem-solving, strategic thinking, and stakeholder management skills.\nAbility to attract, develop, and retain top engineering talent in a competitive market.\n\nWhy Join Us?\nLead a team at the cutting edge of Generative AI innovation.\nWork on scalable, high-impact AI products shaping the future of technology.\nOpportunity to work with a truly entrepreneurial style culture that fosters imagination, innovation and teamwork.\nOpportunity to work with a collaborative and high-calibre team.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Generative Ai', 'Artificial Intelligence', 'Machine Learning', 'Coding', 'Python', 'development', 'Devops', 'cloud', 'NLP', 'infrastructure', 'CI/CD', 'Deploying Models', 'Computer Vision']",2025-06-12 15:10:46
BCN Labs_ Associate - Front end developer,Bain,1 - 2 years,Not Disclosed,['Bengaluru'],"we're seeking an Associate - Front-End Developer to join our team and bring analytical applications to life through elegant, user-friendly interfaces. you'll work closely with data scientists, product leads, and backend developers to build front-end experiences for tools and Applications, used by clients and non-technical stakeholders.\nAs an Associate - Front-End Developer, you will:\nBuild Data-Driven Interfaces : Develop responsive, performant, and intuitive user interfaces that simplify interaction with complex data and analytical frameworks.",,,,"['Backend', 'Front end', 'Version control', 'Debugging', 'Consulting', 'Javascript', 'Information technology', 'Python', 'CSS3']",2025-06-12 15:10:49
Python Developer,Infosys,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Title\nPython Developer\n\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain.\nYou will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements.\nYou will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! Technical and Professional :\nPrimary skills:Technology-Machine Learning-Python Preferred Skills:\nTechnology-Machine Learning-Python Additional Responsibilities:\nKnowledge of design principles and fundamentals of architecture\nUnderstanding of performance engineering\nKnowledge of quality processes and estimation techniques\nBasic understanding of project domain\nAbility to translate functional / nonfunctional requirements to systems requirements\nAbility to design and code complex programs\nAbility to write test cases and scenarios based on the specifications\nGood understanding of SDLC and agile methodologies\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate Educational Bachelor of Engineering Service LineInformation Systems* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SDLC', 'software development', 'report generation', 'MIS', 'Python development', 'Python Developer', 'CI/CD']",2025-06-12 15:10:51
Applied Research Center (ARC),Infosys,5 - 10 years,Not Disclosed,['Bengaluru'],"Responsibilities\n1. Emerging Tech Trends Research - Research on emerging tech trends, ecosystem of players, use cases and their applicability and impact to client businesses. Scan & curate startups, universities and tech partnerships needed and create innovation ecosystem. Rapidly design and develop PoCs in Emerging tech areas. Share design specifications with other team members, get the components developed, integrate and test. Build reusable components and develop PoCs using relevant startups and Open-source solutions.\n2. Thought Leadership - Develop showcases that demonstrate how emerging technologies can be applied in a business context, demo scenarios for the IP. Contribute towards patents, tier-1 publications, whitepapers, blogs in the relevant emerging tech area Get certified on the emerging technology, frameworks\n3. Applied Research Center Activities - Contribute to high level design development, testing and implementation of new proof of concepts in emerging tech areas.\n4. Problem Definition, Requirements - Understand technical requirements and define detailed design. Analyze the reusable components to map the given requirement to existing implementation and identify needs for enhancements\n5. IP Development - Develop program level design, modular components to implement the proposed design. Design and develop reusable components. Ensure compliance with coding standards, secure coding, KM guidelines while developing the IP\n6. Innovation Consulting - Understand client requirements and implement first of kind solutions using emerging tech expertise. Customize and extend IP for client specific features\n7. Talent Management - Mentor the team and help them acquire the identified emerging tech skill. Participate in demo sessions, hackathons8. Emerging Tech Startup Ecosystem Work with startups in providing innovative solutions to client problems and augmenting Infosys offerings\nTechnical and Professional Requirements:\nApplied Research Center [Emerging Areas]Advanced AI [SLM, Inference Scaling, Synthetic Data, Distributed Learning, Agentic AI, ANI]New Interaction Models [Spatial computing, Mixed Reality, 3D visualizations, New Experiences]Platforms and Protocols [Architecting and engineering for Performance, Uptime, Low-latency, Scalability, Efficiency, Data, Interoperability and Low cost, Beckn, CDPI]Cybersecurity [Ethical hacking, Threat Mgmt, Supply chain security & risk, Cyber Resilience]Quantum [Quantum AI, Stack, Simulation & Optimization, Cryptography, Valued use cases]Autonomous Machines [Humanoids, Industrial Robots, Drones, Smart Products]Emerging Research [Brain, AGI, Space, Semicon ]\nPreferred Skills:\nDomain->User Experience Design->Usability Principles->HCI\nFoundational->Learning Experience Design->Learning design Management->IP Management\nTechnology->X Reality (XR)->Augmented Reality\nTechnology->X Reality (XR)->Virtual Reality\nTechnology->Blockchain->Blockchain as a Service (BaaS)->AWS Blockchain\nTechnology->Robotic Process Automation->Intelligent Process Automation\nFoundational->Cybersecurity Competency Management->Cyber Competency Strategy Planning\nFoundational ->Data privacy->Privacy by design\nTechnology->Machine Learning->Generative AI\nAdditional Responsibilities:\nTechnical Competencies\nAdvanced theoretical knowledge in specific domain\nExperimental design and methodology expertise\nData analysis and interpretation skills\nPrototype development capabilities\nResearch tool proficiency relevant to domainSoft Skills and Attributes\nCollaborative mindset for cross-disciplinary research\nCommunication skills for knowledge dissemination\nCreative problem-solving approach\nIntellectual curiosity and innovation focus\nCommercial awareness for translational research\nEducational Requirements\nPhD of Computer Science,Bachelor of Engineering\nService Line\nGlobal Delivery\n* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Research & Development,"Employment Type: Full Time, Permanent","['User Experience Design', 'Agentic AI', '3D visualizations', 'SLM', 'Distributed Learning', 'Inference Scaling', 'Spatial computing', 'ANI', 'Mixed Reality', 'Synthetic Data', 'New Experiences']",2025-06-12 15:10:53
Software Developer 3,Oracle,4 - 9 years,Not Disclosed,['Bengaluru'],"Oracle Cloud Infrastructure (OCI) delivers mission-critical applications for top tier enterprises around the world. Our cloud offers unmatched hyper-scale, multi-tenant services deployed in more than 50 regions worldwide. OCI is expanding its mission beyond the traditional boundaries of public cloud to include dedicated, hybrid and multi cloud, edge computing, and more.\nAt OCI platform organization, our mission is to provide core platform services for OCI cloud and customers. We re embarking on ambitious new initiative to scale our tier-0 services for 10x growth. We re looking for hands-on engineers with expertise and passion in solving difficult problems in distributed systems and highly available services. If this is you, at Oracle you can design and build innovative new systems from the ground up. These are exciting times in our space - we are growing fast, still at an early stage, and working on ambitious new initiatives. An engineer at any level can have significant technical and business impact\nResponsibilities\nAs a member of the software engineering division, you will apply advanced knowledge of software architecture to perform software development tasks associated with developing, debugging or designing software applications or operating systems according to provided design specifications. You will collaborate with principal engineers & architects on team to build out next gen platform dataplane\nBasic Qualifications\nBS or MS degree in Computer Science or relevant technical field involving coding or equivalent practical experience\nDemonstrated ability to write great code using Java, GoLang, or similar languages\n4+ years of software development experience.\nProven ability to deliver products and experience with the full software development lifecycle\nExperience working on large-scale, highly distributed services infrastructure\nExperience working in an operational environment with mission-critical tier-one livesite servicing\nSystematic problem-solving approach, strong communication skills, a sense of ownership, and drive\nExperience designing architectures that demonstrate deep technical depth in one area, or span many products, to enable high availability, scalability, market-leading features and flexibility to meet future business demands",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'software architecture', 'Coding', 'Debugging', 'Software development life cycle', 'Infrastructure', 'Oracle', 'Application software', 'Distribution system', 'Operations']",2025-06-12 15:10:55
Developer - L4,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"The purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n\n\nDo\n\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\nDeliver\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Continuous Integration, Deployment & Monitoring of Software 100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan 2. Quality & CSAT On-Time Delivery, Manage software, Troubleshoot queries,Customer",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Machine Learning', 'software development', 'report generation', 'MIS', 'CI/CD', 'SDLC']",2025-06-12 15:10:58
Business Analyst | Amazon Now,Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"Are you ready to embark on a thrilling journey in the realm of grocery e-commerce? Were on the lookout for a team member to work on our latest initiative, operating at the forefront of innovation in a dynamic, fast-paced environment. This role demands the agility to navigate analytics landscape across multiple functions seamlessly, the resilience to thrive in a fast paced environment, excitement to handle challenges head-on and excellence in analytical abilities.\n\nAs a Business Analyst, youll be deciphering our customers ever-evolving needs and shaping solutions that elevate their experience with Amazon.\n\nWere seeking someone who thrives on ambiguity, harnessing their first-principle problem-solving skills to drive impactful outcomes. Your ability to cultivate a customer-centric mindset, coupled with a penchant for out-of-the-box thinking, will be instrumental in navigating the complex landscape of our initiative.\n\nA successful candidate will possess:\nGood analytical and quantitative skills, leveraging data and metrics to inform strategic decisions.\nImpeccable attention to detail, adept at juggling multiple projects and priorities with finesse.\nA knack for thriving in a fast-paced, innovation-driven environment, where adaptability is key.\nClear and compelling communication skills, capable of articulating data insights to diverse stakeholders.\n\nIf youre ready to challenge the status quo, lead with innovation, and leave an indelible mark on the future of e-commerce, then we want to hear from you!\n\n\nResponsibilities:\nUnderstand the various operations across Amazon Now\nDesign and develop highly available dashboards and metrics using SQL, Quicksight, and Python\nUnderstand the requirements of stakeholders and map them with the data sources/data warehouse\nOwn the delivery and backup of periodic metrics, dashboards to the leadership team\nDraw inferences and conclusions, and create dashboards and visualizations of processed data, identify trends, anomalies\nExecute high priority (i.e. cross functional, high impact) projects to improve business performance across different verticals\nPerform business analysis and data queries using appropriate tools\nWork closely with internal stakeholders such as business teams, engineering teams, and partner teams and align them with respect to your focus area\nExecute analytical projects and understanding of analytical methods (forecasting, Machine Learning Techniques, etc.)\n\nAbout the team\nWe are building and scaling the 10 minute delivery service of Amazon Bachelors degree or equivalent\nExperience defining requirements and using data and metrics to draw business insights\nExperience with SQL or ETL\n2+ years of Excel or Tableau (data manipulation, macros, charts and pivot tables) experience\nKnowledge of Microsoft Excel at an advanced level, including: pivot tables, macros, index/match, vlookup, VBA, data links, etc.\nExperience with reporting and Data Visualization tools such as Quick Sight / Tableau / Power BI or other BI packages Experience using very large datasets",,,,"['Excel', 'Business analysis', 'VLOOKUP', 'Analytical', 'Machine learning', 'Forecasting', 'Macros', 'Analytics', 'SQL', 'Python']",2025-06-12 15:11:00
Application Architect - L1,Wipro,8 - 10 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to create exceptional and detailed architectural application design and provide thought leadership and enable delivery teams to provide exceptional client engagement and satisfaction.\n\nDo\n1. Develop architectural application for the new deals/ major change requests in existing deals\na. Creates an enterprise-wide architecture that ensures systems are scalable, reliable, and manageable.\nb. Manages application assets and directs the development efforts within an enterprise to improve solution delivery and agility\nc. Guides how to construct and assemble application components and services to support solution architecture and application development\nd. Maintains the frameworks and artefacts used in the implementation of an application, with reference to the systematic architecture of the overall application portfolio\ne. Responsible for application architecture paradigms such as service-oriented architecture (SOA) and, more specifically, microservices, ensuring business achieve agility and scalability for a faster time to market\n\nf. Provide solution of RFPs received from clients and ensure overall design assurance\nDevelop a direction to manage the portfolio of to-be-solutions including systems, shared infrastructure services, applications in order to better match business outcome objectives\nAnalyse technology environment, enterprise specifics, client requirements to set a collaboration design framework/ architecture\nDepending on the clients need with particular standards and technology stacks create complete RFPs\nProvide technical leadership to the design, development and implementation of custom solutions through thoughtful use of modern technology\nDefine and understand current state solutions and identify improvements, options & tradeoffs to define target state solutions\nClearly articulate and sell architectural targets, recommendations and reusable patterns and accordingly propose investment roadmaps\nEvaluate and recommend solutions to integrate with overall technology ecosystem\nTracks industry and application trends and relates these to planning current and future IT needs\ng. Provides technical and strategic inputs during the project planning phase in the form of technical architectural designs and recommendations\nh. Account mining to find opportunities in the existing clients\ni. Collaborates with all relevant parties in order to review the objectives and constraints of solutions and determine conformance with the Enterprise Architecture.\nj. Identifies implementation risks and potential impacts.\nk. Create new revenue streams within applications as APIs that can be leveraged by clients\nl. Bring knowledge of automation in application by embracing Agile and dev-ops principles to reduce manual part\n2.Understanding application requirements and design a standardize application\na. Creating Intellectual Property in forms of services, patterns, models and organizational approaches\nb. Designing patterns, best practices and reusable applications that can be used for future references\nc. Ensure system capabilities are consumed by system components and set criteria for evaluating technical and business value in terms of Tolerate, Invest, Migrate and Eliminate\nd. Provide platform to create standardize tools, uniform design and techniques are maintained to reduce costs of maintenance\ne. Coordinating input on risks, costs and opportunities for concepts\nf. Developing customised applications for the customers aligned with their needs\ng. Perform design and code reviews thoroughly on regular basis, keeping in mind the security measures\nh. Understanding design and production procedures and standards to create prototypes and finished products\ni. Work closely with systems analysts, software developers, data managers and other team members to ensure successful production of application software\nj. Offer viable solutions for various systems and architectures to different types of businesses\nk. Seamless integration of new and existing systems to eliminate potential problems and maintain data structure and bring value in terms of development\nl. Transforming all applications into digital form and implement and evolve around mesh app and service architecture that support new technologies like IOT, blockchain, machine learning, automation, BOTS etc\n\nm.Cloud Transformation: (Migration)\nUnderstanding non-functional requirements\nProducing artefacts such as deployment architecture, interface catalogue\nIdentify internal and external dependency, vendor and internal IT management\nSupport build and testing team\nn.Cloud Transformation: (Modernization)\nUnderstanding and Defining target architecture in Integration space\nAssessing project pipeline / demand and align to target architecture\nTechnical support of delivery team in terms and POC and technical guidance\no.Keep Up-to-date with the latest technologies in the market",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Application architecture', 'application design', 'Cloud Transformation', 'solution delivery', 'Application Architect', 'application development']",2025-06-12 15:11:03
Python Senior Developer,Infosys,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Title\nPython Senior Developer\n\nResponsibilities\nSolid development experience in Data Science Arch.\nExperience in Application Architecture & Design of Java Based Applications\nGood Knowledge of Architecture and related technologies\nExperience in Integration Technologies and Architecture\nWorking knowledge of frontend and database technologies\nExcellent Analytical and Debugging Skills\nFamiliarity with Agile & DevSecOps, Log Analytics, APM\nExperience in leading the teams technically\nExperience in requirements gathering, analysis & design and estimation\nGood communication and articulation skills Technical and Professional :\nWe are seeking a skilled Python and SQL Developer to join our dynamic team. The ideal candidate will have a strong background in Python programming and SQL database management.\nDevelop and maintain Python-based applications and scripts.\nWrite efficient SQL queries for data extraction and manipulation.\nCollaborate with cross-functional teams to gather requirements and deliver solutions.\nFamiliarity with Linux operating systems.\nBasic understanding of cloud platforms (e.g., AWS, Azure, Google Cloud).\nKnowledge of Model Quantization and Pruning\nExperience playing a Data Scientist role Preferred Skills:\nPython Technology-Open System-Open System- ALL-Python Technology-Full stack-Java Full stack-Frontend(Vue.js)+Enterprise layer(Python)+DB Additional Responsibilities:\nIn-depth knowledge of design issues and best practices\nSolid understanding of object-oriented programming\nFamiliar with various design, architectural patterns and software development process.\nExperience with both external and embedded databases\nCreating database schemas that represent and support business processes\nImplementing automated testing platforms and unit tests\nGood verbal and written communication skills\nAbility to communicate with remote teams in effective manner\nHigh flexibility to travelSoft Skills\nGood verbal & written communication skills articulate value of AI to business, project managers & other team members\nAbility to break complex problem into smaller problems and create hypothesis\nInnovation and experimentation Educational Master of Computer Science,Master Of Science,Master Of Technology,MCA,Bachelor Of Comp. Applications,Bachelor Of Computer Science,Bachelor of Engineering,Bachelor Of Technology Service LineApplication Development and Maintenance* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Enterprise layer', 'software development', 'report generation', 'MIS', 'CI/CD', 'Java Full stack-Frontend', 'SDLC']",2025-06-12 15:11:05
Asset & Wealth Management - AM FI Macro Strats - Associate,Goldman Sachs,2 - 7 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Goldman Sachs, we connect people, capital and ideas to help solve problems for our clients. We are a leading global financial services firm providing investment banking, securities and investment management services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals.\nAt Goldman Sachs, our Engineers don t just make things - we make things possible. We change the world by connecting people and capital with ideas and solve the most challenging and pressing engineering problems for our clients. Our engineering teams build scalable software and systems, architect low latency infrastructure solutions, proactively guard against cyber threats, and leverage machine learning alongside financial engineering to continuously turn data into action.\nEngineering, which is comprised of our Technology Division and global strategist groups, is at the critical center of our business. Our dynamic environment requires innovative strategic thinking. Want to push the limit of digital possibilities? Start here.\nGoldman Sachs Asset & Wealth Management\nAs one of the worlds leading asset managers, our mission is to help our clients achieve their investment goals. To best serve our clients diverse and evolving needs, we have built our business to be global, broad and deep across asset classes, geographies and solutions.\nGoldman Sachs Asset & Wealth Management is one of the worlds leading asset management institutions. AWM delivers innovative investment solutions managing close to Two Trillion US Dollars on a global, multi-product platform. In addition to traditional products (e.g. Equities, Fixed Income) our product offering also includes Hedge Funds, Private Equity, Fund of Funds, Quantitative Strategies, Fundamental Equity and a Multi-Asset Pension Solutions Business. Software is engineered in a fast-paced, dynamic environment, adapting to market and customer needs to deliver robust solutions in an ever-changing business environment. AM Data Engineering builds on top of cutting edge in-house and cloud platforms complimented with a strong focus on leveraging open source solutions.\nBusiness Overview\nThe External Investing Group ( XIG ) provides investors with investment and advisory solutions across leading private equity funds, hedge fund managers, real estate managers, public equity strategies, and fixed income strategies. XIG manages globally diversified programs, targeted sector-specific strategies, customized portfolios, and a range of advisory services. Our investors access opportunities through new fund commitments, fund-of-fund investments, strategic partnerships, secondary-market investments, co-investments, and seed-capital investments. With over 350 professionals across 11 offices around the world, XIG provides manager diligence, portfolio construction, risk management, and liquidity solutions to investors, drawing on Goldman Sachs market insights and risk management expertise. We extend these global capabilities to the world s leading sovereign wealth funds, pension plans, governments, financial institutions, endowments, foundations, and family offices, for which we invest or advise on over $300 billion of alternative investments, public equity strategies, and fixed income strategies.\nWhat We Do\nWithin Asset Management, Strategists (also known as Strats ) play important roles in research, valuation, portfolio construction, and risk management analytics. A Strategist will apply quantitative and analytical methods to come up with solutions that are accurate, robust, and scalable. Strats are innovators and problem-solvers, building novel and creative solutions for manager selection, portfolio construction, and risk management. You will develop advanced computational models, architectures, and applications to meet the challenges of a rapidly growing and evolving business.\nStrats collaborate across the business to develop solutions. These daily interactions with other team members across geographies demand an ability to communicate clearly about complex financial, business, and mathematical concepts. We look for creative collaborators who evolve, adapt to change, and thrive in a fast-paced global environment.\nBasic Qualifications\nOutstanding background in a quantitative discipline, with excellent analytical, quantitative, and problem-solving skills, and demonstrated abilities in research and data visualization\nProgramming expertise in a scripting language (e.g. Python, R, Matlab)\nStrong general and technical communication skills, with an ability to effectively articulate complex financial and mathematical concepts\nCreativity and problem-solving skills\nAbility to work independently and in a team environment\n2+ years of applicable experience\nGoldman Sachs Engineering Culture",Industry Type: Banking,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Wealth management', 'Analytical', 'Fixed income', 'Investment banking', 'Asset management', 'Investment management', 'Risk management', 'Private equity', 'Analytics', 'Financial services']",2025-06-12 15:11:08
XR Systems Architect,Qualcomm,1 - 6 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nQualcomm's XR Technology Systems team is seeking motivated engineers with who will work on driving next generation technologies & platforms enabling the future of Augmented Reality / Virtual Reality / Mixed Reality applications. This team interfaces with product management, platform architecture, technology/IP and software implementation leads.\n\n\n\nThe successful candidate will be responsible for one or more of the following roles\n\nDriving technical workstreams across key technology tracks (perception, rendering, composition, reprojection, split-processing, user input interfaces, camera & display processing). Key role here is to have forward looking perspective and identify opportunities for prototyping, demonstrating the system level trade-offs (working with respective technology team(s), helping define the feature goodness criteria from end use case perspective. Collaborating with technology experts to drive strategic direction in cross-functional AR/MR/VR areas. Working with technology, hardware, and software experts to translate use-case requirements into implementation specifications and contributing to reference design planning. Engaging on identifying and scoping new use-cases Early engagement with customers and works on aligning with product management on platform requirements. Working with technology tracks to drive the system level what-ifs/trade-offs and helping with competitive analysis.\n\nFor this multi-disciplinary role an ideal candidate has experience in AR/VR, computer vision, perception, camera technology, hardware design, SoC architecture, HW/SW partitioning, and/or system modeling (power, performance, thermal).\n\nMinimum Qualifications\n\nBachelors degree in Electrical Engineering, Information Systems, Computer Science, or related field, and project experience in architecture/micro-architecture\n\n1+ years of system engineering or related work experience\n\nExcellent problem solving and communication skills\n\n\nPreferred Qualifications\n\nMasters and/or PhD degree in Electrical Engineering, Information Systems, Computer Science\n\n1+ years experience in HW architecture/design with emphasis on areas listed above\n\nProven experience in conducting architectural trade-offs, power/performance analysis and/or SW-HW trade-offs\n\nExtensive knowledge of graphics pipeline, computer vision pipelines, machine learning methods and/or camera pipelines\n\nExperience with system level modeling (performance and/or power)\n\nC/C++ programming\n\n\nKeywords Camera, ISP, display, composition, rendering, video coding, computer vision, embedded, multimedia, image, algorithms, SOC architecture, micro-architecture\n\n\n\nEducational\n\nRequiredBachelors, Computer Engineering and/or Electrical Engineering or equivalent experience\n\nPreferredMasters / Doctorate, Computer Engineering and/or Computer Science and/or Electrical Engineering\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['graphics', 'machine learning', 'pipeline', 'computer vision', 'system engineering', 'synthesis', 'algorithms', 'asic', 'sta', 'c++', 'c', 'soc', 'verilog', 'hw', 'rtl design', 'computer science', 'product management', 'fpga', 'embedded systems', 'system architecture', 'soc design', 'rtl coding', 'system verilog']",2025-06-12 15:11:11
"QAE, Professional",Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"Do you want to develop the next generation Payments products for Indias fastest growing e-commerce company? Do you enjoy working in an entrepreneurial environment solving complex technical problems and delivering innovative solutions? If so, join us on Amazon India Payments Tech team. We are a group of talented technical professionals that are empowered and driven to build innovative world class experiences for millions of Amazon customers. The India Payments Tech in Bangalore is responsible to build product and technology solutions to build great payments products and experiences using various technologies to solve complex problems related to distributed systems, scalable architecture, machine learning, and algorithms.\n\nTo meet these challenges, we are looking for a high-energy, talented quality assurance engineer. You will work with multiple development teams and stakeholders in India and worldwide, to drive test strategy and implementation. A successful candidate will have a strong technical ability, excellent project management skills, great communication skills, and a motivation to achieve results in a fast-paced environment. Prior experience with Payment technology testing is a strong plus. The person chosen for this position will have the opportunity to contribute their creative ideas and energy to new, complex and business critical products.\n\n\n1. Coordinate with multiple teams to communicate our technical requirements, drive schedules and review and help build test plans that test end-to-end functionality spanning services owned by multiple organisations.\n2. Design, execute and automate tests of front end applications, and middle and back-end software across a variety of architectures.\n3. Test systems at the user level, both manually and with automated tools.\n4. Grey box testers rather than black-box testers, able to understand software internals, debug problems using log files, and write automated tests with scripts and/or user-level automated tools.\n5. Work with Software Development Engineers and Business Owners to understand the technical implementation of features.\n6. Work with business stakeholders, designers and customer service teams to understand customer usage models and develop test plans and suites that approximate real-world environments.\n7. Help drive the software development process towards quality-centric methodologies, always seeking to avoid defects or find them at the earliest stage possible. 2+ years of quality assurance engineering experience\nExperience in automation testing\nExperience in manual testing\nExperience in UI and API automation testing (Selenium/SOAPUI) Experience in API & Mobile testing\nExperience designing and planning test conditions, test scripts, and test data sets to ensure appropriate and adequate coverage and control",,,,"['Usage', 'Manual testing', 'Front end', 'Test scripts', 'Test strategy', 'Machine learning', 'Selenium', 'Customer service', 'Distribution system', 'Mobile testing']",2025-06-12 15:11:13
XR Systems Technology Architect (2 - 10 years),Qualcomm,1 - 6 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nOrganization overview:\n\nQualcomm is a key enabler for the XR eco-system with a dominant market share. We build custom SoCs and technologies that are at the heart of existing and emerging XR products. Qualcomm XR Systems organization is responsible for architecture definition of Qualcomms next generation XR SoCs. Our portfolio of SoC offerings serve a broad range of XR products covering Mixed Reality, Augmented Reality and AI Glass product families. The span of technologies that go into these products and SoCs include high resolution immersive displays, perception features that are bulit on computer vision and deep learning technology, highly efficient DSP processors, dedicated deep learning accelerators, graphics engines supporting high resolution and high frame rate rendering and reprojection, multimedia processing engines (audio, video, imaging,..), CPUs and SoC infrastructure that ensures efficient and secure processing, as well as very low power architecture features such as power islands and power rail isolation, sleep modes etc.\n\nWe are scaling up our operations!! We are looking for engineers with background in diverse areas including architecture and micro architecture definition, design and verification of IPs and SoCs. People who have experience in areas such as SoC architecture, networks on chip, virtual memory, on-chip and off-chip memory subsystems, security architecture, CPUs, etc, also development of IPs such as camera, video, GPU, DSPs, deep learning accelerators and neural signal processors (NSP), peripherals and interfaces such as PICe, SPI etc. Also, people with strong background on pre silicon and post silicon power estimation and optimization, performance estimation, power architecture design will be highly encouraged. We have openings at senior as well as junior job levels.\n\nWe are keenly interested in you if you are someone who has gained expertise in your specific domain which could be one or more of the areas mentioned above and are excited to take the next step in your career to become architects of the SoCs that will shape the future generations of XR products!! Apart from a rewarding career and growth prospects, the organization offers a unique opportunity to learn from a diverse set of experts working collaboratively under the same roof, towards a common goal.\n\nJob Overview\n\nQualcomm's XR Systems team is seeking system architects who will work on defining the next generation SoC architectures, enabling the future of Augmented Reality / Virtual Reality / Mixed Reality applications. Responsibilities of successful candidates may span one or more of the following areas:\n\nWorking with lead XR OEMs and QCs customer-facing teams to understand end to end use cases\n\nResearching the product family roadmap to align internal IP and SoC architecture roadmap\n\nCollaborating with colleagues in the architecture team and across technology, IP and SoC teams with diverse expertise\n\nExploring architectures for power efficient and performant mapping of use cases on future SoCs and coming up with architecture proposals\n\nDefining and optimizing use case data flows\n\nUse case power modeling, estimation and optimization\n\nWorking with SoC design, and validation teams to ensure that the use case power and performance KPIs are met.\n\n\nMinimum Qualifications\n\nBachelors degree in Electrical Engineering, Information Systems, Computer Science, or related field, and project experience in architecture/micro-architecture\n\nExperience (1 - 10 years) in areas covering at least one of the followingIP and SOC design, DV, micro architecture, architecture, camera, video, GPU, DSP, NSP, CPU, security, NOCs and DRAM controller subsystems, power architecture and power and performance estimation and optimization.\n\nExcellent problem solving and communication skills\n\n\nPreferred Qualifications\n\nMasters and/or PhD degree in Electrical Engineering, Information Systems, Computer Science\n\nExperience with Mixed & Augmented reality system design, constraints, and trade-offs\n\nDeep understanding of system architecture aspects such as NOCs, DRAM controller performance issues, power domains and sleep modes of memories, IPs and cores.\n\nProven experience in conducting architectural trade-offs, power/performance analysis and/or SW-HW trade-offs\n\nExperience with system level modeling (performance and/or power)\n\nProficiency in scripting languages such as python, perl, shell etc.\n\n\nKeywords\n\nCamera, GPU, CPU, SOC, SoC architecture, NOC, DDR subsystem, LPDDR IP, caches, security, virtual memory, development, RTL design, Computer vision, Artificial Intelligence, ML, DSP, AR, VR, MR, XR\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'system architecture', 'aspect', 'scripting languages', 'system engineering', 'synthesis', 'asic', 'cdc', 'soc', 'system design', 'rtl', 'vhdl', 'verilog', 'microservices', 'lint', 'rtl design', 'computer science', 'fpga', 'fpga design', 'soc design', 'rtl coding', 'shell scripting', 'perl', 'system verilog']",2025-06-12 15:11:16
"SDET, Alexa Audio",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Do you want to transform the way people interact with digital content? Come join the team that builds Alexa Audio platform. We are innovating and changing the way our customers interact with media services and devices! You ll be among the first ones in the space and get to define the customer experience, frameworks and processes for the years to come.\nOn the Alexa Audio Team, you will have an enormous opportunity to impact the customer experience, design, architecture, and implementation of new and exciting products that will be used every day by people you know. We re looking for people who are passionate about innovating on behalf of customers, demonstrate a high degree of product ownership, and want to have fun while they make history.\n\n\n* Lead the design and development of quality infrastructure that encourages the optimal combination of unit/component/integration and end to end tests.\n* Lead the design and development of performance testing and platform verification kit.\n* Collaborate with software engineering teams, driving continuous development, integration and deployment\n* Working with Principal Engineers and other Senior engineers to develop best practices for software quality assurance across multiple software teams\n* Actively participate in cross team design reviews\n* Assist in the career development of others, actively mentoring individuals and the community\n* Exert technical influence over multiple teams, increasing their productivity and effectiveness by sharing your deep knowledge and expertise.\n* You will have a profound impact on millions of customers.\n* Report on status of development, quality, operations, and system performance to management\n* The key requirement for this position is established skill designing and developing complex, interactive customer experiences.\n\nIf you have an entrepreneurial spirit, know how to deliver, are deeply technical, highly innovative and long for the opportunity to build pioneering solutions to challenging problems, we want to talk to you.\n\nAbout the team\nAlexa Audio: As Alexa Audio, we own the audio experiences on Alexa enabled devices. These experiences include Music, Podcast, Audio Books, Radio, and Ambient soundscapes. Our integration skill kits enable seamless integration of 1P / 3P audio content providers (e.g., Amazon Music, Spotify, Apple etc.) with Alexa devices. Audio is one of the most widely used experiences on Alexa, and is enjoyed by millions of customers on a daily basis.\n\nThere is a universe of tens of millions of distinct audio content that is available across streaming services. Using your voice, and timely proactive suggestions are the easiest ways to get to what you want. Alexa makes this magical for customers: ask any Alexa-enabled device to play your favorite song, podcast, book or a station, and Alexa will find the right content for you and play it. We also support being able to control and manage playback across multiple devices, and movement of media playback across them. It is still Day 1 for the audio experiences, and were looking for a leader to help us make it even better.\n\nAs SDET you will be responsible for building highly scaled systems that ensure Alexa continues to delight customers world wide.\n3+ years of non-internship professional software development testing experience\n3+ years of test automation frameworks and tools building experience\nExperience programming with at least one modern language such as Java, C++, or C# including object-oriented design Knowledge of overall system architecture, scalability, reliability, and performance in a database environment\nExperience with security in service-oriented architectures and web services\nBachelor s Degree in Computer Science or related technical field",,,,"['Computer science', 'Object oriented design', 'System architecture', 'Career development', 'C++', 'digital content', 'Performance testing', 'Customer experience', 'Internship', 'Software quality assurance']",2025-06-12 15:11:18
XR Systems & Software Architect XR Research Staff,Qualcomm,9 - 14 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nQualcomm XR Research India is rapidly expanding to offer state of the art XR solutions. To scale and strengthen our offering in this domain, we are seeking a systems architect who will drive the next-generation technologies and architecture, shaping the future of Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR) use cases.\n\nResponsibilities:\n\nYour responsibilities will span across technical leadership, system architecture, software architecture, implementation and compute analysis. Here are the key aspects of your role:\n\nDrive technical workstreams related to perception, reprojection, split-processing and other XR technologies.\n\nIdentify and deeply understand the use cases for AR/VR/MR applications. Collaborate with cross-functional teams to translate use case requirements into detailed implementation specifications.\n\nDefine system and software architecture, considering hardware/software tradeoffs, compute and memory constraints. Optimize compute workload distribution across different subsystems on the SoC for efficient performance and power.\n\nValidate and optimize architecture definitions through system-level use case modeling.\n\nPrototype new use cases to understand the compute and memory requirements, and influence future software/hardware features and reference device specification.\n\n\nMinimum Qualifications:\n\n9+ years of experience in systems engineering with a bachelors degree in electrical engineering, information systems, computer science, or related field.\n\nHands-on experience in defining systems architecture and software design for multi-core architectures (CPUs, GPUs, DSPs, etc.), including performance analysis on heterogeneous architectures (core, multi-level cache, memory, etc.).\n\nProficiency in documenting call flows and data flows for both software and hardware components.\n\nStrong communication skills and ability to work effectively in a team.\n\n\nPreferred Qualifications:\n\n8+ years of experience in systems engineering with masters and/or PhD degree in electrical engineering, information systems, computer science.\n\nProven expertise in AR/VR, computer vision, machine learning, perception, camera technology, graphics pipeline, hardware design, SoC architecture, HW/SW partitioning, and system modeling (power, performance).\n\nProficiency in C++, and Object-Oriented SW design\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\n\n2+ years of work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Research & Development,"Employment Type: Full Time, Permanent","['virtual reality', 'c++', 'machine learning', 'computer vision', 'call flow', 'synthesis', 'python', 'c', 'data validation', 'graphics', 'prototype', 'verilog', 'hw', 'pipeline', 'rtl design', 'java', 'computer science', 'hardware design', 'soc design', 'software engineering', 'data flow', 'system engineering']",2025-06-12 15:11:21
"Business Research Analyst - II, RBS ACCX Program",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon.com strives to be Earths most customer-centric company where people can find and discover virtually anything they want to buy online. By giving customers more of what they want low prices, vast selection, and convenience Amazon.com continues to grow and evolve as a world-class e-commerce platform. Amazons evolution from Web site to e-commerce partner to development platform is driven by the spirit of innovation that is part of the companys DNA. The worlds brightest technology minds come to Amazon.com to research and develop technology that improves the lives of shoppers and sellers around the world.\n\nOverview of the role\nThe Business Research Analyst will be responsible for Data and Machine learning part of continuous improvement projects across the Discoverability space. This will require collaboration with local and global teams. The Research Analyst should be a self-starter who is passionate about discovering and solving complicated problems, learning complex systems, working with numbers, and organizing and communicating data and reports. The Research Analyst will perform Big data analysis to identify patterns, train model to generate product to product relationship and product to brand & model relationship. The Research Analyst is also expected to continuously improve the ML/LLM solutions in terms of precision & recall, efficiency and scalability. The Research Analyst should be able to write clear and detailed functional specifications based on business requirements.\n\n\nScoping, driving and delivering complex projects across multiple teams.\nPerforms root cause analysis by understanding the data need, get data / pull the data and analyze it to form the hypothesis and validate it using data.\nBuild programs to create a culture of continuous improvement within the business unit, and foster a customer-centric focus on the quality, productivity, and scalability of our services.\nFind the scalable solution for business problem by executing pilots and build Deterministic and ML/LLM models.\nManages meetings, business and technical discussions regarding their part of the projects.\nMakes recommendations and decisions that impact development schedules and the success for a product or project.\nDrives team(s)/partners to meet program and/or product goals.\nCoordinates design effort between internal team and External team to develop optimal solutions.\nPerforms supporting research, conduct analysis of the bigger part of the projects and effectively interpret reports to identify opportunities, optimize processes, and implement changes.\nAbility to convince and interact with stakeholders at all level either to gather data and information or to execute and implement according to the plan.\nAbility to deal with ambiguity and problem solver\nCommunicate ideas effectively and with influence (both verbally and in writing), within and outside the team.\n\nKey Performance Areas:\nSolve large and complex business problems by aligning multiple teams together.\nData analytics and Data Sciences\nMachine learning\nProject/Program Management\nAutomation initiative conceptualization and implementation\nBig Data analytics\nProduct development Scoping and Testing\nDefect Elimination\nAgile Continuous Improvement\n\nAbout the team\nThe RBS group in Chennai/Bangalore is an integral part of Amazon online product lifecycle and buying operations. The team is designed to ensure Amazon remains competitive in the online retail space with the best price, wide selection and good product information. The team s primary role is to create and enhance retail selection on the worldwide Amazon online catalog. The tasks handled by this group have a direct impact on customer buying decisions and online user experience. 3+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc. experience\nExperience with data visualization using Tableau, Quicksight, or similar tools\nExperience with data modeling, warehousing and building ETL pipelines\nExperience writing complex SQL queries\nExperience in Statistical Analysis packages such as R, SAS and Matlab\nExperience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling Experience with AWS solutions such as EC2, DynamoDB, S3, and Redshift\nExperience in data mining, ETL, etc. and using databases in a business environment with large-scale, complex datasets",,,,"['Automation', 'Data analysis', 'SAS', 'Data modeling', 'Machine learning', 'Agile', 'Oracle', 'Data mining', 'MATLAB', 'Python']",2025-06-12 15:11:23
IN_Manager_Agentic Chatbot_Advisory Corporate_Advisory,PwC Service Delivery Center,8 - 13 years,Not Disclosed,['Bengaluru'],"FS XSector\nSpecialism\nOperations\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nThose in intelligent automation at PwC will focus on conducting process mining, designing next generation small and largescale automation solutions, and implementing intelligent process automation, robotic process automation and digital workflow solutions to help clients achieve operational efficiencies and reduce costs.\nSeeking an innovative Agentic Chatbot proficient in NodeJS, Python. The candidate will design and implement conversational agents to improve client interactions and automate processes.\nRole and responsibilities\nDevelop and maintain chatbot solutions using industrystandard frameworks and tools.\nIntegrate chatbots with existing service platforms using NodeJS and Python.\nImplement natural language processing (NLP) techniques for effective communication.\nCollaborate with product teams to ensure chatbot functionality aligns with business objectives.\nRequirements\nExperience in chatbot development with knowledge of bot frameworks.\nProficiency in NodeJS, Python.\nKnowledge in Generative AI, RAG, indexing, Langchain, Amazon Bedrock, gRPC tools, Cloud services (AWS, Azure).\nExperience in NLP and machine learning related to conversational AI.\nStrong troubleshooting and debugging skills.\nAbility to work collaboratively in a fastpaced environment.\nGood to have\nHandson experience in Voice and chatbots.\nHandson experience in Azure Cognitive Services, Genesys, Azure Bot framework..\nMandatory Skills Set\nProject Management\nStake holder Management\nProject planning\nPrefered Skills Set\nAgentic Chatbot\nYears of experience required\n8+ years of experience in GenAI, machine learning, and architectural design\nBachelor s or Master s degree in Computer Science, Engineering, or a related field\nEducation Qualification BE/B.Tech/MBA/ CA\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Chartered Accountant Diploma, Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nChatbots\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Methodology, Analytical Thinking, Automation Algorithms, Automation Engineering, Automation Framework Design and Development, Automation Programming, Automation Solutions, Automation Studio, Automation System Efficiency, Blue Prism, Business Analysis, Business Performance Management, Business Process Analysis, Business Process Automation (BPA), Business Transformation, Business Value Optimization, C++ Programming Language, Coaching and Feedback, Cognitive Automation, Communication, Conducting Discovery, Configuration Management (CM) {+ 41 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Mining', 'Business transformation', 'Performance management', 'Business analysis', 'Project management', 'Analytical', 'Project planning', 'Troubleshooting', 'Operations', 'Analytics']",2025-06-12 15:11:26
Chipset Architect,Qualcomm,12 - 17 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Chipset Architect, you will research, design, develop, simulate, and/or validate systems-level hardware, software, architecture, algorithms, and solutions that enables the development of cutting-edge technology. Qualcomm Systems Engineers collaborate across functional teams to meet and exceed system-level requirements and standards.\n\nMinimum Qualifications\n\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 12+ years of Systems Engineering or related work experience.ORMaster's degree in Engineering, Information Systems, Computer Science, or related field and 10+ years of Systems Engineering or related work experience.\n\nPreferred Qualifications\n\nMaster's Degree in Engineering, Information Systems, Computer Science or related field.\n\n15+ years of Systems Engineering or related work experience.\n\nExpert knowledge of interfaces (USB, PCIe, SD/eMMC, UFS, LPDDRx, CSI, DSI, SPI, I2C, I3C, PMBUS, SPMI, Slimbus etc)\n\n10+ years of experience working in PC or IoT industry;\n\nKnowledge of commercial, industrial and home automation systems\n\nExperience with full product lifecycle from requirements gathering, prototype development, production, knowledge of PCB design flow, EDA tools, electrical and thermal simulations.\n\nFamiliarity with software stack and hardware-software dependencies including HLOS, drivers, kernel, BIOS\n\nFamiliarity with certification, shock and vibration testing and EMI compliance\n\nIn depth knowledge of power delivery, PDN, component selection, tuning, PMICs, eBOM\n\nGood domain knowledge of 2 or more functional areas of SoCs such as Application Processor, Display, Graphics, Camera, Video, AI, Modem (4G/5G), WLAN, Power Management etc.\n\nExposure to 4G/5G/6G systems and associated cellular standards (e.g. 3GPP NR, LTE).\n\nWorking with a wide cross functional team comprising of various design and software teams, reliability, functional safety, business, sales and customer engineering\n\n\nPrincipal Duties and Responsibilities\n\nApplies Systems knowledge to evaluate Industrial customer asks and propose chipset solutions utilizing Qualcomm ICs.\n\nDevelops and analyzes system level design including requirements, interface definition, functional/performance definition, and implementation of a new system or modification of an existing system.\n\nEvaluates interface signaling for clock, data, sideband requirements and propose solution with internal and third-party components.\n\nEvaluates signal integrity needs and identifies architecture for high-speed interfaces, topologies (stacked boards, different form factors etc), timing needs, and signal conditioning techniques.\n\nUnderstand power requirements and propose suitable powering schemes for the entire platform.\n\nCollaborates with own team and other teams to complete project work, including implementing and testing features and verifying the accuracy of systems.\n\nPerforms functional analysis to drive requirements and specifications and to define and align with standards for hardware and software.\n\nReviews internal and customer board schematics\n\nDevelops new and innovative ideas (e.g. IDFs) for a product or feature area.\n\nDrives triage of problems at the system level to determine root cause and presents results of testing and debugging to team members.\n\nLevel of Responsibility\n\nWorks independently with minimum supervision.\n\nDecision-making may affect work beyond immediate work group.\n\nRequires verbal and written communication skills to convey information. May require basic negotiation, influence, tact, etc.\n\nHas a moderate amount of influence over key organizational decisions (e.g., is consulted by senior leadership to make key decisions).\n\nTasks require multiple steps which can be performed in various orders; some planning, problem-solving, and prioritization must occur to complete the tasks effectively.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['usb', 'sd', 'i2c', 'spi', 'pcie', 'algorithms', 'ufs', 'power system', 'simulation', '3gpp', 'pcb', 'sales', 'artificial intelligence', 'iot', '5g', 'ebom', 'computer science', 'debugging', 'lte', 'digital transformation', 'functional safety', 'system engineering', 'prototype', 'component selection', 'pc', '4g', 'csi']",2025-06-12 15:11:28
Staff HPC Software Developer,Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Area: Information Technology Group, Information Technology Group > IT Software Developer\n\nGeneral Summary:\n\nWhats in it for youQualcomm is enabling a world where everyone and everything can be intelligently connected. Qualcomm 5G and AI innovations are the power behind the connected intelligent edge. Youll find our technologies behind and inside the innovations that deliver significant value across multiple industries and to billions of people every day.Qualcomm engineering teams rely heavily on the latest High Performance Computing (HPC) technologies to design and develop new products using electronic design automation (EDA) tools. This role provides an opportunity to work on the latest HPC technologies and gain experience in building scalable and fault-tolerant software solutions that are deployed on some of the largest supercomputing infrastructures across the globe.What are we looking forEngineering Software Solutions and Data Services team (ESSDS) is looking for an experienced software developer with strong HPC background. The ESSDS team is responsible for development of software solutions enabling High Performance Compute grid and large-scale, distributed, analytical applications. They work on components and services for HPC infrastructure optimization, hardware IP management systems, petabyte-scale cloud data platforms and development of machine learning solutions and pipelines.This is an individual contributor technical role providing subject matter expertise (SME) across the portfolio of HPC software products and services being developed by ESSDS team. The ideal candidate would be a seasoned software developer who is skilled in many of the following areascluster infrastructure management, job scheduling and orchestration, parallel programming, performance tuning and optimizations, efficient algorithms and data structures, compute/storage/network architectures, cloud computing, GPU computing, and EDA workflows.What will you doThis roles responsibilities include:- Design and develop software solutions and services for HPC infrastructure running EDA workflows and AI workloads- Identify opportunities and deliver solutions for EDA workflow optimizations- Provide HPC expertise across portfolio of projects, guiding and mentoring a team of software developers as needed- Execute projects in partnership with global Engineering IT teams- Manage and track the software development process from development to production release in collaboration with other software developersWhat do we want to seeThe ideal candidate will be able to demonstrate some of the following skills:- 8+ years of hand-on experience in developing software solutions for HPC grid infrastructure- Broad knowledge of latest compute, storage and networking architectures- Experience of building HPC infrastructure in public cloud environments such as AWS, Azure or Google Cloud- Proven expertise in parallel and distributed programming, GPU computing and performance engineering- Proficiency in programming languages such as Python, C++, Java, Rust- Deep understanding of HPC job schedulers such as LSF, Slurm and PBS- Familiarity with EDA and semiconductor design process- Exposure to AI and ML workloads running on HPC infrastructure- Expertise in software lifecycle management, version control, and CI/CD best practices for quality, agility and security- Ability to explain technical concepts and analysis implications in a clear manner to a wide audience.- Bachelors or Masters in Computer Science, Computational Science or related field\n\nMinimum Qualifications:\n5+ years of IT-relevant work experience with Bachelor's degree in a technical field (e.g., Computer Engineering, Computer Science, Information Systems).\nOR\n7+ years of IT-relevant work experience without a Bachelors degree.\n\n4+ years of work experience with Full-stack Application Development (e.g., Java, Python, JavaScript, etc.).\n3+ years of work experience with Data Structures, algorithms, and data stores.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'version control', 'ci/cd', 'networking', 'performance engineering', 'algorithms', 'python', 'c++', 'microsoft azure', 'distributed architecture', 'javascript', 'application development', 'java', 'gcp', 'infrastructure', 'hpc', 'data structures', 'aws', 'parallel computing']",2025-06-12 15:11:31
Senior - TM ESW Product Owner,Volvo India,10 - 15 years,Not Disclosed,['Bengaluru'],"Transport is at the core of modern society. Imagine using your expertise to shape sustainable transport and infrastructure solutions for the future? If you seek to make a difference on a global scale, working with next-gen technologies and the sharpest collaborative teams, then we could be a perfect match.\nWhat you will do\nTitle : Thermal Management(Specialist - ESW Engineer)\nAre you ready to make electrifying connections? Help us to design sustainable transportation solutions for the future. As part of the Volvo Group Vehicle Technology team, you ll help us accelerate our journey by engineering exciting next-gen technologies with a global reach. Be part of our evolution as we strengthen our team. Bring your love of developing systems, working collaboratively, and your advanced skills to a place where you can make an impact.\nWe are looking to hire a skilled embedded software engineer to join our dynamic software team. As an embedded software engineer, you will be responsible for executing complete embedded software life cycles for company and client hardware.\nYour Future Team :\nThermal Management is a department within Vehicle Technology responsible for developing, delivering, and maintaining an optimized cab climate and vehicle cooling & heating systems for all types of propulsion installations to all truck brands within the Volvo Group.\nWe are responsible for leading the work with strategies and advanced engineering globally. We are located at Gothenburg & Bengaluru, and we have close cooperation with the sites in Greensboro and Lyon.\nWe understand the final customer needs and apply our knowledge to develop technical concepts and solutions that satisfy customer and business needs. The work is based on innovation, shared technology, common architecture, and brand uniqueness.\nWho are you?\nDo you dream big? We do too, and we are excited to grow together.\nTo be successful in this role, we believe that you are a team player, high on energy, and are interested in working with global sites.\nAs a person, you can take own initiatives and drive them forward with a business and customer mindset. You have excellent communication skills & good at networking with people. You also have a positive attitude and adapt to changing conditions.\nYou have an innovative and creative mindset to recommend engineering solutions and the willingness to learn and develop your own skills and abilities. In addition to this, you hold the below experience: -\nMandatory Skills:\nME/Mtech/BE/BTech in Electrical/Electronics/Computer Science\nMinimum 10+ years of software development experience in Automotive Embedded SW development within the distributed systems & Thermal management and/or Relevant Automotive System\nGood knowledge in Matlab, Simulink, Embedded C, ASPICE Process\nExperience in hands-on control Software development and troubleshooting on embedded targets.\nProven experience in embedded systems design with pre-emptive, multitasking real-time operating systems.\nExcellent knowledge of coding techniques, IP protocols, interfaces and hardware subsystems\nCritical thinker.\nStrong documentation and writing skills.\nAdequate knowledge:\nFamiliarity with software configuration management tools, defect tracking tools, and peer review\nAdequate knowledge of reading schematics and data sheets for components\nKnowledge of the product development life cycle and change management activities for maintenance are required for this role.\nLeading tasks independently, good planning and monitoring skills.\nExperience in future technologies (e-MOB, Fuel Cell, automation) in Software Development\nGood in compiling and presenting information verbally and in writing.\nWhat s in it for you?\nYour key responsibilities Include: -\nResponsible for embedded solutions for Cabin Climate Control software(Driver / IO / Application / Parameter calibration)\nDeveloping System Software Requirements, Supporting with timeplans/roadmaps\nTake full responsibility and independently deliver the planned tasks as well as guide junior analysts within the software team.\nDesign/Implement I/O and Application software of thermal embedded devices and systems like HVAC/Cabin Climate Systems, Cooling/Heating systems\nFunction Development of Feed forward logic and control systems.\nDriving Functional/Non functonal Diagnosis logics software systems\nAnalyzing and enhancing efficiency, stability and scalability of system resources\nIntegrate and validate new product designs.\nSupport software QA and optimize I/O performance.\nProvide post-production support.\nInterface with hardware design and development\nSupport cross functional Teams with control logic and AI/ML implementations.\nContribute with component and system engineers to define and verify the actions agreed related to virtual simulation in FMEA, to reduce the probability of failure occurrences.\nEstablish a strong network with global counterparts and cross functions.\nDevelop the strategy to integrate data analytics and machine learning in the models ultimately leading to development of thermal management digital twins in collaboration with the data analytics and machine learning team\n\nWe value your data privacy and therefore do not accept applications via mail.\nWho we are and what we believe in\n.\nApplying to this job offers you the opportunity to join Volvo Group. Every day, across the globe, our trucks, buses, engines, construction equipment, financial services, and solutions make modern life possible. We are almost 100,000 people empowered to shape the future landscape of efficient, safe and sustainable transport solutions. Fulfilling our mission creates countless career opportunities for talents with sharp minds and passion across the group s leading brands and entities.\nGroup Trucks Technology are seeking talents to help design sustainable transportation solutions for the future. As part of our team, you ll help us by engineering exciting next-gen technologies and contribute to projects that determine new, sustainable solutions. Bring your love of developing systems, working collaboratively, and your advanced skills to a place where you can make an impact. Join our design shift that leaves society in good shape for the next generation.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['HVAC', 'Automation', 'Simulation', 'Hardware design', 'thermal', 'Simulink', 'Troubleshooting', 'MATLAB', 'Automotive', 'Embedded software']",2025-06-12 15:11:33
"Training Manager II, OPTIMA",Amazon,6 - 11 years,Not Disclosed,['Bengaluru'],"The OPTIMA team is seeking a Program Manager, Training.\n\nOPTIMA is a global team and enables Amazon to deliver a superior shopping experience to customers worldwide. We aspire to provide an end-to-end data solution for the LLM lifecycle, leveraging technology alongside our operational excellence. We enable shopping feature teams deliver superior CX quality by providing them reliable and comprehensive insights and ground truth data to measure and train ML (Machine Learning) models and handle annotation and Root Cause Analysis (RCA) across 10 different languages.\n\nThe Training Manager II will be responsible for planning, coordinating, executing and delivering learning and development programs/training for OPTIMA business. The role demands thought clarity, dynamic cross-functional partnership, and strategic thinking. The ideal candidate will be comfortable influencing stakeholders and senior leaders, have strong analytical skills, a track record of using data and tools to drive business impact and be comfortable working in an ambiguous environment.\n\n\nServe as a multi-threaded leader for training and development across various Processes in Optima.\nSchedule large-scale training initiatives, tracking training completion, and reporting out on training progress. Own New Program launch and New Hire Onboarding, performance enhancement of programs\nConsult on learning strategies and effectiveness and gather feedback to improve Learning & Development programs.\nHandle a direct span of trainers, provides regular coaching and feedback to help grow individual functional skills and leadership capability.\nCollaborate with both local and global stakeholders to support Training programs and initiatives.\nEnhance existing training programs, review and supervise the designing of training content for any new process, program and feature/SOP roll out.\nGraduation or Post Graduation in related field.\n6+ years experience working in Training and People management.\nData skills and the ability to understand how learning activities and responsibilities play into the metrics that drive team success.\nThe ability to work in fast-paced ambiguous environments, adapting quickly to changing circumstances, processes and priorities.\nDemonstrated use of multiple learning methods and linking appropriate methods with learners and outcomes.\nAbility to influence stakeholders at all levels to understand their role in employee development and help build their skills.\nDetail-oriented, team-focused, and a quick problem-solver.\nFull proficiency in MS Office\nFamiliarity with online learning technology (e.g., Articulate Story line).\nProven ability to identify opportunities and launch original learning solution(s) with real impact.\nExperience in Learning Management system and Knowledge management systems.\nExperience in driving process improvement projects.\nExperience in requirement gathering and ability to write clear and detailed requirement document",,,,"['RCA', 'Root cause analysis', 'Employee development', 'Operational excellence', 'LMS', 'Management systems', 'Process improvement', 'Machine learning', 'Training and Development', 'MS Office']",2025-06-12 15:11:36
Python Developer-PAN INDIA_RA,Infosys,3 - 6 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Responsibilities\nWrite clean, maintainable, and efficient Python code for backend services or applications. Develop RESTful APIs or work on web applications using frameworks like Django or Flask. Implement data extraction, transformation, and loading (ETL) processes using Python. Collaborate with front-end developers and other team members to ensure seamless integration. Test and debug applications to ensure they meet quality and performance standards.\nParticipate in code reviews and contribute to the development of coding standards.\nKeep up-to-date with Python libraries and tools relevant to the project.\n\nTechnical and Professional Requirements:\nPrimary skills: Technology->Machine Learning->Python\n\nPreferred Skills: Technology->Machine Learning->Python\n\nAdditional Responsibilities:\nSkills Required:\nStrong proficiency in Python 3.x.\nExperience with at least one Python web framework (Django, Flask, etc.).\nKnowledge of database technologies, including SQL and ORM (e.g., SQLAlchemy).\nFamiliarity with version control systems like Git.\nBasic understanding of front-end technologies (HTML, CSS, JavaScript).\nExperience with unit testing frameworks like pytest or unittest.\nKnowledge of REST API design principles.\nEducational Requirements\nBCA/MCA/B.Tech/BE/M.Tech/ME/BSC/MSC",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Django', 'Flask']",2025-06-12 15:11:39
Python developer - Infosys @ Pan India,Infosys,2 - 7 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Responsibilities A day in the life of an Infoscion • As part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain. • You will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements. • You will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers. • You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nTechnical and Professional Requirements: • Primary skills:Technology->Machine Learning->Python\n\nPreferred Skills: Technology->Machine Learning->Python\n\nAdditional Responsibilities: • Knowledge of design principles and fundamentals of architecture • Understanding of performance engineering • Knowledge of quality processes and estimation techniques • Basic understanding of project domain • Ability to translate functional / nonfunctional requirements to systems requirements • Ability to design and code complex programs • Ability to write test cases and scenarios based on the specifications • Good understanding of SDLC and agile methodologies • Awareness of latest technologies and trends • Logical thinking and problem solving skills along with an ability to collaborate\n\nEducational RequirementsMCA,MSc,MTech,Bachelor of Engineering,BCA,BSc,BTech",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Django', 'Python Development']",2025-06-12 15:11:41
AI/ML Engineer,Ravionics,4 - 9 years,Not Disclosed,['Bengaluru'],"Making a career change is a big decision. Why consider Revionics?\nJoin a team of remarkable colleagues who are deeply committed to creating and delivering cutting-edge solutions for the global retail market. At Revionics, we are dedicated to helping you achieve and surpass your career aspirations. Youll enjoy access to industry-leading training programs, global development opportunities, and the chance to thrive within a diverse culture spanning offices in nine countries. Our inclusive culture is rooted in our Companys purpose:\nto make a difference for every colleague, every client, every day\n.\nRevionics sets the standard in retail pricing innovation by leveraging advanced artificial intelligence technologies, including predictive AI, conversational AI, generative AI, and agentic AI. These cutting-edge tools streamline the retail pricing lifecycle, driving measurable business success for our clients. Each year, our solutions enable pricing strategies for retail products that collectively generate over $3 trillion in revenue across leading grocery, health and beauty, DIY, and convenience retailers worldwide.\nWe invite you to join us in bringing innovative solutions to market as part of the worldwide leader in retail pricing.\nAptos market-leading platform drives the world s largest retailers in terms of their product pricing, promotion and merchandising decisions worldwide. Over 33,000 retail locations and $200+B in annual revenue across grocery, drug, convenience, general merchandise, discount, sporting goods stores, fashion, and eCommerce sites optimize with Aptos solutions.\nAptos acquired Revionics in September 2020. Revionics is the worldwide leader in retail pricing, with 20 years of experience delivering AI/ML-driven retail pricing and promotions SaaS solutions for some of the largest and best-known retailers in the world, affecting over $500B revenue under management across more than 50 retailers.\nThe AI team, within the Product Org, plays a central role at the company and is responsible for the GenAI (agents, conversational analytics etc.) and Predictive AI solutions (modeling, forecasting, optimization, etc) at Revionics. As an engineer on the Science team, you will be part of a skilled and diverse team while working with a mix of data scientists and engineers. You ll not only have the opportunity to learn/use state-of-art AI/GenAI and ML techniques but also implement/roll-out modern engineering frameworks and solve problems that have not been solved before.\nIf you re someone who is ready to take on a challenge, drive change, and be part of an awesome team, this is the right role for you!\nAbout the Role:\nThe engineer will be responsible for designing, building, deploying, and evolving the end-to-end AI/ML systems at Aptos (demand modeling and forecasting, optimization, GenAI agents, etc.)\nWho you are?\nYou have a Bachelors/Master s degree in computer science, engineering, or related STEM field, or equivalent work experience\nStrong algorithmic problem-solving skills and an analytical mindset\nHunger to learn new domains and complex code bases\n4+ years of development experienced with Python or another similar language\nExperience with GCP (Kubernetes, Cloud functions, Cloud Run etc.) or similar\nExperience in containerization and container orchestration (Docker, Kubernetes, etc.)\nExperience enabling CI/CD pipelines using tools such as Gitlab, or similar\nExpertise in SQL and exposure to non-relational (MongoDB or similar)\nExposure to ML frameworks such as Tensorflow, Pytorch, Scikit-Learn, Spark, would be a plus\nAble to communicate, collaborate, and work effectively in a distributed team.\nCan think about and write high quality code and can demonstrate that capability\nEnjoy tough technical challenges and are naturally intellectually curious\nSeek to drive change and influence others through clear and effective communication.\nWhat you ll do?\nCollaborate with the cross functional teams to help design, build and deliver the headless product offering\nCollaborate with the data scientists and other ML engineers to design, build and deliver the first agentic Revionics experience\nDesign, build, test and maintain end-to-end AI forecasting, optimization and modeling services\nDive deep into the underlying infra architecture to ensure we are building the right way\nWork with product, engineers, and data scientists to translate ideas into new products, services and features\nMentor junior engineers and continually improve our technical stack and processes\nWe also look for\nPassion\nInitiative and a Pioneering Spirit\nQuality orientation\nResourcefulness and application\nAre you the person we re looking for?\nBig picture thinker with laser focus. You have a unique ability to see both the forest and the trees. It s what sets you apart from the rest. You start with a good understanding of the broader strategy, zoom in to assess one particular aspect of that strategy, and then zoom back out to see how changes to that particular area will affect the broader process.\nExpert relationship cultivator. Product managers think you re a good partner -- because you are. Developers feel you respect their opinions -- because you do. You re a true people person, a natural collaborator, and a highly sought-after resource.\nQuality orientation. You have proven success at writing quality user stories and analysis deliverables through the application of established criteria like INVEST and SMART. Your work is thoughtful, timely and valuable to the team.\nResourcefulness and application. At Aptos, we have a pioneering spirit -- when we have questions, we find answers; when we re faced with challenges, we find solutions. We turn to a variety of resources, including our own colleagues, our professional network, the Internet, articles and books -- whatever helps us get the job done. But it s not just about using a variety of resources to gain knowledge -- it s also about applying that knowledge to other areas of the job or business where it might make sense\n\n\nWe offer a competitive total rewards package including a base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility.\nWe are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. By submitting an application for this job, you acknowledge that any personal data or personally identifiable information that you provide to us will be processed in accordance with our Candidate Privacy Notice .",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'orchestration', 'GCP', 'Analytical', 'Artificial Intelligence', 'MongoDB', 'Forecasting', 'Analytics', 'SQL', 'Python']",2025-06-12 15:11:44
AI Engineer,Lericon Informatics,5 - 7 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Job Summary:\nWe are seeking a passionate and skilled AI Engineer to design, develop, and deploy cutting-edge AI solutions across domains such as large language models (LLMs), computer vision, and autonomous agent workflows. You will collaborate with data scientists, researchers, and engineering teams to build intelligent systems that solve real-world problems using deep learning, transformer-based architectures, and multi-modal AI models.\n\nKey Responsibilities:\n\nDesign and implement AI/ML models, especially transformer-based LLMs (e.g., BERT, GPT, LLaMA) and vision models (e.g., ViT, YOLO, Detectron2).\nDevelop and deploy computer vision pipelines for object detection, segmentation, OCR, and image classification tasks.\nBuild and orchestrate intelligent agent workflows using prompt engineering, memory systems, retrieval-augmented generation (RAG), and multi-agent coordination.\nFine-tune and optimize pre-trained models on domain-specific datasets using frameworks like PyTorch or TensorFlow.\nCollaborate with cross-functional teams to understand problem requirements and translate them into scalable AI solutions.\nImplement inference pipelines and APIs to serve AI models efficiently using tools such as FastAPI, ONNX, or Triton Inference Server.\nConduct model evaluation, benchmarking, A/B testing, and performance tuning.\nStay updated with state-of-the-art research in deep learning, generative AI, and multi-modal learning.\nEnsure reproducibility, versioning, and documentation of all experiments and production models.\n\nQualifications:\n\nBachelors or Masters degree in Computer Science, Artificial Intelligence, Data Science, or a related field.\n35 years of hands-on experience in designing and deploying deep learning models.\nStrong knowledge of LLMs (e.g., GPT, BERT, T5), Vision Models (e.g., CNNs, Vision Transformers), and Computer Vision techniques.\nExperience building intelligent agents or using frameworks like LangChain, Haystack, AutoGPT, or similar.\nProficiency in Python, with expertise in libraries such as PyTorch, TensorFlow, Hugging Face Transformers, OpenCV, and Scikit-learn.\nFamiliarity with MLOps concepts and deployment tools (Docker, Kubernetes, MLflow).\nStrong understanding of NLP, image processing, model fine-tuning, and optimization.\nExperience with cloud platforms (AWS, GCP, Azure) and GPU environments.\nExcellent problem-solving, communication, and teamwork skills.\n\nPreferred Qualifications:\n\nExperience in building multi-modal AI systems (e.g., combining vision + language models).\nExposure to real-time inference systems and low-latency model deployment.\nContributions to open-source AI projects or research publications.\nFamiliarity with vector databases (e.g., FAISS, Pinecone, Weaviate) and RAG pipelines.\n\nLocations : Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, India",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['AI Engineering', 'Object Detection', 'Vision Models', 'LLMs', 'AI Agents', 'LangChain', 'Hugging Face', 'Deep Learning', 'PyTorch', 'NLP', 'Transformer Models', 'Model Deployment', 'RAG', 'Computer Vision', 'TensorFlow', 'OCR']",2025-06-12 15:11:46
Python Developer -ENG - Infosys@ PAN India,Infosys,3 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Responsibilities A day in the life of an Infoscion • As part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain. • You will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements. • You will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers. • You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nTechnical and Professional Requirements: • Primary skills:Technology->Machine Learning->Python\n\nPreferred Skills: Technology->Machine Learning->Python\n\nAdditional Responsibilities: • Knowledge of design principles and fundamentals of architecture • Understanding of performance engineering • Knowledge of quality processes and estimation techniques • Basic understanding of project domain • Ability to translate functional / nonfunctional requirements to systems requirements • Ability to design and code complex programs • Ability to write test cases and scenarios based on the specifications • Good understanding of SDLC and agile methodologies • Awareness of latest technologies and trends • Logical thinking and problem solving skills along with an ability to collaborate\nEducational RequirementsMCA,MSc,MTech,Bachelor of Engineering,BCA,BSc,BTech responsibilities",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Python Development', 'Python', 'Django Framework']",2025-06-12 15:11:49
Quantitative Analytics Manager,Wells Fargo,4 - 8 years,Not Disclosed,['Bengaluru'],"In this role, you will:\nManage a team responsible for the creation and implementation of low to moderate complex financial areas\nMitigate operational risk and compute capital requirements\nDetermine scope and prioritization of work in consultation with experienced management\nParticipate in the development of strategy, policies, procedures, and organizational controls with model users, developers, validators, and technology",,,,"['Quantitative Analytics', 'strategy Planning', 'marketing', 'Git', 'GitHub', 'talent development', 'credit risk analysis']",2025-06-12 15:11:51
"Business Analyst II, FIAT SEPO",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"At Amazon.com, we strive to be Earth s most customer-centric company. To support this vision, we need exceptionally talented, bright, and driven people. If you would like to help us build the place to find and buy anything online, this is your chance to work hard, have fun, and make history.\n\n\nAn ideal candidate for this role:\nWill have relevant experience in data analytics working with large data sets and to extract and transform data using various tools and technologies\nWill transform data into actionable business information, and will make it readily accessible to stakeholders worldwide\nWill use data to support ideas, drive actionable outcomes, and provide unique ways to present data and information in an easy to consume format\nWill be passionate about finding root causes, trends, and patterns and how they impact business.\nWill draw inferences and conclusions, create dashboards and visualizations of processed data\nWill have business and communication skills to be able to work with product owners to understand key business questions to build reports that enable product owners to answer those questions quickly and accurately\n. Will be very comfortable juggling competing priorities and handling ambiguity\n. Will thrive in an agile and fast-paced environment on highly visible projects and initiatives\n\nA day in the life\nYou will be responsible for modeling forecasting problems, discovering insights and identifying opportunities through the use of statistical, machine learning, algorithmic, data mining and visualization techniques. You will need to collaborate effectively with internal stakeholders and cross-functional teams to analyze forecast variances, understand and mitigate variance drivers, identify opportunities to improve operational efficiencies, and deliver successfully against high organizational standards. You should be able to apply a breadth of tools, data sources and analytical techniques to answer a wide range of high-impact business questions and present the insights in concise and effective manner. Additionally, you should be an effective communicator capable of independently driving issues to resolution and communicating insights to non-technical audiences. This is a high impact role with goals that directly impacts the bottom line of the business. Accurate forecasts drive improvements in cost and quality of our customer service on a global scale.\n\nAbout the team\nOur team strives to make Amazon the best way for Partners to reach customers locally and globally and to operate their businesses, driven by the accurate and efficient support and solutions we provide them. We are looking for a Business Analyst for its TSE (Trustworthy Shopping Experience) FIAT SEPO. The team is being grown to provide insights and provide WFM solutions to help drive operational efficiencies, uncover the hidden risks and trends, reduce investigation errors, improve customer experience and predict & recommend the optimizations for future state. 3+ years of Excel (including VBA, pivot tables, array functions, power pivots, etc.) and data visualization tools such as Tableau experience\n3+ years of tax, finance or a related analytical field experience\n3+ years of business or financial analysis experience\nExperience defining requirements and using data and metrics to draw business insights\nExperience making business recommendations and influencing stakeholders\nExperience with Excel Experience using very large datasets",,,,"['Business Analyst', 'Financial analysis', 'Analytical', 'Machine learning', 'Agile', 'Customer service', 'data visualization', 'Customer experience', 'Data mining', 'Operations']",2025-06-12 15:11:54
AI Engineer - Lead,Blend360 India,6 - 10 years,Not Disclosed,['Hyderabad'],"We are looking for someone who is ready for the next step in their career and is excited by the idea of solving problems and designing best in class. However, they also need to be aware of the practicalities of making a difference in the real world - whilst we love innovative advanced solutions, we also believe that sometimes a simple solution can have the most impact.\nOur AI Engineer is someone who feels the most comfortable around solving problems, answering questions and proposing solutions. We place a high value on the ability to communicate and translate complex analytical thinking into non-technical and commercially oriented concepts, and experience working on difficult projects and/or with demanding stakeholders is always appreciated.\nWhat can you expect from the role?\nContribute to design, develop, deploy and maintain AI solutions\nUse a variety of AI Engineering tools and methods to deliver\nOwn parts of projects end-to-end\nContributing to solutions design and proposal submissions\nSupporting the development of the AI engineering team within Blend\nMaintain in-depth knowledge of the AI ecosystems and trends\nMentor junior colleagues\n\n\nContribute to the design, development, testing, deployment, maintenance, and improvement of robust, scalable, and reliable software systems, adhering to best practices.\nApply Python programming skills fo",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Version control', 'orchestration', 'GIT', 'GCP', 'Analytical', 'System integration', 'Software development life cycle', 'Mentor', 'Monitoring', 'Python']",2025-06-12 15:11:56
"Software Engineer(.NET, Azure, C#)",Renewable Energy Equipment Manufacturing,3 - 5 years,7-12 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nDesigning and delivering Azure API and associated data platform solutions\nProvision of data in a secure and reliable manner\nTroubleshoot and resolve issues in our dev, test and production environments.\nComfort with frequent, incremental code testing and deployment\nDelivering and presenting Proofs of Concept variants to prospective customers.\nRequirements Analysis and contribute in solution architecture design.\nDocumentation of solutions (e.g. data models, configurations, and setup).\nWorking with data developers to ensure high quality access to and supply of specified data.\nEnsuring that platforms and data solutions can be deployed and operated in a highly repeatable and predictable manner through interaction and collaboration with DevOps specialists.\nDeal with other stakeholders/ end users in the software development lifecycle\nQualifications\n2 to 7 years of hands-on experience of designing and delivering distributed cloud solutions using Microsoft Azure\nVery strong, in-depth, and demonstrable hands-on experience with the large numbers of the following technologies:\nMicrosoft Azure PaaS and SaaS solution development technologies including Azure Functions, Logic Apps, .NET, JavaScript, Python etc.\nMicrosoft Azure App Service Fabric, App Service Environment, Microsoft Azure API Management platform technologies\nJSON, REST and data based APIs and high scale performant service facades\nMicrosoft Azure Identity Management and Security technologies including custom SAML 2.0 providers\nMicrosoft Visual Studio Team System\nAzure Service Bus and Azure Notifications Hub\nAzure Artificial Intelligence and Machine Learning platforms Microsoft Azure Machine Learning, Azure Cognitive Services – would be a plus to have\nMicrosoft Azure Operational and Monitoring tools\nFamiliarity with CosmosDB, Cassandra, Mongo DB or similar technologies would additionally be very useful\nFamiliarity with any of the following would be distinct advantage: Azure Data Analytics platform (Cortana Intelligence Platform) including Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Search, Azure Databricks and Open Source technologies such Apache Spark, Atlas, Hadoop, NoSQL, Kafka, Solr\nExperience with best practice design principles and approaches for a range of application styles and technologies to help guide and steer decisions.\nExperience working with structured and unstructured data including imaging & geospatial data.\nExperience of working in highly dynamic teams using agile methodologies often under demanding timescales.\nExperience of motivating and managing team performance in delivering to agreed timelines",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['C#', 'WebAPi', 'Azure Cloud', '.Net', 'Sql', 'oops concept']",2025-06-12 15:11:58
"Manager, Applied Science, RBS Tech",Amazon,5 - 10 years,Not Disclosed,['Bengaluru'],"RBS (Retail Business Services) Tech team works towards enhancing the customer experience (CX) and their trust in product data by providing technologies to find and fix Amazon CX defects at scale. Our platforms help in improving the CX in all phases of customer journey, including selection, discoverability & fulfilment, buying experience and post-buying experience (product quality and customer returns).\n\nAs a Sciences team in RBS Tech, we focus on foundational ML research and develop scalable state-of-the-art ML solutions to solve the problems covering customer experience (CX) and Selling partner experience (SPX). We work to solve problems related to multi-modal understanding (text and visual), supervised and unsupervised techniques, multi-task learning, multi-label classification, aspect and topic extraction for Customer Anecdote Mining, product similarity, using GenAI, LLMs, NLP and Computer Vision.\n\n\nAs an Applied Science Manager, you will be responsible to design and deploy scalable GenAI, NLP and Computer Vision solutions that will impact the content visible to millions of customer and solve key customer experience issues. You will Lead scientists on the team and oversee research and development projects at various stages ranging from initial exploration to deployment into production systems. You will partner with business and engineering teams to identify and solve large and significantly complex problems that require scientific innovation. You will help the team leverage your expertise, by coaching and mentoring. You will contribute to the professional development of colleagues, improving their technical knowledge and the engineering practices. You will create the environment in the team to file for patents and/or publish research work where opportunities arise. You will impact the large product strategy, identifies new business opportunities and provides strategic direction to the team. Masters degree in Computer Science, Statistics, Electrical Engineering, or Mathematics with specialization in specialization in Machine Learning, statistical modeling, or Deep learning.\n5+ years of working experience in solving machine learning problems and deploying science solutions for large-scale applications\n2+ years of experience leading a team of scientists and engineers Knowledge of programming languages such as C/C++, Python, Java\nExcellent written and verbal communication skills Experience building machine learning models or developing algorithms for business application\nExperience building complex software systems, especially involving deep learning, machine learning and computer vision, that have been successfully delivered to customers",,,,"['Computer science', 'Mining', 'Business services', 'Product quality', 'Electrical engineering', 'Computer vision', 'C++', 'Machine learning', 'Programming', 'Python']",2025-06-12 15:12:01
Senior Software Program Manager,Nvidia,8 - 11 years,Not Disclosed,['Bengaluru'],"We are looking for Senior Technical Program Manager, to join NVIDIAs Solution Engineering team. In this role, you will work on one of our key Automotive projects. Youll find the work exciting, challenging, and meaningful. You will provide the leadership for the software support team, guide the direction of the program and coordinate with other internal teams, as well as tracking and managing program deliverables. NVIDIA gives automakers, Tier 1 suppliers, automotive research institutions, and start-ups the power and flexibility to develop and deploy breakthrough artificial intelligence systems for self-driving vehicles.\nWhat youll be doing:\nFocus on a new project, developing the next generation of Vehicle Abstraction framework\nPerform release planning, manage features, bug fixes, testing and documentation\nDrive and track software releases for new vehicle platforms and software features.\nManage risks and address issues that impact release scope, schedule, and quality\nCollaborate and communicate with program and product stakeholders\nResponsible for a successful delivery of the program while working as a team with a dedicated technical PIC, who will be helping with the technical aspects.\nWhat we need to see:\nBS/MS Computer Science or related field (or equivalent experience)\n8+ years recent Program/Project Management experience driving the planning and execution of software engineering projects and releasing commercial products.\nExcellent communication and technical presentation skills\nExperience working with a multi cross-region engineering team\nExperience handling successful releases with short release cadence in a multifaceted environment.\nYou have a consistent record of leading and successfully delivering scalable programs and projects, driving process improvements.\nShown ability to evaluate and drive adoption of new and improved process workflows in scalable organizations.\nStrong project management background with good breadth and superior organization skills.\nWays to stand out from the crowd:\nPrevious experience with Embedded/Automotive system integration\nExpertise in ASPICE and ISE 26262 safety standards\nBackground with data driven operations and defining/managing operational metrics. Understanding what makes a good/bad metric and how to drive an organization using those metrics.\nExperience in optimally leading global projects across time zones\nAgile Certification/training a plus as well as PM Certification/training desired\nNVIDIA is widely considered to be one of the technology world s most desirable employers. We have some of the most hard-working and talented people in the world working for us. If youre a creative and autonomous engineer with a real passion for technology, we want to hear from you!",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Software support', 'Project management', 'Artificial Intelligence', 'System integration', 'Agile', 'Engineering projects', 'Management', 'Operations', 'Automotive']",2025-06-12 15:12:03
"Senior Manager, Software Engineering",Diligent Corporation,10 - 15 years,Not Disclosed,['Bengaluru'],"Position Overview\nYou will work closely with technology and business teams to understand requirements, design robust architectures, and influence technology choices to deliver innovative solutions. In addition to leading SaaS software development, you will drive initiatives in Data Engineering, Data Warehousing, and Artificial Intelligence (AI). You will collaborate with principal engineers and leadership, and have opportunities to cross-collaborate with inter-disciplinary teams to solve unique challenges in the GRC & ESG domain.\nKey Responsibilities\nShape the product and technical vision for the team, collaborating with product, business, and engineering leadership across the company.\nManage and mentor a team of engineers developing highly scalable, performant, maintainable, and well-tested SaaS features.\nLead the design and implementation of modern data warehouse solutions, ensuring data quality, scalability, and security.\nOversee the integration of advanced AI and machine learning models into SaaS products to deliver intelligent features and insights.\nHire, mentor, and lead a world-class group of engineers with expertise in SaaS, data engineering, and AI.\nFoster a culture of innovation, experimentation, and continuous improvement.\nEvaluate engineering requirements and design proposals, especially in the context of data-driven and AI-powered applications.\nAssess and develop the technical performance of individual contributors within the team.\nStay current with the latest frameworks and technologies in SaaS, Data Engineering, and AI, influencing technology choices for the application stack.\nFacilitate daily stand-ups, risk identification and mitigation, dependency resolution, and follow-ups for gap closure.\nPartner with Product Management and Business teams to drive the agenda, set priorities, create project plans, and deliver outstanding products.\nRequired Experience/Skills\n10 to 15 years of relevant experience in developing enterprise SaaS applications using MERN/.NET, MySQL, MS SQL, and caching technologies.\n2+ years of experience leading engineering teams building scalable platforms and architectures, including data engineering and AI initiatives.\nProven experience designing, building, and maintaining data warehouse solutions (such as Snowflake, Redshift, or BigQuery) and data pipelines (ETL/ELT).\nHands-on experience with AI/ML frameworks (such as TensorFlow, PyTorch, or Scikit-learn) and integrating AI models into production SaaS environments.\nStrong background in data modeling, data governance, and data quality best practices.\nExperience with cloud platforms (AWS/Azure), CI/CD, DevOps, scripting, and SQL/NoSQL databases.\nDemonstrated success in migrating monolithic applications to microservices and on-premises solutions to cloud environments.\nPassion for building a data-driven culture, growing talent, and making a significant impact through technology.\nStrong communication skills for engaging with end users, technical, and business teams to gather requirements and describe product features and technical designs.\nAbility to seek clarity in ambiguous situations and drive projects to completion.\nExperience in Agile development and knowledge of Scrum and Kanban methodologies.\nSelf-motivated learner and builder with a strong customer focus and a commitment to delivering high-quality solutions.",Industry Type: Design,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'MS SQL', 'NoSQL', 'Data modeling', 'MySQL', 'Machine learning', 'Scrum', 'Data quality', 'SQL', 'Recruitment']",2025-06-12 15:12:06
Staff Engineer Gen-AI,recex,8 - 12 years,Not Disclosed,['Bengaluru'],"Job Title: Staff Engineer Gen-AI\nExperience: 8.0 Year To 10.0 Year\nCTC Salary: 50.00 LPA To 65.00 LPA\nLocation: Bengaluru/Bangalore\n\nJob Description\nBuild Gen-AI native products: Architect, build, and ship platforms powered by LLMs, agents, and predictive AI.\nStay hands-on: Design systems, write code, debug, and drive product excellence.\nLead with depth: Mentor a high-caliber team of full stack engineers.\nSpeed to market: Rapidly ship and iterate on MVPs to maximize learning and feedback.\nOwn the full stack: From backend data pipelines to intuitive UIsfrom Airflow to React from BigQuery to embeddings.\nScale what works: Ensure scalability, security, and performance in multi-tenant, cloud-native environments (GCP).\nCollaborate deeply: Work closely with product, growth, and leadership to align tech with business priorities.\nWhat You Bring\n8+ years of experience building and scaling full-stack, data-driven products\nProficiency in backend (Node.js, Python) and frontend (React), with solid GCP experience\nStrong grasp of data pipelines, analytics, and real-time data processing\nFamiliarity with Gen-AI frameworks (LangChain, LlamaIndex, OpenAI APIs, vector databases)\nProven architectural leadership and technical ownership\nProduct mindset with a bias for execution and iteration\nOur Tech Stack\nCloud: Google Cloud Platform\nBackend: Node.js, Python, Airflow\nData: BigQuery, Cloud SQL\nAI/ML: TensorFlow, OpenAI APIs, custom agents\nFrontend: React.js\n\n\nInterested professional can share Resume at harshita.g@recex.co\n\nThanks & Regards\nHarshita\nRecex",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Cloud SQL', 'Google Cloud Platform', 'OpenAI APIs', 'Node.js', 'BigQuery', 'custom agents', 'React.js', 'Python', 'TensorFlow']",2025-06-12 15:12:08
Senior Generative AI Engineer - Python Programming,Zettamine Labs,7 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a Senior Generative AI Engineer who is passionate about cutting-edge AI innovation and has significant hands-on experience in building and deploying Generative AI models. In this role, you will be responsible for designing, fine-tuning, and optimizing large language models (LLMs), implementing innovative GenAI solutions, and contributing to the architecture of AI-driven platforms that deliver real business value.\n\nYou will collaborate with cross-functional teams including data scientists, machine learning engineers, product managers, and cloud infrastructure teams to build scalable, reliable, and secure AI systems. This is a high-impact position where you will directly influence the AI roadmap and innovation strategy.\n\nKey Responsibilities :\n\n- Design, develop, and fine-tune state-of-the-art Generative AI and LLM models tailored for various business use cases.\n\n- Build, integrate, and optimize solutions using transformer-based architectures (e.g., GPT, BERT, T5, LLaMA, Mistral).\n\n- Apply techniques such as fine-tuning, prompt engineering, RLHF (Reinforcement Learning from Human Feedback), and knowledge distillation to improve model performance.\n\n- Work with vector databases (e.g., FAISS, Pinecone, Weaviate) for implementing retrieval-augmented generation (RAG) pipelines.\n\n- Develop and deploy embedding models and integrate them into LLM pipelines.\n\n- Collaborate with engineering and product teams to deploy scalable AI systems using MLOps practices and CI/CD pipelines.\n\n- Leverage LangChain, Hugging Face Transformers, OpenAI APIs, and similar frameworks/tools to accelerate development.\n\n- Optimize model performance across different environments (cloud/on-premise).\n\n- Develop end-to-end pipelines, from data preprocessing to real-time inference and monitoring.\n\n- Ensure high standards of software quality, including testing, version control, code reviews, and documentation.\n\n- Stay up to date with the latest research in Generative AI and translate breakthroughs into production-ready solutions.\n\nRequired Skills & Qualifications :\n\n- Experience : 7+ years in AI/ML, data science, or software engineering; at least 3 - 4 years in Generative AI/LLMs.\n\n- Advanced Python programming skills, including familiarity with object-oriented design and software engineering best practices.\n\n- Deep expertise in PyTorch, TensorFlow, Transformers (Hugging Face), LangChain, and OpenAI or Anthropic APIs.\n\n- Experience in LLM fine-tuning, parameter-efficient tuning methods (LoRA, PEFT), RLHF, and model evaluation.\n\n- Experience with embeddings, vector stores (FAISS, Pinecone), semantic search, and RAG systems.\n\n- Hands-on experience with AWS, GCP, or Azure; knowledge of MLOps tools (SageMaker, Vertex AI, MLflow, Kubeflow) for training, deploying, and monitoring models.\n\n- Familiarity with structured/unstructured data handling and integrating AI systems with SQL/NoSQL databases.\n\n- Strong analytical thinking, problem-solving ability, and a keen interest in research and innovation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Tensorflow', 'PyTorch', 'Generative AI', 'MLOps', 'NoSQL', 'ChatGPT', 'Artificial Intelligence', 'Data Modeling', 'LLM', 'Python', 'SQL']",2025-06-12 15:12:10
"Senior Manager, Software Engineering",Diligent Corporation,10 - 15 years,Not Disclosed,['Bengaluru'],"About Us\nDiligent is the AI leader in governance, risk and compliance (GRC) SaaS solutions, helping more than 1 million users and 700,000 board members to clarify risk and elevate governance. The Diligent One Platform gives practitioners, the C-Suite and the board a consolidated view of their entire GRC practice so they can more effectively manage risk, build greater resilience and make better decisions, faster.\nAt Diligent, were building the future with people who think boldly and move fast. Whether youre designing systems that leverage large language models or part of a team reimaging workflows with AI, youll help us unlock entirely new ways of working and thinking. Curiosity is in our DNA, we look for individuals willing to ask the big questions and experiment fearlessly - those who embrace change not as a challenge, but as an opportunity. The future belongs to those who keep learning, and we are building it together. At Diligent, you re not just building the future - you re an agent of positive change, joining a global community on a mission to make an impact.\nLearn more at diligent.com or follow us on LinkedIn and Facebook\nPosition Overview\nYou will work closely with technology and business teams to understand requirements, design robust architectures, and influence technology choices to deliver innovative solutions. In addition to leading SaaS software development, you will drive initiatives in Data Engineering, Data Warehousing, and Artificial Intelligence (AI). You will collaborate with principal engineers and leadership, and have opportunities to cross-collaborate with inter-disciplinary teams to solve unique challenges in the GRC & ESG domain.\nKey Responsibilities\nShape the product and technical vision for the team, collaborating with product, business, and engineering leadership across the company.\nManage and mentor a team of engineers developing highly scalable, performant, maintainable, and well-tested SaaS features.\nLead the design and implementation of modern data warehouse solutions, ensuring data quality, scalability, and security.\nOversee the integration of advanced AI and machine learning models into SaaS products to deliver intelligent features and insights.\nHire, mentor, and lead a world-class group of engineers with expertise in SaaS, data engineering, and AI.\nFoster a culture of innovation, experimentation, and continuous improvement.\nEvaluate engineering requirements and design proposals, especially in the context of data-driven and AI-powered applications.\nAssess and develop the technical performance of individual contributors within the team.\nStay current with the latest frameworks and technologies in SaaS, Data Engineering, and AI, influencing technology choices for the application stack.\nFacilitate daily stand-ups, risk identification and mitigation, dependency resolution, and follow-ups for gap closure.\nPartner with Product Management and Business teams to drive the agenda, set priorities, create project plans, and deliver outstanding products.\nRequired Experience/Skills\n10 to 15 years of relevant experience in developing enterprise SaaS applications using MERN/.NET, MySQL, MS SQL, and caching technologies.\n2+ years of experience leading engineering teams building scalable platforms and architectures, including data engineering and AI initiatives.\nProven experience designing, building, and maintaining data warehouse solutions (such as Snowflake, Redshift, or BigQuery) and data pipelines (ETL/ELT).\nHands-on experience with AI/ML frameworks (such as TensorFlow, PyTorch, or Scikit-learn) and integrating AI models into production SaaS environments.\nStrong background in data modeling, data governance, and data quality best practices.\nExperience with cloud platforms (AWS/Azure), CI/CD, DevOps, scripting, and SQL/NoSQL databases.\nDemonstrated success in migrating monolithic applications to microservices and on-premises solutions to cloud environments.\nPassion for building a data-driven culture, growing talent, and making a significant impact through technology.\nStrong communication skills for engaging with end users, technical, and business teams to gather requirements and describe product features and technical designs.\nAbility to seek clarity in ambiguous situations and drive projects to completion.\nExperience in Agile development and knowledge of Scrum and Kanban methodologies.\nSelf-motivated learner and builder with a strong customer focus and a commitment to delivering high-quality solutions.\nWhat Diligent Offers You\nCreativity is ingrained in our culture. We are innovative collaborators by nature. We thrive in exploring how things can be differently both in our internal processes and to help our clients\nWe care about our people. Diligent offers a flexible work environment, global days of service, comprehensive health benefits, meeting free days, generous time off policy and wellness programs to name a few\nWe have teams all over the world . We may be headquartered in New York City, but we have office hubs in Washington D.C., Vancouver, London, Galway, Budapest, Munich, Bengaluru, Singapore, and Sydney.\nDiversity is important to us. Growing, maintaining and promoting a diverse team is a top priority for us. We foster and encourage diversity through our Employee Resource Groups and provide access to resources and education to support the education of our team, facilitate dialogue, and foster understanding.\nDiligent created the modern governance movement. Our world-changing idea is to empower leaders with the technology, insights and connections they need to drive greater impact and accountability - to lead with purpose. Our employees are passionate, smart, and creative people who not only want to help build the software company of the future, but who want to make the world a more sustainable, equitable and better place.\nHeadquartered in New York, Diligent has offices in Washington D.C., London, Galway, Budapest, Vancouver, Bengaluru, Munich, Singapore and Sydney. To foster strong collaboration and connection, this role will follow a hybrid work model. If you are within a commuting distance to one of our Diligent office locations, you will be expected to work onsite at least 50% of the time. We believe that in-person engagement helps drive innovation, teamwork, and a strong sense of community.\nTo all recruitment agencies: Diligent does not accept unsolicited agency resumes. Please do not forward resumes to our jobs alias, Diligent employees or any other organization location. Diligent is not responsible for any fees related to unsolicited resumes.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'MS SQL', 'NoSQL', 'Data modeling', 'MySQL', 'Machine learning', 'Scrum', 'Data quality', 'SQL', 'Recruitment']",2025-06-12 15:12:13
Senior Software Engineer,Xoom,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Summary\nWhat you need to know about the role\nAs a Software Engineer in our Risk department, you will play a critical role in developing and maintaining cutting-edge risk detection and prevention systems that protect PayPals users and merchants from financial loss. You will work closely with cross-functional teams to design, build, and deploy scalable and efficient solutions that leverage machine learning, data analytics, and automation to identify and mitigate potential risks, ensuring the integrity of our platform and driving business growth.\n\nMeet our team\nAs an engineer in Global Fraud Risk - Automation team, You will work closely with data scientists, engineering, and analytical teams, understand the requirements and drive full development lifecycle of the teams products, transforming research work to real products. We are looking for strong technologists who are passionate about technology and able to continuously deliver state of the art software solutions in scalable way.\nJob Description\nYour way to impact\nAt PayPal, Backend Software Engineers are the architects of our global payment platform. Youll design, develop, and optimize core systems that power millions of transactions daily, directly impacting our customers experiences and our companys success.\nYour day-to-day\nAs a Senior Software Engineer - Backend, youll design and implement backend solutions. Youll collaborate with cross-functional teams to deliver high-quality products.\nDesign and develop scalable backend systems.\nOptimize system performance and reliability.\nMentor junior engineers.\nWhat do you need to bring\nBachelors degree in Computer Science or related field.\n3-5 years of backend development experience.\nProficiency in at least one backend language (Python, Java, Ruby on Rails)\nAdvanced proficiency in backend development with either Java EE frameworks, including experience with Spring MVC, or Hibernate.\nExperience designing and implementing RESTful services, focusing on scalability and reliability, using Java.\nProven ability to mentor junior engineers and contribute to code reviews and design discussions.\nExperience with cloud platforms (AWS, GCP, Azure)\nExperience with databases (SQL, NoSQL)\nStrong understanding of database design, including SQL and NoSQL databases, and experience with ORM tools.\nPreferred Qualifications\nExperience with large-scale, high-performance systems.\nKnowledge of the payment processing industry and relevant regulations.\nExperience with cloud platforms (AWS, GCP, Azure).\nContributions to open-source projects .\n**We know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please dont hesitate to apply.\nPreferred Qualification\nSubsidiary\nPayPal\nTravel Percent\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https//www.paypalbenefits.com .\nWho We Are\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. .\nBelonging at PayPal\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Hibernate', 'Automation', 'Backend', 'Database design', 'Analytical', 'Machine learning', 'Open source', 'SQL', 'Python']",2025-06-12 15:12:15
Software Engineer-C#,Definitive Healthcare,3 - 5 years,Not Disclosed,['Bengaluru'],"Analytical Wizards is part of the Definitive Healthcare family. We balance innovation with an open, friendly culture and the backing of a long-established parent company, known for its ethical reputation. We guide customers from what s now to what s next by unlocking the value of their data and applications to solve their challenges, achieving outcomes that benefit both business and society. Our people are our biggest asset, they drive our innovation advantage and we strive to offer a flexible and collaborative workplace where they can thrive. We offer industry-leading benefits packages to promote a creative and inclusive culture. If driving real change gives you a sense of pride and you are passionate about powering social good, we d love to hear from you.\nRole: Software Engineer / Senior Software Engineer / Principal Software Engineer\nOffice Location : Bangalore\nWe write our code leveraging the Microsoft stack (ReactJS, .NET Core, MS SQL Server , PostgreSQL, AWS). We re relentless, curious, and doing the right thing is a tenet of the team s approach to solving business problems. We are flexible and collaborative. We all enjoy our work and helping one another is baked into our operating DNA. We believe in Agile software development principles. We are huge proponents of SOLID principles.\nThe Software Engineer at Analytical Wizards:\nHas 3-5 years of experience in a fast-paced Agile software development environment\nEnsures software is built, according to business and technical specifications, on top of an error-free and high-performing platform\nInnovates!\nPerforms code reviews and QA on team members work items\nParticipates in Agile ceremonies\nMentors more junior members of the team\nIs always thinking of better ways to do something\nIsn t afraid to fail\nCares deeply about quality\nCan deliver high-quality software with minimal supervision\nRequired Skills:\nExperience of 3-5 years experience in C#, .NET & .NET Core (5 & above)\nSolid computer science fundamentals - OOP concepts, SOLID principles & design patterns\nExperience with any relational databases (MS SQL / MySQL /Postgres)\nExperience working with public cloud like Azure/AWS/GCP (AWS is preferred)\nExperience with containerization using Docker & microservices\nExperience developing RESTful APIs\nUnderstanding of various types of testing (unit, system, integration, performance)\nGeneral familiarity with cloud computing and serverless architectures\nPreferred Skills:\nExperience in front-end development (React, JavaScript, KendoUI)\nExposure to GraphQL (HotChocloate)\nExposure to DataDog or similar logging & monitoring tool\nFamiliarity with NoSQL databases (MongoDB /DocumentDB)\nWhy we love Analytical Wizards, and why you will too!\nIndustry leading products\nWork hard, and have fun doing it\nIncredibly fast growth means limitless opportunity\nFlexible and dynamic culture\nWork alongside some of the most talented and dedicated teammates\nA collaborative and friendly culture with very high employee engagement\nAbout Company\nCompany Name: AnalyticalWizards Services Pvt. Ltd.\nProfile: A leading, high growth Analytics Software Development company developing products that touch and positively impact human lives across the globe. We are headquartered in New Jersey and have a software development and delivery center in Bangalore. Our work is mainly focused on Healthcare Industry. We develop core data science products that help our clients draw unique insights from their big data and achieve their business goals. We use advanced algorithms in the space of artificial intelligence and machine learning. Our technology-based software products are being used by the top pharma and biotechnology companies of the world. We have been recognized twice as one of fastest growing private companies of USA.\nWork Culture: Employee-friendly, collaborative, innovative, fast-paced, and conducive to learning\nCompany Address: AnalyticalWizards Services Private Limited, Fortune Summit Business Park, Ground Floor, Hosur Road, Sector 6, HSR Layout, Roopena Agrahara, Bangalore - 560068\nIndustry: Software Development and Data Science",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cloud computing', 'MS SQL', 'data science', 'Postgresql', 'MySQL', 'Javascript', 'Agile', 'Healthcare', 'microsoft', 'Analytics']",2025-06-12 15:12:18
Financial Analyst,Watson Pharama,8 - 13 years,Not Disclosed,['Bengaluru'],"Select how often (in days) to receive an alert: Select how often (in days) to receive an alert: Jun 9, 2025 Bangalore, India, 560064\nWho we are\nThe opportunity\nIn this position, you are part of the Finance GBS Team and work closely with the Internal GBS team, COE team, Global Controlling/Finance directors, Finance Leads and Supply chain team.\nHow you ll spend your day\nApply advanced statistical methods, machine learning, and data mining techniques to analyze data from various sources\nExplore data, identify patterns and trends, and create visualizations to communicate findings effectively to stakeholders\nDevelop and maintain predictive models for various business areas, such as sales forecasting, inventory management, and customer behavior\nCollaborate with stakeholders to identify business problems, opportunities, and areas for improvement\nDesign and implement analytical solutions to address business challenges, leveraging data to support decision-making\nDeploy analytical models and track their performance, as needed to ensure continued effectiveness\nIdentify new opportunities to leverage data and analytics to drive business innovation and create competitive advantages\nEnsure that analytical solutions are aligned with overall business strategy and objectives\nEffectively communicate findings and recommendations to both technical and non-technical audiences, including executive leadership and business stakeholders\nWork with other departments, such as IT, marketing, and operations, to ensure alignment and collaboration on projects\nYour experience and qualifications\nBachelors and Masters Degree in Business, Computer Science, Information Systems, Engineering, Business/Administration, Education, Technical, Finance, MBA, Information Technology\n8+ years of experience ability to work independently taking a lead role\nProficiency in relevant software and tools, such as SQL, Python, R, or Tableau\nStrong analytical and problem-solving skills\nExpertise in advanced analytics techniques, including machine learning, statistical modeling, and data mining\nExcellent communication and interpersonal skills\nAbility to manage multiple projects simultaneously and meet deadlines\nExperience in data modeling, data warehousing, and data governance\nFlexible and able to work in a changing environment\nStrong focus on improvement opportunities\nAssoc Dir Finance Operations\nAlready Working @TEVA\nThe internal career site is available from your home network as well. If you have trouble accessing your EC account, please contact your local HR/IT partner.\nTeva s Equal Employment Opportunity Commitment\nTeva Pharmaceuticals is committed to equal opportunity in employment. It is Tevas global policy that equal employment opportunity be provided without regard to age, race, creed, color, religion, sex, disability, pregnancy, medical condition, sexual orientation, gender identity or expression, ancestry, veteran status, national or ethnic origin or any other legally recognized status entitled to protection under applicable laws. We are committed to a diverse and inclusive workplace for all. If you are contacted for a job opportunity, please advise us of any accommodations needed to support you throughout the recruitment and selection process. All accommodation information provided will be treated as confidential and used only for the purpose of providing an accessible candidate experience.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Computer science', 'Data modeling', 'Pharma', 'Analytical', 'Business strategy', 'Data mining', 'Information technology', 'Recruitment', 'SQL']",2025-06-12 15:12:20
Senior Software Engineer-C#,Definitive Healthcare,5 - 10 years,Not Disclosed,['Bengaluru'],"Analytical Wizards is part of the Definitive Healthcare family. We balance innovation with an open, friendly culture and the backing of a long-established parent company, known for its ethical reputation. We guide customers from what s now to what s next by unlocking the value of their data and applications to solve their challenges, achieving outcomes that benefit both business and society. Our people are our biggest asset, they drive our innovation advantage and we strive to offer a flexible and collaborative workplace where they can thrive. We offer industry-leading benefits packages to promote a creative and inclusive culture. If driving real change gives you a sense of pride and you are passionate about powering social good, we d love to hear from you.\nRole: Senior Software Engineer / Principal Software Engineer\nOffice Location : Bangalore\nWe write our code leveraging the Microsoft stack (ReactJS, .NET Core, MS SQL Server , PostgreSQL, AWS). We re relentless, curious, and doing the right thing is a tenet of the team s approach to solving business problems. We are flexible and collaborative. We all enjoy our work and helping one another is baked into our operating DNA. We believe in Agile software development principles. We are huge proponents of SOLID principles.\nThe Software Engineer at Analytical Wizards:\nHas 5+ years of experience in a fast-paced Agile software development environment\nEnsures software is built, according to business and technical specifications, on top of an error-free and high-performing platform\nInnovates!\nPerforms code reviews and QA on team members work items\nParticipates in Agile ceremonies\nMentors more junior members of the team\nIs always thinking of better ways to do something\nIsn t afraid to fail\nCares deeply about quality\nCan deliver high-quality software with minimal supervision\nRequired Skills:\nExperience of 5+ years experience in C#, .NET & .NET Core (5 & above)\nSolid computer science fundamentals - OOP concepts, SOLID principles & design patterns\nExperience with any relational databases (MS SQL / MySQL /Postgres)\nExperience working with public cloud like Azure/AWS/GCP (AWS is preferred)\nExperience with containerization using Docker & microservices\nExperience developing RESTful APIs\nUnderstanding of various types of testing (unit, system, integration, performance)\nGeneral familiarity with cloud computing and serverless architectures\nPreferred Skills:\nExperience in front-end development (React, JavaScript, KendoUI)\nExposure to GraphQL (HotChocloate)\nExposure to DataDog or similar logging & monitoring tool\nFamiliarity with NoSQL databases (MongoDB /DocumentDB)\nWhy we love Analytical Wizards, and why you will too!\nIndustry leading products\nWork hard, and have fun doing it\nIncredibly fast growth means limitless opportunity\nFlexible and dynamic culture\nWork alongside some of the most talented and dedicated teammates\nA collaborative and friendly culture with very high employee engagement\nAbout Company\nCompany Name: AnalyticalWizards Services Pvt. Ltd.\nProfile: A leading, high growth Analytics Software Development company developing products that touch and positively impact human lives across the globe. We are headquartered in New Jersey and have a software development and delivery center in Bangalore. Our work is mainly focused on Healthcare Industry. We develop core data science products that help our clients draw unique insights from their big data and achieve their business goals. We use advanced algorithms in the space of artificial intelligence and machine learning. Our technology-based software products are being used by the top pharma and biotechnology companies of the world. We have been recognized twice as one of fastest growing private companies of USA.\nWork Culture: Employee-friendly, collaborative, innovative, fast-paced, and conducive to learning\nCompany Address: AnalyticalWizards Services Private Limited, Fortune Summit Business Park, Ground Floor, Hosur Road, Sector 6, HSR Layout, Roopena Agrahara, Bangalore - 560068\nIndustry: Software Development and Data Science",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cloud computing', 'MS SQL', 'data science', 'Postgresql', 'MySQL', 'Javascript', 'Agile', 'Healthcare', 'microsoft', 'Analytics']",2025-06-12 15:12:23
Solution Engineer - AI Infrastructure,Cisco,10 - 15 years,Not Disclosed,['Bengaluru'],"What Youll Do\nWe are seeking a Solutions Engineer - Artificial Intelligence (AI) Practitioner to join our dynamic sales team. As an SE (AI Practitioner), you will consult and drive the adoption of our AI solutions across various industries. You will identify potential clients, understand their specific needs, and provide tailored AI solutions that enhance their business operations. This role requires a deep understanding of AI technologies, real world deployment experience and expert capability to relay business and technical concepts to a diverse audience.\nWho Youll Work With\nThe Cloud and AI Infrastructure team is responsible for helping customers change their business using Cisco technology in the data center and on cloud. The team works with customers, partners and engineering teams to take customer problems and turn them into business advantage.\nWho You Are\nYoure energized by the fast-paced landscape in IT and how AI is changing the world.You love technology and thrive in solving complex problems. You are an amazing presenter and can translate complex technical scenarios into a simple easy to understand message. You inspire those around to use technology in innovative ways and use a hands-on methods to demonstrate your ideas.\nMinimum Qualifications:\n10+ years of technology consulting experience (preferably in systems, software, data, analytics and AI).\nGood understanding of programming/scripting languages\nAbility to provide detailed and consumable documentation and standard methodologies for deployment around application acceleration, automation/management efficiencies, enterprise, and AI/ML solutions.\nBe able to develop and showcase real world examples of how AI technology can help businesses thrive and solve problems.\nExcellent presentation skills ability to value-sell and deliver engaging workshops to both technical and non-technical audiences on AI and/or infrastructure topics.\nPreferred Qualifications:\nBachelor's Degree in Computer Science, Computer Engineering, Electrical Engineering, or related field. Advanced degree in Data Science is a plus.\nExperience with AI relevant infrastructure, including Networking (InfiniBand and RoCE), Storage (FC, IP and scale out) and AI accelerators (GPUs etc).\nIn-depth understanding of AI models, including but not limited to GPT, Llama, Resnet or similar.\nExperienced with data storage and management (SQL, NOSQL, Vector, BigQuery etc) and AI/ML frameworks (scikit-learn, TensorFlow, PyTorch, Jupyter, NIMS and NVAIE etc).\nExpertise in training and fine-tuning AI models on premise or in cloud environments.\nFamiliarity with containerization (k8 etc).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'BigQuery', 'PyTorch', 'scikit-learn', 'NIMS', 'Jupyter', 'NOSQL', 'Vector', 'SQL', 'TensorFlow']",2025-06-12 15:12:25
Python Developer (3-5 Years)-RA @ Infosys,Infosys,3 - 5 years,Not Disclosed,"['Chandigarh', 'Pune', 'Bengaluru']","Responsibilities\nWrite clean, maintainable, and efficient Python code for backend services or applications. Develop RESTful APIs or work on web applications using frameworks like Django or Flask. Implement data extraction, transformation, and loading (ETL) processes using Python. Collaborate with front-end developers and other team members to ensure seamless integration. Test and debug applications to ensure they meet quality and performance standards.\nParticipate in code reviews and contribute to the development of coding standards.\nKeep up-to-date with Python libraries and tools relevant to the project.\nTechnical and Professional Requirements:\nPrimary skills: Technology->Machine Learning->Python\nPreferred Skills: Technology->Machine Learning->Python\nAdditional Responsibilities:\nSkills Required:\nStrong proficiency in Python 3.x.\nExperience with at least one Python web framework (Django, Flask, etc.).\nKnowledge of database technologies, including SQL and ORM (e.g., SQLAlchemy).\nFamiliarity with version control systems like Git.\nBasic understanding of front-end technologies (HTML, CSS, JavaScript).\nExperience with unit testing frameworks like pytest or unittest.\nKnowledge of REST API design principles.\nEducational Requirements\nMCA, MTech, Bachelor of Engineering, BCA, BSc, BTech",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Django', 'Flask']",2025-06-12 15:12:27
Hiring BTech freshers - (Full stack),Guidehouse,0 - 1 years,Not Disclosed,['Thiruvananthapuram'],"Roles and Responsibilities :\nMust be a graduate in\nBE/BTech/MCA in IT/CS or a related field.\nExcellent technical and interpersonal communication skills.\nBasic understanding of programming languages such as GoLang.\nFamiliarity with web development frameworks like React, Angular, Blazor, or Vue.js.\nKnowledge of cloud platforms such as Azure, AWS, or Google Cloud Platform.\nUnderstanding of containerization technologies like Docker.\nBasic knowledge of database management systems such as MSSQL, MySQL, PostgreSQL, or MongoDB.\nGood understanding of SDLC, STLC, Agile methodologies, and business process analysis.\nFamiliarity with RESTful APIs, microservices concepts, and DevOps practices.\nBasic understanding of security best practices in software development.\nFamiliarity with version control systems like Git.\nPreferred/Good to Have\nInternship or project experience in software development, DevOps, cloud, or related fields.\nFamiliarity with business intelligence tools (e.g., Power BI, Tableau).\nKnowledge of identity and access management solutions.\nInterest or coursework in artificial intelligence, machine learning, or data science.\nExperience with scripting languages (e.g., Bash, PowerShell).\nActive participation in technical forums (e.g., Stack Overflow) or GitHub contributions.\nExposure to UI/UX design principles.\nExperience with mobile app development (Android/iOS/Flutter/React Native) is a plus.\nAwareness of CI/CD pipelines and infrastructure as code tools (e.g., Terraform, Ansible).\nExperience working with orchestration technologies like Kubernetes.\nStrong logical, analytical, and problem-solving skills; experience in hackathons, coding challenges, or open-source contributions is a plus.\nAny certifications (e.g., Microsoft, AWS, Google, Scrum) are an added advantage.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Golang', 'Golang Development', 'React.Js']",2025-06-12 15:12:29
AI/ML Engineer,Aptos Retail,20 - 25 years,Not Disclosed,['Bengaluru'],"Making a career change is a big decision. Why consider Aptos?\nBecome a part of a team that is passionate about creating and delivering cutting-edge solutions for retailers worldwide. At our company, we re dedicated to supporting your career aspirations and helping you exceed your goals. You ll benefit from industry-leading training, global development opportunities, and the chance to collaborate within a diverse culture across our offices in nine countries. Our inclusive culture reflects our purpose: to make a difference for every colleague, every client, every day .\nAs a leading provider of Unified Commerce solutions for retail, our technology empowers top retail brands by optimizing product management, promotions, merchandising, and store operations. With the global shift toward our cloud-native, microservices architecture, opportunities for career growth have never been more exciting. Today, more than 100,000 retail stores in fashion, grocery, footwear, general merchandise, discount, and sporting goods rely on our solutions to generate nearly $2 trillion in annual revenue.\nWe hope you ll join us in driving innovation and delivering impactful solutions as we continue leading the Unified Commerce revolution.\nAptos market-leading platform drives the world s largest retailers in terms of their product pricing, promotion and merchandising decisions worldwide. Over 33,000 retail locations and $200+B in annual revenue across grocery, drug, convenience, general merchandise, discount, sporting goods stores, fashion, and eCommerce sites optimize with Aptos solutions.\nAptos acquired Revionics in September 2020. Revionics is the worldwide leader in retail pricing, with 20 years of experience delivering AI/ML-driven retail pricing and promotions SaaS solutions for some of the largest and best-known retailers in the world, affecting over $500B revenue under management across more than 50 retailers.\nThe AI team, within the Product Org, plays a central role at the company and is responsible for the GenAI (agents, conversational analytics etc.) and Predictive AI solutions (modeling, forecasting, optimization, etc) at Revionics. As an engineer on the Science team, you will be part of a skilled and diverse team while working with a mix of data scientists and engineers. You ll not only have the opportunity to learn/use state-of-art AI/GenAI and ML techniques but also implement/roll-out modern engineering frameworks and solve problems that have not been solved before.\nIf you re someone who is ready to take on a challenge, drive change, and be part of an awesome team, this is the right role for you!\nAbout the Role:\nThe engineer will be responsible for designing, building, deploying, and evolving the end-to-end AI/ML systems at Aptos (demand modeling and forecasting, optimization, GenAI agents, etc.)\nWho you are?\nYou have a Bachelors/Master s degree in computer science, engineering, or related STEM field, or equivalent work experience\nStrong algorithmic problem-solving skills and an analytical mindset\nHunger to learn new domains and complex code bases\n4+ years of development experienced with Python or another similar language\nExperience with GCP (Kubernetes, Cloud functions, Cloud Run etc.) or similar\nExperience in containerization and container orchestration (Docker, Kubernetes, etc.)\nExperience enabling CI/CD pipelines using tools such as Gitlab, or similar\nExpertise in SQL and exposure to non-relational (MongoDB or similar)\nExposure to ML frameworks such as Tensorflow, Pytorch, Scikit-Learn, Spark, would be a plus\nAble to communicate, collaborate, and work effectively in a distributed team.\nCan think about and write high quality code and can demonstrate that capability\nEnjoy tough technical challenges and are naturally intellectually curious\nSeek to drive change and influence others through clear and effective communication.\nWhat you ll do?\nCollaborate with the cross functional teams to help design, build and deliver the headless product offering\nCollaborate with the data scientists and other ML engineers to design, build and deliver the first agentic Revionics experience\nDesign, build, test and maintain end-to-end AI forecasting, optimization and modeling services\nDive deep into the underlying infra architecture to ensure we are building the right way\nWork with product, engineers, and data scientists to translate ideas into new products, services and features\nMentor junior engineers and continually improve our technical stack and processes\nWe also look for\nPassion\nInitiative and a Pioneering Spirit\nQuality orientation\nResourcefulness and application\nAre you the person we re looking for?\nBig picture thinker with laser focus. You have a unique ability to see both the forest and the trees. It s what sets you apart from the rest. You start with a good understanding of the broader strategy, zoom in to assess one particular aspect of that strategy, and then zoom back out to see how changes to that particular area will affect the broader process.\nExpert relationship cultivator. Product managers think you re a good partner -- because you are. Developers feel you respect their opinions -- because you do. You re a true people person, a natural collaborator, and a highly sought-after resource.\nQuality orientation. You have proven success at writing quality user stories and analysis deliverables through the application of established criteria like INVEST and SMART. Your work is thoughtful, timely and valuable to the team.\nResourcefulness and application. At Aptos, we have a pioneering spirit -- when we have questions, we find answers; when we re faced with challenges, we find solutions. We turn to a variety of resources, including our own colleagues, our professional network, the Internet, articles and books -- whatever helps us get the job done. But it s not just about using a variety of resources to gain knowledge -- it s also about applying that knowledge to other areas of the job or business where it might make sense\n\n\nWe are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. By submitting an application for this job, you acknowledge that any personal data or personally identifiable information that you provide to us will be processed in accordance with our Candidate Privacy Notice .",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'orchestration', 'GCP', 'Analytical', 'MongoDB', 'Forecasting', 'Analytics', 'SQL', 'Python']",2025-06-12 15:12:31
Simulator Engineer,Cerebras Systems,3 - 6 years,Not Disclosed,['Bengaluru'],"Cerebras Systems builds the worlds largest AI chip, 56 times larger than GPUs. Our novel wafer-scale architecture provides the AI compute power of dozens of GPUs on a single chip, with the programming simplicity of a single device. This approach allows Cerebras to deliver industry-leading training and inference speeds and empowers machine learning users to effortlessly run large-scale ML applications, without the hassle of managing hundreds of GPUs or TPUs.\nCerebras current customers include global corporations across multiple industries, national labs, and top-tier healthcare systems. In January, we announced a multi-year, multi-million-dollar partnership with Mayo Clinic, underscoring our commitment to transforming AI applications across various fields. In August, we launched Cerebras Inference, the fastest Generative AI inference solution in the world, over 10 times faster than GPU-based hyperscale cloud inference services.\nAbout The Role\nThe Simulator team is responsible for a core internal tool that is used by many teams throughout the company to ensure the success of the next generation Cerebras WSE. The WSE is composed of an array of homogenous tiles. Each tile in composed of a compute element, that runs independent code and has access to its own memory, and a router connecting the compute element to the four neighboring tiles.\nThe core of the simulator is a cycle accurate implementation of the tile. In this mode the simulator is used for design verification work, ensuring the quality of the ASIC design and the simulator implementation. The simulator also enables an array of tile to be combined into a 2D array. In this mode the simulator is used to develop kernel algorithms, where many tiles work together to implement a distributed operation, such as matrix multiplication, or an entire neural network, such as GPT-3 .\nResponsibilities\nDevelop cycle accurate software simulators using C, C++ and Python to simulate precise system behavior of the Cerebras hardware.\nEnhance the simulator to extend to multiple architecture generations of the underlying wafer scale engine.\nSimulate and validate design verification coverage stimulus to build in functional and timing correctness into both the simulator and RTL design.\nExtend the simulator using advanced distributed compute frameworks like MPI and OpenMP to scale the simulator to a cluster of machines.\nDevelop the fabric and interface solution for the architecture simulator which is used as a primary development platform for software development.\nProfile, debug and tune the simulator software for underlying micro architectures to help scale the simulations to cover the entire extent of the wafer scale engine.\nDevelop and optimize infrastructure to interface the simulator with Cerebras test infrastructure and provide API s in Python to bridge the model to the PyTorch framework.\nOptimize the threading model and algorithms inherent to the simulator.\nSkills And Qualifications\nBachelor s or masters degree in computer science or related field, or equivalent practical experience.\nProgramming in C, C++ and Python.\nData structures and algorithms.\nDemonstrated knowledge of computer architecture and microarchitecture.\nSoftware Development using Verilog or VHDL.\nVerification of microarchitecture designs using the Universal Verification Methodology (UVM) framework.\nStrong problem solving and debugging skills.\nWhy Join Cerebras\nPeople who are serious about software make their own hardware. At Cerebras we have built a breakthrough architecture that is unlocking new opportunities for the AI industry. With dozens of model releases and rapid growth, we ve reached an inflection point in our business. Members of our team tell us there are five main reasons they joined Cerebras:\nBuild a breakthrough AI platform beyond the constraints of the GPU.\nPublish and open source their cutting-edge AI research.\nWork on one of the fastest AI supercomputers in the world.\nEnjoy job stability with startup vitality.\nOur simple, non-corporate work culture that respects individual beliefs.\n.",Industry Type: Hardware & Networking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'C++', 'VHDL', 'Verilog', 'Machine learning', 'Data structures', 'Healthcare', 'Open source', 'UVM', 'Python']",2025-06-12 15:12:34
Full Stack AI Engineer (Lead),Inclusive Business Solutions,3 - 8 years,20-35 Lacs P.A.,[],"AI specialists for Full Stack, Computer Vision, and Speech Processing roles. Responsibilities include real-time data integration, emotion recognition, and speech analysis. Must have expertise in AI frameworks, ML models, and optimization techniques.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Speech Recognition', 'Natural Language Processing', 'Computer Vision', 'Machine Learning', 'Python', 'Tensorflow', 'Large Language Model', 'Artificial Intelligence', 'AWS', 'Deep Learning']",2025-06-12 15:12:36
Scientific Business Analyst (Associate) – ELN,Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nThis role involves working closely with Amgen Research partners and Technology peers to ensure that the technology/ data needs for drug discovery research are translated into technical requirements for solution implementation. The role leverages scientific domain and business process expertise to detail product requirements as epics and user stories, along with supporting artifacts like business process maps, use cases, and test plans for the software development teams. This enables the delivery team to estimate, plan, and commit to delivery with high confidence and identify test cases and scenarios to ensure the quality and performance of IT Systems.\n\nYou will join a multi-functional team of scientists and software professionals that enables technology and data capabilities to evaluate drug candidates and assess their abilities to affect the biology of drug targets. This team implements scientific software platforms such as Laboratory Information Management Systems (LIMS) that enable the capture of lab workflows & experimental data and Electronic Lab Notebooks (ELN) that act as Amgens System of Record ensuring data integrity and business continuity. You will implement and manage scientific software platforms across the research informatics ecosystem, and provide technical support, training, and infrastructure management, and ensure it meets the needs of our Amgen Research community.\nFunction as a Scientific Business Systems Analyst within a Scaled Agile Framework (SAFe) product team\nServe as a liaison between global Research Informatics functional areas and global research scientists, prioritizing their needs and expectations\nManage a suite of custom internal platforms, commercial off-the-shelf (COTS) software, and systems integrations\nLead the technology ecosystem for in vivo study data management and ensure that the platform meets their requirements for data analysis and data integrity\nTranslate complex scientific and technological needs into clear, actionable requirements for development teams\nDevelop and maintain a product roadmap that clearly outlines the planned features and enhancements, timelines, and landmarks\nIdentify and manage risks associated with the systems, including technological risks, scientific validation, and user acceptance\nDevelop documentations, communication plans and training plans for end users\nEnsure scientific data operations are scoped into building Research-wide Artificial Intelligence/Machine Learning capabilities\nEnsure operational excellence, cybersecurity and compliance.\nCollaborate with geographically dispersed teams, including those in the US and other international locations.\nFoster a culture of collaboration, innovation, and continuous improvement.\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\n\nBasic Qualifications:\nBachelors degree with 0 - 3 years of experience in Life Sciences, Computer Science, IT, Computational Chemistry/Cheminformatics, Computational Biology/Bioinformatics or related field, OR\nDiploma with 4 - 7years of experience in Life Sciences, Computer Science, IT, Computational Chemistry/Cheminformatics, Computational Biology/Bioinformatics or related field, OR\nDemonstrated expertise in a scientific domain area and related technology needs\nExcellent problem-solving skills and a passion for tackling complex challenges in drug discovery with technology and data\nExperience with writing user requirements and acceptance criteria in agile project management systems such as JIRA\nExperience with Benchling, Revvity, IDBS, or similar LIMS/ELN platforms\nPreferred Qualifications:\nExperience with Agile software development methodologies (Scrum)\nExperience performing or enabling data capture and analysis from instruments in a research laboratory or vivarium\nAbility to communicate technical or complex subject matters in business terms\nKnowledge of business analysis standard processes, DevOps, Continuous Integration, and Continuous Delivery methodology\nExperience with cloud (e.g. AWS) and on-premise infrastructure\nExperience supporting ELN/LIMS platforms in biopharma\n\n\n\nProfessional Certifications:\nSAFe for Teams certification (preferred)\n\n\n\nSoft\n\nSkills:\nAble to work under minimal supervision\nExcellent analytical and gap/fit assessment skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business Analysis', 'LIMS platforms', 'ELN platforms']",2025-06-12 15:12:38
Security Engineer II,Tekion Corp,1 - 5 years,Not Disclosed,['Bengaluru'],"About Tekion:\nPositively disrupting an industry that has not seen any innovation in over 50 years, Tekion has challenged the paradigm with the first and fastest cloud-native automotive platform that includes the revolutionary Automotive Retail Cloud (ARC) for retailers, Automotive Enterprise Cloud (AEC) for manufacturers and other large automotive enterprises and Automotive Partner Cloud (APC) for technology and industry partners. Tekion connects the entire spectrum of the automotive retail ecosystem through one seamless platform. The transformative platform uses cutting-edge technology, big data, machine learning, and AI to seamlessly bring together OEMs, retailers/dealers and consumers. With its highly configurable integration and greater customer engagement capabilities, Tekion is enabling the best automotive retail experiences ever. Tekion employs close to 3,000 people across North America, Asia and Europe.",,,,"['Patch management', 'Computer science', 'Automation', 'Coding', 'SOC', 'Machine learning', 'Vulnerability', 'Information technology', 'Automotive', 'Python']",2025-06-12 15:12:40
AI Security Engineer,Nextiva,2 - 5 years,Not Disclosed,"['Chennai', 'Bengaluru']","Redefine the future of customer experiences. One conversation at a time.\nWe re changing the game with a first-of-its-kind, conversation-centric platform that unifies team collaboration and customer experience in one place. Powered by AI, built by amazing humans.\nOur culture is forward-thinking, customer-obsessed and built on an unwavering belief that connection fuels business and life; connections to our customers with our signature Amazing Service , our products and services, and most importantly, each other. Since 2008, 100,000+ companies and 1M+ users rely on Nextiva for customer and team communication.\nIf you re ready to collaborate and create with amazing people, let your personality shine and be on the frontlines of helping businesses deliver amazing experiences, you re in the right place.\nBuild Amazing - Deliver Amazing - Live Amazing - Be Amazing\nThe AI Security and Compliance Engineer is responsible for working with development and compliance teams to ensure secure and compliant AI development throughout the product lifecycle. The engineer applies knowledge of AI and application security risks and threats to design and implement appropriate, cost-effective security controls during development, deployment, and operation of AI based applications. The engineer defines and promotes the implementation guidelines for data classification, segregation, and access controls to AI model inputs and training data to ensure data confidentiality and privacy for different data sources and user groups. The engineer performs audits and vulnerability assessments, penetration testing and supports mitigation of findings.\nKey Responsibilities:\nEnsure AI products have security and privacy by design.\nEstablish and document policies and guidelines for data classification and data used for training to prevent leaks of sensitive data.\nWork with development and compliance teams to ensure secure and compliant AI development throughout the product lifecycle to meet customer, regulatory, and contractual obligations.\nMonitor and audit AI systems and development processes for compliance with policies, regulations and contractual obligations.\nMonitor and respond to security incidents involving AI systems.\nCreate AI-specific incident management procedures to address AI related security incidents.\nEnhance the resilience of AI systems against potential threats by implementing cyber security best practices, controls, and tools to protect AI models from threats such as those in the OWASP AI Top Ten, including supply chain and model poisoning threats and attempts to access, modify, and exfiltrate confidential information via the query interface.\nEstablish policies and guidelines for access controls, limitations and guardrails on usage and prompts for AI inputs and API s.\nEnsure proper access controls on API s and processing pipelines, and segregation of data.\nCreate, update, and maintain threat models for a wide variety of software projects.\nProvide AI security training for internal development teams.\nMaintain current knowledge of AI risks, threats, and AI testing tools and techniques.\nPerform other duties to support the technical and operational security of the organization as required.\nQualifications:\nBachelor s degree in an IT related field or equivalent experience and 2-5 years of experience in working in IT security, software development, or AI development.\nDesired certifications - one or more of the following: CISSP (Certified Information Systems Security Professional), Certified Information Security Manager (CISM), SSCP (Systems Security Certified Practitioner), CCSP (Certified Cloud Security Professional) or CompTIA Security+.\nUnderstanding of Application Security and Data Security for applications and AI, such as the OWASP Top 10 and the OWASP Top 10 for Generative AI.\nProficiency in and strong working knowledge of AI technologies and models such as Llama and ChatGPT.\nExperience and understanding of threats and risks related to web applications and API s, particularly with AI based applications.\nGeneral knowledge of security implications of threats and vulnerabilities related to networks, servers, operating systems, applications, and databases.\nExperience with vulnerability management, patching, and mitigation assessment.\nExperience working within and implementing policies for a security framework such as ISO 27001 and NIST.\nFlexibility to work off-hours to support global project teams and maintenance windows.\nAbility to support 24x7 on-call for incident response on a rotating basis.\nExperience developing software, scripting and using SQL queries to automate controls, processes and reporting.\nCompetencies:\nStrong analytical problem-solving skills and attention to detail.\nOrganization, Time Management & Prioritization - Self-starter that focuses on key priorities; plans, organizes, schedules and executes on tasks and projects in an efficient and productive manner.\nAbility to form productive relationships across the organization to accomplish information security objectives.\nAbility and willingness to learn all aspects of the information security field.\nProfessional verbal and written communication skills in English.\nExpresses ideas using clear, effective and efficient language. Listens patiently and attentively. Adapts to the purpose of the communication with appropriate style, substance, detail, confidence and channel. Possess the ability to manage multiple channels of communication simultaneously; phone, email, tickets, and chat.\nAble to assess, document, and prioritize identified security flaws and vulnerabilities based on risk.\nTotal Rewards\nOur Total Rewards offerings are designed to allow our employees to take care of themselves and their families so they can be their best, in and out of the office.\nOur compensation packages are tailored to each role and candidates qualifications. We consider a wide range of factors, including skills, experience, training, and certifications, when determining compensation. We aim to offer competitive salaries or wages that reflect the value you bring to our team. Depending on the position, compensation may include base salary and/or hourly wages, incentives, or bonuses.\nMedical - Medical insurance coverage is available for employees, their spouse, and up to two dependent children with a limit of 500,000 INR, as well as their parents or in-laws for up to 300,000 INR. This comprehensive coverage ensures that essential healthcare needs are met for the entire family unit, providing peace of mind and security in times of medical necessity.\nGroup Term & Group Personal Accident Insurance - Provides insurance coverage against the risk of death / injury during the policy period sustained due to an accident caused by violent, visible & external means.\nCoverage Type - Employee Only\nSum Insured - 3 times of annual CTC with minimum cap of INR 10,00,000\nFree Cover Limit - 1.5 Crore\nWork-Life Balance - 15 days of Privilege leaves per calendar year, 6 days of Paid Sick leave per calendar year, 6 days of Casual leave per calendar year. Paid 26 weeks of Maternity leaves, 1 week of Paternity leave, a day off on your Birthday, and paid holidays\nFinancial Security - Provident Fund & Gratuity\nWellness - Employee Assistance Program and comprehensive wellness initiatives\nGrowth - Access to ongoing learning and development opportunities and career advancement\nAt Nextiva, were committed to supporting our employees health, well-being, and professional growth. Join us and build a rewarding career!\nEstablished in 2008 and headquartered in Scottsdale, Arizona, Nextiva secured $200M from Goldman Sachs in late 2021, valuing the company at $2.7B.To check out what s going on at Nextiva, check us out on Instagram , Instagram (MX) , YouTube , LinkedIn , and the Nextiva blog .\n#LI-RQ1 #LI-Hybrid",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Cism', 'Testing tools', 'Information security', 'ISO 27001', 'Healthcare', 'Incident management', 'Application security', 'Windows', 'Penetration testing']",2025-06-12 15:12:43
Software Engineer - C | Python | Linux | Platform Infrastructure,Cisco,9 - 12 years,Not Disclosed,['Bengaluru'],"you will:\nDevelop and integrate products deployed by leading service providers worldwide.\nCollaborate with a vibrant, BU-wide technical community to exchange ideas and innovate on next-generation technology.\nExplore opportunities for personal growth while mentoring colleagues and working on cutting-edge technologies.\nAs a key member of this team, you will:\nWork alongside seasoned engineers to architect, design, and develop some of routers and solutions for the world's largest service provider, web centers, and enterprises.\nContribute to the evolution of these systems to support exciting new customer business paradigms.\nInteract and collaborate with some of the finest talent in the industry, making work both fun and challenging.\nEngage with other groups such as Product Management, Marketing, Sales, Customer Support, and Advanced Services.\nWho You Are:\nYou possess:\nIn-depth knowledge of C and a solid understanding of Python.\nExtensive experience in a Unix/Linux-based development environment.\nExcellent coding, automation, and debugging skills.\nStrong teamwork and communication skills.\nFamiliarity with hardware architectures such as PCI, PCIe, DMA, I2C, SPI, NPUs/DPUs and processors like x86, AMD, and ARM. Experience with board bringup is a plus.\nExperience with emerging technologies such as AI/ML and cloud computing is a plus.\nExperience and Qualifications:\nExperience: 9 to 12 years in embedded firmware development.\nEducation: BE/B.Tech/ME/M.Tech/MS in CS/EE/IT/ECE, MCA, or similar education.\nProven ability to derive design and code based on technical standards and write comprehensive, focused design documents.\nExperience in developing software/firmware for networking equipment.\nExcellent knowledge of software architecture and system design.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'networking', 'artificial intelligence', 'sales', 'linux', 'pci', 'debugging', 'software engineering', 'i2c', 'cloud computing', 'arm', 'firmware', 'ml', 'board bringup', 'c', 'embedded firmware development', 'software development', 'system design', 'spi', 'marketing', 'x86', 'embedded c', 'dma', 'pcie', 'unix']",2025-06-12 15:12:45
Mlops Engineer,Rarr Technologies,8 - 13 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']",Key Responsibilities\nResponsible for building and maintaining robust machine learning pipelines ensuring efficient model deployment monitoring and lifecycle management within a cloud-based environment\nExtensive expertise in MLOps specifically with Google Cloud Platform GCP and Vertex AI and a deep understanding of model performance drift detection and GPU accelerators\nBuild and maintain scalable MLOps pipelines in GCP Vertex AI for endtoend machine learning workflows\nManage the full MLOps lifecycle from data preprocessing model training and deployment to model monitoring and drift detection\nImplement realtime model monitoring and drift detection to ensure optimal model performance over time\nOptimize model training and inference processes using GPU accelerators and CUDA\nCollaborate with cross functional teams to automate and streamline machine learning model deployment and monitoring\nUtilize Python 310 with libraries such as pandas NumPy and TensorFlow to handle data processing and model development\nSet up infrastructure for continuous training testing and deployment of machine learning models\nEnsure scalability security and high availability in all machine learning operations by implementing best practices in MLOps\nRequirements\n5 years of experience in MLOps and building ML pipelines 3 years of experience in GCP Vertex AI\nDeep understanding of the MLOps lifecycle and automation of ML workflows\nProficient in Python 310 and related libraries such as pandas NumPy and TensorFlow\nStrong experience in GPU accelerators and CUDA for model training and optimization\nProven experience in model monitoring drift detection and maintaining model accuracy over time\nStrong problemsolving skills with the ability to work in a fast paced environment\nKnowledge of data versioning and model version control techniques\nFamiliarity with TensorFlow Extended TFX or other ML workflow orchestration frameworks,Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['GCP', 'vertexai', 'mlops']",2025-06-12 15:12:47
Senior Engineering Manager,Product Base Company,12 - 18 years,Not Disclosed,['Bengaluru'],"About Client:\n\nOur Client is revolutionizing how the world plans, builds, and manages infrastructure projects with Masterworks, our industry-leading enterprise SaaS platform. Trusted by over 300 customers managing $300 billion in capital programs, Masterworks is setting new standards for project delivery and asset management. Recognized as one of the Top 25 AI Companies of 2024 and a Great Place to Work for three consecutive years, we are leveraging artificial intelligence to create a smarter, more connected future for customers in transportation, water and utilities, healthcare, higher education, and the government, with over 40,000 projects across North America. Our Client dont just develop softwareThey shape the future. If youre excited to join a fastgrowing company and collaborate with some of the brightest minds in the industry to solve realworld challenges, lets connect.\n\n\nJob Summary :\n\nThe Senior Engineering Manager will lead multiple engineering teams responsible for designing, developing, and scaling software products using Microsoft technologies. This role combines strong leadership capabilities with deep technical expertise, particularly in C#, ASP.NET, .NET Core, SQL Server, and IIS. The Senior Engineering Manager will set the technical direction, ensure engineering excellence, and collaborate with cross-functional teams to deliver high-quality, scalable solutions that align with business goals.\n\nKey Responsibilities\n\n1. Technical Leadership & Strategy:\n\nLead the development and implementation of scalable software solutions, with a strong focus on Microsoft technologies (C#, ASP.NET, .NET Core, SQL Server, IIS).\n\no Define the technical strategy and roadmap, ensuring alignment with overall product and business objectives.\n\no Provide hands-on technical guidance to engineering teams, including architecture, design, and code reviews, to ensure high-quality, scalable solutions.\n\n2. Engineering Excellence: o Establish best practices for software development, focusing on clean code, maintainability, performance, and security, especially for Microsoft stack-based solutions.\n\no Drive innovation in technology choices and design patterns, ensuring efficient use of C#, ASP.NET Core, SQL Server, and other key Microsoft frameworks.\n\no Collaborate with the infrastructure team to optimize the deployment and performance of applications on IIS and cloud environments like AWS or Azure.\n\n3. Project & Delivery Management:\n\no Oversee the execution of multiple software development projects, ensuring that engineering teams are aligned with the technical roadmap.\n\no Ensure effective sprint planning, task prioritization, and on-time delivery of projects using Agile methodologies, with a focus on .NET technologies",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['C#', 'Engineering Manager', 'ASP.NET', 'roadmap', '.NET Core', 'design', 'code reviews', 'SQL Server']",2025-06-12 15:12:51
AI Engineer,Shashwath Solution,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary:\nWe are seeking a highly skilled and hands-on AI Engineer with 3+ years of proven experience in building and deploying solutions using Generative AI and Large Language Models (LLMs). You will work on cutting-edge applications leveraging transformer-based architectures, fine-tuning, prompt engineering, and scalable AI deployments.\n\nThis role is ideal for engineers passionate about AI research and real-world productization of generative AI technologies.\n\nKey Responsibilities:\nDesign, develop, and deploy solutions using LLMs (e.g., GPT, LLaMA, Mistral, Claude, PaLM, etc.) for various NLP and content generation tasks.\n\nWork on fine-tuning, prompt engineering, and retrieval-augmented generation (RAG) pipelines.\n\nIntegrate LLMs into enterprise applications with APIs and orchestrate workflows using Python, LangChain, or similar frameworks.\n\nOptimize model performance, latency, and cost for production use.\n\nCollaborate with data scientists, MLOps engineers, and product managers to deliver scalable AI features.\n\nConduct experiments, analyze results, and publish internal findings or contribute to whitepapers.\n\nEnsure ethical, secure, and responsible use of AI technologies in all implementations.\n\nRequired Skills & Experience:\n3+ years of hands-on experience working with Generative AI, LLMs, and NLP technologies.\n\nStrong programming skills in Python and experience with libraries like Transformers (Hugging Face), LangChain, PyTorch, TensorFlow, etc.\n\nProven track record of fine-tuning LLMs, developing embeddings, and working with vector databases (e.g., FAISS, Pinecone, Weaviate).\n\nExperience deploying models on cloud platforms (AWS, Azure, GCP) and using ML pipelines or MLOps tools.\n\nSolid understanding of deep learning, NLP architectures, tokenization, and evaluation metrics for generative models.\n\nExperience in API development and integration of LLMs into user-facing applications.\n\nPreferred Qualifications:\nMasters or PhD in Computer Science, AI/ML, Data Science, or related field.\n\nExperience with OpenAI APIs, Anthropic, Cohere, or open-source LLMs (e.g., Mistral, Falcon, LLaMA 3).\n\nUnderstanding of RLHF (Reinforcement Learning from Human Feedback) and model alignment techniques.\n\nContributions to open-source AI projects or publications in GenAI/LLM.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Generative AI', 'LLaMA', 'GPT', 'Large Language Models', 'Azure', 'LangChain', 'Hugging Face', 'deep learning', 'OpenAI API', 'NLP', 'PyTorch', 'MLOps', 'GCP', 'PaLM', 'AWS', 'Python', 'TensorFlow']",2025-06-12 15:12:54
Staff Systems Integration Engineer,Analog Devices,6 - 11 years,Not Disclosed,['Bengaluru'],"About Analog Devices\nAnalog Devices, Inc. (NASDAQ: ADI ) is a global semiconductor leader that bridges the physical and digital worlds to enable breakthroughs at the Intelligent Edge. ADI combines analog, digital, and software technologies into solutions that help drive advancements in digitized factories, mobility, and digital healthcare, combat climate change, and reliably connect humans and the world. With revenue of more than $9 billion in FY24 and approximately 24,000 people globally, ADI ensures todays innovators stay Ahead of Whats Possible . Learn more at www.analog.com and on LinkedIn and Twitter (X) .\nAbout Us\nCome develop technology solutions that solve planetary-scale problems in the Analog Devices Battery Management business unit. The electric vehicle and green-energy revolution is here, and we are changing the world with our industry-leading technology. From the high performance and thrill that EVs deliver to the carbon-footprint reduction that makes the world a greener place, we are looking for more innovators to help make it all a reality. Whether you are a car-lover, a battery-geek, or a technologist with a passion for a cleaner and more sustainable future, the ADI Battery Management business unit is a premier career destination with a bright future.\nAnalog Devices Electrification Group is focused on developing world-class solutions for battery management for electro-mobility and stationary energy storage.\nThe Role\nVerification and Validation (V&V) is about providing objective evidence that the system/product, when in use, fulfills the requirements in the intended operating environment. Algorithms are cross-functional in nature, and the verification of these solutions is as critical as their design. This role requires excellent system understanding, system verification thinking, and close teamwork with hardware and software design, algorithm design, systems application, system architecture, and quality teams.\nResponsibilities:\nDrive the verification and validation effort of our key programs.\nWork with algorithm engineers and Embedded SW to understand requirements, specifications, and implementations to prepare and execute suitable verification and validation plans.\nDefine a long-term verification and validation roadmap of capabilities, flows, and methodologies to continuously improve productivity.\nCreatively plan to test devices in corner cases where potential marginalities and issues could arise beyond the given specifications.\nConsult on required hardware system boards and software to automate test cases to achieve the best coverage at the highest efficiency.\nQualifications\nMinimum BS in Electrical or Computer Engineering; MSEE or MSCE and 6+ years of experience preferred.\nExperience and Skills\nExperience in verifying complex algorithms such as battery state estimation and health algorithms (SoC, SoH, SoP) for automotive or industrial applications.\nUnderstanding the working theory of lithium-ion batteries and familiarity with new chemistries.\nExperience with machine learning and/or statistical analysis methods.\nProficiency in C/C++ (embedded) and Python or other programming/scripting languages.\nProficiency in MATLAB/Simulink\nBackground in Systems Engineering and/or formal testing of complex systems (e.g., automotive).\nExcellent hardware and software troubleshooting skills.\nExperience with software compiler/debug tools, such as IAR, Keil, or Segger.\nExperience with software revision control, repositories, and regression testing (e.g., Git, Bitbucket, SVN).\nAbility to work in teams and collaborate effectively with people in different functions.\nClear communicator with excellent verbal, written, and organizational skills.\nMotivated, proactive, fast learner, and hands-on.\nNice to Have\nBackground in automated software and system verification.\nExperience with requirements management tools (e.g., JAMA, DOORs).\nExperience with ARM-based microcontrollers (Cortex-Mx or Cortex-Rx series).",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['C++', 'Software design', 'Semiconductor', 'Analog', 'SOC', 'Healthcare', 'Simulink', 'MATLAB', 'Automotive', 'Python']",2025-06-12 15:12:56
Java (Backend) Engineer,Service based Top B2C/B2B MNC in Analyti...,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Job Title: Java (Backend) Engineer\nLocation: Bangalore & Chennai\nWork Mode: Hybrid (Work from Client Office)\nJob Description:\nWe are looking for a Java Backend Engineer to join our dynamic team. The ideal candidate will have solid experience in back-end development using Java, Microservices architecture, and Spring Boot framework. You will be responsible for developing robust server-side logic, building scalable APIs, and ensuring application performance and responsiveness.\nMust-Have Skills:\nStrong experience in Java programming (Back-end Server & SDK Development)\nHands-on experience with Spring Boot, Microservices, and RESTful APIs\nFamiliar with tools such as Git, Maven, and Jenkins\nSolid understanding of system reliability, availability, scalability, and performance\nBasic working knowledge of SQL\nGood to Have:\nExposure to database design and optimization\nKnowledge of front-end basics and mobile-responsive design\nRoles and Responsibilities:\nControl all technical, functional, and visual aspects of software in development\nDesign and develop robust, scalable server-side architecture\nDevelop and maintain well-functioning databases and back-end logic\nWrite, test, and maintain REST APIs\nTroubleshoot, debug, and upgrade applications\nOptimize software for performance and responsiveness\nCollaborate with frontend developers, analysts, and data scientists\nImplement security and data protection best practices\nCreate and maintain technical documentation\nEducation:\nBachelors degree in Computer Science, Software Engineering, MIS, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spring Boot', 'Microservices', 'Restfull Api', 'Msql', 'Orcale']",2025-06-12 15:12:59
Solution Design Lead & implimantation,Excellerate Global Solutions,13 - 23 years,10-20 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Solution Design & Implementation:\nLead and participate in the full project lifecycle of SAP S/4HANA Public Cloud implementations, focusing on Procurement (Sourcing & Procurement/MM), Finance (FI/CO), and Sales & Distribution (SD) modules.\nConduct in-depth business process analysis, gather requirements, and translate them into robust and scalable SAP S/4HANA Public Cloud solutions aligned with SAP best practices.\nDesign, configure, and customize SAP S/4HANA Public Cloud functionalities for Order-to-Cash (O2C), Procure-to-Pay (P2P), Record-to-Report (R2R), and other relevant cross-functional processes.\nEnsure seamless integration between Procurement, Finance, and SD modules, as well as with other SAP Cloud modules (e.g., EWM, PP) and third-party applications where applicable.\nLeverage SAP Activate methodology for project delivery, guiding clients through fit-to-standard workshops and solution design.\nFunctional Expertise:\nProcurement (MM): Expertise in Material Master, Vendor Master, Purchase Requisitions, Purchase Orders, Contracts, Sourcing, Inventory Management, Invoice Verification, and supplier collaboration.\nFinance (FI/CO): Strong knowledge of General Ledger, Accounts Payable, Accounts Receivable, Asset Accounting, Bank Accounting, Cost Center Accounting, Profit Center Accounting, Internal Orders, Product Costing, Profitability Analysis (CO-PA), and treasury functions. Understanding of the Universal Journal (ACDOCA) and its impact.\nSales & Distribution (SD): Proficiency in Sales Order Management, Pricing, Delivery Processing, Billing, Credit Management, Returns Management, and ATP (Available-to-Promise).\nTechnical Acumen (Public Cloud Specific):\nUnderstanding of SAP S/4HANA Public Cloud architecture, standard scope, extensibility options (e.g., in-app extensibility, side-by-side extensions using SAP BTP).\nFamiliarity with SAP Fiori applications and user interfaces for relevant modules.\nKnowledge of data migration strategies and tools within the Public Cloud environment (e.g., Migration Cockpit).\nExperience with SAP Cloud ALM for implementation, operations, and monitoring.\nClient Engagement & Leadership:\nAct as a trusted advisor to clients, effectively communicating complex technical and functional concepts to both business and IT stakeholders.\nLead workshops, facilitate discussions, and drive decisions throughout the project lifecycle.\nProvide expert guidance on cloud transformation strategies, change management, and user adoption.\nMentor and guide junior consultants, fostering a culture of knowledge sharing and continuous improvement.\nTesting, Training & Support:\nDevelop and execute comprehensive test plans (unit, integration, UAT) to ensure the solution meets business requirements and is defect-free.\nPrepare detailed training materials and conduct engaging training sessions for end-users.\nProvide post-implementation support, troubleshoot issues, and drive resolution in collaboration with technical teams.\nContinuous Improvement & Innovation:\nStay updated with the latest SAP S/4HANA Public Cloud releases, functionalities, and industry best practices.\nIdentify opportunities for process optimization and leverage new SAP innovations (e.g., AI, Machine Learning capabilities within S/4HANA) to enhance client value.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Sap Hana', 'Solution Design', 'SAP FICO', 'SAP SD', 'SAP MM', 'SAP Finance']",2025-06-12 15:13:01
Cloud Senior Automation Engineer,Enphase Energy,5 - 10 years,Not Disclosed,['Bengaluru'],"Description\nEnphase Energy is a global energy technology company and leading provider of solar, battery, and electric vehicle charging products. Founded in 2006, Enphase transformed the solar industry with our revolutionary microinverter technology, which turns sunlight into a safe, reliable, resilient, and scalable source of energy to power our lives. Today, the Enphase Energy System helps people make, use, save, and sell their own power. Enphase is also one of the fastest growing and innovative clean energy companies in the world, with approximately 68 million products installed across more than 145 countries.\nWe are building teams that are designing, developing, and manufacturing next-generation energy technologies and our work environment is fast-paced, fun and full of exciting new projects.\nIf you are passionate about advancing a more sustainable future, this is the perfect time to join Enphase!\nAbout the role:\nThe Enphase cloud team is looking for a Sr Quality Engineer for its software products . In this role, you will work with product managers, business owners and developers to assess the quality of software products at par with best-in-class solutions in the market.\nYou will be responsible for analyzing and making recommendations for improving the quality of our cloud services and applications across our ecosystem. You and your team will be involved in every part of the product cycle, from high-level design to development and validation.\nWhat you will do:\nOwn the complete quality ownership of the system under test .\nAble to design test cases that are of high quality which helps improving quality of the system .\nHelp team to design good test cases, review their work and give feedback .\nDesign test framework for different layers of the application.\nAdd automated tests on web services, database and UI layer.\nBe the gate keeper on the quality of the product .\nDefine, write, and execute non-functional tests and analyze results.\nDebug the issues and help developers to design the best software product.\nDrive quality metrics on system under test, be responsible for quality of the system.\nAnalyse the tests, progress of the tests and report to the management on the quality of the system on day-to-day basis .\nImprovise the test case designs, negative tests, repeatability tests, data validation, impact and risk-based testing.\nAble to work on agile, early to market without any compromise on quality .\nAble to work on continuous development and continuous testing\nHelp team to setup pipeline which executes automated test based on schedule, on-demand\nWho you are and what you bring:\nBE/BTech in Computer Science, Computer Engineering, or equivalent experience .\n5+ years of experience in software development/testing.\n2+ years of experience testing high scale applications with distributed architecture.\nExcellent programming skills and ability to debug issues at the code level, development .experience is an added advantage.\nExperience in negative testing.\nProvide out of box solution to deliver quality in a fast-paced development organization.\nStrong understanding of cloud, databases, and performance tradeoffs .\nGo-getter attitude and capable of pursuing tough engineering challenges to final solutions.\nLean / Agile attitude .\nProactive and results-oriented .\nTeam player/builder; possess a can-do attitude.\nGood at stakeholder management.\nExperience in providing estimates, plan and committed to deliverables.\nStrong technical and leadership skills: decisive, determined, strategic, and able to lead, motivate, and inspire others.\nExperience as software technical lead/architect with cloud and digital SW is a plus.\nKnowledge of data analytics and machine learning are a plus.",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data validation', 'Machine learning', 'Cloud', 'Agile', 'Test cases', 'High level design', 'Stakeholder management', 'Testing']",2025-06-12 15:13:03
Senior QA Automation Engineer,Luxoft,6 - 11 years,Not Disclosed,['Bengaluru'],"Lead the design and implementation of robust, scalable test automation frameworks.\nDevelop and maintain automated test scripts for functional, regression, and integration testing.\nCollaborate with cross-functional teams including developers, BAs, and DevOps to ensure high-quality releases.\nDrive test strategy, planning, and execution for Murex-related projects.\nMentor junior QA team members and enforce best practices in test automation and quality assurance.\nParticipate in code reviews, CI/CD pipeline integration, and test data management.\nSupport UAT and production validation efforts.\nSkills\nMust have\n6+ years of experience in software testing with at least 4+ years in automation.\nStrong hands-on experience with test automation tools such as Selenium, TestNG, Cucumber, RestAssured, or similar.\nProficiency in Java/Python or other scripting languages used in automation.\nExperience in building and maintaining custom automation frameworks.\nSolid understanding of Murex architecture, trade lifecycle, Trade insertion, E2E deal flow (FO, BO, Confo and settlements).\nStrong knowledge of SQL and database validation.\nExperience with CI/CD tools like Jenkins, Git, Maven, or similar.\nNice to have\nWorking knowledge of Xceptor for data transformation and reconciliation.\nExperience with performance testing tools (e.g., JMeter, LoadRunner) is a plus.\nExposure to cloud platforms (AWS, Azure) and using Machine Learning skills\nISTQB Advanced Level or equivalent certification.\nStrong leadership and mentoring capabilities.\nExcellent communication and stakeholder management skills.\nAnalytical mindset with a proactive approach to problem-solving.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Maven', 'Manager Quality Assurance', 'Data management', 'Testing tools', 'Performance testing', 'Selenium', 'Murex', 'Testing', 'SQL', 'Python']",2025-06-12 15:13:06
Senior Research Engineer - LLM,Trask,10 - 15 years,Not Disclosed,['Bengaluru'],"Develop, execute, implement and check methods, plans, toolsets and approaches that are appropriate and compliant to achieve digital solutions for the process intended\nIdentify technical problems and apply/integrate solutions as needed with a set based design approach\nAnalyse customer requirements and define technical solutions as input for proposals to develop proof of concepts\nWork with cross functional teams and multiple sites during the development process\nGenerate assigned project deliverables, documents, and reports according to the project milestones\nSupport and participate in technical reviews, including the creation and preparation of technical data and presentations as needed and support other engineers with peer to peer reviews\nSupport Lean culture and improvement initiatives in the organisation\nTake part in regular sprint planning meetings to plan, review and deliver outputs based on agile philosophy\nSupport in creation of training material and knowledge sharing in the relevant area of work\nSupport idea generation and CI activities\nPerform all activities independently and help other engineers within the program as required.\nBachelors in Engineering or higher, with minimum of 10 years of relevant experience Automation & Software development.\nDesign data pipelines to handle large-scale data for training, ensuring data security and compliance with aerospace and defence standards.\nExcellent experience in shop floor automation and I4.0/IOT Integration\nExcellent Understanding of Industrial Communication protocols and establishing communication between different Industrial systems.\nGood knowledge of data structure, data modelling and database architecture\nGood Knowledge of implementing business process into functional codes\nExcellent knowledge of software coding , integrated development platforms\nProficiency in programming with python, C++,C, C#, Java, .NET, VB, SQL and working knowledge in GIT\nAbility to conduct POCs and guide team members to extract valuable insights and drive data-driven decision-making.\nEvaluate and select appropriate tools and applications for tasks.\nStrong software development skills, including version control (e.g., Git), debugging, testing, and documentation. Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes) is beneficial.\nStay updated with latest developments in Automation, Software Development, NLP, ML, AI, LLM technology around the globe relevant to aerospace and defence sector and work along with team to quickly leverage, test and validate new solutions applicable to the working projects by applying cutting edge technologies.\nShould have strong problem-solving skills and ability to collaborate with cross-functional teams, including domain experts, data scientists, and engineering teams, to gather requirements and translate them into scalable applications.\nBachelors in Engineering or higher, with minimum of 10 years of relevant experience Automation & Software development.\nDesign data pipelines to handle large-scale data for training, ensuring data security and compliance with aerospace and defence standards.\nExcellent experience in shop floor automation and I4.0/IOT Integration\nExcellent Understanding of Industrial Communication protocols and establishing communication between different Industrial systems.\nGood knowledge of data structure, data modelling and database architecture\nGood Knowledge of implementing business process into functional codes\nExcellent knowledge of software coding , integrated development platforms\nProficiency in programming with python, C++,C, C#, Java, .NET, VB, SQL and working knowledge in GIT\nAbility to conduct POCs and guide team members to extract valuable insights and drive data-driven decision-making.\nEvaluate and select appropriate tools and applications for tasks.\nStrong software development skills, including version control (e.g., Git), debugging, testing, and documentation. Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes) is beneficial.\nStay updated with latest developments in Automation, Software Development, NLP, ML, AI, LLM technology around the globe relevant to aerospace and defence sector and work along with team to quickly leverage, test and validate new solutions applicable to the working projects by applying cutting edge technologies.\nShould have strong problem-solving skills and ability to collaborate with cross-functional teams, including domain experts, data scientists, and engineering teams, to gather requirements and translate them into scalable applications.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business process', 'C++', 'Automation', 'GIT', 'Coding', 'data security', 'Aerospace', 'VB', 'SQL', 'Python']",2025-06-12 15:13:08
Senior Staff Engineer I Custom Layout - Serdes,Alphawave Semi,2 - 7 years,Not Disclosed,['Bengaluru'],"The Opportunity\n\nWere looking for the Wavemakers of tomorrow.\nCustom Layout / High-Speed Analog Layout Engineer\nAlphawave IP builds industry-leading wired connectivity solutions that enable data to travel faster, more reliably, and with higher performance at lower power. Our technology is embedded in leading-edge semiconductors built to power global network and computer systems. It is an essential part of the core infrastructure enabling next generation services in data centers, artificial intelligence, 5G wireless infrastructure, data networking, autonomous vehicles, and solid-state storage.\nThe Opportunity\nThe Alphawave IP team combines technologists from different disciplines who come together with a shared passion for electronics, software, and communication technology. We look for individuals with a deep desire to build great products and we value collaboration, curiosity, and a commitment to solving hard problems.\nThe Alphawave Custom Layout team is composed of a group of highly technical, innovative, and passionate engineers, collaborating to develop the analog layouts and architectures for our world class high-speed SerDes IP s.\nWhat You ll Do\nCustom analog layout design for industry leading high speed Serdes architectures\nWorking in leading edge semiconductor nodes and cad tools including latest 3nm node\nDetailed collaboration in optimizing layouts with analog design team\nFloor-planning\nPerform physical verification (DRC,ANT,LVS,ERC, )\nDevelopment and maintenance of layout software automation capabilities\nWork with a team of world-class engineers who are willing to help when needed and are happy to receive help when offered\nWhat You ll Need\nBachelors in Electrical/Computer Engineering, EngSci, or equivalent\nFamiliarity with high-speed analog layout, electronics and CMOS transistors\nBonus points if you have worked in recent FinFet technologies (7nm, 5nm, etc)\nMinimum of 2 years of custom layout experience\nThe position is located in Vancouver\n5+ years of experience is preferred\nAbout You\nExcellent communication skills\nAble to listen to and appreciate ideas and opinions that differ from yours\nExtremely detail oriented\nSuperb analytical and problem-solving skills\nDrives for consistency\nTakes personal pride in high standard of outputs\nSelf-motivated and self-managing\n""We have a flexible work environment to support and help employees thrive in personal and professional capacities""\nAs part of our commitment to the well-being and satisfaction of our employees, we have designed a comprehensive benefits package that includes:\nCompetitive Compensation Package\nRestricted Stock Units (RSUs)\nProvisions to pursue advanced education from Premium Institute, eLearning content providers\nMedical Insurance and a cohort of Wellness Benefits\nEducational Assistance\nAdvance Loan Assistance\nOffice lunch & Snacks Facility\nEqual Employment Opportunity Statement\nAlphawave Semi is an equal opportunity employer, welcoming all applicants regardless of age, gender, race, disability, or other protected characteristics. We value diversity and provide accommodations during the recruitment process.",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Wireless', 'Automation', 'Product innovation', 'Analytical', 'Artificial Intelligence', 'CAD', 'Physical verification', 'Wellness', 'Data communication', 'Analog layout']",2025-06-12 15:13:11
Senior Engineer - Structures,Wsp Consultants,4 - 9 years,Not Disclosed,['Bengaluru'],"Role Summary\n\n\nThis role is to work as part of engineering team, focus on project delivery, production and liaison with the WSP in India Netherland team and mentoring. Role will be working under the supervision of an Principal Engineer or Associate .\n\n\n\n\nResponsibilities:\n\n\nCore Functions\n\n\n\nPrepare feasibility study reports to meet brief requirements in the agreed format and review with the Local CRC Head of Structures\n\nWork with the team to assemble a design specification compliant with the employers requirements, agree its format and content, and monitor and review its preparation ensuring delivery by the due date\n\nExpertise in Concept design to Detailed design stage for Steel and Concrete buildings\n\nAgree and monitor scope of works with the local CRC Head of Structures\n\nCarry out detailed design as per client requirements in accordance with standard codes, QA and technical review and sign off by the local CRC Head of Structures, including complex calculations and co-ordination issues\n\nReview and monitor the production of calculations including QA, technical reviews and sign off\n\nCo-ordinate project contract documents (drawings and specifications) and reviews input from team members\n\nDeal with the day to day queries from the team, ensuring that relevant information is available on time for construction activity\n\nLead the design process and encourage the rest of the team to deliver appropriate and cost effective solutions to the agreed programme.\n\nManagement of a team of engineers and BIM technicians.\n\n\n\n\n\nTechnical and Project Management\n\n\n\nRaise the level of technical competence within the teams\n\nImplement delivery and quality measurement processes\n\nPromote technical excellence in all our projects\n\nUndertake technical reviews and contribute to the concept design\n\nDevelop positive professional relationship with the WSP Netherlands team, communicating openly about project progress\n\nParticipate in team meetings, disseminate information within the team, and communicate with other teams in WSP\n\nIdentify and act on, or refer, potential risk issues and follow in full the company commercial and contracting processes\n\nManage delegated tasks to ensure that deadlines are met and flag resourcing concerns to team leader\n\nComplete timesheet accurately ahead of weekly deadlines\n\n\n\nKey Competencies / Skills\n\n\n\nThe applicant will have proven experience in the design of Building Structures, Concrete and Streel building designs, Seismic design with significant experience in a similar role or demonstration of a good track record\n\nGood presentation skills are also required\n\nMust be fully conversant with technical structural software, such as RFEM, FEM Design, ROBOT, ETABS, SAFE, RAM and STAAD Pro\n\nExperience with International design codes viz. , Eurocode, ACI etc. , is required\n\nA sound understanding of Microsoft Outlook, Word, Excel, Powerpoint is essential\n\nMust be fluent in English with an excellent understanding of technical terminology\n\nDemonstrate good management, communication and technical skills and be capable of working both within the team and independently, as dictated by work load\n\n\n\nQualifications\n\n\n\nThe candidate should possess a Bachelor s degree in Civil or Structural Engineering and possess membership to an accredited engineering body. Master s degree is preferred.\n\nIt is desirable that the candidate has obtained UK Chartered Engineer status or pursuing the same.\n\nExperience: 8 to 14 years",Industry Type: Management Consulting,Department: Construction & Site Engineering,"Employment Type: Full Time, Permanent","['Head Business Development', 'Concept design', 'CRC', 'Project management', 'MS Outlook', 'Bim', 'Resourcing', 'Staad Pro', 'Structural engineering', 'Monitoring']",2025-06-12 15:13:13
Python Developer,Coartha Technosolutions,0 - 1 years,Not Disclosed,['Hyderabad( Madhapur )'],"Seeking a passionate Python Developer (Fresher) to work on innovative projects in Python, AI, and ML. Great opportunity to build and maintain high-quality product features and grow your skills in a dynamic, collaborative environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Large Language Model', 'Pandas', 'MongoDB', 'Machine Learning', 'Deep Learning', 'Numpy', 'Elastic Search', 'Flask']",2025-06-12 15:13:16
Artificial Intelligence Architect,Ltimindtree,12 - 16 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']",We are looking for an experienced AI ML Developers experience in data science specializing in machine learning python statistical modelling and big data technologies pyspark sql.\n\nThe ideal candidate will have a strong background in developing and deploying machine learning models optimizing ML pipelines and handling largescale structured and unstructured data to drive business impact.\n\nDeep understanding of supervised and unsupervised learning including regression classification Multiclass classification clustering and NLP Proficiency in statistical analysis AB testing and causal inference techniques Experience with model deployment and MLOps in cloud environments AWS GCP \n\nKey Responsibilities\n\nDevelop and deploy machine learning models and predictive analytics solutions for business impact\nWork with largescale structured and unstructured data to extract insights and build scalable models\nDesign implement and optimize ML pipelines for realtime and batch processing\nCollaborate with engineering product and business stakeholders to translate business problems into data science solutions\nApply statistical modeling AB testing and causal inference techniques to evaluate business performance\nApply machine learning and statistical techniques for audience segmentation helping to identify patterns and optimise business strategies\nDrive research and innovation by staying updated with cuttingedge MLAI advancements and incorporating them into our solutions\nOptimize data science models for performance scalability and interpretability in production environments\nMentor junior data scientists and contribute to best practices in data science and engineering,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Architect', 'MLOps', 'Machine Learning', 'Ai Solutions', 'Aiml', 'Ml']",2025-06-12 15:13:18
Gen AI Experts,Axtria,5 - 10 years,Not Disclosed,"['Noida', 'Hyderabad', 'Bengaluru']","Axtria: -Axtria is a global provider of award-winning cloud software and data analytics to the life sciences industry. Axtria enables life sciences organizations to transform the product commercialization journey and deliver much-improved healthcare outcomes for patients worldwide. We are acutely aware that our work impacts millions of people and are incredibly passionate about the improvement we can bring to patients lives.\nOur focus is on delivering solutions that help pharmaceutical, medical device, and diagnostics companies complete the journey from data to insights to action and get superior returns from their investments. As a participant of the United Nations Global Compact, Axtria is committed to aligning strategies and operations with universal principles on human rights, labor, environment, and anti-corruption and taking actions that advance societal goals.\nOur people are our core strength, and they make us proud of our work; this has helped us grow exponentially and make tremendous strides toward developing great products. It is for the grit, thinking of an entrepreneur, and a family-like environment where each member is valued and treasured that we are growing rapidly. For more information, visit www.axtria.com.\n\n\nJob Title: - Gen AI Experts ( Open across levels – Senior Associate to Associate Director)\n\nJob Location: -Gurgaon/Bangalore/Pune/Hyderabad\n\nJob Responsibilities: -\nBe an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT\nSoftware development experience in python is needed as backend for UI based applications\nBe a part of large delivery teams working on advanced projects when expert assistance is required.\nDeliver advanced Data Science capabilities to businesses in a meaningful manner through successful proof-of-concept solutions, and later smoothly transition the proof-of-concept into production.\nCreate Technical documents, develop, test, and deploy data analytics processes using Python, SQL on Azure/AWS platforms\nCan interact with client on GenAI related capabilities and use cases\n\nQualification: -\n- B-Tech or BE in Computer Science / Computer Applications from Tier 1-2 college\nOR\n- Master’s degree in Machine Learning / Statistics / Econometrics, or related discipline from Tier 1-2 college\n\nMust have Skills: -\nRequire 3-15 years experience to develop, test, and deploy Python based applications on Azure/AWS platforms\nMust have basic knowledge on concepts of Generative AI / LLMs / GPT\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.\nFamiliarity with frontend technologies like HTML, JavaScript, REACT\nSkills that give you an edge: -\nStrong analytical skills to solve and model complex business requirements are a plus. With life sciences or pharma background.\nWe will provide– (Employee Value Proposition)\nOffer an inclusive environment that encourages diverse perspectives and ideas\nDeliver challenging and unique opportunities to contribute to the success of a transforming organization\nOpportunity to work on technical challenges that may impact across geographies\nVast opportunities for self-development: online Axtria Institute, knowledge sharing opportunities globally, learning opportunities through external certifications\nSponsored Tech Talks & Hackathons\nPossibility to relocate to any Axtria office for short and long-term projects\nBenefit package:\n-Health benefits\n-Retirement benefits\n-Paid time off\n-Flexible Benefits\n-Hybrid /FT Office/Remote\nAxtria is an equal-opportunity employer that values diversity and inclusiveness in the workplace.\nWho we are\nAxtria 14 years journey\nAxtria, Great Place to Work\nLife at Axtria\nAxtria Diversity",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'Model Building', 'Python', 'SQL', 'Modeling Tools']",2025-06-12 15:13:20
AI/ML Engineer/Architect -,Choice Consultants,6 - 10 years,20-35 Lacs P.A.,"['New Delhi', 'Bengaluru']","Automotive, Business Relationship Management, Collaborative Leadership, Communication, Computer Vision, AI, Machine Learning, Team Leadership, Technological Innovation, Proposal",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['C/C++', 'AI/ML', 'PYTHON', 'COMPUTER VISION', 'AIML', 'Medical Imaging', 'DEEP LEARING']",2025-06-12 15:13:23
Principal Engineer (Python),Hughes Systique,4 - 7 years,Not Disclosed,['Gurugram'],"Job Overview\nWe are looking for a high end Principal Engineer who will play a role of technical lead along with hands-on contributor. This role demands someone with deep expertise in Python and its frameworks, strong exposure to cloud-native development (AWS, Kubernetes), and a proven track record in driving engineering excellence across multiple domains such as backend services, infrastructure, and DevOps practices.\nYou would be responsible for designing and building scalable backend services and APIs using Python frameworks, integrating relational databases, and deploying in containerized environments on prem and cloud.\nKey Responsibilities\n. Lead technical architecture and design for scalable, resilient, and secure systems.\n\n. Design and develop RESTful backend APIs using FastAPI and Flask\n.Build server-side logic and business functionalities using Python\n.Design and integrate MySQL and PostgreSQL databases\n.Deploy and manage applications in Docker/Kubernetes environments\n.Maintain CI/CD pipelines using Git and Jenkins\n.Collaborate with DevOps and frontend teams for integration\n.Ensure code quality through peer reviews and documentation\nTechnical Stack\n.Python (FastAPI, Flask)\n.RESTful API\n.MySQL, PostgreSQL\n.Docker, Kubernetes\n.Git, Jenkins\n.Linux-based development environment\n. Strong experience working with AWS services (e.g., EC2, Lambda, EKS, S3, RDS, CloudWatch, DynamoDB).\n\nNice to Have:\n-Experience with event-driven or microservices architecture.\n-Exposure to serverless computing and IaC tools like Terraform or AWS CDK.\n-Familiarity with security and compliance in cloud-native applications.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['amazon ec2', 'Backend', 'GIT', 'Linux', 'Postgresql', 'MySQL', 'Cloud', 'jenkins', 'AWS', 'Python']",2025-06-12 15:13:25
Lead AI Engineer,Insnapsys Technologies,7 - 10 years,8.4-18 Lacs P.A.,['Nashik'],"We're hiring a Lead AI Engineer (Remote/Nashik) to build & lead AI/ML & LLM solutions. 7+ yrs exp, strong in Python, LLMs, AWS, MLOps, vector DBs. Bonus: voice AI, agents, open-source.\n\nTeam Leading Experience is Mandatory\n\nApply: hr@insnapsys.com.\n\n\nWork from home\nHouse rent allowance",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Machine Learning', 'Data Science', 'English', 'Marathi', 'Team Leading', 'Hindi', 'Deep Learning', 'Ml', 'Python']",2025-06-12 15:13:27
Python/Pyspark developer,Zensar,4 - 5 years,Not Disclosed,"['Pune', 'Bengaluru']","Job Description:\nWe are seeking a highly skilled and motivated Python/PySpark Developer to join our growing team. In this role, you will be responsible for designing, developing, and maintaining high-performance data processing pipelines using Python and the PySpark framework. You will work closely with data engineers, data scientists, and other stakeholders to deliver impactful data-driven solutions.\nResponsibilities:\n- Design, develop, and implement scalable and efficient data pipelines using PySpark.\n- Write clean, well-documented, and maintainable Python code.\n- Optimize data processing performance and resource utilization.\n- Implement ETL (Extract, Transform, Load) processes to migrate and transform data across various systems.\n- Collaborate with data scientists and analysts to understand data requirements and translate them into technical solutions.\n- Troubleshoot and debug data processing issues.\n- Stay up-to-date with the latest advancements in big data technologies and best practices.\nQualifications:\n- Bachelor's degree in Computer Science, Engineering, or a related field.\n- 3+ years of experience in Python development.\n- 2+ years of experience with PySpark and Spark ecosystem.\n- Strong understanding of data structures, algorithms, and object-oriented programming.\n- Experience with SQL and relational databases.\n- Familiarity with cloud platforms such as AWS, Azure, or GCP (preferred).\n- Excellent problem-solving and analytical skills.\n- Strong communication and teamwork skills.\nBonus Points:\n- Experience with data visualization tools (e.g., Tableau, Power BI).\n- Knowledge of machine learning and data science concepts.\n- Experience with containerization technologies (e.g., Docker, Kubernetes).\n- Contributions to open-source projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Cloud Technologies', 'SQL', 'Python']",2025-06-12 15:13:29
Principal Engineer - IT Quality & Assurance Testing,Oliver Wyman,4 - 9 years,Not Disclosed,['Mumbai'],"Company: Marsh\nDescription:\nCompany:\nMarsh McLennan\n\nDescription:\nMarsh McLennan is seeking candidates for the following position based in the India Mumbai/Pune Office\nSenior Principal Engineer - IT Quality Assurance\n\nWhat can you expect?\nMarsh McLennan offers an amazing opportunity for distinguished digital technologists to transform the company s digital capabilities while unleashing the power of our industry leading platforms and data assets to deliver innovative products and disruptive business models globally.\nAt Marsh McLennan, we are all about developing and integrating innovative digital products for our clients. This group is nimble, creative, and empowered to shape the Marsh s digital landscape.\n\nWhat is in it for you?\nIf you are looking for an opportunity to be part of an exciting team of entrepreneurs with the mission to disrupt and deliver the art of the possible in the insurance industry, look no further!\nCompetitive Benefits\nCareer Development Opportunities\n\nIn this role you will be responsible for:\nEnvision and develop a library of frontend and backend test automation frameworks to a best-in-class Marsh digital platform\nBe a change agent focusing on quality assurance, and exhibit proficiency in manual and automation testing\nHave an ability to explore, debug and investigate issues\nBelieve in bringing efficiencies through automation using new technologies, testing and automation tools\nDrive culture change in technology to become a truly Agile team which is self-organizing, DevOps and believe in everything automated\nCollaborate with Marsh technology teams on new / existing test development opportunities\nContinuously learn about new technologies and help keep the entire group abreast of industry developments and evolving best practices in quality assurance and test automation\n\nWe would like you to have:\nBachelor s Degree in Computer Science or related\n4+ years of experience with a proven track record of successfully delivering global and highly scalable customer-facing digital products and capabilities\nLeadership in thinking and ideating new and innovative solutions for QA\nExpertise in Web application Testing\nHands on API testing\nHands on with Playwright or Selenium\nHands on with agile project management tools - ADO\nHands on with SQL and RDBMS like ORacle/MSSql\nExposure to any JavaScript based testing frameworks\nExposure to public cloud technology stack in AWS, and Azure\nStrong analytical skills to be able to analyze complex problems using a number of problem solving techniques.\nExperience in delivering complex software quality assurance in an Agile environment\nProactively partner with business and technology organizations across system and organizational boundaries to identify opportunities and drive automated solutions",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Backend', 'Reinsurance', 'RDBMS', 'Javascript', 'Selenium', 'Oracle', 'Risk management', 'Software quality assurance', 'SQL']",2025-06-12 15:13:31
Snowflake - Senior Technical Lead,Sopra Steria,2 - 11 years,Not Disclosed,['Noida'],"Position: Snowflake - Senior Technical Lead\nExperience: 8-11 years\nLocation: Noida/ Bangalore\nEducation: B.E./ B.Tech./ MCA\nPrimary Skills: Snowflake, Snowpipe, SQL, Data Modelling, DV 2.0, Data Quality, AWS, Snowflake Security\nGood to have Skills: Snowpark, Data Build Tool, Finance Domain\nPreferred Skills",,,,"['Performance tuning', 'Schema', 'HIPAA', 'Javascript', 'Data quality', 'Informatica', 'Analytics', 'SQL', 'Python', 'Auditing']",2025-06-12 15:13:34
STEM / Robotics Trainer,Mindsightz Education,0 - 2 years,1.5-4 Lacs P.A.,"['Tiruppur', 'Coimbatore', 'Erode']","The position is responsible for implementing innovative, challenging, and engaging Science, Technology, Engineering and Math hands-on time tested curriculum; providing STEM training for students and participate in special Robotic events at schools\n\nRequired Candidate profile\nOral and written communication skills\nPassion for teaching\nTraining delivery and presentation skills\nThe deployment of hands-on learning technologies\nWork effectively with partner",Industry Type: Education / Training,Department: Teaching & Training,"Employment Type: Full Time, Permanent","['Training', 'Teaching', 'computer programming', 'Curriculum development', 'SCRATCH', 'STEM Robotics', 'Arduino', 'Artificial intelligence', 'Robotics', 'STEM']",2025-06-12 15:13:36
Azure & AWS Cloud Azure & AWS Cloud,Zensar,5 - 9 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Zensar Technologies is looking for Azure & AWS Cloud Azure & AWS Cloud to join our dynamic team and embark on a rewarding career journey\n\nManages deployment and operations across Azure and AWS cloud platforms\n\nEnsures high availability, security, and scalability of cloud services\n\nImplements automation for infrastructure provisioning and CI/CD pipelines\n\nMonitors performance and optimizes cost across hybrid cloud environments",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['kubernetes', 'continuous integration', 'python', 'iso', 'microsoft azure', 'networking', 'artificial intelligence', 'docker', 'ansible', 'iot', 'aws cloud', 'gcp', 'awsazure', 'devops', 'linux', 'jenkins', 'shell scripting', 'agile', 'aws', 'cloud computing', 'architecture']",2025-06-12 15:13:39
Wordpress Intern,Swadhin It Solutions,0 - 3 years,Not Disclosed,['Bhubaneswar'],"Swadhin IT Solutions is looking for Wordpress Intern to join our dynamic team and embark on a rewarding career journey\nDesign, develop, and maintain WordPress websites.\nCustomize themes and plugins to meet client requirements.\nOptimize websites for performance, security, and SEO.\nConduct testing and debugging to ensure website functionality.\nStay current with emerging technologies and best practices in WordPress development.\nProvide technical support and troubleshooting for WordPress websites.\nWrite and maintain technical documentation for website features and codebase.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['digital marketing', 'matlab', 'python', 'css', 'c++', 'software development', 'software testing', 'wordpress', 'machine learning', 'javascript', 'jquery', 'java', 'embedded c', 'linux', 'social media marketing', 'debugging', 'php', 'html', 'mysql', 'data structures', 'seo', 'communication skills']",2025-06-12 15:13:41
Gen AI Specialist,Lumen Technologies,7 - 12 years,Not Disclosed,['Bengaluru'],"About Lumen Technologies\nLumen Technologies is a global technology company that delivers innovative communication and network solutions. Our mission is to empower businesses and individuals to connect, grow, and thrive in the digital age. With a focus on customer experience and operational excellence, we strive to provide cutting-edge solutions that meet the evolving needs of our customers.\nThe Main Responsibilities\nDevelopment of Gen AI applications and conduct reviews.\nWrite clean and maintainable code, improve the system architecture\nParticipate in meetings and conferences to gather requirements, discuss architectural decisions, and collaborate on innovative solutions.\nDevelop frameworks, libraries, and tools to expedite GenAI application development.\nExplore new technologies in the Gen AI space like MCP and agent protocols, and drive adoption of these advancements to enhance our AI capabilities.\nRequired Qualifications:\nBachelor s degree in computer science or related field or the equivalent in training and experience\n7+ years of professional experience in Java/Python development\nWell-versed with development using frameworks like Spring / Spring Boot / FastAPI / Django.\nWell-versed with REST API development and security standards (OAuth2.0, OIDC etc),\nExperience using any SQL database (Oracle/PostgreSQL, etc.) and any NoSQL databases -MongoDB/Couchbase.\nExperience on GenAI Development using Python, Langchain, Vector Datastores(Azure, Pinceone etc.), Large language models like Llama, Open AI etc.\nWell-versed with prompt engineering techniques and using frameworks like Semantic Kernel, LangGraph etc.\nExperience using a container ecosystem - Kubernetes, Docker.\nExperience working on developing and deploying applications on any cloud platform (AWS / GCP / Azure)\nExperience using CI/CD pipelines for application development.\nWe are an equal opportunity employer committed to fair and ethical hiring practices. We do not charge any fees or accept any form of payment from candidates at any stage of the recruitment process. If anyone claims to offer employment opportunities in our company in exchange for money or any other benefit, please treat it as fraudulent and report it immediately.\n\n#LI-MP1",Industry Type: Telecom / ISP,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'System architecture', 'NoSQL', 'Claims', 'Architecture', 'Postgresql', 'Django', 'Application development', 'MongoDB', 'Python']",2025-06-12 15:13:43
Hiring FCT Mentor( Education Loan Team Leader)-Shiksha.com,Info Edge,3 - 7 years,Not Disclosed,['Noida'],"About Info Edge\nInfoEdges mission is to create world-class platforms that transform lives by continuously innovating. Our products and services are built keeping our customers in mind. We always delight our customers by delivering superior value through enhanced offerings on the internet and other platforms. Through our continuous investment across various businesses, especially in cutting-edge technology, machine learning and artificial intelligence (AI), we have built a robust system that constantly increases our predictive powers on customer behaviour, and optimizes and improves our systems. Our various teams tirelessly work together to solve problems, innovate, and create something to empower our customers.",,,,"['Education Loan', 'Client Management', 'Team Handling', 'sales orientation', 'account manager', 'overseas education', 'study abroad', 'loan counsellor']",2025-06-12 15:13:46
Walkin Drive II Corporate Sales B2B II 99acres_Bangalore,Info Edge,1 - 5 years,Not Disclosed,['Bengaluru'],"Walkin Drive on 13th and 14th June\nRole: Corporate Sales (B2B)\nExperience: 1- 5 years\nSkill: B2B Sales, New client acquisition\n\nAbout Info Edge:\nInfoEdges mission is to create world-class platforms that transform lives by continuously innovating. Our products and services are built keeping our customers in mind. We always delight our customers by delivering superior value through enhanced offerings on the internet and other platforms. Through our continuous investment across various businesses, especially in cutting-edge technology, machine learning and artificial intelligence (AI), we have built a robust system that constantly increases our predictive powers on customer behaviour, and optimizes and improves our systems. Our various teams tirelessly work together to solve problems, innovate, and create something to empower our customers.",,,,"['B2B Sales', 'Client Acquisition', 'Field Sales', 'Key Account Management', 'Business Development', 'Fresher Hiring']",2025-06-12 15:13:49
Principal Site Reliability Engineer,Swimlane,10 - 15 years,Not Disclosed,['Hyderabad'],"Do you have an interest and desire to work with cutting edge technologies to solve challenging problems? Do you love cyber-security? Would you like to help build a platform that helps security teams process millions of security alerts every day? Are you interested in a role where you can use the latest JavaScript technologies and frameworks, and contribute to open source?\n\nAs the most senior technical individual contributor within an entire division of Engineering at Swimlane, you will be deeply involved with and guide the reliability, availability, security, quality, and extensibility of our offerings. You must have an extreme ownership mentality. You will work very closely with other senior Engineering, Product, and Support resources to quickly advance the number and state of our offerings. You will create, test, and operate new services, as well as enhance existing ones.\n\nJob Requirements:\n8+ years experience developing in at least one common, general languages (i.e., C, C#, Java, Python, etc)\n8+ years experience as a senior or principal engineer\n15+ years experience as an engineer managing mission-critical services at scale\n6+ years experience with Amazon Web Services (AWS), Microsoft Azure, and/or Google Compute Platform (GCP)\nDeep, demonstrable experience with various Cloud-native monitoring, logging, and dashboarding platforms (e.g., New Relic, Datadog, Prometheus, etc)\nExcellent understanding of and ability to work with Hashicorp Terraform\nStrong understanding of modern continuous integration/continuous deployment (CI/CD) platforms (e.g., GitHub Actions, GitLab pipelines, AWS CodeBuild / Codedeploy / Codepipeline , etc)\nSubstantial experience with managing Kubernetes resources\nExcellent written and verbal English communication skills\nStrong ability to work in a fast-paced environment that does also have security and compliance requirements\nMust be available for on-call escalations\nMust be able to mentor resources of different levels\nAbility to manage up, down, and across the organization\nAutomate, automate, automate!\n\nDont meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Swimlane, we are dedicated to building a diverse, inclusive and authentic workplace, so if you are excited about this role but your past experience doesnot align perfectly with every qualification in the job description, we encourage you to apply anyway! You may just be the right candidate for this or other roles!\n\nWho we are, and what we offer:\nSwimlane is a rapidly growing, innovative startup that provides cloud-scale, low-code security automation for organizations of all industries and sizes. Our technology is relied upon by major security-forward companies around the globe and we are consistently rated as the #1 trusted low-code security automation platform. Our mission is to prevent breaches and enable continuous compliance via a low-code security automation platform that serves as the system of record for the entire security organization.\n\nWhat s the best thing about working at Swimlane? If you ask the team, they will tell you its the people. Swimlaners are innovative, collaborative and driven by the purpose of revolutionizing the way security teams automate and respond to alerts. Headquartered in beautiful Louisville,Colorado, directly between Denver and Boulder, Swimlanes staff spans 28 states and 16 countries!\n\nThe Perks of being a Swimlaner\nCompetitive Benefits & Compensation\nTraining & Professional Development Opportunities\nMacbook Pro\nGreat Company Culture\nWe value collaboration and innovation\nGive-back Volunteering Opportunities\n\nHere at Swimlane, our core focus is to Automate the World of Security and we strive to represent our five core values in everything we do:\nPunch above your weight class - We make the most of our circumstances and constantly surprise and impress with our ability to deliver.\nBe a happy innovator - The hard problems are the fun problems to solve, we re excited to take on difficult challenges and find creative solutions.\nAlways be leveling up - We are continuously improving, embracing change, and consuming information to better ourselves and each other.\nMove at the speed of WOW - We work with an extreme sense of urgency, but we never compromise quality.\nHave honesty and integrity in all the things - We make decisions with the best of intentions, doing what is right for as many stakeholders as possible.",Industry Type: Hardware & Networking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'github', 'cyber security', 'Compliance', 'GCP', 'Cloud', 'Javascript', 'Open source', 'Monitoring', 'Python']",2025-06-12 15:13:51
Big Data Developer,Techstar Group,7 - 10 years,Not Disclosed,['Hyderabad'],"Responsibilities of the Candidate :\n\n- Be responsible for the design and development of big data solutions. Partner with domain experts, product managers, analysts, and data scientists to develop Big Data pipelines in Hadoop\n\n- Be responsible for moving all legacy workloads to a cloud platform\n\n- Work with data scientists to build Client pipelines using heterogeneous sources and provide engineering services for data PySpark science applications\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- Define needs around maintainability, testability, performance, security, quality, and usability for the data platform\n\n- Drive implementation, consistent patterns, reusable components, and coding standards for data engineering processes\n\n- Convert SAS-based pipelines into languages like PySpark, and Scala to execute on Hadoop and non-Hadoop ecosystems\n\n- Tune Big data applications on Hadoop and non-Hadoop platforms for optimal performance\n\n- Apply an in-depth understanding of how data analytics collectively integrate within the sub-function as well as coordinate and contribute to the objectives of the entire function.\n\n- Produce a detailed analysis of issues where the best course of action is not evident from the information available, but actions must be recommended/taken.\n\n- Assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients, and assets, by driving compliance with applicable laws, rules, and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct, and business practices, and escalating, managing and reporting control issues with transparency\n\nRequirements :\n\n- 6+ years of total IT experience\n\n- 3+ years of experience with Hadoop (Cloudera)/big data technologies\n\n- Knowledge of the Hadoop ecosystem and Big Data technologies Hands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Pig, Impala, Spark, Kafka, Kudu, Solr)\n\n- Experience in designing and developing Data Pipelines for Data Ingestion or Transformation using Java Scala or Python.\n\n- Experience with Spark programming (Pyspark, Scala, or Java)\n\n- Hands-on experience with Python/Pyspark/Scala and basic libraries for machine learning is required.\n\n- Proficient in programming in Java or Python with prior Apache Beam/Spark experience a plus.\n\n- Hand on experience in CI/CD, Scheduling and Scripting\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- System level understanding - Data structures, algorithms, distributed storage & compute\n\n- Can-do attitude on solving complex business problems, good interpersonal and teamwork skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Hive', 'Data Engineering', 'Data Pipeline', 'PySpark', 'Hadoop', 'Kafka', 'HDFS', 'Spark', 'Python']",2025-06-12 15:13:54
"Sr. QA Engineer, AI",Conga,5 - 8 years,Not Disclosed,"['Pune', 'Ahmedabad', 'Bengaluru']","Job Title: Sr. QA Engineer\nLocations: Ahmedabad/ Bangalore/ Pune\nReports to: Manager, Quality Engineering\n\nA quick snapshot\n\nAs Senior QA Engineer your responsibilities will include designing and implementing tests, debugging, and defining corrective actions. You will also review system requirements and track quality assurance metrics. These tests entail other tasks such as developing and running new tests and reporting their results to stakeholders, who will collaborate to fix program bugs or problems. You will mentor juniors, collect daily updates, and circulate to managers/ higher forums making this role more important in the system.\n\nWhy its a big deal\n\nA Senior QA Engineer role has significance in the Testing Center of Excellence (TCoE) team at Conga, managing the production of test documents, the creation of test procedures, and ensuring high-quality products. Your expertise in agile methodology, and automation tools, will help in accelerating a continuous enhancement of our product features is a truly Big Deal in Conga Way. Your extensive contribution to scrum teams in the implementation of automation footprints with a Sprint/Release will bring a high-quality impact on Congas products. Your collaboration with cross-functional teams ensures the smooth running of the QA department and ultimately customer satisfaction.\n\nAre you the person were looking for?\n\nProven success in testing (Automation and Manual).Your experiences will include at least 5 years in test case planning, assessments, script development, and maintenance. You have hands-on experience with automation tools and frameworks and developing automation scripts.\n\nSelenium and API. You have expertise with automation tools such as Selenium web driver, frameworks, and developing automation scripts using Java. Strong hands-on experience with API approach using Rest Assured or any such client. Hands-on with test management software such as qTest, JIRA, Jmeter, Load Runner.\n\nAI Technology. You have experience in Large Language model, machine learning experience, AI Git knowledge for Advance Automation as well as familiar with AI Microsoft CoPilot. Candidate should be aware with attorney use cases for variety of documents\n\nAgile Methodology. You are proficient with Agile and a collaborative cross-functional approach to building awesome software. You are comfortable working with teams and collaborating on best practices across multiple Agile teams. You constantly seek opinions and solicit feedback to create the best work possible. You dont know any other way. Its a team effort and you completely appreciate that. Strong experience in software testing lifecycle (STLC) and knowledge of software development lifecycle (SDLC).\n\nEducation. A bachelors degree in engineering or equivalent.\n\nHere’s what will give you an edge\n\nStrong attention to detail. The Conga revenue lifecycle management solution showcases a wide variety of use cases, across multiple regions and languages. As a senior QA paying attention to the smallest details can help identify bugs that others might miss.\n\nStrong testing skills and logic based thinking is your forte. This is an absolute must. Your proven ability to analyze and apply logical thinking to determine the root cause of an issue is fundamental to success in this role. You can easily understand how systems interact/integrate with each other and as well as how changes in one application will affect others.\n\nInitiative. As a Senior QA, we need to own and initiate multiple things to make the quality better. Functional aspects, Non-functional aspects, Broader thinking, Integration approach, Reuse approach in Automation, Performance, Security, Database testing, and a lot more.\n\nAwareness. This role should be aware of the company vision, Goals, and Requirements, and work towards that direction to deliver quality so participation in multiple forums makes it more vital.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['copilot', 'Rest Assured', 'Playwright', 'Selenium With Java']",2025-06-12 15:13:56
Social Media Intern,The Peak View Stories,0 - 3 years,Not Disclosed,[],"The Peak View Stories is looking for Social Media Intern to join our dynamic team and embark on a rewarding career journey\nContent Creation: Assist in creating and curating content for various social media platforms, including text posts, images, videos, and infographics\nEnsure that content aligns with the brand's voice and messaging\nScheduling and Posting: Schedule and publish social media posts on platforms like Facebook, Instagram, Twitter, LinkedIn, and others\nUse social media management tools to plan content calendars\nAudience Engagement: Monitor social media channels for comments, messages, mentions, and direct interactions from followers\nEngage with the audience by responding to inquiries and comments\nAnalytics and Reporting: Track the performance of social media campaigns and posts using analytics tools\nProvide insights and data on key performance metrics, such as reach, engagement, and conversion rates\nTrend Analysis: Stay updated on social media trends, hashtags, and discussions related to the industry and brand\nUse this information to inform content creation and engagement strategies\nCompetitor Research: Research and analyze the social media presence of competitors to identify opportunities for improvement and differentiation\nHashtag Research: Identify relevant hashtags to use in posts to increase discoverability and engagement\nCreate branded hashtags when appropriate",Industry Type: Miscellaneous,Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['digital marketing', 'infographics', 'instagram', 'linkedin', 'media', 'machine learning', 'research', 'content creation', 'excel', 'marketing', 'twitter', 'online marketing', 'campaigns', 'performance metrics', 'media management', 'quality standards', 'social media marketing', 'trend analysis']",2025-06-12 15:13:59
Principal Engineer- Project Purchasing,Burns and Mc Donnells Engineering India,7 - 10 years,Not Disclosed,['Mumbai'],"About us:\nBurns & McDonnell  is a leading player in the Engineering, Procurement, and Construction (EPC) industry, delivering innovative solutions to clients  across multiple industries like Chemicals,Oil & Gas, Transmission & Distribution, Power among other verticals.We are proud to be part of a global network with our US parent company. With a track record of successful projects across various industries, we are committed to innovation, sustainability, and client satisfaction. As we continue to grow, we are seeking an experienced Purchasing  Engineers to join our team.\nPosition Overview :\nAs a Principal Purchasing Engineer at Burns & McDonnell, your role will be pivotal in the successful execution of work share projects between our consultancy firm and our US parent company.You will support US procurement manager and Buyers Purchasing on project to ensure seamless integration between the two entities, adhering to the highest standards of efficiency and quality.\nKey Responsibilities:\n1.Provide procurement support to Operations in relation to purchasing Project Procurement items like Pressure vessels, Rotary Items, E& I  items, etc. in a timely manner as assigned with supervision and support from the Business Support Manager.\n2.Responsible for RFP compilation ,Quality review of  RFP with Engineering, Floating Enquiry, Bid evaluation, Bid Tabulation, Purchase Recommendation and post order PO management.\n3.Coordiation with International supplier for bid clarification and with US counterpart to update the status of purchasing\n4.Receive and Check Supplier Invoice, Tag to proper Project , Process through Oracle OnBase application for further Projects approval and  for final processing by Finance. Complete Tracking to be followed until Invoice is processed and release to supplier.\n5.Coordinate with Procurement leadership team & project Management team  to provide Monthly status on Purchasing.\nKey Technical Deliverables:\n1.Approved Manufacturer list\n2.Prepare Request For Quotations (RFQs) and evaluate responses\n3.Negotiate with suppliers on all matters relating to terms and conditions, improved pricing of quotes received and delivery options that may be more economic and timely.\n4.Tabulate Commercial Bid Evaluation\n5.Issue Purchase Recommendation\n4.Coordinate on contractual, commercial, taxation, insurance, and legal issues with relevant internal stakeholders.\n6.Raise/Revise Purchase Orders (POs) and resolve queries as require.\n7.Prepare & Maintain Purchase reports.\n-\n1.11 to 12  years  Procurement experience, preferably in an  EPC environment in Oil and Gas, Transmission & Distribution industry.\n2. Good understanding of PO contact terms and condition, logistics, Supplier Qualification.\n3. Experience in contract formulation activities, systems and processes\n4. Good communication skills, both oral and written.\n5. Computer Knowledge and operating skills on MS office.",Industry Type: Engineering & Construction,Department: Procurement & Supply Chain,"Employment Type: Full Time, Permanent","['execution', 'project management', 'engineering purchase', 'purchase', 'purchase order', 'engineering', 'environment', 'purchase management', 'operations', 'vendor development', 'procurement', 'project procurement', 'writing', 'epc', 'construction', 'communication skills', 'ms office', 'project purchase']",2025-06-12 15:14:01
Associate ML Scientist - II,Wadhwani Ai,3 - 6 years,Not Disclosed,['Mumbai'],"SUMMARY\nAn Associate Machine Learning (ML) Scientist at Wadhwani AI will build scientifically rigorous and robustly evaluated AI solutions that will be deployed in order to bring AI to the benefit of underserved billions across the developing world.\nABOUT US - https://www.wadhwaniai.org/\nWadhwani AI is a nonprofit institute building and deploying applied AI solutions to solve critical issues in public health, agriculture, education, and urban development in underserved communities in the global south. We collaborate with governments, social sector organizations, academic and research institutions, and domain experts to identify real-world problems, and develop practical AI solutions to tackle these issues with the aim of making a substantial positive impact.\nWe have over 30+ AI projects supported by leading philanthropies such as Bill & Melinda Gates Foundation, USAID and Google.org. With a team of over 200 professionals, our expertise encompasses AI/ML research and innovation, software engineering, domain knowledge, design and user research.\nIn the Press:\nOur Founder Donors are among the Top 100 AI Influencers\nG20 India s Presidency: AI Healthcare, Agriculture, & Education Solutions Showcased Globally.\nUnlocking the potentials of AI in Public Health\nWadhwani AI Takes an Impact-First Approach to Applying Artificial Intelligence - data.org\nWinner of the H&M Foundation Global Change Award 2022\nIndian Winners of the 2019 Google AI Impact Challenge, and the first in the Asia Pacific to host Google Fellows\nROLES AND RESPONSIBILITIES\nBuild robust machine learning solutions that address problems of societal importance, under the guidance of senior ML scientists.\nAssist in the design and evaluation of automated speech recognition (ASR) solutions, both internal as well as submitted for funding to the India AI mission at the Ministry of Electronics and Information Technology (MEITy).\nTranslate challenges in the social sector into well-defined AI problems.\nDevelop and execute algorithms and models to solve the identified problems effectively.\nContribute to the successful and scalable deployment of AI solutions in real-world settings.\nDefine and apply appropriate evaluation metrics to assess the impact and effectiveness of deployed solutions.\nUnderstand user challenges and contextual factors that influence the solution s performance and relevance.\nCurate, clean, and transform datasets for use in training and validating ML models.\nConduct model training, validation, simulations, and extract meaningful insights from data.\nCollaborate with cross-functional teams including engineers, solution leads, and domain experts to develop holistic AI solutions.\nInterface with social sector organizations and stakeholders to ensure the solution meets on-ground needs.\nREQUIREMENTS\nAssociate ML scientists will have a strong academic background in a quantitative field such as B.Tech. / B.E. / B.S. / M.Tech. / M.E. / M.S. / M.Sc. or equivalent in Computer Science, Electrical Engineering, Statistics, Applied Mathematics, Physics, Economics or a relevant quantitative field with a work experience of 3-6 years.\nCandidates should preferably have experience with ASR methods.\nCandidates should have excellent communication skills and a willingness to adapt to the challenges of doing applied work for social good.\nSolid software engineering skills across one or multiple languages including Python, C++, Java.\nInterest in applying software engineering practices to ML projects.\nTrack record of project work in applied machine learning. Experience in applying AI models to concrete real-world problems is a plus.\nStrong verbal and written communication skills in English.",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'ASR', 'Electrical engineering', 'C++', 'Artificial Intelligence', 'Machine learning', 'Healthcare', 'Information technology', 'Public health', 'Python']",2025-06-12 15:14:04
Sr. Data Analyst,Icims,4 - 9 years,Not Disclosed,['Hyderabad'],"Overview\nThe Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.",,,,"['server', 'data', 'vlookup', 'market data', 'data mapping', 'dashboards', 'research', 'sql', 'analytics', 'tables', 'prep', 'pivot', 'data visualization', 'communication skills', 'python', 'data analytics', 'data analysis', 'insights', 'pivot table', 'data engineering', 'graph', 'excel', 'data quality', 'tableau', 'data governance', 'root cause']",2025-06-12 15:14:07
Product Manager -Data Science CoE,Algoleap Technologies,10 - 15 years,Not Disclosed,['Hyderabad'],"We are establishing a Data Science Center of Excellence (CoE) to drive data-driven innovation and insights across multiple global business units. Were looking for a seasoned Product Manager to lead this initiative from inception to scale. This role will act as the strategic interface between business stakeholders and the data science team to define, prioritize, and deliver impactful data products and solutions.\nKey Responsibilities:\nCollaborate with global business leaders to identify and prioritize high-impact data science opportunities.",,,,"['Product management', 'Usage', 'data science', 'Manager Program Management', 'Machine learning', 'Management', 'Stakeholder management', 'Analytics', 'Monitoring']",2025-06-12 15:14:09
Senior Data Analyst,OnlineSales.ai,2 - 7 years,Not Disclosed,['Pune'],"About OnlineSales.ai\nBuilt by ex-Amazon ad-tech experts, OnlineSales.ai offers a future-proof Retail Media Operating System - boosting Retailer s profitability by 7% of Sales! We are an Enterprise B2B SaaS startup, based out of Pune India. With OnlineSales.ais platform, retailers activate and delight 10x more Brands by offering an omni-channel media buying experience, advanced targeting, analytics & 2x better ROAS. Tier 1 Retailers and Marketplaces globally are accelerating their Monetization strategy with OnlineSales.ai and are innovating ahead of the market by at least 2 years.\n\nAbout the Role\nWe are seeking a talented and motivated individual to join our team as a Senior Data Analyst who will be responsible for extracting insights from complex datasets to drive informed decision-making and enhance business performance. You will collaborate closely with cross-functional teams to identify key metrics, develop data-driven strategies, and provide actionable recommendations. Additional responsibilities may include managing daily regulatory reporting tasks and remediation activities, as well as process improvement.\n\nWhat will you do @OnlineSales?\nData Analysis: Utilize advanced analytical techniques to explore large datasets, identify trends, patterns, and anomalies, and extract actionable insights.\nData Visualization: Create visually compelling dashboards and reports to communicate findings effectively to stakeholders, enabling them to make informed decisions.\nData Extraction: regular extraction of relevant data from internal databases using SQL queries. Design and optimize SQL queries to retrieve specific datasets required for performance analysis and reporting\nIssue Identification: Proactively identify performance-related issues by monitoring key performance indicators (KPIs), analyzing trends, and investigating anomalies reported by internal stakeholders or external clients.\nAddressing Client Exceptions and Issues: Responsively address performance-related exceptions and issues raised by clients, ensuring timely resolution and effective communication throughout the process. Collaborate with client-facing teams to understand client requirements, prioritize tasks, and deliver solutions that meet or exceed client expectations.\nRoot Cause Analysis: Dive deep into data to understand the root causes of performance issues, considering factors such as system architecture, infrastructure, code efficiency, and user behavior.\nHypothesis Testing: Apply hypothesis testing techniques to validate assumptions and identify statistically significant factors impacting performance.\nDocumentation and SOP Creation: Create clear and detailed Standard Operating Procedures (SOPs) outlining the process for diagnosing, troubleshooting, and resolving performance issues. Ensure that documentation is organized, easily accessible, and regularly updated to reflect changes in systems, processes, or configurations.\nCross-Functional Collaboration: Collaborate with teams across the organization, including business development, marketing, product development and operations, to understand their data needs and provide analytical support\n\nYou will be a great fit, if you have :\n2-4 years of relevant experience.\nBachelors or Masters degree in Computer Science, Engineering, or a related technical field.\nProficiency in SQL for data extraction and manipulation from relational databases.\nFamiliarity with programming languages such as Python for Data Analysis and Data modeling is a plus.\nStrong analytical skills with the ability to interpret complex datasets and draw meaningful insights.\nStrong problem-solving abilities with a proactive approach to troubleshooting and issue resolution.\nAdvanced proficiency in Excel and adept data manipulation skills for efficient analysis and visualization of large datasets.\nEffective communication and interpersonal skills for collaboration with cross-functional teams and stakeholders.\nUnderstanding of E-Commerce as a domain.\nExcellent documentation skills with the ability to create clear and comprehensive reports and SOPs.\nAttention to detail and commitment to data accuracy and quality. Willingness to work for a startup.\n\nWhy Online Sales.ai?\nStartup-y . We believe Startup is a mindset. It s about being scrappy, being nimble, solving tough problems with constrained resources, and more. It s about working hard and playing hard\nEnterprise SaaS . Opportunity to work with an Enterprise Product SaaS firm with aspirations of growing 10x across the globe\nAI-led Retail Tech . We are working to digitize & democratize one of the most exciting and growing verticals - Retail Tech leveraging data, machine learning, and automation (culmination of ad-tech, mar-tech, and analytics for Retail vertical)\nMeaningful work . This is not just a job. You can find a job anywhere. This is a place for the bold to get paid who make a real impact on business\nNo red tape . Say goodbye to pointless meetings or political hoops to jump through. We re scrappy, believe in autonomy, and empower our teams to do whatever it takes to do the unthinkable\nProblem Solving . We ignite the best in you. We exist not only to deliver meaningful innovation but to ignite and inspire the creative problem-solver in you\nQuirky & fun . Enjoy new skills and hobbies like being a quiz master, playing board games, trying your hands on percussion, playing Djembe, and spreading love within the org!",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Automation', 'Data modeling', 'Process improvement', 'Online sales', 'Troubleshooting', 'Analytics', 'Monitoring', 'SQL', 'Data extraction']",2025-06-12 15:14:11
Data Specialist - Research,Horizon Therapeutics,9 - 13 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgens data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leverages state-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. You will leverage domain, technical and business process expertise to provide exceptional support of Amgen s data governance framework. This role involves working closely with business stakeholders and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with the Product Owner and other Business Analysts to ensure operational support and excellence from the team.\nRoles Responsibilities:\nResponsible for the data governance and data management framework implementation for the Research domain of the biopharma lifecycle.\nResponsible for the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nDrives cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles.\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. for assigned domains.\nEnsure compliance requirements with data privacy, security, and regulatory policies for the assigned domains\nJointly with Technology teams, business functions, and enterprise teams (e. g. , MDM, Enterprise Data Fabric, etc. ) define the specifications shaping the development and implementation of data foundations .\nBuild strong relationships with key business leads and partners to ensure their needs are being met\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills with knowledge of Pharma processes with specialization in the Research domain of the biopharma lifecycle.\nIn depth knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc.\nIn depth experience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer-focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nIn depth experience of working with or supporting systems used to data governance framework. E. g. Collibra, Alation\nExcellent problem-solving skills and committed attention to detail in finding solutions\nGood-to-Have Functional Skills:\nExperience of working with data governance councils or forums\nExperience with Agile software development methodologies (Scrum)\nProficiency in data analysis and quality tools (e. g. , SQL, Excel, Python, or SAS)\nSoft Skills:\nHighly organized and able to work under minimal supervision\nExcellent analytical and assessment skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAmbitious to further develop their skills and career\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills\nHigh degree of initiative and self-motivation.\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management and customer focus.\nBasic Qualifications:\nAny Degree and 9-13 years of experience\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business process', 'Data analysis', 'Change management', 'operational support', 'SAS', 'Pharma', 'Analytical', 'Agile', 'Data quality', 'SQL']",2025-06-12 15:14:13
Data Architect,Calibo,12 - 16 years,Not Disclosed,[],"About the Role:\n\nWe are looking for a highly skilled Data Engineering Architect with strong Data Engineering pipeline implementation experience to serve as the lead Solution/Technical Architect and Subject Matter Expert for customer experience data solutions across multiple data sources. The ideal candidate will collaborate with the Enterprise Architect and the client IT team to establish and implement strategic initiatives.\n\nResponsibilities and Technical Skills:\n12+ years of relevant experience in designing and Architecting ETL, ELT, Reverse ETL, Data Management or Data Integration, Data Warehouse, Data Lake, and Data Migration.\nMust have expertise in building complex ETL pipelines and large Data Processing, Data Quality and Data security\nExperience in delivering quality work on time with multiple, competing priorities.\nExcellent troubleshooting and problem-solving skills must be able to consistently identify critical elements, variables and alternatives to develop solutions.\nExperience in identifying, analyzing and translating business requirements into conceptual, logical and physical data models in complex, multi-application environments.\nExperience with Agile and Scaled Agile Frameworks.\nExperience in identifying and documenting data integration issues, and challenges such as duplicate data, non-conformed data, and unclean data. Multiple platform development experience.\nStrong experience in performance tuning of ETL processes using Data Platforms\nMust have experience in handling Data formats like Delta Tables, Parquet files, Iceberg etc.\nExperience in Cloud technologies such as AWS/Azure or Google Cloud.\nApache Spark design and development experience using Scala, Java, Python or Data Frames with Resilient Distributed Datasets (RDDs).\nDevelopment experience in databases like Oracle, AWS Redshift, AWS RDS, Postgres Databricks and/or Snowflake.\nHands-on professional work experience with Python is highly desired.\nExperience in Hadoop ecosystem tools for real-time or batch data ingestion.\nStrong communication and teamwork skills to interface with development team members, business analysts, and project management. Excellent analytical skills.\nIdentification of data sources, internal and external, and defining a plan for data management as per business data strategy.\nCollaborating with cross-functional teams for the smooth functioning of the enterprise data system.\nManaging end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nPlanning and execution of big data solutions using Databricks, Big Data, Hadoop, Big Query, Snowflake, MongoDB, DynamoDB, PostgreSQL and SQL Server\nHands-on experience in defining and implementing various Machine Learning models for different business needs.\nIntegrating technical functionality, ensuring data accessibility, accuracy, and security.\nProgramming / Scripting Languages like Python / Java / Go, Microservices\nMachine Learning / AI tools like Scikit-learn / TensorFlow / PyTorch",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud', 'ETL', 'AWS', 'Data Handling', 'Spark']",2025-06-12 15:14:15
Data Science Architect (Full Stack),Hubnex,4 - 9 years,Not Disclosed,['Gurugram'],"Data Science Architect (Full Stack)\nLocation: Gurugram, India (On-site/Hybrid)\nType: Full-Time | 4+ Years Experience | AI, Architecture & Product Engineering\nHubnex Labs is seeking a visionary and hands-on Full Stack Data Science Architect to lead the development of scalable AI products and reusable intellectual property (IP) that power data-driven solutions across global enterprise clients. This role requires deep technical expertise in AI/ML, data architecture, backend/frontend systems, and cloud-native technologies.\nKey Responsibilities AI & Data Science Leadership\nLead design and development of end-to-end AI/ML solutions across enterprise applications\nArchitect data pipelines, model training, validation, and deployment workflows\nApply cutting-edge techniques in NLP, Computer Vision, Speech Recognition, Reinforcement Learning , etc.\nEvaluate and rank algorithms based on business impact, accuracy, and scalability\nDesign and optimize data augmentation, preprocessing, and feature engineering pipelines\nTrain, validate, and fine-tune models using state-of-the-art tools and strategies\nMonitor and improve model performance post-deployment\nFull Stack & Cloud Architecture\nDesign and implement cloud-native systems using microservices , serverless , and event-driven architectures\nBuild robust APIs and UIs for intelligent applications (using Python, Node.js, React, etc.)\nUse Docker , Kubernetes , and CI/CD pipelines for scalable deployment\nLeverage technologies like Kafka, TensorFlow, Elixir, Golang , and NoSQL/Graph DBs for high-performance ML products\nDefine infrastructure to meet latency and throughput goals for ML systems in production\nInnovation & Productization\nBuild reusable IP that can be adapted across industries and clients\nRapidly prototype AI features and user-facing applications for demos and validation\nCollaborate closely with product managers and business stakeholders to translate use cases into scalable tech\nExplore and adopt new technologies and frameworks to maintain a forward-looking tech stack\nRequired Skills & Experience\n4+ years of experience building and deploying AI/ML models and scalable software systems\nStrong understanding of ML frameworks (TensorFlow, Keras, PyTorch), data libraries (pandas, NumPy), and model tuning\nProven track record of working with large-scale data , data cleaning, and visualization\nExpertise in Python , and experience with at least one other language (Go, Java, Scala, etc.)\nExperience with front-end frameworks (React, Vue, or Angular) is a plus\nProficient in DevOps practices , CI/CD, and cloud platforms (AWS/GCP/Azure)\nFamiliarity with event-driven systems , real-time protocols (WebSockets, MQTT), and container orchestration\nHands-on experience with NoSQL databases , data lakes , or distributed data platforms\nPreferred Traits\nExperience leading agile engineering teams and mentoring junior developers\nStrong architectural thinking, with an eye on scalability, maintainability, and performance\nEntrepreneurial mindset with a focus on building reusable components and IP\nExcellent communication skills, capable of bridging business and technical conversations\nWhy Join Hubnex Labs?\nOwn and architect impactful AI products used across industries\nShape the data science foundation of a fast-scaling software consulting powerhouse\nEnjoy a creative, high-performance environment in Gurugram , with flexibility and long-term growth opportunities\nContribute to next-gen solutions in AI, cloud, and digital transformation",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'Backend', 'Product engineering', 'Prototype', 'NoSQL', 'Enterprise applications', 'Consulting', 'Agile', 'Python', 'Data architecture']",2025-06-12 15:14:17
Senior AI Scientist,Vuram,7 - 9 years,Not Disclosed,['Chennai'],"1. Generative & Agentic AI Build and deploy GenAI models for text generation and content automation. Experience on working latest AI stacks like Nvidia- Nemo, NIM Microservices, Unsloth, Pytorch, Tensorflow etc. Develop agentic AI systems with autonomous task planning and decision-making capabilities.\n2. Large Language Models (LLMs) Fine-tune and operationalize LLMs (e.g., GPT, Llama, BERT) for NLP tasks using Nemo, NIM, Unsloth etc frameworks Establish the best practices for LLMOps, including prompt engineering and monitoring.\nDevelop solution based on latest coding standards like Pep-83. Deep Learning and NLP components Experience in developing like QnA, chatbots, Image/Video/Audio processing, OCR based components like Extraction etc. Experience in designing and implementing end-to-end pipelines for Retrieval-Augmented Generation (RAG), including document indexing, retrieval mechanisms Experience in evaluating AI solutions using appropriate metrics\n\n\nQualifications\n1. Bachelors or master s in computer science, Artificial Intelligence, or a related field2.\n7+ years of experience in AI/ML, NLP, Deep Learning, Gen AI, Model fine tuning, Reinforcement learning, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'deep learning', 'Automation', 'NIM', 'Coding', 'Artificial Intelligence', 'Deployment', 'Monitoring', 'microservices']",2025-06-12 15:14:19
Data Science Professional,Algoleap Technologies,6 - 11 years,Not Disclosed,['Hyderabad'],"Job_Description"":""\nJob Title: Data Science CoE\nLocation: Hyderabad, India (Hybrid)\nExperience: 6+ years\nRole Type: Full-time\nStart Date : Immediate\nAbout the Role:\nAs we build our Data Science Center of Excellence (CoE), we are looking for an entrepreneurial and technically strong Data Science Lead who can lay the foundation for a high-performing team. You will work directly with stakeholders across multiple business units to define use cases, lead model development, and ensure successful deployment and value realization.",,,,"['customer analytics', 'Usage', 'data science', 'GCP', 'Machine learning', 'model development', 'Deployment', 'Stakeholder management', 'SQL', 'Python']",2025-06-12 15:14:22
CDnA - Data Science Manager,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will collaborate with business partners, service owners and IS peers to develop predictive models and insights across the US Commercial Organization. This position will innovate and build significant business impact through the use of sophisticated analytics techniques to help Amgen with its mission to serve patients by helping them get the therapies they need.\n\nFlexible Commuter role to Amgen India office. You will work on-site 2-3 days a week.\n\nThis position will be primarily responsible for:\nWorking collaboratively with multi-functional teams on projects and/or programs with aims to systematically derive insights that ultimately derive substantial business value for Amgen and our patients\nIdentifying business needs and proposing potential analytics approaches for solutions\nCrafting and deploying a framework to supervise the performance of various campaigns, and tactics at a granular level\nLeading measurement and tracking of various omnichannel CX enablement initiatives\nSupporting the development of data science, machine learning prototypes, proof of concepts and models for testing various omnichannel strategies\nCommunicating analysis ideas, progress and results to leadership and business partners\n\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nDoctorate degree OR\nMasters degree and 4 to 6 years of data science and/or analytics experience OR\nBachelors degree and 6 to 8 years of data science and/or analytics experience OR\nDiploma and 10 to 12 years of data science and/or analytics experience\nPreferred Qualifications:\nRelevant work experience in campaign measurement, marketing analytics and resource optimization in the pharma domain\nProgramming experience with Python, R, or SAS and experience with ML libraries like scikit-learn, MLib, or TensorFlow\nExperience working with large datasets, experience working with distributed computing tools (Spark, Hive, etc.) is a plus\nAbility to communicate analysis in a clear, detailed, and practical manner\nPassion for learning and staying on top of current developments in sophisticated analytics\nBiotech / Pharma experience",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'hive', 'python', 'tensorflow', 'r', 'scikit-learn', 'spark', 'MLib']",2025-06-12 15:14:25
Data Privacy Specialist,Horizon Therapeutics,3 - 13 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgens data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leverages state-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. You will leverage domain, technical and business process expertise to provide exceptional support of Amgen s data governance framework. This role involves working closely with business stakeholders and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with the Product Owner and other Business Analysts to ensure operational support and excellence from the team.\nRoles Responsibilities:\nResponsible for the data governance and data management framework implementation for a given domain of expertise (Research, Development, Supply Chain, etc. ).\nResponsible for the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nDrives cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles. Create and maintain privacy policies and procedures to protect sensitive data and ensure compliance.\nConduct regular privacy risk assessments and audits to identify and mitigate potential risks as required\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. for assigned domains.\nEnsure compliance requirements with data privacy, security, and regulatory policies for the assigned domains including GDPR, CCPA, and other relevant legislations.\nJointly with Technology teams, business functions, and enterprise teams (e. g. , MDM, Enterprise Data Fabric, etc. ) define the specifications shaping the development and implementation of data foundations .\nBuild strong relationships with key business leads and partners to ensure their needs are being met\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills with knowledge of Pharma processes with specialization in a domain (e. g. , Research, Clinical Trials, Commercial, etc. )\nIn depth knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc. Strong understanding of data protection laws and regulations, including GDPR, CCPA, and other relevant legislations.\nIn depth experience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer-focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nIn depth experience of working with or supporting systems used to data governance framework. E. g. Collibra, Alation\nExcellent problem-solving skills and committed attention to detail in finding solutions\nGood-to-Have Functional Skills:\nExperience of working with data governance councils or forums\nExperience with Agile software development methodologies (Scrum)\nProficiency in data analysis and quality tools (e. g. , SQL, Excel, Python, or SAS)\n3-5 years of experience in data privacy, compliance, or a related field.\nSoft Skills:\nIntegrity: Commitment to maintaining the highest ethical standards and protecting confidential information.\nAdaptability: Ability to adapt to changing regulations and emerging privacy challenges.\nProactivity: Self-motivated with a proactive approach to identifying and addressing privacy issues.\nLeadership: Strong leadership skills and the ability to influence and drive change within the organization.\nHighly organized and able to work under minimal supervision\nExcellent analytical and assessment skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAmbitious to further develop their skills and career\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills\nHigh degree of initiative and self-motivation.\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management and customer focus.\nBasic Qualifications:\nAny Degree and 9-13 years of experience\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Business process', 'Data analysis', 'Change management', 'SAS', 'Pharma', 'Analytical', 'Clinical trials', 'Data quality', 'SQL']",2025-06-12 15:14:27
Data Specialist - Development,Horizon Therapeutics,9 - 13 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgens data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leverages state-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. You will leverage domain, technical and business process expertise to provide exceptional support of Amgen s data governance framework. This role involves working closely with business stakeholders and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with the Product Owner and other Business Analysts to ensure operational support and excellence from the team.\nRoles Responsibilities:\nResponsible for the data governance and data management framework implementation for the Development domain of the biopharma lifecycle.\nResponsible for the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nDrives cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles.\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. for assigned domains.\nEnsure compliance requirements with data privacy, security, and regulatory policies for the assigned domains\nJointly with Technology teams, business functions, and enterprise teams (e. g. , MDM, Enterprise Data Fabric, etc. ) define the specifications shaping the development and implementation of data foundations .\nBuild strong relationships with key business leads and partners to ensure their needs are being met\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills with knowledge of Pharma processes with specialization in the Development domain of the biopharma lifecycle.\nIn depth knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc.\nIn depth experience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer-focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nIn depth experience of working with or supporting systems used to data governance framework. E. g. Collibra, Alation\nExcellent problem-solving skills and committed attention to detail in finding solutions\nGood-to-Have Functional Skills:\nExperience of working with data governance councils or forums\nExperience with Agile software development methodologies (Scrum)\nProficiency in data analysis and quality tools (e. g. , SQL, Excel, Python, or SAS)\nSoft Skills:\nHighly organized and able to work under minimal supervision\nExcellent analytical and assessment skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAmbitious to further develop their skills and career\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills\nHigh degree of initiative and self-motivation.\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management and customer focus.\nBasic Qualifications:\nAny Degree and 9-13 years of experience\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business process', 'Data analysis', 'Change management', 'operational support', 'SAS', 'Pharma', 'Analytical', 'Agile', 'Data quality', 'SQL']",2025-06-12 15:14:30
Technical Consultant(ETL + SQL + Data Migration),Insightsoftware,4 - 7 years,Not Disclosed,['Hyderabad'],"Insightsoftware (ISW) is a growing, dynamic computer software company that helps businesses achieve greater levels of financial intelligence across their organization with our world-class financial reporting solutions. At insightsoftware, you will learn and grow in a fast-paced, supportive environment that will take your career to the next level. The Data Conversion Specialist is a member of the insightsoftware Project Management Office (PMO) who demonstrates teamwork, results orientation, a growth mindset, disciplined execution, and a winning attitude.\nLocation: Hyderabad (Work from Office)\nWorking Hours: 5:00 PM - 2:00AM IST or 6:00 PM to 3:00 AM IS T, should be ok to work in night shift as per requirement.\nPosition Summary\nThe Consultant will integrate and map customer data from client source system(s) to our industry-leading platform. The role will include, but is not limited to:\nUsing strong technical data migration, scripting, and organizational skills to ensure the client data is converted efficiently and accurately to the insightsoftware (ISW) platform.\nPerforming extract, transform, load (ETL) activities to ensure accurate and timely data conversions.\nProviding in-depth research and analysis of complex scenarios to develop innovative solutions to meet customer needs whilst remaining within project governance.\nMapping and maintaining business requirements to the solution design using tools such as requirements traceability matrices (RTM).\nPresenting findings, requirements, and problem statements for ratification by stakeholders and working groups.\nIdentifying and documenting data gaps to allow change impact and downstream impact analysis to be conducted.\nExperience assessing data and analytic requirements to establish mapping rules from source to target systems to meet business objectives.\nExperience with real-time, batch, and ETL for complex data conversions.\nWorking knowledge of extract, transform, load (ETL) methodologies and tools such as Talend, Dell Boomi, etc.\nUtilize data mapping tools to prepare data for data loads based on target system specifications.\nWorking experience using various data applications/systems such as Oracle SQL, Excel, .csv files, etc.\nStrong SQL scripting experience.\nCommunicate with clients and/or ISW Project Manager to scope, develop, test, and implement conversion/integration\nEffectively communicate with ISW Project Managers and customers to keep project on target\nContinually drive improvements in the data migration process.\nCollaborate via phone and email with clients and/or ISW Project Manager throughout the conversion/integration process.\nDemonstrated collaboration and problem-solving skills.\nWorking knowledge of software development lifecycle (SDLC) methodologies including, but not limited to: Agile, Waterfall, and others.\nClear understanding of cloud and application integrations.\nAbility to work independently, prioritize tasks, and manage multiple tasks simultaneously.\nEnsure client s data is converted/integrated accurately and within deadlines established by ISW Project Manager.\nExperience in customer SIT, UAT, migration and go live support.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data migration', 'Data conversion', 'Financial reporting', 'Project management', 'project governance', 'Agile', 'Software development life cycle', 'data mapping', 'SDLC', 'Downstream']",2025-06-12 15:14:32
Data Architect Telecom Domain databrick BSS OSS,fast growing Data Driven IT solutions an...,10 - 20 years,45-55 Lacs P.A.,"['Noida', 'Hyderabad', 'Gurugram']","Data Architect Telecom Domain\nTo design comprehensive data architecture and technical solutions specifically for telecommunications industry challenges, leveraging TMforum frameworks and modern data platforms. To work closely with customers, and technology partners to deliver data solutions that address complex telecommunications business requirements including customer experience management, network optimization, revenue assurance, and digital transformation initiatives.\nResponsibilities:\nDesign and articulate enterprise-scale telecom data architectures incorporating TMforum standards and frameworks, including SID (Shared Information/Data Model), TAM (Telecom Application Map), and eTOM (enhanced Telecom Operations Map)\nDevelop comprehensive data models aligned with TMforum guidelines for telecommunications domains such as Customer, Product, Service, Resource, and Partner management\nCreate data architectures that support telecom-specific use cases including customer journey analytics, network performance optimization, fraud detection, and revenue assurance\nDesign solutions leveraging Microsoft Azure and Databricks for telecom data processing and analytics\nConduct technical discovery sessions with telecom clients to understand their OSS/BSS architecture, network analytics needs, customer experience requirements, and digital transformation objectives\nDesign and deliver proof of concepts (POCs) and technical demonstrations showcasing modern data platforms solving real-world telecommunications challenges\nCreate comprehensive architectural diagrams and implementation roadmaps for telecom data ecosystems spanning cloud, on-premises, and hybrid environments\nEvaluate and recommend appropriate big data technologies, cloud platforms, and processing frameworks based on telecom-specific requirements and regulatory compliance needs.\nDesign data governance frameworks compliant with telecom industry standards and regulatory requirements (GDPR, data localization, etc.)\nStay current with the latest advancements in data technologies including cloud services, data processing frameworks, and AI/ML capabilities\nContribute to the development of best practices, reference architectures, and reusable solution components for accelerating proposal development\nQualifications:\nBachelor's or Master's degree in Computer Science, Telecommunications Engineering, Data Science, or a related technical field\n10+ years of experience in data architecture, data engineering, or solution architecture roles with at least 5 years in telecommunications industry\nDeep knowledge of TMforum frameworks including SID (Shared Information/Data Model), eTOM, TAM, and their practical implementation in telecom data architectures\nDemonstrated ability to estimate project efforts, resource requirements, and implementation timelines for complex telecom data initiatives\nHands-on experience building data models and platforms aligned with TMforum standards and telecommunications business processes\nStrong understanding of telecom OSS/BSS systems, network management, customer experience management, and revenue management domains\nHands-on experience with data platforms including Databricks, and Microsoft Azure in telecommunications contexts\nExperience with modern data processing frameworks such as Apache Kafka, Spark and Airflow for real-time telecom data streaming\nProficiency in Azure cloud platform and its respective data services with an understanding of telecom-specific deployment requirements\nKnowledge of system monitoring and observability tools for telecommunications data infrastructure\nExperience implementing automated testing frameworks for telecom data platforms and pipelines\nFamiliarity with telecom data integration patterns, ETL/ELT processes, and data governance practices specific to telecommunications\nExperience designing and implementing data lakes, data warehouses, and machine learning pipelines for telecom use cases\nProficiency in programming languages commonly used in data processing (Python, Scala, SQL) with telecom domain applications\nUnderstanding of telecommunications regulatory requirements and data privacy compliance (GDPR, local data protection laws)\nExcellent communication and presentation skills with ability to explain complex technical concepts to telecom stakeholders\nStrong problem-solving skills and ability to think creatively to address telecommunications industry challenges\nGood to have TMforum certifications or telecommunications industry certifications\nRelevant data platform certifications such as Databricks, Azure Data Engineer are a plus\nWillingness to travel as required\nif you will all or most of the criteria contact bdm@intellisearchonline.net M 9341626895",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Telecom Bss', 'Data Architect', 'Telecom OSS', 'ETOM', 'Data Bricks']",2025-06-12 15:14:34
Data Specialist - G&A,Horizon Therapeutics,9 - 13 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgens data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leverages state-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. You will leverage domain, technical and business process expertise to provide exceptional support of Amgen s data governance framework. This role involves working closely with business stakeholders and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with the Product Owner and other Business Analysts to ensure operational support and excellence from the team.\nRoles Responsibilities:\nResponsible for the data governance and data management framework implementation for the General and Administrative operations (GA) domain of the biopharma lifecycle.\nResponsible for the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nDrives cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles.\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. for assigned domains.\nEnsure compliance requirements with data privacy, security, and regulatory policies for the assigned domains\nJointly with Technology teams, business functions, and enterprise teams (e. g. , MDM, Enterprise Data Fabric, etc. ) define the specifications shaping the development and implementation of data foundations .\nBuild strong relationships with key business leads and partners to ensure their needs are being met\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills with knowledge of Pharma processes with specialization in the General and Administrative operations (GA) domain of the biopharma lifecycle.\nIn depth knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc.\nIn depth experience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer-focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nIn depth experience of working with or supporting systems used to data governance framework. E. g. Collibra, Alation\nExcellent problem-solving skills and committed attention to detail in finding solutions\nGood-to-Have Functional Skills:\nExperience of working with data governance councils or forums\nExperience with Agile software development methodologies (Scrum)\nProficiency in data analysis and quality tools (e. g. , SQL, Excel, Python, or SAS)\nSoft Skills:\nHighly organized and able to work under minimal supervision\nExcellent analytical and assessment skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAmbitious to further develop their skills and career\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills\nHigh degree of initiative and self-motivation.\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management and customer focus.\nBasic Qualifications:\nAny Degree and 9-13 Years of Experience\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business process', 'Administration', 'Data analysis', 'Change management', 'operational support', 'SAS', 'Pharma', 'Analytical', 'Data quality', 'SQL']",2025-06-12 15:14:36
Data Specialist - Commercial,Horizon Therapeutics,9 - 13 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgens data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leverages state-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. You will leverage domain, technical and business process expertise to provide exceptional support of Amgen s data governance framework. This role involves working closely with business stakeholders and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with the Product Owner and other Business Analysts to ensure operational support and excellence from the team.\nRoles Responsibilities:\nResponsible for the data governance and data management framework implementation for the Commercialization domain of the biopharma lifecycle.\nResponsible for the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nDrives cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles.\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. for assigned domains.\nEnsure compliance requirements with data privacy, security, and regulatory policies for the assigned domains\nJointly with Technology teams, business functions, and enterprise teams (e. g. , MDM, Enterprise Data Fabric, etc. ) define the specifications shaping the development and implementation of data foundations .\nBuild strong relationships with key business leads and partners to ensure their needs are being met\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills with knowledge of Pharma processes with specialization in the Commercialization domain of the biopharma lifecycle.\nIn depth knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc.\nIn depth experience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer-focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nIn depth experience of working with or supporting systems used to data governance framework. E. g. Collibra, Alation\nExcellent problem-solving skills and committed attention to detail in finding solutions\nGood-to-Have Functional Skills:\nExperience of working with data governance councils or forums\nExperience with Agile software development methodologies (Scrum)\nProficiency in data analysis and quality tools (e. g. , SQL, Excel, Python, or SAS)\nSoft Skills:\nHighly organized and able to work under minimal supervision\nExcellent analytical and assessment skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAmbitious to further develop their skills and career\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills\nHigh degree of initiative and self-motivation.\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management and customer focus.\nBasic Qualifications:\nAny Degree and 9-13 years of experience\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business process', 'Data analysis', 'Change management', 'operational support', 'SAS', 'Pharma', 'Analytical', 'Agile', 'Data quality', 'SQL']",2025-06-12 15:14:38
Data Specialist - Supply Chain,Horizon Therapeutics,9 - 13 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgens data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leverages state-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. You will leverage domain, technical and business process expertise to provide exceptional support of Amgen s data governance framework. This role involves working closely with business stakeholders and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with the Product Owner and other Business Analysts to ensure operational support and excellence from the team.\nRoles Responsibilities:\nResponsible for the data governance and data management framework implementation for the Supply Chain domain of the biopharma lifecycle.\nResponsible for the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nDrives cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles.\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. for assigned domains.\nEnsure compliance requirements with data privacy, security, and regulatory policies for the assigned domains\nJointly with Technology teams, business functions, and enterprise teams (e. g. , MDM, Enterprise Data Fabric, etc. ) define the specifications shaping the development and implementation of data foundations .\nBuild strong relationships with key business leads and partners to ensure their needs are being met\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills with knowledge of Pharma processes with specialization in the Supply Chain domain of the biopharma lifecycle.\nIn depth knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc.\nIn depth experience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer-focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nIn depth experience of working with or supporting systems used to data governance framework. E. g. Collibra, Alation\nExcellent problem-solving skills and committed attention to detail in finding solutions\nGood-to-Have Functional Skills:\nExperience of working with data governance councils or forums\nExperience with Agile software development methodologies (Scrum)\nProficiency in data analysis and quality tools (e. g. , SQL, Excel, Python, or SAS)\nSoft Skills:\nHighly organized and able to work under minimal supervision\nExcellent analytical and assessment skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAmbitious to further develop their skills and career\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills\nHigh degree of initiative and self-motivation.\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management and customer focus.\nBasic Qualifications:\nAny Degree and 9-13 years of experience\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Business process', 'Data analysis', 'Change management', 'operational support', 'SAS', 'Pharma', 'Analytical', 'Data quality', 'SQL']",2025-06-12 15:14:41
Job opening For Data Warehouse + ADF + ETL,bct,3 - 6 years,Not Disclosed,['Pune'],"Greetings of the Day !!!\n\nWe have job opening for Data Warehouse + ADF + ETL with one of our Client .If you are interested for this role , kindly share update resume along with below details in this email id : shaswati.m@bct-consulting.com\n\nJob Description:\nSenior Data Engineer\nAs a Senior Data Engineer, you will support the European World Area using the Windows & Azure suite of Analytics & Data platforms. The focus of the role is on the technical aspects and implementation of data gathering, integration and database design.\nWe look forward to seeing your application!\nIn This Role, Your Responsibilities Will Be:\nData Ingestion and Integration: Collaborate with Product Owners and analysts to understand data requirements & design, develop, and maintain data pipelines for ingesting, transforming, and integrating data from various sources into Azure Data Services.\nMigration of existing ETL packages: Migrate existing SSIS packages to Synapse pipelines\nData Modelling: Assist in designing and implementing data models, data warehouses, and databases in Azure Synapse Analytics, Azure Data Lake Storage, and other Azure services.\nData Transformation: Develop ETL (Extract, Transform, Load) processes using SQL Server Integration Services (SSIS), Azure Synapse Pipelines, or other relevant tools to prepare data for analysis and reporting.\nData Quality and Governance: Implement data quality checks and data governance practices to ensure the accuracy, consistency, and security of data assets.\nMonitoring and Optimization: Monitor and optimize data pipelines and workflows for performance, scalability, and cost efficiency.\nDocumentation: Maintain comprehensive documentation of processes, including data lineage, data dictionaries, and pipeline schedules.\nCollaboration: Work closely with cross-functional teams, including data analysts, data scientists, and business stakeholders, to understand their data needs and deliver solutions accordingly.\nAzure Services: Stay updated on Azure data services and best practices to recommend and implement improvements in our data architecture and processes\nFor This Role, You Will Need:\n3-5 years of experience in Data Warehousing with On-Premises or Cloud technologies\nStrong practical experience of Synapse pipelines / ADF.\nStrong practical experience of developing ETL packages using SSIS.\nStrong practical experience with T-SQL or any variant from other RDBMS.\nGraduate degree educated in computer science or a relevant subject.\nStrong analytical and problem-solving skills.\nStrong communication skills in dealing with internal customers from a range of functional areas.\nWillingness to work flexible working hours according to project requirements.\nTechnical documentation skills.\nFluent in English.\nPreferred Qualifications that Set You Apart:\nOracle PL/SQL.\nExperience in working on Azure Services like Azure Synapse Analytics, Azure Data Lake.\nWorking experience with Azure DevOps paired with knowledge of Agile and/or Scrum methods of delivery.\nLanguages: French, Italian, or Spanish would be an advantage.\nAgile certification.\nThanks,\nShaswati",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ADF', 'ETL', 'SSIS', 'Data ware house']",2025-06-12 15:14:43
Data Governance & Data Quality Sr Associate Analyst,Amgen Inc,2 - 5 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nYou will play a key role in the implementation and adoption of the data governance framework which will modernize Amgen's data ecosystem, positioning Amgen as a leader in biopharma innovation. This role leveragesstate-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. This role involves working closely with business stakeholder and data analysts to ensure implementation and adoption of the data governance framework. You will collaborate with Data Product Owners, Data Stewards and technology teams to increase the trust and reuse of data across Amgen.\nRoles & Responsibilities:\nResponsible for the execution of data governance framework for a given domain of expertise (Research, Development, Supply Chain, etc.).\nContribute to the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nContribute to the cross functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles.\nMaintain documentation on data definitions, data standards, data flows, legacy data structures / hierarchies, common data models, data harmonization etc. for assigned domains.\nPartner with business teams to identify compliance requirements with data privacy, security, and regulatory policies for the assigned domains\nJointly with Technology teams, business functions, and enterprise teams (e.g., MDM, Enterprise Data Fabric, etc.) delivers data foundations.\nBuild strong relationship with key business leads and partners to ensure their needs are met.\nFunctional Skills:\nMust-Have Functional Skills:\nTechnical skills (Advanced SQL, Python etc) with knowledge of Pharma processes with specialization in a domain (e.g., Research, Clinical Trials, Commercial, etc.)\nExperience of working with or supporting systems used to data governance framework. E.g. Collibra, Alation\nGeneral knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc.\nExperience with data products development life cycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and technology topics\nExcellent problem-solving skills and a committed attention to detail in finding solutions\nGood-to-Have Functional Skills:\nExperience with Agile software development methodologies (Scrum)\nSoft Skills:\nExcellent analytical skills\nAbility to work effectively with global, virtual teams\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nAbility to build business relationships and understand end-to-end data use and needs.\nStrong verbal and written communication skills\nBasic Qualifications:\nExperience with 5 - 9 years of experience in Business, Engineering, IT or related field",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Governance', 'data quality', 'Collibra', 'data stewardship', 'metadata management', 'Agile software development methodologies', 'Alation', 'data protection', 'master data management', 'SQL', 'Python']",2025-06-12 15:14:45
Data Science (SSE) | FINJO I766,Omni Recruit,3 - 8 years,Not Disclosed,['Mumbai (All Areas)'],"Python Developer\nWork from Office\nLocation : Airoli , Navi Mumbai\n& :\n3.55 years of relevant experience in Python\nMinimum 3.5 years in Python programming\nAt least 1+ year in machine learning and natural language processing (NLP)\nMinimum 1.5 years with LLMs and GenAI\nAt least 2 years of experience with any database\n1+ year of experience deploying ML models/Python applications on Azure or AWS",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Machine Learning', 'Python']",2025-06-12 15:14:47
"SDE, IIoT, Decision Science and Technology (DST)",Amazon,3 - 8 years,Not Disclosed,['Hyderabad'],"Interested in creating systems and services that bring the power of Machine Learning (ML) to new application fields? With IIoT products, our organization is setting the standard for high-performance, easy-to-use, and cost-effective ML services. Our team expands this portfolio to new applications, enhancing our condition-based maintenance program, and maximizing equipment availability.\n\nAs a Software Development Engineer, you will be responsible for designing, developing, testing, and deploying distributed machine learning systems and large-scale solutions for our world-wide customer base. In this role, you will collaborate closely with a team of research and applied scientists to influence our overall strategy and define the team s roadmap. You will also drive the system architecture, spearhead best practices that enable a quality product, and help coach and develop junior engineers. A successful candidate will have an established background in engineering large-scale software systems, a strong technical ability, great communication skills, and a motivation to achieve results in a fast-paced environment.\n\nOur team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we re building an environment that celebrates knowledge sharing and mentorship. Our senior members enjoy one-on-one mentoring and thorough, but kind, code reviews. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded engineer and enable them to take on more complex tasks in the future.\n\n\nSolving difficult problems with elegant and practical code\nHelping define engineering best practices and providing technical mentorship to other members of the engineering team\nBeing thoughtful for the customer and ensuring their needs come first\nDesigning and building software for a multitude of sensors (vibration, temperature), mobile clients, and back-end cloud server systems\n\nYoure an awesome fit if you demonstrate:\n\nIndustry-leading technical abilities show-casing a breadth and depth of technical knowledge\nThe ability to build good working relationships within the team by communicating clearly both verbally and in writing\nStrong problem solving and trouble shooting skills with the ability to come up with creative solutions to seemingly impossible problems\nEffective technical leadership skills to improve technologies and infrastructure of the team\nAre curious trying new technologies, and passionate about innovating on behalf of customers\n\nAbout the team\nDST combines the expertise from talented program, product managers, engineers, and scientists to create programs and products that support such programs to drive cost optimization, and prevent events (e.g., unplanned downtime) that negatively impact customer experience. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Coding', 'Machine learning', 'Architectural design', 'Software development life cycle', 'Technical leadership', 'Sensors', 'Customer experience', 'Internship']",2025-06-12 15:14:49
"SDE, IIoT, Decision Science and Technology",Amazon,3 - 8 years,Not Disclosed,['Hyderabad'],"Interested in creating systems and services that bring the power of Machine Learning (ML) to new application fields? With IIoT products, our organization is setting the standard for high-performance, easy-to-use, and cost-effective ML services. Our team expands this portfolio to new applications, enhancing our condition-based maintenance program, and maximizing equipment availability.\n\nAs a Software Development Engineer, you will be responsible for designing, developing, testing, and deploying distributed machine learning systems and large-scale solutions for our world-wide customer base. In this role, you will collaborate closely with a team of research and applied scientists to influence our overall strategy and define the team s roadmap. You will also drive the system architecture, spearhead best practices that enable a quality product, and help coach and develop junior engineers. A successful candidate will have an established background in engineering large-scale software systems, a strong technical ability, great communication skills, and a motivation to achieve results in a fast-paced environment.\n\nOur team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we re building an environment that celebrates knowledge sharing and mentorship. Our senior members enjoy one-on-one mentoring and thorough, but kind, code reviews. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded engineer and enable them to take on more complex tasks in the future.\n\n\nSolving difficult problems with elegant and practical code\nHelping define engineering best practices and providing technical mentorship to other members of the engineering team\nBeing thoughtful for the customer and ensuring their needs come first\nDesigning and building software for a multitude of sensors (vibration, temperature), mobile clients, and back-end cloud server systems\n\nYoure an awesome fit if you demonstrate:\n\nIndustry-leading technical abilities show-casing a breadth and depth of technical knowledge\nThe ability to build good working relationships within the team by communicating clearly both verbally and in writing\nStrong problem solving and trouble shooting skills with the ability to come up with creative solutions to seemingly impossible problems\nEffective technical leadership skills to improve technologies and infrastructure of the team\nAre curious trying new technologies, and passionate about innovating on behalf of customers\n\nAbout the team\nDST combines the expertise from talented program, product managers, engineers, and scientists to create programs and products that support such programs to drive cost optimization, and prevent events (e.g., unplanned downtime) that negatively impact customer experience. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Coding', 'Machine learning', 'Architectural design', 'Software development life cycle', 'Technical leadership', 'Sensors', 'Customer experience', 'Internship']",2025-06-12 15:14:52
AI-Enabler Custom Developer,Hoffmann La Roche,2 - 4 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position\nThroughout our 125-year history, Roche has grown into one of the world s largest biotech companies and a global supplier of transformative, innovative solutions across major disease areas.\n\nWe are now entering an exciting new chapter of our digital transformation journey by embracing the power of Artificial Intelligence. In line with our Roche Group AI Strategy and our 10-Year Ambition to Transform our business with data & digital solutions, we are developing AI capabilities across all levels of the organization from EverydayAI, which enhances individual productivity, to Reshape initiatives, which reimagine business processes, to Big Ideas, which push the boundaries of what s possible in healthcare.\nWe are looking for forward-thinking professionals to join Roche Informatics and help us bring this strategy to life.\n\nPune continues to play the role of a Technology Acceleration Hub, building capabilities that drive digital innovation, including cutting-edge AI solutions that support Roche s mission to prevent, stop, or cure diseases with the highest societal burden.\n\nOur Expectations\nWe are looking for a Software Engineer eager to develop and implement AI-powered solutions within Roches\ntechnology ecosystem. The ideal candidate should have a strong foundation in software development, a willingness to\nupskill in AI and Generative AI technologies, and the ability to integrate large language models (LLMs) into applications\nand software development processes (testing, refactoring, requirements management, deployment).\n\nKey Competencies & Skills\nAI Expertise\nUnderstanding of how LLMs work, their strengths, limitations, and practical applications.\nExperience in basic prompt engineering.\nFamiliarity with direct LLM API usage (e.g., OpenAI API, SDKs).\nConceptual understanding of RAG architecture.\nHands-on experience with libraries to create basic LLM workflows (e.g., LangChain, LlamaIndex) and\nvector databases (e.g., Qdrant) is a plus.\nBasic understanding of NLP concepts such as tokens and embeddings.\nSoftware Development & Cloud Engineering:\nStrong programming skills in at least one language (e.g., Python, Go, TypeScript, Java, Kotlin) with a\ngood understanding of tooling, ecosystem, and software development best practices.\nExperience with API usage and basic understanding of cloud-based AI deployments (AWS/Azure/GCP).\nFamiliarity with AI software development tools (e.g., Github Copilot).\n\nDevOps Practices:\nUnderstanding of CI/CD pipelines and automation testing.\nFamiliarity with GitHub and GitLab.\n\nCollaboration & Knowledge Sharing:\nGood communication skills in English (B2/C1 level) to work within cross-functional teams.\nAbility to collaborate with Engineers and potentially Data Scientists on AI-driven enhancements.\nKey Responsibilities\nDesign, develop, and optimize custom applications, potentially incorporating AI-powered features within Roche s\necosystem.\nIntegrate Large Language Models (LLMs) into custom applications, leveraging APIs and prompt engineering\ntechniques, under guidance.\nUtilize cloud-based AI services (AWS, Azure, or GCP) to build basic AI-driven functionalities.\nCollaborate with internal teams to apply AI for the enhancement of custom applications.\nContribute to identifying opportunities for AI to improve application functionality.\nParticipate in knowledge sharing activities within the team.\n\nExample Projects You May Work On\nIntegrating AI features into existing custom applications to improve user experience.\nDeveloping basic AI-powered tools to aid in application development.\nAssisting in the integration of AI into application workflows.\n\nWhat We Value\nGood analytical and problem-solving skills.\nAdaptability to learn about AI and work in Agile environments.\nCuriosity and a willingness to take ownership of tasks.\nWho we are\nA healthier future drives us to innovate. Together, more than 100 000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. Our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our Diagnostics products. We empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Basic', 'github', 'Usage', 'GCP', 'Analytical', 'Artificial Intelligence', 'Agile', 'Healthcare', 'Application development', 'Python']",2025-06-12 15:14:54
AI-Enabler Custom Developer,Roche Diagnostics,2 - 4 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position\nThroughout our 125-year history, Roche has grown into one of the world s largest biotech companies and a global supplier of transformative, innovative solutions across major disease areas.\n\nWe are now entering an exciting new chapter of our digital transformation journey by embracing the power of Artificial Intelligence. In line with our Roche Group AI Strategy and our 10-Year Ambition to Transform our business with data & digital solutions, we are developing AI capabilities across all levels of the organization from EverydayAI, which enhances individual productivity, to Reshape initiatives, which reimagine business processes, to Big Ideas, which push the boundaries of what s possible in healthcare.\nWe are looking for forward-thinking professionals to join Roche Informatics and help us bring this strategy to life.\n\nPune continues to play the role of a Technology Acceleration Hub, building capabilities that drive digital innovation, including cutting-edge AI solutions that support Roche s mission to prevent, stop, or cure diseases with the highest societal burden.\n\nOur Expectations\nWe are looking for a Software Engineer eager to develop and implement AI-powered solutions within Roches\ntechnology ecosystem. The ideal candidate should have a strong foundation in software development, a willingness to\nupskill in AI and Generative AI technologies, and the ability to integrate large language models (LLMs) into applications\nand software development processes (testing, refactoring, requirements management, deployment).\n\nKey Competencies & Skills\nAI Expertise\nUnderstanding of how LLMs work, their strengths, limitations, and practical applications.\nExperience in basic prompt engineering.\nFamiliarity with direct LLM API usage (e.g., OpenAI API, SDKs).\nConceptual understanding of RAG architecture.\nHands-on experience with libraries to create basic LLM workflows (e.g., LangChain, LlamaIndex) and\nvector databases (e.g., Qdrant) is a plus.\nBasic understanding of NLP concepts such as tokens and embeddings.\nSoftware Development & Cloud Engineering:\nStrong programming skills in at least one language (e.g., Python, Go, TypeScript, Java, Kotlin) with a\ngood understanding of tooling, ecosystem, and software development best practices.\nExperience with API usage and basic understanding of cloud-based AI deployments (AWS/Azure/GCP).\nFamiliarity with AI software development tools (e.g., Github Copilot).\n\nDevOps Practices:\nUnderstanding of CI/CD pipelines and automation testing.\nFamiliarity with GitHub and GitLab.\n\nCollaboration & Knowledge Sharing:\nGood communication skills in English (B2/C1 level) to work within cross-functional teams.\nAbility to collaborate with Engineers and potentially Data Scientists on AI-driven enhancements.\nKey Responsibilities\nDesign, develop, and optimize custom applications, potentially incorporating AI-powered features within Roche s\necosystem.\nIntegrate Large Language Models (LLMs) into custom applications, leveraging APIs and prompt engineering\ntechniques, under guidance.\nUtilize cloud-based AI services (AWS, Azure, or GCP) to build basic AI-driven functionalities.\nCollaborate with internal teams to apply AI for the enhancement of custom applications.\nContribute to identifying opportunities for AI to improve application functionality.\nParticipate in knowledge sharing activities within the team.\n\nExample Projects You May Work On\nIntegrating AI features into existing custom applications to improve user experience.\nDeveloping basic AI-powered tools to aid in application development.\nAssisting in the integration of AI into application workflows.\n\nWhat We Value\nGood analytical and problem-solving skills.\nAdaptability to learn about AI and work in Agile environments.\nCuriosity and a willingness to take ownership of tasks.\nWho we are\n.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Basic', 'github', 'Usage', 'GCP', 'Analytical', 'Artificial Intelligence', 'Agile', 'Healthcare', 'Application development', 'Python']",2025-06-12 15:14:56
Sr. Technology Auditor,AMERICAN EXPRESS,2 - 4 years,13-18 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Role & responsibilities\n•       Translate business risks, controls and supporting data into analytic requirements and partners with colleagues to build effective analytics and insights\n•       Responsible for multiple simultaneous audit projects of all sizes and complexity across multiple business areas within and outside of local region, in unfamiliar areas, and for different audit leaders\n•       Link analytics and insights to ongoing strategic initiatives\n•       Apply proven/ advanced data algorithms, advanced analytic and modeling techniques to draw insights essential to driving improvement initiatives",,,,"['Natural Language Processing', 'Tableau', 'Machine Learning', 'SQL', 'Python']",2025-06-12 15:14:59
Strategic Systems Director,Ericsson,10 - 15 years,Not Disclosed,['Noida'],"About this opportunity:\nNetworks are evolving rapidly, and so is the way we manage them. Our Managed Services business is at the forefront of industry change, leveraging the capabilities of our innovative Ericsson Operations Engine, which integrates autonomous operations to enhance efficiency and effectiveness. This transformation combines our community of creative, driven individuals with digital capabilities like automation, machine learning, and autonomous operations at the core of the Ericsson Operations Engine.\nWe are spearheading a digital transformation at scale, redefining how networks are managed globally across the industry. Are you ready to become a change agent in our forward-thinking organization.\nWhat you will do:\nTechnically lead a business-critical transformation programs by collaborating with forward-thinking colleagues from diverse cultures and technology partners worldwide to reshape our platform architecture to cloud-native.\nDesign the optimal solution within the constraints of specifications, costs, products, quality, and timelines, and be ultimately accountable for its successful implementation, ensuring the final solution meets business needs.\nInspire change and improvements within and across functions/units, processes, and value flows, with a focus on incorporating autonomous operations.\nThe skills you bring:\nTotal industry experience of 10 years+\nExperience as a leader or innovator enabling technology transformation and delivering strategic direction\nExpertise in cross-domain/E2E solution architecture\nAbility to assume responsibility and coordination for solution architecture/design and lead technical issues, with a focus on autonomous operations\nStrong technical and business insight related to software delivery and operations in a cloud solution environment\nTeam leadership capability, with experience in directly leading the technical acceptance of the solution\nAbility to work and produce targeted results with minimal supervision\nSelf-starter with a strong sense of business ownership and leadership\nMotivation and commitment to meeting challenging individual and team deadlines\nIndependence and self-direction\nCapability to create order in an unstructured environment\nExperience in large and complex system integration projects\nStrong skills in system engineering and architecture design methodologies\nBusiness understanding and critical thinking capabilities to develop creative solutions to opportunities\nCommunication and presentation skills to confidently engage and influence at all levels within the company\nCultural awareness and excellent interpersonal communications and networking skills\nThought leadership in identifying short- and long-term priorities to improve business strategy, with a focus on autonomous operations",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'Automation', 'Managed services', 'Networking', 'Focus', 'System integration', 'Architectural design', 'Machine learning', 'Business strategy', 'Business understanding']",2025-06-12 15:15:01
"Technical Training Specialist, Staff",Qualcomm,6 - 11 years,Not Disclosed,['Gurugram'],"Job Area: Engineering Services Group, Engineering Services Group > Technical Training\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nJob TitleTechnical Training Specialist, Staff\n\nJob Overview:\n\nIn collaboration with subject matter experts, develop high-quality technical training materials for use in training courses geared towards engineers and business professionals, with keen attention to quality and engaging learning experience.\n\nJob Overview:\n\nThe primary responsibility of this role is to teach AI courses, with a strong emphasis on AI-related Qualcomm technologies. In collaboration with subject matter experts, you will develop and deliver high-quality technical training materials for engineers and business professionals, ensuring an engaging and effective learning experience.\n\nAdditional :\n\nKnowledge and\n\nSkills:\n\nDemonstrated proficiency in designing, developing, and delivering a variety of technical training programs, with a strong focus on AI and AI-related Qualcomm technologies, for a technical workforce encompassing engineering, business, IT, and other technical professionals.\nRobust practical knowledge of Instructional Design Methodologies and Adult Learning Theory, particularly as they apply to AI and advanced technology training.\nExcellent written and verbal communication skills in English, with the ability to convey complex AI concepts and Qualcomm technologies clearly and effectively.\nUncompromising approach towards content quality, accuracy, and effectiveness, ensuring that training materials are both informative and engaging.\nDemonstrable prior leadership experienceproven track record to successfully lead a team and drive a variety of projects to completion in a dynamic work environment.\nSuperb organizational skills, with the ability to prioritize and manage multiple simultaneous tasks in a systematic, process-oriented manner.\nAbility to work with and gain a deep understanding of highly technical content spanning multiple domains such as 5G, Artificial Intelligence, Extended Reality, etc.\nExperience in developing and managing self-paced online training using tools like Adobe Captivate and Camtasia is a plus.\n\n\nPreferred Qualifications:\nMasters degree in Educational Technology, Instructional Design, or related fields.\n6+ years of relevant experience, preferably in the technology industry.\n\n\nA degree in engineering/technology fields is a must.\n\n\nAdditional Skills & Qualifications:\nPrior experience or ability to work with a variety of AI tools applicable to learning environments, with a focus on Qualcomm technologies.\nWillingness and ability to be available for online work meetings according to US time zones.\nAbility to provide prior work samples is a plus.\n\nIf you are a self-driven leader who excels in a collaborative environment and has a consultative, customer service orientation, we want to hear from you.",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['hiring', 'artificial intelligence', 'camtasia', 'staffing', 'online tutoring', 'technical writing', 'head hunting', 'leadership hiring', 'screening', 'framemaker', 'sourcing', 'technical hiring', 'talent acquisition', 'it recruitment', 'technical recruitment', 'recruitment', 'lateral hiring']",2025-06-12 15:15:04
Spark Scala + AWS & SQL,Cognizant,10 - 12 years,Not Disclosed,['Chennai'],Job Summary\nAs a Technical Lead specializing in Generative AI and Python you will play a pivotal role in driving innovation and excellence within our team. With 10 to 12 years of experience you will leverage your expertise to develop cutting-edge solutions that align with our companys strategic goals. This office-based position offers the opportunity to work in a dynamic environment during day shifts contributing to impactful projects that enhance our technological capabilities.,,,,"['algorithms', 'python', 'data analysis', 'technical leadership', 'software development', 'workflow', 'scala', 'machine learning', 'hibernate', 'artificial intelligence', 'scalability', 'sql', 'microservices', 'spring', 'spring boot', 'security', 'java', 'spark', 'ai techniques', 'j2ee', 'agile', 'aws', 'programming', 'agile methodology']",2025-06-12 15:15:06
Generative AI- Sr. Associate,Cognizant,7 - 10 years,Not Disclosed,['Chennai'],Job Summary\nShould have worked hands-on in setting up ML services on public cloud and delivering ML based solutions.\nShould have 7 to 10 years of experience in AI ML and at least 1 year in GenAI leveraging cloud based GenAI services.\nShould have through knowledge of end-to-end deployment of GenAI solutions to customers cutting across the industry prevalence of GenAI.\nResponsibilities,,,,"['screening', 'artificial intelligence', 'research', 'sourcing', 'sql', 'cloud', 'analytics', 'talent acquisition', 'java', 'data science', 'recruitment', 'model development', 'end', 'deployment', 'architecture', 'sr', 'ml', 'interfaces', 'python', 'natural language processing', 'microsoft azure', 'aiml', 'hrsd', 'machine learning', 'r', 'aws']",2025-06-12 15:15:09
Analytics & Visualization Developer,Qualcomm,7 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Programmer Analyst\n\nGeneral Summary:\n\nQualcomms Engineering IT EDAAP team is looking for an independent contributor experienced in development and sustaining enterprise level software applications. Experience:7-10 years of experience developing dashboard with reporting tools- Tableau (Tableau API), Power BI, OBIEE. SkillsMust:\nExpert in developing visualizations/dashboards with Tableau\nStrong knowledge with SQL\nFundamentals in object-oriented design, data structures, algorithms and problem solving.\nTest, debug and performance tuning of dashboards/reports\nExperience working in an agile development environment.\nTranslate ad hoc report requests into common dashboards and application requirements\nKnowledge of different types of enterprise systems, their interaction, boundaries within an enterprise.\nUnderstanding of complex data models.\nExperience working with Oracle/MySQl/Postgres\nWILLINGNESS to multi task and work in a fast paced environment.\nMust be willing to take ownership and drive tasks to completion. Desirable:\nPython programming experience.\nExperience developing dashboards with Power BI.\nExposure to Qlikview, OBIEE and ThoughtSpot\nExperience with semiconductor industry.\nExperience working with NoSQL Databases (MongoDB) as well as relational DBs (MySQL/Oracle) Education\nBachelor's degree in technical discipline or equivalent experience required.\n\nQualifications\n5 or more years of experience in applying AI and machine learning techniques to practical and comprehensive technology solutions.\nA strong background in machine learning, deep learning, and natural language processing.\nExpertise in ML, deep learning, Py Torch, Python, NLP and Transformer architecture.\nExperience in deploying LLMs, embedding model/sentence transformers in production use cases.\nThorough knowledge in basic algorithms, object-oriented and functional design principles, and best-practice patterns\nStrong expertise in programming (Rust/Python)\nExperience in fine-tuning a large language model using custom content (documents, data, code).\nExperience in developing Generative AI applications, Agentic Systems and Retrieval Augmented Generation.\nExperience working with large-scale datasets, preprocess them, and create appropriate data representations.\nSolid understanding of statistics, linear algebra, and probability theory.\n\nPreferred Qualifications\nBachelors/masters degree in computer science, Artificial Intelligence, Data Science, or a related field.\nExperience in implementing projects involving end to end ML/NLP systems from development to deployment.\nExperience with transformer-based models (e.g., BERT, GPT, T5, Llama).\nExperience working in a distributed team.\nExperience with cloud environments (GCP/AWS).\nWorking knowledge of Rust is a plus.\n\nMinimum Qualifications:\n4+ years of work experience in programming, scripting, and/or automation or IT-relevant work experience with a Bachelor's degree.\nOR\n6+ years of work experience in programming, scripting, and/or automation or IT-relevant work experience without a Bachelors degree.\n\n2+ years experience with Database Design structures such as Mongo DB, MySQL.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'natural language processing', 'machine learning', 'deep learning', 'pytorch', 'algorithms', 'functional design', 'dashboards', 'artificial intelligence', 'sql', 'database design', 'tableau', 'data visualization', 'design principles', 'linear algebra', 'ml', 'statistics']",2025-06-12 15:15:11
AIML Security Risk Assessment Specialist - Information Security,Hdfc Bank,7 - 12 years,Not Disclosed,['Mumbai (All Areas)'],"Job Summary\nThe AIML Security Risk Assessment Specialist will play a critical role in validating reports and making final risk assessments for AIML models used in various business applications and use cases. This role will work closely with the Digital Risk Management Portfolio team to ensure the security and integrity of AIML models, use case along with applications.\nKey Responsibilities\n1. Risk Assessment: understand the business requirement, finalise the scope and perform end to end risk assessment.",,,,"['Generative Ai', 'Cyber Security', 'Information Security']",2025-06-12 15:15:13
"Program Manager, Geospatial",Amazon,3 - 8 years,Not Disclosed,['Hyderabad'],"Amazon s Geospatial team is looking for a Program Manager to join our team. Our team s our vision is to provide the best mapping solution for logistics with focus on a safe, efficient and frictionless delivery experience. We build and operate software and hardware solutions that support drivers globally. We use innovative technology, such as machine learning, computer vision and large language learning models to enrich our maps. In order to so, we work with a portfolio of map vendors to enrich our maps with attributes important to transporters, as well as hardware vendors for strategic projects.\n\nIn this role, you manage complex initiatives, delivering critical solutions, significant improvements, new mechanisms, or deprecating processes that are no longer needed. These efforts require you to work with multiple teams in and/or across organizations.\n\nThe ideal candidate has extensive Program Management experience. This role also requires demonstrated experience managing cross functional relationships, performing financial analysis, creating and implementing processes and great communication skills to influence a variety of internal and external audiences. You are a self-starter and have the business acumen to unpack complex business needs. You thrive in a fast-paced environment and are comfortable with ambiguity.\n\n\nOwn program level goals and initiatives\nWork closely with Vendor Managers, Technology teams, Product teams, Legal and Finance teams to identify opportunities for standardizing processes\nManage procurement asks for strategic initiatives including creating POs and tracking against budgets\nDevelop internal and external governance mechanisms 3+ years of program or project management experience\n3+ years of working cross functionally with tech and non-tech teams experience\n3+ years of defining and implementing process improvement initiatives using data and metrics experience\nBachelors degree\nKnowledge of Excel (Pivot Tables, VLookUps) at an advanced level and SQL\nExperience defining program requirements and using data and metrics to determine improvements 3+ years of driving end to end delivery, and communicating results to senior leadership experience\n3+ years of driving process improvements experience\nExperience in stakeholder management, dealing with multiple stakeholders at varied levels of the organization\nExperience building processes, project management, and schedules",,,,"['Procurement', 'Computer vision', 'Financial analysis', 'Project management', 'Process improvement', 'Manager Program Management', 'Machine learning', 'Stakeholder management', 'SQL', 'Logistics']",2025-06-12 15:15:16
Senior Analyst - Direct Display,Merkle B2b,2 - 8 years,Not Disclosed,['Chennai'],"The purpose of this role is to assist with the planning, reviewing and optimisation of Display campaigns whilst supporting the team in reporting and managing client accounts.\nJob Description:\nKey responsibilities:\nFocuses on day-to-day execution\nProactively reviews and manages client data to ensure optimal performance on all campaigns\nTracks and reports on campaign results, gathers data analysis and participates in weekly calls\nGenerates campaign reports and is responsible for pacing, QA and trafficking\nDevelops and maintains accurate project plans for client status updates\nLocation:\nChennai\nBrand:\nParagon\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['QA', 'Data analysis', 'Senior Analyst', 'Management']",2025-06-12 15:15:18
Senior Analyst - Direct Display,Merkle Science,1 - 6 years,Not Disclosed,['Chennai'],"The purpose of this role is to assist with the planning, reviewing and optimisation of Display campaigns whilst supporting the team in reporting and managing client accounts.\nJob Description:\nKey responsibilities:\nFocuses on day-to-day execution\nProactively reviews and manages client data to ensure optimal performance on all campaigns\nTracks and reports on campaign results, gathers data analysis and participates in weekly calls\nGenerates campaign reports and is responsible for pacing, QA and trafficking\nDevelops and maintains accurate project plans for client status updates\nLocation:\nChennai\nBrand:\nParagon\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['QA', 'Data analysis', 'Senior Analyst', 'Management']",2025-06-12 15:15:20
IN-Associate-KYC/AML - Fincrime COE-Advisory,PwC Service Delivery Center,3 - 6 years,Not Disclosed,['Gurugram'],"Not Applicable\nSpecialism\nRisk\nManagement Level\nAssociate\n& Summary\n.\n\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nAt PwC , we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n& Summary We are seeking a highly skilled KYC Analyst experience to join our dynamic team in the Financial Crime Compliance. The ideal candidate will be responsible for conducting thorough due diligence on clients by gathering and analyzing client information to verify compliance with regulatory requirements.\nResponsibilities\n1. Conduct client due diligence (CDD) to gather information such as identity verification, source of funds, and beneficial ownership for different entity types like Banks, Trust, Funds, SPV etc. 2. Perform initial checks on client documents and data to ensure completeness and accuracy. 3. Support in conducting research using various databases and sources to verify client information. 4. Evaluate based on client risk levels which includes business activities, geographic location, and other relevant factors. 5. Conduct sanction screening and adverse media screening of customers using specialized tools and databases and analyze screening results to identify matches with sanctioned individuals, entities, or countries. 6. Maintain accurate documentation for all clients, including KYC profiles and ongoing monitoring records.\nMandatory skill sets 1. Strong understanding of financial regulations, including but not limited to Bank Secrecy Act (BSA), AntiMoney Laundering (AML), and Know Your Customer (KYC) requirements. 2. Experienced analyst with a in depthknowledge of financial products, services, and industry regulations. 3. Excellent analytical skills with the ability to interpret complex financial data and identify potential risks. 4. Detailoriented with strong organizational and time management abilities\nPreferred skill sets Strong understanding of financial regulations, including but not limited to Bank Secrecy Act (BSA), AntiMoney Laundering (AML), and Know Your Customer (KYC) requirements.\nYears of experience required 36 years of experience in KYC, AML compliance, or a related role within the banking industry.\nEducation Qualification Any Grad\nEducation\nDegrees/Field of Study required Bachelor Degree\nDegrees/Field of Study preferred\nRequired Skills\nKYC Compliance\nAccepting Feedback, Accepting Feedback, Accounting and Financial Reporting Standards, Active Listening, Artificial Intelligence (AI) Platform, Auditing, Auditing Methodologies, Business Process Improvement, Communication, Compliance Auditing, Corporate Governance, Data Analysis and Interpretation, Data Ingestion, Data Modeling, Data Quality, Data Security, Data Transformation, Data Visualization, Emotional Regulation, Empathy, Financial Accounting, Financial Audit, Financial Reporting, Financial Statement Analysis, Generally Accepted Accounting Principles (GAAP) {+ 19 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Risk Management & Compliance,"Employment Type: Full Time, Permanent","['Manager Internal Audit', 'Assurance', 'Financial statements', 'Due diligence', 'Data analysis', 'Financial reporting', 'GAAP', 'Financial statement analysis', 'Risk management', 'financial auditing']",2025-06-12 15:15:24
Power BI Developer,Kellogg Brown & Root (KBR),5 - 10 years,Not Disclosed,['Chennai'],"Title:\nPower BI Developer\nCollaborate with all levels of finance organization on reporting requirements for both internal and external customers.\nWork independently and in partnership with business owners to provide innovative interactive reporting solutions to address a wide range of business needs using Power BI, Power Query, VBA, Cognos and other reporting tools.\nTransform financial data into visualization charts using Power BI and other reporting tools.\nLeverage multiple databases to merge and compile information to calculate relevant financial and business performance metrics.\nMaximize automation of routine tasks and processes using advanced toolsets (Artificial Intelligence or AI , Optical Character Recognition or OCR , Robotic Process Automation or RPA or Bots ).\nAutomate translation and migration of data between different systems (Costpoint, Cobra, EPM, EDW, OnBase).\nEnsure data quality by identifying and correcting errors, inconsistencies, and missing data to improve accuracy.\nCreate documentation and work instructions for applications and processes, ensure compliance with KBR IT standards and controls.\nBasic Qualifications:\nBachelor s Degree or equivalent in Finance, Accounting, Business Information Technology, Business Analytics, Information Systems or a related field.\nProficiency in Power BI, Data Modeling, SQL, VBA, Power Query.\nExpert understanding of Power BI functionality (reporting, publishing, security, mobile app).\nFoundational understanding of financial reporting metrics (Revenue, Cost of Goods Sold, Indirect Rate Application, EBIT, Cashflow, DSO, DPO)\nWorking knowledge of project management core concepts (contract types, cost sets, schedule, budgets).\nExperience with data analysis techniques, data integration, data modeling and data visualization.\nFamiliarity with basic software testing and implementation concepts and methods.\nPreferred Qualifications:\nWorking knowledge of Costpoint, Cobra, Hyperion (EPM, FCCS), OnBase, EDW, MSD.\nCapability with alternate programming and reporting tools (DAX, Python or R, Appian, Cognos).\nProject management Professional (PMP) or EVMS certification.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['PMP', 'Data analysis', 'Publishing', 'Data modeling', 'Cognos', 'Hyperion', 'Data quality', 'Information technology', 'SQL', 'Python']",2025-06-12 15:15:26
"Product Designer, People Technology",Assarain Group,2 - 7 years,Not Disclosed,"['Pune', 'Chennai']","Product Designer, People Technology\n\nKONE Technology and Innovation (KTI) is where the magic happens at KONE. Its where we combine the physical world - escalators and elevators - with smart and connected digital systems. We are changing and improving the way billions of people move within buildings every day. We are on a mission to shape the future of the industry with new technologies and sustainable innovations.\n\nKONE IT is part of the KONE Technology and Innovation (KTI) unit with the mission to power KONE with sustainable information flow. KONE IT also supports KONE in its digital transformation journey by introducing digital cloud-based IT services, artificial intelligence (AI) and automation to support productivity, business growth and technological disruption.\nOur Corporate Functions IT team is seeking a Product Designer for People Technology (Workday).\nYou will be responsible for designing IT solutions for HR applications, focusing on integrations and automation.\nYou will drive the vision of People Technology architecture forward, using agile and DevOps methods. As the main contact for business stakeholders, you will assess development ideas and oversee technical design and delivery in your area.\nIn this role, you collaborate closely with People Tech Product Owner, Product Architect, Functional Leads and HR and IT Operations teams, and actively support HR related projects. Current People Technology scope is based on global platforms and tools such as Workday (with wide range of capabilities), connected with SAP ERP, AWS, Microsoft Power Platform, and local solutions (e. g. payrolls, time tracking). Your job is to ensure that the solution designs meet KONEs business needs and follow market best practices.\nMain stakeholders of this role are People & Communications Functional Leads, People Tech Product Owner and Product Architect, IT and People & Comms Service Owners, local payroll teams, and business representatives from People & Communications.\n\nPosition will be based in Pune/Chennai India.\n\nIn this role, you get to:\nOwn the IT solution design for People Technology product area\nWork in close collaboration with the stakeholders to influence, collect and manage business demands and requirements for your responsibility area\nParticipate in requirement-gathering workshops to facilitate & influence the strategic direction of our projects and lead business stakeholders through solution design\nTurn requirements into specification and manage development and testing to final solution together with other IT teams and our implementation partner\nSupport solution road mapping and backlog prioritization with Product Owner\nSupport global roll-out and key user competence development of the solutions in your responsibility area\nContribute in People Tech projects\n\nSkills and Experience we`re looking for:\n5+ years of experience in People Technology, especially with Workday\nEnd-to-end People Tech process and technical solution understanding with focus on automation and integrations with global business solutions and payrolls.\nDeep understanding of Workday (Core HCM, Compensation, integration tools, Extend, Prism), MS Power Platform knowledge is considered as a plus\nPrevious experience working in IT projects and delivering end-to-end e. g. integration or automation development project\nAnalytical and conceptual thinking, innovative and ability to think big\nExperience in working in a multicultural, global organization\nExcellent communication and documentation skills in English\nWe are looking for a collaborative and proactive team player who approaches situations with an open mind, adapts quickly in a changing and agile environment and can create structure, clarity and results in a transparent way. Strong communication and interpersonal skills in English are essential for building relationships with stakeholders and peers. You naturally embrace and promote cultural diversity and inclusion within your team and organization.\nWhat do you get in return?\nPossibility to have an impact on People Technology product environment\nOpportunity to make a difference on designing more automated, improved E2E solutions and IT processes\nInternational and professional environment\nAttractive package of benefits and bonus scheme\nSocial and sport events on regular basis\nGreat place to work\nInformal culture and friendly colleagues\nKONE trainings in various areas\nPromotion on performance/competence\nWe can offer strong support in your personal growth and global opportunities for future career development at KONE. We believe in inspiring, engaging and developing our people.",Industry Type: Engineering & Construction,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['IT services', 'Career development', 'Automation', 'Payroll', 'SAP ERP', 'Analytical', 'Agile', 'microsoft', 'Business solutions', 'IT operations']",2025-06-12 15:15:29
"Financial Analyst II - AR, FinOps",Amazon,5 - 10 years,Not Disclosed,['Hyderabad'],"Are you an experienced Program Manager interested in an opportunity to help drive Amazon s flywheel and develop your A to Z business understanding? Do you enjoy learning about different Amazon business types and new subsidiaries, and thinking creatively about brand new businesses that Amazon is inventing on behalf of customers? The Global Accounts Receivable (GAR) team is seeking a creative and passionate program manager to help achieve our vision to provide a world-class Order-to-Cash (O2C) onboarding experience to our global business partners in support of Amazon s journey to become earth s most customer-centric company. We love to offer our customers unique world-class experiences, and we invite you to help Amazon make history!\n\nThe Program Manager will have global oversight of the integration of new initiatives onto O2C platforms, driving effective people, processes, and technology to achieve organizational goals and deliver results. This individual will have ownership over new business integration programs while standardizing the global implementation processes and driving efficiency. This role will require engagement and alignment with global business teams, finance teams, operational teams, system developers and product managers. Responsibilities include supporting new business initiatives through designing transactional workflows in line with the business model, defining requirements and testing of the solutions to ensure delivery is as expected and delivering and improving the customer experience. Implementation of mechanisms to monitor and measure performance is essential.\n\nThe ability to thrive in a fast-paced, ambiguous and demanding work environment is critical to success in this role. The ideal candidate will be a self-starter with knowledge of program management, experience with accounts receivable operational processes, demonstrate faster learning and adoptability, demonstrate good relationship and strategic influencing skills, experienced in large scale change management across functions and geographies, and exhibit a relentless pursuit for improvement. This individual must have a proven record of delivering results through good program management skills, problem solving skills, financial process and system knowledge, and a passion for customer experience.\n\nCore Requirements:\n5+ years of Accounts Receivable experience, with at least 2 years in a leadership role( not mandate)\nBachelors degree in Finance, Accounting, Business Administration, or related field\nAdvanced Excel skills and experience with ERP systems\nData Analytics Requirements:\n3+ years experience with data analysis and reporting tools\nProficiency in SQL for data extraction and analysis\nExperience with visualization tools (e.g., Tableau, Power BI)\nDemonstrated ability to translate data insights into actionable recommendations\n\nProgram Management Skills:\n3+ years experience managing complex projects or programs\nTrack record of process improvement initiatives\nExperience leading cross-functional teams\nGood stakeholder management abilities\nTechnical Skills:\nExperience with AR automation tools and systems\nKnowledge of financial control frameworks\nProficiency in Microsoft Office Suite\nExperience with business intelligence platforms\n\nAdditional Desired Qualifications:\nMBA or relevant masters degree\nProfessional certifications (CPA, PMP, or similar)\nExperience with machine learning or predictive analytics\nKnowledge of Python or R for advanced data analysis\n\n\nOwnership and implementation of new businesses and subsidiaries onto AR platforms\nPartner with key counterparts across geographies to launch and support initiatives globally in a scalable manner\nDevelop a solid understanding of Amazon s Finance Operations systems and processes\nDefine and implement global standards for business integration program management\nDefine and describe various business scenarios that can be relevant to New Businesses and convert them into system and operational requirements.\nTranslate complex business requirements into functional designs\nOversee comprehensive testing of systems changes and development of standard operating procedures, process documentation and performance metrics\nManage process transitions/implementations across multiple functions and geographies\nMotivate and influence business, operational and technical teams to ensure that best practices are followed and implemented\nIdentify, assess, track and mitigate risks at multiple levels\nProactively monitor program performance to identify, address and prevent potential issues\nAddress barriers through problem solving, communication and active coordination with stakeholders\nDrive effective teamwork, communication collaboration and commitment across multiple disparate groups with competing priorities\nIdentify gaps and strive constantly for re-engineering of systems and processes\nAmazon is an Equal Opportunity-Affirmative Action Employer Minority / Female / Disability /\nVeteran / Gender Identity / Sexual Orientation\n5+ years of Accounts Receivable (AR) experience 4+ years of participating in continuous improvement projects in your team to scale and improve controllership with measurable results experience\nMBA, or CPA\nKnowledge of Tableau\nExperience working with large-scale data mining and reporting tools (examples: SQL, MS Access, Essbase, Cognos) and other financial systems (examples: Oracle, SAP, Lawson, JD Edwards)",,,,"['Data analysis', 'Change management', 'PMP', 'SAP', 'MS Access', 'Process improvement', 'Oracle', 'Data mining', 'Business intelligence', 'SQL']",2025-06-12 15:15:31
Media AdTech Specialist,Capgemini,5 - 10 years,Not Disclosed,['Kolkata'],"Provide ad operations and/or AdTech operations expertise\nExecute and help implement an AdTech compliance program\nStrong understanding of Programmatic Ad Eco system and Retail Media bidding advertisement.\nYouTube, Connected TV and Video Ads advertisement and hands on ad set up experience.\nDesign and implement advertising solutions tailored for retail media needs.\nBuild operational systems that enhances the productivity of our Ad Operations team\nCollaborate with other departments and stakeholders to identify and solve complex problems\nContinuously testing and improving software solutions to ensure optimal performance and user experience\nCreate and manage strong relationships with DSPs and other relevant players in the ad tech space that can help our clients achieve their objectives.\nSpearhead initiatives to refine and expand digital media services.\nCollaborate with a diverse team of experts to drive innovation. (data scientists, developers, engineers, clients and stakeholders).\nEnsure seamless integration and service delivery.\nApply the latest industry trends and best practices to achieve our clients outcomes\nStay abreast of media regulations and trends affecting digital advertising.\nBuild highly performant AdTech platforms that will support our future growth in the Ads Space\n\nQualifications:\n5 years experience in advertising operations (AdTech ops) and/or revenue operations (Revops).\nStrong understanding of DSPs, digital advertising ecosystems, ad networks, and/or advertising exchanges.\nDemonstrated excellence in client relationship management.\nDemonstrated ability to build and work across teams.\nExperience in Technical Solutions Architecture and design leadership.\nManage multiple projects and prioritize tasks effectively\nBroad knowledge across multiple technology areas Marketing Operation, Ecommerce Domain and Retail Media.\nStrong organizational skills and attention to detail.\nAbility to work independently and as part of a team.\n\nTools:\nFacebook Ads Manager, Pinterest Ad Manager, Instagram ads, DV360, Programmatic, Campaign Manager 360, Google Ad Manager, TTD\nPower-Bi, Excel, PowerPoint will be a plus point.\nYouTube, Videos, CTV related ad platforms.",Industry Type: IT Services & Consulting,Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['DSP', 'Programmatic Buying', 'DV360', 'Facebook Ads Manager', 'Bidding', 'Display Video', 'Google Ads', 'Media Buying', 'Atl', 'Media Planning', 'Pinterest', 'Campaign Management', 'Btl']",2025-06-12 15:15:34
IN-Senior Associate-KYC/AML - Fincrime COE-Advisory,PwC Service Delivery Center,3 - 6 years,Not Disclosed,['Gurugram'],"Not Applicable\nSpecialism\nRisk\nManagement Level\nSenior Associate\n& Summary\n.\n\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nAt PwC , we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n& Summary We are seeking a highly skilled KYC Analyst experience to join our dynamic team in the Financial Crime Compliance. The ideal candidate will be responsible for conducting thorough due diligence on clients by gathering and analyzing client information to verify compliance with regulatory requirements.\nResponsibilities\n1. Conduct client due diligence (CDD) to gather information such as identity verification, source of funds, and beneficial ownership for different entity types like Banks, Trust, Funds, SPV etc. 2. Perform initial checks on client documents and data to ensure completeness and accuracy. 3. Support in conducting research using various databases and sources to verify client information. 4. Evaluate based on client risk levels which includes business activities, geographic location, and other relevant factors. 5. Conduct sanction screening and adverse media screening of customers using specialized tools and databases and analyze screening results to identify matches with sanctioned individuals, entities, or countries. 6. Maintain accurate documentation for all clients, including KYC profiles and ongoing monitoring records.\nMandatory skill sets 1. Strong understanding of financial regulations, including but not limited to Bank Secrecy Act (BSA), AntiMoney Laundering (AML), and Know Your Customer (KYC) requirements. 2. Experienced analyst with a in depthknowledge of financial products, services, and industry regulations. 3. Excellent analytical skills with the ability to interpret complex financial data and identify potential risks. 4. Detailoriented with strong organizational and time management abilities\nPreferred skill sets Strong understanding of financial regulations, including but not limited to Bank Secrecy Act (BSA), AntiMoney Laundering (AML), and Know Your Customer (KYC) requirements.\nYears of experience required 36 years of experience in KYC, AML compliance, or a related role within the banking industry.\nEducation Qualification Any Grad\nEducation\nDegrees/Field of Study required\nDegrees/Field of Study preferred\nRequired Skills\nKYC Compliance\nAccepting Feedback, Accepting Feedback, Accounting and Financial Reporting Standards, Active Listening, Analytical Thinking, Artificial Intelligence (AI) Platform, Auditing, Auditing Methodologies, Business Process Improvement, Communication, Compliance Auditing, Corporate Governance, Creativity, Data Analysis and Interpretation, Data Ingestion, Data Modeling, Data Quality, Data Security, Data Transformation, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Financial Accounting, Financial Audit {+ 24 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['Manager Internal Audit', 'Assurance', 'Financial statements', 'Due diligence', 'Data analysis', 'Financial reporting', 'Corporate governance', 'Risk management', 'financial auditing', 'Monitoring']",2025-06-12 15:15:36
"Quality Engineer, QA",XL India Business Services Pvt. Ltd,2 - 6 years,Not Disclosed,['Gurugram'],"Quality Engineer Bangalore/ Gurgaon, India AXA XL offers risk transfer and risk management solutions to clients globally\n\nWe offer worldwide capacity, flexible underwriting solutions, a wide variety of client-focused loss prevention services and a team-based account management approach\n\nAXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained advantage\n\nOur Chief Data Office is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking an Engineer for the Quality Engineering team\n\nThe Engineer sits next to our Business Partners and tests our AXIOM platform according to our stakeholders needs\n\nWhat you ll be DOING What will your essential responsibilities include? Possess excellent domain knowledge of Data warehousing technologies, SQL, Data Models to develop test strategies, approaches from Quality Engineering perspective\n\nIn close coordination with Project teams help lead all efforts from Quality Engineering perspective\n\nWork with data engineers or data scientists to collect and prepare the necessary test data sets\n\nEnsure the data adequately represents real-world scenarios and covers a diverse range of inputs\n\nExcellent domain knowledge of Data warehousing technologies, SQL, Data Models to build out test strategies and lead projects from Quality Engineering perspective\n\nWith an Automation-first mindset, work towards testing of user interfaces such as Business Intelligence solutions and validation of functionalities while constantly looking out for efficiency gains and process improvements\n\nTriage and Prioritization of stories and epics with all stakeholders to ensure optimal deliveries\n\nEngage with various stakeholders like Business Partners, Product Owners, Development and Infrastructure teams to ensure alignments with overall roadmap\n\nTrack current progress of testing activities, finding and tracking test metrics, estimating and communicating improvement actions based on the test metrics results and the experience\n\nAutomation for processes such as Data Loads, user interfaces such as Business Intelligence solutions and other validations of business KPIs\n\nAdopt and implement best practices towards Documentation of test plan, cases, results in JIRA\n\nTriage and Prioritization of defects with all stakeholders\n\nLeadership accountability for ensuring that every release to customers is fit for purpose, performant\n\nKnowledge on Scaled Agile, Scrum or Kanban methodology\n\nYou will report to Lead UAT\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: A minimum of a bachelor s or masters degree (preferred) in a relevant discipline\n\nRelevant years of excellent testing background, including knowledge/experience in automation\n\nInsurance experience in data, underwriting, claims or operations, including influencing, collaborating, and leading efforts in complex, disparate, and interrelated teams\n\nExcellent Experience with SQL Server, Azure Databricks Notebook, PowerBI, ADLS, CosmosDB, SQL DW Analytics\n\nShould have a robust background in Software development with experience in ingesting, transforming, and storing data from large datasets using Pyspark in Azure Databricks with robust knowledge of distributed computing concepts\n\nHands-on experience in designing and developing ETL Pipelines in Pyspark in Azure Databricks with robust python scripting\n\nDesired Skills and Abilities: Having experience doing UAT/System Integration testing in the insurance industry\n\nExcellent technical testing experience such as API testing, UI automation is a plus\n\nKnowledge/Experience of Testing in cloud-based systems in different data staging layers",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System integration testing', 'Test planning', 'Account management', 'Business strategy', 'Business intelligence', 'Risk management', 'JIRA', 'Analytics', 'SQL', 'Python']",2025-06-12 15:15:38
Jr.AI Engineer,Tekone It Services,1 - 3 years,1.5-6.5 Lacs P.A.,['Hyderabad'],"Position Overview\nWe are hiring five AI Engineers with 12 years of experience to join our dynamic team in Hyderabad. The ideal candidates will have a solid foundation in Large Language Models (LLMs), LangChain, and Generative AI (GenAI) frameworks. This is a great opportunity to work on innovative AI solutions, contributing to projects that integrate LLMs, prompt engineering, RAG pipelines, and cloud-based deployments.\nKey Responsibilities\nContribute to the design and development of AI-powered applications utilizing LLMs (GPT-3.5, GPT-4, Gemini).\nAssist in building LangChain-based pipelines and workflows, including LangSmith and LangGraph.\nSupport the implementation of Retrieval-Augmented Generation (RAG) frameworks using vector databases such as ChromaDB.\nApply prompt engineering techniques to optimize model responses and improve contextual accuracy.\nDevelop RESTful APIs using Flask or FastAPI to enable model consumption in production environments.\nWrite and manage data workflows using SQL, PySpark, and Spark SQL.\nDeploy and monitor models on Azure Machine Learning or AWS Bedrock platforms.\nCollaborate with cross-functional teams, including data scientists, engineers, and business stakeholders.\nRequired Skills\nProficiency in Python, SQL, PySpark, and Spark SQL\nHands-on experience with LLMs: GPT-3.5, GPT-4, Gemini\nKnowledge of LangChain, LangSmith, LangGraph\nFamiliarity with Vector Databases (e.g., ChromaDB) and embeddings\nExperience with prompt engineering and RAG-based architectures\nExposure to cloud platforms such as Azure ML or AWS Bedrock\nStrong understanding of REST APIs and version control systems (Git/GitHub)\nPreferred Qualifications\nBachelor's degree in Computer Science, Artificial Intelligence, Data Science, or a related field\nInternship or academic project experience in NLP, LLMs, or GenAI technologies\nFamiliarity with MLOps tools and practices (e.g., CI/CD, Airflow)\nStrong problem-solving abilities, attention to detail, and a collaborative mindset",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'Prompt Engineering', 'Artificial Intelligence', 'llm']",2025-06-12 15:15:40
Python Developer @ Infosys- Pan India,Infosys,4 - 9 years,Not Disclosed,"['Pune', 'Delhi / NCR', 'Mumbai (All Areas)']","Responsibilities A day in the life of an Infoscion • As part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain. • You will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements. • You will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers. • You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nTechnical and Professional Requirements: • Primary skills: Process->Testing processes->Test Automation Process, Technology->Machine Learning->Python\n\nPreferred Skills: Process->Testing processes->Test Automation Process Technology->Machine Learning->Python\n\nAdditional Responsibilities: • Knowledge of design principles and fundamentals of architecture • Understanding of performance engineering • Knowledge of quality processes and estimation techniques • Basic understanding of project domain • Ability to translate functional / nonfunctional requirements to systems requirements • Ability to design and code complex programs • Ability to write test cases and scenarios based on the specifications • Good understanding of SDLC and agile methodologies • Awareness of latest technologies and trends • Logical thinking and problem solving skills along with an ability to collaborate\n\nEducational Requirements MCA,MSc,MTech,Bachelor of Engineering,BCA,BE,BSc,BTech",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Django', 'Python Development', 'Flask']",2025-06-12 15:15:43
Applied AI Engineer,Xenonstack,3 - 5 years,Not Disclosed,['Mohali'],"XenonStack's Artificial Intelligence team is looking for a Machine Learning (ML) Engineer to help us create artificial intelligence products. Machine Learning Engineer responsibilities include creating machine learning models and retraining systems. To do this job successfully, you need exceptional skills in statistics and programming. Your goal will be to shape and build efficient self-learning applications.\n\n\nKey Responsibilities:\nStudy and transform data science prototypes\nDesign machine learning systems\nResearch and implement appropriate ML algorithms and tools\nDevelop machine learning applications according to requirements\nSelect relevant datasets and data representation methods\nRun machine learning tests and experiments\nPerform statistical analysis and fine-tuning using test results\nTrain and retrain systems when necessary\nExtend existing ML libraries and frameworks\nKeep abreast of developments in the field\n\nRequirements\nTechnical Requirement:\n\nKnowledge in Machine Learning Engineer or similar role\nUnderstanding of data structures, data modelling, and software architecture\nDeep knowledge of maths, probability, statistics, and algorithms\nAbility to write robust code in Python, Java, and R\nFamiliarity with machine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn)\n\nProfessional Attributes:\nExcellent communication skills & Attention to detail.\nAnalytical mind and problem-solving Aptitude with Strong Organizational skills & Visual Thinking.\nEducation: Technical Graduates (BCA, BSC, B. Tech), MCA, MSC, and M.Tech with strong data structure and algorithm Skills \n\nBenefits:\nDynamic and purposeful work culture in a people-oriented organization contributing to multi-million-dollar projects with guaranteed job security.\nOpen, authentic, and transparent communication fostering a warm work environment.\nRegular constructive feedback and exposure to diverse technologies.\nRecognition and rewards for exceptional performance achievements.\nAccess to certification courses & Skill Sessions to develop continually and refine your skills.\nAdditional allowances for team members assigned to specific projects.\nSpecial skill allowances to acknowledge and compensate for unique expertise.\nComprehensive medical insurance policy for your health and well-being.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Programming', 'Machine Learning', 'Tensorflow', 'Algorithm Development', 'Cnn', 'Natural Language Processing', 'Neural Networks', 'Deep Learning', 'Cuda', 'Pytorch', 'Pattern Recognition', 'Rnn', 'Image Processing', 'Keras', 'Data Processing', 'Computer Vision', 'Python']",2025-06-12 15:15:45
Python Engineer,Forbes Global 2000 MNC in Investment Ban...,3 - 8 years,20-22.5 Lacs P.A.,"['Noida', 'Gurugram']","Python/Quant Engineer\nKey Responsibilities:\nDesign, develop, and maintain scalable Python-based quantitative tools and libraries.\nCollaborate with quants and researchers to implement and optimize pricing, risk, and trading models.\nProcess and analyze large datasets (market, fundamental, alternative data) to support research and live trading.\nBuild and enhance backtesting frameworks and data pipelines.\nIntegrate models with execution systems and trading platforms.\nOptimize code for performance and reliability in low-latency environments.\nParticipate in code reviews, testing, and documentation efforts.\nRequired Qualifications:\n3-8 years of professional experience in quantitative development or similar roles.\nProficiency in Python, including libraries like NumPy, Pandas, SciPy, Scikit-learn, and experience in object-oriented programming.\nStrong understanding of data structures, algorithms, and software engineering best practices.\nExperience working with large datasets, data ingestion, and real-time processing.\nExposure to financial instruments (equities, futures, options, FX, fixed income, etc.) and financial mathematics.\nFamiliarity with backtesting, simulation, and strategy evaluation tools.\nExperience with Git, Docker, CI/CD, and modern development workflows.\nPreferred Qualifications:\nPreferred Experience with C++ for performance-critical modules.\nKnowledge of machine learning techniques and tools (e.g., TensorFlow, XGBoost).\nFamiliarity with SQL / NoSQL databases and cloud platforms (AWS, GCP).\nPrior experience in hedge funds, proprietary trading firms, investment banks, or financial data providers.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['OOPS', 'Pandas', 'Numpy', 'Python', 'Data Structures And Algorithms', 'Scipy', 'GIT', 'Docker', 'Ci/Cd', 'Scikit-Learn']",2025-06-12 15:15:48
Chat Bot Developer,Capgemini,2 - 7 years,Not Disclosed,"['Hyderabad', 'Pune', 'Chennai']","Key Responsibilities:\nDesign and develop chatbot solutions using platforms like Dialogflow, Microsoft Bot Framework, Rasa, or similar.\nIntegrate chatbots with messaging platforms (e.g., WhatsApp, Facebook Messenger, Slack, web chat).\nImplement NLP and machine learning techniques to improve chatbot understanding and responses.\nCollaborate with UX/UI designers to create engaging conversational flows.\nConnect chatbots to backend systems, APIs, and databases.\nMonitor chatbot performance and continuously improve based on analytics and user feedback.\nEnsure security, scalability, and reliability of chatbot solutions.\nRequired Skills:\nProficiency in programming languages such as Python, JavaScript, or Node.js.\nExperience with chatbot development platforms (Dialogflow, Rasa, IBM Watson, etc.).\nUnderstanding of NLP concepts and tools (spaCy, NLTK, BERT, etc.).\nFamiliarity with RESTful APIs and webhook integration.\nKnowledge of cloud platforms (AWS, Azure, GCP) is a plus.\nStrong problem-solving and communication skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Bot', 'Chatbot Development', 'Chatbot']",2025-06-12 15:15:50
Software Development Engineer,Appen,4 - 9 years,Not Disclosed,['Hyderabad'],"About Appen\n\nAppen is a leader in AI enablement for critical tasks such as model improvement, supervision, and evaluation. To do this we leverage our global crowd of over one million skilled contractors, speaking over 180 languages and dialects, representing 130 countries. In addition, we utilize the industrys most advanced AI-assisted data annotation platform to collect and label various types of data like images, text, speech, audio, and video.\n\nOur data is crucial for building and continuously improving the worlds most innovative artificial intelligence systems and Appen is already trusted by the worlds largest technology companies. Now with the explosion of interest in generative AI, Appen is helping leaders in automotive, financial services, retail, healthcare, and governments the confidence to deploy world-class AI products.\n\nAt Appen, we are purpose driven. Our fundamental role in AI is to ensure all models are helpful, honest, and harmless, so we firmly believe in unlocking the power of AI to build a better world. We have a learn-it-all culture that values perspective, growth, and innovation. We are customer-obsessed, action-oriented, and celebrate winning together.\n\nAt Appen, we are committed to creating an inclusive and diverse workplace. We are an equal opportunity employer that does not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n\nWe are looking for a self-motivated Software Development Engineer to become part of our innovative team. In this role, you will be responsible for developing high-quality software that will play a key role in shaping the future of AI and machine learning.\nKey Responsibilities:\nCollaborate with our team to design, develop, and maintain software systems and applications.\nWrite clean, maintainable, and efficient code in languages such as Python, Java etc.\nImplement and test software components and ensure that they meet design specifications.\nParticipate in code and design reviews to maintain our high development standards.\nWork closely with other team members to troubleshoot, debug, and improve existing software systems.\nContribute to all phases of the software development lifecycle, from concept to deployment.\nEngage with cross-functional teams to understand and translate business requirements into software solutions.\nQualifications:\nBachelors Degree in Computer Science, Software Engineering, or a related field.\n4+ years of experience in software development.\nProficiency in one or more programming languages such as Java, Python etc.\nWork experience in developing microservices and building RESTful APIs.\nStrong understanding of algorithms and data structures.\nWork experience in designing relational database management systems (RDBMS) or NoSQL databases.\nFamiliarity with AI and machine learning concepts is a plus.\nExcellent problem-solving skills and attention to detail.\nStrong verbal and written communication skills.\nAbility to work effectively in a fast-paced, dynamic environment.\nAppen is the global leader in data for the AI Lifecycle with more than 25 years experience in data sourcing, annotation, and model evaluation. Through our expertise, platform, and global crowd, we enable organizations to launch the world s most innovative artificial intelligence products with speed and at scale. Appen maintains the industry s most advanced AI-assisted data annotation platform and boasts a global crowd of more than 1 million contributors worldwide, speaking more than 235 languages. Our products and services make Appen a trusted partner to leaders in technology, automotive, finance, retail, healthcare, and government. Appen has customers and offices globally.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'NoSQL', 'RDBMS', 'Artificial Intelligence', 'Machine learning', 'Data structures', 'Healthcare', 'Automotive', 'Financial services', 'Python']",2025-06-12 15:15:53
Ai Ml Engineer,Compunnel,4 - 9 years,Not Disclosed,"['Noida', 'Chandigarh']","Company: Compunnel INC\nJob Location: Noida/Chandigarh\nExperience Required: 4+ years\nMode of Work: 5 days work from the office\nJob Title: AI /ML Engineer\n\nWe are seeking a talented and innovative Generative AI Engineer to join our team. The ideal candidate will have expertise in training and testing large language models (LLMs) for applications like speech-to-text and text-to-speech, with a focus on the Hindi language. This role requires proficiency in cutting-edge AI technologies, including transformers, GPU acceleration, and CUDA, along with strong Python programming skills.\n\nKey Responsibilities:\nDesign, train, and fine-tune LLMs for speech-to-text and text-to-speech applications in Hindi.\nDevelop and optimize transformer-based architectures for natural language processing (NLP) tasks.\nLeverage GPU acceleration and CUDA for efficient model training and deployment.\nPre process and manage large datasets to ensure high-quality data for model training.\nCollaborate with cross-functional teams to integrate AI models into production systems.\nConduct rigorous testing and evaluation of models to ensure accuracy, efficiency, and scalability.\nStay updated with the latest advancements in generative AI and NLP technologies.\n\nRequired Skills and Qualifications:\nProficiency in Python and experience with deep learning frameworks like TensorFlow or PyTorch.\nStrong understanding of transformer architectures (e.g., BERT, GPT, T5).\nHands-on experience with GPU acceleration and CUDA programming.\nFamiliarity with Hindi language processing, including phonetics, grammar, and linguistic nuances.\nExperience in developing and deploying speech-to-text and text-to-speech systems.\nKnowledge of data preprocessing techniques for audio and text datasets.\nStrong problem-solving skills and ability to work in a collaborative environment.\n\nPreferred Qualifications:\nExperience with tools like Hugging Face Transformers, Kaldi, or Mozilla Deep Speech.\nFamiliarity with cloud platforms (e.g., AWS, Azure, or Google Cloud) for AI model deployment.\nUnderstanding of end-to-end speech recognition and synthesis pipelines.\nA background in linguistics or computational linguistics is a plus.\n\nPlease fill in all the essential details which are given below & attach your updated resume, and send it to ralish.sharma@compunnel.com\n1. Total Experience:\n2. Relevant Experience in Python :\n3. Experience in Pytorch :\n4. Experience in Tensorflow:\n5. Experience in LLM :\n6. Experience in RAG :\n7. Experience in NLP :\n8. Experience in GPT :\n9. Experience in Bert:\n10 . Experience in CUDA :\n11. Current company :\n12. Current Designation :\n13. Highest Education :\n14. Notice Period:\n15. Current CTC:\n16. Expected CTC:\n17. Current Location:\n18. Preferred Location:\n19. Hometown:\n20. Contact No:\n21. If you have any offer from some other company, please mention the Offer amount and Offer Location:\n22. Reason for looking for change:\n\nIf the job description is suitable for you, please get in touch with me at the number below: 9910044363.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pytorch', 'Tensorflow', 'Bert', 'LLM', 'Python', 'NLP', 'RAG', 'Cuda']",2025-06-12 15:15:55
Devops Engineer,Attentive Ai,2 - 3 years,Not Disclosed,['Noida'],"About Us - Attentive.ai is a fast-growing vertical SaaS start-up, funded by Peak XV (Surge), InfoEdge, Vertex Ventures, and Tenacity Ventures that provides innovative software solutions for the landscape, paving & construction industries in the United States. Our mission is to help businesses in this space improve their operations and grow their revenue through our simple & easy-to-use software platforms.\n\nPosition Description: We are looking for a DevOps Engineer to join our engineering team and help us develop and expand various our internal pipelines and infrastructure As a DevOps Engineer at Attentive, you will be working closely with different engineering, computer vision, testing, and product teams to improve and expand their workflows and cloud resources. We offer an inspiring environment full of young people with a lot of ambition. You get the freedom to implement your own designs, solutions, and creativity\n\nRoles & Responsibilities:\nKnowledge of building and setting up new development tools and infrastructure\nSetup uptime checks, resource health monitoring, and other monitoring tools (Gcp stack-driver, ELK)\nManaging and scaling cloud-based infrastructure\nTroubleshooting and resolving infrastructure issues\nDevelop and integrate solutions for the automation of SDL processes such as automated code checks, tests, deployments, rollbacks, etc\nAutomating the build, test, and release process\nCreating and maintaining documentation for infrastructure and processes\nFollows the established processes and best practices to ensure code quality and security.\n\nRequirements\n2-3 years of work experience as a Cloud & DevOps engineer\nExcellent understanding of Python, Groovy , and bash scripting\nExperience working on Linux-based infrastructure\nExperience working on cloud services like GCP,AWS\nHands-on experience with CICD Tools\nExperienced in deploying a containerized application, static websites deployment, etc\nWorking knowledge of deploying and maintaining tools like Github, JIRA, Jenkins\nExperience in IaC tools Terraform, Ansible\n\nGood To Haves:\nExperience working with serverless application deployments.\nExperience working with source code scanning and dependency management.\nFamiliarity with data management and ML Ops (Machine Learning) process and tools.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer vision', 'Automation', 'github', 'Linux', 'Data management', 'GCP', 'Machine learning', 'Troubleshooting', 'JIRA', 'Python']",2025-06-12 15:15:58
Responsible AI Engineer,Overture Rede,3 - 7 years,Not Disclosed,['Pune'],"Job Role (20 Words)\nEnsure ethical, responsible AI system development by auditing compliance, designing mitigation strategies, and guiding cross-functional teams.\n\nJob Summary\nWe are seeking a seasoned Responsible AI Engineer to lead ethical AI development initiatives. This role focuses on auditing AI systems, advising teams, and embedding Responsible AI frameworks into enterprise solutions, ensuring alignment with regulatory and ethical standards.\n\nRequired Skills\nDeep expertise in Responsible AI and ethical governance\nStrong advisory and decision-making capabilities\nProficiency in Responsible AI frameworks and implementation\nExperience in statistical analysis and ML algorithms\nKnowledge of linear/logistic regression, decision trees, clustering\nSkilled in data preprocessing (cleaning, transformation, normalization)\nFamiliarity with visualization tools like Tableau and Power BI\nAbility to audit AI systems and enforce compliance protocols",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Logistic regression', 'tableau', 'Statistical analysis', 'Compliance', 'power bi', 'System development', 'Advisory', 'Auditing']",2025-06-12 15:16:01
AI/ML Engineer,Oak Tree Cloud Software,3 - 5 years,Not Disclosed,['Indore'],"Job Title: Python Developer AI/ML & Generative AI\nExperience: 3+ Years\nLocation: Indore (WFO)\nEmployment Type: Full-Time\nIndustry: AI / Technology / Software Development\n\nJob Description\nWe are seeking a skilled Python Developer with 3+ years of hands-on experience in Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL) and Generative AI (GenAI). The ideal candidate will be passionate about building intelligent systems and creating real-world applications using modern AI tools and frameworks.\n\nKey Responsibilities:\nDevelop and deploy AI/ML models using Python for real-world use cases including classification, regression, clustering, NLP, and computer vision.\nDesign and fine-tune Generative AI models, including transformers, large language models (LLMs), and text-to-image/audio tools.\nImplement data preprocessing pipelines, feature engineering, and model evaluation metrics.\nCollaborate with cross-functional teams (data scientists, backend developers, and product managers) to integrate models into applications or APIs.\nOptimize and scale ML/AI pipelines for performance and accuracy using techniques like model compression or distributed training.\nWork with GenAI frameworks/tools such as Hugging Face Transformers, LangChain, OpenAI API, or LLaMA.\nPerform research and experimentation with state-of-the-art models and suggest improvements for production use.\nMaintain clear documentation for models, datasets, experiments, and deployment procedures.\nRequired Skills & Qualifications:\nStrong proficiency in Python with focus on data structures, OOPs, and libraries like NumPy, Pandas, Scikit-learn, and Matplotlib.\nExperience with AI/ML frameworks such as TensorFlow, PyTorch, or Keras.\nPractical knowledge of Gen AI tools, including Hugging Face Transformers, OpenAI GPT models, or LLM fine-tuning.\nUnderstanding of ML lifecycle, from data cleaning to model deployment and monitoring.\nDevelop and implement data extraction pipelines for unstructured documents using OCR techniques and libraries (e.g., Tesseract, EasyOCR) to extract text and structured data from PDFs (PyMuPDF), scanned images, and DOCX files.\nExperience with NLP techniques, text generation, summarization, or vector embeddings.\nHands-on experience with REST APIs, FastAPI or Flask to deploy and serve models.\nFamiliarity with version control (Git), CI/CD pipelines, and containerization (Docker).\nBachelor's/Master’s in Computer Science, Artificial Intelligence, Data Science, or related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'Machine Learning', 'Deep Learning', 'Python']",2025-06-12 15:16:03
Manager - Software Development,Amway,8 - 12 years,Not Disclosed,['Hyderabad'],"Primary Responsibilities\nCloud Expertise: Familiarity or hands-on experience with AWS and Google Cloud Platform (GCP) technologies to support data transformation, data structures, metadata management, dependency tracking, and workload orchestration.\nCollaboration & Independence: Self-motivated and capable of supporting the data needs of multiple teams, systems, and products within Amways data ecosystem.\nBig Data & Distributed Systems: Strong understanding of distributed systems for large-scale data processing and analytics, with a proven track record of manipulating, processing, and deriving insights from large, complex, and disconnected datasets.",,,,"['Data Transformation', 'GCP', 'Cloud', 'AWS']",2025-06-12 15:16:06
Senior Analyst Programmer- Platform Engineering,Fidelity International,5 - 7 years,Not Disclosed,['Gurugram'],"Title Senior Analyst Programmer- Platform Engineering\nDepartment FIL India Technology - ISS Tech\nLocation Gurgaon, India\nLevel 3\nWe re proud to have been helping our clients build better financial futures for over 50 years. How have we achieved this? By working together - and supporting each other - all over the world. So, join our ISS team and feel like you re part of something bigger.\nAbout your team\nInvestment Management Technology provides systems development, implementation and support services for our global\nInvestment Management division. We support Fund Managers, Research Analysts and Traders in all of our international\nlocations, including London, Hong Kong, Ireland & Tokyo.\nAbout your role\nCRD delivery team needs highly motivated self-driven Analyst Programmer to provide Platform Support. The CRD platform consists of the Charles River product, CRD Integration Layer, PaaS and Kubernetes Services. CRD Platform is Fidelity s core trading platform, used by Portfolio Managers, Traders, Compliance and Post Trade.\nThe core elements of the role are as follows:\nPlatform Engineering - Primary objective of platform engineering is to focus on future planning and design of platform to maintain long term sustainability and supportability.\nNon-Production Incident management - Troubleshoot non-production issues and find root cause through analysis.\nNon-Production Support & Operations - Perform routine operational tasks such as critical batch monitoring, morning checks on application s readiness for business use, health check reports, maintenance etc\nProblem management & Change management - Identify and drive the changes required to bring stability on non-prod environments; Participate in Application releases, Infrastructure changes, Preventive maintenance activities like DR role swaps.\nAbout you\nSeasoned IT software delivery professional with an experience of 5+ years of relevant industry experience in supporting IT applications.\nHands on experience on Unix scripting, Oracle & SQLServer, scheduling tools - Autosys and Control-M, IBM MQ, Kubernetes and Python.\nUnderstanding of DevOps concepts, Jenkins, Urban Deploy, JIRA and Power BI.\nKnowlege of Financial Domain (Investment Banking / Wealth Management) and understanding of Fixed Income and Equity Trading, Trade flow and Fund Management and FIX connectivity and infrastructure.\nFeel rewarded",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Change management', 'Production support', 'Fixed income', 'Problem management', 'Incident management', 'Investment banking', 'Oracle', 'JIRA', 'Monitoring', 'Python']",2025-06-12 15:16:08
Responsible AI Engineer,Overture Rede,15 - 20 years,Not Disclosed,['Pune'],": 15 Years Full-Time\n\nJob Role\nEnsure ethical, responsible AI system development by auditing compliance, designing mitigation strategies, and guiding cross-functional teams.\n\nJob Summary\nWe are seeking a seasoned Responsible AI Engineer to lead ethical AI development initiatives. This role focuses on auditing AI systems, advising teams, and embedding Responsible AI frameworks into enterprise solutions, ensuring alignment with regulatory and ethical standards.\n\nRequired Skills\nDeep expertise in Responsible AI and ethical governance\nStrong advisory and decision-making capabilities\nProficiency in Responsible AI frameworks and implementation\nExperience in statistical analysis and ML algorithms\nKnowledge of linear/logistic regression, decision trees, clustering\nSkilled in data preprocessing (cleaning, transformation, normalization)\nFamiliarity with visualization tools like Tableau and Power BI\nAbility to audit AI systems and enforce compliance protocols",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Logistic regression', 'tableau', 'Statistical analysis', 'Compliance', 'power bi', 'System development', 'Advisory', 'Auditing']",2025-06-12 15:16:10
Ai Ml Engineer,Openeyes Software Solutions,8 - 10 years,Not Disclosed,['Vadodara'],"Location : Vadodara (Onsite only)\n\nJob Summary:\nWe are looking for a highly skilled and experienced AI/ML Engineer with 8 to 10 years of relevant experience in developing end-to-end machine learning solutions. The ideal candidate will be well-versed in deep learning, natural language processing (NLP), computer vision, and large language models (LLMs). You will be responsible for designing, developing, and deploying scalable AI models tailored to real-world business use cases.\n\nKey Responsibilities:\n\nDesign, train, evaluate, and deploy machine learning and deep learning models aligned with business goals.\nWork with various ML tasks such as classification, regression, and clustering.\nHandle NLP, LLMs, computer vision, and speech-to-text use cases.\nImplement and fine-tune models like LLaMA, Falcon, BERT, T5 Transformer, and Hugging Face models.\nUse libraries such as NumPy, Pandas, Matplotlib, SpaCy, Scikit-learn, TensorFlow, and PyTorch.\nWrite clean, efficient Python code for data processing and model development.\nUtilize tools like Google Colab, Jupyter Notebook, and AWS SageMaker for experimentation and deployment.\nMonitor data drift and automate model retraining pipelines as required.\nDeploy AI models on cloud infrastructure (preferably AWS) using services like SageMaker, EC2, ECS, and Kubernetes.\nCollaborate with cross-functional teams to translate complex problems into AI solutions.\nGood to Have:\n\nExperience working with LangChain or LLaMAIndex.\nFamiliarity with RAG-based (Retrieval-Augmented Generation) application development.\nHands-on experience with Google Cloud Platform (GCP) for model deployment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Natural Language Processing', 'Machine Learning', 'Deep Learning', 'Python', 'Communication Skills', 'Artificial Intelligence', 'Pandas', 'Problem Solving', 'Aws Sagemaker', 'Numpy']",2025-06-12 15:16:13
Automation Engineer,Pentair,3 - 8 years,Not Disclosed,['Noida'],"Position Title: Automation Engineer Connected Factory\n\nPosition Summary:\nPentair is currently seeking an Automation Engineer to work for Connected Factory\nSolution with Public/Private Cloud. This individual must be well-versed in the latest\ntechnologies in IoT 4.0 and digital space. This role is responsible for end-to-end design\nprovisioning and commissioning.\n\nJob Summary:\nThe Automation Engineer will play a critical role in designing, developing, and maintaining\nautomation systems, controls, and instrumentation for various industrial processes. This\nposition requires expertise in electrical engineering, automation technologies, a deep\nunderstanding of MES systems, ERP, SCADA softwares and control systems like PLC, HMI,\nVFD, CNC, IM,VMC etc.\n\nKey Responsibilities:\nAutomation System Design: Design, develop, and implement electrical automation\nsystems and controls for industrial processes, machinery, and manufacturing\nequipment.\nPLC Programming: Develop and maintain PLC (Programmable Logic Controller)\nprograms to automate and control processes efficiently. Troubleshoot PLC-related\nissues.\nHMI (Human-Machine Interface) Development: Design and create user-friendly\nHMI interfaces for operators to monitor and control automation systems.\nSensor Integration: Select, install, and calibrate various sensors, transducers, and\ninstruments to gather data and provide feedback for control systems.\nElectrical Panel Design: Design and oversee the construction of electrical control\npanels, ensuring they comply with safety and regulatory standards.\nNetworking and Communication: Establish communication protocols between\ndifferent devices, controllers, and systems, including Routing, NAT, Ethernet IP,\nModbus TCP, OPC, RS-232, RS-485 and Profinet.\nIT/OT Connectivity: Ignition Edge gateway, Ignition Cloud Edition, KepServer and\nEdge devices.\nTesting and Commissioning: Conduct testing and commissioning of automation\nsystems(FAT & SAT), ensuring they operate efficiently and meet performance\nspecifications.\nDocumentation: Create and maintain detailed documentation, including electrical\nschematics, wiring diagrams, and system manuals.\nTroubleshooting: Diagnose and resolve electrical and automation-related issues,\nboth in the design phase and during system operation.\nMaintenance and Upgrades: Perform routine maintenance and recommend system\nupgrades to improve reliability and efficiency.\nEnterprise : Experience on MES and ERP (Sepasoft,SAP,etc.) systems.\nSafety and Compliance: Ensure that all automation systems adhere to safety\nregulations and industry standards.\nCollaboration: Collaborate with cross-functional teams, including mechanical\nengineers, software developers, and project managers, to integrate automation\nsolutions into larger projects.\n\nQualifications:\nDiploma/Bachelor's degree in Electrical Engineering, Automation, or a related field.\n3-5 years of experience in electrical automation system design and\nimplementation.\nProfessional certifications related to automation (e.g., ISA Certified Automation\nProfessional).\n\nExperience:\nMust be have experience in Ignition and Sepasoft SCADA.\nExperience with PLC Programming (e.g., Siemens, Allen-Bradley, ABB,RS-Logic 500,\nRS-Logic 5000, TIA Portal, Automation Builder)\nExperience with SCADA software (Supervisory Control and Data Acquisition)\nsystems (Ignition, WinCC, FactoryTalkView, Proficy historian)\nExperience with Injection moulding, motion control systems and robotics\n(Roboshot Link-i).\nExperience in HMI programming tools and software (e.g., Siemens, Allen-Bradley,\nABB).\nExperience in SCADA programming (e.g.,Ignition, WinCC, RSView, Proficy historian)\nExperience in Machine Connectivity(Fanuc , Siemens, ABB, Robots, CNC and IM\nmachines).\nStrong understanding of ISA-95 model, electrical and control system design\nprinciples.\nExcellent problem-solving and troubleshooting skills.\nEffective communication and teamwork abilities.\nKnowledge of regulatory & safety standards and regulations (e.g., UL, CE, NFPA\n70E, UL-508).\nExperience with industrial networking and communication protocols with IT/OT\nconnectivity.\nExperience with electrical switchgear and control instruments.\nExposure on Electrical Panel Design: Design and develop electrical panel\ncomponents and select the right switchgears, power supplies, and control systems,\nadhering to industry standards and best practices.\nKnowledge of advanced control algorithms and machine learning for automation.\nKnowledge of Project management skills.\n\nSkills and Abilities Required:\nCan-do positive attitude, always looking to accelerate development.\nDriven; commit to high standards of performance and demonstrate personal\nownership for getting the job done.\nInnovative and entrepreneurial attitude; stays up to speed on all the latest\ntechnologies and industry trends; healthy curiosity to evaluate, understand and\nutilize new technologies.\nAbility to learn and adapt innovative solutions.\nMust be ready to work on multiple timezone.\nMust be able to contribute to the technology team while managing multiple tasks\nand responsibilities.\nExcellent communication and presentation skills for interactions with technology\nglobal team members, SBU stakeholders, company leadership, vendors and\ncustomers.",Industry Type: Emerging Technologies (IoT),"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['sepasoft', 'PLC', 'Ignition Scada', 'IOT', 'HMI', 'Automation Engineering']",2025-06-12 15:16:50
AI Infrastructure Engineer (DevOps/MLOps),TechVantage,5 - 10 years,Not Disclosed,['Thiruvananthapuram'],"is a next-generation technology and product engineering company at the forefront of innovation in Generative AI, Agentic AI , and autonomous intelligent systems . We build intelligent, cutting-edge solutions designed to scale and evolve with the future of artificial intelligence.\nRole Overview:\nWe are looking for a skilled and versatile AI Infrastructure Engineer (DevOps/MLOps) to build and manage the cloud infrastructure, deployment pipelines, and machine learning operations behind our AI-powered products. You will work at the intersection of software engineering, ML, and cloud architecture to ensure that our models and systems are scalable, reliable, and production-ready.\n\nWhat we are looking from an ideal candidate?\nDesign and manage CI/CD pipelines for both software applications and machine learning workflows.\nDeploy and monitor ML models in production using tools like MLflow, SageMaker, Vertex AI, or similar.\nAutomate the provisioning and configuration of infrastructure using IaC tools (Terraform, Pulumi, etc.).\nBuild robust monitoring, logging, and alerting systems for AI applications.\nManage containerized services with Docker and orchestration platforms like Kubernetes .\nCollaborate with data scientists and ML engineers to streamline model experimentation, versioning, and deployment.\nOptimize compute resources and storage costs across cloud environments (AWS, GCP, or Azure).\nEnsure system reliability, scalability, and security across all environments.\n\nPreferred Skills:\nWhat skills do you need?\n5+ years of experience in DevOps, MLOps , or infrastructure engineering roles.\nHands-on experience with cloud platforms ( AWS, GCP, or Azure ) and services related to ML workloads.\nStrong knowledge of CI/CD tools (e.g., GitHub Actions, Jenkins, GitLab CI).\nProficiency in Docker , Kubernetes , and infrastructure-as-code frameworks.\nExperience with ML pipelines , model versioning, and ML monitoring tools.\nScripting skills in Python , Bash , or similar for automation tasks.\nFamiliarity with monitoring/logging tools (Prometheus, Grafana, ELK, CloudWatch, etc.).\nUnderstanding of ML lifecycle management and reproducibility.\nPreferred Qualifications:\nExperience with Kubeflow , MLflow , DVC , or Triton Inference Server .\nExposure to data versioning , feature stores , and model registries .\nCertification in AWS/GCP DevOps or Machine Learning Engineering is a plus.\nBackground in software engineering, data engineering, or ML research is a bonus.\nWhat We Offer:\nWork on cutting-edge AI platforms and infrastructure\nCross-functional collaboration with top ML, research, and product teams\nCompetitive compensation package no constraints for the right candidate",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Product engineering', 'orchestration', 'GCP', 'devops', 'Artificial Intelligence', 'Machine learning', 'Infrastructure', 'AWS', 'Python']",2025-06-12 15:16:53
Senior High Performance Computing Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will.\nRole Description:\nThe role is responsible for the design, integration, and management of high performance computing (HPC) systems that encompass both hardware and software components into the organizations network infrastructure. This individual will be responsible for all activities related to handling and supporting the Business and platforms including system administration, as well as incorporating new technologies under the challenge of a sophisticated and constantly evolving technology landscape. This role involves ensuring that all parts of a system work together seamlessly to meet the organizations requirements.\nRoles & Responsibilities:\nImplement, and manage cloud-based infrastructure that supports HPC environments that support data science (e.g. AI/ML workflows, Image Analysis).\nCollaborate with data scientists and ML engineers to deploy scalable machine learning models into production.\nEnsure the security, scalability, and reliability of HPC systems in the cloud.\nOptimize cloud resources for cost-effective and efficient use.\nKeep abreast of the latest in cloud services and industry standard processes.\nProvide technical leadership and guidance in cloud and HPC systems management.\nDevelop and maintain CI/CD pipelines for deploying resources to multi-cloud environments.\nMonitor and fix cluster operations/applications and cloud environments.\nDocument system design and operational procedures.\nBasic Qualifications:\nMasters degree with a 4 - 6 years of experience in Computer Science, IT or related field with hands-on HPC administration OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT or related field with hands-on HPC administration OR\nDiploma with 10-12 years of experience in Computer Science, IT or related field with hands-on HPC administration\nDemonstrable experience in cloud computing (preferably AWS) and cloud architecture.\nExperience with containerization technologies (Singularity, Docker) and cloud-based HPC solutions.\nExperience with infrastructure-as-code (IaC) tools such as Terraform, CloudFormation, Packer, Ansible and Git.\nExpert with scripting (Python or Bash) and Linux/Unix system administration (preferably Red Hat or Ubuntu).\nProficiency with job scheduling and resource management tools (SLURM, PBS, LSF, etc.).\nKnowledge of storage architectures and distributed file systems (Lustre, GPFS, Ceph).\nUnderstanding of networking architecture and security best practices.\nPreferred Qualifications:\nExperience supporting research in healthcare life sciences.\nExperience with Kubernetes (EKS) and service mesh architectures.\nKnowledge of AWS Lambda and event-driven architectures.\nExposure to multi-cloud environments (Azure, GCP).\nFamiliarity with machine learning frameworks (TensorFlow, PyTorch) and data pipelines.\nCertifications in cloud architecture (AWS Certified Solutions Architect, Google Cloud Professional Cloud Architect, etc.).\nExperience in an Agile development environment.\nPrior work with distributed computing and big data technologies (Hadoop, Spark).\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nRed Hat Certified Engineer (RHCE) or Linux Professional Institute Certification (LPIC)\nAWS Certified Solutions Architect Associate or Professional\nSoft Skills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced, cloud-first environment.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud computing', 'resource management', 'Ubuntu', 'Unix system administration', 'linux', 'unix production support', 'Python']",2025-06-12 15:16:55
Senior High Performance Computing Engineer,Amgen Inc,6 - 8 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will be responsible for deploying, maintaining and supporting HPC infrastructure in a multi-cloud environment. Hands-on engineering which requires\n\ndeep technical expertise in HPC technology and standard methodologies.\nImplement, and manage cloud-based infrastructure that supports HPC environments that support data science (e.g. AI/ML workflows, Image Analysis).\nCollaborate with data scientists and ML engineers to deploy scalable machine learning models into production.\nEnsure the security, scalability, and reliability of HPC systems in the cloud.\nOptimize cloud resources for cost-effective and efficient use.\nStay ahead of with the latest in cloud services and industry standard processes.\nProvide technical leadership and guidance in cloud and HPC systems management.\nDevelop and maintain CI/CD pipelines for deploying resources to multi-cloud environments.\nMonitor and fix cluster operations/applications and cloud environments.\nDocument system design and operational procedures.\n\n\n\nMust-Have\n\nSkills:\nExpert with Linux/Unix system administration (RHEL, CentOS, Ubuntu, etc.).\nProficiency with job scheduling and resource management tools (SLURM, PBS, LSF, etc.).\nGood understanding of parallel computing, MPI, OpenMP, and GPU acceleration (CUDA, ROCm).\nKnowledge of storage architectures and distributed file systems (Lustre, GPFS, Ceph).\nExperience with containerization technologies (Singularity, Docker) and cloud-based HPC solutions.\nExpert in scripting languages (Python, Bash) and containerization technologies (Docker, Kubernetes).\nFamiliarity with automation tools (Ansible, Puppet, Chef) for system provisioning and maintenance.\nUnderstanding of networking protocols, high-speed interconnects, and security best practices.\nDemonstrable experience in cloud computing (AWS, Azure, GCP) and cloud architecture.\nExperience with infrastructure as code (IaC) tools like Terraform or CloudFormation and Git.\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. Expert knowledge in\n\nlarge Linux environments, networking, storage, and cloud related technologies. Also, the candidate will have\n\nexpertise in root-cause analysis and fix while working with a team and stakeholders.\n\nTop-level communication and documentation skills are required.\n\nExpertise in coding in\n\nPython, Bash, YAML is expected.\n\n\n\nGood-to-Have\n\nSkills:\nExperience with Kubernetes (EKS) and service mesh architectures.\nKnowledge of AWS Lambda and event-driven architectures.\nFamiliarity with AWS CDK, Ansible, or Packer for cloud automation.\nExposure to multi-cloud environments (Azure, GCP).\nBasic Qualifications:\nBachelors degree in computer science, IT, or related field with 6-8 years of hands-on HPC administration or a related field.\n\n\n\nProfessional Certifications (preferred):\nRed Hat Certified Engineer (RHCE) or Linux Professional Institute Certification (LPIC)\nAWS Certified Solutions Architect Associate or Professional\nPreferred Qualifications:\n\n\n\nSoft\n\nSkills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced, cloud-first environment.\nShift Information: This position is required to be onsite and participate in 24/5 and weekend on call in rotation fashion and may require you to work a later shift. Candidates must be willing and able to work off hours, as required based on business requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance Computing', 'python', 'cloud architecture', 'linux', 'bash', 'networking', 'linux internals', 'cloud computing', 'scripting languages']",2025-06-12 15:16:58
Principal Machine Learning Engineer,Paypal,0 - 7 years,Not Disclosed,['Bengaluru'],"The Company\nPayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.\nWe operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.\nOur beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do - and they push us to ensure we take care of ourselves, each other, and our communities.\nJob Summary:\nWhat you need to know about the role\n\nThis job will drive the strategic vision and development of cutting-edge machine learning models and algorithms to solve complex problems. You will work closely with data scientists, software engineers, and product teams to enhance services through innovative AI/ML solutions. Your role will involve building scalable ML pipelines, ensuring data quality, and deploying models into production environments to drive business insights and improve customer experiences.\n\nYour Way to Impact\n\nAs a Principal Machine Learning Engineer, you ll lead mission-critical initiatives that define PayPal s AI edge from large-scale model fine-tuning to the architecture of foundational AI systems. Your leadership will enable AI-native capabilities across personalization, fraud detection, customer experience, and internal productivity. You will shape how PayPal delivers trust, speed, and intelligence in every user interaction.\n\nMeet Our Team\nYou ll work within the core Applied Intelligence team, a cross-functional hub driving AI-first innovation across PayPal s product and platform landscape. Your team s work powers smarter workflows and more seamless experiences for our global customer base. This is a hands-on leadership role where you ll help align strategy, technology, and business outcomes.\nJob Description:\nYour Day to Day\nDefine and drive strategic vision for model development and ML applications across business domains.\nLead architecture and experimentation for foundational model pipelines.\nManage end-to-end lifecycle from data prep and training to deployment and monitoring.\nCollaborate with product, infra, and engineering leaders to ship impactful solutions.\nGuide model evaluation frameworks, bias detection, and performance monitoring practices.\nMentor technical leads and contribute to thought leadership internally and externally.\nWhat You Need to Bring\nMinimum of 15 years of relevant experience with a Bachelor s degree or equivalent.\nDeep expertise in building and fine-tuning advanced ML models at scale.\nStrong experience with cloud-native ML solutions (e.g., SageMaker, Vertex AI).\nProven success in leading multi-functional ML projects from research to production.\nStrong communication and strategic planning abilities to align tech with business.\nPreferred Qualification:\nSubsidiary:\nPayPal\nTravel Percent:\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits:\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com .\nWho We Are:\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at talentaccommodations@paypal.com .\nBelonging at PayPal:\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: FinTech / Payments,Department: Other,"Employment Type: Full Time, Permanent","['Architecture', 'User interaction', 'Diversity and Inclusion', 'Machine learning', 'Strategic planning', 'Manager Technology', 'Wellness', 'Data quality', 'Customer experience', 'Fraud detection']",2025-06-13 06:11:24
"Engineer, Principal/Manager - Machine Learning, AI",Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"General Summary:\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Systems Engineer, you will research, design, develop, simulate, and/or validate systems-level software, hardware, architecture, algorithms, and solutions that enables the development of cutting-edge technology. Qualcomm Systems Engineers collaborate across functional teams to meet and exceed system-level requirements and standards.\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 8+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 7+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 6+ years of Systems Engineering or related work experience.\nPrincipal Engineer Machine Learning\nWe are looking for a Principal AI/ML Engineer with expertise in model inference, optimization, debugging, and hardware acceleration. This role will focus on building efficient AI inference systems, debugging deep learning models, optimizing AI workloads for low latency, and accelerating deployment across diverse hardware platforms.\nIn addition to hands-on engineering, this role involves cutting-edge research in efficient deep learning, model compression, quantization, and AI hardware-aware optimization techniques. You will explore and implement state-of-the-art AI acceleration methods while collaborating with researchers, industry experts, and open-source communities to push the boundaries of AI performance.\nThis is an exciting opportunity for someone passionate about both applied AI development and AI research, with a strong focus on real-world deployment, model interpretability, and high-performance inference.\nEducation & Experience:\n20+ years of experience in AI/ML development, with at least 5 years in model inference, optimization, debugging, and Python-based AI deployment.\nMasters or Ph.D. in Computer Science, Machine Learning, AI\nLeadership & Collaboration\nLead a team of AI engineers in Python-based AI inference development.\nCollaborate with ML researchers, software engineers, and DevOps teams to deploy optimized AI solutions.\nDefine and enforce best practices for debugging and optimizing AI models\nKey Responsibilities\nModel Optimization & Quantization\nOptimize deep learning models using quantization (INT8, INT4, mixed precision etc), pruning, and knowledge distillation.\nImplement Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) for deployment.\nFamiliarity with TensorRT, ONNX Runtime, OpenVINO, TVM\nAI Hardware Acceleration & Deployment\nOptimize AI workloads for Qualcomm Hexagon DSP, GPUs (CUDA, Tensor Cores), TPUs, NPUs, FPGAs, Habana Gaudi, Apple Neural Engine.\nLeverage Python APIs for hardware-specific acceleration, including cuDNN, XLA, MLIR.\nBenchmark models on AI hardware architectures and debug performance issues\nAI Research & Innovation\nConduct state-of-the-art research on AI inference efficiency, model compression, low-bit precision, sparse computing, and algorithmic acceleration.\nExplore new deep learning architectures (Sparse Transformers, Mixture of Experts, Flash Attention) for better inference performance.\nContribute to open-source AI projects and publish findings in top-tier ML conferences (NeurIPS, ICML, CVPR).\nCollaborate with hardware vendors and AI research teams to optimize deep learning models for next-gen AI accelerators.\nDetails of Expertise:\nExperience optimizing LLMs, LVMs, LMMs for inference\nExperience with deep learning frameworks: TensorFlow, PyTorch, JAX, ONNX.\nAdvanced skills in model quantization, pruning, and compression.\nProficiency in CUDA programming and Python GPU acceleration using cuPy, Numba, and TensorRT.\nHands-on experience with ML inference runtimes (TensorRT, TVM, ONNX Runtime, OpenVINO)\nExperience working with RunTimes Delegates (TFLite, ONNX, Qualcomm)\nStrong expertise in Python programming, writing optimized and scalable AI code.\nExperience with debugging AI models, including examining computation graphs using Netron Viewer, TensorBoard, and ONNX Runtime Debugger.\nStrong debugging skills using profiling tools (PyTorch Profiler, TensorFlow Profiler, cProfile, Nsight Systems, perf, Py-Spy).\nExpertise in cloud-based AI inference (AWS Inferentia, Azure ML, GCP AI Platform, Habana Gaudi).\nKnowledge of hardware-aware optimizations (oneDNN, XLA, cuDNN, ROCm, MLIR, SparseML).\nContributions to open-source community\nPublications in International forums conferences journals",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'AWS Inferentia', 'Azure ML', 'AI/ML', 'ONNX Runtime', 'OpenVINO', 'GCP AI', 'TVM', 'XLA', 'MLIR', 'TensorRT', 'Python']",2025-06-13 06:11:26
Principal Machine Learning Engineer,Xoom,15 - 18 years,Not Disclosed,['Bengaluru'],"Job Summary\nWhat you need to know about the role\n\nThis job will drive the strategic vision and development of cutting-edge machine learning models and algorithms to solve complex problems. You will work closely with data scientists, software engineers, and product teams to enhance services through innovative AI/ML solutions. Your role will involve building scalable ML pipelines, ensuring data quality, and deploying models into production environments to drive business insights and improve customer experiences.\n\nYour Way to Impact\n\nAs a Principal Machine Learning Engineer, you ll lead mission-critical initiatives that define PayPal s AI edge from large-scale model fine-tuning to the architecture of foundational AI systems. Your leadership will enable AI-native capabilities across personalization, fraud detection, customer experience, and internal productivity. You will shape how PayPal delivers trust, speed, and intelligence in every user interaction.\n\nMeet Our Team\nYou ll work within the core Applied Intelligence team, a cross-functional hub driving AI-first innovation across PayPal s product and platform landscape. Your team s work powers smarter workflows and more seamless experiences for our global customer base. This is a hands-on leadership role where you ll help align strategy, technology, and business outcomes.\nJob Description\nYour Day to Day\nDefine and drive strategic vision for model development and ML applications across business domains.\nLead architecture and experimentation for foundational model pipelines.\nManage end-to-end lifecycle from data prep and training to deployment and monitoring.\nCollaborate with product, infra, and engineering leaders to ship impactful solutions.\nGuide model evaluation frameworks, bias detection, and performance monitoring practices.\nMentor technical leads and contribute to thought leadership internally and externally.\nWhat You Need to Bring\nMinimum of 15 years of relevant experience with a Bachelor s degree or equivalent.\nDeep expertise in building and fine-tuning advanced ML models at scale.\nStrong experience with cloud-native ML solutions (e.g., SageMaker, Vertex AI).\nProven success in leading multi-functional ML projects from research to production.\nStrong communication and strategic planning abilities to align tech with business.\nPreferred Qualification\nSubsidiary\nPayPal\nTravel Percent\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https//www.paypalbenefits.com .\nWho We Are\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. .\nBelonging at PayPal\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Architecture', 'User interaction', 'Diversity and Inclusion', 'Machine learning', 'Strategic planning', 'Manager Technology', 'Wellness', 'Data quality', 'Customer experience', 'Fraud detection']",2025-06-13 06:11:27
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Bengaluru'],"Key Responsibilities :\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n- Experience with imbalanced datasets and techniques to improve model generalization.\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n- Strong mathematical and statistical knowledge with problem-solving skills.\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n- Experience in automating ML pipelines with MLOps practices.\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'Azure', 'MLOps', 'GCP', 'Big Data', 'Neural Networks', 'AWS', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-13 06:11:29
Machine Learning Engineer,Tek Ninjas,8 - 13 years,Not Disclosed,['Pune'],"Skills:\n     Proficient:\nLanguages/Framework: Fast API, Azure UI Search API (React)\nCloud: Azure Cloud Basics (Azure DevOps)\nGitlab: Gitlab Pipeline\nAnsible and REX: Rex Deployment\nData Science: Prompt Engineering + Modern Testing\nData pipeline development\nUnderstanding of AI/ML algorithms and their applications\nMLOps frameworks\nKnowledge of cloud platforms (Azure ML especially)\nModel deployment process\nData pipeline monitoring\nLanguages/Framework: Azure Open AI\nData Science: Open AI GPT Family of models 4o/4/3, Embeddings + Vector Search\nDatabases and ETL: Azure Storage Account, Postgresql, Cosmos\nExperience with ML frameworks (TensorFlow, PyTorch, Scikit-learn)\nKnowledge of cloud platforms (AWS SageMaker, Google AI Platform)\nExpertise in data preprocessing, feature engineering, and model evaluation\nUnderstanding of software engineering principles (version control, CI/CD, containerization)\nFamiliarity with distributed computing and big data tools (Spark, Hadoop)\nAbility to optimize models for performance and scalability\nExperience with Azure AI Search\nDesired skills*\nAzure DevOps; MLOps frameworks; Postgresql; Cosmos",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['MLOPS', 'Aiml', 'Machine Learning', 'Azure Devops']",2025-06-13 06:11:31
Machine Learning Ops Engineer,Quadrangle,6 - 11 years,Not Disclosed,"['Pune', 'Bengaluru', 'Delhi / NCR']","Location: Bangalore/Noida/Pune/Gurgaon\nEducation: B.E. / B. Tech / M.E. / M. Tech / MCA\nJob Responsibilities:\nModel Deployment and Management:\nDrive ML prototypes into production ensuring seamless deployment and management on cloud at scale.\nMonitor real-time performance of deployed models, analyze data, and proactively address performance issues.\nTroubleshoot and resolve production issues related to ML model deployment, performance, and scalability.\nCollaboration and Integration:\nCollaborate with DevOps engineers to manage cloud compute resources for ML model deployment and performance optimization.\nWork closely with ML scientists, software engineers, data engineers, and other stakeholders to implement best practices for MLOps, including CI/CD pipelines, version control, model versioning, and automated deployment.\nInnovation and Continuous Improvement:\nStay updated with the latest advancements in MLOps technologies and recommend new tools and techniques.\nContribute to the continuous improvement of team processes and workflows.\nShare knowledge and expertise to promote a collaborative learning environment.\nDevelopment and Documentation:\nBuild software to run and support machine-learning models.\nDevelop and maintain documentation, standard operating procedures, and guidelines related to MLOps processes.\nParticipate in fast iteration cycles and adapt to evolving project requirements.\nBusiness Solutions and Strategy:\nPropose solutions and strategies to business challenges.\nCollaborate with Data Science team, Front End Developers, DBA, and DevOps teams to shape architecture and detailed designs.\nMentorship:\nConduct code reviews and mentor junior team members.\nFoster strong interpersonal skills, excellent communication skills, and collaboration skills within the team.\nMandatory Skills:\nProgramming Languages: Proficiency in Python (3.x) and SQL.\nML Frameworks and Libraries: Extensive knowledge of ML frameworks, libraries, data structures, data modeling, and software architecture.\nDatabases: Proficiency in SQL and NoSQL databases.\nMathematics and Algorithms: In-depth knowledge of mathematics, statistics, and algorithms.\nML Modules and REST API: Proficient with ML modules and REST API.\nVersion Control: Hands-on experience with version control applications (GIT).\nModel Deployment and Monitoring: Experience with model deployment and monitoring.\nData Processing: Ability to turn unstructured data into useful information (e.g., auto-tagging images, text-to-speech conversions).\nProblem-Solving: Analytically agile with strong problem-solving capabilities.\nLearning Agility: Quick to learn new concepts and eager to explore and build new features.\nQualifications:\nEducation: Bachelors or Master’s degree in Computer Science, Data Science, or a related field.\nExperience: Minimum of 6 years of hands-on experience in MLOps, deploying and managing machine learning models in production environments, preferably in cloud-based environments.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MLOPS', 'ML operations', 'ML ops']",2025-06-13 06:11:33
Machine Learning Engineer,Avani Infosoft,1 - 2 years,2.4-6.6 Lacs P.A.,['Bengaluru( Kamakshipalya )'],"Responsibilities:\n* Develop machine learning models using TensorFlow, NumPy & OpenCV.\n* Implement computer vision solutions with CNNs & object detection techniques.\n\n\nProvident fund",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Handling', 'Object Detection', 'Opencv', 'Deployment', 'Model Development', 'Tensorflow', 'Cnn', 'Computer Vision', 'Machine Learning', 'Numpy', 'Deep Learning']",2025-06-13 06:11:34
Data Scientist - Python / Machine Learning,Blueberry Unicorn Services,6 - 11 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Working Hours : 2PM to 11PM IST\n\nMid-Level ML Engineers / Data Scientist Role : (4-5 years of experience )\n\n- Experience processing, filtering, and presenting large quantities (100K to Millions of rows) of data using Pandas and PySpark\n\n- Experience with statistical analysis, data modeling, machine learning, optimizations, regression modeling and forecasting, time series analysis, data mining, and demand modeling.\n\n- Experience applying various machine learning techniques and understanding the key parameters that affect their performance.\n\n- Experience with Predictive analytics (e.g., forecasting, time-series, neural networks) and Prescriptive analytics (e.g., stochastic optimization, bandits, reinforcement learning).\n\n- Experience with Python and Python packages like NumPy, Pandas and deep learning frameworks like TensorFlow, Pytorch and Keras\n\n- Experience in Big Data ecosystem with frameworks like Spark, PySpark , Unstructured DBs like Elasticsearch and MongoDB\n\n- Proficiency with TABLEAU or other web-based interfaces to create graphic-rich customizable plots, charts data maps etc.\n\n- Able to write SQL scripts for analysis and reporting (Redshift, SQL, MySQL).\n\n- Previous experience in ML, data scientist or optimization engineer role with a large technology company.\n\n- Experience in an operational environment developing, fast-prototyping, piloting, and launching analytic products.\n\n- Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations.\n\n- Experience in creating data driven visualizations to describe an end-to-end system.\n\n- Excellent written and verbal communication skills. The role requires effective communication with colleagues from computer science, operations research, and business backgrounds.\n\n- Bachelors or Masters in Artificial Intelligence, Computer Science, Statistics, Applied Math, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Data Science', 'Data Scientist', 'Artificial Intelligence', 'Data Management', 'Big Data', 'Data Modeling', 'Spark', 'Numpy', 'Python', 'Predictive Analytics']",2025-06-13 06:11:36
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"As an Associate Data Scientist at IBM, you will work to solve business problems using leading edge and open-source tools such as Python, R, and TensorFlow, combined with IBM tools and our AI application suites. You will prepare, analyze, and understand data to deliver insight, predict emerging trends, and provide recommendations to stakeholders.\n\nIn your role, you may be responsible for\nImplementing and validating predictive and prescriptive models and creating and maintaining statistical models with a focus on big data & incorporating machine learning. techniques in your projects\nWriting programs to cleanse and integrate data in an efficient and reusable manner\nWorking in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors\nCommunicating with internal and external clients to understand and define business needs and appropriate modelling techniques to provide analytical solutions.\nEvaluating modelling results and communicating the results to technical and non-technical audiences\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nCollaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nCreate technical documentation, white papers, and best practice guides\n\n\nPreferred technical and professional experience\nStrong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face.\nUnderstanding in the usage of libraries such as SciKit Learn, Pandas, Matplotlib, etc. Familiarity with cloud platforms\nExperience and working knowledge in COBOL & JAVA would be preferred",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'scikit-learn', 'tensorflow', 'pytorch', 'keras', 'natural language processing', 'neural networks', 'predictive', 'huggingface', 'machine learning', 'prototype', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'r', 'java', 'cobol', 'data science', 'matplotlib', 'big data', 'statistics']",2025-06-13 06:11:38
Gen AI and Machine Learning Data Scientist,Diverse Lynx,4 - 5 years,Not Disclosed,['Hyderabad'],"Hiring for Gen AI and Machine Learning Data Scientist-Pan India-\nProficiency in programming languages such as Python or R .\nGood understanding of statistical principles for ML applications\nExperience with data manipulation and analysis using libraries such as pandas, NumPy, and SciPy.\nExceptional knowledge of Deep Learning techniques , model architectures, and parameter fine-tuning\nVery good knowledge of GenAI with good understanding of foundation models and transformer architecture\nFamiliarity with data visualization tools (e.g., Matplotlib, Seaborn).\nExperience with SQL and relational databases.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['deep learning', 'Architecture', 'data manipulation', 'Machine learning', 'Programming', 'data visualization', 'SQL', 'Python']",2025-06-13 06:11:40
Machine Learning Engineer - Python / Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Indore'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'MLOps', 'Big Data', 'Neural Networks', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-13 06:11:41
Machine Learning Engineer - Python / Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Ahmedabad'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'MLOps', 'Big Data', 'Neural Networks', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-13 06:11:43
Machine Learning Engineer - Python / Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Nashik'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'MLOps', 'Big Data', 'Neural Networks', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-13 06:11:44
Machine Learning Engineer - Python / Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Delhi / NCR'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'MLOps', 'Big Data', 'Neural Networks', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-13 06:11:46
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Chennai'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'MLOps', 'Big Data', 'Neural Networks', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-13 06:11:48
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Visakhapatnam'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'MLOps', 'Big Data', 'Neural Networks', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-13 06:11:49
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Lucknow'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'MLOps', 'Big Data', 'Neural Networks', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-13 06:11:51
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Thane'],"Key Responsibilities :\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n- Experience with imbalanced datasets and techniques to improve model generalization.\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n- Strong mathematical and statistical knowledge with problem-solving skills.\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n- Experience in automating ML pipelines with MLOps practices.\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'Azure', 'MLOps', 'GCP', 'Big Data', 'Neural Networks', 'AWS', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-13 06:11:53
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Surat'],"Key Responsibilities :\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n- Experience with imbalanced datasets and techniques to improve model generalization.\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n- Strong mathematical and statistical knowledge with problem-solving skills.\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n- Experience in automating ML pipelines with MLOps practices.\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'Azure', 'MLOps', 'GCP', 'Big Data', 'Neural Networks', 'AWS', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-13 06:11:54
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"An AI Data Scientist at IBM is not just a job title - it’s a mindset. You’ll leverage the watsonx,AWS Sagemaker,Azure Open AI platform to co-create AI value with clients, focusing on technology patterns to enhance repeatability and delight clients.\n\nWe are seeking an experienced and innovative AI Data Scientist to be specialized in foundation models and large language models. In this role, you will be responsible for architecting and delivering AI solutions using cutting-edge technologies, with a strong focus on foundation models and large language models. You will work closely with customers, product managers, and development teams to understand business requirements and design custom AI solutions that address complex challenges. Experience with tools like Github Copilot, Amazon Code Whisperer etc. is desirable.\n\nSuccess is our passion, and your accomplishments will reflect this, driving your career forward, propelling your team to success, and helping our clients to thrive.\n\nDay-to-Day Duties:\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocumentation and Knowledge SharingDocument solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members.\nIndustry Trends and InnovationStay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nStrong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face. Understanding in the usage of libraries such as SciKit Learn, Pandas, Matplotlib, etc. Familiarity with cloud platforms (e.g. Kubernetes, AWS, Azure, GCP) and related services is a plus.\nExperience and working knowledge in COBOL & JAVA would be preferred\nHaving experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus (e.g. Amazon Code Whisperer, Github Copilot etc.) * Soft\n\nSkills:\nExcellent interpersonal and communication skills. Engage with stakeholders for analysis and implementation. Commitment to continuous learning and staying updated with advancements in the field of AI.\nGrowth mindsetDemonstrate a growth mindset to understand clients' business processes and challenges.\nExperience in python and pyspark will be added advantage\n\n\nPreferred technical and professional experience\nExperienceProven experience in designing and delivering AI solutions, with a focus on foundation models, large language models, exposure to open source, or similar technologies. Experience in natural language processing (NLP) and text analytics is highly desirable. Understanding of machine learning and deep learning algorithms.\nStrong track record in scientific publications or open-source communities\nExperience in full AI project lifecycle, from research and prototyping to deployment in production environments",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'keras', 'kubernetes', 'github', 'natural language processing', 'scikit-learn', 'pyspark', 'microsoft azure', 'artificial intelligence', 'text analytics', 'pandas', 'deep learning', 'java', 'code generation', 'cobol', 'gcp', 'matplotlib', 'aws']",2025-06-13 06:11:56
Data Scientist-Artificial Intelligence,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"Work with broader team to build, analyze and improve the AI solutions.\nYou will also work with our software developers in consuming different enterprise applications\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nResource should have 5-7 years of experience. Sound knowledge of Python and should know how to use the ML related services.\nProficient in Python with focus on Data Analytics Packages.\nStrategy Analyse large, complex data sets and provide actionable insights to inform business decisions.\nStrategy Design and implementing data models that help in identifying patterns and trends. Collaboration Work with data engineers to optimize and maintain data pipelines.\nPerform quantitative analyses that translate data into actionable insights and provide analytical, data-driven decision-making. Identify and recommend process improvements to enhance the efficiency of the data platform. Develop and maintain data models, algorithms, and statistical models\n\n\nPreferred technical and professional experience\nExperience with conversation analytics. Experience with cloud technologies\nExperience with data exploration tools such as Tableu",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data analytics', 'tableau', 'ml', 'hive', 'data analysis', 'natural language processing', 'pyspark', 'data warehousing', 'machine learning', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'java', 'data science', 'spark', 'kafka', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 06:11:58
Join us as a Data Scientist!!,Zensar,6 - 11 years,Not Disclosed,"['Hyderabad', 'Delhi / NCR']","-\nData Scientist\n\n-6+ years of experience in data science, with at least 2 years focused on LLMs or Generative AI.\n\n-Proven implementation experience in Data Science, Machine Learning, Deep Learning, and NLP for multiple domains.\n\n-Strong programming skills in Python, with experience in libraries such as Transformers (Hugging Face), PyTorch, or TensorFlow.\n\n-Hands-on experience with fine-tuning, prompt engineering, RAG (Retrieval-Augmented Generation), and LLM evaluation.\n\n-Familiarity with vector databases and embedding techniques.\n\n-Experience deploying models using APIs, Docker, and cloud platforms\n\n-Strong analytical, problem-solving, and communication skills.\n\n-Experience in ML Ops, Model deployment, Model lifecycle and management",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Pytorch', 'NLP', 'Machine Learning', 'Deep Learning', 'Tensorflow', 'Cloud Deployment', 'ML Ops', 'Generative AI', 'Hugging Face', 'LLM', 'Python']",2025-06-13 06:12:00
Data Scientist,HMG Technology,3 - 8 years,10-20 Lacs P.A.,['Bengaluru( Banaswadi )'],"Job Title: AI/ML Engineer (with LLM, Azure, Python & PySpark expertise)\nJob Description:\nWe are looking for a skilled and experienced AI/ML Engineer to join our data science and AI team. The ideal candidate will have a strong foundation in machine learning, artificial intelligence, and large language models (LLMs), along with deep proficiency in Python, PySpark, and Microsoft Azure services. You will be responsible for developing and deploying scalable AI solutions, working with big data frameworks, and leveraging cloud platforms to operationalize machine learning models.\nKey Responsibilities:\nArtificial Intelligence (AI) & Machine Learning (ML):\nDesign, develop, and optimize machine learning and AI models to solve business problems.\nPerform exploratory data analysis and feature engineering for model development.\nUse supervised, unsupervised, and reinforcement learning techniques where appropriate.\nBuild AI pipelines and integrate models into production systems.\nLarge Language Models (LLM):\nFine-tune and deploy LLMs (e.g., OpenAI, Hugging Face, or custom-trained models).\nDevelop prompt engineering strategies for LLM applications.\nImplement RAG (Retrieval-Augmented Generation) systems or LLMOps workflows.\nEvaluate LLM outputs for accuracy, bias, and performance.\nPython Programming:\nWrite efficient, reusable, and testable Python code for data processing, modeling, and API services.\nBuild automation scripts for data pipelines and model training workflows.\nUse popular libraries such as Scikit-learn, TensorFlow, PyTorch, Pandas, and NumPy.\nPySpark and Big Data:\nWork with large datasets using PySpark for data wrangling, transformation, and feature extraction.\nOptimize Spark jobs for performance and scalability.\nCollaborate with data engineering teams to implement end-to-end data pipelines.\nMicrosoft Azure:\nDeploy models and applications using Azure ML, Azure Databricks, Azure Functions, and Azure Synapse.\nManage compute resources, storage, and data security on Azure.\nUse Azure DevOps for CI/CD of ML pipelines and automation.\nCross-Functional Collaboration & Documentation:\nCollaborate with data engineers, product managers, and business stakeholders to align technical solutions with business needs.\nMaintain clear documentation of models, code, and workflows.\nPresent technical findings and model outcomes to both technical and non-technical audiences.\nRequired Skills & Qualifications:\nBachelor's or Masters degree in Computer Science, Data Science, Engineering, or a related field.\n3+ years of experience in AI/ML and data engineering roles.\nProficient in Python and PySpark.\nExperience with cloud platforms, especially Microsoft Azure.\nHands-on experience with LLMs (e.g., GPT, BERT, Claude, etc.).\nFamiliarity with ML frameworks like Scikit-learn, TensorFlow, or PyTorch.\nSolid understanding of ML lifecycle, MLOps, and deployment strategies.\nNice to Have:\nExperience with LLMOps and vector databases (e.g., FAISS, Pinecone).\nKnowledge of data governance and responsible AI practices.\nAzure certifications (e.g., Azure AI Engineer Associate, Azure Data Scientist Associate).\nExperience with REST APIs and containerization (Docker, Kubernetes).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Large Language Model', 'Azure Cloud', 'Artificial Intelligence', 'Machine Learning', 'Pyspark', 'Deep Learning', 'Python']",2025-06-13 06:12:01
Data Scientist,Dwplacesolutions,3 - 5 years,Not Disclosed,['Bengaluru'],We are seeking an experienced Data Scientist to join our team.\nThe ideal candidate will have a strong background in developing and deploying\nconversational AI solutions using Large Language Models (LLMs) and RASA\nframework.,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Machine Learning', 'Tensorflow', 'R', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Chatbot', 'Deep Learning', 'Python']",2025-06-13 06:12:03
Data Scientist,Virtana Corp,3 - 8 years,Not Disclosed,"['Pune', 'Chennai']","Position Overview:\nWe are seeking a Data Scientist Engineer with experience bringing highly scalable enterprise SaaS applications to market. This is a uniquely impactful opportunity to help drive our business forward and directly contribute to long-term growth at Virtana.\nIf you thrive in a fast-paced environment, take initiative, embrace proactivity and collaboration, and you re seeking an environment for continuous learning and improvement, we d love to hear from you!\nVirtana is a remote first work environment so you ll be able to work from the comfort of your home while collaborating with teammates on a variety of connectivity tools and technologies.\nJob Location- Pune/ Chennai/ Remote\nRole Responsibilities:\nResearch and test machine learning approaches for analyzing large-scale distributed computing applications.\nImplement different models AI and ML algorithms for prototype and production systems.\nTest and refine the models and algorithms with live customer data to improve accuracy and efficacy.\nWork with other functional teams to integrate implemented systems into the SaaS platform\nSuggest innovative and creative concepts and ideas that would improve the overall platform\nQualifications:\nThe ideal candidate must have the following qualifications:\n3+ years experience in practical implementation and deployment of ML based systems preferred.\nBS/B Tech or M Tech/ MS (preferred) in Applied Mathematics or Statistics, or CS/Engineering with strong mathematical/statistical background\nStrong quantitative and analytical skills, especially statistical and ML techniques, including familiarity with different supervised and unsupervised learning algorithms\nImplementation experiences and deep knowledge of Classification, Time Series Analysis, Pattern Recognition, Reinforcement Learning, Deep Learning, Dynamic Programming and Optimization\nExperience in working on modeling graph structures related to spatiotemporal systems\nProgramming skills in Python\nExperience in developing and deploying on cloud (AWS or Google or Azure)\nExperience in understanding and usage of LLM models and Prompt engineering is preferred.\nGood verbal and written communication skills\nFamiliarity with well-known ML frameworks such as Pandas, Keras and TensorFlow",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Usage', 'Prototype', 'Time series analysis', 'Artificial Intelligence', 'IT operations management', 'Machine learning', 'Cloud', 'Cash flow', 'Pattern recognition', 'Python']",2025-06-13 06:12:04
Data Scientist,PS Human Resources And Consultants,3 - 6 years,7.5-15 Lacs P.A.,['Pune'],"Data Scientist\n\n\n\n\nResponsibilities\nDesign and implement AI agent workflows. Develop end-to-end intelligent pipelines and multi-agent systems (e.g., LangGraph/LangChain workflows) that coordinate multiple LLM-powered agents to solve complex tasks. Create graph-based or state-machine architectures for AI agents, chaining prompts and tools as needed.\n\nBuild and fine-tune generative models. Develop, train, and fine-tune advanced generative models (transformers, diffusion models, VAEs, GANs, etc.) on domain-specific data. Deploy and optimize foundation models (such as GPT, LLaMA, Mistral) in production, adapting them to our use cases through prompt engineering and supervised fine-tuning.\n\nDevelop data pipelines. Build robust data collection, preprocessing, and synthetic data generation pipelines to feed training and inference workflows. Implement data cleansing, annotation, and augmentation processes to ensure high-quality inputs for model training and evaluation.\n\nImplement LLM-based agents and automation. Integrate generative AI agents (e.g., chatbots, AI copilots, content generators) into business processes to automate data processing and decision-making tasks. Use Retrieval-Augmented Generation (RAG) pipelines and external knowledge sources to enhance agent capabilities. Leverage multimodal inputs when applicable.\n\nOptimize performance and safety. Continuously evaluate and improve model/system performance. Use GenAI-specific benchmarks and metrics (e.g., BLEU, ROUGE, TruthfulQA) to assess results, and iterate to optimize accuracy, latency, and resource efficiency. Implement safeguards and monitoring to mitigate issues like bias, hallucination, or inappropriate outputs.\n\nCollaborate and document. Work closely with product managers, engineers, and other stakeholders to gather requirements and integrate AI solutions into production systems. Document data workflows, model architectures, and experimentation results. Maintain code and tooling (prompt libraries, model registries) to ensure reproducibility and knowledge sharing.\n\nRequired Skills & Qualifications\nEducation: Bachelors or Masters degree in Computer Science, Data Science, Artificial Intelligence, or a related quantitative fieldanalyticsvidhya.com (or equivalent practical experience). A strong foundation in algorithms, statistics, and software engineering is expected.\n\nProgramming proficiency: Expert-level skills in Pythoncoursera.org, with hands-on experience in machine learning and deep learning frameworks (PyTorch, TensorFlow)analyticsvidhya.com. Comfortable writing production-quality code and using version control, testing, and code review workflows.\n\nGenerative model expertise: Demonstrated ability to build, fine-tune, and deploy large-scale generative modelsanalyticsvidhya.com. Familiarity with transformer architectures and generative techniques (LLMs, diffusion models, GANs)analyticsvidhya.comanalyticsvidhya.com. Experience working with model repositories and fine-tuning frameworks (Hugging Face, etc.).\n\nLLM and agent frameworks: Strong understanding of LLM-based systems and agent-oriented AI patterns. Experience with frameworks like LangGraph/LangChain or similar multi-agent platformsgyliu513.medium.com. Knowledge of agent communication standards (e.g., MCP/Agent Protocol)gyliu513.medium.comblog.langchain.dev to enable interoperability between AI agents.\n\nAI integration and MLOps: Experience integrating AI components with existing systems via APIs and services. Proficiency in retrieval-augmented generation (RAG) setups, vector databases, and prompt engineeringanalyticsvidhya.com. Familiarity with machine learning deployment and MLOps tools (Docker, Kubernetes, MLflow, KServe, etc.) for managing end-to-end automation and scalable workflowsanalyticsvidhya.com.\n\nFamiliarity with GenAI tools: Hands-on experience with state-of-the-art GenAI models and APIs (OpenAI GPT, Anthropic, Claude, etc.) and with popular libraries (Hugging Face Transformers, LangChain, etc.). Awareness of the current GenAI tooling ecosystem and best practices.\n\nSoft skills: Excellent problem-solving and analytical abilities. Strong communication and teamwork skills to collaborate across data, engineering, and business teams. Attention to detail and a quality-oriented mindset. (See Ideal Candidate below for more on personal attributes.)\n\nIdeal Candidate:\n\nInnovative, problem-solver: You are a creative thinker who enjoys tackling open-ended challenges. You have a solutions-oriented mindset and proactively experiment with new ideas and techniquesanalyticsvidhya.com.\n\nSystems thinker: You understand how different components (data, models, services) fit together in a large system. You can architect end-to-end AI solutions with attention to reliability, scalability, and integration points.\n\nCollaborative communicator: You work effectively in multidisciplinary teams. You are able to explain complex technical concepts to non-technical stakeholders and incorporate feedback. You value knowledge sharing and mentorship.\n\nAdaptable learner: The generative AI landscape evolves rapidly. You are passionate about staying current with the latest research and tools. You embrace continuous learning and are eager to upskill and try new libraries or platformsanalyticsvidhya.com.\n\nEthical and conscientious: You care about the real-world impact of AI systems. You take responsibility for the quality and fairness of models, and proactively address concerns like data privacy, bias, and security.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Langchain', 'Machine Learning', 'AI Agent workflo', 'Python', 'Langgraph']",2025-06-13 06:12:06
Data Scientist,Mindpro Technologies,4 - 9 years,5-12 Lacs P.A.,"['Karur', 'Dharwad']","Greetings From Mind Pro Technologies Pvt ltd (www.mindprotech.com)\n\nJob Title : Data Scientist\nWork Location : Karur (Tamil Nadu) or Dharwad (Karnataka )\nNp : 15days or Less\n\n\nJOB DESCRIPTION:\n Must have At least 4+ Years of experience in Python with Data Science.\n Must have worked on at least one Live project.\nExperience in relevant field such as Statistics, Computer Science or Applied Math or Operational Research.\nMust have Masters in (Maths/Statistics or Applied Mathematics/Machine Learning etc.)\nHistory of successfully performing customer implementations\nStrong customer facing skills, and previous consulting experience.\nExperience of handling high frequency streaming data for real time analysis and reporting.\nFamiliarity with - Natural Language Processing, Statistical Analysis (distribution analysis, correlation, variance, deep learning.\nExperience in tools like AWS, IBM Watson is a plus.\nExperience with open source technologies is a must.\nExcellent communication\nAbility to lead & build strong teams\nAbility to work in an ambiguous environment\n\nDesired Skills and Experience\nLanguages/Tools: Python/R.\nApproaches: Machine Learning\nConcepts: Supervised ANN, Bayesian, Gaussian, Vector Quantization, Logistic Model, Statistical, Predictive Modeling, Minimum Message Length, SVM, Random Forest, Ensembles, ANOVA, Decision Trees, Hidden Markov Models\nUnsupervised ANN, ARL, Clustering Hierarchical, Cluster Analysis\nReinforcement\nGen AI, LLM, LSTM, RNN, CNN, KNN\nBig Data (Good to have): Hadoop /Kafka / Storm / Spark streaming\nOS: Linux, Windows 32/64 bits.\n\nNote:  should know supervised and unsupervised learning,   semi-supervised learning, neural networks concepts, and how ML algorithms works with training and testing data. Experience on particular data set to train, test and roll-out for production use\n\nTool sets : Python, R, MATLAB or  any AI frame work, Neural network, Gen AI, LLM\nContact Details:\n\nRecruitment Team\nMindpro Technologies Pvt Ltd (www.mindprotech.com)\n+91-04324-240904 / +91-9600672304",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Gen AI', 'Statistical Modeling', 'LLM', 'Predictive Modeling', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Machine Learning', 'Deep Learning', 'Python']",2025-06-13 06:12:08
Data Scientist,An Indian NBFC,3 - 8 years,Not Disclosed,['Chennai'],"Responsibilities:\nCollect, clean, and analyze large sets of structured and unstructured data to extract meaningful insights and trends\nDevelop and implement advanced machine learning algorithms to solve complex business problems\nSupport moving models to production, by creating high quality code modules that can be seamlessly integrated into existing systems (both on-prem and cloud)\nCommunicate complex findings to both technical and non-technical audiences through effective data visualization and storytelling.\nCollaborate with cross-functional teams to identify data-driven opportunities and translate business requirements into actionable data solutions.\nSupport the development and maintenance of data pipelines and infrastructure\nStay up-to-date with industry trends and advancements in Data Science and Machine Learning technologies.\n\nSkills Required:\nStrong foundation in statistics, and machine learning algorithms\nStrong proficiency in programming languages like Python and SQL.\nExcellent problem-solving and analytical skills.\nAbility to work independently and as part of a team.\nShould have built production models using at least 2 of the ML techniques: Clustering, Regression, Classification\nExperience in Banking & Financial Services is preferred.\nExperience working on cloud platforms (e.g., AWS, GCP) is preferred.\nA passion for data and a curiosity to explore new trends and technologies",Industry Type: NBFC,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Pipeline', 'Data Extraction', 'Model Building', 'Artificial Intelligence', 'Cloud', 'Machine Learning']",2025-06-13 06:12:10
Data Scientist,Simplify Healthcare,5 - 10 years,Not Disclosed,"['Pune( Hadapsar, Kharadi, Keshav Nagar, Vishrantwadi, Dhanori, Mundhwa, Viman Nagar )']","Engineer/Sr. Engineer Data Science\nLocation: Pune, India\n\nCompany Overview:\nSimplify Healthcare is one of the fastest-growing healthcare technology solutions providers serving the US health insurance (Payer) industry. Headquartered in Chicago with a Global Delivery Centre in Pune, we are trusted by 65+ payer organizations and supported by a team of 800+ professionals.\nWe specialize in delivering SaaS-based enterprise software solutions focused on product and benefits configuration, provider lifecycle management, and more. In 2023, we launched Simplify Health Cloud, our flagship Payer Platform, establishing our position as a leader in cloud-native, low-code configurable platforms for the healthcare sector.\nWith our strategic acquisition of Virtical.ai in 2024, we’re accelerating innovation through AI integration, particularly in areas such as LLMs, conversational AI, and cloud-based intelligence. Our proprietary Simplify App Fabric™ enables fast, secure, and low-code development for modern Payer solutions.\nOur innovation has earned us repeated recognition in Deloitte Technology Fast 500™, Inc. 5000, and reports by IDC and Gartner.",,,,"['Speech Recognition', 'Artificial Intelligence', 'Natural Language Processing', 'Conversational Ai', 'Chatbot', 'Cognitive Services', 'Text Mining', 'Machine Learning', 'Azure Cognitive Services']",2025-06-13 06:12:12
Sr Staff Machine Learning Engineer,Xoom,12 - 15 years,Not Disclosed,['Bengaluru'],"Job Summary\nThis job will oversee the strategic direction and execution of machine learning projects. You will work closely with data scientists, software engineers, and product teams to enhance services through innovative AI/ML solutions. Your role will involve building scalable ML pipelines, ensuring data quality, and deploying models into production environments to drive business insights and improve customer experiences.\n\nYour Way to Impact\nAs a Sr. Staff Machine Learning Engineer, you ll be instrumental in scaling PayPal s AI capabilities through advanced fine-tuning and foundational model development. Your work will directly enhance how we serve our customers with intelligent, adaptive, and personalized experiences. From conversational agents to smart automation, the models you build will be embedded across PayPal s ecosystem, impacting millions of users globally.\nMeet Our Team\nYou ll be part of the Applied Intelligence organization, working alongside product, platform, and infrastructure teams. We power next-gen experiences through a unified AI stack accelerating time-to-value for internal users and building intuitive solutions for consumers and merchants. You ll be joining a team that values experimentation, scalability, and operational excellence in AI systems.\nJob Description\nYour Day to Day\nFine-tune and optimize advanced machine learning and foundational models.\nBuild reusable training pipelines and inference systems for production-grade deployment.\nCollaborate with cross-functional teams to integrate models into product experiences and backend workflows.\nContinuously monitor, test, and improve model performance in live environments.\nMentor junior engineers and contribute to team-wide knowledge sharing.\nParticipate in internal forums, technical design reviews, and possibly external publications.\nWhat You Need to Bring\nBachelor s degree or equivalent, with at least 12-15 years of relevant experience.\nStrong hands-on experience with ML frameworks like PyTorch, TensorFlow, or Hugging Face.\nExperience fine-tuning large models (e.g., LLMs, vision-language models) using SFT, LoRA, RLHF, etc.\nProven experience deploying ML models on cloud platforms such as AWS, GCP, or Azure.\nDeep knowledge of MLOps practices and tooling (e.g., model registries, CI/CD for ML).\nPreferred Qualification\nSubsidiary\nPayPal\nTravel Percent\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https//www.paypalbenefits.com .\nWho We Are\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. .\nBelonging at PayPal\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Backend', 'Automation', 'Operational excellence', 'GCP', 'Technical design', 'Diversity and Inclusion', 'Machine learning', 'Wellness', 'model development', 'Data quality']",2025-06-13 06:12:13
Senior Machine Learning Engineer,Bebo Technologies,3 - 8 years,Not Disclosed,"['Chandigarh', 'Pune', 'Delhi / NCR']","3+ years of experience in software engineering and ML development.\nStrong proficiency in Python and ML libraries such as Scikit-learn, TensorFlow, or PyTorch.\nExperience building and evaluating models, along with data preprocessing and feature engineering.\nProficiency in REST APIs, Docker, Git, and CI/CD tools.\nSolid foundation in software engineering principles, including data structures, algorithms, and design patterns.\nHands-on experience with MLOps platforms (e.g., MLflow, TFX, Airflow, Kubeflow).\nExposure to NLP, large language models (LLMs), or computer vision projects.\nExperience with cloud platforms (AWS, GCP, Azure) and managed ML services.\nContributions to open-source ML libraries or participation in ML competitions (e.g., Kaggle, DrivenData) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Tensorflow', 'Machine Learning', 'Pytorch', 'Keras', 'Scikit-Learn', 'Python']",2025-06-13 06:12:15
Data Scientist,Tesco,1 - 3 years,Not Disclosed,['Bengaluru( Whitefield )'],"Job Summary:\n\nEnable data driven decision making across the Tesco business globally by developing analytics solutions using a combination of math, tech and business knowledge\n\nRoles and Responsibilities:\n- Identifying operational improvements and finding solutions by applying CI tools and techniques\n- Responsible for completing tasks and transactions within agreed KPI's",,,,"['Data Science', 'Advanced Excel', 'Data Analytics', 'Python', 'SQL', 'Applied Mathematics', 'Machine Learning', 'Statistics']",2025-06-13 06:12:16
Senior Machine Learning Engineer - NLP/Python,Maimsd Technology,4 - 7 years,Not Disclosed,['Pune'],"Notice Period : Immediate - 15 Days\n\nAbout the Role :\n\nWe are seeking a highly skilled Senior Data Scientist to join our team and contribute to cutting-edge projects. As a key member of our data science team, you will leverage your expertise in machine learning, natural language processing, and data engineering to develop innovative solutions that drive business value.\n\nResponsibilities :\n\n- Machine Learning Model Development : Design, develop, and deploy advanced machine learning models, focusing on natural language processing tasks such as text classification, sentiment analysis, and language generation.\n\n- Data Engineering : Extract, transform, and load (ETL) data from various sources, ensuring data quality and consistency.\n\n- NLP Techniques : Apply state-of-the-art NLP techniques, including language models and text processing, to solve complex problems.\n\n- Python and ML Frameworks : Utilize Python programming language and popular ML frameworks like PyTorch or TensorFlow to build efficient and scalable models.\n\n- Cloud and Containerization : Leverage cloud platforms (GCP, AWS) and containerization technologies (Docker) for efficient deployment and management of ML models.\n\n- Database Management : Work with relational databases (Postgres, MySQL) to store, manage, and query large datasets.\n\n- Knowledge Graphs and ML Publications : Contribute to the development of knowledge graphs and stay updated with the latest advancements in the field through research and publications.\n\nQualifications :\n\nExperience : 4-7 years of hands-on experience in data science, with a strong focus on machine learning and natural language processing.\n\nTechnical Skills :\n\n- Proficiency in Python programming language.\n\n- Deep understanding of machine learning algorithms and techniques.\n\n- Expertise in NLP, including language models and text processing.\n\n- Familiarity with ML frameworks like PyTorch or TensorFlow.\n\n- Experience with cloud platforms (GCP, AWS) and containerization (Docker).\n\n- Knowledge of relational databases (Postgres, MySQL).\n\nSoft Skills :\n\n- Strong problem-solving and analytical skills.\n\n- Excellent communication and collaboration abilities.\n\n- Ability to work independently and as part of a team.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Data Science', 'Tensorflow', 'Azure', 'NLP', 'Data Scientist', 'AWS', 'Python']",2025-06-13 06:12:18
Data Scientist-Artificial Intelligence,IBM,10 - 15 years,Not Disclosed,['Bengaluru'],"We're seeking a results-driven and collaborative Software Development Manager to lead the design and development of IBM Consulting Advantage Platform. As a management leader, you'll collaborate with peers and stakeholders to ensure business continuity. You'll also be responsible for building and leading an impactful team of Developers & QA engineers, focusing on software developments, productivity improvements and fostering a culture of continuous learning and improvement.\nIn this role, you will be responsible for:\nLead a team of engineers to meet release dates along with committed deliverables on-time and with quality\nBalance priorities and work assignments across team members following agile processes to meet delivery schedules\nInterface with product management and offering managers to understand customer requirements and business prioritization\nDrive development activities, monitor progress, collaborate to align dependencies, remove blockers for team members and manage risks\nDevelop and implement effective strategies for software development, testing, and deployment\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n10+ years of professional experience; 5+ years as team lead/manager\nExcellent organizational skills including attention to details, time management, and multi-tasking skills\nHands-on experience Experienced building Microservices & REST APIs using Java, and other related technologies\nExperience with Front End Development programming languages and design Frameworks\nStrong project management, organizational, problem-solving, communication, and collaboration skills\n\n\nPreferred technical and professional experience\nHands-on experience with SpringBoot, ReactJS, NodeJS etc\nExperience in working on a production SaaS application with SOC2 certification\nKnowledge of Containerisation technologies such as Kubernetes & Docker, and CI/CD pipelines such as Tekton, ArgoCD etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'ci/cd', 'microservices', 'java', 'project management', 'kubernetes', 'docker', 'ansible', 'sql', 'react.js', 'git', 'devops', 'linux', 'jenkins', 'html', 'shell scripting', 'rest', 'python', 'github', 'maven', 'microsoft azure', 'javascript', 'spring boot', 'node.js', 'saas', 'terraform', 'aws']",2025-06-13 06:12:19
Data Scientist,Mastercard,4 - 8 years,Not Disclosed,['Gurugram'],"As consumer preference for digital payments continues to grow, ensuring a seamless and secure consumer experience is top of mind. Optimization Soltions team focuses on tracking of digital performance across all products and regions, understanding the factors influencing performance and the broader industry landscape. This includes delivering data-driven insights and business recommendations, engaging directly with key external stakeholders on implementing optimization solutions (new and existing), and partnering across the organization to drive alignment and ensure action is taken.\n\nThe Role:\n\nWork closely with global optimization solutions team to architect, develop, and maintain advanced reporting and data visualization capabilities on large volumes of data to support data insights and analytical needs across products, markets, and services\nThe candidate for this position will focus on Building solutions using Machine Learning and creating actionable insights to support product optimization and sales enablement.\nPrototype new algorithms, experiment, evaluate and deliver actionable insights.\nDrive the evolution of products with an impact focused on data science and engineering.\nDesigning machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.\nPerform data ingestion, aggregation, and processing on high volume and high dimensionality data to drive and enable data unification and produce relevant insights.\nContinuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nA superior academic record at a leading university in Computer Science, Data Science, Technology, mathematics, statistics, or a related field or equivalent work experience\nExperience in data management, data mining, data analytics, data reporting, data product development and quantitative analysis\nStrong analytical skills with track record of translating data into compelling insights\nPrior experience working in a product development role.\nknowledge of ML frameworks, libraries, data structures, data modeling, and software architecture.\nproficiency in using Python/Spark, Hadoop platforms & tools (Hive, Impala, Airflow, NiFi), and SQL to build Big Data products & platforms\nExperience with Enterprise Business Intelligence Platform/Data platform ie Tableau, PowerBI is a plus.\nDemonstrated success interacting with stakeholders to understand technical needs and ensuring analyses and solutions meet their needs effectively.\nAbility to build a strong narrative on the business value of products and actively participate in sales enablement efforts.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'Information security', 'Machine learning', 'Data structures', 'Data mining', 'Business intelligence', 'SQL', 'Python']",2025-06-13 06:12:21
Data Scientist,Dynamic Yield,5 - 10 years,Not Disclosed,['Gurugram'],"Our Purpose\nMastercard powers economies and empowers people in 200+ countries and territories worldwide. Together with our customers, we re helping build a sustainable economy where everyone can prosper. We support a wide range of digital payments choices, making transactions secure, simple, smart and accessible. Our technology and innovation, partnerships and networks combine to deliver a unique set of products and services that help people, businesses and governments realize their greatest potential.\nTitle and Summary\nData Scientist\nWho is Mastercard?\nMastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships, and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.\nOur decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.\n\nOur Team:\nAs consumer preference for digital payments continues to grow, ensuring a seamless and secure consumer experience is top of mind. Optimization Soltions team focuses on tracking of digital performance across all products and regions, understanding the factors influencing performance and the broader industry landscape. This includes delivering data-driven insights and business recommendations, engaging directly with key external stakeholders on implementing optimization solutions (new and existing), and partnering across the organization to drive alignment and ensure action is taken.\nAre you excited about Data Assets and the value they bring to an organization?\nAre you an evangelist for data-driven decision-making?\nAre you motivated to be part of a team that builds large-scale Analytical Capabilities supporting end users across 6 continents?\nDo you want to be the go-to resource for data science & analytics in the company?\n\n\nThe Role:\n\nWork closely with global optimization solutions team to architect, develop, and maintain advanced reporting and data visualization capabilities on large volumes of data to support data insights and analytical needs across products, markets, and services\nThe candidate for this position will focus on Building solutions using Machine Learning and creating actionable insights to support product optimization and sales enablement.\nPrototype new algorithms, experiment, evaluate and deliver actionable insights.\nDrive the evolution of products with an impact focused on data science and engineering.\nDesigning machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.\nPerform data ingestion, aggregation, and processing on high volume and high dimensionality data to drive and enable data unification and produce relevant insights.\nContinuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nA superior academic record at a leading university in Computer Science, Data Science, Technology, mathematics, statistics, or a related field or equivalent work experience\nExperience in data management, data mining, data analytics, data reporting, data product development and quantitative analysis\nStrong analytical skills with track record of translating data into compelling insights\nPrior experience working in a product development role.\nknowledge of ML frameworks, libraries, data structures, data modeling, and software architecture.\nproficiency in using Python/Spark, Hadoop platforms & tools (Hive, Impala, Airflow, NiFi), and SQL to build Big Data products & platforms\nExperience with Enterprise Business Intelligence Platform/Data platform i.e. Tableau, PowerBI is a plus.\nDemonstrated success interacting with stakeholders to understand technical needs and ensuring analyses and solutions meet their needs effectively.\nAbility to build a strong narrative on the business value of products and actively participate in sales enablement efforts.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor.\nCorporate Security Responsibility\n\nAll activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:\nAbide by Mastercard s security policies and practices;\nEnsure the confidentiality and integrity of the information being accessed;\nReport any suspected information security violation or breach, and\nComplete all periodic mandatory security trainings in accordance with Mastercard s guidelines.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'Information security', 'Machine learning', 'Data structures', 'Data mining', 'Business intelligence', 'SQL', 'Python']",2025-06-13 06:12:23
Senior Data Scientist | Snowflakes | Tableau | AI/ML,Cisco,0 - 2 years,Not Disclosed,['Bengaluru'],"Job posting may be removed earlier if the position is filled or if a sufficient number of applications are received.\n\nMeet the Team\n\nWe are a dynamic and innovative team of Data Engineers, Data Architects, and Data Scientists based in Bangalore, India. Our mission is to harness the power of data to provide actionable insights that empower executives to make informed, data-driven decisions. By analyzing and interpreting complex datasets, we enable the organization to understand the health of the business and identify opportunities for growth and improvement.\n\nYour Impact\n\nWe are seeking a highly experienced and skilled Senior Data Scientist to join our dynamic team. The ideal candidate will possess deep expertise in machine learning models, artificial intelligence (AI), generative AI, and data visualization. Proficiency in Tableau and other visualization tools is essential. This role requires hands-on experience with databases such as Snowflake and Teradata, as well as advanced knowledge in various data science and AI techniques. The successful candidate will play a pivotal role in driving data-driven decision-making and innovation within our organization.\n\nKey Responsibilities\nDesign, develop, and implement advanced machine learning models to solve complex business problems.\nApply AI techniques and generative AI models to enhance data analysis and predictive capabilities.\nUtilize Tableau and other visualization tools to create insightful and actionable dashboards for stakeholders.\nManage and optimize large datasets using Snowflake and Teradata databases.\nCollaborate with cross-functional teams to understand business needs and translate them into analytical solutions.\nStay updated with the latest advancements in data science, machine learning, and AI technologies.\nMentor and guide junior data scientists, fostering a culture of continuous learning and development.\nCommunicate complex analytical concepts and results to non-technical stakeholders effectively.\nKey Technologies &\n\nSkills:\nMachine Learning ModelsSupervised learning, unsupervised learning, reinforcement learning, deep learning, neural networks, decision trees, random forests, support vector machines (SVM), clustering algorithms, etc.\nAI TechniquesNatural language processing (NLP), computer vision, generative adversarial networks (GANs), transfer learning, etc.\nVisualization ToolsTableau, Power BI, Matplotlib, Seaborn, Plotly, etc.\nDatabasesSnowflake, Teradata, SQL, NoSQL databases.\nProgramming LanguagesPython (essential), R, SQL.\nPython LibrariesTensorFlow, PyTorch, scikit-learn, pandas, NumPy, Keras, SciPy, etc.\nData ProcessingETL processes, data warehousing, data lakes.\nCloud PlatformsAWS, Azure, Google Cloud Platform.\nMinimum Qualifications\nBachelor's or Master's degree in Computer Science, Statistics, Mathematics, Data Science, or a related field.\nMinimum of [X] years of experience as a Data Scientist or in a similar role.\nProven track record in developing and deploying machine learning models and AI solutions.\nStrong expertise in data visualization tools, particularly Tableau.\nExtensive experience with Snowflake and Teradata databases.\nExcellent problem-solving skills and the ability to work independently and collaboratively.\nExceptional communication skills with the ability to convey complex information clearly.\nPreferred Qualifications (Provide up to five (5) bullet points these can include soft skills)\nExcellent communication and collaboration skills to work effectively in cross-functional teams.\nAbility to translate business requirements into technical solutions.\nStrong problem-solving skills and the ability to work with complex datasets.\nExperience in statistical analysis and machine learning techniques.\nUnderstanding of business domains such as sales, financials, marketing, and telemetry.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['machine learning', 'artificial intelligence', 'sql', 'tableau', 'data visualization', 'snowflake', 'scipy', 'python', 'scikit-learn', 'data warehousing', 'numpy', 'pandas', 'tensorflow', 'data integration tools', 'matplotlib', 'pytorch', 'keras', 'machine learning algorithms', 'etl', 'nosql databases']",2025-06-13 06:12:25
"PRINCIPAL, DATA SCIENTIST",Walmart,10 - 15 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team\nWalmart s Enterprise Business Services (EBS) is a powerhouse of several exceptional teams delivering world-class technology solutions and services making a profound impact at every level of Walmart.\nAs a key part of Walmart Global Tech, our teams set the bar for operational excellence and leverage emerging technology to support millions of customers, associates, and stakeholders worldwide. Each time an associate turns on their laptop, a customer makes a purchase, a new supplier is onboarded, the company closes the books, physical and legal risk is avoided, and when we pay our associates consistently and accurately, that is EBS. Joining EBS means embarking on a journey of limitless growth, relentless innovation, and the chance to set new industry standards that shape the future of Walmart.\nWhat you will do\nYou will work with the multiple teams and guide them on technical aspects, set quality standards and participate in design discussion and drive technical decisions\nLead the end-to-end lifecycle of AI/ML projects, from ideation to deployment, ensuring alignment with Walmarts strategic goals.\nDesign and implement scalable cloud-based machine learning and data science solutions, leveraging, GCP, or other cloud platforms.\nDevelop novel algorithms and leverage state-of-the-art AI frameworks (e.g., TensorFlow, PyTorch, HuggingFace) to solve complex problems in indirect procurement optimization, customer personalization, and operational efficiency.\nBuild highly parallelized compute environments for processing large-scale datasets, optimizing performance across CPU and GPU architectures.\nCollaborate with diverse teams across engineering, business, and operations to understand requirements and integrate data science solutions seamlessly.\nAdvocate for best practices in software development, including CI/CD, unit testing, and documentation, to ensure robust and reliable systems.\nMentor junior data scientists and contribute to building a culture of innovation and learning within the data science community at Walmart.\nCode Reviews across teams\nEngage with Product Management and Business to drive the agenda, set your priorities and deliver awesome products.\nDrive design, development, implementation and documentation\nBuild, test and deploy cutting edge solutions at scale, impacting associates of Walmart worldwide.\nInteract with Walmart engineering teams across geographies to leverage expertise and contribute to the tech community.\nDrive the success of the implementation by applying technical skills, to design and build enhanced processes and technical solutions in support of strategic initiatives.\nYou will use your engineering experience and technical skill to develop highly scalable and robust solutions. You will work with Engineering Lead/architect.\nWork closely with the Architects and cross functional teams and follow established practices for the delivery of solutions meeting QCD (Quality, Cost & Delivery). Within the established architectural guidelines.\nWork with senior leadership to chart out the future roadmap of the products\nParticipate in hiring and build teams enabling them to be high performing agile teams.\nYou will help and participate with the teams that leverage and contribute to open source technologies to Make impact on a global scale\nInteract closely for requirements with Business owners and technical teams both within India and across the globe.\nWhat you will bring\nB.Tech. / B.E. / M.Tech. / M.S. in Computer Science or relevant discipline\n10+ years of experience in design and development of highly -scalable applications and platform development\nWork in a highly collaborative environment with a multidisciplinary team.\nWork with senior data scientists to design, architect, and build AI/ML model and model systems.\nWork with machine learning engineers to deploy, operate, and optimize scalable solutions\nWork with product managers to design user journeys, feedback loop and analyze user telemetry.\nCreate opportunities to develop yourself with an end-to-end AI/ML product experience.\nWork with a set of robust work standards to ensure we build trustworthy AI/ML solutions\nHosted & Participated Architecture Review & Design/Code Review events.\nHands on System Designing experience.\nStrong computer science fundamentals: data structures, algorithms, design patterns.\nExtensive hands-on experience building services using these technologies (Scala, Java, Springboot, Microservices ,NodeJs)\nHands-on experience in web technologies like React JS/Angular Js, Java script, Type script, CSS\nGood Knowledge in messaging systems: Kafka/RabbitMQ\nWorking knowledge of SQL and NoSQL database technologies.\nKnowledge on Linux platform\nKnowledge on unit testing frameworks (Junit, Jest , Spock etc) and code quality control platforms like Sonar\nKnowledge on cloud platforms any cloud platforms like IAAS/PAAS\nCI/CD development environments/tools: Git, Maven, Gradle, Docker, Kubernetes, Jenkins, Azure DevOps\nExperience in implementing Distributed Cache(Redis/Hazlecast)\nWell-Versed with Logging and Metrics tools and technologies (ELK/Splunk/Grafana)\nKnowledge in search engines like Lucene/Solr\nDemonstrated end-to-end ownership for development and design of least one cloud based project.\nStrong hands on development skills to prototype technical solutions.\nStrong desire to drive change, and ability to adapt to change quickly. Willing to learn new and emerging technologies.\nExceptional communication and interpersonal skills - including negotiation, facilitation, and consensus building skills; ability to influence and persuade, without direct control.\nPractitioner of Agile (Scrum) methodology\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 5 years experience in an analytics related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 3 years experience in an analytics related field. Option 3: 7 years experience in an analytics or related field.\nPreferred Qualifications...",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Maven', 'Linux', 'Networking', 'Data structures', 'Unit testing', 'Open source', 'Information technology', 'Analytics', 'SQL']",2025-06-13 06:12:26
Data Scientist,Leading Automobile Manufacturing Company...,5 - 10 years,Not Disclosed,['Chennai'],"Kindly share your resume on sv17@svmanagement.com\nResponsibility:\nWork with different user groups/ departments\nIdentify processes where Analytics driven decision making can create powerful impact\nDesign original analysis that helps generate relevant insights\nEstablishes credibility by thought partnering with business and service teams on analytics topics; takes positions and draws conclusions on a range of external and internal issues\nCommunicates analytical insights through sophisticated synthesis and packaging of results (including PPT slides, dashboards, mailers and alerts)\nCollect, synthesize, analyze team learning & inputs into new best practices and methodologies\nWork with IT teams for implementation of solutions in a production environment\nWork on development of internal capability on the subject\nKeep abreast of most recent developments in the Analytics space and identify new tools and capabilities relevant to company needs.\nAptitude to constantly learn and explore new analytical advancements\nContributes to development of new topic- and sector-related analytics products (development in scope for separate proprietary data & tools team)\nDevelops topic and content related to analytics work for trainings\nProfile:\nExperience in designing analytical solutions using machine learning algorithms\nKnowledge of advanced Excel for preliminary data analysis and good presentation skills\nProficient Coding Knowledge in Python is essential. Developing visualizations in Tableau is desirable\nAdditional coding knowledge in R & Visual Basic will be an advantage\nProficient in web analytics and predictive analytics\nGraduate/certificate in Business Analytics from premier Institute would be an advantage",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'R', 'Visual Basic', 'Machine Learning', 'Python', 'Web Analytics', 'Tableau', 'Data Analytics', 'Predictive Analytics']",2025-06-13 06:12:28
Data Analyst with AI OR Machine learning,Teleperformance (TP),3 - 7 years,3-8 Lacs P.A.,['Hyderabad'],"Key Responsibilities:\nAnalyze large volumes of labeled and unlabeled data to identify trends, anomalies, and labeling patterns that can improve model training or operational efficiency.\nDesign and maintain automated dashboards and reporting frameworks to track labeling quality, throughput, and issue trends.\nPartner with Client leadership to understand data requirements and provide actionable insights for model optimization.\nDevelop scalable data pipelines for data validation, aggregation, and visualization.\nApply data mining techniques to evaluate annotation consistency, inter-rater reliability, and data quality.\nContribute to AI data evaluation strategies through analytical experimentation and feedback integration.\nCollaborate with cross-functional teams to enhance data annotation workflows and ensure metrics alignment.\nRequirements:\nBachelors degree in Statistics, Mathematics, Computer Science, Data Science, or a related field.\n3–8 years of hands-on experience in data analysis roles, preferably in AI/ML or data labeling environments.\nProficient in SQL and Python for data manipulation, analysis, and automation.\nUnderstanding of data labeling workflows and familiarity with metrics like accuracy, precision, recall, and inter-rater agreement.\nStrong analytical thinking with the ability to interpret large datasets and provide actionable insights.\nExcellent communication skills with the ability to present findings to both technical and non-technical audiences.\nSelf-starter with a keen eye for detail and a passion for working in AI-driven data environments.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data anayst', 'SQL', 'Artificial Intelligence', 'Google Suite', 'Data Analysis', 'Advanced Excel', 'MI', 'Machine Learning']",2025-06-13 06:12:29
"Senior Python Developer (Machine Learning,Data Analysis,Visualization)",Synechron,3 - 5 years,Not Disclosed,"['Pune', 'Hinjewadi']","Software Requirements\nRequired Skills:\nProficiency in Python (version 3.6+) with experience in data analysis, manipulation, and scripting\nKnowledge of SQL for data extraction, transformation, and database querying\nExperience with data visualization tools such as PowerBI, Tableau, or QlikView\nFamiliarity with AI and Machine Learning frameworks such as TensorFlow, Keras, PyTorch, or equivalent",,,,"['Python', 'PostgreSQL', 'MySQL', 'Data Analysis', 'Data Visualization', 'Oracle', 'ETL', 'Machine Learning']",2025-06-13 06:12:31
Data Scientist,NatWest Markets,5 - 10 years,Not Disclosed,['Bengaluru'],"Join us as a Data Scientist\nYou ll design and implement data science tools and methods which harness our data in order to drive market leading purposeful customer solutions\nWe ll look to you to actively participate in the data community to identify and deliver opportunities to support the bank s strategic direction through better use of data\nThis is an opportunity to promote data literacy education with business stakeholders supporting them to foster a data driven culture and to make a real impact with your work\nWere offering this role at associate level\nWhat youll do\nAs a Data Scientist, you ll bring together statistical, mathematical, machine-learning and software engineering skills to consider multiple solutions, techniques and algorithms to develop and implement ethically sound models end-to-end. We ll look to you to understand the needs of business stakeholders, form hypotheses and identify suitable data and analytics solutions to meet those needs in order to support the achievement of our business strategy.\nYou ll also be:\nUsing data translation skills to work closely with business stakeholders to define detailed business questions, problems or opportunities which can be supported through analytics\nApplying a software engineering and product development lens to business problems, creating, scaling and deploying software driven products and services\nWorking in an Agile way within multi-disciplinary data and analytics teams to achieve agreed project and scrum outcomes\nSelecting, building, training and testing machine learning models considering model valuation, model risk, governance and ethics, making sure that models are ready to implement and scale\nIteratively building and prototyping data analysis pipelines to provide insights that will ultimately lead to production deployment\nThe skills youll need\nYou ll need a strong academic background in a STEM discipline such as Mathematics, Physics, Engineering or Computer Science. You ll have an experience of atleast five years with statistical modelling and machine learning techniques.\nWe ll also look for financial services knowledge, and an ability to identify wider business impact, risk or opportunities and make connections across key outputs and processes\nYou ll also demonstrate:\nThe ability to use data to solve business problems from hypotheses through to resolution\nExperience using Python, Tableau, SQL and software engineering fundamentals\nExperience of of analytics in fraud prevention and detection\nExperience of monitoring and maintaining model performance through developing new dashboards and reports, improve existing dashboards and in-house Python packages\nExperience of exploratory data analysis\nGood communication skills with the ability to proactively engage with a wide range of stakeholders",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Machine learning', 'Agile', 'Scrum', 'Analytics', 'Monitoring', 'Financial services', 'SQL', 'Python']",2025-06-13 06:12:33
Engineering-Bengaluru-Analyst,Goldman Sachs,0 - 3 years,Not Disclosed,['Bengaluru'],"Join our Control, Finance Operating (CFO) Artificial Intelligence (AI) Strategies (Strats) team at Goldman Sachs and play a pivotal role in advancing AI initiatives to drive operational efficiency and enhance processes\nthrough cutting-edge AI and Generative AI (GenAI) modeling and automation. You will get to work on one of the firm s priorities for 2025, and leverage AI solutions to accelerate and transform business technology as well as enhancing the productivity of our people.\nAs a CFO AI Strategist, you will be at the forefront of designing, developing, and implementing innovative AI solutions across diverse areas including Risk, Controllers, Legal, and other critical domains. This exciting opportunity will see you collaborating with key stakeholders from these divisions to strategically deploy prioritized AI applications, thereby significantly improving our operational efficiency and improving productivity. The CFO AI Strats team operates horizontally across CFO divisions, with a clear mandate to strategically roll out impactful AI applications.\nKey Responsibilities:\nDesign, develop, and implement advanced AI and GenAI solutions to address complex challenges in Risk, Controllers, Legal, and other domains.\nCollaborate with stakeholders from various divisions to identify and prioritize AI applications that can drive operational efficiency and process improvements.\nLeverage state-of-the-art GenAI techniques, including Retrieval-Augmented Generation (RAG), AI agents, and other advanced methodologies, to develop robust AI models and solutions.\nPartner with business leaders and classic business teams to understand their needs and integrate AI solutions seamlessly into their workflows.\nWork closely with Goldman Sachs Engineering teams to leverage their technical solutions and infrastructure, ensuring the successful deployment and scalability of AI applications.\nContinuously monitor and evaluate the performance of AI models, making necessary adjustments to optimize their effectiveness and accuracy.\nStay abreast of the latest advancements in AI and GenAI technologies, and proactively identify opportunities to incorporate these innovations into our AI strategies.\nQualifications, Experience, and Attributes:\nBachelors degree or higher in a quantitative subject such as Computer Science, Mathematics, Statistics, Engineering, or a related field.\nProven commercial experience in designing, developing, and implementing AI and GenAI solutions, with a strong focus on practical applications and real-world impact.\nDemonstrated expertise in GenAI techniques, including but not limited to Retrieval-Augmented Generation (RAG), AI agents, and other advanced AI methodologies.\nStrong analytical and problem-solving skills, with the ability to translate complex business requirements into effective AI solutions.\nExcellent collaboration and communication skills, with the ability to work effectively with both technical and non-technical stakeholders.\nA proactive and innovative mindset, with a passion for staying at the forefront of AI and GenAI advancements and applying them to solve business challenges.\nStrong programming skills in languages commonly used in AI development, most importantly Python.\nFamiliarity with AI and machine learning frameworks and tools, such as TensorFlow, PyTorch, or similar, is desirable.\nWe re committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. Learn more: https: / / www.goldmansachs.com / careers / footer / disability-statement.html",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Analytical', 'Artificial Intelligence', 'Machine learning', 'HTML', 'Investment banking', 'Investment management', 'Operations', 'Python']",2025-06-13 06:12:34
"STAFF, DATA SCIENTIST",Walmart,5 - 10 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart s product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat youll do:\nWe are looking for a Staff Machine Learning Engineer who can help build large scale AI/ML/Optimization products. Expected qualities include ability to build, deploy, maintain and troubleshoot large scale systems.\nAs a Staff ML Engineer, you ll have the opportunity to\nDrive research initiatives and proof-of-concepts that push the state of the art in generative AI and large-scale machine learning.\nDesign and implement high-throughput, low-latency AI/ML pipelines and microservices that operate at global scale.\nOversee data ingestion, model training, evaluation, deployment and monitoring-ensuring performance, quality and reliability.\nCustomize and optimize LLMs for specific business use cases, balancing accuracy, latency and cost.\nPrototype novel generative AI solutions, integrate advancements into production, and collaborate with research partners.\nChampion best practices in data quality, lineage, governance and cost optimization across ML pipelines.\nMentor a team of ML engineers, establish coding standards, conduct design reviews, and foster a culture of continuous improvement.\nPresent your team s work at top-tier AI/ML conferences, publish scientific papers, and cultivate partnerships with universities and research labs.\nWhat youll bring:\nPhD in Computer Science, Statistics, Applied Mathematics or related field with 5+ years experience in ML engineering-or Master s with 8+ years or Bachelor s with 10+ years.\nProven track record of leading and scaling AI/ML products in production environments.\nDeep expertise in generative AI, large-scale model deployment, and fine-tuning of transformer-based architectures.\nStrong programming skills in Python, or equivalent, and experience with big data frameworks (Spark, Hadoop) and ML platforms (TensorFlow, PyTorch).\nDemonstrated history of scientific publications or patents in AI/ML.\nExcellent communication skills, a growth mindset, and the ability to drive cross-functional collaboration.\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 4 years experience in an analytics related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 2 years experience in an analytics related field. Option 3: 6 years experience in an analytics or related field.\nPreferred Qualifications...\nPrimary Location...",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Prototype', 'Networking', 'Coding', 'Machine learning', 'Continuous improvement', 'Information technology', 'Monitoring', 'Analytics', 'Python']",2025-06-13 06:12:36
Senior Data Scientist - Multi-Agent AI Systems,Capgemini,9 - 14 years,Not Disclosed,"['Pune', 'Bengaluru']","Role & responsibilities\nWe are seeking an exceptional Data Scientist with specialized expertise in developing multi-agent AI systems. In this role, you will design, implement, and optimize complex AI ecosystems where multiple intelligent agents collaborate to solve sophisticated problems. You will leverage your deep understanding of generative AI, retrieval-augmented generation (RAG), and prompt engineering to create cutting-edge solutions that push the boundaries of artificial intelligence.\nKey Responsibilities\nDesign and develop generative AI-based multi-agent systems that can collaborate, communicate, and coordinate to achieve complex objectives\nArchitect and implement RAG-based chatbot solutions that effectively leverage knowledge bases and external data sources\nCreate sophisticated prompt engineering strategies to optimize AI agent behavior and inter-agent communication\nBuild, train, and fine-tune generative AI models for various applications within multi-agent systems\nDevelop robust evaluation frameworks to measure and improve multi-agent system performance\nImplement efficient knowledge sharing mechanisms between AI agents\nWrite clean, efficient, and well-documented Python code for production-ready AI systems\nCollaborate with cross-functional teams to integrate multi-agent systems into broader product ecosystems\nStay at the forefront of AI research and incorporate state-of-the-art techniques into our solutions\n\nPreferred candidate profile\nMaster's or PhD in Computer Science, Machine Learning, Artificial Intelligence, or related field\n4+ years of professional experience in data science or machine learning engineering\nExtensive experience with Python programming and related data science/ML libraries\nDemonstrated expertise in developing and deploying generative AI models (e.g., LLMs, diffusion models)\nProven experience building RAG-based systems and implementing vector databases\nStrong background in prompt engineering for large language models\nExperience designing and implementing generative AI-based multi-agent architectures\nExcellent problem-solving skills and ability to optimize complex AI systems\n\nPreferred Qualifications\nExperience with LangChain, AutoGPT, CrewAI, or similar frameworks for building agent-based systems\nFamiliarity with orchestration tools for managing complex AI workflows\nKnowledge of agent communication protocols and collaborative problem-solving frameworks\nExperience with distributed systems and cloud computing platforms (AWS, GCP, Azure)\nContributions to open-source AI projects or research publications in relevant fields\nExperience with knowledge graphs and semantic reasoning systems\nFamiliarity with MLOps practices and deployment of AI systems at scale",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Crewai', 'Langchain', 'AutoGPT']",2025-06-13 06:12:37
Senior Data Scientist,Ericsson,3 - 8 years,Not Disclosed,['Bengaluru'],"About this Opportunity\nThe complexity of running and optimizing the next generation of wireless networks, such as 5G with distributed edge compute, will require Machine Learning (ML) and Artificial Intelligence (AI) technologies. Ericsson is setting up an AI Accelerator Hub in India to fast-track our strategy execution, using Machine Intelligence (MI) to drive thought leadership, automate, and transform Ericsson s offerings and operations. We collaborate with academia and industry to develop state-of-the-art solutions that simplify and automate processes, creating new value through data insights.\n\nAs a Senior Data Scientist, you will apply your knowledge of data science and ML tools backed with strong programming skills to solve real-world problems.\nResponsibilities:\n1. Lead AI/ML features/capabilities in product/business areas\n2. Define business metrics of success for AI/ML projects and translate them into model metrics\n3. Lead end-to-end development and deployment of Generative AI solutions for enterprise use cases\n4. Design and implement architectures for vector search, embedding models, and RAG systems\n5. Fine-tune and evaluate large language models (LLMs) for domain-specific tasks\n6. Collaborate with stakeholders to translate vague problems into concrete Generative AI use cases\n7. Develop and deploy generative AI solutions using AWS services such as SageMaker, Bedrock, and other AWS AI tools. Provide technical expertise and guidance on implementing GenAI models and best practices within the AWS ecosystem.\n8. Develop secure, scalable, and production-grade AI pipelines\n9. Ensure ethical and responsible AI practices\n10. Mentor junior team members in GenAI frameworks and best practices\n11. Stay current with research and industry trends in Generative AI and apply cutting-edge techniques\n12. Contribute to internal AI governance, tooling frameworks, and reusable components\n13. Work with large datasets including petabytes of 4G/5G networks and IoT data\n14. Propose/select/test predictive models and other ML systems\n15. Define visualization and dashboarding requirements with business stakeholders\n16. Build proof-of-concepts for business opportunities using AI/ML\n17. Lead functional and technical analysis to define AI/ML-driven business opportunities\n18. Work with multiple data sources and apply the right feature engineering to AI models\n19. Lead studies and creative usage of new/existing data sources",,,,"['Wireless', 'Computer science', 'Data analysis', 'cassandra', 'Neural networks', 'Artificial Intelligence', 'Machine learning', 'Telecommunication', 'data visualization', 'Python']",2025-06-13 06:12:39
"SENIOR, DATA SCIENTIST",Walmart,3 - 8 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart s product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat youll do:\nAs a Senior Data Scientist - ML Engineer, you ll have the opportunity to:\nDrive research initiatives and proof-of-concepts that push the state of the art in generative AI and large-scale machine learning.\nDesign and implement high-throughput, low-latency AI/ML pipelines and microservices that operate at global scale.\nOversee data ingestion, model training, evaluation, deployment and monitoring-ensuring performance, quality and reliability.\nCustomize and optimize LLMs for specific business use cases, balancing accuracy, latency and cost.\nPrototype novel generative AI solutions, integrate advancements into production, and collaborate with research partners.\nChampion best practices in data quality, lineage, governance and cost optimization across ML pipelines.\nMentor a team of ML engineers, establish coding standards, conduct design reviews, and foster a culture of continuous improvement.\nPresent your team s work at top-tier AI/ML conferences, publish scientific papers, and cultivate partnerships with universities and research labs.\nWhat youll bring\nPhD in Computer Science, Statistics, Applied Mathematics or related field with 3+ years experience in ML engineering-or Master s with 6+ years or Bachelor s with 8+ years.\nProven track record of leading and scaling AI/ML products in production environments.\nDeep expertise in generative AI, large-scale model deployment, and fine-tuning of transformer-based architectures.\nStrong programming skills in Python, or equivalent, and experience with big data frameworks (Spark, Hadoop) and ML platforms (TensorFlow, PyTorch).\nDemonstrated history of scientific publications or patents in AI/ML.\nExcellent communication skills, a growth mindset, and the ability to drive cross-functional collaboration.\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1- Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\nPreferred Qualifications...\nPrimary Location...\n\n\n",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Prototype', 'Networking', 'Coding', 'Machine learning', 'Continuous improvement', 'Information technology', 'Monitoring', 'Analytics', 'Python']",2025-06-13 06:12:40
"STAFF, DATA SCIENTIST",Walmart,4 - 9 years,Not Disclosed,['Bengaluru'],"Position Summary... Drives the execution of multiple business plans and projects by identifying customer and operational needs; developing and communicating business plans and priorities; removing barriers and obstacles that impact performance; providing resources; identifying performance standards; measuring progress and adjusting performance accordingly; developing contingency plans; and demonstrating adaptability and supporting continuous learning. Provides supervision and development opportunities for associates by selecting and training; mentoring; assigning duties; building a team-based work environment; establishing performance expectations and conducting regular performance evaluations; providing recognition and rewards; coaching for success and improvement; and ensuring Belonging awareness. Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity by training and providing direction to others in their use and application; ensuring compliance with them; and utilizing and supporting the Open Door Policy. Ensures business needs are being met by evaluating the ongoing effectiveness of current plans, programs, and initiatives; consulting with business partners, managers, co-workers, or other key stakeholders; soliciting, evaluating, and applying suggestions for improving efficiency and cost-effectiveness; and participating in and supporting community outreach events.\nWhat youll do...\nAbout the Team :\nCentroid team at Walmart serves as the backbone of Walmarts end-to-end supply chain strategy. They are entrusted with the task of designing and implementing a long-term supply chain strategy that uses advanced data analytics and data science. Their primary objective is to ensure that Walmart provides top-tier customer service while supporting the increasing demand over time and simultaneously operating at low and efficient costs.\nThe team utilizes sophisticated data analysis methods to understand patterns, identify potential bottlenecks, and predict future trends. This enables them to optimize processes, make informed business decisions, and enhance overall operational efficiency.\nOne of Centroids key responsibilities also includes the creation of a Digital Twin Simulation platform for Walmarts supply chain. This innovative tool allows the team to test and validate all future strategies and tactical decisions before they are launched operationally. It also enables a deep assessment of long-term strategic sensitivity.\nIn essence, the Centroid teams work is integral to ensuring Walmarts supply chain is robust, flexible, and capable of adapting to ever-changing market demands. Their work helps to keep Walmart at the forefront of retail supply chain management, delivering exceptional service to customers while maintaining efficient operational costs.\nWhat Youll do :\nDevelop and manage advanced data analytics models to optimize supply chain strategies, balancing customer satisfaction with operational cost and asset efficiency.\nLeverage data analytics to identify opportunities for improvement and drive impactful results through collaboration with cross-functional teams.\nEstablish relationships across Walmart functional areas to identify best practices, solicit data/input, coordinate interdisciplinary initiatives, and rally support for data-driven recommendations.\nSecure alignment and support from relevant business partners and management for data-centric projects, leading discussions to drive necessary change.\nUtilize all available data resources effectively to ensure successful project outcomes.\nCommunicate data insights clearly and persuasively through emails, verbal discussions, and presentations, tailoring communication methods to the audience for maximum impact.\nCollaborate with multiple supply chain business teams to proactively identify, assess, and leverage cost-saving and service improvement opportunities through advanced data analytics.\nUtilize advanced analytics models to derive insights that will inform policy design across various supply chain areas, laying out multiple scenarios and performing sensitivity analysis.\nCollaborate with Data Scientists and Engineers to productionize and scale advanced analytics models as needed.\nDevelop and present compelling data-driven narratives/documents/visuals to influence key stakeholders in their decision-making.\nProvide coaching and training support to other team members in the supply chain area, leveraging your expertise in advanced data analytics.\nWhat Youll bring :\nStrong analytical acumen with technical expertise in Advanced Data Analytics and modelling\nExpert in SQL, - BigQuery like cloud data platforms.\nExpert in programming in Python, (or R)\nExperience in using data visualization tools like Tableau and Looker and be able to drive powerful insights.\nExperience working with large data sets and distributed computing tools (Map/Reduce, Hadoop, Hive, and/or Spark)\nExperience in operating from a cloud environment such as Google Could Platform or Microsoft Azure.\nAbility to work in a fast-paced, iterative development environment.\nStrong communication skills, both written and verbal, plus ability to work with cross functional teams of technical and non-technical members.\nStrong ability to understand the business and have good stakeholder management capabilities.\nExperience of working in cross-functional environment and leading or mentoring teams.\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 4 years experience in an analytics related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 2 years experience in an analytics related field. Option 3: 6 years experience in an analytics or related field.\nPreferred Qualifications...\nPrimary Location... G, 1, 3, 4, 5 Floor, Building 11, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Data analysis', 'Supply chain management', 'Networking', 'Analytical', 'Consulting', 'Programming', 'Analytics', 'Python', 'SQL']",2025-06-13 06:12:42
Data Analyst-Having Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Analyst\n\nLocation: Bangalore\nExperience: 8 - 15 Yrs\nType: Full-time\n\nRole Overview\n\nWe are seeking a skilled Data Analyst to support our platform powering operational intelligence across airports and similar sectors. The ideal candidate will have experience working with time-series datasets and operational information to uncover trends, anomalies, and actionable insights. This role will work closely with data engineers, ML teams, and domain experts to turn raw data into meaningful intelligence for business and operations stakeholders.\n\nKey Responsibilities\n\nAnalyze time-series and sensor data from various sources\nDevelop and maintain dashboards, reports, and visualizations to communicate key metrics and trends.\nCorrelate data from multiple systems (vision, weather, flight schedules, etc) to provide holistic insights.\nCollaborate with AI/ML teams to support model validation and interpret AI-driven alerts (e.g., anomalies, intrusion detection).\nPrepare and clean datasets for analysis and modeling; ensure data quality and consistency.\nWork with stakeholders to understand reporting needs and deliver business-oriented outputs.\n\n\nQualifications & Required Skills\n\nBachelors or Masters degree in Data Science, Statistics, Computer Science, Engineering, or a related field.\n5+ years of experience in a data analyst role, ideally in a technical/industrial domain.\nStrong SQL skills and proficiency with BI/reporting tools (e.g., Power BI, Tableau, Grafana).\nHands-on experience analyzing structured and semi-structured data (JSON, CSV, time-series).\nProficiency in Python or R for data manipulation and exploratory analysis.\nUnderstanding of time-series databases or streaming data (e.g., InfluxDB, Kafka, Kinesis).\nSolid grasp of statistical analysis and anomaly detection methods.\nExperience working with data from industrial systems or large-scale physical infrastructure.\n\n\nGood-to-Have Skills\n\nDomain experience in airports, smart infrastructure, transportation, or logistics.\nFamiliarity with data platforms (Snowflake, BigQuery, Custom-built using open-source).\nExposure to tools like Airflow, Jupyter Notebooks and data quality frameworks.\nBasic understanding of AI/ML workflows and data preparation requirements.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Kafka', 'SQL', 'airports', 'InfluxDB', 'Airflow', 'structured Data', 'time-series', 'JSON', 'Tableau', 'Grafana', 'R', 'AI/ML', 'Kinesis', 'Snowflake', 'time-series databases', 'Data Preparation', 'Python', 'smart infrastructure', 'BigQuery', 'streaming data', 'Power BI', 'CSV', 'transportation', 'logistic', 'reporting tools']",2025-06-13 06:12:44
S&C Global Network - AI - CG&S - Data Engineer Consultant,Accenture,3 - 7 years,Not Disclosed,['Bengaluru'],"Job Title:Industry & Function AI Data Engineer + S&C GN\n\n\n\nManagement Level:09 - Consultant\n\n\n\nLocation:Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have Skills:Data Engineering expertise, Cloud platforms:AWS, Azure, GCP, Proficiency in Python, SQL, PySpark and ETL frameworks\n\n\n\nGood-to-Have Skills:LLM Architecture, Containerization tools:Docker, Kubernetes, Real-time data processing tools:Kafka, Flink, Certifications like AWS Certified Data Analytics Specialty, Google Professional Data Engineer,Snowflake,DBT,etc.\n\n\n\nJob\n\n\nSummary:\n\nAs a Data Engineer, you will play a critical role in designing, implementing, and optimizing data infrastructure to power analytics, machine learning, and enterprise decision-making. Your work will ensure high-quality, reliable data is accessible for actionable insights. This involves leveraging technical expertise, collaborating with stakeholders, and staying updated with the latest tools and technologies to deliver scalable and efficient data solutions.\n\n\n\n\nRoles & Responsibilities:\nBuild and Maintain Data Infrastructure:Design, implement, and optimize scalable data pipelines and systems for seamless ingestion, transformation, and storage of data.\nCollaborate with Stakeholders:Work closely with business teams, data analysts, and data scientists to understand data requirements and deliver actionable solutions.\nLeverage Tools and Technologies:Utilize Python, SQL, PySpark, and ETL frameworks to manage large datasets efficiently.\nCloud Integration:Develop secure, scalable, and cost-efficient solutions using cloud platforms such as Azure, AWS, and GCP.\nEnsure Data Quality:Focus on data reliability, consistency, and quality using automation and monitoring techniques.\nDocument and Share Best Practices:Create detailed documentation, share best practices, and mentor team members to promote a strong data culture.\nContinuous Learning:Stay updated with the latest tools and technologies in data engineering through professional development opportunities.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nStrong proficiency in programming languages such as Python, SQL, and PySpark\nExperience with cloud platforms (AWS, Azure, GCP) and their data services\nFamiliarity with ETL frameworks and data pipeline design\nStrong knowledge of traditional statistical methods, basic machine learning techniques.\nKnowledge of containerization tools (Docker, Kubernetes)\nKnowing LLM, RAG & Agentic AI architecture\nCertification in Data Science or related fields (e.g., AWS Certified Data Analytics Specialty, Google Professional Data Engineer)\n\n\n\n\n\nAdditional Information:\n\nThe ideal candidate has a robust educational background in data engineering or a related field and a proven track record of building scalable, high-quality data solutions in the Consumer Goods sector.\n\nThis position offers opportunities to design and implement cutting-edge data systems that drive business transformation, collaborate with global teams to solve complex data challenges and deliver measurable business outcomes and enhance your expertise by working on innovative projects utilizing the latest technologies in cloud, data engineering, and AI.\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:Minimum 3-7 years in data engineering or related fields, with a focus on the Consumer Goods Industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Computer Science, Information Systems, Engineering, or a related field",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'data engineering', 'sql', 'machine learning algorithms', 'kubernetes', 'snowflake', 'data analytics', 'microsoft azure', 'cloud platforms', 'machine learning', 'apache flink', 'artificial intelligence', 'docker', 'pipeline', 'data science', 'gcp', 'kafka', 'aws', 'etl', 'etl scripts']",2025-06-13 06:12:45
Data Engineer-Data Platforms-Google,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"Skilled Multiple GCP services - GCS, BigQuery, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer etc.\nMust have Python and SQL work experience & Proactive, collaborative and ability to respond to critical situation\nAbility to analyse data for functional business requirements & front face customer\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5 to 7 years of relevant experience working as technical analyst with Big Query on GCP platform.\nSkilled in multiple GCP services - GCS, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer\nYou love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies\nAmbitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work\n\n\nPreferred technical and professional experience\nCreate up to 3 bullets maxitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications (encouraging then to focus on required skills)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'gcp', 'bigquery', 'cloud sql', 'python', 'hive', 'gen', 'java', 'postgresql', 'spark', 'linux', 'mysql', 'hadoop', 'big data', 'pubsub', 'airflow', 'application engine', 'machine learning', 'sql server', 'dataproc', 'cloud storage', 'bigtable', 'agile', 'sqoop', 'aws', 'data flow']",2025-06-13 06:12:47
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Neo4j, Stardog\n\n\n\n\nGood to have skills :JavaMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand their data needs and provide effective solutions, ensuring that the data infrastructure is robust and scalable to meet the demands of the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge in data engineering.- Continuously evaluate and improve data processes to enhance efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Neo4j.- Good To Have\n\n\n\n\nSkills:\nExperience with Java.- Strong understanding of data modeling and graph database concepts.- Experience with data integration tools and ETL processes.- Familiarity with data quality frameworks and best practices.- Proficient in programming languages such as Python or Scala for data manipulation.\nAdditional Information:- The candidate should have minimum 5 years of experience in Neo4j.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'java', 'data modeling', 'python', 'neo4j', 'hive', 'pyspark', 'data warehousing', 'sql', 'spark', 'hadoop', 'data visualization', 'etl', 'big data', 'data manipulation', 'airflow', 'machine learning', 'data engineering', 'data quality', 'tableau', 'mapreduce', 'kafka', 'sqoop', 'aws', 'etl process']",2025-06-13 06:12:49
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Pune'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Neo4j, Stardog\n\n\n\n\nGood to have skills :JavaMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand their data needs and provide effective solutions, ensuring that the data infrastructure is robust and scalable to meet the demands of the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge in data engineering.- Continuously evaluate and improve data processes to enhance efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Neo4j.- Good To Have\n\n\n\n\nSkills:\nExperience with Java.- Strong understanding of data modeling and graph database concepts.- Experience with data integration tools and ETL processes.- Familiarity with data quality frameworks and best practices.- Proficient in programming languages such as Python or Scala for data manipulation.\nAdditional Information:- The candidate should have minimum 5 years of experience in Neo4j.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'java', 'data modeling', 'python', 'neo4j', 'hive', 'pyspark', 'data warehousing', 'sql', 'spark', 'hadoop', 'data visualization', 'etl', 'big data', 'data manipulation', 'airflow', 'machine learning', 'data engineering', 'data quality', 'tableau', 'mapreduce', 'kafka', 'sqoop', 'aws', 'etl process']",2025-06-13 06:12:51
Sales Excellence - COE - Data Engineering Specialist,Accenture,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nSales Excellence - COE - Data Engineering Specialist\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nMumbai, MDC2C\n\n\n\nMust-have skills:Sales\n\n\n\n\nGood to have skills:Data Science, SQL, Automation, Machine Learning\n\n\n\nJob\n\n\nSummary:\n\nApply deep statistical tools and techniques to find relationships between variables\n\n\n\n\nRoles & Responsibilities:\n\n- Apply deep statistical tools and techniques to find relationships between variables.\n\n- Develop intellectual property for analytical methodologies and optimization techniques.\n\n- Identify data requirements and develop analytic solutions to solve business issues.\n\nJob Title - Analytics & Modelling Specialist\n\nManagement Level :9-Specialist\n\nLocation:Bangalore/ Gurgaon/Hyderabad/Mumbai\n\nMust have skills:Python, Data Analysis, Data Visualization, SQL\nGood to have skills:Machine Learning\n\nJob\n\n\nSummary:\n\nThe Center of Excellence (COE) makes sure that the sales and pricing methods and offerings of Sales Excellence are effective.\n\n- The COE supports salespeople through its business partners and Analytics and Sales Operations teams.\n\nThe Data Engineer helps manage data sources and environments, utilizing large data sets and maintaining their integrity to create models and apps that deliver insights to the organization.\nRoles & Responsibilities:\n\nBuild and manage data models that bring together data from different sources.\n\nHelp consolidate and cleanse data for use by the modeling and development teams.\n\nStructure data for use in analytics applications.\n\nLead a team of Data Engineers effectively.\nProfessional & Technical\n\n\n\n\nSkills:\nA bachelors degree or equivalent\n\nTotal experience Range:5-8 years in the relevant field\n\nA minimum of 3 years of GCP experience with exposure to machine learning/data science\n\nExperience in configuration the machine learning workflow in GCP.\n\nA minimum of 5 years Advanced SQL knowledge and experience working with relational databases\n\nA minimum of 3 years Familiarity and hands on experience in different SQL objects like stored procedures, functions, views etc.,\n\nA minimum of 3 years Building of data flow components and processing systems to extract, transform, load and integrate data from various sources.\n\nA minimum of 3 years Hands on experience in advanced excel topics such as cube functions, VBA Automation, Power Pivot etc.\n\nA minimum of 3 years Hands on experience in Python\nAdditional Information:\n\nUnderstanding of sales processes and systems.\n\nMasters degree in a technical field.\n\nExperience with quality assurance processes.\n\nExperience in project management.\n\nYou May Also Need:\n\nAbility to work flexible hours according to business needs.\n\nMust have good internet connectivity and a distraction-free environment for working at home, in accordance with local guidelines.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:8 to 10 Years\n\n\n\n\nEducational Qualification:\n\n\n\nB.Com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data analysis', 'sales', 'sql', 'data visualization', 'hive', 'advance sql', 'ssas', 'dbms', 'machine learning', 'data engineering', 'power pivot', 'sql server', 'vba automation', 'data science', 'gcp', 'spark', 'advanced excel', 'hadoop', 'ssis', 'etl', 'big data', 'data flow', 'sql joins']",2025-06-13 06:12:53
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analysis', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 06:12:54
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, encompassing the relevant data platform components. You will collaborate with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Your typical day will involve working on the data platform blueprint and design, collaborating with architects, and ensuring seamless integration between systems and data models.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects.- Ensure cohesive integration between systems and data models.- Implement data platform components.- Troubleshoot and resolve data platform issues.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'data architecture', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining']",2025-06-13 06:12:56
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the implementation of data platform solutions.- Conduct performance tuning and optimization of data platform components.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of cloud-based data platforms.- Experience in designing and implementing data pipelines.- Knowledge of data governance and security best practices.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'data analytics', 'data modeling', 'spark', 'data governance', 'python', 'amazon redshift', 'data warehousing', 'microsoft azure', 'emr', 'machine learning', 'sql', 'nosql', 'amazon ec2', 'java', 'kafka', 'mysql', 'hadoop', 'sqoop', 'big data', 'aws', 'etl']",2025-06-13 06:12:58
Data Platform Engineer,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 06:13:00
Data Platform Engineer,Accenture,12 - 15 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Collibra Data Governance\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, encompassing the relevant data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, while also engaging in discussions to refine and enhance the overall data architecture. You will be involved in various stages of the data platform lifecycle, ensuring that all components work harmoniously to support the organization's data needs and objectives.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities and foster a culture of continuous improvement.- Monitor and evaluate team performance, providing constructive feedback to ensure alignment with project goals.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Collibra Data Governance.- Strong understanding of data governance frameworks and best practices.- Experience with data integration tools and techniques.- Familiarity with data modeling concepts and methodologies.- Ability to analyze and interpret complex data sets to inform decision-making.\nAdditional Information:- The candidate should have minimum 12 years of experience in Collibra Data Governance.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data architecture', 'sql', 'data modeling', 'data governance', 'data analysis', 'oracle', 'data management', 'data warehousing', 'business analysis', 'machine learning', 'business intelligence', 'javascript', 'sql server', 'data quality', 'tableau', 'java', 'html', 'mysql', 'etl', 'informatica']",2025-06-13 06:13:02
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 06:13:04
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 06:13:05
Data Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon s Consumer Payments organization is seeking a highly quantitative, experienced Data Engineer to drive growth through analytics, automation of data pipelines, and enhancement of self-serve experiences. . You will succeed in this role if you are an organized self-starter who can learn new technologies quickly and excel in a fast-paced environment. In this position, you will be a key contributor and sparring partner, developing analytics and insights that global executive management teams and business leaders will use to define global strategies and deep dive businesses.\nYou will be part the team that is focused on acquiring new merchants from around the world to payments around the world. The position is based in India but will interact with global leaders and teams in Europe, Japan, US, and other regions. You should be highly analytical, resourceful, customer focused, team oriented, and have an ability to work independently under time constraints to meet deadlines. You will be comfortable thinking big and diving deep. A proven track record in taking on end-to-end ownership and successfully delivering results in a fast-paced, dynamic business environment is strongly preferred.\nResponsibilities include but not limited to:\nDesign, develop, implement, test, and operate large-scale, high-volume, high-performance data structures for analytics and Reporting.\nImplement data structures using best practices in data modeling, ETL/ELT processes, and SQL, AWS Redshift, and OLAP technologies, Model data and metadata for ad hoc and pre-built reporting.\nWork with product tech teams and build robust and scalable data integration (ETL) pipelines using SQL, Python and Spark.\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.\nInterface with business customers, gathering requirements and delivering complete reporting solutions.\nCollaborate with Analysts, Business Intelligence Engineers and Product Managers to implement algorithms that exploit rich data sets for statistical analysis, and machine learning.\nParticipate in strategic tactical planning discussions, including annual budget processes.\nCommunicate effectively with product / business / tech-teams / other Data teams.\n3+ years of data engineering experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['Automation', 'metadata', 'Data modeling', 'Machine learning', 'Data structures', 'OLAP', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-13 06:13:07
IN Senior Associate GenAI S/W Engineer- Data and Analytics,PwC Service Delivery Center,1 - 7 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\n& Summary\nJob Overview\nWe are seeking a highly skilled and versatile polyglot Full Stack Developer with expertise in modern frontend and backend technologies, cloudbased solutions, AI/ML and Gen AI. The ideal candidate will have a strong foundation in fullstack development, cloud platforms (preferably Azure), and handson experience in Gen AI, AI and machine learning technologies.\nKey Responsibilities\nDevelop and maintain web applications using Angular / React.js , .NET , and Python .\nDesign, deploy, and optimize Azure native PaaS and SaaS services, including but not limited to Function Apps , Service Bus , Storage Accounts , SQL Databases , Key vaults, ADF, Data Bricks and REST APIs with Open API specifications.\nImplement security best practices for data in transit and rest. Authentication best practices SSO, OAuth 2.0 and Auth0.\nUtilize Python for developing data processing and advanced AI/ML models using libraries like pandas , NumPy , scikitlearn and Langchain , Llamaindex , Azure OpenAI SDK\nLeverage Agentic frameworks like Crew AI, Autogen etc.\nWell versed with RAG and Agentic Architecture.\nStrong in Design patterns Architectural, Data, Object oriented\nLeverage azure serverless components to build highly scalable and efficient solutions.\nCreate, integrate, and manage workflows using Power Platform , including Power Automate , Power Pages , and SharePoint .\nApply expertise in machine learning , deep learning , and Generative AI to solve complex problems.\nPrimary Skills\nProficiency in React.js , .NET , and Python .\nStrong knowledge of Azure Cloud Services , including serverless architectures and data security.\nExperience with Python Data Analytics libraries\npandas\nNumPy\nscikitlearn\nMatplotlib\nSeaborn\nExperience with Python Generative AI Frameworks\nLangchain\nLlamaIndex\nCrew AI\nAutoGen\nFamiliarity with REST API design , Swagger documentation , and authentication best practices .\nSecondary Skills\nExperience with Power Platform tools such as Power Automate, Power Pages, and SharePoint integration.\nKnowledge of Power BI for data visualization (preferred).\nPreferred Knowledge Areas Nice to have\nIndepth understanding of Machine Learning , deep learning, supervised, unsupervised algorithms.\nMandatory skill sets\nAI, ML\nPreferred skill sets\nAI, ML\nYears of experience required\n3 7 years\nEducation qualification\nBE/BTECH, ME/MTECH, MBA, MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration, Bachelor of Engineering, Master of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nGame AI\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 28 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Front end', 'Architecture', 'Data modeling', 'data security', 'Machine learning', 'data visualization', 'Apache', 'SQL', 'Python', 'Data architecture']",2025-06-13 06:13:08
Assistant Data Scientist,Rocket Software,0 - 1 years,Not Disclosed,['Pune'],"Face to Face interview in Pune . Please apply only if you are available for a Face to Face interview .\n\nJob highlights\n\nRequired Qualifications . 0 -2 years of relevant industry experience or fresh graduates are welcome to apply.\nBasic experience or understanding in applying Data Science methodologies to extract, process, and transform data from multiple sources.\nPreferred Qualifications . Bachelors degree in Data Science , AI, Statistics ,Computer Science, Economics, or a directly related field.\n\nEssential Duties and Responsibilities\n\nAssist in developing, fine-tuning, and deploying machine learning models.\nAid in consulting with key internal and external stakeholders to understand and frame model requirements and potential applications.\nParticipate in the development of sound analytic plans based on available data sources, business partner needs, and required timelines.\nWork with software engineers in integrating trained models into end-user applications.\nHelp manage deliverables across multiple projects in a deadline-driven environment.\nPresent results, insights, and recommendations to both technical and non-technical stakeholders.\n\nRequired Qualifications\n\n0 -2 years of relevant industry experience or fresh graduates are welcome to apply.\nGood knowledge of Python and Linux, familiarity with ML frameworks, and a willingness to learn.\nDemonstrated problem-solving abilities and creative thinking.\nBasic experience or understanding in applying Data Science methodologies to extract, process, and transform data from multiple sources.\nExcellent communication and interpersonal skills.\nMust be comfortable working in a team-oriented environment.\n\nPreferred Qualifications\n\nBachelor's degree in Statistics, Computer Science, Economics, or a directly related field.\nMasters degree or current enrollment in a Masters program in Statistics, Computer Science, Mathematics, Economics, or directly related fields is a plus.\nDemonstrated passion for continued learning and innovation.\nAs a Data Science Assistant, we expect not just skills and qualifications, but also an enthusiasm for learning and growing within our team. We value those who are adaptable, innovative, and ready to take on challenges in a fast-paced work environment.\n\nDiversity, Inclusion & Equity\n\nAt Rocket we are committed to an inclusive workplace environment, where every Rocketeer can thrive by bringing their full selves to work. Being a Rocketeer means you are part of our movement to continually drive inclusivity, diversity and equity in our workforce.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'NLP', 'Natural Language Processing', 'Artificial Intelligence', 'Machine Learning', 'Python']",2025-06-13 06:13:10
LLM Engineer,Factspan Analytics,6 - 11 years,Not Disclosed,['Bengaluru'],"Job Description\nAs an LLM (Large Language Model) Engineer, you will be responsible for designing, optimizing, and standardizing the architecture, codebase, and deployment pipelines of LLM-based systems. Your primary mission will focus on modernizing legacy machine learning codebases (including 40+ models) for a major retail clientenabling consistency, modularity, observability, and readiness for GenAI-driven innovation. You’ll work at the intersection of ML, software engineering, and MLOps to enable seamless experimentation, robust infrastructure, and production-grade performance for language-driven systems.",,,,"['Vertex Ai', 'Natural Language Processing', 'Retrieval Augmented Generation', 'Python', 'Kubernetes', 'GCP']",2025-06-13 06:13:11
Data Engineer-Business Intelligence,IBM,5 - 10 years,Not Disclosed,['Hyderabad'],"Provide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client’s environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5+ years of experience with BI tools, with expertise and/or certification in at least one major BI platform – Tableau preferred.\nAdvanced knowledge of SQL, including the ability to write complex stored procedures, views, and functions.\nProven capability in data storytelling and visualization, delivering actionable insights through compelling presentations.\nExcellent communication skills, with the ability to convey complex analytical findings to non-technical stakeholders in a clear, concise, and meaningful way.\n5.Identifying and analyzing industry trends, geographic variations, competitor strategies, and emerging customer behavior\n\n\nPreferred technical and professional experience\nTroubleshooting capabilities to debug Data controls Capable of converting business requirements into workable model.\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advance sql', 'sql', 'bi tools', 'debugging', 'troubleshooting', 'python', 'data analysis', 'data analytics', 'bi', 'data warehousing', 'power bi', 'business analysis', 'machine learning', 'business intelligence', 'sql server', 'qlikview', 'tableau', 'r', 'data visualization', 'etl', 'ssis']",2025-06-13 06:13:13
Senior Data Research Analyst,Morningstar,0 - 7 years,Not Disclosed,['Mumbai'],"As a Senior Data Research Analyst , you will be responsible for acquiring and validating portfolio holdings data from various vendor sources. Your core responsibilities will involve standardizing this data into agreed formats using internal collection tools and resolving exceptions through thorough validation processes.\nWorking within the Portfolio Data Team, your role will focus on ensuring the accuracy and completeness of portfolio information, which is critical for downstream analytics and reporting. You will collaborate closely with leadership and cross-functional teams to support strategic goals, enhance operational performance, and contribute to the achievement of key KPIs.\nShift: UK/AU /US\nRoles Responsibilities:\nActively collect managed investment data using Morningstar collection systems, and ensure data timelines, completeness and accuracy to meet business goals.\nManage relationships between Morningstar and Asset Management companies, insurance companies and other data vendors.\nPartner with quality assurance, products, and technical departments to resolve clients data issues timely and effectively.\nP articipat e in the initiative s focused on consolidating global data collection platforms and supporting database integration projects.\nEstablish and achieve the set O bjectives K ey R esults (OKRs) with the direction of team lead.\nMonitor, analyze and execute summary reports including an investigation of potential data error to c ontinuously improve data collection and quality assurance process using L ean S ix S igma tools.\nActively discover and raise issues in work (including system, process, and collection methodology ) and propose enhancement suggestions to further improve system functionality, process efficiency and data quality.\nParticipate in data and process related projects such as industry/market research, market expansion, process certification, new product development support, etc.\nFacilitate cross-team projects to implement approved solutions based on priority and impact .\nDemonstrate a high sense of ownership of the process , u nderstand roles responsibilities by act ing as a process trainer and mentor\nRequirements:\n> 3 years experience in finance domain , w ith emphasis on collection systems and methodologies, senior data research analyst role or above .\nFund Portfolio experience would be preferred\nG ood command i n MS Office (Excel, PowerPoint etc.); advanced users preferred. SQL, Macro or Python and machine learning will be a plus.\nShould be critical thinker and should possess good communication skill .\nShould be equipped with understanding of data competencies like data content expertise , data analysis etc.\nStrong analytical, problem-solving capabilities, and excellent communication written as well as verbal reporting skills.\nShould be a good team player with good learning ability and equipped with self-motivation in an independent, fast-paced work environment.\nAbility to exercise control over the planned activities like training / mentoring new hires, doing quality checks etc .\nAble to work under tight deadlines and handle pressure during peak seasons.\nGood project management skills with proven track record of working on and delivering projects independently.\nRemote team working experience is a plus .\nFlexibility to work in shifts.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Manager Quality Assurance', 'Analytical', 'Data collection', 'Market research', 'Data quality', 'Asset management', 'Operations', 'Analytics', 'SQL']",2025-06-13 06:13:14
Full Stack Data Scientist,Vimo Getinsured,2 - 7 years,Not Disclosed,['Gurugram( Sector 61 Gurgaon )'],"About the Role\nAs a Data Science Engineer, you will need strong technical skills in data modeling, machine learning, data engineering, and software development. You will have the ability to conduct literature reviews and critically evaluate research papers to identify applicable techniques. Additionally, you should be able to design and implement efficient and scalable data processing pipelines, perform exploratory data analysis, and collaborate with other teams to integrate data science models into production systems. Passion for conversational AI and a desire to solve some of the most complex problems in the Natural Language Processing space are essential. You will work on highly scalable, stable, and automated deployments, aiming for high performance. Taking on the challenge of building and scaling a truly remarkable AI platform to impact the lives of millions of customers will be part of your responsibilities. Working in a challenging yet enjoyable environment, where learning new things is the norm, you should think of solutions beyond boundaries. You should also drive outcomes with full ownership, deeply believe in customer obsession, and thrive in a fast-paced environment of learning and innovation.\nYou will work in a challenging, consumer-facing problem space, where you can make an immediate impact. You will get to work with the latest technologies, learn to use new tools and get the opportunity to have your say in the final product. Youll work alongside a great team in an open, collaborative environment. We are part of Vimo, a well-funded, stable mid-size company with excellent salaries, medical/dental/vision coverage, and perks. Vimo is an Equal Opportunity Employer.",,,,"['python', 'Langchain', 'Neural Networks', 'LLM', 'Linux', 'Data Structures', 'Natural Language Processing', 'Jupyter Notebook', 'Machine Learning', 'Deep Learning', 'Numpy', 'Data Science', 'pandas', 'Nltk', 'Langgraph', 'Transformers', 'BERT', 'langsmith']",2025-06-13 06:13:16
Data Scientist,Callaway Digital Technologies,6 - 9 years,Not Disclosed,['Hyderabad'],"JOB OVERVIEW\nThe ideal candidate will be responsible for analyzing and interpreting large data sets related to finance, sales and supply chain operations to optimize business processes, identify opportunities for improvement, and provide strategic insights to support decision-making. The Data Scientist will work closely with cross-functional teams to identify key business questions, design and implement statistical models, and develop innovative data-driven solutions.\nKey Responsibilities:",,,,"['Statistical Modeling', 'Machine Learning', 'Python', 'Data Visualization', 'Azzure', 'R Program', 'SQL']",2025-06-13 06:13:18
Data Scientist,Paypal,2 - 5 years,Not Disclosed,['Bengaluru'],"The Company\nPayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.\nWe operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.\nOur beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do - and they push us to ensure we take care of ourselves, each other, and our communities.\nJob Summary:\nMeet your team:\n\nThis role sits within Credit card fraud risk strategy team of Global Fraud Prevention Org., focused on safeguarding our customers and business from evolving fraud threats. The team is responsible for developing and executing data-driven strategies to mitigate fraud in UK PPC portfolio.\n\nWhat do you need to know about the role:\n\nGlobal Fraud Prevention resides in the Global Risk Management (GRM) organization that supports various business lines in optimizing risk and rewards to enable profitable business growth.\n\nYou will be working closely with global fraud risk professionals focusing on managing and mitigating fraud risk in the UK PayPal Credit Portfolio,\n\nBe a part of this fraud revolution and enjoy the journey being with PayPal s growing team. Why You ll Love It here:\n\nImpact: Your work directly influences the success and growth of PayPal s credit offerings.\nLearning: Expect to level up your analytical and problem-solving skills every day with more challenges to solve\nGrowth: The credit card industry is constantly evolving, and you ll be right there on the cutting edge, sharpening your skills and new learnings along the way.\nCulture: We re a team of passionate professionals who love challenges and are always ready to celebrate a job well done.\nJob Description:\nYour way to impact:\nOwn the areas of Transaction Fraud risk policy: Work on Broad area of projects from Card risk strategies, acquisition and payment risk strategies, all depending on the business need.\nWork closely with Stakeholders: this includes Credit Risk, Product, finance teams to optimize fraud strategies and portfolio performance.\nProactively identify emerging fraud trends and propose mitigation strategies .\nMaintain and develop Monitoring and Alerting capabilities: to clearly monitor the PPC Card program health and simplify insights for key stakeholders.\nPresent regular updates to senior leaders: on Portfolio Health, highlights, lowlights, and actionable insights.\nYour day to day:\nIn this role you will have full ownership of portfolio and is responsible for end-to-end management of Fraud loss and decline rates.\nWorks independently and proficiently. Accountable for own results.\nCollaborate with different teams to develop strategies for fraud prevention, loss savings, and optimize transaction declines .\nAnalyze and assess risks to provide informed recommendations for mitigation strategies\nPrepare periodic KPI reports summarizing the business units risk and control environment for senior management\nWhat do you need to bring:\n2-5 years of domain expertise\nExcellent Problem-Solving Skills : Strong judgment and the ability to think strategically, creatively, and practically to address complex challenges.\nAdvanced Analytics expertise : Proficiency in SQL, Python, Advanced Excel, Tableau, and other analytics tools, with a proven track record of using them to solve real-world problems.\nExceptional communication skills : Outstanding written, verbal communication abilities, capable of translating complex technical concepts into clear, actionable insights for diverse audiences\nCollaboration Influence : Strong ability to collaborate across teams, build relationships, and drive results through influence and teamwork\nExperience in Payments / Transaction risk management / Credit / Fraud Risk is a strong plus.\n** We know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please dont hesitate to apply.\nPreferred Qualification:\nSubsidiary:\nPayPal\nTravel Percent:\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits:\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com .\nWho We Are:\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at talentaccommodations@paypal.com .\nBelonging at PayPal:\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: FinTech / Payments,Department: Other,"Employment Type: Full Time, Permanent","['advanced analytics', 'PPC', 'Analytical', 'Diversity and Inclusion', 'Wellness', 'Advanced Excel', 'Risk management', 'Analytics', 'Monitoring', 'SQL']",2025-06-13 06:13:20
Software Engineer,MNC,0 - 1 years,2.25-3.5 Lacs P.A.,['Pune'],"We Have requirements for a Software Engineer position:\n\nKey Responsibilities\n\n1. Design and Development: Design, develop, test, and maintain software applications.\n2. Coding: Write high-quality, efficient, and well-documented code.\n3. Troubleshooting: Identify and resolve software issues.\n4. Collaboration: Work with cross-functional teams, including QA, DevOps, and Product Management.\n5. Staying Up-to-Date: Stay current with industry trends, technologies, and best practices.\n\nRequirements\n\n\n1. Education: Bachelor's or Master's degree in Computer Science, Engineering, or related fields Any Graduate.\n2. Programming Skills: Proficiency in one or more programming languages (e.g., Java, Python, C++, JavaScript)\n3. Software Development Methodologies: Experience with Agile development methodologies.\n4. Communication: Excellent communication and problem-solving skills.\n5. Teamwork: Ability to work collaboratively in a team environment.\n\nNice-to-Have Skills\n\n1. Cloud Platforms: Experience with cloud platforms (e.g., AWS, Azure, Google Cloud).\n2. DevOps Tools: Familiarity with DevOps tools (e.g., Jenkins, Docker, Kubernetes).\n3. Data Structures and Algorithms: Strong understanding of data structures and algorithms.\n4. Machine Learning: Basic understanding of machine learning concepts.\n\nLocation - Pune\n\nApply Now",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Java', 'Python', 'C++', 'C', 'HTML', 'SQL', 'Software Development', 'It Development', 'IT Consulting', 'Software Engineering', 'Technical Support', 'Javascript', 'Software Support', 'Software Programming']",2025-06-13 06:13:22
Senior Data Scientist,Capgemini,5 - 9 years,Not Disclosed,['Gurugram'],"At Capgemini Invent, we believe difference drives change. As inventive transformation consultants, we blend our strategic, creative and scientific capabilities,collaborating closely with clients to deliver cutting-edge solutions. Join us to drive transformation tailored to our client's challenges of today and tomorrow.Informed and validated by science and data. Superpowered by creativity and design. All underpinned by technology created with purpose.\n\n \n\nYour role \n\nAs a Senior Data Scientist, you are expected to develop and implement Artificial Intelligence based solutions across various disciplines for the Intelligent Industry vertical of Capgemini Invent. You are expected to work as an individual contributor or along with a team to help design and develop ML/NLP models as per the requirement. You will work closely with the Product Owner, Systems Architect and other key stakeholders right from conceptualization till the implementation of the project. You should take ownership while understanding the client requirement, the data to be used, security & privacy needs and the infrastructure to be used for the development and implementation.\n\nThe candidate will be responsible for executing data science projects independently to deliver business outcomes and is expected to demonstrate domain expertise, develop, and execute program plans and proactively solicit feedback from stakeholders to identify improvement actions. This role requires a strong technical background, excellent problem-solving skills, and the ability to work collaboratively with stakeholders from different functional and business teams.\nThe role also requires the candidate to collaborate on ML asset creation and eager to learn and impart trainings to fellow data science professionals. We expect thought leadership from the candidate, especially on proposing to build a ML/NLP asset based on expected industry requirements. Experience in building Industry specific (e.g. Manufacturing, R&D, Supply Chain, Life Sciences etc), production ready AI Models using microservices and web-services is a plus.\n\nProgramming Languages Python NumPy, SciPy, Pandas, MatPlotLib, Seaborne\nDatabases RDBMS (MySQL, Oracle etc.), NoSQL Stores (HBase, Cassandra etc.)\nML/DL Frameworks SciKitLearn, TensorFlow (Keras), PyTorch,\nBig data ML Frameworks - Spark (Spark-ML, Graph-X), H2O\nCloud Azure/AWS/GCP\n\n \n\nYour Profile \n\nPredictive and Prescriptive modelling using Statistical and Machine Learning algorithms including but not limited to Time Series, Regression, Trees, Ensembles, Neural-Nets (Deep & Shallow CNN, LSTM, Transformers etc.). Experience with open-source OCR engines like Tesseract, Speech recognition, Computer Vision, face recognition, emotion detection etc. is a plus.\nUnsupervised learning Market Basket Analysis, Collaborative Filtering, Dimensionality Reduction, good understanding of common matrix decomposition approaches like SVD. Various Clustering approaches Hierarchical, Centroid-based, Density-based, Distribution-based, Graph-based clustering like Spectral.\nNLP Information Extraction, Similarity Matching, Sentiment Analysis, Text Clustering, Semantic Analysis, Document Summarization, Context Mapping/Understanding, Intent Classification, Word Embeddings, Vector Space Models, experience with libraries like NLTK, Spacy, Stanford Core-NLP is a plus. Usage of Transformers for NLP and experience with LLMs like (ChatGPT, Llama) and usage of RAGs (vector stores like LangChain & LangGraps), building Agentic AI applications.\nModel Deployment ML pipeline formation, data security and scrutiny check and ML-Ops for productionizing a built model on-premises and on cloud.\n\nRequired Qualifications\nMasters degree in a quantitative field such as Mathematics, Statistics, Machine Learning, Computer Science or Engineering or a bachelors degree with relevant experience.\nGood experience in programming with languages such as Python/Java/Scala, SQL and experience with data visualization tools like Tableau or Power BI.\n\nPreferred Experience\nExperienced in Agile way of working, manage team effort and track through JIRA\nExperience in Proposal, RFP, RFQ and pitch creations and delivery to the big forum.\nExperience in POC, MVP, PoV and assets creations with innovative use cases\nExperience working in a consulting environment is highly desirable.\nPresupposition\n\nHigh Impact client communication\nThe job may also entail sitting as well as working at a computer for extended periods of time. Candidates should be able to effectively communicate by telephone, email, and face to face.\n\n \n\nWhat you will love about working here \nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['numpy', 'sql', 'java', 'python', 'pandas', 'scala', 'poc', 'nltk', 'dl', 'artificial intelligence', 'tensorflow', 'spacy', 'spark', 'gcp', 'pytorch', 'keras', 'mysql', 'hbase', 'ml', 'jira', 'scipy', 'rdbms', 'oracle', 'mvp', 'microsoft azure', 'power bi', 'nosql', 'tableau', 'cassandra', 'matplotlib', 'agile', 'aws']",2025-06-13 06:13:23
Data Scientist-Advanced Analytics,IBM,3 - 7 years,Not Disclosed,['Kochi'],"We are seeking a highly skilled Advanced Analytics Specialist to join our dynamic team. The successful candidate will be responsible for leveraging advanced analytics techniques to derive actionable insights, inform business decisions, and drive strategic initiatives. This role requires a deep understanding of data analysis, statistical modeling, machine learning, and data visualization.\nIn this role, you will be responsible for architecting and delivering AI solutions using cutting-edge technologies, with a strong focus on foundation models and large language models. You will work closely with customers, product managers, and development teams to understand business requirements and design custom AI solutions that address complex challenges. Experience with tools like Github Copilot, Amazon Code Whisperer etc. is desirable.\nSuccess is our passion, and your accomplishments will reflect this, driving your career forward, propelling your team to success, and helping our clients to thrive.\nDay-to-Day Duties:\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocumentation and Knowledge SharingDocument solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members.\nIndustry Trends and InnovationStay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop and implement advanced analytical models and algorithms to solve complex business problems, analyze large datasets to uncover trends, patterns, and insights that drive business performance.\nCollaborate with cross-functional teams to identify key business challenges and opportunities, Create and maintain data pipelines and workflows to ensure the accuracy and integrity of data, Design and deliver insightful reports and dashboards to communicate findings to stakeholders.\nStay up to date with the latest advancements in analytics, machine learning, and data science. Provide technical expertise and mentorship to junior team members.\nQualificationsBachelor’s or master’s degree in data science, Statistics, Mathematics, Computer Science, or a related field. Proven experience in advanced analytics, data science, or a similar role. Proficiency in programming languages such as Python, R, or SQL. Experience with data visualization tools like Tableau, Power BI, or similar.\nStrong understanding of statistical modelling and machine learning algorithms. Excellent analytical, problem-solving, and critical thinking skills. Ability to communicate complex analytical concepts to non-technical stakeholders. Experience with big data technologies (e.g., Hadoop, Spark) is a plus\n\n\nPreferred technical and professional experience\nFamiliarity with cloud-based analytics platforms (e.g., AWS, Azure).\nKnowledge of natural language processing (NLP) and deep learning techniques.\nExperience with project management and agile methodologies",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'machine learning', 'statistical modeling', 'data visualization', 'machine learning algorithms', 'advanced analytics', 'python', 'github', 'natural language processing', 'power bi', 'microsoft azure', 'sql', 'r', 'tableau', 'java', 'data science', 'spark', 'hadoop', 'aws']",2025-06-13 06:13:25
Data Scientist Sr. Analyst,Accenture,5 - 10 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Big Data, Python or R\n\n\n\n\nGood to have skills:Scala, SQL\n\n\n\nJob\n\n\nSummary\n\nA Data Scientist is expected to be hands-on to deliver end to end vis a vis projects undertaken in the Analytics space. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes.\n\n\n\nRoles and Responsibilities\nIdentify valuable data sources and collection processes\nSupervise preprocessing of structured and unstructured data\nAnalyze large amounts of information to discover trends and patterns for insurance industry.\nBuild predictive models and machine-learning algorithms\nCombine models through ensemble modeling\nPresent information using data visualization techniques\nCollaborate with engineering and product development teams\nHands-on knowledge of implementing various AI algorithms and best-fit scenarios\nHas worked on Generative AI based implementations\n\n\n\nProfessional and Technical Skills\n3.5-5 years experience in Analytics systems/program delivery; at least 2 Big Data or Advanced Analytics project implementation experience\nExperience using statistical computer languages (R, Python, SQL, Pyspark, etc.) to manipulate data and draw insights from large data sets; familiarity with Scala, Java or C++\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks\nKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications\nHands on experience in Azure/AWS analytics platform (3+ years)\nExperience using variations of Databricks or similar analytical applications in AWS/Azure\nExperience using business intelligence tools (e.g. Tableau) and data frameworks (e.g. Hadoop)\nStrong mathematical skills (e.g. statistics, algebra)\nExcellent communication and presentation skills\nDeploying data pipelines in production based on Continuous Delivery practices.\n\n\n\n\nAdditional Information\nMulti Industry domain experience\nExpert in Python, Scala, SQL\nKnowledge of Tableau/Power BI or similar self-service visualization tools\nInterpersonal and Team skills should be top notch\nNice to have leadership experience in the past\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'scala', 'sql', 'r', 'big data', 'advanced analytics', 'mathematics', 'data manipulation', 'presentation skills', 'microsoft azure', 'pyspark', 'power bi', 'machine learning', 'javascript', 'aws kinesis', 'tableau', 'decision tree', 'java', 'hadoop', 'data visualization', 'aws', 'statistics']",2025-06-13 06:13:27
Data Scientist,Jsg. Consulting. Pvt.Ltd.,3 - 5 years,9.6-10.8 Lacs P.A.,['Jaipur'],"Familiarity with MDM (Meter Data Management), HES, and utility billing systems.\nExposure to AMI events analysis, load curves, and customer behavior analytics.\nKnowledge of regulatory requirements, data retention, and data .",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['billing exceptions', 'load profiling', 'Machine Learning', 'Meter Data Management', 'Smart Metering', 'Hes']",2025-06-13 06:13:28
Lead Data Scientist,Grab,8 - 13 years,Not Disclosed,['Bengaluru'],"Get to know the team\nGrabFin is an aggregate of FinTech businesses spread across 6 countries in S.E. Asia, in the Lending, Payments and Insurance domains. We are excited to provide innovative financial services to all participants of the Grab Ecosystem be it our Drivers, Consumers or Merchants. Our products are built on fundamental market insights combined with data science and engineering to bring the best product market fit across the cross section of our user base. This understanding of our ecosystem combined with world class engineering execution continues to create tremendous value for our customers.\nThe data scientist will work in a relatively flat team structure with an independent goal of building and manage critical data science models daily. You can expect to solve hard technical problems and grow into an expert on both batch and real-time Data Science use cases. You will have experience with technology and data science.\nYou will be reporting to Senior Manager, Data Science.\nThis role is onsite based in Bangalore.\nYoull develop credit risk scoring models for consumer loans, including PD, LGD, and collection models. Youll work with alternative data sources to boost model signal and accuracy. Your role will involve full ownership of the end-to-end model lifecycle from building and validation to deployment and maintenance. Youll collaborate with business, risk, and operations teams to shape solutions and influence product strategy with your insights. This is an individual contributor role suited for professionals with 8+ years of experience.\nThe Critical Tasks You Will Perform\nBuild predictive models using a mix of machine learning and traditional analytics methods to segregate between Good vs Bad borrowers\nBuild Machine learning & Deep learning models to estimate losses from of a given portfolio.\nValidate models on new datasets, based on in-market performance.\nEngineer predictive features from internal data assets to build refined customer profiles. Identify external data assets to bring into the model mix.\nDrive model governance by collaborating with risk policy, compliance, and audit teams to ensure adherence to regulatory expectations.\nIdentify model gaps or performance drifts and lead model refresh cycles.\nPresent findings to senior leadership with clear articulation of risk trade-offs and growth.\nTranslate model insights into strategic recommendations (e.g., policy changes, pricing levers, customer targeting strategies).\nSolve previously unsolved analytics problems using best in class data analytics and machine learning methodologies.\nRead more\nSkills you need\nThe Essential Skills You Need\n8+ years of experience.\nStrong understanding of credit business - lifecycle of a loan, collections process, and credit KPIs like NPL, ECL.\nExpert in building machine learning and predictive models in Python and Spark is an absolute must.\nSQL, Presto, Hive proficiency.\nSound knowledge of machine learning concepts. Illustrative machine learning concepts/methods are: Bagging, Boosting, Regularisation, Online Learning, Recommendation Engines\nExperience with LLMs, and Generative AI\nExperience with model deployment pipelines - using MLFlow, Airflow, or other MLOps tools.\nDemonstrated experience building machine learning models\nUnderstand the trade-offs between model performance and our needs.\nStrong problem-solving mindset is critical for success in this role.\nRead more\nWhat we offer\nAbout Grab and Our Workplace\nGrab is Southeast Asias leading superapp. From getting your favourite meals delivered to helping you manage your finances and getting around town hassle-free, weve got your back with everything. In Grab, purpose gives us joy and habits build excellence, while harnessing the power of Technology and AI to deliver the mission of driving Southeast Asia forward by economically empowering everyone, with heart, hunger, honour, and humility.\nRead more\nLife at Grab\nLife at Grab\nWe care about your well-being at Grab, here are some of the global benefits we offer:\nWe have your back with Term Life Insurance and comprehensive Medical Insurance.\nWith GrabFlex, create a benefits package that suits your needs and aspirations.\nCelebrate moments that matter in life with loved ones through Parental and Birthday leave, and give back to your communities through Love-all-Serve-all (LASA) volunteering leave\nWe have a confidential Grabber Assistance Programme to guide and uplift you and your loved ones through lifes challenges.\nWhat we stand for at Grab\nWe are committed to building an inclusive and equitable workplace that enables diverse Grabbers to grow and perform at their best. As an equal opportunity employer, we consider all candidates fairly and equally regardless of nationality, ethnicity, religion, age, gender identity, sexual orientation, family commitments, physical and mental impairments or disabilities, and other attributes that make them unique.\n#LI-DNI\nRead more",Industry Type: IT Services & Consulting,"Department: UX, Design & Architecture","Employment Type: Full Time, Permanent","['Loans', 'data science', 'Machine learning', 'Manager Technology', 'Medical insurance', 'Financial services', 'SQL', 'Python', 'Auditing', 'ECL']",2025-06-13 06:13:30
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\n Your primary responsibilities include: \nDevelop & maintain data pipelines for batch & stream processing using informatica power centre or cloud ETL/ELT tools.\nLiaise with business team and technical leads, gather requirements, identify data sources, identify data quality issues, design target data structures, develop pipelines and data processing routines, perform unit testing and support UAT.\nWork with data scientist and business analytics team to assist in data ingestion and data-related technical issues.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter\nKnowledge of Cloud, Power BI, Data migration on cloud skills.\nExperience in Unix shell scripting and python\nExperience with relational SQL, Big Data etc\n\n\nPreferred technical and professional experience\nKnowledge of MS-Azure Cloud\nExperience in Informatica PowerCenter\nExperience in Unix shell scripting and python",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'elastic search', 'unix shell scripting', 'splunk', 'agile', 'big data', 'informatica']",2025-06-13 06:13:31
Data Engineer Specialist,Accenture,3 - 4 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level :\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python, Pyspark\n\n\n\n\nGood to have skills:Redshift\n\n\n\nJob\n\n\nSummary: We are seeking a highly skilled and experienced Senior Data Engineer to join our growing Data and Analytics team. The ideal candidate will have deep expertise in Databricks and cloud data warehousing, with a proven track record of designing and building scalable data pipelines, optimizing data architectures, and enabling robust analytics capabilities. This role involves working collaboratively with cross-functional teams to ensure the organization leverages data as a strategic asset. Your responsibilities will include:\n\n\n\n\nRoles & Responsibilities\nDesign, build, and maintain scalable data pipelines and ETL processes using Databricks and other modern tools.\nArchitect, implement, and manage cloud-based data warehousing solutions on Databricks (Lakehouse Architecture)\nDevelop and maintain optimized data lake architectures to support advanced analytics and machine learning use cases.\nCollaborate with stakeholders to gather requirements, design solutions, and ensure high-quality data delivery.\nOptimize data pipelines for performance and cost efficiency.\nImplement and enforce best practices for data governance, access control, security, and compliance in the cloud.\nMonitor and troubleshoot data pipelines to ensure reliability and accuracy.\nLead and mentor junior engineers, fostering a culture of continuous learning and innovation.\nExcellent communication skills\nAbility to work independently and along with client based out of western Europe\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 3-4 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:5-8 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'data bricks', 'glue', 'amazon redshift', 'data warehousing', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'snowflake', 'scipy', 'data analysis', 'azure data lake', 'microsoft azure', 'power bi', 'javascript', 'pandas', 'tableau', 'lambda expressions', 'aws']",2025-06-13 06:13:33
Data Engineer,Capgemini,6 - 9 years,Not Disclosed,['Gurugram'],"\nesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nWork together with data scientists and analysts to understand the needs for data and create effective data workflows.\nCreate and maintain data storage solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations.\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nImprove the scalability, efficiency, and cost-effectiveness of data pipelines.\nMonitoring and resolving data pipeline problems will guarantee consistency and availability of the data.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'azure data factory', 'sql', 'azure blob storage', 'sql azure', 'hive', 'azure databricks', 'python', 'data validation', 'pyspark', 'data warehousing', 'power bi', 'data engineering', 'spark', 'data ingestion', 'software engineering', 'hadoop', 'etl', 'big data', 'aws', 'sql database']",2025-06-13 06:13:35
Data Engineer,Capgemini,6 - 9 years,Not Disclosed,['Hyderabad'],"\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nWork together with data scientists and analysts to understand the needs for data and create effective data workflows.\nCreate and maintain data storage solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations.\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nImprove the scalability, efficiency, and cost-effectiveness of data pipelines.\nMonitoring and resolving data pipeline problems will guarantee consistency and availability of the data.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'azure data factory', 'sql', 'azure blob storage', 'sql azure', 'hive', 'azure databricks', 'python', 'data validation', 'pyspark', 'data warehousing', 'power bi', 'data engineering', 'spark', 'data ingestion', 'software engineering', 'hadoop', 'etl', 'big data', 'aws', 'sql database']",2025-06-13 06:13:37
Data Engineer-Data Platforms-Google,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['elastic search', 'gcp', 'splunk', 'hadoop', 'big data', 'hive', 'python', 'data management', 'presentation skills', 'microsoft azure', 'machine learning', 'javascript', 'sql', 'docker', 'java', 'git', 'spark', 'linux', 'jenkins', 'html', 'mysql', 'aws']",2025-06-13 06:13:39
Data Engineer-Data Platforms,IBM,5 - 10 years,Not Disclosed,['Navi Mumbai'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.\nGood working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\n\n\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'hadoop spark', 'spark', 'big data', 'python', 'hive', 'cloudera', 'pyspark', 'sql', 'java', 'git', 'postgresql', 'devops', 'jenkins', 'data ingestion', 'mysql', 'hadoop', 'etl', 'hbase', 'data analysis', 'dynamo db', 'oozie', 'microsoft azure', 'impala', 'data engineering', 'lambda expressions', 'kafka', 'sqoop', 'aws']",2025-06-13 06:13:40
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Mumbai'],"Experience with Scala object-oriented/object function Strong SQL background.\nExperience in Spark SQL, Hive, Data Engineer.\nSQL Experience with data pipelines & Data Lake Strong background in distributed comp.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nSQL Experience with data pipelines & Data Lake Strong background in distributed comp\nExperience with Scala object-oriented/object function Strong SQL background\n\n\nPreferred technical and professional experience\nCore Scala Development Experience",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'sql', 'spark', 'data lake', 'amazon redshift', 'pyspark', 'data warehousing', 'emr', 'java', 'data modeling', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'microsoft azure', 'machine learning', 'data engineering', 'sql server', 'nosql', 'amazon ec2', 'kafka', 'sqoop', 'aws']",2025-06-13 06:13:42
Data Engineer Sr. Analyst,Accenture,5 - 7 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Databricks including Spark-based ETL, Delta Lake\n\n\n\n\nGood to have skills:Pyspark\n\n\n\nJob\n\n\nSummary\n\nWe are seeking a highly skilled and experienced Senior Data Engineer to join our growing Data and Analytics team. The ideal candidate will have deep expertise in Databricks and cloud data warehousing, with a proven track record of designing and building scalable data pipelines, optimizing data architectures, and enabling robust analytics capabilities. This role involves working collaboratively with cross-functional teams to ensure the organization leverages data as a strategic asset. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesign, build, and maintain scalable data pipelines and ETL processes using Databricks and other modern tools.\nArchitect, implement, and manage cloud-based data warehousing solutions on Databricks (Lakehouse Architecture)\nDevelop and maintain optimized data lake architectures to support advanced analytics and machine learning use cases.\nCollaborate with stakeholders to gather requirements, design solutions, and ensure high-quality data delivery.\nOptimize data pipelines for performance and cost efficiency.\nImplement and enforce best practices for data governance, access control, security, and compliance in the cloud.\nMonitor and troubleshoot data pipelines to ensure reliability and accuracy.\nLead and mentor junior engineers, fostering a culture of continuous learning and innovation.\nExcellent communication skills\nAbility to work independently and along with client based out of western Europe.\n\n\n\nProfessional and Technical Skills\n3.5-5 years of experience in Data Engineering roles with a focus on cloud platforms.\nProficiency in Databricks, including Spark-based ETL, Delta Lake, and SQL.\nStrong experience with one or more cloud platforms (AWS preferred).\nHandson Experience with Delta lake, Unity Catalog, and Lakehouse architecture concepts.\nStrong programming skills in Python and SQL; experience with Pyspark a plus.\nSolid understanding of data modeling concepts and practices (e.g., star schema, dimensional modeling).\nKnowledge of CI/CD practices and version control systems (e.g., Git).\nFamiliarity with data governance and security practices, including GDPR and CCPA compliance.\n\n\n\n\nAdditional Information\nExperience with Airflow or similar workflow orchestration tools.\nExposure to machine learning workflows and MLOps.\nCertification in Databricks, AWS\nFamiliarity with data visualization tools such as Power BI\n\n(do not remove the hyperlink)Qualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data warehousing', 'sql', 'data modeling', 'python', 'data bricks', 'hive', 'kubernetes', 'catalog', 'pyspark', 'data architecture', 'docker', 'ansible', 'git', 'java', 'spark', 'devops', 'hadoop', 'etl', 'big data', 'data lake', 'airflow', 'power bi', 'cloud platforms', 'machine learning', 'data engineering', 'aws']",2025-06-13 06:13:44
Data Engineer Sr. Analyst,Accenture,2 - 3 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python/Scala, Pyspark/Pytorch\n\n\n\n\nGood to have skills:Redshift\n\n\n\nJob\n\n\nSummary\n\nYoull capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'microsoft azure', 'glue', 'amazon redshift', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'scipy', 'snowflake', 'data analysis', 'azure data lake', 'power bi', 'data engineering', 'javascript', 'pandas', 'data bricks', 'tableau', 'lambda expressions', 'aws']",2025-06-13 06:13:45
Data Engineer - Senior Analyst,Accenture,2 - 3 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python/Scala, Pyspark/Pytorch\n\n\n\n\nGood to have skills:Redshift\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)\n\n\n\nJob\n\n\nSummary\n\nYoull capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'microsoft azure', 'glue', 'amazon redshift', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'scipy', 'snowflake', 'data analysis', 'azure data lake', 'power bi', 'data engineering', 'javascript', 'pandas', 'data bricks', 'tableau', 'lambda expressions', 'aws']",2025-06-13 06:13:47
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 06:13:49
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 06:13:50
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Indore'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 06:13:52
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 06:13:54
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 06:13:56
Data Engineer,Accenture,2 - 3 years,Not Disclosed,['Kochi'],"Job Title - Data Engineer Sr.Analyst ACS SONG\n\n\n\nManagement Level:Level 10 Sr. Analyst\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python/Scala, Pyspark/Pytorch\n\n\n\n\nGood to have skills:Redshift\n\n\n\n\n\n\n\nJob\n\n\nSummary\n\nYoull capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\n\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\n\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\n\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\n\nCreating data products for analytics team members to improve productivity\n\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\n\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\n\nPreparing data to create a unified database and build tracking solutions ensuring data quality\n\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\n\n\nProfessional and Technical Skills\n\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\n\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies.\n\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\n\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\n\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\n\nExperience working in cloud Data warehouses like Redshift or Synapse\n\nCertification in any one of the following or equivalent\n\nAWS- AWS certified data Analytics- Speciality\n\nAzure- Microsoft certified Azure Data Scientist Associate\n\nSnowflake- Snowpro core- Data Engineer\n\nDatabricks Data Engineering\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\n\n\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'microsoft azure', 'glue', 'amazon redshift', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'snowflake', 'scipy', 'data analysis', 'azure data lake', 'power bi', 'data engineering', 'javascript', 'data bricks', 'pandas', 'tableau', 'lambda expressions', 'aws']",2025-06-13 06:13:57
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 06:13:59
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 06:14:01
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 06:14:03
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 06:14:05
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data platform components using Databricks Unified Data Analytics Platform.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data platform components and architecture.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data pipeline development and maintenance.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.-This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'data security', 'microsoft azure', 'data bricks', 'aws', 'hive', 'amazon redshift', 'pyspark', 'data warehousing', 'emr', 'sql', 'java', 'data modeling', 'spark', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'machine learning', 'sql server', 'nosql', 'pipeline', 'amazon ec2', 'kafka', 'sqoop']",2025-06-13 06:14:06
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the organization.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the organization.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Hyderabad office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 06:14:08
Data Platform Engineer,Accenture,2 - 7 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 2 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analysis', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 06:14:10
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analysis', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 06:14:12
Senior PySpark Data Engineer,Synechron,7 - 12 years,Not Disclosed,"['Pune', 'Hinjewadi']","Job Summary\nSynechron is seeking an experienced and technically proficient Senior PySpark Data Engineer to join our data engineering team. In this role, you will be responsible for developing, optimizing, and maintaining large-scale data processing solutions using PySpark. Your expertise will support our organizations efforts to leverage big data for actionable insights, enabling data-driven decision-making and strategic initiatives.\nSoftware Requirements\nRequired Skills:\nProficiency in PySpark\nFamiliarity with Hadoop ecosystem components (e.g., HDFS, Hive, Spark SQL)\nExperience with Linux/Unix operating systems\nData processing tools like Apache Kafka or similar streaming platforms\nPreferred Skills:\nExperience with cloud-based big data platforms (e.g., AWS EMR, Azure HDInsight)\nKnowledge of Python (beyond PySpark), Java or Scala relevant to big data applications\nFamiliarity with data orchestration tools (e.g., Apache Airflow, Luigi)\nOverall Responsibilities\nDesign, develop, and optimize scalable data processing pipelines using PySpark.\nCollaborate with data engineers, data scientists, and business analysts to understand data requirements and deliver solutions.\nImplement data transformations, aggregations, and extraction processes to support analytics and reporting.\nManage large datasets in distributed storage systems, ensuring data integrity, security, and performance.\nTroubleshoot and resolve performance issues within big data workflows.\nDocument data processes, architectures, and best practices to promote consistency and knowledge sharing.\nSupport data migration and integration efforts across varied platforms.\nStrategic Objectives:\nEnable efficient and reliable data processing to meet organizational analytics and reporting needs.\nMaintain high standards of data security, compliance, and operational durability.\nDrive continuous improvement in data workflows and infrastructure.\nPerformance Outcomes & Expectations:\nEfficient processing of large-scale data workloads with minimum downtime.\nClear, maintainable, and well-documented code.\nActive participation in team reviews, knowledge transfer, and innovation initiatives.\nTechnical Skills (By Category)\nProgramming Languages:\nRequired: PySpark (essential); Python (needed for scripting and automation)\nPreferred: Java, Scala\nDatabases/Data Management:\nRequired: Experience with distributed data storage (HDFS, S3, or similar) and data warehousing solutions (Hive, Snowflake)\nPreferred: Experience with NoSQL databases (Cassandra, HBase)\nCloud Technologies:\nRequired: Familiarity with deploying and managing big data solutions on cloud platforms such as AWS (EMR), Azure, or GCP\nPreferred: Cloud certifications\nFrameworks and Libraries:\nRequired: Spark SQL, Spark MLlib (basic familiarity)\nPreferred: Integration with streaming platforms (e.g., Kafka), data validation tools\nDevelopment Tools and Methodologies:\nRequired: Version control systems (e.g., Git), Agile/Scrum methodologies\nPreferred: CI/CD pipelines, containerization (Docker, Kubernetes)\nSecurity Protocols:\nOptional: Basic understanding of data security practices and compliance standards relevant to big data management\nExperience Requirements\nMinimum of 7+ years of experience in big data environments with hands-on PySpark development.\nProven ability to design and implement large-scale data pipelines.\nExperience working with cloud and on-premises big data architectures.\nPreference for candidates with domain-specific experience in finance, banking, or related sectors.\nCandidates with substantial related experience and strong technical skills in big data, even from different domains, are encouraged to apply.\nDay-to-Day Activities\nDevelop, test, and deploy PySpark data processing jobs to meet project specifications.\nCollaborate in multi-disciplinary teams during sprint planning, stand-ups, and code reviews.\nOptimize existing data pipelines for performance and scalability.\nMonitor data workflows, troubleshoot issues, and implement fixes.\nEngage with stakeholders to gather new data requirements, ensuring solutions are aligned with business needs.\nContribute to documentation, standards, and best practices for data engineering processes.\nSupport the onboarding of new data sources, including integration and validation.\nDecision-Making Authority & Responsibilities:\nIdentify performance bottlenecks and propose effective solutions.\nDecide on appropriate data processing approaches based on project requirements.\nEscalate issues that impact project timelines or data integrity.\nQualifications\nBachelors degree in Computer Science, Information Technology, or related field. Equivalent experience considered.\nRelevant certifications are preferred: Cloudera, Databricks, AWS Certified Data Analytics, or similar.\nCommitment to ongoing professional development in data engineering and big data technologies.\nDemonstrated ability to adapt to evolving data tools and frameworks.\nProfessional Competencies\nStrong analytical and problem-solving skills, with the ability to model complex data workflows.\nExcellent communication skills to articulate technical solutions to non-technical stakeholders.\nEffective teamwork and collaboration in a multidisciplinary environment.\nAdaptability to new technologies and emerging trends in big data.\nAbility to prioritize tasks effectively and manage time in fast-paced projects.\nInnovation mindset, actively seeking ways to improve data infrastructure and processes.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PySpark', 'S3', 'Unix operating systems', 'Spark SQL', 'Luigi', 'HDFS', 'AWS EMR', 'Apache Airflow', 'Hive', 'Linux', 'Azure HDInsight', 'Apache Kafka', 'AWS']",2025-06-13 06:14:14
Senior - AWS Data Engineering,KPMG India,4 - 8 years,Not Disclosed,['Gurugram'],"KPMG India is looking for Senior - AWS Data Engineering to join our dynamic team and embark on a rewarding career journey Designs and builds scalable data pipelines using AWS servicesOptimizes data ingestion, storage, and processingCollaborates with data scientists and analystsEnsures performance, security, and compliance",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Networking', 'Focus', 'Manager Technology', 'professional services', 'AWS', 'international clients']",2025-06-13 06:14:16
Azure Cloud Data Engineering Consultant,Optum,7 - 10 years,17-27.5 Lacs P.A.,['Gurugram'],"Primary Responsibilities:\nDesign and develop applications and services running on Azure, with a strong emphasis on Azure Databricks, ensuring optimal performance, scalability, and security.\nBuild and maintain data pipelines using Azure Databricks and other Azure data integration tools.\nWrite, read, and debug Spark, Scala, and Python code to process and analyze large datasets.\nWrite extensive query in SQL and Snowflake\nImplement security and access control measures and regularly audit Azure platform and infrastructure to ensure compliance.\nCreate, understand, and validate design and estimated effort for given module/task, and be able to justify it.\nPossess solid troubleshooting skills and perform troubleshooting of issues in different technologies and environments.\nImplement and adhere to best engineering practices like design, unit testing, functional testing automation, continuous integration, and delivery.\nMaintain code quality by writing clean, maintainable, and testable code.\nMonitor performance and optimize resources to ensure cost-effectiveness and high availability.\nDefine and document best practices and strategies regarding application deployment and infrastructure maintenance.\nProvide technical support and consultation for infrastructure questions.\nHelp develop, manage, and monitor continuous integration and delivery systems.\nTake accountability and ownership of features and teamwork.\nComply with the terms and conditions of the employment contract, company policies and procedures, and any directives.\nRequired Qualifications:\nB.Tech/MCA (Minimum 16 years of formal education)\nOverall 7+ years of experience.\nMinimum of 3 years of experience in Azure (ADF), Databricks and DevOps.\n5 years of experience in writing advanced level SQL.\n2-3 years of experience in writing, reading, and debugging Spark, Scala, and Python code.\n3 or more years of experience in architecting, designing, developing, and implementing cloud solutions on Azure.\nProficiency in programming languages and scripting tools.\nUnderstanding of cloud data storage and database technologies such as SQL and NoSQL.\nProven ability to collaborate with multidisciplinary teams of business analysts, developers, data scientists, and subject-matter experts.\nFamiliarity with DevOps practices and tools, such as continuous integration and continuous deployment (CI/CD) and Teraform.\nProven proactive approach to spotting problems, areas for improvement, and performance bottlenecks.\nProven excellent communication, writing, and presentation skills.\nExperience in interacting with international customers to gather requirements and convert them into solutions using relevant skills.\nPreferred Qualifications:\nKnowledge of AI/ML or LLM (GenAI).\nKnowledge of US Healthcare domain and experience with healthcare data.\nExperience and skills with Snowflake.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'ETL', 'SQL', 'Python', 'Airflow', 'Pyspark', 'Snowflake', 'SCALA', 'Spark', 'Data Bricks']",2025-06-13 06:14:17
"Applied Scientist, Alexa Sensitive Content Intelligence (ASCI)",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Alexa is the voice activated digital assistant powering devices like Amazon Echo, Echo Dot, Echo Show, and Fire TV, which are at the forefront of this latest technology wave. To preserve our customers experience and trust, the Alexa Privacy team creates policies and builds services and tools through Machine Learning techniques to detect and mitigate sensitive content across Alexa. We are looking for an experienced Senior Applied Scientist to build industry-leading technologies in attribute extraction and sensitive content detection across all languages and countries.\n\nAn Applied Scientist will be in a team of exceptional scientists to develop novel algorithms and modeling techniques to advance the state of the art in Natural Language Processing (NLP) or Computer Vision (CV) related tasks. They will work in a hybrid, fast-paced organization where scientists, engineers, and product managers work together to build customer facing experiences. They will collaborate with and mentor other scientists to raise the bar of scientific research in Amazon. Their work will directly impact our customers in the form of products and services that make use of speech, language, and computer vision technologies.\n\nWe are looking for candidate with strong technical experiences and a passion for building scientific driven solutions in a fast-paced environment. This Senior Applied Scientist should have good understanding of NLP models (e.g. LSTM, transformer based models) or CV models (e.g. CNN, AlexNet, ResNet) and where to apply them in different business cases. They should leverage exceptional technical expertise, a sound understanding of the fundamentals of Computer Science, and practical experience of building large-scale distributed systems to creating reliable, scalable, and high-performance products. In addition to technical depth, they must possess exceptional communication skills and understand how to influence key stakeholders.\n\nThis Applied Scientist will be joining a select group of people making history producing one of the most highly rated products in Amazons history, so if you are looking for a challenging and innovative role where you can solve important problems while growing as a leader, this may be the place for you.\n\n\nThis Applied Scientist will lead the science solution design, run experiments, research new algorithms, and find new ways of optimizing customer experience. They will set examples for the team on good science practice and standards. Besides theoretical analysis and innovation, they will work closely with talented engineers and ML scientists to put algorithms and models into practice.\n\nThis Applied Scientists work will also directly impact the trust customers place in Alexa, globally. They will contribute directly to our growth by hiring smart and motivated scientists to establish teams that can deliver swiftly and predictably, adjusting in an agile fashion to deliver what our customers need.\n\nA day in the life\nYou will be working with a group of talented scientists on researching algorithm and running experiments to test scientific proposal/solutions to improve our sensitive contents detection and mitigation. This will involve collaboration with partner teams including engineering, PMs, data annotators, and other scientists to discuss data quality, policy, and model development. You will mentor other scientists, review and guide their work, help develop roadmaps for the team. You work closely with partner teams across Alexa to deliver platform features that require cross-team leadership.\n\nAbout the team\nThe mission of the Alexa Sensitive Content Intelligence (ASCI) team is to (1) minimize negative surprises to customers caused by sensitive content, (2) detect and prevent potential brand-damaging interactions, and (3) build customer trust through appropriate interactions on sensitive topics.\nThe term sensitive content includes within its scope a wide range of categories of content such as offensive content (e.g., hate speech, racist speech), profanity, content that is suitable only for certain age groups, politically polarizing content, and religiously polarizing content. The term content refers to any material that is exposed to customers by Alexa (including both 1P and 3P experiences) and includes text, speech, audio, and video. 3+ years of building models for business application experience\nPhD, or Masters degree and 4+ years of CS, CE, ML or related field experience\nExperience in patents or publications at top-tier peer-reviewed conferences or journals\nExperience programming in Java, C++, Python or related language\nExperience in any of the following areas: algorithms and data structures, parsing, numerical optimization, data mining, parallel and distributed computing, high-performance computing Experience using Unix/Linux\nExperience in professional software development",,,,"['Unix', 'Computer science', 'Computer vision', 'C++', 'Linux', 'Machine learning', 'Agile', 'Data structures', 'Data mining', 'Python']",2025-06-13 06:14:19
Golang-Lead Software Engineer/ Senior Software/Software Engineer,Tech Mahindra,4 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Demonstrable ability to write Python/Golang and SQL. You are happy to learn new programming languages and frameworks as necessary.\nYou are interested in, contemporary approaches to service design, including the use of containers and container orchestration technologies, streaming data platforms, APIs and in-memory/NoSQL stores.\nYou are familiar with working in a devops based software development workflow, including building, testing, and continuous integration/deployment. You are also happy to be evolve along with the development process and contribute to its success.\nYou have the ability to communicate with a range of stakeholders, including subject matter experts, data scientists, software engineers and enterprise devops and security professionals.\nYou are keen to engage with best practices for code review, version control, and change control, balancing the need for a quality codebase with the unique and particular demands of scale up stage software engineering.\nYou have experience or are keen to engage with productionising machine learning technologies.Role & responsibilities",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Golang', 'devops', 'API', 'SQL']",2025-06-13 06:14:21
Lead Software Engineering - Python Developer,JPMorgan Chase Bank,1 - 9 years,Not Disclosed,['Bengaluru'],"Are you ready to elevate your career in software engineeringJoin our dynamic team as a Python Developer, where your expertise will drive cutting-edge solutions and contribute to impactful projects. We offer unparalleled opportunities for career growth and a collaborative environment where you can thrive and make a significant impact.\nAs a Lead Software Engineer at JPMorgan Chase within the Technology and Engineering division, you will execute software solutions, design, and development, collaborating with cross-functional teams to deploy machine learning services. You will be responsible for producing architecture and design artifacts for complex applications, ensuring design constraints are met. Your role will involve contributing to the engineering community and influencing the use of leading-edge technologies.\nJob Responsibilities\nExecute software solutions, design, development, and technical troubleshooting, thinking beyond routine approaches.\nCreate secure and high-quality production code, maintaining algorithms that run synchronously with systems.\nProduce architecture and design artifacts for complex applications, ensuring design constraints are met.\nCollaborate with cross-functional teams, including Data Science partners, to design and deploy machine learning services.\nContribute to the engineering community as an advocate of firmwide frameworks, tools, and practices.\nInfluence peers and project decision-makers to consider leading-edge technologies.\nAdd to the team culture of diversity, equity, inclusion, and respect.\nRequired Qualifications, Capabilities, and Skills\nFormal training or certification on Software Engineering concepts and 5+ years applied experience.\nExperience in building complex software systems in both private and public cloud environments (AWS).\nHands-on practical experience delivering system design, application development, testing, and operational stability.\nAdvanced Python Programming Skills including Pandas, Numpy.\nProficiency with AIM algorithms.\nAdvanced knowledge of software applications and technical processes in technical disciplines (e. g. , cloud, AI, ML).\nAbility to tackle design and functionality problems independently with little oversight.\nPreferred Qualifications, Capabilities, and Skills\nAdvanced skills in additional programming languages (Java).\nFamiliarity with building services and consuming data via GraphQL, REST, or gRPC and SQL\nExperience in building and deploying machine learning models, with knowledge of the ML Lifecycle; expertise in MLOps and AIOps is an advantage.\nWorking knowledge of security best practices and compliance standards for machine learning systems.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Lead Software', 'Machine learning', 'System design', 'Deployment', 'Application development', 'Troubleshooting', 'Operations', 'Software solutions', 'SQL', 'Python']",2025-06-13 06:14:22
Data Science & AI Engineer,Blue Altair,5 - 8 years,Not Disclosed,['Pune'],"Greetings from Blue Altair!\nJob Overview:\nWe are seeking an experienced and highly skilled Data Science and AI Engineer to join our dynamic team. The ideal candidate will have 5+ years of experience working on cutting-edge data science and AI technologies across various cloud platforms with a strong focus to work on LLMs and SLMs. The role demands a professional capable of performing in a client-facing environment, as well as mentoring and guiding junior team members.\n\nTitle: Consultant/Sr. Consultant - Data Science Engineer\nExperience: 5-8 years\nLocation: Pune/Bangalore (Hybrid)\n\nRoles and responsibilities:\nDevelop, implement, and optimize machine learning models and AI algorithms to solve complex business problems.\nDesign, build, and fine-tune AI models, particularly focusing on LLMs and SLMs, using state-of-the-art techniques and architectures.\nApply advanced techniques in prompt engineering, model fine-tuning, and optimization to tailor models for specific business needs.\nDeploy and manage machine learning models and pipelines on cloud platforms (AWS, GCP, Azure, etc.).\nWork closely with clients to understand their data and AI needs and provide tailored solutions.\nCollaborate with cross-functional teams to integrate AI solutions into broader software architectures.\nMentor junior team members and provide guidance in implementing best practices in data science and AI development.\nStay up-to-date with the latest trends and advancements in data science, AI, and cloud technologies.\nPrepare technical documentation and present insights to both technical and non-technical stakeholders.\n\nRequirement:\n5+ years of experience in data science, machine learning, and AI technologies.\nProven experience working with cloud platforms such as Google Cloud, Microsoft Azure, or AWS.\nExpertise in programming languages such as Python, R, Julia, and AI frameworks like TensorFlow, PyTorch, Scikit-learn, Hugging face Transformers.\nKnowledge of data visualization tools (e.g., Matplotlib, Seaborn, Tableau)\nSolid understanding of data engineering concepts including ETL, data pipelines, and databases (SQL, NoSQL).\nExperience with MLOps practices and deployment of models in production environments.\nFamiliarity with NLP (Natural Language Processing) tasks and working with large-scale datasets.\nHands-on experience with generative AI models like GPT, Gemini, Claude, Mistral etc.\nClient-facing experience with strong communication skills to manage and engage stakeholders.\nStrong problem-solving skills and analytical mindset.\nAbility to work independently and as part of a team and mentor and provide technical leadership to junior team members.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLMs', 'Artificial Intelligence', 'MLOps', 'RAG', 'Natural Language Processing', 'Neural Networks', 'LLM', 'Machine Learning', 'AI Models', 'Data Science', 'PyTorch', 'SLM', 'AI Automation']",2025-06-13 06:14:24
Software Development Engineer 1,Meesho,0 - 1 years,20-25 Lacs P.A.,['Bengaluru'],"Join us for an exciting SDE Traineeship at Meesho.\nBased on the performance at Meesho assessment, successful candidates will be considered for a full-time opportunity (FTE).\n\nAPPLY HERE: https://p.hck.re/6TcJ\n\nAbout the role:\nAs an SDE Trainee , we expect you to be motivated in solving real-life complex problems and creating compelling experiences for our resellers. Being a small company we have a culture of creative problem- solving, intellectual design, fast-paced development, and passionate product delivery. The pace of our growth is incredible. If you want to tackle hard, interesting and UNIQUE problems, and create an impact within an entrepreneurial environment, JOIN US!\nKey Responsibilities:\nCollaborate with teams to develop new features for Meesho customers and suppliers\nLeverage state-of-the-art technologies and write highly performant code\nTake end-to-end ownership of features, from ideation to production\n\nTechnical Requirements:\n0-1 years of experience\nStrong problem-solving skills\nExcellent understanding of data structures and algorithms, and their space & time complexities\nStrong hands-on and practical working experience with at least one programming language: Java/Python/Javascript\nExcellent coding skills should be able to convert design into code fluently.",Industry Type: Internet (E-Commerce),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Generative Ai', 'Gen AI', 'java', 'Artificial Intelligence', 'Data Structures', 'Javascript', 'Python']",2025-06-13 06:14:26
Lead Data Scientist (AI/ML) - Immediate hiring - Chennai/Pune,Optimum Solutions,10 - 20 years,Not Disclosed,"['Pune', 'Chennai']","Experience:\nMinimum 10+ years in AI/ML or data science, with at least 5 years in a leadership role.\nProven experience in banking or financial services is highly preferred.\nHands-on with AI/ML frameworks (e.g., TensorFlow, PyTorch, Scikit-learn) and tools (Python, SQL, Spark).\nExperience in Azure ML tools: Databricks etc.\nExperience with end-to-end model lifecycle management and MLOps.\n\nMinimum Qualification: Masters Degree/PhD in Computer Science, Econometrics, Statistics, or related fields.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Machine Learning', 'Data Scientist', 'Tensorflow', 'Pyspark', 'Azure Databricks', 'Deep Learning', 'Scikit-Learn', 'Data Science', 'Pytorch', 'Azure Data Lake', 'Azure Machine Learning', 'Python']",2025-06-13 06:14:28
Lead Data Scientist,Tezo,8 - 12 years,Not Disclosed,['Hyderabad'],"ql-editor "">\nTezo is a new generation Digital & AI solutions provider, with a history of creating remarkable outcomes for our customers. We bring exceptional experiences using cutting-edge analytics, data proficiency, technology, and digital excellence.\n\nTezo is seeking passionate AI Engineers who are excited about harnessing the power of Generative AI to transform our company and provide cutting-edge solutions for our clients. Join us in revolutionizing enterprises by building intelligent, generative solutions that leverage AI/ML. If youve ever dreamed of contributing to impactful projects on a large scale, this is the opportunity for you!\nIn this role, you will be an integral part of the Machine Learning Platforms/Data Science team, focusing on developing, testing, and deploying generative AI models.\n\nWhat Makes Our AI/ML Practice Unique:\nPurpose-driven: We actively respond to our customers evolving needs with innovative solutions.\nCollaborative: We foster a positive and engaging work environment where collective ideas thrive.\nAccountable: We take ownership of our performance, both individually and as a team.\nService Excellence: We maximize our potential through continuous learning and improvement.\nTrusted: We empower individuals to make informed decisions and take calculated risks.\n\nJob Summary:\nWe are looking for a dedicated Lead Data Scientist with a strong background in Generative AI to join our team. You will support product, leadership, and client teams by providing insights derived from advanced data analysis and generative modeling.\nIn this role, you will collaborate closely with the development team, architects, and product owners to build efficient generative models and manage their lifecycle using the appropriate technology stack.\nCore Requirements:\n\nAt least 6 years of experience working with geographically distributed teams\n2+ years of experience working in a client-facing role on AI/ML .\nDemonstrable experience in leading a substantive area of work, or line management of a team.\nProven experience in building production grade Retrieval-Augmented Generation (RAG) solutions with hands on experience with advanced RAG techniques for retrieval, re-ranking etc.\nBuild GenAI applications using LangChain, LlamaIndex and familiarity with Vector Stores and Large Language Models.\nExperience in fine-tuning Large Language Models (LLMs) for business use cases will be preferred.\nMinimum of 4 years of experience in developing end-to-end classical machine learning and NLP projects.\nDemonstrated experience in deploying ML solutions in production using cloud services like Azure,AWS.\nBusiness Understanding, Stakeholder management and Team leading skills.\nStrong practical expertise in Python and SQL needed for data science projects.\nJoin us at Tezo to be part of a dynamic team committed to driving innovation through Generative AI solutions!",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Team leading', 'data science', 'Cloud Services', 'Machine learning', 'Stakeholder management', 'Business understanding', 'Analytics', 'SQL', 'Python']",2025-06-13 06:14:30
Software Principal Engineer,Dell Technologies,8 - 12 years,Not Disclosed,['Bengaluru'],"Software Principal Engineer\nThe Software Engineering team delivers next-generation application enhancements and new products for a changing world. Working at the cutting edge, we design and develop software for platforms, peripherals, applications and diagnostics all with the most advanced technologies, tools, software engineering methodologies and the collaboration of internal and external partners.\nJoin us to do the best work of your career and make a profound social impact as a Software Principal Engineer on our ISG-PowerSizer team in Bangalore.\n\nWhat you ll achieve\nAs a Software Principal Engineer, you will be responsible for developing sophisticated systems and software based on the customer s business goals, needs and general business environment creating software solutions.\nYou will:\nLeads the design and architecture of high quality, complex systems and software/storage\nPrepares, reviews and analyses software specifications for complex products and systems\nLeads the review and analysis of design, functional, technical and user documentation\nLeads the development, review, analysis and implementation of test strategies for software/storage products and systems\nLeads the development, test and integration of code for new or existing software of significant complexity involving multiple teams\n\nTake the first step towards your dream career\nEvery Dell Technologies team member brings something unique to the table. Here s what we are looking for with this role:\nEssential Requirements\n8-12 years of industry experience, extensive working experience of programming in Python Automation, Java, and Selenium.\nKubernetes, Microservices, Docker, containers, Cloud architecture experience.\nOpen-source components - Postgres, Keycloak, RabbitMQ, TimeScale etc, is preferred.\nExperience working on Systems Management Software, Layered application development, third-party libraries, and development debug tools.\nDesirable Requirements\nEducational Qualification (BE/BTech/ME/MTech/MS/MCA)\n\n\n\nApplication closing date: 30 June 2025",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Architecture', 'Application development', 'Selenium', 'Management', 'Open source', 'Software solutions', 'Principal', 'Python', 'Testing']",2025-06-13 06:14:31
"Principal Engineer I, Scrum Master & PO",HARMAN,10 - 11 years,Not Disclosed,['Bengaluru'],"Introduction: Automotive\n.\nEngineer audio systems and integrated technology platforms that augment the driving experience.\nCombine ingenuity, in-depth research, and a spirit of collaboration with design and engineering excellence.\nAdvance in-vehicle infotainment, safety, efficiency, and enjoyment.",,,,"['System architecture', 'C++', 'Software design', 'Linux', 'Coding', 'Agile', 'Telematics', 'Scrum', 'Troubleshooting', 'Automotive']",2025-06-13 06:14:33
Lead - Data Scientist - Mumbai,One of the Leading Electronics Manufactu...,10 - 15 years,37.5-45 Lacs P.A.,"['Mumbai', 'Mumbai Suburban', 'Mumbai (All Areas)']","Role & responsibilities\nDevelop and implement a comprehensive analytics strategy to support the organization's business objectives.\nLead cross-functional projects using advanced data modeling and analysis techniques to discover insights that will guide strategic decisions and uncover optimization opportunities.\nBuild, develop and maintain data models, reporting systems, data automation systems, dashboards and performance metrics support that support key business decisions on Microsoft Power BI and Snowflake.\nDriving key business impacting processes like demand forecast generation and SNOP processes.\nGenerating sales recommendation for trade sales team.\nOptimizing business process like network, inventory, etc., with data modeling and predicting / prescribing algorithms.\nOversee the design and delivery of reports and insights that analyze business functions and key operations and performance metrics.\nManage and optimize processes for data intake, validation, mining and engineering as well as modeling, visualization and communication deliverables.\nAnticipate future demands of initiatives related to people, technology, budget and business within your department and design/implement solutions to meet these needs.\nApply advanced analytics techniques to improve commercial routines and processes.\nOrganize and drive successful completion of data insight initiatives through effective management of analyst and data employees and effective collaboration with stakeholders.\nCommunicate results and business impacts of insight initiatives to stakeholders within and outside of the company.\nAbility to look, analyse, critically think, and communicate insights to support data-driven decisions.\n\n\nPreferred candidate profile\nWith 10+ years of experience in a position monitoring, managing, manipulating and drawing insights from data, and someone with at least 3 years of experience leading a team\nExperience with data visualization tools: Power BI, Tableau, Raw, chart.js, etc.\nExperience in understanding and managing the data flow across multiple systems to datawarehouse.\nWorking knowledge of data mining principles: predictive analytics, mapping, collecting data from multiple data systems on premises and cloud-based data sources.\nUnderstanding of and experience using analytical concepts and statistical techniques: hypothesis development, designing tests/experiments, analyzing data, drawing conclusions, and developing actionable recommendations for business units.",Industry Type: Electronics Manufacturing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['predictive modeling', 'Statistical Modeling', 'data scientist', 'Clustering', 'Predictive Analytics', 'Logistic Regression', 'Decision Tree', 'Time Series Analysis', 'Linear Regression', 'Advanced Analytics', 'Data Modeling']",2025-06-13 06:14:35
Principal Engineer,Wells Fargo,7 - 12 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Principal Engineer\n\nIn this role, you will:\nAct as an advisor to leadership to develop or influence applications, network, information security, database, operating systems, or web technologies for highly complex business and technical needs across multiple groups",,,,"['Underwriting', 'VSAM', 'JCL', 'Git', 'DB2', 'REXX', 'COBOL', 'SonarQube', 'debugging', 'IMS']",2025-06-13 06:14:36
Senior Machine Learning Lead - LLM,Avalara India,8 - 13 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","We are looking for experienced Machine Learning Engineer with a background in software development and a deep enthusiasm for solving complex problems. You will lead a dynamic team dedicated to designing and implementing a large language model framework to power diverse applications across Avalara. Your responsibilities as a Senior Technical Lead will span the entire development lifecycle, including conceptualization, prototyping and delivery of the LLM platform features. You will be reporting to Senior Manager, Software Engineering\n\nWhat Your Responsibilities Will Be\n\n\n\n\nHow we'll Take Care of You\n\n\n\n\nWhat You Need To Know About Avalara",,,,"['Computer science', 'Cloud computing', 'GCP', 'Analytical', 'Machine learning', 'Debugging', 'Conceptualization', 'Data structures', 'Distribution system', 'Python']",2025-06-13 06:14:38
Senior Data Scientist,Cradlepoint,3 - 8 years,Not Disclosed,['Bengaluru'],"Join our Team\nAbout this Opportunity\nThe complexity of running and optimizing the next generation of wireless networks, such as 5G with distributed edge compute, will require Machine Learning (ML) and Artificial Intelligence (AI) technologies. Ericsson is setting up an AI Accelerator Hub in India to fast-track our strategy execution, using Machine Intelligence (MI) to drive thought leadership, automate, and transform Ericsson s offerings and operations. We collaborate with academia and industry to develop state-of-the-art solutions that simplify and automate processes, creating new value through data insights.\nWhat you will do\nAs a Senior Data Scientist, you will apply your knowledge of data science and ML tools backed with strong programming skills to solve real-world problems.\nResponsibilities:\n1. Lead AI/ML features/capabilities in product/business areas\n2. Define business metrics of success for AI/ML projects and translate them into model metrics\n3. Lead end-to-end development and deployment of Generative AI solutions for enterprise use cases\n4. Design and implement architectures for vector search, embedding models, and RAG systems\n5. Fine-tune and evaluate large language models (LLMs) for domain-specific tasks\n6. Collaborate with stakeholders to translate vague problems into concrete Generative AI use cases\n7. Develop and deploy generative AI solutions using AWS services such as SageMaker, Bedrock, and other AWS AI tools. Provide technical expertise and guidance on implementing GenAI models and best practices within the AWS ecosystem.\n8. Develop secure, scalable, and production-grade AI pipelines\n9. Ensure ethical and responsible AI practices\n10. Mentor junior team members in GenAI frameworks and best practices\n11. Stay current with research and industry trends in Generative AI and apply cutting-edge techniques\n12. Contribute to internal AI governance, tooling frameworks, and reusable components\n13. Work with large datasets including petabytes of 4G/5G networks and IoT data\n14. Propose/select/test predictive models and other ML systems\n15. Define visualization and dashboarding requirements with business stakeholders\n16. Build proof-of-concepts for business opportunities using AI/ML\n17. Lead functional and technical analysis to define AI/ML-driven business opportunities\n18. Work with multiple data sources and apply the right feature engineering to AI models\n19. Lead studies and creative usage of new/existing data sources\nWhat you will bring\n1. Bachelors/Masters/Ph.D. in Computer Science, Data Science, AI, ML, Electrical Engineering, or related disciplines from reputed institutes\n2. 3+ years of applied ML/AI production-level experience\n3. Strong programming skills (R/Python)\n4. Proven ability to lead AI/ML projects end-to-end\n5. Strong grounding in mathematics, probability, and statistics\n6. Hands-on experience with data analysis, visualization techniques, and ML frameworks (Python, R, H2O, Keras, TensorFlow, Spark ML)\n7. Experience with semi-structured/unstructured data for AI/ML models\n8. Strong understanding of building AI models using Deep Neural Networks\n9. Experience with Big Data technologies (Hadoop, Cassandra)\n10. Ability to source and combine data from multiple sources for ML models\nPreferred Qualifications:\n1. Good communication skills in English\n2. Certifying MI MOOCs, a plus\n3. Domain knowledge in Telecommunication/IoT, a plus\n4. Experience with data visualization and dashboard creation, a plus\n5. Knowledge of Cognitive models, a plus\n6. Experience in partnering and collaborative co-creation in a global matrix organization.\nWhy join Ericsson\n\n\nWhat happens once you apply\nPrimary country and city: India (IN) || Bangalore\nReq ID: 766481",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Wireless', 'Computer science', 'Data analysis', 'cassandra', 'Neural networks', 'Artificial Intelligence', 'Machine learning', 'Telecommunication', 'data visualization', 'Python']",2025-06-13 06:14:39
Data Scientist Specialist (GenAI),Rarr Technologies,7 - 12 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Role & responsibilities:\nOutline the day-to-day responsibilities for this role.\n\nPreferred candidate profile:\nSpecify required role expertise, previous job experience, or relevant certifications.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Langchain', 'Artificial Intelligence', 'Natural Language Processing', 'Python', 'RAG', 'Machine Learning', 'Deep Learning']",2025-06-13 06:14:41
"Data Scientist,VP",NatWest Markets,10 - 12 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Join us as a Data Scientist\nIn this role, you ll drive and embed the design and implementation of data science tools and methods, which harness our data to drive market-leading purpose customer solutions\nDay-to-day, you ll act as a subject matter expert and articulate advanced data and analytics opportunities, bringing them to life through data visualisation\nIf you re ready for a new challenge, and are interested in identifying opportunities to support external customers by using your data science expertise, this could be the role for you\nWere offering this role at vice president level\nWhat you ll do\nWe re looking for someone to understand the requirements and needs of our business stakeholders. You ll develop good relationships with them, form hypotheses, and identify suitable data and analytics solutions to meet their needs and to achieve our business strategy.\nYou ll be maintaining and developing external curiosity around new and emerging trends within data science, keeping up to date with emerging trends and tooling and sharing updates within and outside of the team.\nYou ll also be responsible for:\nProactively bringing together statistical, mathematical, machine-learning and software engineering skills to consider multiple solutions, techniques, and algorithms\nImplementing ethically sound models end-to-end and applying software engineering and a product development lens to complex business problems\nWorking with and leading both direct reports and wider teams in an Agile way within multi-disciplinary data to achieve agreed project and Scrum outcomes\nUsing your data translation skills to work closely with business stakeholders to define business questions, problems or opportunities that can be supported through advanced analytics\nSelecting, building, training, and testing complex machine models, considering model valuation, model risk, governance, and ethics throughout to implement and scale models\nThe skills you ll need\nTo be successful in this role, you ll need evidence of project implementation and work experience gained in a data-analysis-related field as part of a multi-disciplinary team. We ll also expect you to hold an undergraduate or a master s degree in Data science, Statistics, Computer science, or related field .\nYou ll also need an experience of 10 years with statistical software, database languages, big data technologies, cloud environments and machine learning on large data sets. And we ll look to you to bring the ability to demonstrate leadership, self-direction and a willingness to both teach others and learn new techniques.\nAdditionally, you ll need:\nExperience of deploying machine learning models into a production environment\nProficiency in Python and relevant libraries such as Pandas, NumPy, Scikit-learn coupled with experience in data visualisation tools.\nExtensive work experience with AWS Sage maker , including expertise in statistical data analysis, machine learning models, LLMs, and data management principles\nEffective verbal and written communication skills , the ability to adapt communication style to a specific audience and mentoring junior team members",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'data science', 'Data management', 'Machine learning', 'Agile', 'Scrum', 'SAGE', 'Business strategy', 'Python']",2025-06-13 06:14:43
Senior Data Scientist,eka.care,3 - 5 years,Not Disclosed,['Bengaluru'],"EkaCare (Orbi Health) is a well-funded startup working on a suite of technologies in the healthcare domain ranging from AI-powered EMR for doctors to one of the most comprehensive personal health record (PHR) applications for consumers. EkaCare seeks enthusiastic senior candidates to develop Large Language Models around medical/clinical data.\n\nWe look forward to a candidate with\nPassion for problem-solving and taking end-to-end ownership of projects\nExtensive knowledge and prior work experience in machine learning (specifically in developing LLMs)\nDesire for a high-paced start-up ride\n\nKey Responsibilities :\nFormulate and implement data-driven solutions in the HealthTech domain:\nBuilding LLMs around healthcare data, wherein the work would involve creating datasets, continual pre-training, supervised fine-tuning, and preference alignment of models.\nDeveloping product-led AI solutions for various healthcare entities.\n\nQualifications / Requirements\nMaster / PhD degree in a relevant academic discipline (Preferred)\n3-5 years of industry experience in building ML production-level pipelines.\nExtensive experience with LLMs (production-level deployment and fine-tuning)\nStrong track record of project delivery\n\nExperience Required: 3-5 years\n\nFull Time Employee Benefits:\nInsurance Benefits - Medical Insurance, Accidental Insurance\nParental Support - Maternity Benefit, Paternity Benefit Program\nMobility Benefits - Relocation benefits, Transfer Support Policy, Travel Policy\nRetirement Benefits - Employee PF Contribution, Flexible PF Contribution, Gratuity, NPS, Leave Encashment\nOther Benefits - Car Lease, Salary Advance Policy",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Training', 'Machine learning', 'Leasing', 'Healthcare', 'Deployment', 'Medical insurance', 'Project delivery', 'Supervision', 'clinical data']",2025-06-13 06:14:45
Senior Data Scientist,Hindustan Unilever (HUL),2 - 5 years,Not Disclosed,['Bengaluru'],"Job Title: Senior Data Scientist\nLocation: Bangalore\nJob Title: Assistant Manager - Security Engineering\nLocation: UniOps Bangalore\nABOUT UNILEVER:\nEvery individual here can bring their purpose to life through their work. Join us and you ll be surrounded by inspiring leaders and supportive peers. Among them, you ll channel your purpose, bring fresh ideas to the table, and simply be you. As you work to make a real impact on the business and the world, we ll work to help you become a better you.\nABOUT UNIOPS:\nUnilever Operations (UniOps) is the global technology and operations engine of Unilever offering business services, technology, and enterprise solutions. UniOps serves over 190 locations and through a network of specialized service lines and partners delivers insights and innovations, user experiences and end-to-end seamless delivery making Unilever Purpose Led and Future Fit\nBackground\nFor Unilever to remain competitive in the future, the business needs to continue on the path to become data intelligent. The Data Analytics team will persevere to make Unilever Data Intelligent, powering key decisions with data, insights, advanced analytics and AI. Our ambition is to enable democratization of data, information and insights as a completely agile organization that builds fantastic careers for our people and is accountable for delivering great work that maximizes impact and delivers growth.\nThis Data Analytics function endeavours to create clear accountability for all aspects of Data Strategy, Data Management, Information Management, Analytics, and Insights. We are accountable for impact of solutions, maintaining market relevance and minimising unnecessary overlaps in analytics products, ensuring simplicity and that our solutions better meet the needs of our users. We partner with the Digital and Data Legal Counsel to ensure that our Data Defence (Privacy, Governance, Quality, etc) is well structured and sufficiently robust to use data and AI correctly throughout the enterprise. We democratize information across the business, while supporting the culture shift required for data driven decision making.\nOur vision is to make Unilever data intelligent, partnering with the business to power key decisions with data, advanced analytics and AI to accelerate growth. Our 5 strategies to achieve this are:\nAccelerate simplify access to relevant data, information and insights Build in-house, leading-edge data, information, insights analytics capability Lead the data insights culture and careers to empower employees across Unilever Rapidly embed analytics products, solutions and services to drive growth Advance Information Automation at Scale\nThe Senior Data Scientist is an exciting role in the Data Foundation. This team builds state of the art machine learning algorithms, maximising the impact of analytic solutions in driving enterprise performance. Typical initiatives include optimizing trade promotion investments, accurately forecasting customer demand, using NLP to glean insight on consumer trends from search data, and making individual assortment recommendations for each of the millions of stores that sell Unilever products.\nMain Purpose of the Job:\nThe Senior Data Scientist improves business performance in the functional area of Unilever they serve, through the application of world class data science capability. They own delivery of data science on moderate projects or specific modules of a major global initiative.\nKey accountabilities:\nInteract with relevant teams to identify business challenges where data science can help\nApply comprehensive data science knowledge to propose optimal techniques for key business challenges\nCreate detailed data science proposals and project plans, flagging any limitations of proposed solution\nDesign and prototype experimental solutions, particularly machine learning models\nDesign scaled solutions and ensure high quality and timely delivery\nFacilitate industrialization and ongoing operation of solutions through well organised code, clear documentation and collaboration with ML Ops resources\nGovern the work of 3rd party vendors where needed to support delivery, while maximising creation of Unilever IP\nRepresent Data Science in cross-functional governance of projects, engaging with stakeholders up to Director level\nHighlight recent developments in data science capability which could solve additional challenges\nLead a team of up 1-2 data scientists / interns, providing career mentorship and line management\nProvide technical guidance to data scientists across DA, particularly on the projects you lead\nSupport the growth of DA s data science capability by contributing to activities such as tool and vendor selection, best practice definition, recruitment, and creation of training materials\nBuild the reputation of DA s data science capability within Unilever and externally, through activities such as community engagement (e. g. Yammer), publications or blogs\nProvide ad-hoc immediate support to the business when needed (for example Covid-19 crisis support)\nDepending on the specific project, the Senior Data Scientist can expect 60-90% of their work to be hands-on prototyping solutions, with the remainder spent planning and designing, overseeing and reviewing work of project staff, interfacing with stakeholders and managing team members.\nExperience and qualifications required:\nStandards of Leadership Required in This Role\nPersonal Mastery (Data-science and advanced analytics)\nAgility\nBusiness acumen\nPassion for High Performance\nKey Skills Required\nProfessional Skills\nMachine learning - Expert\nStatistical modelling - Expert\nForecasting - Expert\nOptimisation techniques and tools - Fully Operational\nPython coding - Fully Operational\nData science platform tools e. g. MS Azure, Databricks - Fully Operational\nDeep learning (and applications to NLP Computer Vision) - Fully Operational\nCollaborative development using Git repos - Fully Operational\nAutomated Machine Learning platforms - Foundational knowledge\nWhile a broad data science technical background is required, the role will benefit from deeper skills (for example graduate studies or prior work experience) in one of the following areas, optimization, simulation, forecasting, natural language processing, computer vision or geospatial analysis.\nGeneral Skills\nProject Management - Expert\nCommunication / presentation skills - Expert\n3rd party resource management - Expert\nCPG Industry analytics - Expert\nStrong communication and stakeholder engagement skills are essential, including the ability to influence peers and senior business stakeholders across Unilever.\nRelevant Experience:\nMinimum of B. E. in a relevant technical field (e. g. Computer Science, Engineering, Statistics, Operations Research); preferably a postgraduate (Masters or Doctorate) degree\nAt least 4 years building data science solutions to solve business problems, preferably in the CPG industry (less experience may be acceptable if balanced by strong post-grad qualifications)\nExperience with open source languages (eg. Python) and preferably with distributed computing (PySpark)\nExperience deploying solutions in a modern cloud-based architecture\nExperience managing the work of team members and 3rd party resource vendors\nExperience presenting insights and influencing decisions of senior non-technical stakeholders\nKey interfaces\nInternal\nUnilever operational, marketing, customer development, supply chain, product finance teams\nInternal DA teams (Engagement teams; Data CoE; Solution Factory; BDL Factory; Information Factory; Tech Transformation)\nWider Unilever analytics and data science professionals\nExternal\n3rd party Data Science vendors\nUniversities\nIndustry bodies",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Operations research', 'Automation', 'data science', 'Data management', 'Project management', 'Information management', 'Resource management', 'Forecasting', 'Recruitment']",2025-06-13 06:14:47
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-13 06:14:48
Modeling Technical Lead Engineer,Lam Research,4 - 6 years,Not Disclosed,['Bengaluru'],"The Group You ll Be A Part Of\nIn the Global Products Group, we are dedicated to excellence in the design and engineering of Lams etch and deposition products. We drive innovation to ensure our cutting-edge solutions are helping to solve the biggest challenges in the semiconductor industry.\nThe Impact You ll Make\nPlans, designs and develops hardware, software, semiconductor or telecommunications systems.\nWho We re Looking For\nModel development: Developing computational / numerical models to simulate fluid flow, heat transfer and chemical reactions. Experience of modeling semiconductor processing equipment is an added advantage.\nSimulation Setup: Setting up and configuring simulations based on specific semiconductor process requirements such as deposition, etching or cleaning processes.\nReduced Order models: Build reduced order models by applying dimensionality reduction techniques, developing mathematical formulations or algorithms. Train the reduced order models using preprocesses data and validate them. Utilize machine learning algorithms regression techniques to optimize model parameters and improve predictive accuracy.\nParameter Optimization: Performing parametric studies to optimize process parameters for improved flow and thermal performance of the semiconductor equipment.\nVerification and Validation: Validate and verify CFD models against experimental data or analytical solutions to ensure accuracy of numerical models.\nTroubleshooting: Investigate and troubleshoot process issues or anomalies by analyzing simulation results and identifying potential root causes.\nCollaboration: Collaborate with process, hardware engineers and R&D teams to integrate modeling insights into real-world semiconductor manufacturing processes and address process challenges.\nInnovation and Research: Stay up to date with the latest advancements in CFD techniques, Fluid and heat transfer modeling techniques to propose innovative solutions and drive continuous improvements.\nDocumentation and Reporting: Maintain detailed records of simulations, methodologies, and results. Prepare technical reports and presentations for internal and external stakeholders. Analyze large datasets generated from simulations, extract meaningful insights, and present findings to the team. Use data-driven approaches to make informed decisions.\nQuality Assurance: Ensure quality and reliability of CFD simulations through testing, validation and adherence to best practices and industry standards.\nRisk Assessment: Assess potential risks associated with process variations and design changes through predictive modeling and sensitivity analyses.\nPreferred Qualifications\nM.S./M.E./M.Tech/PhD in Mechanical Engineering, Chemical Engineering, Materials Science, or a related field with chemical reactions modeling as an added advantage\nYears of Experience: Masters with 4 to 6 years of relevant experience/PhD with 1-2 years of relevant experience\nStrong background in Fluid and Heat transfer, Multiphase flow, Discrete Phase Modeling, Reaction Kinetics and Chemistry models, computational fluid dynamics (CFD)\nProficiency in using software packages such as ANSYS FLUENT, OPTISLANG, TWIN BUILDER, STARCCM+.\nProficiency in programming languages such as Python, MATLAB or FORTRAN.\nUnderstanding of parallel computing architectures and experience in using HPC resources.\nStrong analytical and problem-solving skills with the ability to translate simulation data into actionable recommendations.\nUse of AI/ML techniques for Data modeling and evaluation\nKnowledge of optimization techniques\nExcellent communication skills with the ability to convey complex technical information effectively to both technical and non-technical audiences.\nSelf-driven and able to work independently while also collaborating effectively within a team.\nOur Commitment\nWe believe it is important for every person to feel valued, included, and empowered to achieve their full potential. By bringing unique individuals and viewpoints together, we achieve extraordinary results.\nLam Research (""Lam"" or the ""Company"") is an equal opportunity employer. Lam is committed to and reaffirms support of equal opportunity in employment and non-discrimination in employment policies, practices and procedures on the basis of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex (including pregnancy, childbirth and related medical conditions), gender, gender identity, gender expression, age, sexual orientation, or military and veteran status or any other category protected by applicable federal, state, or local laws. It is the Companys intention to comply with all applicable laws and regulations. Company policy prohibits unlawful discrimination against applicants or employees.\nLam offers a variety of work location models based on the needs of each role. Our hybrid roles combine the benefits of on-site collaboration with colleagues and the flexibility to work remotely and fall into two categories - On-site Flex and Virtual Flex. On-site Flex you ll work 3+ days per week on-site at a Lam or customer/supplier location, with the opportunity to work remotely for the balance of the week. Virtual Flex you ll work 1-2 days per week on-site at a Lam or customer/supplier location, and remotely the rest of the time.",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['CFD', 'Manager Quality Assurance', 'Simulation', 'Fluid dynamics', 'Analytical', 'Ansys', 'thermal', 'MATLAB', 'Fortran', 'Python']",2025-06-13 06:14:50
Data Scientist,Grid Dynamics,10 - 20 years,Not Disclosed,['Hyderabad'],"Role & responsibilitiMes\n\nCandiate needs to be 8+ Years of Experience\n\nDetails on tech stack\nPython\nPrompt engineering\nBest practices for prompt engineering\nHow LLM can be used in applications for a variety of tasks\nNLP\nUnderstanding of typical NLP problems: classification, NER, summarization, question answering, sentiment analysis, etc.\nTheoretical intuitive understanding of how Transformers work (tokenization, attention, etc).\nWord and sentence embeddings\nVector search\nVector databases, performance tuning\nDocument chunking techniques\nLLM applications development\nLangChain, LlamaIndex\nChain of Thoughts, DSP, and other techniques\nAgents and tools\nGoogle cloud (GCP)\nNice to have requirements to the candidate\nPreferable, the engineers are expected to have IT services/consulting experience.\nProficient in developing LLM-powered systems using advanced prompt engineering techniques, RAG and agentic design patterns. Experienced with frameworks like LangChain, LlamaIndex, and DSPy.\nFamiliar with evaluation approaches and metrics for different types of LLM-based systems.\nExperienced with keyword and vector search methods, including understanding of their underlying algorithms. Familiar with popular vector search engines.\nCompetent in various document understanding models and techniques to parse complex documents and implement effective chunking strategies for RAG systems.\nFamiliar with LLM and embedding models fine-tuning techniques.\nCompetent in using joint vision-language and generative models to solve various problems related to image generation, visual question answering, and multi-modal search. Familiar with diffusion models and associated techniques like LoRA, Dreambooth, and ControlNet.\nUnderstanding of the challenges and risks associated with the development of Generative AI systems and how to mitigate them.\nFamiliar with various architecture design patterns for different types of LLM-based applications such as chatbots, text2sql, document understanding, etc. Familiar with various approaches to scalability and cost reduction in Generative AI systems.\nAbility to stay updated with the latest advancements in Generative AI and integrate emerging technologies to drive innovation and improve the performance of AI systems.\nFamiliar with Responsible AI principles and Human-AI interaction design best practices.\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Lora', 'Natural Language Processing', 'Deep Learning', 'Python']",2025-06-13 06:14:52
Senior Data Scientist with GCP,TVS Next,5 - 7 years,Not Disclosed,['Bengaluru'],"What you’ll do:\nUtilize advanced mathematical, statistical, and analytical expertise to research, collect, analyze, and interpret large datasets from internal and external sources to provide insight and develop data driven solutions across the company\nBuild and test predictive models including but not limited to credit risk, fraud, response, and offer acceptance propensity\nResponsible for the development, testing, validation, tracking, and performance enhancement of statistical models and other BI reporting tools leading to new innovative origination strategies within marketing, sales, finance, and underwriting",,,,"['analytical', 'scikit-learn', 'searching', 'bi', 'pyspark', 'numpy', 'sql', 'analytics', 'apache', 'automation', 'data science', 'spark', 'gcp', 'bigquery', 'data visualization', 'xgboost', 'programming', 'reporting', 'ml', 'advanced analytics', 'python', 'data processing', 'predictive', 'jupyter notebook', 'bert', 'pandas', 'matplotlib', 'statistics']",2025-06-13 06:14:54
Compiler Engineer,Luxoft,3 - 8 years,Not Disclosed,['Bengaluru'],"Project description\nThe ideal candidate will have a strong background in compiler development, with a minimum of 3 years of experience in the field. He / She will be responsible for contributing to the development and optimization of our compilers, with a focus on enhancing performance and functionality. Experience with open-source development, particularly with GCC and/or LLVM, is highly preferred.\n\nResponsibilities\n\nDesign, develop, and optimize compiler components for AOCC, focusing on performance improvements and new feature implementation.\n\nCollaborate with cross-functional teams to integrate compiler enhancements into the overall software ecosystem.\n\nParticipate in code reviews, providing constructive feedback to peers and ensuring high-quality code standards.\n\n-Contribute to open-source projects, particularly GCC and/or LLVM, by submitting patches, reviewing code, and engaging with the community.\n\nAnalyze and resolve complex compiler-related issues, ensuring robust and efficient solutions.\n\nStay updated with the latest advancements in compiler technology and incorporate relevant innovations into AOCC products.\n\nDocument development processes, technical specifications, and user guides to facilitate knowledge sharing and product support.\n\nSkills\nMust have\n\n3+ years of experience in compiler development or a related area.\n\nProven experience with open-source development, preferably with GCC and/or LLVM.\n\nStrong programming skills in C/C++ and familiarity with other programming languages.\n\nSolid understanding of compiler design principles, optimization techniques, and code generation.\n\nExperience with performance analysis and profiling tools.\n\nExcellent problem-solving skills and the ability to work independently and collaboratively.\n\nStrong communication skills, both written and verbal, with the ability to convey complex technical concepts to diverse audiences.\n\nNice to have\n\nExperience with parallel programming models such as OpenMP.\n\nFamiliarity with performance benchmarking tools like SPEC CPU2017.\n\nKnowledge of modern processor architectures and instruction sets.\n\nOther\n\nLanguages\n\nEnglishB2 Upper Intermediate\n\nSeniority\n\nRegular",Industry Type: Legal,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'c', 'gcc', 'code generation', 'design principles', 'matlab', 'targetlink', 'simulink', 'python', 'canoe', 'stateflow', 'machine learning', 'autosar', 'r', 'java', 'rtw', 'embedded systems', 'design patterns', 'sil', 'embedded c', 'data structures', 'model based development', 'can bus', 'performance analysis']",2025-06-13 06:14:56
S&C GN - Data&AI - Retail - Consultant,Accenture,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Title - Retail Specialized Data Scientist Level 9 SnC GN Data & AI\n\n\n\nManagement Level:09 - Consultant\n\n\n\nLocation:Bangalore / Gurgaon / Mumbai / Chennai / Pune / Hyderabad / Kolkata\n\n\n\nMust have skills:\nA solid understanding of retail industry dynamics, including key performance indicators (KPIs) such as sales trends, customer segmentation, inventory turnover, and promotions.\nStrong ability to communicate complex data insights to non-technical stakeholders, including senior management, marketing, and operational teams.\nMeticulous in ensuring data quality, accuracy, and consistency when handling large, complex datasets.\nGather and clean data from various retail sources, such as sales transactions, customer interactions, inventory management, website traffic, and marketing campaigns.\nStrong proficiency in Python for data manipulation, statistical analysis, and machine learning (libraries like Pandas, NumPy, Scikit-learn).\nExpertise in supervised and unsupervised learning algorithms\nUse advanced analytics to optimize pricing strategies based on market demand, competitor pricing, and customer price sensitivity.\n\n\n\n\nGood to have skills:\nFamiliarity with big data processing platforms like Apache Spark, Hadoop, or cloud-based platforms such as AWS or Google Cloud for large-scale data processing.\nExperience with ETL (Extract, Transform, Load) processes and tools like Apache Airflow to automate data workflows.\nFamiliarity with designing scalable and efficient data pipelines and architecture.\nExperience with tools like Tableau, Power BI, Matplotlib, and Seaborn to create meaningful visualizations that present data insights clearly.\n\n\nJob\n\n\nSummary: The Retail Specialized Data Scientist will play a pivotal role in utilizing advanced analytics, machine learning, and statistical modeling techniques to help our retail business make data-driven decisions. This individual will work closely with teams across marketing, product management, supply chain, and customer insights to drive business strategies and innovations. The ideal candidate should have experience in retail analytics and the ability to translate data into actionable insights.\n\n\n\n\nRoles & Responsibilities:\nLeverage Retail Knowledge:Utilize your deep understanding of the retail industry (merchandising, customer behavior, product lifecycle) to design AI solutions that address critical retail business needs.\nGather and clean data from various retail sources, such as sales transactions, customer interactions, inventory management, website traffic, and marketing campaigns.\nApply machine learning algorithms, such as classification, clustering, regression, and deep learning, to enhance predictive models.\nUse AI-driven techniques for personalization, demand forecasting, and fraud detection.\nUse advanced statistical methods help optimize existing use cases and build new products to serve new challenges and use cases.\nStay updated on the latest trends in data science and retail technology.\nCollaborate with executives, product managers, and marketing teams to translate insights into business actions.\n\n\n\n\nProfessional & Technical Skills:\nStrong analytical and statistical skills.\nExpertise in machine learning and AI.\nExperience with retail-specific datasets and KPIs.\nProficiency in data visualization and reporting tools.\nAbility to work with large datasets and complex data structures.\nStrong communication skills to interact with both technical and non-technical stakeholders.\nA solid understanding of the retail business and consumer behavior.\nProgramming Languages:Python, R, SQL, Scala\nData Analysis Tools:Pandas, NumPy, Scikit-learn, TensorFlow, Keras\nVisualization Tools:Tableau, Power BI, Matplotlib, Seaborn\nBig Data Technologies:Hadoop, Spark, AWS, Google Cloud\nDatabases:SQL, NoSQL (MongoDB, Cassandra)\n\n\n\n\nAdditional Information: -\n\nQualification\n\n\n\nExperience:Minimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification:Bachelors or Master's degree in Data Science, Statistics, Computer Science, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'artificial intelligence', 'data visualization', 'statistics', 'algorithms', 'data manipulation', 'scikit-learn', 'scala', 'numpy', 'unsupervised learning', 'sql', 'pandas', 'tensorflow', 'spark', 'consumer behavior', 'keras', 'hadoop', 'aws', 'reporting tools', 'retail business']",2025-06-13 06:14:58
S&C GN - Data&AI - Hi Tech - Data Science - Consultant,Accenture,3 - 7 years,Not Disclosed,['Bengaluru'],"Job Title - S&C Global Network - AI - Hi Tech - Data Science Consultant\n\n\n\nManagement Level:9-Team Lead/Consultant\n\n\n\nLocation:Hyderabad, HDC2A\n\n\n\nMust-have skills:Data Science\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nWe are seeking a skilled and experienced Data Scientist to join our Hi-Tech practice.\nThe ideal candidate should have hands-on experience in data science within industries such as semiconductors, enterprise technology, consumer technology, medical technology.\nAs a Data Scientist, you will be responsible for developing AI models/applying GenAI techniques in areas such as marketing & consumer analytics, predictive asset maintenance, production optimization, supply chain, sales & channel partner program analytics, and connected products.\n\n\n\nWhat you would do in this role\nDevelop and implement AI models and GenAI applications to address business challenges in semiconductors, enterprise technology, consumer technology, medical technology and related industries.\nCollaborate with cross-functional teams to gather requirements, design solutions, and deploy models into production environments.\nDevelop and implement GenAI based solutions through contextual prompt engineering and prompt tuning and supporting solution architects on the design of GenAI-powered solutions/assets.\nUtilize your expertise in PLM/ERP/CRM/Contact Center systems to integrate data sources and ensure seamless operation of AI solutions.\nDesign and develop machine learning models using Python, with proficiency in NLP and Computer Vision techniques.\nArchitect functional solutions and provide technical guidance to enhance the performance and scalability of AI systems.\nLeverage cloud platforms, with preference for Azure/GCP, and experience with AWS is also valued.\nStay updated on emerging technologies and industry trends, contributing to continuous improvement initiatives within the organization.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nProven track record of developing AI models in areas such as channel analytics, marketing & customer experience, supply chain analytics, predictive maintenance, production optimization, and connected products.\nStrong proficiency in Python programming, with experience in NLP and Computer Vision.\nExposure to PLM/ERP/CRM systems and understanding of their integration with AI solutions.\nExperience with cloud platforms, preferably Azure/GCP, and familiarity with AWS.\nKnowledge of LLM exposure and experience with tools such as ChatGPT, Llama 2, Claude 2, Hugging Face, etc. for prompt engineering, prompt tuning, etc will be an advantage.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'erp', 'natural language processing', 'data science', 'computer vision', 'appium', 'ai solutions', 'cucumber', 'microsoft azure', 'machine learning', 'artificial intelligence', 'eclipse', 'plm', 'deep learning', 'tensorflow', 'java', 'gcp', 'ai techniques', 'aws', 'testng', 'bdd framework', 'crm']",2025-06-13 06:14:59
Applied Scientist,Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"Amazon.com s Buyer Risk Preventions (BRP) mission is to make Amazon the safest and most trusted place worldwide to transact online. BRP safeguards every financial transaction across all Amazon sites. As such, BRP designs and builds the software systems, risk models, and operational processes that minimize risk and maximize trust in Amazon.com. The BRP organization is looking for an Applied Scientist for the Buyer Abuse team, whose mission is to combine advanced analytics with investigator insight to create mechanisms to proactively and reactively reduce the impact of abuse across Amazon.\n\n\nAs an Applied Scientist, you will be responsible for modeling complex problems, discovering insights, and building risk algorithms that identify opportunities through statistical models, machine learning, and visualization techniques to improve operational efficiency and reduce monetary losses and improve customer trust.\n\nYou will need to collaborate effectively with business and product leaders within BRP and cross-functional teams to build scalable solutions against high organizational standards. The candidate should be able to apply a breadth of tools, data sources, and ML techniques to answer a wide range of high-impact business questions and proactively present new insights in concise and effective manner.\n\nThe candidate should be an effective communicator capable of independently driving issues to resolution and communicating insights to non-technical audiences. This is a high impact role with goals that directly impacts the bottom line of the business.\n\nResponsibilities:\nInvent, implement, and deploy state of the art machine learning algorithms and systems\nBuild prototypes and explore conceptually new solutions\nDefine and conduct experiments to validate/reject hypotheses, and communicate insights and recommendations to Product and Tech teams\nTake ownership of how ML solutions impact Amazon resources and Customer experience\nDevelop efficient data querying infrastructure for both offline and online use cases\nCollaborate with cross-functional teams from multidisciplinary science, engineering and business backgrounds to enhance current automation processes\nLearn and understand a broad range of Amazon s data resources and know when, how, and which to use and which not to use.\nResearch and implement novel machine learning and statistical approaches\nMaintain technical document and communicate results to diverse audiences with effective writing, visualizations, and presentations\nPlease visit https://www.amazon.science for more information PhD, or Masters degree and 2+ years of CS, CE, ML or related field experience\nExperience programming in Java, C++, Python or related language\nExperience in any of the following areas: algorithms and data structures, parsing, numerical optimization, data mining, parallel and distributed computing, high-performance computing Experience using Unix/Linux\nExperience in professional software development",,,,"['Unix', 'C++', 'Automation', 'Linux', 'Machine learning', 'Data structures', 'high performance computing', 'Data mining', 'Operations', 'Python']",2025-06-13 06:15:01
S&C GN - Data&AI - CMT Eng - Manager,Accenture,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Title - S&C Global Network - AI - CMT ML Ops - Manager\n\n\n\nManagement Level:7- Manager\n\n\n\nLocation:Open\n\n\n\nMust-have skills:AI Architecture/ Gen AI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\nAI Architecture strategy and Assessment:\nLead the development and assessment of AI architectures, ensuring they align with business goals and objectives.\nEvaluate existing AI solutions, identifying areas for improvement, optimization, and innovation.\n\nOperating Model Design:\nDesign robust operating models for Generative AI implementations, considering scalability, efficiency, and business impact.\nCollaborate with cross-functional teams to integrate AI capabilities seamlessly into existing operational workflows.\nDevelop comprehensive blueprints for Generative AI solutions, outlining technical specifications, data requirements, and integration points.\nEnsure blueprints adhere to industry best practices and standards.\n\nRoadmap Building:\nCreate strategic roadmaps for the implementation and evolution of Generative AI solutions.\nDefine milestones, key performance indicators (KPIs), and success criteria to measure the progress of AI initiatives.\n\nIncubating GenAI Solutions:\nIdentify emerging trends and technologies in Generative AI and incubate innovative solutions.\nWork closely with research and development teams to experiment and prototype new Generative AI concepts.\nOversee the end-to-end implementation of Generative AI solutions, collaborating with engineering teams to ensure successful deployment.\nProvide technical leadership and guidance throughout the implementation lifecycle.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nProven track record of designing and implementing successful AI solutions in real-world business environments.\nIn-depth knowledge of AI architectures, frameworks, and tools.\nStrong analytical and problem-solving skills.\nExcellent communication and collaboration abilities, with experience working cross-functionally.\nDemonstrated leadership in driving AI initiatives from conception to deployment.\nExperience in Telecom or Hi Tech or Software and platform industry desirable\nTools & Techniques\nProficiency in Python programming language and key libraries like Pandas, TensorFlow, PyTorch, and Keras.\nUnderstanding and implemention knowledge of deep learning architectures such as CNNs, RNNs, and GANs.\nGood software engineering practices, including code modularization, documentation, and testing\nFamiliarity with cloud platforms like AWS, Google Cloud, or Microsoft Azure can be beneficial for deploying and scaling GenAI models.\n\nIf you are a visionary AI architect with a passion for innovation and a proven track record of delivering impactful Generative AI solutions, we invite you to apply and contribute to our dynamic and forward-thinking organization.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:8+ years\n\n\n\n\nEducational Qualification:Btech/ BE",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'gen', 'tensorflow', 'pytorch', 'keras', 'hi', 'cnn', 'data management', 'natural language processing', 'neural networks', 'microsoft azure', 'machine learning', 'artificial intelligence', 'sales', 'sql', 'pandas', 'deep learning', 'rnn', 'data science', 'gcp', 'computer vision', 'telecom', 'aws']",2025-06-13 06:15:03
S&C Global Network - AI - Life Sciences -Data Science Sr. Manager,Accenture,11 - 15 years,Not Disclosed,['Bengaluru'],"JR:\n\n\n\nR00229254\n\n\n\nExperience:\n\n\n\n11-15 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\n\n\n\n---------------------------------------------------------------------\n\n\n\nJob Title -\n\n\n\nS&C Global Network - AI - Healthcare Analytics - Senior Manager\n\n\n\nManagement Level:\n\n\n\n6-Senior Manager\n\n\n\nLocation:\n\n\n\nBangalore/Gurgaon\n\n\n\nMust-have skills:R,Phython,SQL,Spark,Tableau ,Power BI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\nAs part of our Data & AI practice, you will join a worldwide network of smart and driven colleagues experienced in leading AI/ML/Statistical tools, methods and applications. From data to analytics and insights to actions, our forward-thinking consultants provide analytically-informed, issue-based insights at scale to help our clients improve outcomes and achieve high performance.\n\n\n\nWHATS IN IT FOR YOU\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPotential to Co-create with leaders in strategy, industry experts, enterprise function practitioners, and business intelligence professionals to shape and recommend innovative solutions that leverage emerging technologies.\nAbility to embed responsible business into everythingfrom how you service your clients to how you operate as a responsible professional.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nOpportunity to thrive in a culture that is committed to accelerating equality for all. Engage in boundaryless collaboration across the entire organization.\n\n\n\n\nWhat you would do in this role\nLead proposals, and business development efforts and coordinate with other colleagues to cross-sell/ up-sell Life Sciences offerings to existing as well as potential clients.\nLead client discussions, developing new industry Point of View (PoV), re-usable assets (tools)\nCollaborate closely with cross-functional teams including Data engineering, technology, and business stakeholders to identify opportunities for leveraging data to drive business solutions.\nLead and manage teams to deliver transformative and innovative client projects.\nGuide teams on analytical and AI methods and approaches\nManage client relationships to foster trust, deliver value, and build the Accenture brand\nDrive consulting practice innovation and thought leadership in your area of specialization\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\nExecute a transformational change plan aligned with the clients business strategy and context for change. Engage stakeholders in the change journey and build commitment to change.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nProven experience in cross-sell/ up-sell\nLeverage ones hands-on experience of working across one or more of these areas such as real-world evidence data, R&D clinical data, and digital marketing data.\nExperience with handling Datasets like Komodo, RAVE, IQVIA, Truven, Optum, SHS, Specialty Pharmacy, PSP, etc.\nExperience in building and deployment of Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\nExcellent analytical and problem-solving skills, with a data-driven mindset.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood Client handling skills; able to demonstrate thought leadership & problem-solving skills.\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:\n\n\n\n11-15 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'power bi', 'sql', 'tableau', 'r', 'hypothesis testing', 'time series', 'business analytics', 'machine learning', 'data engineering', 'business intelligence', 'artificial intelligence', 'data science', 'computer science', 'spark', 'predictive modeling', 'segmentation', 'statistics', 'ml']",2025-06-13 06:15:04
S&C Global Network - AI - CG&S - Manager Data Science,Accenture,7 - 9 years,Not Disclosed,['Bengaluru'],"Job Title:Industry & Function AI Decision Science Manager + S&C GN\n\n\n\nManagement Level:07 - Manager\n\n\n\nLocation:Primary Bengaluru, Secondary Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nConsumer Goods & Services domain expertise\n\n\n\n, AI & ML, Proficiency in Python, R, PySpark, SQL\n\n\n\n, Experience in cloud platforms (Azure, AWS, GCP)\n\n\n\n, Expertise in Revenue Growth Management, Pricing Analytics, Promotion Analytics, PPA/Portfolio Optimization, Trade Investment Optimization.\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nExperience with Large Language Models (LLMs) like ChatGPT, Llama 2, or Claude 2\n\n\n\n, Familiarity with optimization methods, advanced visualization tools (Power BI, Tableau), and Time Series Forecasting\n\n\n\nJob\n\n\nSummary:\n\nAs a\n\n\n\nDecision Science Manager, you will lead the design and delivery of AI solutions in the Consumer Goods & Services domain. This role involves working closely with clients to provide advanced analytics and AI-driven strategies that deliver measurable business outcomes. Your expertise in analytics, problem-solving, and team leadership will help drive innovation and value for the organization.\n\n\n\n\nRoles & Responsibilities:\nAnalyze extensive datasets and derive actionable insights for Consumer Goods data sources (e.g., Nielsen, IRI, EPOS, TPM).\nEvaluate AI and analytics maturity in the Consumer Goods sector and develop data-driven solutions.\nDesign and implement AI-based strategies to deliver significant client benefits.\nEmploy structured problem-solving methodologies to address complex business challenges.\nLead data science initiatives, mentor team members, and contribute to thought leadership.\nFoster strong client relationships and act as a key liaison for project delivery.\nBuild and deploy advanced analytics solutions using Accentures platforms and tools.\nApply technical proficiency in Python, Pyspark, R, SQL, and cloud technologies for solution deployment.\nDevelop compelling data-driven narratives for stakeholder engagement.\nCollaborate with internal teams to innovate, drive sales, and build new capabilities.\nDrive insights in critical Consumer Goods domains such as\nRevenue Growth Management\nPricing Analytics and Pricing Optimization\nPromotion Analytics and Promotion Optimization\nSKU Rationalization/ Portfolio Optimization\nPrice Pack Architecture\nDecomposition Models\nTime Series Forecasting\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in AI and analytics solutions (descriptive, diagnostic, predictive, prescriptive, generative).\nExpertise in delivering large scale projects/programs for Consumer Goods clients on Revenue Growth Management - Pricing Analytics, Promotion Analytics, Portfolio Optimization, etc.\nDeep and clear understanding of typical data sources used in RGM programs POS, Syndicated, Shipment, Finance, Promotion Calendar, etc.\nStrong programming skills in Python, R, PySpark, SQL, and experience with cloud platforms (Azure, AWS, GCP) and proficient in using services like Databricks and Sagemaker.\nDeep knowledge of traditional and advanced machine learning techniques, including deep learning.\nExperience with optimization techniques (linear, nonlinear, evolutionary methods).\nFamiliarity with visualization tools like Power BI, Tableau.\nExperience with Large Language Models (LLMs) like ChatGPT, Llama 2.\nCertifications in Data Science or related fields.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate has a strong educational background in data science and a proven track record in delivering impactful AI solutions in the Consumer Goods sector.\nThis position offers opportunities to lead innovative projects and collaborate with global teams.\nJoin Accenture to leverage cutting-edge technologies and deliver transformative business outcomes.\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:Minimum 7-9 years of experience in data science, particularly in the Consumer Goods sector\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or MBA (Data Science specialization preferred)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'machine learning', 'sql', 'ml', 'ppa', 'data sources', 'analytics services', 'pricing analytics', 'portfolio optimization', 'aws sagemaker', 'cloud technologies', 'artificial intelligence', 'data bricks', 'r', 'revenue', 'trade', 'data science', 'aws', 'consumer goods']",2025-06-13 06:15:06
S&C Global Network - AI - CDI - Data Science Analyst,Accenture,1 - 6 years,Not Disclosed,['Bengaluru'],"Job Title Ind & Func AI Decision Science Analyst - S&C GN\n\n\n\nManagement Level :11 - Analyst\n\n\n\nLocation:Gurgaon\n\n\n\nMust have skills:Generative AI, Machine Learning, Large Language Models (LLMs), Python, SQL\n\n\n\n\nGood to have skills:Spark, Cloud Platforms (AWS, Azure, GCP), NLP, Computer Vision\n\n\n\nJob\n\n\nSummary: As an AI Decision Science Analyst, you will play a key role in designing, building, and deploying advanced AI models and solutions to address business challenges across various industries. You will leverage Generative AI, Machine Learning, and Large Language Models (LLMs) to drive innovation and deliver impactful insights for clients. Your work will involve collaborating with cross-functional teams, and contributing to the development of advanced analytics capabilities.\n\n\n\n\nRoles & Responsibilities:\nLeverage Advanced Data Science Techniques\nDevelop solutions using Generative AI, Machine Learning, and Large Language Models (LLMs).\nDefine data requirements, clean, aggregate, analyze, and interpret data while conducting data quality assessments.\nDevelop and Implement AI Models\nBuild and deploy AI models and Generative AI applications.\nTrain and fine-tune LLMs using large-scale datasets to optimize performance and accuracy.\nEvaluate model performance and implement iterative improvements.\nSolution Integration and Deployment\nCollaborate to integrate AI solutions into end-to-end workflows, ensuring scalability.\nUtilize cloud platforms like AWS, Azure, or GCP for model development and deployment.\nInnovation and Knowledge Sharing\nStay updated on advancements in AI and Data Science, exploring innovative techniques and frameworks.\nDocument methodologies and findings for effective knowledge sharing.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Generative AI, Machine Learning, LLMs, Python, SQL.\nSpark, Cloud platforms (AWS, Azure, GCP), NLP, Computer Vision.\nExperience in developing and AI/ML models.\n\n\n\n\nAdditional Information: - The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful solutions using data science and analytics.\nThis position is based at our Gurugram office.\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:Minimum 1+ years of experience in Data Science, preferably within a consulting environment\n\n\n\n\nEducational Qualification:Bachelors or Masters degree (BE/BTech/MBA) in Statistics, Computer Science, Mathematics, or related disciplines with an excellent academic record",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'natural language processing', 'machine learning', 'sql', 'data science', 'advanced analytics', 'mathematics', 'microsoft azure', 'artificial intelligence', 'data quality', 'computer science', 'gcp', 'spark', 'computer vision', 'model development', 'aws', 'ml', 'statistics']",2025-06-13 06:15:07
S&C Global Network - AI - Life Sciences -Data Science Manager,Accenture,8 - 12 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nS&C Global Network - AI - Healthcare Analytics - Manager\n\n\n\nManagement Level:\n\n\n\n7-Manager\n\n\n\nLocation:\n\n\n\nBangalore/Gurgaon\n\n\n\nMust-have skills:R,Phython,SQL,Spark,Tableau ,Power BI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPotential to Co-create with leaders in strategy, industry experts, enterprise function practitioners, and business intelligence professionals to shape and recommend innovative solutions that leverage emerging technologies.\nAbility to embed responsible business into everythingfrom how you service your clients to how you operate as a responsible professional.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nOpportunity to thrive in a culture that is committed to accelerating equality for all. Engage in boundaryless collaboration across the entire organization.\n\n\n\n\nWhat you would do in this role\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nManage daily operations within the team, guide & counsel the team members towards driving the Solution delivery to the Client.\nParticipating in client discussions, developing new industry Point of View (PoV), re-usable assets (tools)\nTranslate complex analytical findings into clear and concise reports and presentations for various stakeholders, including clinicians, executives, and patients (depending on the role).\nProvide Subject matter expertise in various sub-segments of the LS industry.\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\nCo-lead proposals, and business development efforts and coordinate with other colleagues to create consensus-driven deliverables.\nExecute a transformational change plan aligned with the clients business strategy and context for change. Engage stakeholders in the change journey and build commitment to change.\nMake presentations wherever required to a known audience or client on functional aspects of his or her domain.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:\n\n\n\n8-12 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'power bi', 'sql', 'tableau', 'r', 'natural language processing', 'mathematics', 'business analytics', 'machine learning', 'business intelligence', 'artificial intelligence', 'deep learning', 'data science', 'computer science', 'spark', 'predictive modeling', 'statistics']",2025-06-13 06:15:09
S&C GN - Data&AI - CMT Eng - Sr. Manager,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Title - S&C Global Network - AI - CMT AI Architecture - Sr Manager\n\n\n\nManagement Level:6-Senior Manager\n\n\n\nLocation:Open\n\n\n\nMust-have skills:AI Architecture/ Gen AI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\nAI Architecture strategy and Assessment:\nLead the development and assessment of AI architectures, ensuring they align with business goals and objectives.\nEvaluate existing AI solutions, identifying areas for improvement, optimization, and innovation.\n\nOperating Model Design:\nDesign robust operating models for Generative AI implementations, considering scalability, efficiency, and business impact.\nCollaborate with cross-functional teams to integrate AI capabilities seamlessly into existing operational workflows.\nDevelop comprehensive blueprints for Generative AI solutions, outlining technical specifications, data requirements, and integration points.\nEnsure blueprints adhere to industry best practices and standards.\n\nRoadmap Building:\nCreate strategic roadmaps for the implementation and evolution of Generative AI solutions.\nDefine milestones, key performance indicators (KPIs), and success criteria to measure the progress of AI initiatives.\n\nIncubating GenAI Solutions:\nIdentify emerging trends and technologies in Generative AI and incubate innovative solutions.\nWork closely with research and development teams to experiment and prototype new Generative AI concepts.\nOversee the end-to-end implementation of Generative AI solutions, collaborating with engineering teams to ensure successful deployment.\nProvide technical leadership and guidance throughout the implementation lifecycle.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nProven track record of designing and implementing successful AI solutions in real-world business environments.\nIn-depth knowledge of AI architectures, frameworks, and tools.\nStrong analytical and problem-solving skills.\nExcellent communication and collaboration abilities, with experience working cross-functionally.\nDemonstrated leadership in driving AI initiatives from conception to deployment.\nExperience in Telecom or Hi Tech or Software and platform industry desirable\nTools & Techniques\nProficiency in Python programming language and key libraries like Pandas, TensorFlow, PyTorch, and Keras.\nUnderstanding and implemention knowledge of deep learning architectures such as CNNs, RNNs, and GANs.\nGood software engineering practices, including code modularization, documentation, and testing\nFamiliarity with cloud platforms like AWS, Google Cloud, or Microsoft Azure can be beneficial for deploying and scaling GenAI models.\n\nIf you are a visionary AI architect with a passion for innovation and a proven track record of delivering impactful Generative AI solutions, we invite you to apply and contribute to our dynamic and forward-thinking organization.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:12+ years\n\n\n\n\nEducational Qualification:Btech/ BE",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'gen', 'tensorflow', 'pytorch', 'keras', 'hi', 'cnn', 'data management', 'natural language processing', 'neural networks', 'microsoft azure', 'machine learning', 'artificial intelligence', 'sales', 'pandas', 'deep learning', 'rnn', 'data science', 'gcp', 'computer vision', 'telecom', 'aws', 'opencv']",2025-06-13 06:15:11
S&C GN - Data&AI - CMT Eng - Associate Manager,Accenture,6 - 11 years,Not Disclosed,['Bengaluru'],"Job Title - S&C Global Network - AI - CMT ML Ops Ass Manager\n\n\n\nManagement Level:8- Associate Manager\n\n\n\nLocation:Open\n\n\n\nMust-have skills:AI Architecture/ Gen AI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\nAI Architecture strategy and Assessment:\nLead the development and assessment of AI architectures, ensuring they align with business goals and objectives.\nEvaluate existing AI solutions, identifying areas for improvement, optimization, and innovation.\n\nOperating Model Design:\nDesign robust operating models for Generative AI implementations, considering scalability, efficiency, and business impact.\nCollaborate with cross-functional teams to integrate AI capabilities seamlessly into existing operational workflows.\nDevelop comprehensive blueprints for Generative AI solutions, outlining technical specifications, data requirements, and integration points.\nEnsure blueprints adhere to industry best practices and standards.\n\nRoadmap Building:\nCreate strategic roadmaps for the implementation and evolution of Generative AI solutions.\nDefine milestones, key performance indicators (KPIs), and success criteria to measure the progress of AI initiatives.\n\nIncubating GenAI Solutions:\nIdentify emerging trends and technologies in Generative AI and incubate innovative solutions.\nWork closely with research and development teams to experiment and prototype new Generative AI concepts.\nOversee the end-to-end implementation of Generative AI solutions, collaborating with engineering teams to ensure successful deployment.\nProvide technical leadership and guidance throughout the implementation lifecycle.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nProven track record of designing and implementing successful AI solutions in real-world business environments.\nIn-depth knowledge of AI architectures, frameworks, and tools.\nStrong analytical and problem-solving skills.\nExcellent communication and collaboration abilities, with experience working cross-functionally.\nDemonstrated leadership in driving AI initiatives from conception to deployment.\nExperience in Telecom or Hi Tech or Software and platform industry desirable\nTools & Techniques\nProficiency in Python programming language and key libraries like Pandas, TensorFlow, PyTorch, and Keras.\nUnderstanding and implemention knowledge of deep learning architectures such as CNNs, RNNs, and GANs.\nGood software engineering practices, including code modularization, documentation, and testing\nFamiliarity with cloud platforms like AWS, Google Cloud, or Microsoft Azure can be beneficial for deploying and scaling GenAI models.\n\nIf you are a visionary AI architect with a passion for innovation and a proven track record of delivering impactful Generative AI solutions, we invite you to apply and contribute to our dynamic and forward-thinking organization.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:6+ years\n\n\n\n\nEducational Qualification:Btech/ BE",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'gen', 'tensorflow', 'pytorch', 'keras', 'hi', 'cnn', 'data management', 'natural language processing', 'neural networks', 'microsoft azure', 'machine learning', 'artificial intelligence', 'sales', 'pandas', 'deep learning', 'rnn', 'data science', 'gcp', 'computer vision', 'telecom', 'aws', 'opencv']",2025-06-13 06:15:13
S&C GN - Data&AI - Life Sciences - Consultant,Accenture,4 - 9 years,Not Disclosed,['Bengaluru'],"Management Level:Ind&Func AI Decision Science Consultant\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nExcellent understanding of Pharma data sets commercial, clinical, Leverage ones hands on experience of working across one or more of these areas such as real-world evidence data, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProgramming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI.\n\n\n\nExperience:Proven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\n\n\n\nJob\n\n\nSummary\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions. Provide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nKey Responsibilities\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nProvide Subject matter expertise in various sub-segments of the LS industry.\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\n\n\n\n\n\nAdditional Information\nProficient in Excel, MS Word, PowerPoint, etc.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\nQualification\n\n\n\nExperience:Proven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'data engineering', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 06:15:14
S&C Global Network - AI - CG&S - Consultant Data Science,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: Industry & Function AI Decision Science Consultant S&C Global Network\n\n\n\nManagement Level: 9 Consultant\n\n\n\nLocation: Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nData Science, AI, ML, Experience with cloud platforms such as AWS, Azure, or Google Cloud, Hands-on experience in programming languages like Python, R, PySpark, and SQL\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nDeep Learning Techniques (e.g. RNN, CNN), Visualization tools like Power BI and Tableau, Exposure to tools like ChatGPT, Llama 2, Hugging Face, etc.\n\n\n\nJob\n\n\nSummary:\n\nAs an Industry & Function AI Decision Science Consultant, you will leverage your expertise in data science and Consumer Goods domain knowledge to design and deliver AI-driven solutions. Your role will include strategic analysis, project delivery, solution development, and technical execution to empower businesses with actionable insights and enable automated and augmented decision-making.\n\n\n\n\nRoles & Responsibilities:\nConduct strategic analysis of the AI, analytics, and data maturity landscape for clients in the Consumer Goods domain\nLead data science engagements, manage delivery teams, and build innovative AI capabilities\nDevelop and implement advanced analytics solutions tailored to client requirements\nUtilize languages like Python, PySpark, R, and SQL for data wrangling and machine learning model development\nLeverage cloud technologies (Azure, AWS, GCP) to integrate and implement AI solutions\nTranslate complex data into compelling narratives for effective data storytelling\nMentor junior team members and contribute to thought leadership\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in Python, R, PySpark, and SQL\nStrong knowledge of traditional statistical methods, machine learning techniques, and deep learning\nHands-on experience in Consumer Goods & Services domain\nCloud integration skills with platforms like AWS, Azure, or Google Cloud\nExperience with optimization techniques (exact and evolutionary)\nCertifications like AWS Certified Data Analytics Specialty or Google Professional Data Engineer\nFamiliarity with visualization tools like Tableau and Power BI\nExposure to large language models (e.g., ChatGPT, Llama 2)\nFamiliarity with version control systems like Git.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate will have a strong educational background in data science, computer science, or a related field, along with a proven track record of delivering impactful AI-driven solutions in the Consumer Goods industry.\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 4-8 years of hands-on experience in data science with a focus on the Consumer Goods industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or equivalent degree with Data Science specialization (from a premier institute)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'artificial intelligence', 'deep learning', 'data science', 'ml', 'advanced analytics', 'cnn', 'data analytics', 'pyspark', 'microsoft azure', 'power bi', 'machine learning', 'sql', 'r', 'tableau', 'git', 'rnn', 'gcp', 'machine learning algorithms', 'aws', 'consumer goods', 'statistics']",2025-06-13 06:15:16
S&C GN - Data&AI - CMT Eng - Consultant,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Title - S&C Global Network - AI - CMT AI ML Consultant\n\n\n\nManagement Level:9- Consultant\n\n\n\nLocation:Open\n\n\n\nMust-have skills:Gen AI ML\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nAccenture has committed to invest USD 3Billion into GenAI in the next 3 years. We will continually invest in your learning and growth. You'll work with Accentures highly skilled and experienced practitioners, and Accenture will support you in growing your own career path and interests.\nYoull be part of a diverse, vibrant, global Accenture Data and AI community, continually pushing the boundaries of business capabilities.\n\n\n\nWhat you would do in this role\n\n\n\nML Maturity Assessment:\nConduct comprehensive assessments of the organization's ML maturity, identifying strengths, weaknesses, and areas for improvement.\nProvide strategic recommendations to enhance the overall ML capability and align it with business objectives.\n\n\n\nML Ops Roadmap & Processes:\nDevelop ML Ops roadmaps and establish robust processes for the end-to-end machine learning lifecycle, including data preparation, model training, deployment, and monitoring.\nImplement best practices in ML Ops to ensure efficiency, scalability, and reliability of ML systems.\n\n\n\nImplementation of Gen AI Solutions:\nLead the design and implementation of state-of-the-art Generative AI solutions, leveraging deep learning frameworks such as TensorFlow and PyTorch.\nDrive innovation in Gen AI, staying abreast of the latest advancements and incorporating cutting-edge technologies into solutions.\n\n\n\nIncubating and Designing:\nProactively identify opportunities for ML and / or Gen AI applications within the organization.\nWork closely with cross-functional teams to incubate and design bespoke ML solutions tailored to business requirements.\n\n\n\nTechnical Leadership:\nProvide technical leadership and mentorship to data scientists, engineers, and other team members.\nCollaborate with stakeholders to ensure alignment between technical solutions and business objectives.\n\n\n\nCollaboration and Communication:\nCollaborate with business stakeholders to understand their needs and translate them into ML and Gen AI requirements.\nEffectively communicate complex technical concepts to non-technical audiences.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nProven expertise in conducting ML maturity assessments and developing ML Ops roadmaps.\nHands-on experience in operationalizing the Machine learning system on a cloud and / or an On-Prem platform.\nExperience in implementing Generative AI solutions, including incubation, design, and deployment will be a big plus.\nProficiency in deep learning frameworks such as TensorFlow and PyTorch.\nGood knowledge of ML Ops best practices and processes.\nExcellent problem-solving skills and ability to design scalable and reliable ML architectures.\nStrong leadership and communication skills, with a track record of leading successful ML initiatives.\nExperience in Telecom or Hi Tech or Software and platform industry desirable\nTools & Techniques\nTensorFlow, PyTorch, Scikit-learn, Keras\nNumPy, Pandas\nMatplotlib, Seaborn\nTensorFlow Serving, Docker and Kubernetes\nGood software engineering practices, including code modularization, documentation, and testing.\nExperience with open API , Integration architecture , microservices\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:4+ years\n\n\n\n\nEducational Qualification:Btech/ BE",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['deep learning frameworks', 'machine learning', 'assessment', 'gen', 'ml', 'kubernetes', 'hi', 'python', 'scikit-learn', 'ai solutions', 'numpy', 'integration architecture', 'docker', 'microservices', 'pandas', 'deep learning', 'tensorflow', 'seaborn', 'matplotlib', 'open api', 'pytorch', 'keras', 'telecom']",2025-06-13 06:15:18
S&C GN - Data&AI - Life Sciences - Analyst,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Management Level:Ind & Func AI Decision Science Analyst\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nLife Sciences/Pharma/Healthcare projects and delivering successful outcomes, commercial, clinical, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProficiency in Programming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI\n\n\n\nJob\n\n\nSummary\n\nWe are seeking an experienced and visionary - Accenture S&C Global Network - Data & AI practice help our clients grow their business in entirely new ways. Analytics enables our clients to achieve high performance through insights from data - insights that inform better decisions and strengthen customer relationships. From strategy to execution, Accenture works with organizations to develop analytic capabilities - from accessing and reporting on data to predictive modelling - to outperform the competition.\n\n\n\nKey Responsibilities\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nWork on variety of projects in Data Modeling, Data Engineering, Data Visualization, Data Science etc.,\nAcquire new skills that have utility across industry groups.\n\n\n\n\n\nAdditional Information\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\n\nQualification\n\n\n\nExperience:Proven experience (2+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'presentation skills', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 06:15:19
S&C Global Network - AI - Data Science,Accenture,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nS&C Global Network - AI - Data Science\n\n\n\nManagement Level:\n\n\n\n11-Analyst\n\n\n\nLocation:\n\n\n\nBengaluru, BDC7C\n\n\n\nMust-have skills:Data Science\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\nAs part of our Data & AI practice, you will join a worldwide network of smart and driven colleagues experienced in leading AI/ML/Statistical tools, methods and applications. From data to analytics and insights to actions, our forward-thinking consultants provide analytically-informed, issue-based insights at scale to help our clients improve outcomes and achieve high performance.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\nWhat you would do in this role:\nIdentifying, assessing, and solving complex business problems for area of responsibility, where analysis of situations or data requires an in-depth evaluation of variable factors.\nManage large databases proficiently, conducting exploratory data analytics to derive valuable insights for stakeholders\nDesign and develop machine learning models using Python, with proficiency in NLP and Computer Vision techniques.\nDefining how our clients can leverage new technologies for greatest strategic impact in the digital world that supports business requirements and reduces costs.\nSolving key business problems and challenges by painting a picture of, and charting a journey from the current state to a to-be enterprise environment\nDeveloping detailed and actionable business cases and plans which reflect our practices deep industry, Digital and business process acumen to leverage technology for greatest strategic impact.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:\n\n\n\n3-5Years\n\n\n\n\nEducational Qualification:\n\n\n\nAny Degree",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'natural language processing', 'data science', 'management skills', 'computer vision', 'strategic initiatives', 'business process optimization', 'stakeholder management', 'business transformation', 'artificial intelligence', 'strategic advisory services']",2025-06-13 06:15:21
S&C Global Network - AI - CG&S - Consultant Data Science,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: Industry & Function AI Decision Science Consultant S&C Global Network\n\n\n\nManagement Level: 9 Consultant\n\n\n\nLocation: Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nData Science, AI, ML, Experience with cloud platforms such as AWS, Azure, or Google Cloud, Hands-on experience in programming languages like Python, R, PySpark, and SQL\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nDeep Learning Techniques (e.g. RNN, CNN), Visualization tools like Power BI and Tableau, Exposure to tools like ChatGPT, Llama 2, Hugging Face, etc.\n\n\n\nJob\n\n\nSummary:\n\nAs an Industry & Function AI Decision Science Consultant, you will leverage your expertise in data science and Consumer Goods domain knowledge to design and deliver AI-driven solutions. Your role will include strategic analysis, project delivery, solution development, and technical execution to empower businesses with actionable insights and enable automated and augmented decision-making.\n\n\n\n\nRoles & Responsibilities:\nConduct strategic analysis of the AI, analytics, and data maturity landscape for clients in the Consumer Goods domain\nLead data science engagements, manage delivery teams, and build innovative AI capabilities\nDevelop and implement advanced analytics solutions tailored to client requirements\nUtilize languages like Python, PySpark, R, and SQL for data wrangling and machine learning model development\nLeverage cloud technologies (Azure, AWS, GCP) to integrate and implement AI solutions\nTranslate complex data into compelling narratives for effective data storytelling\nMentor junior team members and contribute to thought leadership\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in Python, R, PySpark, and SQL\nStrong knowledge of traditional statistical methods, machine learning techniques, and deep learning\nHands-on experience in Consumer Goods & Services domain\nCloud integration skills with platforms like AWS, Azure, or Google Cloud\nExperience with optimization techniques (exact and evolutionary)\nCertifications like AWS Certified Data Analytics Specialty or Google Professional Data Engineer\nFamiliarity with visualization tools like Tableau and Power BI\nExposure to large language models (e.g., ChatGPT, Llama 2)\nFamiliarity with version control systems like Git.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate will have a strong educational background in data science, computer science, or a related field, along with a proven track record of delivering impactful AI-driven solutions in the Consumer Goods industry.\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 4-8 years of hands-on experience in data science with a focus on the Consumer Goods industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or equivalent degree with Data Science specialization (from a premier institute)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'artificial intelligence', 'deep learning', 'data science', 'ml', 'advanced analytics', 'cnn', 'data analytics', 'pyspark', 'microsoft azure', 'power bi', 'machine learning', 'sql', 'r', 'tableau', 'git', 'rnn', 'gcp', 'machine learning algorithms', 'aws', 'consumer goods', 'statistics']",2025-06-13 06:15:22
S&C Global Network - AI - CG&S - Consultant Data Science,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: Industry & Function AI Decision Science Consultant S&C Global Network\n\n\n\nManagement Level: 9 Consultant\n\n\n\nLocation: Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nData Science, AI, ML, Experience with cloud platforms such as AWS, Azure, or Google Cloud, Hands-on experience in programming languages like Python, R, PySpark, and SQL\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nDeep Learning Techniques (e.g. RNN, CNN), Visualization tools like Power BI and Tableau, Exposure to tools like ChatGPT, Llama 2, Hugging Face, etc.\n\n\n\nJob\n\n\nSummary:\n\nAs an Industry & Function AI Decision Science Consultant, you will leverage your expertise in data science and Consumer Goods domain knowledge to design and deliver AI-driven solutions. Your role will include strategic analysis, project delivery, solution development, and technical execution to empower businesses with actionable insights and enable automated and augmented decision-making.\n\n\n\n\nRoles & Responsibilities:\nConduct strategic analysis of the AI, analytics, and data maturity landscape for clients in the Consumer Goods domain\nLead data science engagements, manage delivery teams, and build innovative AI capabilities\nDevelop and implement advanced analytics solutions tailored to client requirements\nUtilize languages like Python, PySpark, R, and SQL for data wrangling and machine learning model development\nLeverage cloud technologies (Azure, AWS, GCP) to integrate and implement AI solutions\nTranslate complex data into compelling narratives for effective data storytelling\nMentor junior team members and contribute to thought leadership\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in Python, R, PySpark, and SQL\nStrong knowledge of traditional statistical methods, machine learning techniques, and deep learning\nHands-on experience in Consumer Goods & Services domain\nCloud integration skills with platforms like AWS, Azure, or Google Cloud\nExperience with optimization techniques (exact and evolutionary)\nCertifications like AWS Certified Data Analytics Specialty or Google Professional Data Engineer\nFamiliarity with visualization tools like Tableau and Power BI\nExposure to large language models (e.g., ChatGPT, Llama 2)\nFamiliarity with version control systems like Git.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate will have a strong educational background in data science, computer science, or a related field, along with a proven track record of delivering impactful AI-driven solutions in the Consumer Goods industry.\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 4-8 years of hands-on experience in data science with a focus on the Consumer Goods industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or equivalent degree with Data Science specialization (from a premier institute)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'artificial intelligence', 'deep learning', 'data science', 'ml', 'advanced analytics', 'cnn', 'data analytics', 'pyspark', 'microsoft azure', 'power bi', 'machine learning', 'sql', 'r', 'tableau', 'git', 'rnn', 'gcp', 'machine learning algorithms', 'aws', 'consumer goods', 'statistics']",2025-06-13 06:15:24
Data Science,Global Banking Organization,5 - 10 years,Not Disclosed,['Bengaluru'],"Key Skills: Machine Learning, Data Science, Azure, Python, Hadoop.\nRoles and Responsibilities:\nStrong understanding of Math, Statistics, and the theoretical foundations of Statistical & Machine Learning, including Parametric and Non-parametric models.\nApply advanced data mining techniques to curate, process, and transform raw data into reliable datasets.\nUse various statistical techniques and ML methods to perform predictive modeling/classification for problems related to clients, distribution, sales, client profiles, and segmentation, and provide actionable insights for business decision-making.\nDemonstrate expertise in the full Machine Learning lifecycle--feature engineering, training, validation, scaling, deployment, scoring, monitoring, and feedback loops.\nProficiency in Python visualization libraries such as matplotlib and seaborn.\nExperience with cloud computing infrastructure like Azure, including Machine Learning Studio, Azure Data Factory, Synapse, Python, and PySpark.\nAbility to develop, test, and deploy models on cloud/web platforms.\nExcellent knowledge of Deep Learning Architectures, including Convolutional Neural Networks and Transformer/LLM Foundation Models.\nStrong expertise in supervised and adversarial learning techniques.\nRobust working knowledge of deep learning frameworks such as TensorFlow, Keras, and PyTorch.\nExcellent Python coding skills.\nExperience with version control tools (Git, GitHub/GitLab) and data version control.\nExperience in end-to-end model deployment and productionization.\nDemonstrated proficiency in deploying, scaling, and optimizing ML models in production environments with low latency, high availability, and cost efficiency.\nSkilled in model interpretability and CI/CD for ML using tools like MLflow and Kubeflow, with the ability to implement automated monitoring, logging, and retraining strategies.\nExperience Requirement:\n5-12 years of experience in designing and deploying deep learning and machine learning solutions.\nProven track record of delivering AI/ML solutions in real-world business applications at scale.\nHands-on experience working in cross-functional teams including data engineers, product managers, and business stakeholders.\nExperience mentoring junior data scientists and providing technical leadership within a data science team.\nExperience working with big data tools and environments such as Hadoop, Spark, or Databricks is a plus.\nPrior experience in managing model lifecycle in enterprise production environments including drift detection and retraining pipelines.\nEducation: B.Tech.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure', 'Hadoop.', 'Machine Learning', 'Python']",2025-06-13 06:15:26
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-13 06:15:27
S&C GN - Data&AI - CMT Eng - Consultant,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Title - S&C Global Network - AI - CMT DE- Consultant\n\n\n\nManagement Level:9- Consultant\n\n\n\nLocation:Open\n\n\n\nMust-have skills:Data Engineering\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nWe are looking for a passionate and results-driven\n\n\n\nData Engineerto join our growing data team. You will be responsible for designing, building, and maintaining scalable data pipelines and infrastructure that support data-driven decision-making across the organization.\n\n\n\n\nRoles & Responsibilities:\n\nDesign, build, and maintain robust, scalable, and efficient data pipelines (ETL/ELT).\nWork with structured and unstructured data across a wide variety of data sources.\nCollaborate with data analysts, data scientists, and business stakeholders to understand data requirements.\nOptimize data systems and architecture for performance, scalability, and reliability.\nMonitor data quality and support initiatives to ensure clean, accurate, and consistent data.\nDevelop and maintain data models and metadata.\nImplement and maintain best practices in data governance, security, and compliance.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n2+ years in data engineering or related fields\nProficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL).\nStrong programming skills in Python, Scala, or Java.\nExperience with big data technologies such as Spark, Hadoop, or Hive.\nFamiliarity with cloud platforms like AWS, Azure, or GCP, especially services like S3, Redshift, BigQuery, or Azure Data Lake.\nExperience with orchestration tools like Airflow, Luigi, or similar.\nSolid understanding of data warehousing concepts and data modeling techniques.\nGood problem-solving skills and attention to detail.\nExperience with modern data stack tools like dbt, Snowflake, or Databricks.\nKnowledge of CI/CD pipelines and version control (e.g., Git).\nExposure to containerization (Docker, Kubernetes) and infrastructure as code (Terraform, CloudFormation).\n\n\n\n\nAdditional Information: - The ideal candidate will possess a strong educational background in quantitative discipline and experience in working with Hi-Tech clients\n\n- This position is based at our Bengaluru (preferred) and other AI Accenture locations.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:4+ years\n\n\n\n\nEducational Qualification:Btech/ BE",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'scala', 'data engineering', 'sql', 'java', 'hive', 'continuous integration', 'kubernetes', 'snowflake', 'amazon redshift', 'airflow', 'microsoft azure', 'ci/cd', 'aws cloudformation', 'docker', 'data bricks', 'data modeling', 'spark', 'gcp', 'data warehousing concepts', 'terraform', 'hadoop', 'aws']",2025-06-13 06:15:29
"Data Eng, Mgmt & Governance Sr Analyst",Accenture,5 - 8 years,Not Disclosed,['Bengaluru'],"Skill required: Data Management - Microsoft Fabric\n\n\n\n\nDesignation: Data Eng, Mgmt & Governance Sr Analyst\n\n\n\n\nQualifications:BE,BTech\n\n\n\n\nYears of Experience:5 - 8 Years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nData & AIEnd to end, unified analytics platform that brings together existing offerings like Data Factory, Synapse, and Power BI into a single unified product for all your data and analytics workloads.\n\n\n\n\nWhat are we looking for\nMicrosoft Fabric Microsoft Azure PySpark Strong analytical skills Adaptable and flexible Problem-solving skills Agility for quick learning Ability to meet deadlines\n\n\n\nRoles and Responsibilities: In this role you are required to do analysis and solving of moderately complex problems May create new solutions, leveraging and, where needed, adapting existing methods and procedures The person would require understanding of the strategic direction set by senior management as it relates to team goals Primary upward interaction is with direct supervisor May interact with peers and/or management levels at a client and/or within Accenture Guidance would be provided when determining methods and procedures on new assignments Decisions made by you will often impact the team in which they reside Individual would manage small teams and/or work efforts (if in an individual contributor role) at a client or within Accenture\n\nQualification\n\nBE,BTech",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data management', 'data analysis', 'pyspark', 'microsoft azure', 'power bi', 'python', 'data analytics', 'natural language processing', 'bi', 'data warehousing', 'machine learning', 'business intelligence', 'sql', 'tableau', 'r', 'data science', 'data modeling', 'data visualization', 'etl', 'ssis']",2025-06-13 06:15:31
Data Governance Practitioner,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Data Governance Practitioner\n\n\n\n\n\nProject Role Description :Establish and enforce data governance policies to ensure the accuracy, integrity, and security of organizational data. Collaborate with key stakeholders to define data standards, facilitate effective data collection, storage, access, and usage; and drive data stewardship initiatives for comprehensive and effective data governance.\n\n\n\nMust have skills :Snowflake Data Warehouse\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Governance Practitioner, you will establish and enforce data governance policies to ensure the accuracy, integrity, and security of organizational data. Your typical day will involve collaborating with key stakeholders to define data standards, facilitating effective data collection, storage, access, and usage, and driving data stewardship initiatives for comprehensive and effective data governance. You will engage in discussions that shape the data governance framework and contribute to the overall data strategy of the organization.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist in the development and implementation of data governance frameworks and policies.- Monitor compliance with data governance policies and report on data quality metrics.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Snowflake Data Warehouse.- Strong understanding of data governance principles and best practices.- Experience with data quality assessment and improvement techniques.- Familiarity with data management tools and technologies.- Ability to communicate complex data concepts to non-technical stakeholders.\nAdditional Information:- The candidate should have minimum 3 years of experience in Snowflake Data Warehouse.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'data management', 'warehouse', 'sql', 'data governance', 'hive', 'python', 'data analysis', 'data analytics', 'data warehousing', 'business analytics', 'machine learning', 'business intelligence', 'tableau', 'data science', 'data modeling', 'hadoop', 'sqoop', 'etl', 'informatica']",2025-06-13 06:15:32
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Indore'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 06:15:34
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Pune'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 06:15:36
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,"['Mumbai', 'Any Location']","We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 06:15:37
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Nagpur'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 06:15:39
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Ahmedabad'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 06:15:40
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Delhi / NCR'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 06:15:42
Lead Data Engineer,Moreyeahs,7 - 9 years,Not Disclosed,['Indore'],"We are seeking a highly skilled and experienced Lead Data Engineer (7+ years) to join our dynamic team. As a Lead Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure. You will be responsible for ensuring the efficient and reliable collection, storage, and transformation of large-scale data to support business intelligence, analytics, and data-driven decision-making.\n\nKey Responsibilities :\n\nData Architecture & Design :\n- Lead the design and implementation of robust data architectures that support data warehousing (DWH), data integration, and analytics platforms.\n- Develop and maintain ETL (Extract, Transform, Load) pipelines to ensure the efficient processing of large datasets.\n\nETL Development :\n- Design, develop, and optimize ETL processes using tools like Informatica Power Center, Intelligent Data Management Cloud (IDMC), or custom Python scripts.\n- Implement data transformation and cleansing processes to ensure data quality and consistency across the enterprise.\n\nData Warehouse Development :\n- Build and maintain scalable data warehouse solutions using Snowflake, Databricks, Redshift, or similar technologies.\n\n- Ensure efficient storage, retrieval, and processing of structured and semi-structured data.\n\nBig Data & Cloud Technologies :\n- Utilize AWS Glue and PySpark for large-scale data processing and transformation.\n- Implement and manage data pipelines using Apache Airflow for orchestration and scheduling.\n- Leverage cloud platforms (AWS, Azure, GCP) for data storage, processing, and analytics.\n\nData Management & Governance :\n- Establish and enforce data governance and security best practices.\n- Ensure data integrity, accuracy, and availability across all data platforms.\n- Implement monitoring and alerting systems to ensure data pipeline reliability.\n\nCollaboration & Leadership :\n\n- Work closely with data Stewards, analysts, and business stakeholders to understand data requirements and deliver solutions that meet business needs.\n- Mentor and guide junior data engineers, fostering a culture of continuous learning and development within the team.\n- Lead data-related projects from inception to delivery, ensuring alignment with business objectives and timelines.\n\nDatabase Management :\n- Design and manage relational databases (RDBMS) to support transactional and analytical workloads.\n- Optimize SQL queries for performance and scalability across various database platforms.\n\nRequired Skills & Qualifications :\nEducation: Bachelors or Masters degree in Computer Science, Information Systems, Engineering, or a related field.\n\nExperience :\n- Minimum of 7+ years of experience in data engineering, ETL, and data warehouse development.\n- Proven experience with ETL tools like Informatica Power Center or IDMC.\n- Strong proficiency in Python and PySpark for data processing.\n- Experience with cloud-based data platforms such as AWS Glue, Snowflake, Databricks, or Redshift.\n- Hands-on experience with SQL and RDBMS platforms (e.g., Oracle, MySQL, PostgreSQL).\n- Familiarity with data orchestration tools like Apache Airflow.\nTechnical Skills :\n- Advanced knowledge of data warehousing concepts and best practices.\n- Strong understanding of data modeling, schema design, and data governance.\n- Proficiency in designing and implementing scalable ETL pipelines.\n- Experience with cloud infrastructure (AWS, Azure, GCP) for data storage and processing.\n\nSoft Skills :\n- Excellent communication and collaboration skills.\n- Ability to lead and mentor a team of engineers.\n- Strong problem-solving and analytical thinking abilities.\n- Ability to manage multiple projects and prioritize tasks effectively.\n\nPreferred Qualifications :\n- Experience with machine learning workflows and data science tools.\n- Certification in AWS, Snowflake, Databricks, or relevant data engineering technologies.\n- Experience with Agile methodologies and DevOps practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data modeling', 'RDBMS', 'MySQL', 'Agile', 'Informatica', 'Oracle', 'Apache', 'Business intelligence', 'Analytics', 'Python']",2025-06-13 06:15:43
Azure Senior Data Engineers,IT Services & Consulting,5 - 9 years,14-24 Lacs P.A.,['Bengaluru'],"Job Title: Senior Data Engineer Azure\nLocation: Bengaluru\nExperience: 6+ years (3+ years on Azure data services preferred)\nDepartment: Data Engineering / IT\nJob Summary:\nWe are seeking a highly skilled Senior Data Engineer with expertise in Microsoft Azure to design, develop, and optimize data pipelines, data lakes, and warehouse solutions. The ideal candidate will play a key role in building scalable and secure data platforms to support business intelligence, analytics, and machine learning use cases.\nKey Responsibilities:\nDesign, build, and maintain scalable data pipelines using Azure Data Factory, Databricks, Synapse Analytics, and related tools.\nDevelop and optimize ETL/ELT processes for structured and unstructured data.\nImplement data lake and data warehouse solutions following best practices and security standards.\nCollaborate with data scientists, analysts, and business stakeholders to understand data requirements.\nEnsure data quality, lineage, and governance using tools like Purview, Azure Monitor, and Data Catalog.\nMonitor and troubleshoot performance issues across data flows and batch processing pipelines.\nSupport real-time data integration and streaming solutions using Azure Event Hubs, Stream Analytics, or Kafka.\nMaintain and enhance CI/CD pipelines for data solutions using Azure DevOps or GitHub Actions.\nLead and mentor junior engineers in best practices for Azure data engineering.\nRequired Skills & Qualifications:\nBachelor’s or Master’s degree in Computer Science, Engineering, or related field.\n6+ years of experience in data engineering roles, with 3+ years working with Azure data services.\nProficiency in SQL, Python or Scala.\nExperience with tools like Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL Database, and Azure Blob Storage.\nStrong understanding of data modeling, data warehousing, and data lake architecture.\nFamiliarity with DevOps practices, infrastructure-as-code (IaC) tools (ARM, Bicep, Terraform), and CI/CD pipelines.\nKnowledge of data governance and data security best practices on cloud platforms.\nExcellent communication and documentation skills.\nPreferred Qualifications:\nAzure certification (e.g., Azure Data Engineer Associate, Azure Solutions Architect, etc.)\nExperience with big data frameworks (e.g., Spark, Hadoop).\nKnowledge of machine learning pipelines or MLOps in Azure.\nExperience with Power BI or integration with other visualization tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Senior Data Engineers', 'Azure Data Factory', 'azure']",2025-06-13 06:15:44
Senior Data Engineer -Bangalore,Happiest Minds Technologies,6 - 10 years,Not Disclosed,['Bengaluru'],"Job Overview:\nThe primary purpose of this role is to translate business requirements and functional specifications into logical program designs and to deliver dashboards, schema, data pipelines, and software solutions. This includes developing, configuring, or modifying data components within various complex business and/or enterprise application solutions in various computing environments. You will partner closely with multiple Business partners, Product Owners, Data Strategy, Data Platform, Data Science and Machine Learning (MLOps) teams to drive innovative data products for end users. Additionally, you will help shape overall solution & data products, develop scalable solutions through best-in-class engineering practices.",,,,"['NoSQL', 'big data systems', 'Data Pipeline', 'MongoDB', 'SQL', 'Hive', 'GIT', 'Hadoop', 'Kafka', 'Agile', 'MQL', 'Ci/Cd']",2025-06-13 06:15:46
Azure Data Engineer ( Azure Databricks),Apex One,4 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Job Summary\nWe are seeking a skilled Azure Data Engineer with 4 years of overall experience, including at least 2 years of hands-on experience with Azure Databricks (Must). The ideal candidate will have strong expertise in building and maintaining scalable data pipelines and working across cloud-based data platforms.\nKey Responsibilities\nDesign, develop, and optimize large-scale data pipelines using Azure Data Factory, Azure Databricks, and Azure Synapse.\nImplement data lake solutions and work with structured and unstructured datasets in Azure Data Lake Storage (ADLS).\nCollaborate with data scientists, analysts, and engineering teams to design and deliver end-to-end data solutions.\nDevelop ETL/ELT processes and integrate data from multiple sources.\nMonitor, debug, and optimize workflows for performance and cost-efficiency.\nEnsure data governance, quality, and security best practices are maintained.\nMust-Have Skills\n4+ years of total experience in data engineering.\n2+ years of experience with Azure Databricks (PySpark, Notebooks, Delta Lake).\nStrong experience with Azure Data Factory, Azure SQL, and ADLS.\nProficient in writing SQL queries and Python/Scala scripting.\nUnderstanding of CI/CD pipelines and version control systems (e.g., Git).\nSolid grasp of data modeling and warehousing concepts.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Azure Data Factory', 'SQL queries', 'PySpark', 'Delta Lake', 'Azure Databricks', 'Notebooks', 'Azure SQL']",2025-06-13 06:15:48
Data Engineer,Talent Aspire,2 - 7 years,Not Disclosed,"['Chandigarh', 'Bengaluru']","As the Data Engineer, you will play a pivotal role in shaping our data infrastructure and\nexecuting against our strategy. You will ideate alongside engineering, data and our clients to\ndeploy data products with an innovative and meaningful impact to clients. You will design, build, and maintain scalable data pipelines and workflows on AWS. Additionally, your expertise in AI and machine learning will enhance our ability to deliver smarter, more predictive solutions.\n\nKey Responsibilities\nCollaborate with other engineers, customers to brainstorm and develop impactful data\nproducts tailored to our clients.\nLeverage AI and machine learning techniques to integrate intelligent features into our\nofferings.\nDevelop, and optimize end-to-end data pipelines on AWS\nFollow best practices in software architecture and development.\nImplement effective cost management and performance optimization strategies.\nDevelop and maintain systems using Python, SQL, PySpark, and Django for front-end\ndevelopment.\nWork directly with clients and end-users and address their data needs\nUtilize databases and tools including and not limited to, Postgres, Redshift, Airflow, and\nMongoDB to support our data ecosystem.\nLeverage AI frameworks and libraries to integrate advanced analytics into our solutions.\nQualifications\n\nExperience:\nMinimum of 3 years of experience in data engineering, software development, or\nrelated roles.\nProven track record in designing and deploying AWS cloud infrastructure\nsolutions\nAt least 2 years in data analysis and mining techniques to aid in descriptive and\ndiagnostic insights\nExtensive hands-on experience with Postgres, Redshift, Airflow, MongoDB, and\nreal-time data workflows.\n\nTechnical Skills:\nExpertise in Python, SQL, and PySpark\nStrong background in software architecture and scalable development practices.\nTableau, Metabase or similar viz tools experience\nWorking knowledge of AI frameworks and libraries is a plus.\nLeadership & Communication:\nDemonstrates ownership and accountability for delivery with a strong\ncommitment to quality.\nExcellent communication skills with a history of effective client and end-user\nengagement.\nStartup & Fintech Mindset:\nAdaptability and agility to thrive in a fast-paced, early-stage startup environment.\nPassion for fintech innovation and a strong desire to make a meaningful impact\non the future of finance.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'performance optimization strategies', 'PySpark', 'Django', 'cost management', 'AWS', 'AI frameworks', 'Python', 'SQL']",2025-06-13 06:15:49
Data Engineer I,Swiss Re,1 - 3 years,Not Disclosed,['Bengaluru'],"About the Role:\nAs a Data Engineer, you will be responsible for implementing data pipelines and analytics\nsolutions to support key decision-making processes in our Life Health Reinsurance business. You will become part of a project that is leveraging cutting edge technology that applies Big Data and Machine Learning to solve new and emerging problems for Swiss Re. You will be expected to gain a full understanding of the reinsurance data and business logic required to deliver analytics solutions.\nKey responsibilities include:\nWork closely with Product Owners and Engineering Leads to understand requirements and evaluate the implementation effort.\nDevelop and maintain scalable data transformation pipelines\nImplement analytics models and visualizations to provide actionable data insights\nCollaborate within a global development team to design and deliver solutions.\nAbout the Team:\nLife Health Data Analytics Engineering is a key tech partner for our Life Health Reinsurance division, supporting in the transformation of the data landscape and the creation of innovative analytical products and capabilities. A large globally distributed team working in an agile development landscape, we deliver solutions to make better use of our reinsurance data and enhance our ability to make data-driven decisions across the business value chain.\nAbout You:\nAre you eager to disrupt the industry with us and make an impactDo you wish to have your talent recognized and rewardedThen join our growing team and become part of the next wave of data\ninnovation. Key qualifications include:\nBachelors degree level or equivalent in Computer Science, Data Science or similar discipline\nAt least 1-3 years of experience working with large scale software systems\nProficient in Python/PySpark\nProficient in SQL (Spark SQL preferred)\nPalantir Foundry experience is a strong plus.\nExperience working with large data sets on enterprise data platforms and distributed computing (Spark/Hive/Hadoop preferred)\nExperience with JavaScript/HTML/CSS a plus\nExperience working in a Cloud environment such as AWS or Azure is a plus\nStrong analytical and problem-solving skills\nEnthusiasm to work in a global and multicultural environment of internal and external professionals\nStrong interpersonal and communication skills, demonstrating a clear and articulate standard of written and verbal communication in complex environments\nAbout Swiss Re\n.\n\n\n\nIf you are an experienced professional returning to the workforce after a career break, we encourage you to apply for open positions that match your skills and experience.\nKeywords:\nReference Code: 134085",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Reinsurance', 'data science', 'spark', 'Analytical', 'Machine learning', 'Javascript', 'HTML', 'SQL', 'Python']",2025-06-13 06:15:51
Senior Data Engineer- (Big data and Data Pipelines),Findem,5 - 9 years,Not Disclosed,"['New Delhi', 'Bengaluru']","What is Findem:\n\nFindem is the only talent data platform that combines 3D data with AI. It automates and consolidates top-of-funnel activities across your entire talent ecosystem, bringing together sourcing, CRM, and analytics into one place. Only 3D data connects people and company data over time - making an individual s entire career instantly accessible in a single click, removing the guesswork, and unlocking insights about the market and your competition no one else can. Powered by 3D data, Findem s automated workflows across the talent lifecycle are the ultimate competitive advantage. Enabling talent teams to deliver continuous pipelines of top, diverse candidates while creating better talent experiences, Findem transforms the way companies plan, hire, and manage talent. Learn more at www.findem.ai\n\nExperience - 5 - 9 years\n\nWe are looking for an experienced Big Data Engineer, who will be responsible for building, deploying and managing various data pipelines, data lake and Big data processing solutions using Big data and ETL technologies.\n\nLocation- Delhi, India\nHybrid- 3 days onsite\nResponsibilities\nBuild data pipelines, Big data processing solutions and data lake infrastructure using various Big data and ETL technologies\nAssemble and process large, complex data sets that meet functional non-functional business requirements ETL from a wide variety of sources like MongoDB, S3, Server-to-Server, Kafka etc., and processing using SQL and big data technologies\nBuild analytical tools to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics Build interactive and ad-hoc query self-serve tools for analytics use cases\nBuild data models and data schema for performance, scalability and functional requirement perspective Build processes supporting data transformation, metadata, dependency and workflow management\nResearch, experiment and prototype new tools/technologies and make them successful\nSkill Requirements\nMust have-Strong in Python/Scala\nMust have experience in Big data technologies like Spark, Hadoop, Athena / Presto, Redshift, Kafka etc\nExperience in various file formats like parquet, JSON, Avro, orc etc\nExperience in workflow management tools like airflow Experience with batch processing, streaming and message queues\nAny of visualization tools like Redash, Tableau, Kibana etc\nExperience in working with structured and unstructured data sets\nStrong problem solving skills\nGood to have\nExposure to NoSQL like MongoDB\nExposure to Cloud platforms like AWS, GCP, etc\nExposure to Microservices architecture\nExposure to Machine learning techniques\nThe role is full-time and comes with full benefits. We are globally headquartered in the San Francisco Bay Area with our India headquarters in Bengaluru.\n\nEqual Opportunity",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAN', 'metadata', 'Prototype', 'Machine learning', 'Schema', 'JSON', 'Analytics', 'SQL', 'CRM', 'Python']",2025-06-13 06:15:52
"Data Engineer, Product Analytics",Meta,2 - 7 years,Not Disclosed,['Bengaluru'],"Apply to this job\nAs a Data Engineer at Meta, you will shape the future of people-facing and business-facing products we build across our entire family of applications (Facebook, Instagram, Messenger, WhatsApp, Reality Labs, Threads). Your technical skills and analytical mindset will be utilized designing and building some of the worlds most extensive data sets, helping to craft experiences for billions of people and hundreds of millions of businesses worldwide.In this role, you will collaborate with software engineering, data science, and product management teams to design/build scalable data solutions across Meta to optimize growth, strategy, and user experience for our 3 billion plus users, as well as our internal employee community.You will be at the forefront of identifying and solving some of the most interesting data challenges at a scale few companies can match. By joining Meta, you will become part of a world-class data engineering community dedicated to skill development and career growth in data engineering and beyond.Data Engineering: You will guide teams by building optimal data artifacts (including datasets and visualizations) to address key questions. You will refine our systems, design logging solutions, and create scalable data models. Ensuring data security and quality, and with a focus on efficiency, you will suggest architecture and development approaches and data management standards to address complex analytical problems.Product leadership: You will use data to shape product development, identify new opportunities, and tackle upcoming challenges. Youll ensure our products add value for users and businesses, by prioritizing projects, and driving innovative solutions to respond to challenges or opportunities.Communication and influence: You wont simply present data, but tell data-driven stories. You will convince and influence your partners using clear insights and recommendations. You will build credibility through structure and clarity, and be a trusted strategic partner.\nData Engineer, Product Analytics Responsibilities\nCollaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way\nDesign, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains\nDefine and manage Service Level Agreements for all data sets in allocated areas of ownership\nSolve challenging data integration problems, utilizing optimal Extract, Transform, Load (ETL) patterns, frameworks, query techniques, sourcing from structured and unstructured data sources\nImprove logging\nAssist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts\nOptimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts\nInfluence product and cross-functional teams to identify data opportunities to drive impact\nMinimum Qualifications\nBachelors degree in Computer Science, Computer Engineering, relevant technical field, or equivalent\n2+ years of experience where the primary responsibility involves working with data. This could include roles such as data analyst, data scientist, data engineer, or similar positions\n2+ years of experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala or others.)\nPreferred Qualifications\nMasters or Ph.D degree in a STEM field\nAbout Meta\n.\n\n\nEqual Employment Opportunity\n.\n\nMeta is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, fill out the Accommodations request form .",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'C++', 'Data management', 'Data modeling', 'data security', 'Analytical', 'Analytics', 'SQL', 'Python']",2025-06-13 06:15:54
Data Engineer - GCP,Happiest Minds Technologies,5 - 10 years,8-18 Lacs P.A.,"['Pune', 'Bengaluru']","Key Responsibilities:\n• Data Pipeline Development: Designing, building, and maintaining robust data pipelines to move data from various sources (e.g., databases, external APIs, logs) to centralized data systems, such as data lakes or warehouses.\n• Data Integration: Integrating data from multiple sources and ensuring it's processed in a consistent, usable format. This involves transforming, cleaning, and validating data to meet the needs of products, analysts and data scientists.\n• Database Management: Creating, managing, and optimizing databases for storing large amounts of structured and unstructured data. Ensuring high availability, scalability, and security of data storage solutions.\nIdentifying and resolving issues related to the speed and efficiency of data systems. This could include optimizing queries, storage systems, and improving overall system architecture.\n• : Automating routine tasks, such as data extraction, transformation, and loading (ETL), to ensure smooth data flows with minimal manual intervention.\n• : Working closely with Work closely with product managers, UX/UI designers, and other stakeholders to understand data requirements and ensure data is in the right format for analysis and modeling.\n• : Ensuring data integrity and compliance with data governance policies, including data quality standards, privacy regulations (e.g., GDPR), and security protocols.\n: Continuously monitoring data pipelines and databases for any disruptions or errors and troubleshooting any issues that arise to ensure continuous data flow.\n• : Staying up to date with emerging data tools, technologies, and best practices in order to improve data systems and infrastructure.\n• : Documenting data systems, pipeline processes, and data architectures, providing clear instructions for the team to follow, and ensuring that the architecture is understandable for stakeholders.",,,,"['Data Engineering', 'gcp', 'Python', 'SQL', 'Pyspark', 'Bigquery', 'Google Cloud Platforms']",2025-06-13 06:15:55
Data Engineer - Databricks,KPI Partners,3 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About KPI Partners.\nKPI Partners is a leading provider of data analytics solutions, dedicated to helping organizations transform data into actionable insights. Our innovative approach combines advanced technology with expert consulting, allowing businesses to leverage their data for improved performance and decision-making.\n\nJob Description.\nWe are seeking a skilled and motivated Data Engineer with experience in Databricks to join our dynamic team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and data processing solutions that support our analytics initiatives. You will collaborate closely with data scientists, analysts, and other engineers to ensure the consistent flow of high-quality data across our platforms.",,,,"['python', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'data pipeline', 'data architecture', 'data engineering', 'sql', 'data bricks', 'cloud', 'analytics', 'data quality', 'data modeling', 'gcp', 'teamwork', 'integration', 'aws', 'etl', 'programming', 'communication skills', 'etl scripts']",2025-06-13 06:15:57
Gcp Data Engineer,Saama Technologies,3 - 8 years,Not Disclosed,"['Pune', 'Chennai', 'Coimbatore']","We are looking for immediate joiners only.\nPosition: GCP Data Engineer\nWe are seeking a skilled and experienced GCP Data Engineer to join our dynamic team. The ideal candidate will have a strong background in Google Cloud Platform (GCP), BigQuery, Dataform, and data warehouse concepts. Experience with Airflow/Cloud Composer and cloud computing knowledge will be a significant advantage.\nResponsibilities:\n- Designing, developing, and maintaining data pipelines and workflows on the Google Cloud Platform.",,,,"['Pyspark', 'GCP', 'Python', 'SQL', 'Google Cloud Platforms']",2025-06-13 06:15:59
GCP Data Engineer,TVS Next,3 - 5 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['kubernetes', 'pyspark', 'data pipeline', 'sql', 'docker', 'cloud', 'tensorflow', 'java', 'spark', 'gcp', 'pytorch', 'bigquery', 'programming', 'ml', 'cloud sql', 'cd', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'application engine', 'machine learning', 'apache flink', 'data engineering', 'dataproc', 'kafka', 'cloud storage', 'terraform', 'bigtable']",2025-06-13 06:16:00
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-13 06:16:02
AI / ML Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Large Language Models\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI/ML Engineer, you will develop applications and systems utilizing AI tools, Cloud AI services, and GenAI models. Your role involves creating cloud or on-prem application pipelines with production-ready quality, incorporating deep learning, neural networks, chatbots, and image processing.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead the implementation of large language models in AI applications.- Research and apply cutting-edge AI techniques to enhance system performance.- Contribute to the development and deployment of AI solutions across various domains.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Large Language Models.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Large Language Models.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['machine learning', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'image processing', 'algorithms', 'chatbot', 'python', 'natural language processing', 'power bi', 'artificial intelligence', 'sql', 'deep learning', 'data science', 'text mining', 'data visualization', 'ml']",2025-06-13 06:16:04
AI / ML Engineer,Accenture,15 - 25 years,Not Disclosed,['Bengaluru'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Machine Learning\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n15 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI / ML Engineer, you will engage in the development of applications and systems that leverage artificial intelligence tools and cloud AI services. Your typical day will involve designing and implementing production-ready solutions, ensuring that they meet quality standards. You will work with various AI models, including generative AI, deep learning, and neural networks, while also exploring innovative applications such as chatbots and image processing. Collaboration with cross-functional teams will be essential as you contribute to the overall success of projects and drive advancements in AI technology.\nRoles & Responsibilities:- Expected to be a Subject Matter Expert with deep knowledge and experience.- Should have influencing and advisory skills.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities and understanding of AI technologies.- Mentor junior professionals to foster their growth and development in AI and machine learning.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Machine Learning.- Experience with cloud-based AI services and deployment strategies.- Strong understanding of deep learning frameworks such as TensorFlow or PyTorch.- Familiarity with data preprocessing and feature engineering techniques.- Ability to design and implement neural networks for various applications.\nAdditional Information:- The candidate should have minimum 15 years of experience in Machine Learning.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['deep learning frameworks', 'machine learning', 'artificial intelligence', 'tensorflow', 'pytorch', 'image processing', 'chatbot', 'python', 'cnn', 'natural language processing', 'neural networks', 'numpy', 'sql', 'pandas', 'deep learning', 'r', 'rnn', 'data science', 'computer vision', 'text mining', 'ml']",2025-06-13 06:16:05
AI / ML Engineer,Accenture,15 - 20 years,Not Disclosed,['Pune'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Natural Language Processing (NLP)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI/ML Engineer, you will develop applications and systems utilizing AI tools, Cloud AI services, and GenAI models. Your role involves creating production-ready solutions with deep learning, neural networks, chatbots, and image processing.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead the implementation of NLP algorithms for text analysis.- Design and develop AI models for language understanding.- Optimize NLP models for performance and scalability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Natural Language Processing (NLP).- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Natural Language Processing (NLP).- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['natural language processing', 'machine learning', 'machine learning algorithms', 'statistics', 'data munging', 'image processing', 'chatbot', 'algorithms', 'python', 'power bi', 'artificial intelligence', 'sql', 'deep learning', 'tableau', 'data science', 'data visualization', 'ml']",2025-06-13 06:16:06
AI / ML Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Data Science\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI / ML Engineer, you will engage in the development of applications and systems that leverage artificial intelligence tools and cloud AI services. Your typical day will involve collaborating with cross-functional teams to design and implement production-ready solutions, ensuring that the applications meet high-quality standards. You will also explore innovative approaches to integrate generative AI models into various projects, contributing to advancements in deep learning, neural networks, and image processing technologies. Your role will require a balance of technical expertise and creative problem-solving to address complex challenges in the field of artificial intelligence.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge.- Continuously evaluate and improve existing processes to enhance efficiency.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Science.- Strong understanding of machine learning algorithms and their applications.- Experience with data preprocessing and feature engineering techniques.- Familiarity with cloud platforms and services related to AI and machine learning.- Ability to work with large datasets and perform data analysis.\nAdditional Information:- The candidate should have minimum 5 years of experience in Data Science.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'machine learning', 'artificial intelligence', 'data science', 'machine learning algorithms', 'image processing', 'algorithms', 'chatbot', 'python', 'natural language processing', 'sql', 'deep learning', 'tensorflow', 'r', 'computer vision', 'pytorch', 'keras', 'ml']",2025-06-13 06:16:08
AI / ML Engineer,Accenture,12 - 15 years,Not Disclosed,['Bengaluru'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Data Science\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI / ML Engineer, you will engage in the development of applications and systems that leverage artificial intelligence tools and cloud AI services. Your typical day will involve designing and implementing production-ready solutions, ensuring that they meet quality standards. You will work with various AI models, including generative AI, deep learning, and neural networks, while also exploring innovative applications such as chatbots and image processing. Collaboration with cross-functional teams will be essential to integrate these advanced technologies into existing systems and workflows, driving efficiency and enhancing user experiences.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate knowledge sharing and mentorship within the team to foster professional growth.- Continuously evaluate and improve existing processes and systems to enhance performance and reliability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Science.- Strong understanding of machine learning algorithms and their applications.- Experience with cloud platforms and services for deploying AI solutions.- Familiarity with programming languages such as Python or R for data analysis.- Knowledge of data preprocessing techniques and data visualization tools.\nAdditional Information:- The candidate should have minimum 12 years of experience in Data Science.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'r', 'data science', 'machine learning algorithms', 'image processing', 'algorithms', 'chatbot', 'c++', 'data analysis', 'natural language processing', 'artificial intelligence', 'sql', 'deep learning', 'tensorflow', 'computer vision', 'keras', 'data visualization', 'ml']",2025-06-13 06:16:09
AI / ML Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Data Science\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI / ML Engineer, you will engage in the development of applications and systems that leverage artificial intelligence tools and cloud AI services. Your typical day will involve collaborating with cross-functional teams to design and implement production-ready solutions, ensuring that the applications meet high-quality standards. You will also explore innovative approaches to integrate generative AI models into various projects, contributing to advancements in deep learning, neural networks, and image processing technologies. Your role will require a balance of technical expertise and creative problem-solving to address complex challenges in the AI landscape.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge.- Continuously evaluate and improve existing processes and workflows to increase efficiency.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Science.- Strong understanding of machine learning algorithms and their applications.- Experience with cloud-based AI services and deployment strategies.- Familiarity with programming languages such as Python or R.- Knowledge of data visualization techniques and tools.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in Data Science.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'r', 'data science', 'machine learning algorithms', 'image processing', 'algorithms', 'chatbot', 'natural language processing', 'artificial intelligence', 'sql', 'deep learning', 'tensorflow', 'predictive modeling', 'keras', 'data visualization', 'ml', 'statistics']",2025-06-13 06:16:11
AI / ML Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Data Science\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI / ML Engineer, you will engage in the development of applications and systems that leverage artificial intelligence tools and cloud AI services. Your typical day will involve designing and implementing production-ready solutions, ensuring that they meet quality standards. You will work with various AI models, including generative AI, deep learning, and neural networks, while also exploring innovative applications such as chatbots and image processing. Collaboration with cross-functional teams will be essential to integrate these advanced technologies into existing systems and workflows, driving efficiency and enhancing user experiences.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge in AI and machine learning.- Continuously evaluate and implement new technologies to improve system performance and efficiency.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Science.- Good To Have\n\n\n\n\nSkills:\nExperience with machine learning frameworks such as TensorFlow or PyTorch.- Strong understanding of data analysis and statistical modeling techniques.- Experience in deploying AI models in cloud environments like AWS, Azure, or Google Cloud.- Familiarity with data preprocessing and feature engineering techniques.\nAdditional Information:- The candidate should have minimum 5 years of experience in Data Science.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'machine learning', 'tensorflow', 'data science', 'statistical modeling', 'image processing', 'chatbot', 'python', 'natural language processing', 'scikit-learn', 'microsoft azure', 'numpy', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'gcp', 'pytorch', 'aws', 'opencv', 'ml']",2025-06-13 06:16:12
AI / ML Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Large Language Models\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI/ML Engineer, you will develop applications and systems utilizing AI tools, Cloud AI services, and GenAI models. Your role involves creating cloud or on-prem application pipelines with production-ready quality, incorporating deep learning, neural networks, chatbots, and image processing.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead research and development initiatives- Implement cutting-edge AI/ML algorithms- Stay updated on industry trends and advancements\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Large Language Models- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Large Language Models- This position is based at our Hyderabad office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['machine learning', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'image processing', 'algorithms', 'chatbot', 'python', 'natural language processing', 'power bi', 'artificial intelligence', 'sql', 'deep learning', 'data science', 'text mining', 'data visualization', 'ml']",2025-06-13 06:16:14
AI / ML Engineer,Accenture,15 - 25 years,Not Disclosed,['Bengaluru'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Large Language Models\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n15 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI/ML Engineer, you will develop applications and systems utilizing AI tools, Cloud AI services, and GenAI models. Your role involves creating cloud or on-prem application pipelines with production-ready quality, incorporating deep learning, neural networks, chatbots, and image processing.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead the implementation of large language models in AI applications.- Research and apply cutting-edge AI techniques to enhance system performance.- Contribute to the development and deployment of AI solutions across various domains.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Large Language Models.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Large Language Models.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['machine learning', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'image processing', 'algorithms', 'chatbot', 'python', 'natural language processing', 'power bi', 'artificial intelligence', 'sql', 'deep learning', 'data science', 'text mining', 'data visualization', 'ml']",2025-06-13 06:16:16
Software Engineer III,Walmart,2 - 7 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team\nWe are in search of a skilled and dedicated Machine Learning Software Engineer to join our team.\nYour primary role will encompass developing machine learning models, working on distributed and personalization systems in the e-commerce.\nYou will be integral in creating algorithms that drive intelligent business decisions and improve our user experience and drive business growth.\nWhat You Will Do\nDesign, develop, and implement high-performance machine learning models.\nWork on distributed systems and sync/async services in an e-commerce domain.\nDevelop personalization systems to enhance the user experience and drive business growth.\nTranslate complex functional and technical requirements into detailed design.\nCollaborate with cross-functional teams to define, design, and ship new features.\nParticipate in code and design reviews to maintain high development standards.\nManage individual project priorities, deadlines, and deliverables.\nProvide technical guidance and coaching to developers and engineers.\nKeep abreast of the latest advancements in machine learning and artificial intelligence.\nWhat You Will Bring\nBachelor s / Masters degree in Computer Science, Mathematics, or a related field.\nProven experience as a Machine Learning Engineer or similar role.\nExperience in distributed and concurrent systems, backend services, and sync/async services.\nUnderstanding of e-commerce systems and experience in personalization systems.\nProficiency with machine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn).\nFamiliarity with code versioning tools such as Git. Strong analytical and problem-solving skills.\nExcellent communication and teamwork skills. Ability to work in a fast-paced, agile environment.\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in computer science, information technology, engineering, information systems, cybersecurity, or related area and 2years experience in software engineering or related area at a technology, retail, or data-driven company.\n\nOption 2: 4 years experience in software engineering or related area at a technology, retail, or data-driven company.\nPreferred Qualifications...\nCertification in Security+, Network+, GISF, GSEC, CISSP, or CCSP, Master s degree in Computer Science, Information Technology, Engineering, Information Systems, Cybersecurity, or related area",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Backend', 'Networking', 'Analytical', 'Artificial Intelligence', 'Machine learning', 'Agile', 'E-commerce', 'Information technology', 'Distribution system']",2025-06-13 06:16:17
"Software Development Engineer III, RISC",Amazon,5 - 10 years,Not Disclosed,['Bengaluru'],"Are you interested in joining a team that safeguards and enhances the buying experience of Amazon customers worldwide while empowering sellers worldwide to build the most customer-centric marketplace on EarthOur team leverages a host of technologies including machine learning, data mining, big data analytics, cloud computing, and highly scalable distributed systems to support scalable global transactions every day.\n\nWe have an exciting opportunity with the Regulatory Intelligence, Safety, and Compliance (RISC) engineering team, to architect and build next-generation engineering systems to quickly and accurately identify and mitigate product safety issues and potential risks to the customer experience.\n\nAs a Software Development Engineer, you will collaborate with a team of talented software, data, and machine learning engineers to design, develop, and manage highly scalable distributed systems. These systems are engineered to ensure high availability, low latency, and seamless scalability safeguarding customers and ensuring product compliance. Your work will contribute to building the worlds most trusted data platform for all safety and compliance information related to products and supply chains.\n\nEach and every person buying, selling, or handling Amazon products will be your customer.\n\nAs a member of this growing team, you ll be able to build the groundwork and influence its direction for the years to come. Our work cuts across various disciplines from delivering an awesome user experience via great UI/UX, to building massively scalable backend systems to support the most high-traffic pages on Amazon.com, to analytical and feedback systems which give us data-driven customer insights, to using machine learning and AI to influence recommendations and marketing. If you have a passion for consumer-facing applications, and are obsessed with customer experience, we want you!\n\nIf you d like to make a real-world difference by working hard, having fun, and making history, this is the team for you!\n\n\nIn this role you will:\n\nHelp define the system architecture, own and implement specific components, and help shape the overall experience\nCollaborate closely with product managers, UX designers, and other SDE team members to help define the scope of the product\nTake responsibility for technical problem solving, creatively meeting product objectives, and developing best practices\nDemonstrate cross-functional resource interaction to accomplish your goals\nWrite high-quality, efficient, testable code in Java and other object-oriented languages\nDesign Amazon-scale tools to facilitate internal business\nBuild highly available, secure, and low-latency systems\nMentor other developers\nFind out what it takes to engineer systems for ""Amazon Scale""\nDesign and build microservices\nOwn and operate the systems that you build based on real-time customer data and demanding service-level agreements\nContribute to planning, design, implementation, testing, operations, and process improvement\n\nA day in the life\nHigh-level designs, cross-team alignment, long-term architectural roadmap and technical strategy, understanding the business domain and proposing solutions to address customer and business problems, helping scope and analyze product requirements, mentorship, reviewing CRs, writing high-quality code to be an example for the team. 5+ years of non-internship professional software development experience\n5+ years of programming with at least one software programming language experience\n5+ years of leading design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nExperience as a mentor, tech lead or leading an engineering team 5+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Cloud computing', 'Backend', 'Coding', 'Analytical', 'Machine learning', 'Data mining', 'Internship', 'Distribution system']",2025-06-13 06:16:19
Software Engineer III,JPMorgan Chase Bank,8 - 13 years,Not Disclosed,['Bengaluru'],"As an Experienced Software Engineer at JPMorgan Chase within the Global Technology team, you serve as member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. Depending on the team that you join, you could be developing mobile features that give our customers and clients more control over how they bank with us, strategizing on how big data can make our trading systems quicker, creating the next innovation in payments for merchants, or supporting the integration of our private and public cloud platforms.\n\nJob Responsibilities\nParticipates in, design and develop scalable and resilient systems using Java to contribute to continual, iterative improvements for product teams\nExecutes software solutions, design, development, and technical troubleshooting\nCreates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems\nProduces or contributes to architecture and design artifacts for applications while ensuring design constraints are met by software code development\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems\nIdentifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture\nContributes to software engineering communities of practice and events that explore new and emerging technologies\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nMin 8+ years strong hands on experience as a JAVA Programmer\nHands-on practical experience in system design, application development, testing and operational stability\nProficient in coding in Java language\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nOverall knowledge of the Software Development Life Cycle\nUnderstanding of agile methodologies such as CI/CD, Application Resiliency, and Security\nKnowledge of software applications and technical processes within a technical discipline (eg, cloud, artificial intelligence, machine learning, mobile, etc)\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System architecture', 'Front end', 'Coding', 'Artificial Intelligence', 'Debugging', 'Machine learning', 'Agile', 'System design', 'Application development', 'Troubleshooting']",2025-06-13 06:16:21
Software Engineer III - Java / Python Full Stack,JPMorgan Chase Bank,3 - 8 years,Not Disclosed,['Bengaluru'],"As a Software Engineer III at JPMorgan Chase within the Asset and Wealth Management, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm s business objectives.\n  Job responsibilities\nExecutes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems\nCreates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems\nProduces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems\nProactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture\nContributes to software engineering communities of practice and events that explore new and emerging technologies\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 3+ years applied experience\nStrong in AWS Services like S3, Terraform for infrastructure setup.\nStrong in Python/Java with React as front end\nHands-on practical experience in system design, application development, testing, and operational stability\nProficient in coding in one or more languages\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nOverall knowledge of the Software Development Life Cycle\nSolid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\nDemonstrated knowledge of software applications and technical processes within a technical discipline (eg, cloud, artificial intelligence, machine learning, mobile, etc)\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System architecture', 'Front end', 'Coding', 'Debugging', 'Machine learning', 'Agile', 'System design', 'Application development', 'Troubleshooting', 'Python']",2025-06-13 06:16:23
Software Engineer III,JPMorgan Chase Bank,3 - 8 years,Not Disclosed,['Bengaluru'],"As a Software Engineer III at JPMorgan Chase within the Asset and Wealth Management, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm s business objectives.\nJob responsibilities\nExecutes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems\nCreates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems\nProduces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems.\nWorks closely with the product team to understand the business requirements and user acceptance testing teams to support during UAT phase.\nProactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture\nContributes to software engineering communities of practice and events that explore new and emerging technologies\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 3+ years applied experience\nHands-on practical experience in system design, application development, testing, and operational stability\nExtensive design, coding, testing and debugging skills in Java and Spring Boot framework\nProficient in coding in in Java 17, Spring Boot, and Databases\nAdvanced in two or more technologies - Functional Programming, Microservices, RESTful webservices development, JMS, Kafka, GraphQL\nStrong Hands-on Cloud Native Architecture - Azure / AWS, Containerization / Kubernetes\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nOverall knowledge of the Software Development Life Cycle\nSolid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\nStrong skills around object-oriented analysis and design (OOAD), data structures, algorithms, design patterns\nDemonstrated knowledge of software applications and technical processes within a technical discipline (e.g., cloud, artificial intelligence, machine learning, mobile, etc.)\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nUnderstanding or experience with Language Models (LLM).\nExperience with Machine Learning or AI technologies.\nExposure to cloud technologies\nExposure to Python programming language\nIndependent and self-motivated\nStrong interpersonal and communication skills",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['JMS', 'Web services', 'Coding', 'OOAD', 'Debugging', 'Agile', 'Data structures', 'Application development', 'Troubleshooting', 'Python']",2025-06-13 06:16:24
Sr Manager of Software Engineering,JPMorgan Chase Bank,14 - 20 years,Not Disclosed,['Bengaluru'],"When you mentor and advise multiple technical teams and move financial technologies forward, it s a big challenge with big impact. You were made for this.\n\n\nAs a Senior Manager of Software Engineering at JPMorgan Chase within the Consumer and Community Banking Technology Team, you serve in a leadership role by providing technical coaching and advisory for multiple technical teams, as well as anticipate the needs and potential dependencies of other functions within the firm. As an expert in your field, your insights influence budget and technical considerations to advance operational efficiencies and functionalities.\n\nJob responsibilities\n\n\n\nProvide direction, oversight, and coaching for a team of entry-level to mid-level software engineers working on basic to moderately complex tasks.\n\nBe accountable for decisions affecting team resources, budget, tactical operations, and the execution and implementation of processes and procedures.\n\nLead the design, development, testing, and implementation of data visualization projects to support business objectives.\n\nCollaborate with data analysts, data scientists, and business stakeholders to understand data requirements and translate them into effective visual solutions.\n\nWork in an Agile development environment with team members, including Product Managers, UX Designers, QA Engineers, and other Software Engineers.\n\nValidate the technical feasibility of UI/UX designs and provide regular technical guidance to support business and technical teams, contractors, and vendors.\n\nDevelop secure, high-quality production code, review and debug code written by others, and drive decisions influencing product design, application functionality, and technical operations.\n\nServe as a subject matter expert in one or more areas of focus and actively contribute to the engineering community as an advocate of firmwide frameworks, tools, and practices of the Software Development Life Cycle.\n\nInfluence peers and project decision-makers to consider the use and application of leading-edge technologies.\n\nDevelop and maintain dashboards, reports, and interactive visualizations using tools such as Tableau, ensuring data accuracy and integrity by implementing best practices in data visualization and management.\n\nStay current with industry trends and emerging technologies in data visualization and analytics, communicate complex data insights clearly to various audiences, including senior management, and manage a team of data visualization associates, providing guidance and mentorship to junior team members.\n\n\n\nRequired qualifications, capabilities, and skills\n\n\n\nFormal training or certification in software engineering concepts and 5+ years of applied experience.\n\n5+ Years of experience as a Web/UI Lead Architect\n\nProficiency in Javascript, Typescript, HTML, CSS\n\nExpert knowledge in ReactJs, Redux, React hooks.\n\nStrong understanding of front-end coding and development technologies\n\nHands-on practical experience delivering system design, application development, testing, and operational stability\n\nAdvanced knowledge of software applications and technical processes with considerable in-depth knowledge in UI and Web Technologies\n\nAbility to tackle design and functionality problems independently with little to no oversight\n\nPractical cloud native experience\n\nExperience in Computer Science, Computer Engineering, Mathematics, or a related technical field\n\n\n\nPreferred qualifications, capabilities, and skills\n\n\n\nFull stack development with Node/. NET/Java\n\nFamiliarity with working in event driven environments\n\nA good understanding of cross-browser compatibility issues and their solutions along with Typescript\n\nExperience working with Databases and ability to write SQL queries along with experience with messaging platforms\n\nBachelor s degree in data science, Computer Science, Information Systems, Statistics, or a related field.\n\nProblem solver and solution oriented. Strong written and verbal communication skills. Jira and Agile practices\n\nExperience with big data technologies and machine learning is a plus.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Front end', 'Coding', 'Javascript', 'Agile', 'System design', 'HTML', 'Application development', 'JIRA', 'Analytics']",2025-06-13 06:16:26
"Software Engineer III - Java, Spring, AWS",JPMorgan Chase Bank,3 - 10 years,Not Disclosed,['Bengaluru'],"As a Software Engineer III at JPMorgan Chase within the Consumer Community Banking Team, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm s business objectives.\nJob responsibilities\nExecutes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems\nCreates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems\nProduces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems\nProactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture\nContributes to software engineering communities of practice and events that explore new and emerging technologies\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 3+ years applied experience in Java, Springboot, Terraforms, AWS\nHands-on practical experience in system design, application development, testing, and operational stability.\nHands-on experience working with AWS stack/services, Java and Spring\nExperience of AWS RDS/Aurora Database/AWS EKS/ECS/Lambda\nKnowledge of AWS SQS/SNS, Terraform for AWS resource/service provisioning\nExperience building and debugging large-scale web services, and microservices based, Kubernetes-orchestrated applications.\nExperience of software applications and technical processes within AWS public cloud\nStrong communication skills and stakeholder management is must.\nExperience in providing technical leadership and guidance to the team.\nSolid understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security.\nDemonstrated knowledge of software applications and technical processes within a technical discipline (e. g. , cloud, artificial intelligence, machine learning, mobile\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies, web services is a plus",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System architecture', 'Front end', 'Coding', 'Debugging', 'Machine learning', 'Technical leadership', 'Agile', 'System design', 'Application development', 'Troubleshooting']",2025-06-13 06:16:27
"Software Engineer III - Java, AWS, Terraforms",JPMorgan Chase Bank,3 - 10 years,Not Disclosed,['Bengaluru'],"As a Software Engineer III at JPMorgan Chase within the Consumer Community Banking Team, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm s business objectives.\nJob responsibilities\nExecutes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems\nCreates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems\nProduces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems\nProactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture\nContributes to software engineering communities of practice and events that explore new and emerging technologies\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 3+ years applied experience in Java, AWS, Terraforms\nHands-on practical experience in system design, application development, testing, and operational stability\nProficient in coding in one or more languages\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nOverall knowledge of the Software Development Life Cycle\nSolid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\nDemonstrated knowledge of software applications and technical processes within a technical discipline (e. g. , cloud, artificial intelligence, machine learning, mobile, etc. )\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System architecture', 'Front end', 'Coding', 'Artificial Intelligence', 'Debugging', 'Machine learning', 'Agile', 'System design', 'Application development', 'Troubleshooting']",2025-06-13 06:16:29
Software Engineer II - Java AWS Terraforms,JPMorgan Chase Bank,3 - 10 years,Not Disclosed,['Bengaluru'],"You re ready to gain the skills and experience needed to grow within your role and advance your career and we have the perfect software engineering opportunity for you.\nAs a Software Engineer II at JPMorgan Chase within the Consumer Community Banking Team, you are part of an agile team that works to enhance, design, and deliver the software components of the firm s state-of-the-art technology products in a secure, stable, and scalable way. As an emerging member of a software engineering team, you execute software solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role.\nJob responsibilities\nExecutes standard software solutions, design, development, and technical troubleshooting\nWrites secure and high-quality code using the syntax of at least one programming language with limited guidance\nDesigns, develops, codes, and troubleshoots with consideration of upstream and downstream systems and technical implications\nApplies knowledge of tools within the Software Development Life Cycle toolchain to improve the value realized by automation\nApplies technical troubleshooting to break down solutions and solve technical problems of basic complexity\nGathers, analyzes, and draws conclusions from large, diverse data sets to identify problems and contribute to decision-making in service of secure, stable application development\nLearns and applies system processes, methodologies, and skills for the development of secure, stable code and systems\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 2+ years applied experience in Java, AWS, Terraforms\nHands-on practical experience in system design, application development, testing, and operational stability, with hands-on experience in developing and deploying applications on AWS\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nDemonstrable ability to code in one or more languages\nExperience across the whole Software Development Life Cycle\nExposure to agile methodologies such as CI/CD, Application Resiliency, and Security\nEmerging knowledge of software applications and technical processes within a technical discipline (e. g. , cloud, artificial intelligence, machine learning, mobile, etc. )\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Artificial Intelligence', 'Debugging', 'Machine learning', 'Agile', 'System design', 'Application development', 'Troubleshooting', 'Downstream']",2025-06-13 06:16:30
Software Engineer II,JPMorgan Chase Bank,3 - 10 years,Not Disclosed,['Bengaluru'],"You re ready to gain the skills and experience needed to grow within your role and advance your career and we have the perfect software engineering opportunity for you.\nAs a Software Engineer II at JPMorgan Chase within the Consumer Community Banking Team, you are part of an agile team that works to enhance, design, and deliver the software components of the firm s state-of-the-art technology products in a secure, stable, and scalable way. As an emerging member of a software engineering team, you execute software solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role.\nJob responsibilities\nExecutes standard software solutions, design, development, and technical troubleshooting\nWrites secure and high-quality code using the syntax of at least one programming language with limited guidance\nDesigns, develops, codes, and troubleshoots with consideration of upstream and downstream systems and technical implications\nApplies knowledge of tools within the Software Development Life Cycle toolchain to improve the value realized by automation\nApplies technical troubleshooting to break down solutions and solve technical problems of basic complexity\nGathers, analyzes, and draws conclusions from large, diverse data sets to identify problems and contribute to decision-making in service of secure, stable application development\nLearns and applies system processes, methodologies, and skills for the development of secure, stable code and systems\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 2+ years applied experience in Java, AWS, Terraforms\nHands-on practical experience in system design, application development, testing, and operational stability\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nDemonstrable ability to code in one or more languages\nExperience across the whole Software Development Life Cycle\nExposure to agile methodologies such as CI/CD, Application Resiliency, and Security\nEmerging knowledge of software applications and technical processes within a technical discipline (e. g. , cloud, artificial intelligence, machine learning, mobile, etc. )\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Artificial Intelligence', 'Debugging', 'Machine learning', 'Agile', 'System design', 'Application development', 'Troubleshooting', 'Downstream']",2025-06-13 06:16:32
Full Stack Engineer,Accenture,7 - 12 years,Not Disclosed,['Coimbatore'],"Project Role :Full Stack Engineer\n\n\n\n\n\nProject Role Description :Responsible for developing and/or engineering the end-to-end features of a system, from user experience to backend code. Use development skills to deliver innovative solutions that help our clients improve the services they provide. Leverage new technologies that can be applied to solve challenging business problems with a cloud first and agile mindset.\n\n\n\nMust have skills :Java Full Stack Development, Node.js\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :BE\n\n\nSummary:As a Full Stack Engineer, you will be responsible for developing and/or engineering the end-to-end features of a system, from user experience to backend code. You will use your development skills to deliver innovative solutions that help our clients improve the services they provide. Additionally, you will leverage new technologies to solve challenging business problems with a cloud-first and agile mindset.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Develop and engineer end-to-end features of a system.- Deliver innovative solutions to improve client services.- Utilize development skills to solve challenging business problems.- Stay updated with new technologies and apply them to projects.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Java Full Stack Development, Apache Kafka.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Java Full Stack Development.- This position is based at our Bengaluru office.- A BE degree is required.\n\nQualification\n\nBE",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['java', 'javascript', 'node', 'kafka', 'full stack', 'algorithms', 'css', 'c++', 'bootstrap', 'jquery', 'sql', 'react.js', 'elastic search', 'linux', 'json', 'html', 'mysql', 'machine learning algorithms', 'mongodb', 'python', 'c', 'power bi', 'machine learning', 'angular', 'node.js', 'tableau', 'statistics', 'data munging']",2025-06-13 06:16:34
Full Stack Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Full Stack Engineer\n\n\n\n\n\nProject Role Description :Responsible for developing and/or engineering the end-to-end features of a system, from user experience to backend code. Use development skills to deliver innovative solutions that help our clients improve the services they provide. Leverage new technologies that can be applied to solve challenging business problems with a cloud first and agile mindset.\n\n\n\nMust have skills :Data Science\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Full Stack Engineer, you will be responsible for developing and engineering the end-to-end features of a system. Your typical day will involve collaborating with cross-functional teams to design and implement innovative solutions that enhance user experience and optimize backend processes. You will leverage new technologies to address complex business challenges while maintaining a cloud-first and agile approach, ensuring that the services provided to clients are continuously improved and aligned with their needs. Your role will require a balance of creativity and technical expertise to deliver high-quality software solutions that meet project requirements and client expectations.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with team members to design and implement new features.- Conduct code reviews to ensure quality and adherence to best practices.""Experience with Hugging Face Transformers, LangChain, OpenAI APIs Strong knowledge of machine learning libraries:scikit-learn, XGBoost, LightGBM Experience with Retrieval-Augmented Generation (RAG):Faiss, Pinecone, Weaviate, ChromaDB Training, fine-tuning, and optimizing transformer models (BERT, GPT, T5, LLaMA) Deep understanding of self-attention, positional embeddings, sequence modeling Strong NLP expertise:Named Entity Recognition (NER), topic modeling, text embeddings Good grasp of linear algebra:vectors, matrices, eigenvalues, eigenvectors etcExpertise in gradient descent, backpropagation, activation functions, loss functions Experience with dimensionality reduction techniques for feature selection Knowledge of time-series forecasting, anomaly detection, and probabilistic modeling Understanding of model serving techniques, including REST APIs and real-time inference""""Develop and fine-tune LLMs (GPT, BERT, T5, LLaMA, Mistral) for text generation, summarization, translation, and question-answering tasks.Work with transformer-based architectures, understanding self-attention mechanisms, token embeddings, and model interpretability.Implement custom tokenization techniques, text embedding models (Sentence-BERT, OpenAI Embeddings, Cohere, etc.), and efficient inference optimizations.Apply prompt engineering and prompt tuning techniques to optimize model responses.Work with LangChain and LLM orchestration frameworks to build AI-powered applications.Build Retrieval-Augmented Generation (RAG) pipelines using vector databases like FAISS, Pinecone, Weaviate, ChromaDB.Work with Hugging Face Transformers, spaCy, NLTK, and OpenAI APIs.""\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Python (Programming Language).- Good To Have\n\n\n\n\nSkills:\nExperience with JavaScript frameworks such as React or Angular.- Strong understanding of RESTful API design and development.- Familiarity with cloud platforms such as AWS or Azure.- Experience with database management systems like MySQL or PostgreSQL.""Must Have\n\n\n\n\nSkills:\n1. Must have 5-7+ years of experience working as a Microsoft FED Angular.2. Strong understanding of software development principles, design patterns, and best practices.3. Develop, test, and maintain software applications using Angular and .NET.4. Designing & developing user interfaces using angular best practices5. Developing application code & unit tests in angular.6. Experience in consuming Web API/ Rest API using Javascript.7. Should have good troubleshooting and debugging skills Good to Have\n\n\n\n\nSkills:\n1. Knowledge/Experience in Excel 2 Knowledge/Experience of Agile methodology and Sprint based project deliveryNice to Have\n\n\n\n\nSkills:\n1. .Net Core2. Agile Development Methodology2. MySQL"""" Should be good team player. Good Written & Verbal Communication skills Good analytical and troubleshooting skills.""\nAdditional Information:- The candidate should have minimum 3 years of experience in Python (Programming Language).- This position is based at our Bengaluru office.- A 15 years full time education is required.Resource should be open to work in B Shift (Afternoon Shifts 12:30 to 10:30) during week days.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'software development', 'natural language processing', 'machine learning', 'data science', 'rest', 'microsoft fed reactjs', 'scikit-learn', 'rest api design', 'microsoft azure', 'javascript', 'angular', 'react.js', 'postgresql', 'design patterns', 'mysql', 'web api', 'agile', 'aws', 'xgboost']",2025-06-13 06:16:36
Engineering Manager,Flipkart,10 - 14 years,Not Disclosed,['Bengaluru'],"Skills Required :\nEngineering Management, People Management, Technical Leadership, System Architecture\nEducation/Qualification :\nB.E / B.Tech / MCA / M.Tech equivalent\nDesirable Skills :\nAgile Methodology, Cloud Computing, Artificial Intelligence\nYears Of Exp :\n10 to 14 Years",Industry Type: Courier / Logistics,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Engineering Management', 'Agile Methodology', 'Cloud Computing', 'System Architecture', 'Artificial Intelligence', 'Technical Leadership', 'People Management']",2025-06-13 06:16:38
Engineering IT Software Solutions Manager,Qualcomm,7 - 12 years,Not Disclosed,['Bengaluru'],"General Summary:\nQualcomm is enabling a world where everyone and everything can be intelligently connected. Qualcomm 5G and AI innovations are the power behind the connected intelligent edge. Youll find our technologies behind and inside the innovations that deliver significant value across multiple industries and to billions of people every day.\nQualcomm engineering teams rely heavily on the latest High Performance Computing (HPC) technologies to design and develop new products using electronic design automation (EDA) tools. This role offers an exciting opportunity to manage and deliver a portfolio of distributed software solutions and services for core engineering teams. You will gain experience leading a portfolio of critical projects while building scalable and fault-tolerant software solutions that are deployed on some of the largest supercomputing infrastructures across the globe.\nMinimum Qualifications:\n7+ years of IT-related work experience with a Bachelor's degree.\nOR\n9+ years of IT-related work experience without a Bachelors degree.\n\n4+ years in a leadership role in projects/programs.\nWhat are we looking for?\nEngineering Data Analytics & Applications (EDAAP) team is looking for an experienced software development manager preferably with exposure to HPC technologies. The team handles development of software and analytics solutions enabling High Performance Compute grid and large-scale, distributed applications. They work on components and services for HPC infrastructure optimization, hardware IP management systems, petabyte-scale cloud data platforms, development of machine learning solutions, data pipelines, and operational insights.\nThis role will lead a team of about 30 software developers and data engineers working on a portfolio of software products and analytics being developed by the team. The ideal candidate would be a seasoned Software Development Manager experienced in engaging with business and technical stakeholders, understanding complex problem statements, and proposing value-driven software and analytics solutions.\nWhat will you do?\nThis roles responsibilities include:\nLead and manage a team of software developers, data engineers and project manager, providing mentorship and guidance to foster professional growth.\nProvide technical expertise across a portfolio of software development and analytics projects, creating designs and performing code reviews.\nIdentify opportunities and deliver solutions for EDA workflow optimizations.\nSet and manage team priorities in line with organizational goals and objectives, working closely with diverse set of stakeholders in Engineering & Infrastructure Services.\nOversee the entire software development lifecycle, from planning and design to implementation, testing, and deployment for a portfolio of products and services developed by the team.\nCollaborate with global teams to define project requirements, scope, and deliverables.\nEnsure the delivery of high-quality software solutions and analytics that meet business objectives and customer needs.\nImplement best practices for software development, including coding standards, code reviews, and automated testing.\nManage project timelines and resources to ensure successful project completion.\nStay updated with the latest industry trends and technologies to drive continuous improvement and innovation.\nBuild a culture of collaboration, accountability, and continuous learning within the team.\nWhat do we want to see?\nThe ideal candidate will be able to demonstrate some of the following skills:\n14+ years of hands-on experience in large-scale distributed software engineering and analytics, with at least 4 years in a leadership role\nStrong proficiency in programming languages such as Java, C++, Python, Rust or similar.\nExpertise in software lifecycle management, version control, and CI/CD best practices for quality, agility and security, data engineering and analytics\nProven ability to manage multiple projects and conflicting priorities.\nExperience with public cloud environments such as AWS, Azure or Google Cloud\nExperience with microservices architecture and containerization\nFamiliarity with EDA and semiconductor design process\nAbility to explain technical concepts and analysis implications in a clear manner to a wide audience.\nExposure to HPC technologies is a plus.\nBachelors or Masters in Computer Science or related field",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['CI/CD', 'Java', 'C++', 'Azure', 'Rust', 'microservices architecture', 'AWS', 'Python', 'Google Cloud']",2025-06-13 06:16:40
Sr SW Engineer - ML,Visa,2 - 7 years,Not Disclosed,['Bengaluru'],"This position is for a Senior Data Engineer with solid development experience who will focus on creating new capabilities for AI as a Service while maturing our code base and development processes. In this position, you are first a passionate and talented developer that can work in a dynamic environment as a member of Agile Scrum teams. Your strong technical leadership, problem-solving abilities, coding, testing and debugging skills is just a start. You must be dedicated to filling product backlog and delivering production-ready code. You must be willing to go beyond the routine and prepared to do a little bit of everything.\nEssential Functions\nCollaborate with project teams, data science teams and development teams to drive the technical roadmap and guide development and implementation of new data driven business solutions.\nDrive technical standard and best practices, and continuously improve AI Platform engineering scalability.\nArchitecture and design of AI Platform services including Machine Learning Engines, In Memory Computing Systems, Streaming Computing Systems, Distributed Data Systems and etc, in Golang, Java, and Python.\nCoordinate the implementation among development teams to ensure system performance, security, scalability and availability.\nCoaching and mentoring junior team members and evolving team talent pipeline.\n\n\nBasic Qualifications\n2+ years of relevant work experience and a Bachelors degree, OR 5+ years of relevant work experience\n\nPreferred Qualifications\n3 or more years of work experience with a Bachelor s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Basic', 'data science', 'Agile scrum', 'Architecture', 'Coding', 'Debugging', 'Machine learning', 'Technical leadership', 'Business solutions', 'Python']",2025-06-13 06:16:41
Staff BI Developer (BI Engineer / Visualization developer),Visa,5 - 10 years,Not Disclosed,['Bengaluru'],"The Risk and Identity Solutions (RaIS) team provides risk management services for banks, merchants, and other payment networks. Machine learning and AI models are the heart of the real-time insights used by our clients to manage risk. Created by the Visa Predictive Models (VPM) team, continual improvement and efficient deployment of these models is essential for our future success. To support our rapidly growing suite of predictive models we are looking for engineers who are passionate about managing large volumes of data, creating efficient, automated processes and standardizing ML/AI tools.\nPrimary responsibilities\nPossess a strong understanding of data interpretation, and the ability to effectively represent data using appropriate visualization techniques to deliver actionable insights.\nFocus on the user experience to design interactive prototype with strong understanding of business context and data following the industry and Visa best practices.\nCollect, analyze, transform, and interpret raw data from various sources. Design and develop BI solutions, data models and KPI measures to solve business problems.\nAbility to create visualizations that are user-friendly, intuitive, and tailored to the needs of the end user, ensuring that the visual elements effectively convey the intended message.\nDevelop and maintain interactive dashboards and reports using BI tools such as Power BI using visual elements like charts, graphs, maps, visual design principles.\nEnsure dashboards and reports are functioning correctly, meet user requirements, and provide accurate, up-to-date insights and perform bug triage by systematically testing data visualizations for accuracy and functionality, identifying issues, prioritizing their resolution based on severity and impact, and ensuring all bugs are fixed in a timely manner.\nOptimize dashboard performance by enhancing data processing speeds, improving query performance, and refining data models to ensure efficient, reliable, and timely data retrieval and analysis for business intelligence applications.\nEnsure the security of data and BI solutions, implement data security measures, complying with all relevant regulations and best practices.\nSet up and maintain the data visualization platform, manage access controls, and ensure systems overall health and performance using usage reports.\nDocument all processes, methodologies, and instructions related to the BI solutions, create comprehensive and accessible documentation, conduct end-user training sessions, and ensure all documentation is consistently updated and available to relevant stakeholders.\nTechnical skills (Must have)\nExpertise in LOD( Level of Detail), DAX(Data Analysis Expressions), Power Query, M language, Tableau Prep to create measures and transform data.\nProficiency in data visualization tools such as Power BI.\nAdvanced proficiency in SQL, including a deep understanding of queries, joins, stored procedures, triggers, and views, as we'll as experience in optimizing SQL for improved performance and efficiency.\nComfortable with creating and maintaining database schemas, indexes, and writing complex SQL scripts for data analysis and extraction\nExperience in interacting with data warehouses and data lakes, utilizing tools like pyspark, Apache Hadoop Amazon Redshift, snowflake and Amazon S3 to ingest and extract data for insights.\nNon-technical skills\nExperienced in working closely with cross-functional teams and stakeholders to ensure understanding and usability of data visualizations.\nContinually stays updated with the latest trends and advancements in data visualization techniques and tools.\nExcellent problem-solving skills and strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.\nExcellent communication and interpersonal skills for managing relationships with stakeholders, strong presentation skills to effectively communicate data insights and visualizations to diverse audiences, with the ability to tailor the presentation to the audiences level of expertise.\nAbility to plan, prioritize, and manage time effectively, keep track of tasks and deadlines, maintain a tidy and systematic work environment, and coordinate resources to achieve goals in a timely and efficient manner.\nTake full responsibility for the BI projects, ensuring accurate and timely delivery of insights, and addressing any issues or inaccuracies in the data promptly and effectively.\nThis is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.\n\n\n5+ years of relevant work experience with a Bachelor s Degree or at least 2 years of work experience with an Advanced degree (e.g. Masters, MBA, JD, MD) or 0 years of work experience with a PhD, OR 8+ years of relevant work experience.\nBachelor s degree in computer science, Engineering, or a related field.\nProven experience as a BI Engineer / Visualization developer or similar role for 6+ years.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Prototype', 'data security', 'Machine learning', 'Stored procedures', 'Apache', 'Business intelligence', 'Risk management', 'SQL']",2025-06-13 06:16:43
Software Engineer II,JPMorgan Chase Bank,2 - 14 years,Not Disclosed,['Bengaluru'],"Design, code, test, and deliver software to automate manual operational work, with a focus on Python development and AI integration.\nPartner with different application teams throughout the life cycle to understand their application infrastructure and monitoring and apply Site Reliability Principles to baseline and setup SLOs. Identify application patterns and data-driven approaches to defend and improve service level objectives.\nDevelop automated software upgrades, change management, release management, and self-healing solutions, leveraging AI technologies where applicable.\nResponsibilities extend to application deployment, change management, incident management, capacity upgrades, reporting, system integration, and essentially ensuring the availability of stable and performing platforms.\nTroubleshoot priority incidents, facilitate blameless post-mortems, and ensure permanent closure of incidents.\nExcellent communication and ability to build and maintain relationships is essential.\nThis role requires someone who is a quick learner, detail-oriented with the experience of working efficiently and effectively in a global distributed environment.\nDesigns and conducts performance tests, identifies bottlenecks, opportunities for optimization, and capacity demand.\nDefines and drives adoption of a best-in-class monitoring framework to accomplish end-to-end flow monitoring and noiseless alerting.\nFacilitates maximum speed of delivery by objectively binding to error budgets of the service.\nCoaches other team members and manages teams as needed.\nExecutes small to medium projects independently with initial direction and eventually graduates to designing and delivering projects by yourself\nLeverages technology to solve business problems by writing high quality, maintainable, and robust code following best practices in software engineering\nParticipates in triaging, examining, diagnosing, and resolving incidents and work with others to solve problems at their root\nRecognizes the toil within your role and proactively works towards eliminating it through either systems engineering or updating application code\nUnderstands observability patterns and strives to implement and improve service level indicators, objectives monitoring, and alerting solutions for optimal transparency and analysis\nRequired qualifications, capabilities, and skills\nThis role requires a wide variety of strengths and capabilities, including\nBS/BA degree or equivalent experience.\nAdvanced understanding of application monitoring stack (Metrics, Events, Traces, Alerts, Logs) and ability to visualize and setup end-to-end observability (Infrastructure and Application components).\nStrong experience in using industry-standard monitoring tools (APM, Synthetic monitoring, Splunk/ELK, Prometheus, Grafana, etc.).\nExperience in deploying applications to cloud platforms (Kubernetes, CloudFoundry preferable).\nExperience in using Continuous Integration and Continuous Deployment tools. Build and Release skills using Jenkins, Maven, Gradle, Groovy, Git, Jenkins, etc.\nExperience in working with automation tools like Ansible, Puppet, etc.\nGood SQL skills and experience on databases.\nExperience in Resiliency patterns (Scalability, infra-as-code), Self-healing, Chaos Engineering, Performance Monitoring, Continuous performance improvements.\nStrong understanding of Site Reliability Engineering concept (SLO, SLIs Error Budget).\nProficient in Python programming, with experience in AI implementation and scripting.\nKnowledge of Web Services - Apache, Tomcat, SOAP, REST.\nExperience of working in Agile Methodology.\nDemonstrated ability to conceptualize, launch, and deliver multiple IT projects on time and within budget.\nAbility to articulate to more experienced management a technical strategy in clear, concise, understandable terms.\nAbility to code in at least one programming language\nExperience maintaining a Cloud-base infrastructure\nFamiliar with site reliability concepts, principles, and practices\nFamiliar with observability such as white and black box monitoring, service level objective alerting, and telemetry collection using tools such as Grafana, Dynatrace, Prometheus, Datadog, Splunk, and others\nFamiliarity with containers or a common Server OS such as Linux and Windows\nEmerging knowledge of software, applications and technical processes within a given technical discipline (e.g., Cloud, artificial intelligence, Android, etc.)\nEmerging knowledge of continuous integration and continuous delivery tools like Jenkins, GitLab, or Terraform\nEmerging knowledge of common networking technologies\nAbility to work in a large, collaborative team and demonstrates the willingness to vocalize ideas with peers and managers\nUnderstanding of how to prioritize and adjust work plans to adapt to changes in assigned responsibilities and projects\nEagerness to participate in learning opportunities to enhance one s effectiveness in executing day-to-day project activities\nAbility to demonstrate and apply existing and new system processes, methodologies, and skills to contribute to the development of systems\nPreferred qualifications, capabilities, and skills\nGeneral knowledge of financial services industry",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Maven', 'Change management', 'Linux', 'Networking', 'Windows', 'Release management', 'Monitoring', 'SQL', 'Android', 'Python']",2025-06-13 06:16:44
Software Engineer III,JPMorgan Chase Bank,3 - 8 years,Not Disclosed,['Bengaluru'],"Are you ready to elevate your career in software engineeringJoin our dynamic team as a Python Developer, where your expertise will drive cutting-edge solutions and contribute to impactful projects. We offer unparalleled opportunities for career growth and a collaborative environment where you can thrive and make a significant impact.\nAs a Python Developer at JPMorgan Chase within the Technology and Data Science team, you will execute software solutions, design, and development, collaborating with cross-functional teams to deploy machine learning services. You will be responsible for producing architecture and design artifacts for complex applications, ensuring design constraints are met. Your role will involve contributing to the engineering community and influencing the use of leading-edge technologies.\nJob Responsibilities\nExecute software solutions, design, development, and technical troubleshooting, thinking beyond routine approaches.\nCreate secure and high-quality production code, maintaining algorithms that run synchronously with systems.\nProduce architecture and design artifacts for complex applications, ensuring design constraints are met.\nCollaborate with cross-functional teams, including Data Science partners, to design and deploy machine learning services.\nContribute to the engineering community as an advocate of firmwide frameworks, tools, and practices.\nInfluence peers and project decision-makers to consider leading-edge technologies.\nAdd to the team culture of diversity, equity, inclusion, and respect.\nRequired Qualifications, Capabilities, and Skills\nFormal training or certification on Software Engineering concepts and 3+ years applied experience.\nExperience in building complex software systems in both private and public cloud environments (AWS).\nHands-on practical experience delivering system design, application development, testing, and operational stability.\nAdvanced Python Programming Skills including Pandas, Numpy.\nProficiency with AIM algorithms.\nAdvanced knowledge of software applications and technical processes in technical disciplines (e.g., cloud, AI, ML).\nAbility to tackle design and functionality problems independently with little oversight.\nPreferred Qualifications, Capabilities, and Skills\nAdvanced skills in additional programming languages (Java).\nFamiliarity with building services and consuming data via GraphQL, REST, or gRPC and SQL.\nExperience in building and deploying machine learning models, with knowledge of the ML Lifecycle; expertise in MLOps and AIOps is an advantage.\nWorking knowledge of security best practices and compliance standards for machine learning systems.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data science', 'Machine learning', 'System design', 'Application development', 'Troubleshooting', 'Software Engineer III', 'Operations', 'Software solutions', 'SQL', 'Python']",2025-06-13 06:16:46
Cloud Operations Engineer,Oracle,2 - 7 years,Not Disclosed,['Bengaluru'],"The Company\nOracle is the world s leading provider of business software. With a presence in over 175 countries, we are one of the biggest technology companies on the planet. Were using innovative emerging technologies to tackle real-world problems today. From advancing energy efficiency to re-imagining online commerce, the work we do is not only transforming the world of business it is helping advance governments, power nonprofits, and giving billions of people the tools they need to outpace change. For more information about Oracle (NYSE:ORCL), visit us at oracle.com .\nOracle s commitment to RD is a driving factor in the development of technologies that have kept Oracle at the forefront of the computer industry. If you are passionate about advanced development and working on the next-generation large-scale distributed systems for the most popular open source database in the world, which is optimized for the cloud providing the best performance, we would like to talk with you.\nWhat you will do:\nThe HeatWave service team is responsible for the massively parallel, high performance, in-memory query accelerator. HeatWave is 6.5X faster than Amazon Redshift at half the cost, 7X faster than Snowflake at one-fifth the cost, and 1400X faster than Amazon Aurora at half the cost. It is the only cloud-native database service that combines transactions, analytics, and machine learning services into HeatWave, delivering real-time, secure analytics without the complexity, latency, and cost of ETL duplication.. This eliminates the need for complex, time-consuming, and expensive data movement and integration with a separate analytics database. The new MySQL Autopilot uses advanced machine-learning techniques to automate HeatWave, which make it easier to use and further improves performance and scalability. Join us to help further develop this amazing technology.\nThis cutting edge technology serves critical business needs, which is changing the way data transactions function, all over the world. You will make a technical impact on the world with the work you do.\nJoin a fun and flexible workplace where you ll enhance your skills and build a solid professional foundation. As a Cloud Operations engineer for Oracles Heatwave service team you will contribute to an exciting team working on one of the hottest cloud services. You will use your skills to learn how to constantly deliver and improve on these tremendous cloud services. Operations work will include troubleshooting production issues and handling requests for upgrades, patches or modifications. When not working on operations you will be working on software engineering tasks such as review of incidents to drive improvement of services, tools or runbooks to increase our reliability, scalability and reduce operational overhead through automation, training, documentation, service enhancement, or process. This position has the opportunity to leverage and learn the ins and outs of current cloud service architecture, deployment, monitoring and operational technologies. There are many useful and desirable skills which will be acquired if not already present. See below for the many cool and current technologies in play. The ideal candidate has some of the many skills, but key is the motivation and ability to learn quickly as well as a passion for an excellent customer experience. Learn more at https: / / www.mysql.com / products / heatwave /\nCareer Level - IC2\nCareer Level - IC2\nEngineer will:\nImprove monitoring, notifications, configuration of Heatwave services.\nPerform proactive service checks and monitor/triage and address incoming system/application alerts, email and phone calls to ensure appropriate priority and meeting service SLA response time.\nTriage and troubleshoot service impacting events from multiple signals including phone, email, service telemetry and alerting.\nParticipate in activities for services such as upgrades and patching.\nIdentify and work with engineering to implement opportunities for automation, signal noise reduction, recurring issues and other actions to reduce time to mitigate service impacting events and increase the productivity of cloud operations and development resources.\nCoordinate, document and track critical incidents ensuring rapid and complete issue resolution and an appropriate closed loop to customers and other key stakeholders.\nContribute to a healthy, supportive and inclusive team culture.\nProvide feedback to development teams about operations administration dashboards functionality and UIs.\nUp-skill by learning new features delivered for the service in accordance with the product roadmap.\nImprove the availability, scalability, latency, ease of use, and efficiency of service control plans and operational tooling.\nParticipate in service capacity planning and demand forecasting, software performance analysis and system tuning.\nSupport secondary Heatwave on AWS cloud service as per business requirement.\nPotentially participate in regular rotations as a central part of the 24x7 Operations team, Includes rotational work on weekends, Public Holidays, US East timezone shifts.\nNeed to be reliable in terms of working scheduled hours.\nNeed to be motivated quick learners.\nDesired skills include\nAWS-specific skills which would be a plus but are not strictly required:\nFamiliarity with AWS services (e.g.: Lambda service, Step functions, DynamoDB, AWS Session manager, CloudWatch, etc.).\nFamiliarity with OCI or equivalent Cloud services (e.g.: IAM, Compute, Load Balancer, Object Storage, Health Monitor).\nGeneral skills for working in this operational role:\nBasic understanding of serverless cloud architecture.\nFamiliarity with MySQL database, SQL query interface and general database concepts.\nExperience with Python programming, bash scripting Git.\nBasic Linux system administration knowledge and experience and familiarity with Linux troubleshooting and internals.\nFamiliarity with Networking concepts, DevOps model.\nWork productively in a fast-paced, team-oriented Agile environment.\nContribute to operational activities such as writing runbooks, troubleshooting, operations automation, and instrumentation for metrics and events.\nGood technical writing and communication skills. Engineers will need to be able to clearly write descriptions of operational issues and corrective actions for incidents.\nExperience with Agile methodology (Scrum or Kanban).\nVery strong analytical skills to identify problem root causes.\nExperience in collaborating with cross-functional teams like Development, QA, Product management, etc.\nSystematic problem-solving approach, combined with a strong sense of ownership and drive in resolving operations issues.\nExperience working under pressure to mitigate customer issues affecting service reliability, data integrity, and overall customer experience.\nMonitoring, management, analysis and troubleshooting of large-scale, distributed systems.\nBS/BE or MS/Mtech degree in Computer Science, Electrical/Hardware Engineering or related field.\n2+ years experience delivering and operating large scale, highly available distributed systems.\n2+ years of work experience as a Software, Site reliability, Operations or Customer Support engineer.",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Product management', 'Automation', 'Networking', 'Technical writing', 'MySQL', 'Scrum', 'Oracle', 'Open source', 'Analytics', 'Python']",2025-06-13 06:16:47
SOFTWARE ENGINEER III,Walmart,3 - 8 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team:\nU.S. Technology: This business closely partners with our U.S. stores and eCommerce business to serve customers by empowering associates, stores and merchants with technology innovation. rom grocery and entertainment to sporting goods and crafts, Walmart U.S. provides the deep assortment that our customers appreciate whether theyre shopping online at Walmart.com, through one of our mobile apps or shopping in a store. The focus areas include customer, stores and associates, in-store service, merchant tools, merchant data science and search ; personalization.\n\nOmni Availability org is responsible for managing ; orchestrating item, its fulfilment and inventory reservation in WM website and stores. This system is a collection of multiple high qps low latency Tier Zero systems which power WM customer journey in discovery(search, deals, item page etc) transaction and post-transaction flows. We operate at the intersection of huge Walmart assortment, millions of customers, thousands of stores, and Walmart associates - a multi-layer multi-objective problem space which has a unique impact on Global consumer base.\n\nWhat you will do:\n\nAs a Software Engineer III for Walmart , youll have the opportunity to:\nDevelop intuitive software that meets and exceeds the needs of the customer and the company.\nYou also get to collaborate with team members to develop best practices and requirements for the software.\nIn this role it would be important for you to professionally maintain all codes and create updates regularly to address the customers and companys concerns.\nYou will show your skills in analysing and testing programs/products before formal launch to ensure flawless performance.\nTroubleshooting coding problems quickly and efficiently will offer you a chance to grow your skills in a high-pace, high-impact environment.\nSoftware security is of prime importance and by developing programs that monitor sharing of private information, you will be able to add tremendous credibility to your work.\nYou will also be required to seek ways to improve the software and its effectiveness.\nAdhere to Company policies, procedures, mission, values, and standards of ethics and integrity\nWhat you will bring:\nB.E./B. Tech/MS/MCA in Computer Science or related technical field.\nMinimum 3 years of object-oriented programming experience in Java.\nExcellent computer systems fundamentals, DS/Algorithms and problem solving skills.\nHands-on experience in building web based Jxava EE services/applications and Kafka, Apache Camel, RESTful Web-Services, Spring, Hibernate, Splunk, Caching.\nExcellent organisation, communication and interpersonal skills.\nLarge scale distributed services experience, including scalability and fault tolerance.\nExposure to cloud infrastructure, such as Open Stack, Azure, GCP, or AWS\nExposure to build, CI/CD ; deployment pipelines and related technologies like Kubernetes, Docker, Jenkins etc.\nA continuous drive to explore, improve, enhance, automate and optimize systems and tools.\nExperience in systems design and distributed systems.\nExposure to SQL/NoSQL data stores like Cassandra, Elastic, Mongo etc.\nAbout Global Tech.\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. Thats what we do at Walmart Global Tech. Were a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the worlds largest retailer, delivering innovations that improve how our customers shop and empower our 2.3 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption. We are people-led and tech-empowered.\n\nFlexible, hybrid work:\n\n\n\nBenefits:\n\n.\nBelonging\n.\nAt Walmart, our vision is ""everyone included."" By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, were able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\n\nEqual Opportunity Employer:\n\nWalmart, Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting, and valuing valuing unique unique styles, experiences, identities, ideas, and opinions while being welcoming of all people.\n\nThe above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in computer science, information technology, engineering, information systems, cybersecurity, or related area and 2years experience in software engineering or related area at a technology, retail, or data-driven company.\n\nOption 2: 4 years experience in software engineering or related area at a technology, retail, or data-driven company.\nPreferred Qualifications...\nCustomer Care, Customer Service, Information Technology, Project Management, Retail Operations, Support, Technical Strategy, Troubleshooting\nPrimary Location... BLOCK- 1, PRESTIGE TECH PACIFIC PARK, SY NO. 38/1, OUTER RING ROAD KADUBEESANAHALLI, , India",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Hibernate', 'Coding', 'Project management', 'Customer service', 'Software Engineer III', 'Troubleshooting', 'Information technology', 'Distribution system', 'SQL']",2025-06-13 06:16:49
Software Engineer III,Walmart,2 - 7 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout the Team:\nAs a member of the Buybox prod Eng. group, you ll be responsible for driving site/customer impacting production incidents, with tactical/long-term solutions identified, working with cross functional teams. You ll independently handle high impact, critical software/systems monitoring issues, troubleshoot business and production issues, developing automations/tools leading to Operational excellence. As a member of the team, you ll be able to say that you work for the world s largest retailer and contribute to the development to best-in-class methodologies that impacted perception and drastically changed business as we know it.\nWhat You ll Do:\nSupporting java full stack backend application system components in a massively scalable, high performance, multi-tenant, international eCommerce platform with multiple micro-services deployed in cloud environment, root causing every reactive/proactive production issues.\nLeads and participates in medium- to large-scale, complex, cross-functional projects\nPartners with architects and development leads to come up with high level design to accelerate omni customer experience, recommending out-of-box engineering best practices.\nPro-Actively identifies areas to drive automation/speed/innovation\nTroubleshoots business and production issues by gathering information (for example, issue, impact, criticality, possible root cause); performing root cause analysis to reduce future issues; engaging support teams to assist in the resolution of issues; developing solutions; driving the development of an action plan; performing actions as designated in the plan; interpreting the results to determine further action; and completing online documentation.\nProvides support to the business by responding to user questions, concerns, and issues (for example, technical feasibility, implementation strategies); researching and identifying needed solutions; determining implementation designs; providing guidance regarding implications of new and enhanced systems; identifying short and long term solutions; and directing users to appropriate contacts for issues outside of associates domain.\nAssists in providing guidance to small groups of 5 to 6 engineers, including offshore associates, for assigned Engineering projects by proving pertinent documents, directions, examples, and timeline.\nDemonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; and building commitment for perspectives and rationales.\nModels compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation/Support of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices.\nProvides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities.\nWhat you ll Bring ...\nExpertise in Java/Spring based Rest web services and Python Experience development and its maintenance\nExpertise in AI ops to build based on LLM\nStrong Analytical thinking/troubleshooting in complex ecosystem problem solving skills\nExpertise in Application container handling using Kubernetes and Docker\nExperience in Asynchronous inter micro-service communication Messaging channels like Kafka\nExperience in Reviewing and tuning the SQL scripts for data handling, in Cassandra, Big Data\nExperience in Developing Spark jobs, Java utilities, Python scripts, for operational success\nHands on experience debugging 5xx and 4xx service errors\nAbility to work with application developers to find root cause analysis, tactical and permanent solutions to critical business and customer impacting issues\nExcellent Written and Oral communication skills\nExperience in trouble shooting production incidents on eCommerce retailer site and identify root cause, working with all business and Engineering stakeholders to bring it to closure with action items and define timeline for tactical and permanent fixes.\nAbility to drive critical production incident calls, communicating up to the point & summarizing action plans for each owners and follow-up until closure\nExperience with creating Alerting and Monitoring Dashboards, as Subject Matter Expert\nStrong in developing Innovation strategies, processes, automation, failover experience\nExposure to driving the execution of multiple business plans and projects\nExperience in managing infrastructure scaling, setup and decommissioning\nStrong Hands-on experience in both public (Azure/GCP) and private cloud experience, planning and driving efficiencies\nExperience with best in class analytics & monitoring platform like Grafana/Dynatrace/MMS/Splunk metrics and dashboards\nExperience with Machine Learning model\nAbility to take right priority decision and run the operational excellence with innovative ideas, without much guidance/supervision\nDocumenting SOPs for repetitive issues, building knowledge base articles for team s benefit\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in computer science, information technology, engineering, information systems, cybersecurity, or related area and 2years experience in software engineering or related area at a technology, retail, or data-driven company.\n\nOption 2: 4 years experience in software engineering or related area at a technology, retail, or data-driven company.\nPreferred Qualifications...\nCertification in Security+, Network+, GISF, GSEC, CISSP, or CCSP, Master s degree in Computer Science, Information Technology, Engineering, Information Systems, Cybersecurity, or related area\nPrimary Location...",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Networking', 'Debugging', 'Machine learning', 'Information technology', 'Analytics', 'Monitoring', 'SQL', 'Python']",2025-06-13 06:16:51
"Software Development Engineer II, RISC",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Want to join a team that protects and improves the buyer experience of millions Amazon customers and builds earths most customer-centric sellers daily using innovative technology including machine learning, data mining and big data analytics, cloud computing services, and highly available/scalable distributed systems that support hundreds of millions of transactions across the globe\n\nWe have an exciting opportunity with the Regulatory Intelligence, Safety, and Compliance (RISC) engineering team, to architect and build next-generation engineering systems to quickly and accurately identify and mitigate product safety issues and potential risks to the customer experience.\n\nAs a Software Development Engineer, you will work with your team of highly skilled software, data, and ML engineers to invent, design, build and manage highly scalable distributed systems that provide availability, scalability and latency guarantees. You will work with your internal customers to balance customer requirements with team requirements and help your team and business evolve, by working with LLMs and large data sets. You will be using the latest AI, AWS and industry technologies to deliver a one-stop risk identification and remediation ecosystem for Amazon, keeping our customers safe and products compliant, building the software creating the world s most trustworthy data set on everything companies and customers need to know related to the safety and compliance of products and chains.\n\nEach and every person buying, selling, or handling Amazon products will be your customer.\n\nAs a member of this growing team, you ll be able to build the groundwork and influence its direction for the years to come. Our work cuts across various disciplines from delivering an awesome user experience via great UI/UX, to building massively scalable backend systems to support the most high-traffic pages on Amazon.com, to analytical and feedback systems which give us data-driven customer insights, to using machine learning and AI to influence recommendations and marketing. If you have a passion for consumer-facing applications, and are obsessed with customer experience, we want you!\n\nIf you d like to make a real-world difference by working hard, having fun, and making history, this is the team for you!\n\n\nIn this role you will:\nHelp define the system architecture, own and implement specific components, and help shape the overall experience\nCollaborate closely with product managers, UX designers, and other SDE team members to help define the scope of the product\nTake responsibility for technical problem solving, creatively meeting product objectives, and developing best practices\nDemonstrate cross-functional resource interaction to accomplish your goals\nWrite high-quality, efficient, testable code in Java and other object-oriented languages\nDesign Amazon-scale tools to facilitate internal business\nBuild highly available, secure, and low-latency systems\nMentor other developers\nFind out what it takes to engineer systems for ""Amazon Scale""\nDesign and build microservices\nOwn and operate the systems that you build based on real-time customer data and demanding service-level agreements\nContribute to planning, design, implementation, testing, operations, and process improvement\n\nA day in the life\nHigh-level designs, cross-team alignment, long-term architectural roadmap and technical strategy, understanding the business domain and proposing solutions to address customer and business problems, helping scope and analyze product requirements, mentorship, reviewing CRs, writing high-quality code to be an example for the team. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\n3+ years of Video Games Industry (supporting title Development, Release, or Live Ops) experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Cloud computing', 'Backend', 'Coding', 'Analytical', 'Machine learning', 'Data mining', 'Internship', 'Distribution system']",2025-06-13 06:16:52
"Staff, Software Engineer (Fullstack)",Walmart,9 - 10 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team:\nWalmart s Enterprise Business Services (EBS) is a powerhouse of several exceptional teams delivering world-class technology solutions and services making a profound impact at every level of Walmart.\nAs a key part of Walmart Global Tech, our teams set the bar for operational excellence and leverage emerging technology to support millions of customers, associates, and stakeholders worldwide. Each time an associate turns on their laptop, a customer makes a purchase, a new supplier is onboarded, the company closes the books, physical and legal risk is avoided, and when we pay our associates consistently and accurately, that is EBS. Joining EBS means embarking on a journey of limitless growth, relentless innovation, and the chance to set new industry standards that shape the future of Walmart.\n\nWhat youll do:\nThrough this role you have an opportunity to design and develop intuitive software that meets and exceeds the needs of the customer and the company.\nArchitecture & Design Reviews and Code Reviews across teams.\nYou also get to collaborate with team members to develop best practices and client requirements for the software.\nEngage with Product Management and Business to drive the agenda, set your priorities and deliver awesome products.\nIn this role it would be important for you to professionally maintain all codes, best practices and create updates regularly to address the customer s and company s concerns\nYou will show your skills in analyzing and testing programs/products before formal launch to ensure flawless performance\nTroubleshooting coding problems quickly and efficiently will offer you a chance to grow your skills in a high-pace, high-impact environment.\nSoftware security is of prime importance and by developing programs that monitor sharing of private information, you will be able to add tremendous credibility to your work\nYou will also be required to seek ways to improve the software and its effectiveness.\nParticipate in hiring and build teams enabling them to be high performing agile teams.\nYou will be called upon to support the coaching and training of other team members to ensure all employees are confident in the use of software applications\n\nWhat youll bring:\nBachelor s/Masters degree in Computer Science or related technical field.\nMinimum 9 years of object-oriented programming experience in Java, Spring boot, Microservices, React, NestJS, Node.\nGood understanding of Data Structures\n8 years of experience in systems design, algorithms, and distributed systems.\nHave programming experience preferably in the Finance/ retail / ecommerce industry.\nPython knowledge would be an added advantage\nExposure to cloud infrastructure, such as Open Stack, Azure, GCP, or AWS\nExperience working in Agile Methodology\nExperience using CI/CD, tools for logging and metrics\nA continuous drive to explore, improve, enhance, automate and optimize systems and tools.\nStrong computer science fundamentals in data structures and algorithms\nExposure to information retrieval, statistics, and machine learning.\nExcellent oral and written communication skills.\nGood analytical and problem-solving skills\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in computer science, computer engineering, computer information systems, software engineering, or related area and 4 years experience in software engineering or related area.Option 2: 6 years experience in software engineering or related area.\nPreferred Qualifications...\nMaster s degree in Computer Science, Computer Engineering, Computer Information Systems, Software Engineering, or related area and 2 years experience in software engineering or related area",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Business services', 'Product management', 'Operational excellence', 'Networking', 'Coding', 'Analytical', 'Information retrieval', 'Data structures', 'Troubleshooting']",2025-06-13 06:16:54
Software Engineer III-UI,Walmart,4 - 6 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team :\nWalmart Fulfilment Services\nAt Walmart, our eCommerce success is powered by state-of-the-art supply chain capabilities, and now Marketplace sellers can benefit from our expertise to grow their businesses. With Walmart Fulfilment Services, the seller can focus on sales while Walmart expertly take care of fast shipping, seamless returns, and customer service. The Seller Simply sends his inventory to Walmart fulfilment centres, where Walmart stores the products securely and prepare them swiftly for shipping when an order is placed.About the position\nAs a UI engineer, you are responsible for technically building & maintaining a high performance, scalable, micro-service architecture based application that meet the needs of next generation Supply Chain products for Walmart Fulfilment Services. Hosted on public cloud, the Application uses a large number of technologies and middle ware for empowering the sellers to use Walmarts services to fulfil Customer orders.We are seeking a highly skilled and experienced UI Engineer to join our team. This position is focused more on building UI components and experience for the seller that enables to the seller to create Inbound orders, Manage Inventory, provide visibility on Inventory, Items and Sales.\nWhat you will do:\nAs a software engineer ( Frontend ) you will develop feature sets that involve Responsive UIs with Restful Services and ensure a seamless product experience:\nExcellent proficiency in front-end technologies Excellent proficiency in front-end technologies (React / NodeJS / TypeScript / JavaScript / HTML / CSS / React Native and related frameworks).\nSoftware development by providing engineering patterns to deliver the optimal product, including implementing design patterns. Work closely with peers and senior engineers/architects.\nPartner with UX, product owners and business SMEs to analyses the business need and provide a supportable and sustainable engineered solution. Ensure that the overall technical solution is aligned with the business needs.\nDrive the creation and modifications of the product portfolio components, identify, and engage all technical resources necessary to contribute to the solution ensure the solution is consistent with Walmart architecture, design and development standards.\nBuild reusable React components with Typescript & modular CSS, manage data on the client with Redux, and test everything with Jest.\nMeasure and resolve performance bottlenecks, using tools like Chrome DevTools, Lighthouse, Webpage test, or custom tooling.\nExperiment: This is a startup-like environment so everything can change as we experiment with new ideas.\nHack, extend and improve open-source tools/framework.\nDevelop applications using industry best practices. Adjust adopt new methodologies that provide the business with increased flexibility and agility.\nStay current with latest development tools, technology ideas, patterns and methodologies, share knowledge by clearly articulating results and ideas to key stakeholders.\nWhat you will bring :\nBachelor s degree in computer science or related discipline.\n4 - 6 years of experience in React development.\nExtensive experience building web applications using MVC frameworks (ReactJS, NodeJS) for REST like applications.\nExcellent debugging and problem-solving capability.\nWell versed in a variety of design patterns\nExperience with frontend toolings like webpack, babel, etc\nUnderstanding of frontend security and performance\nAbout Walmart Global Tech\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. Thats what we do at Walmart Global Tech. Were a team of software engineers, data scientists, cybersecurity experts and service professionalswithin the worlds leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasingtheir first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale,impact millions and reimagine the future of retail.\nFlexible, hybrid work\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approachhelps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\nBenefits\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include ahost of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\nBelonging\n.\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, were able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer:\nWalmart, Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing uniquestyles, experiences, identities, ideas and opinions while being welcoming of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in computer science, information technology, engineering, information systems, cybersecurity, or related area and 2years experience in software engineering or related area at a technology, retail, or data-driven company.\n\nOption 2: 4 years experience in software engineering or related area at a technology, retail, or data-driven company.\nPreferred Qualifications...\nCertification in Security+, Network+, GISF, GSEC, CISSP, or CCSP, Master s degree in Computer Science, Information Technology, Engineering, Information Systems, Cybersecurity, or related area\nPrimary Location... BLOCK- 1, PRESTIGE TECH PACIFIC PARK, SY NO. 38/1, OUTER RING ROAD KADUBEESANAHALLI, , India",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Computer science', 'Retail', 'Front end', 'Networking', 'Javascript', 'HTML', 'Customer service', 'Open source', 'Information technology']",2025-06-13 06:16:55
"SENIOR, SOFTWARE ENGINEER",Walmart,6 - 10 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do... Skillset: Full Stack (Java and Reactjs) Experience range: 6 - 10 years\nAbout Team:\nWalmart s Enterprise Business Services (EBS) is a powerhouse of several exceptional teams delivering world-class technology solutions and services making a profound impact at every level of Walmart.\nAs a key part of Walmart Global Tech, our teams set the bar for operational excellence and leverage emerging technology to support millions of customers, associates, and stakeholders worldwide. Each time an associate turns on their laptop, a customer makes a purchase, a new supplier is onboarded, the company closes the books, physical and legal risk is avoided, and when we pay our associates consistently and accurately, that is EBS. Joining EBS means embarking on a journey of limitless growth, relentless innovation, and the chance to set new industry standards that shape the future of Walmart.\n\nWhat youll do:\nThrough this role you have an opportunity to design and develop intuitive software that meets and exceeds the needs of the customer and the company.\nArchitecture & Design Reviews and Code Reviews across teams.\nYou also get to collaborate with team members to develop best practices and client requirements for the software.\nEngage with Product Management and Business to drive the agenda, set your priorities and deliver awesome products.\nIn this role it would be important for you to professionally maintain all codes, best practices and create updates regularly to address the customer s and company s concerns\nYou will show your skills in analysing and testing programs/products before formal launch to ensure flawless performance\nTroubleshooting coding problems quickly and efficiently will offer you a chance to grow your skills in a high-pace, high-impact environment.\nSoftware security is of prime importance and by developing programs that monitor sharing of private information, you will be able to add tremendous credibility to your work\nYou will also be required to seek ways to improve the software and its effectiveness.\nParticipate in hiring and build teams enabling them to be high performing agile teams.\nYou will be called upon to support the coaching and training of other team members to ensure all employees are confident in the use of software applications\n\nWhat youll bring:\nBachelor s/Masters degree in Computer Science or related technical field.\nMinimum 6 years of object-oriented programming experience in Java, Spring boot, Microservices, React, NestJS, Node.\nGood understanding of Data Structures\n8 years of experience in systems design, algorithms, and distributed systems.\nHave programming experience preferably in the Finance/ retail / ecommerce industry.\nPython knowledge would be an added advantage\nExposure to cloud infrastructure, such as Open Stack, Azure, GCP, or AWS\nExperience working in Agile Methodology\nExperience using CI/CD, tools for logging and metrics\nA continuous drive to explore, improve, enhance, automate and optimize systems and tools.\nStrong computer science fundamentals in data structures and algorithms\nExposure to information retrieval, statistics, and machine learning.\nExcellent oral and written communication skills.\nGood analytical and problem-solving skills\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in computer science, information technology, engineering, information systems, cybersecurity, or related area and 3years experience in software engineering or related area at a technology, retail, or data-driven company.\nOption 2: 5 years experience in software engineering or related area at a technology, retail, or data-driven company.\nPreferred Qualifications...\nCertification in Security+, GISF, CISSP, CCSP, or GSEC, Master s degree in computer science, information technology, engineering, information systems, cybersecurity, or related area and 1 year s experience leading information security or cybersecurity projects\nInformation Technology - CISCO Certification - Certification",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Retail', 'Networking', 'Coding', 'Information security', 'Analytical', 'Data structures', 'Information technology', 'Distribution system', 'Python']",2025-06-13 06:16:57
"Senior, Software Engineer",Walmart,6 - 9 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team:\nThis is the team that powers services and applications to manage Customer Orders, Trip Lifecycle, Delivery Partners Profile and Work Assignment in a friction-less, predictableway. Build reusable Saa S products and services that manage Customer accounts and Identities and power end-to-end account creation, login, session, profile and membership journey.\nWhat youll do:\nDesign ; develop highly scalable services and solve complex software systems problems by leveraging state-of-the-art technology Coding, unit testing, building high performance and scalable applications that meet the needs of millions of Walmart-International customers, in the areas of supply chain management ; Customer experience. Gain exposure to various emerging technologies used in E-commerce platforms. Provide technical direction, architecture leadership and expertise to the team. Participate in medium- to large-scale, complex, cross-functional projects by reviewing project, product and business requirements; translating requirements into technical solutions; gathering requested information (for example, design documents, product requirement); designing robust and scalable architectures; writing and developing code; conducting unit testing; communicating status and issues to team members and stakeholders; collaborating with cross functional teams; troubleshooting open issues and bug-fixes; enhancing design to prevent re-occurrences of defects; ensuring on-time delivery;\nWhat youll bring:\n6 to 9 years of total experience of which 6+ years in Backend engineering platform development. 5+ years of experience in Java technologies, Distributed systems and large-scale application development and design. Experience with a containerization technology and Microservice Well versed in CI/CD Work with Java, Multithreading, Data Structures, Algorithm, Design Patterns and develop robust high- performance and scalable applications. Extremely strong technical background with the capability of being hands-on and ability to mentor top individual technical talent\nAbout Walmart Global Tech\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. Thats what we do at Walmart Global Tech. Were a team of software engineers, data scientists, cybersecurity experts and service professionalswithin the worlds leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasingtheir first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale,impact millions and reimagine the future of retail.\nFlexible, hybrid work\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approachhelps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\nBenefits\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include ahost of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\nBelonging\n.\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, were able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer:\nWalmart, Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing uniquestyles, experiences, identities, ideas and opinions while being welcoming of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in computer science, information technology, engineering, information systems, cybersecurity, or related area and 3years experience in software engineering or related area at a technology, retail, or data-driven company.\nOption 2: 5 years experience in software engineering or related area at a technology, retail, or data-driven company.\nPreferred Qualifications...\nCertification in Security+, GISF, CISSP, CCSP, or GSEC, Master s degree in computer science, information technology, engineering, information systems, cybersecurity, or related area and 1 year s experience leading information security or cybersecurity projects\nInformation Technology - CISCO Certification - Certification Primary Location... BLOCK- 1, PRESTIGE TECH PACIFIC PARK, SY NO. 38/1, OUTER RING ROAD KADUBEESANAHALLI, , India\n\n\n",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain management', 'Multithreading', 'Networking', 'Coding', 'Information security', 'Data structures', 'Application development', 'Unit testing', 'Troubleshooting', 'Information technology']",2025-06-13 06:16:58
Linux Build - Devops Engineer,HARMAN,6 - 10 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Digital Transformation Solutions (DTS)\n.\nCombine the physical and digital, making technology a more dynamic force to solve challenges and serve humanity s needs\nWork at the convergence of cross channel UX, cloud, insightful data, IoT and mobility\nEmpower companies to create new digital business models, enter new markets, and improve customer experiences\nAbout the Role",,,,"['Computer science', 'Automation', 'DTS', 'GIT', 'Linux', 'Subversion', 'Agile', 'Release management', 'Automotive', 'Python']",2025-06-13 06:17:00
Training & Internship - Data Analytics,SSS Grameen Services,3 months duration,Unpaid,[],"This is a remote position.\n\nThere are internships and Projects for\n- Final Year University students (BBA/MBA)\n- Freshers & housewives\nPassionate students willing to learn and hone their skills are welcome to apply.\n\nRequirements\nWhat Next:\n- Apply with Resume, bonafide certificate, UID Aadhaar, Tentative area of project ( Analytics / AI / Sustainability / Cybersecurity etc.,))\nAppear for\nTechnical Screening #1 (Python or R Coding)\nTechnical Screening #2 (Coding test on ML algorithms like SVM and its implementation or similar)\nAptitude #3 (for Industry Usecases)\n\n- Technical assessment result\nPass - direct Project internship 2-3 months\n- Closure: Letter of internship, Project Report, Completion certificate (apply for respective UGC credits from your University)\n\n\nBenefits\nStipend: nil\nBenefits: Certificate of internship and Project\nMode: LIVE Virtual Remote\n",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Training', 'Technical training', 'LMS', 'Coding', 'Soft skills training', 'Data analytics', 'Internship', 'Python', 'Testing', 'Recruitment']",2025-06-13 06:17:02
Data Mining Specialist,Magic Infomedia,0 - 3 years,Not Disclosed,['New Delhi'],"We have a job opportunity for Data Mining Specialist who is responsible for helping businesses leverage data and work with stakeholders to understand the current pain points, conduct internal data audits, write audit reports, and close gaps with delivering continual data driven process improvement projects.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Email marketing', 'Process improvement', 'Data mining', 'Auditing']",2025-06-13 06:17:03
Digital Mktg Advisory New Associate,Accenture,0 - 1 years,Not Disclosed,['Gurugram'],"Skill required: Marketing Operations - Campaign Analytics & Reporting\n\n\n\n\nDesignation: Digital Mktg Advisory New Associate\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:0 to 1 years\n\n\n\nLanguage - Ability:English(International) - Proficient\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nHelp balance increased marketing complexity and diminishing marketing resources. Drive marketing performance with deep functional and technical expertise, while accelerating time-to-market and operating efficiencies at scale through Data and Technology, Next Generation Content Services, Digital Marketing Services & Customer Engagement and Media Growth Services.Role requires Digital Marketing Ads & Promotion creation/designEncompasses a set of technologies that enable the process of collecting & analyzing user behavioral activities with different marketing touch points, to reach on a web site or a mobile app with the ultimate aim of enhancing the targeted business goals. It comprises the processes and technologies that enable marketers to evaluate the success of their marketing initiatives, by measuring performance.\n\n\n\n\nWhat are we looking for\nGoogle AdsDigital MarketingDigital Marketing CampaignsCampaign ManagementCampaign OptimizationsAbility to establish strong client relationshipAbility to manage multiple stakeholdersCommitment to qualityStrong analytical skillsDetail orientation\n\n\n\nRoles and Responsibilities: In this role you are required to solve routine problems, largely through precedent and referral to general guidelines Your primary interaction is within your own team and your direct supervisor In this role you will be given detailed instructions on all tasks The decisions that you make impact your own work and are closely supervised You will be an individual contributor as a part of a team with a predetermined, narrow scope of work Please note that this role may require you to work in rotational shifts\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['digital marketing', 'sql', 'marketing', 'marketing operations', 'campaign analytics', 'email marketing', 'python', 'data analytics', 'data analysis', 'sas', 'google adwords', 'business analysis', 'machine learning', 'tableau', 'marketing analytics', 'social media marketing', 'google analytics']",2025-06-13 06:17:06
Lead Software Engineer - Fullstack,Capital One,10 - 15 years,Not Disclosed,['Bengaluru'],"Voyager (94001), India, Bangalore, Karnataka\nLead Software Engineer - Fullstack\nAt Capital One, we think big and do bigger things. We were the first company to develop and offer mass customization and personalization of credit card, auto loans and other financial services products, and we have been innovating relentlessly ever since.\nToday, we are a progressive financial services powerhouse spanning Credit Cards, Auto Loans, Savings, and Commercial Banking and at the same time a high-tech company, a scientific laboratory, and a well-recognized brand all in one. We are a passionate and entrepreneurial team with heart - a team that embraces bold ideas, fosters collaboration, and delivers world-class products and services impacting over 65 million customer accounts.\nStill founder-led by Chairman and CEO Richard Fairbank, we dare to dream, disrupt and deliver a better way for our customers, the financial industry and for each other. Our goal is simple - bring ingenuity, simplicity and humanity to an industry ripe for change.\nAt DataLabs, Capital One India, we solve fundamental business problems at scale using advanced analytics, data science and machine learning. We specialize in deriving valuable insights about various aspects of the business - including product and process design, consumer behavior, regulatory and credit risk, and much more - from large volumes of data to build cutting edge patentable products at an industrial scale.\nDataLabs is looking for a Lead Engineer to spearhead our foray into autonomous and intelligent data products that solve the full gamut of business and analytical needs. This role involves working with heads of lines of businesses in our Financial Services division to understand contemporary business needs, lead analytical teams to make impactful and actionable offers, and work with Engineering groups to lead tech design and delivery of large product builds.\nAt DataLabs, you will work in a fast paced and intellectually rigorous environment. You will apply strategic analytical and product leadership skills to major business challenges. You will have the opportunity to learn and build deep expertise in the core areas of advanced analytics, industrial scale product design, development and deployment, data science and machine learning. And you will do it all in a collaborative environment that values problem solving, encourages creativity, promotes learning, and rewards innovation.\nDataLabs prides itself on its exceptionally vibrant culture. Our Associate Development program enables us to shape amazing career and professional development opportunities for our associates. Our best-in-class Corporate Social Responsibility program has nurtured longstanding partnerships with committed organizations that make a meaningful difference to the communities around us. The enthusiastic volunteerism of our associates are the backbone of all that we do - it enables us to push the envelope of possibilities and have incredible fun along the way. We bend backwards to take care of one another through thick and thin. Our work and the people we are surrounded by are an enduring source of strength and fulfillment in our lives.\nWhat You ll Do:\nLead a portfolio of diverse technology projects and a team of developers with deep experience in large scale full stack systems, distributed microservices, and machine learning systems to create solutions that help meet regulatory needs for the company.\nShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal external technology communities, mentoring other members of the engineering community, and from time to time, be asked to code or evaluate code.\nCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment\nWork with cloud native stack, build on AWS, use technologies like Kubernetes and Serverless infrastructure\nBasic Qualifications:\nBachelor s degree in computer science or a related field\nAt least 10 years of experience in software engineering\nAt least 5 years experience with a public cloud (AWS, Microsoft Azure, Google Cloud)\nAt least 2 years of experience as a lead engineer working on a complex project\nPreferred Qualifications:\nMasters Degree\nAt least 10 years of experience in at least one of the following: Java, Scala, Python, Go, Javascript, Angular.js or Node.js\nAt least 5 years of experience in open-source frameworks\nAt least 4 years of lead engineer experience\nAt least 2 years of experience in Agile practices\nAWS Certification\n\nNo agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City s Fair Chance Act; Philadelphia s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Process design', 'Computer science', 'SAN', 'Analytical', 'Machine learning', 'Agile', 'Product design', 'Technical support', 'Financial services', 'Python']",2025-06-13 06:17:07
Management Trainee,Hitachi Energy,0 - 2 years,Not Disclosed,['Vadodara'],"We are seeking highly motivated fresh engineering graduates with a strong technical foundation to join our dynamic team. As a Management Trainee, you will work on real-world engineering challenges, apply your academic knowledge to practical scenarios, and contribute to innovative projects.\n\nHow you will make an impact\nDesign & Development: Work on CAD modeling, simulation, and prototyping of engineering systems.\nSoftware & Tools: Utilize industry-standard software such as MATLAB, AutoCAD, SolidWorks, ANSYS, or relevant programming languages (Python, C++, Java).\nSystem Integration: Assist in integrating hardware and software solutions to optimize performance.\nTesting & Validation: Perform structural, thermal, and reliability testing of components, ensuring compliance with industry standards.\nData Analytics & Optimization: Use statistical analysis and machine learning techniques to enhance system efficiency.\nTroubleshooting & Debugging: Diagnose and resolve technical issues in product design, manufacturing, or software development.\nDocumentation & Reporting: Prepare technical reports, process documentation, and engineering insights.\nYour background\nBE/ B.Tech in Electrical and Mechanical\nA min of 60% or equivalent CGPA in 10th, 12th & Graduation (accumulated score)\nProficiency in both spoken and written English language\nSolid organizational skills including attention to detail, multi tasking and ability to prioritize effectively in a demanding environment\nShadowing and working with individuals within various organizational roles, learning complex skills and inner company comprehension, including but not limited to site experience, product training and mentorship program work\nDemonstrating a high technical comprehension, including strong aspects of a conceptual and analytical mindset, proving an ability to innovate and improve processes where necessary.\nMaintaining a close and strong relationship with all colleagues and stakeholders, showing a true ability to lead and motivate others.\nProving a high-level comprehension of the companys core values of safety and integrity, taking responsibility where applicable, typically achieving discipline within a given role after approximately twelve months.for emerging technologies such as AI, IoT, Robotics, or Advanced Manufacturing.\n\n\n\nWhy Join Us?\nWork on cutting-edge engineering solutions with industry experts.\nExposure to high-impact projects in a structured learning environment.\nCareer development through mentorship, technical training, and certifications.",Industry Type: Electrical Equipment,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['Mechanical Engineering', 'Electrical Engineering']",2025-06-13 06:17:09
AI/ML Engineer,Aionos,0 - 2 years,Not Disclosed,['Gurugram'],"We are Hiring - AI/ML & Engineering Professional\n\nAbout www.aionos.ai\nAIonOS is a fast-growing AI-first venture by InterGlobe Enterprises and the Assago Group. Co-founded by Rahul Bhatia (InterGlobe and other brands) and CP Gurnani (ex-CEO & MD of Tech Mahindra), AIonOS is on a mission to bring AI@scale to enterprises.\nWe are building cutting-edge generative AI solutions that solve real-world business challenges.\nAbout the Role\nThis is a unique, full-time program where selected candidates will:\n• Collaborate closely with experienced AI engineers, product leaders and researchers\n• Be exposed to real product environments\n• Grow rapidly in a fast-paced, innovation-focused culture\n• We offer competitive pay and great benefits.\nWho Should Apply:\n• Aspiring tech professionals in their final year or recent graduates from BTech/MTech\n• Must have technical skills Python, JavaScript, C#/ .NET\n• Background in AI/ML, Data Science, Computer Science\n• 0–2 years of exp with a demonstrated interest in machine learning and applied AI\n• Eagerness to learn, experiment and build with the latest AI technologies\n• Fluent in English with excellent written and spoken communication skills\nInterested candidates can send their CVs to:\ncareer@aionos.ai\nSubject Line: Application – AI/ML trainee program\nWe would be happy to provide any further information.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AI/ML', '.Net', 'Python', 'C#', 'JavaScript']",2025-06-13 06:17:11
DE&A - AIML - Data Science - Data Science (Other) DE&A - AIML,Zensar,15 - 18 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Experience: 15+ years overall | Minimum 10 full-cycle AI/ML project implementations , including GenAI experience\nRole Summary:\nWe are seeking a AI Architect to lead strategic AI transformation initiatives. This role demands deep hands-on experience in AI, Machine Learning (ML), and Generative AI (GenAI) , along with the ability to engage directly with C-level stakeholders , align technical delivery with business objectives, and drive enterprise-wide adoption of advanced AI solutions.\nThe ideal candidate is a techno-strategic leader who can take AI/ML/GenAI projects from ideation to production building architectures, leading cross-functional teams, and ensuring regulatory and operational alignment in BFSI environments.\nKey\nConsulting & Business Alignment\nPartner with senior business and IT leadership , including CIOs, CDOs, and COOs , to identify high-impact use cases across retail banking, insurance, credit, and capital markets.\nTranslate complex BFSI challenges into technically feasible and scalable AI/ML/GenAI solutions.\nCreate strategic roadmaps, capability assessments, and PoV/PoC execution plans that align with business KPIs and regulatory needs.\nSolution Architecture & Delivery Leadership\nDesign and lead delivery of AI/ML/GenAI pipelines covering data ingestion, model training, validation, deployment, and monitoring.\nBuild and scale GenAI-based solutions like LLM-driven chatbots, intelligent document processing, RAG pipelines, summarization tools , and virtual assistants.\nArchitect cloud-native AI platforms using AWS (SageMaker, Bedrock) , Azure (ML, OpenAI) , or GCP (Vertex AI, BigQuery, LangChain) .\nDefine and implement MLOps and LLMOps frameworks for versioning, retraining, CI/CD, and production observability.\nEnsure adherence to Responsible AI principles , including explainability, bias mitigation, auditability, and regulatory compliance\nEngineering & Integration\nWork closely with data engineering teams to acquire, transform, and pipeline data from core banking systems, CRMs, claims systems, and real-time feeds.\nDesign architecture for data lakes, feature stores, and vector databases supporting AI and GenAI use cases.\nEnable seamless integration of AI capabilities into enterprise workflows, customer platforms, and decision engines via APIs and microservices.\nRequired Skills & Experience:\n15+ years of experience in AI/ML, data engineering, and cloud architecture.\nMinimum of 10 end-to-end AI/ML project implementations from use case discovery through to productionization.\nProven expertise in: (Any One)\nAI/ML frameworks : scikit-learn, XGBoost, TensorFlow, PyTorch\nGenAI/LLM platforms : OpenAI, Cohere, Mistral, LangChain, Hugging Face, vector DBs (Pinecone, FAISS, Chroma)\nCloud platforms : AWS, Azure, GCP - including AI/ML & GenAI native services\nMLOps/LLMOps tools : MLflow, Kubeflow, SageMaker Pipelines, Vertex AI Pipelines\nStrong experience with data security, governance, model risk management , and AI compliance frameworks relevant to BFSI.\nAbility to lead large cross-functional teams and engage both technical teams and senior stakeholders.\nExperience: 15+ years overall | Minimum 10 full-cycle AI/ML project implementations , including GenAI experience\nRole Summary:\nWe are seeking a AI Architect to lead strategic AI transformation initiatives. This role demands deep hands-on experience in AI, Machine Learning (ML), and Generative AI (GenAI) , along with the ability to engage directly with C-level stakeholders , align technical delivery with business objectives, and drive enterprise-wide adoption of advanced AI solutions.\nThe ideal candidate is a techno-strategic leader who can take AI/ML/GenAI projects from ideation to production building architectures, leading cross-functional teams, and ensuring regulatory and operational alignment in BFSI environments.\nKey\nConsulting & Business Alignment\nPartner with senior business and IT leadership , including CIOs, CDOs, and COOs , to identify high-impact use cases across retail banking, insurance, credit, and capital markets.\nTranslate complex BFSI challenges into technically feasible and scalable AI/ML/GenAI solutions.\nCreate strategic roadmaps, capability assessments, and PoV/PoC execution plans that align with business KPIs and regulatory needs.\nSolution Architecture & Delivery Leadership\nDesign and lead delivery of AI/ML/GenAI pipelines covering data ingestion, model training, validation, deployment, and monitoring.\nBuild and scale GenAI-based solutions like LLM-driven chatbots, intelligent document processing, RAG pipelines, summarization tools , and virtual assistants.\nArchitect cloud-native AI platforms using AWS (SageMaker, Bedrock) , Azure (ML, OpenAI) , or GCP (Vertex AI, BigQuery, LangChain) .\nDefine and implement MLOps and LLMOps frameworks for versioning, retraining, CI/CD, and production observability.\nEnsure adherence to Responsible AI principles , including explainability, bias mitigation, auditability, and regulatory compliance\nEngineering & Integration\nWork closely with data engineering teams to acquire, transform, and pipeline data from core banking systems, CRMs, claims systems, and real-time feeds.\nDesign architecture for data lakes, feature stores, and vector databases supporting AI and GenAI use cases.\nEnable seamless integration of AI capabilities into enterprise workflows, customer platforms, and decision engines via APIs and microservices.\nRequired Skills & Experience:\n15+ years of experience in AI/ML, data engineering, and cloud architecture.\nMinimum of 10 end-to-end AI/ML project implementations from use case discovery through to productionization.\nProven expertise in: (Any One)\nAI/ML frameworks : scikit-learn, XGBoost, TensorFlow, PyTorch\nGenAI/LLM platforms : OpenAI, Cohere, Mistral, LangChain, Hugging Face, vector DBs (Pinecone, FAISS, Chroma)\nCloud platforms : AWS, Azure, GCP - including AI/ML & GenAI native services\nMLOps/LLMOps tools : MLflow, Kubeflow, SageMaker Pipelines, Vertex AI Pipelines\nStrong experience with data security, governance, model risk management , and AI compliance frameworks relevant to BFSI.\nAbility to lead large cross-functional teams and engage both technical teams and senior stakeholders.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Solution architecture', 'Architect', 'Bfsi', 'data security', 'Consulting', 'Machine learning', 'Risk management', 'Operations', 'Monitoring', 'Core banking']",2025-06-13 06:17:13
"Associate Principal Engineer, Pega",Nagarro,11 - 15 years,Not Disclosed,[],"Total Experience 11+years.\nHands-on Pega development experience.\nExperience working with Pega Smart Investigate framework.\nStrong understanding of Pega s BPM architecture, rules engine, and integrations.\nFamiliar with DevOps tools: Jenkins, Git, Docker, Kubernetes.\nStrong scripting and troubleshooting skills for deployment automation.\nGood knowledge of process modeling, rule management, and performance tuning.\nExcellent communication and collaboration skills.\nWillingness to travel for short-term client engagements when required.\nExcellent verbal & written communication; ability to guide discussions and document technical decisions.\nRESPONSIBILITIES:\nDesign, develop, and implement scalable Pega BPM solutions using Pega PRPC and Smart Investigate.\nWork on Pega UI, case management, business rules, and reporting features.\nIntegrate Pega with databases, REST/SOAP web services, and third-party systems.\nConduct code reviews and ensure adherence to Pega best practices and guardrails.\nCollaborate with business and technical teams to translate requirements into Pega functionality.\nDevelop reusable components, utilities, and accelerators to enhance productivity.\nParticipate in pre-sales activities like RFPs, demos, POCs, and technical solutioning.\nAutomate deployments using Jenkins, Git, and Pega Deployment Manager.\nStay updated on Pega platform upgrades, new features, and BPM trends.\n\n\nBachelor s or master s degree in computer science, Information Technology, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'GIT', 'Presales', 'business rules', 'Pega', 'Troubleshooting', 'Information technology', 'Bpm']",2025-06-13 06:17:14
"Principal, Infrastructure Engineering",Fiserv,17 - 20 years,Not Disclosed,['Pune'],"As an Enterprise Infrastructure Architect, you will be a part of the Fiserv s Technical Services team responsible for designing and implementing large-scale reliable platforms and applications across Cloud and On-Premises. working closely with internal technology teams, business teams, peer infrastructure & application architects. The Enterprise Infrastructure Architect will lead the technical effort to deliver infrastructure solutions that are technically sound, resilient to failure, operationally supportable, and meets the requirements of the business. You will be tasked with being the subject matter expert for our cloud and on-premises environments and be part of a team that builds and supports new applications. Delivery of the infrastructure solution requires a variety of skills, such as: cloud architecture, network, security, AI/ML and hands-on configuration of the cloud and On-Premises infrastructure. A high degree of collaboration with development and other teams is required to provide right level of agility within a defined delivery process.\nThe role is required to be performed from Fiserv India offices providing overlap with US working hours for all 5 working days in a week.\nWhat you will do:\nDrive the technical roadmap, architecture, design, and solution architecture for enterprise application in Multi-Cloud and On-Premises environments.\nLead the assessment, gap analysis, process improvement, and design of the end-state operating model for the platform.\nDesign, build, and maintain platforms that support Cloud infrastructure, DevOps pipelines, and tools to manage various Monolithic and Microservices-based enterprise applications.\nCollaborate with cross-functional project teams to implement highly available infrastructure solutions for a multitude of cloud technologies.\nDevelop and document comprehensive architecture blueprints outlining the configuration, integration, and implementation details.\nResearch, evaluate, and recommend appropriate On-Prem & Cloud technologies, tools, and platforms to enhance enterprise capabilities.\nEstablish best practices and the development/implementation of an enterprise CI/CD roadmap\nStay updated with industry trends and emerging technologies to make informed decisions.\nImplement robust security measures to safeguard infrastructure systems (Cloud & On-Premises), including access controls, encryption, and vulnerability assessments.\nCollaborate with software developers, network engineers, and other IT professionals to ensure seamless integration of systems with other enterprise applications and services.\nPerform capacity planning to anticipate future infrastructure requirements.\nDesign scalable solutions that can handle increasing workloads efficiently.\nCreate detailed technical documentation, including system configurations, procedures, and troubleshooting guides.\nCollaborate effectively with stakeholders, project managers, and other team members to achieve project goals.\nProvide technical leadership and guidance to the team members that need the support.\nParticipate in automation initiatives to achieve operational maturity.\nUse the core infrastructure engineering principles of change management, monitoring, emergency response, capacity planning, and production readiness reviews to run the infrastructure.\nPartner with security engineers and develop plans and automation to aggressively and safely respond to new risks and remediate security vulnerabilities.\nTechnology Roadmap: Develop a technology roadmap that outlines the evolution and growth of the infrastructure to meet current and future business needs.\nIntegration Planning: Plan the integration of new technologies into existing systems, considering compatibility, scalability, and security requirements.\nSecurity and Compliance: Collaborate with cybersecurity professionals to ensure that infrastructure designs adhere to security best practices and regulatory compliance.\nSolution Recommendations: Propose tailored solutions and recommendations to address clients infrastructure challenges, considering technology, budget, and business requirements.\nImplementation Support: Provide support during the implementation phase, working with internal teams or external vendors to ensure successful deployment of recommended solutions.\nWhat you will need to have:\nAn Enterprise Infrastructure Architect needs to have the following key skills/behaviors:\n17+ years of extensive experience in designing and implementing enterprise infrastructure solutions.\nStrong proficiency in both Linux distributions (e.g., Red Hat Enterprise Linux, Ubuntu Server) and Windows server platforms\nIn-depth knowledge of system and network administration, virtualization, and cloud technologies.\nExperience in AWS/Azure/GCP managed services and infrastructure management\nExperience with configuration management tools (e.g., Ansible, Puppet) and containerization technologies (e.g., Docker, Kubernetes).\nNeeds to have one OS Platform Certification, Red Hat Certified Engineer (RHCE) or Red Hat Certified Architect (RHCA) or Linux Professional Institute Certification (LPIC) or MCSA Professional certification and a Cloud associate certification, AWS/Azure/GCP Certified Solutions Architect or similar cloud certifications.\nFamiliarity with scripting/programming languages (e.g., Bash, Python, PowerShell, Ansible, Terraform) for automation tasks.\nShould have solid experience in cloud-based AI and machine learning services offered by major providers such as AWS, Azure, and Google Cloud Platform\nExperience with CI/CD toolset (Gitlab, Harness, Jenkins, Github)\nStrong understanding of the OSI Model and how it relates to cloud computing.\nExpert level in managing the lifecycle of IaaS VMs - troubleshoot VM boot issues, Backup/Restore VM and Disks (Snapshot and Recovery Vault), Imaging, Resizing and Scaling VMs\nExceptional analytical skills and the ability to see the connections between layers of business operations.\nStrong communication skills and the ability to convey complex technical concepts to non-technical stakeholders.\nWhat would be great to have:\nAWS/Azure/GCP Certifications: Solutions Architect Professional, or DevOps Engineer Professional.\nCloud Security Posture/Governance/Entitlement management & Network Security knowledge\nBackground in fintech or similar domains.\nFamiliarity with agile and DevOps cultures.\nThank you for considering employment with Fiserv. Please:\nApply using your legal name\nComplete the step-by-step profile and attach your resume (either is acceptable, both are preferable).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Network administration', 'Linux', 'Configuration management', 'Agile', 'Network security', 'Infrastructure Architect', 'Windows', 'Troubleshooting', 'Python']",2025-06-13 06:17:16
Sr. Principal Software Engineer,Morningstar,12 - 18 years,Not Disclosed,"['Thane', 'Navi Mumbai', 'Mumbai (All Areas)']","Position Title: Senior Principal Software Engineer\nThe Area: Morningstar Data for equities provides comprehensive coverage of global stock markets from as early as 1975. We are continuously broadening our coverage and creating new products to help clients prepare for regulatory changes and other industry shifts. Our data features proprietary statistics and is developed using stringent quality screens to ensure accuracy. From APIs to data feeds, our solutions are delivered quickly to help institutions meet a broad range of functions. The Role: In this role, you will collaborate with technology manager, scrum Master, functional experts, and developers to build technology solutions for Morningstar`s Equity applications. The team is looking for forward-thinking problem solvers who thrive in a fastpaced environment. The Lead is responsible in mentoring the team members, provide guidance and opportunities for the team members to expand their capabilities and skills. They will have to coordinate and work with the members in a global team. We are looking for an individual that can apply discipline, create solid software products.\n\nResponsibilities:\n• Design & develop web and enterprise solutions to be flexible, scalable & extensible.\n• Improve complex data flow, data structures and db design to move to next platform.\n• Enforce good agile practices like test driven development, Continuous Integration.\n• Hands-on development will be an integral part of the responsibilities.\n• Develop areas of continuous and automated deployment.\n• Introduce and follow good development practices, innovative frameworks and technology solutions that help business move faster.\n• Follow best practices like estimation, planning, reporting and improvement brought to processes in every day work.\n• Analyses and reviews system requirements. Uses requirement and other design documents to gain overall understanding of the functionality of the new or enhanced application.\n• Participate actively in the design, architecture and build phases, to aim at producing high quality deliverables.\n• Provide recommendations on product and development environment improvements.\n\nRequirements:\nThese are the most important skills, qualities, etc. that we’d like for this role.\n• Minimum 12 Years of experience\n• Bachelor of Science in Computer Science, Engineering, or equivalent.\n• At least 2+ years as a software Lead/Architect\n• Demonstrated familiarity with AI-powered assistants (e.g., GitHub Copilot, ChatGPT) for code generation, debugging, and/or other technical tasks.\n• Hands-on in Java 8, Adv Java, Spring Framework\n• Strong knowledge and hands-on on micro-services based architecture.\n• Very Strong knowledge of databases and hands on MS SQL/MySQL/PostgreSQL or NoSQL DB DynamoDB/MongoDB.\n• Experience with building REST based APIs.\n• Experience in analysis, design, coding and implementation of large-scale, n-tier Java based platforms.\n• Knowledge of any JavaScript framework like Vue, Angular JS (version >2)/ NodeJS etc.\n• Be aware of activity in the open source world. Contributing back to open source is a big plus.\n• Distributed computing, with experience in cloud computing (Amazon Web Services platform and associated technologies)\n• Experience on agile practices\n• Experience with modern development practices in areas of Product design, Requirement Analysis, Test Driven Development, Automation & Unit Testing, in a product development environment.\n• Excellent listening, written and verbal communication skills. Good to Have: • Machine Learning knowledge.\n• Exposure to Capital Market domain preferred (Indexes, Equity etc.) Morningstar is an equal opportunity employer",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java Fullstack', 'Java', 'Vue.Js', 'Java Spring Boot', 'Spring Boot', 'React.Js', 'AWS', 'Angular']",2025-06-13 06:17:17
Data Scientist ( Developer ),Mobile Programming,4 - 7 years,Not Disclosed,['Hyderabad'],"Candidate Skill:\nTechnical Skills -\nPython | R | AI/ML | TensorFlow | Keras | PyTorch | Scikit-learn | SQL | Data Visualization | Deep Learning | Cloud Platforms (AWS, Azure, GCP) | Docker | Kubernetes\nWe are looking for a talented Data Scientist (Developer) to join our team in Hyderabad. The ideal candidate will have a strong background in AI/ML, with hands-on experience in developing data-driven solutions.\nYou will collaborate with cross-functional teams to build and deploy machine learning models that solve real-world problems, improve business processes, and drive decision-making.\nKey Responsibilities:\nDevelop machine learning models and algorithms to extract insights and make data-driven decisions.\nImplement advanced data analytics and machine learning models using AI/ML techniques.\nWork closely with product and business teams to understand business requirements and translate them into machine learning solutions.\nCollaborate with data engineers to collect, clean, and process large datasets.\nPerform exploratory data analysis (EDA) to identify trends, patterns, and anomalies in data.\nDesign and implement data pipelines to automate the processing and analysis of data.\nTest and validate models to ensure they perform optimally in production.\nOptimize existing models for improved performance and scalability.\nCommunicate results to non-technical stakeholders through clear visualizations and reports.\nStay up-to-date with the latest developments in the field of AI/ML.\nRequired Skills and Qualifications:\n4-7 years of experience in Data Science, with a minimum of 3 years in AI/ML development.\nProficiency in programming languages such as Python and R for building machine learning models.\nStrong understanding of machine learning algorithms, including supervised and unsupervised learning, deep learning, and reinforcement learning.\nHands-on experience with AI/ML frameworks such as TensorFlow, Keras, PyTorch, or Scikit-learn.\nExperience with data wrangling, feature engineering, and model evaluation techniques.\nFamiliarity with data visualization tools such as Matplotlib, Seaborn, or Tableau.\nStrong knowledge of SQL for querying large datasets.\nFamiliarity with cloud platforms (e.g., AWS, Azure, GCP) and tools for model deployment and scaling.\nExperience with Docker and Kubernetes for model deployment is a plus.\nExcellent problem-solving skills and ability to work in a fast-paced environment.\nStrong communication skills to present technical concepts to non-technical stakeholders.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure', 'Scikit-learn', 'machine learning', 'artificial intelligence', 'Deep Learning', 'SQL', 'R', 'PyTorch', 'Docker', 'GCP', 'Data Visualization', 'Keras', 'AWS', 'Python', 'TensorFlow', 'Kubernetes']",2025-06-13 06:17:19
Senior Data Scientist,Virtana Corp,5 - 10 years,Not Disclosed,"['Pune', 'Chennai']","Position Overview:\nWe are seeking a Senior Data Scientist Engineer with experience bringing highly scalable enterprise SaaS applications to market. This is a uniquely impactful opportunity to help drive our business forward and directly contribute to long-term growth at Virtana.\nIf you thrive in a fast-paced environment, take initiative, embrace proactivity and collaboration, and you re seeking an environment for continuous learning and improvement, we d love to hear from you!\nVirtana is a remote first work environment so you ll be able to work from the comfort of your home while collaborating with teammates on a variety of connectivity tools and technologies.\nRole Responsibilities:\nResearch and test machine learning approaches for analyzing large-scale distributed computing applications.\nDevelop production-ready implementations of proposed solutions across different models AI and ML algorithms, including testing on live customer data to improve accuracy, efficacy, and robustness\nWork closely with other functional teams to integrate implemented systems into the SaaS platform\nSuggest innovative and creative concepts and ideas that would improve the overall platform.\nJob Location - Pune, Chennai or Remote\nQualifications:\nThe ideal candidate must have the following qualifications:\n6 + years experience in practical implementation and deployment of large customer-facing ML based systems.\nMS or M Tech (preferred) in applied mathematics/statistics; CS or Engineering disciplines are acceptable but must have with strong quantitative and applied mathematical skills\nIn-depth working, beyond coursework, familiarity with classical and current ML techniques, both supervised and unsupervised learning techniques and algorithms\nImplementation experiences and deep knowledge of Classification, Time Series Analysis, Pattern Recognition, Reinforcement Learning, Deep Learning, Dynamic Programming and Optimization\nExperience in working on modeling graph structures related to spatiotemporal systems\nProgramming skills in Python is a must\nExperience in understanding and usage of LLM models and Prompt engineering is preferred.\nExperience in developing and deploying on cloud (AWS or Google or Azure)\nGood verbal and written communication skills\nFamiliarity with well-known ML frameworks such as Pandas, Keras, TensorFlow\nAbout Virtana:\nVirtana delivers the industry s only unified software multi-cloud management platform that allows organizations to monitor infrastructure, de-risk cloud migrations, and reduce cloud costs by 25% or more.\nOver 200 Global 2000 enterprise customers, such as AstraZeneca, Dell, Salesforce, Geico, Costco, Nasdaq, and Boeing, have valued Virtana s software solutions for over a decade.\nOur modular platform for hybrid IT digital operations includes Infrastructure Performance Monitoring and Management (IPM), Artificial Intelligence for IT Operations (AIOps), Cloud Cost Management (Fin Ops), and Workload Placement Readiness Solutions. Virtana is simplifying the complexity of hybrid IT environments with a single cloud-agnostic platform across all the categories listed above. The $30B IT Operations Management (ITOM) Software market is ripe for disruption, and Virtana is uniquely positioned for success.\nCompany Profitable Growth and Recognition\nIn FY2023 (Fiscal year ending January 2023), Virtana earned:\nBest CEO, Best CEO for Women, and Best CEO for Diversity by Comparably\nTwo years in a row YoY Profitable Annual Recurring Revenue (ARR) Growth\nTwo consecutive years of +EBITDA, 78% YoY EBITDA growth, or 20% of Revenue\nPositive Cash Flow, 171% YoY cash flow growth\n\nYou can schedule with us through Calendly at https: / / calendly.com / bimla-dhirayan / zoom-meeting-virtana",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Usage', 'Time series analysis', 'Artificial Intelligence', 'IT operations management', 'Machine learning', 'Cloud', 'Cash flow', 'Pattern recognition', 'Performance monitoring', 'Python']",2025-06-13 06:17:21
Product Manager - Artificial Intelligence,IBM,5 - 10 years,Not Disclosed,['Bengaluru'],"The IBM Z organization is looking for an AI on Z Product Manager to work with topics including open source packages, Linux on Z strategy, and technical enablement for business scale out of AI. An ideal candidate would be experienced with AI concepts, familiar with Linux on Z, and comfortable in technology discussions. This candidate should be comfortable learning & teaching new skills, a good listener for client and sales discussions, and be able to address analytical business questions with research and interactions with both development and clients.This role will include some investigation into country or industry-based regulatory requirements coming in the future related to AI model prediction accuracy, governance and drift over time.\n\n\n\nRequired education\nMaster's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5+ years of either Product Management or Product Development experience, with responsibility for researching feature value, developing features, and managing the lifecycle of offerings, preferably on IBM Z or IBM LinuxONE.\nExpertise in Analytics, including defining and measuring key performance indicators (KPIs).\nAI experience to build from – The experience could be a wide range of scenarios, as a Db2 SQL user, analytics professional, a machine learning or AI advocate, etc.\nGood People and Project Management skills, as this person will need to adjust quickly to get best results from unique or unexpected scenarios.\nSince the AI space is new, we expect to be iterating products with sponsor users, knowing that some products will need to evolve from their initial MVP. Candidates should be good communicators in order to keep teams in sync across the various business milestones.ng manager and Recruiter should collaborate to create the relevant verbiage.\nCandidates should have exposure to industry and country based regulations and regulatory bodies. The candidate should be comfortable documenting model performance findings, as well as evaluating potential risks of model inaccuracies.",Industry Type: IT Services & Consulting,Department: Product Management,"Employment Type: Full Time, Permanent","['artificial intelligence', 'product management', 'product development', 'linux', 'machine learning', 'rexx', 'dbms', 'sales', 'sql', 'ispf', 'tso', 'sdsf', 'jcl', 'vsam', 'ca', 'zos', 'mainframes', 'project management', 'management skills', 'mvs', 'spufi', 'cobol', 'xpeditor', 'cics', 'agile', 'file-aid']",2025-06-13 06:17:23
Principle Data Scientist,Hilabs,5 - 7 years,Not Disclosed,['Pune'],"The HiLabs Story\nHiLabs is a leading provider of AI-powered solutions to clean dirty data, unlocking its hidden potential for healthcare transformation. HiLabs is committed to transforming the healthcare industry through innovation, collaboration, and a relentless focus on improving patient outcomes.\nHiLabs Team\nMultidisciplinary industry leaders\nHealthcare domain experts\nAI/ML and data science experts\nProfessionals hailing from the worlds best universities, business schools, and engineering institutes including Harvard, Yale, Carnegie Mellon, Duke, Georgia Tech, Indian Institute of Management (IIM), and Indian Institute of Technology (IIT).\nJob Title : Lead/Senior Data Scientist\n\nJob Location : Pune\n\nJob summary: HiLabs is looking for highly motivated and skilled Lead/Sr. Data Scientist focused on the application of emerging technologies. The candidates must be well versed with Python, Scala, Spark, SQL and AWS platform. The individuals who will join the new Evolutionary Platform team should be continually striving to advance AI/ML excellence and technology innovation. The mission is to power the next generation of the digital product and services through innovation, collaboration, and transparency. You will be a technology leader and doer who enjoys working in a dynamic, fast- paced environment.\nResponsibilities:\nLeverage AI/ML techniques and solutions to identify and mathematically interpret complex healthcare problems.\nFull-stack development of data pipelines involving Big Data.\nDesign and development of robust application/data pipelines using Python, Scala, Spark, and SQL\nLead a team of Data Scientists, developers as well as clinicians to strategize, design and evaluate AI based solutions to healthcare problems.\nIncrease efficiency and improve the quality of solutions offered.\nManaging the complete ETL pipeline development process from conception to deployment\nCollaborating with and guiding the team on writing, building, and deployment of data software\nFollowing best design and development practices to ensure high quality code.\nDesign, build and maintain efficient, secure, reusable, and reliable code\nPerform code reviews, testing, and debugging\nDesired Profile:\nBachelors or Master s degrees in computer science, Mathematics, or any other quantitative discipline from Premium/Tier 1 institutions\n5 to 7 years of experience in developing robust ETL data pipelines and implementing advanced AI/ML algorithms (GenAI is a plus).\nStrong experience working with technologies like Python, Scala, Spark, Apache Solr, MySQL, Airflow, AWS etc.\nExperience working with Relational databases like MySQL, SQLServer, Oracle etc.\nGood understanding of large system architecture and design\nUnderstands the core concepts of Machine Learning and the math behind it.\nExperience working in AWS/Azure cloud environment\nExperience using Version Control tools such as Bitbucket/GIT code repository\nExperience using tools like Maven/Jenkins, JIRA\nExperience working in an Agile software delivery environment, with exposure to continuous integration and continuous delivery tools\nGreat collaboration and interpersonal skills\nAbility to work with team members and lead by example in code, feature development, and knowledge sharing\nHiLabs is an equal opportunity employer (EOE). No job applicant or employee shall receive less favorable treatment or be disadvantaged because of their gender, marital or family status, color, race, ethnic origin, religion, disability, or age; nor be subject to less favorable treatment or be disadvantaged on any other basis prohibited by applicable law.\nHiLabs is proud to be an equal opportunity workplace dedicated to pursuing and hiring a diverse and inclusive workforce to support individual growth and superior business results.\n\nHiLabs Total Rewards\nCompetitive Salary, Accelerated Incentive Policies, H1B sponsorship, Comprehensive benefits package that includes ESOPs, financial contribution for your ongoing professional and personal development, medical coverage for you and your loved ones, 401k, PTOs & a collaborative working environment, Smart mentorship, and highly qualified multidisciplinary, incredibly talented professionals from highly renowned and accredited medical schools, business schools, and engineering institutes.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['System architecture', 'Maven', 'MySQL', 'Debugging', 'Agile', 'h1b', 'Oracle', 'Apache', 'SQL', 'Python']",2025-06-13 06:17:24
Senior Data Scientist,Codetru Software Solutions,8 - 12 years,Not Disclosed,['Hyderabad'],"Senior Data ScientistLocation: Hyderabad, IndiaExperience: 8-10 YearsWe are seeking a highly motivated, driven, and experienced Senior Data Scientist to join our dynamic team. As a go-getter with a passion for uncovering insights from complex data, you will play a pivotal role in shaping our data strategy and driving business decisions. The ideal candidate is a proactive problem-solver who thrives in a fast-paced environment and possesses a deep understanding of the entire data lifecycle, from extraction to model deployment.\n\nNote: This is purely a Technical role not Managerial.\n\nKey Responsibilities\nLead and execute end-to-end data science projects, from conception and data collection to model building and delivering actionable insights.\nDesign, build, and maintain robust and scalable ETL pipelines to process large volumes of structured and unstructured data from our data lake.\nUtilize advanced SQL and Python (Pandas, NumPy) for data extraction, manipulation, and in-depth analysis to identify critical trends, patterns, and opportunities.\nDevelop and implement a variety of machine learning algorithms, such as regression, classification, clustering, and forecasting models, to solve key business challenges.\nCreate compelling and intuitive data visualizations and dashboards using tools like Tableau or Power BI to communicate complex findings to both technical and non-technical stakeholders.\nMentor junior data scientists and contribute to the team's technical growth and best practices.\n\nMust-Have Qualifications & Skills\n\nExperience: A minimum of 8-10 years of hands-on experience in a data science or related role.\nSQL and Visualization: Expert-level proficiency in SQL for complex querying and proven experience with data visualization tools such as Tableau, Power BI, or Looker.\nData Engineering: Strong, hands-on experience building and managing ETL processes and working extensively within a data lake environment.\nPython and Data Analysis: Mastery of Python and its core data science libraries, especially Pandas, for data wrangling, exploration, and identifying hidden patterns.\nMachine Learning: In-depth theoretical knowledge and practical application of various ML algorithms, including supervised and unsupervised learning techniques. A portfolio of successfully deployed models is a strong plus.\nAttitude: A proactive, self-starting go-getter with excellent problem-solving skills and the drive to take ownership of projects from start to finish.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pandas', 'Fast Api', 'Machine Learning', 'Numpy', 'Python', 'Power Bi', 'Tableau', 'SQL', 'Flask']",2025-06-13 06:17:27
Artificial Intelligence Engineer,Wissen Technology,4 - 9 years,Not Disclosed,"['Bengaluru', 'Mumbai (All Areas)']","Roles and Responsibilities\nDesign, develop, test, and deploy AI models using Python and deep learning frameworks such as TensorFlow, PyTorch, or scikit-learn.\nCollaborate with cross-functional teams to integrate AI solutions into existing systems and applications.\nDevelop REST APIs using Flask or Django for data processing and integration purposes.\nTroubleshoot issues related to model performance, data quality, and system architecture.\nStay up-to-date with industry trends and advancements in artificial intelligence research.\nDesired Candidate Profile\n4-9 years of experience in developing AI models using Python programming language.\nStrong proficiency in at least two deep learning frameworks (TensorFlow, PyTorch, or scikit-learn).\nExperience working on projects involving natural language processing (NLP), computer vision, or other areas of machine learning.\nBachelor's degree in Any Specialization (B.Tech/B.E.).\nHands-on experience with Pandas library for data manipulation and analysis.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Rest Api Development', 'Tensorflow', 'Pytorch', 'Pandas', 'Scikit-Learn']",2025-06-13 06:17:28
Test Automation Software Engineer - CDOT,National Institute for Smart Government (NISG),3 - 6 years,8-12 Lacs P.A.,['Bengaluru'],"Job Description\nName of the Post: Test Automation Software Engineer\nNo of Posts: 1\nLocation: Bangalore\nRoles and Responsibilities:\nDesign, Develop and Build Test Automation frameworks\nDesign and Development of Test suites for the 4G and 5G system.\nWrite, design, and execute automated tests by creating scripts that run testing functions automatically\nMaximize test coverage for the most critical features of the system\nDetermine the priority for test scenarios and create execution plans to implement these scenarios\n\nSkills and Abilities:\n\nComprehensive knowledge in the field of software development.\nKnowledge of Artificial Intelligence/Machine Language\nKnowledge on Cloud computing, CI/CD tools like Jekins\nUnderstanding the Industry Standards: 3gpp standards, Tec standards\nModerate Knowledge on simulators: 2G/3G/4G node Simulators, UE simulators\nModerate knowledge on Software testing tools: Wire shark, Scripting languages.\nLinux administration and Linux networking\nProgramming expertise: High level languages C,C++, Perl, Java and Python\nModerate knowledge on Databases: SQL, ORACLE etc.\nKnowledge on Android OS to Support Mobile-Telephone Automations\nFull stack developers for Test Automation of Validation Test Cases Execution, Configuration, monitoring and maintaining the Test Environment\n\nMinimum Qualifications:\n\nB.E/B.Tech in Computer Science & Engineering or equivalent degree from a recognized college/university\nMinimum 3 years of professional work experience post qualification.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Software Development', '4G', '3G', 'Artificial Intelligence', 'Machine Learning', '2G']",2025-06-13 06:17:30
Artificial Intelligence Engineer,Indovation Lab,5 - 10 years,Not Disclosed,"['Bengaluru', 'Delhi / NCR']","The Opportunity:\nWe are seeking a talented and experienced Senior AI Engineer to play a pivotal role in developing and enhancing our conversational AI capabilities across multiple modalities (text, voice/telephony) and building intelligent agents to drive workflow automation. You will work closely with our Director of AI Engineering, product managers, and other engineering teams to bring our AI vision to life, primarily focusing on fine-tuning off-the-shelf Large Language Models (LLMs) and developing agentic systems for end-users (members, CBO staff, healthcare providers).\nThis is a unique opportunity to apply your AI expertise to solve meaningful problems in the social and healthcare space, directly impacting people's lives.\nWhat You'll Do:\nConversational AI Development & Fine-Tuning (Text & Voice):\nLead the fine-tuning, evaluation, and deployment of pre-trained LLMs (e.g., Gemini, GPT series, open-source models) to create natural, empathetic, and effective conversational experiences for various user interactions via text-based channels (chat, SMS) and voice-based telephonic systems (IVR chatbots).\nDevelop and implement strategies for data collection, preparation, and augmentation to support model fine-tuning and continuous improvement for both text and voice modalities.\nDesign and implement robust evaluation frameworks to measure conversational AI performance, including metrics for accuracy, fluency, empathy, task completion, and call handling efficiency for telephonic agents.\nWork on prompt engineering, context management, and dialogue flow design to optimize conversational AI interactions across channels.\nIntegrate and manage speech-to-text (STT) and text-to-speech (TTS) services for telephonic AI solutions.\nAgentic System Development:\nDesign, build, and deploy AI agents that can reason, make decisions, and take actions to automate and optimize key workflows within the platform (e.g., intelligent referral initiation, proactive follow-ups, task management assistance).\nDevelop agents capable of interacting with internal platform APIs, external data sources, and potentially third-party tools to achieve their goals.\nExplore and implement techniques for agent planning, tool usage, and multi-step reasoning.\nCollaboration & Technical Leadership:\nCollaborate closely with the Director of AI Engineering to define AI strategy, architecture, and technical roadmap for conversational AI and agentic systems.\nPartner with Product Managers to understand user needs and translate them into technical requirements for AI features.\nWork with platform and application engineers to integrate AI models and agents into the broader ecosystem.\nMentor junior engineers and contribute to building a strong AI engineering culture.\nStay up-to-date with the latest advancements in LLMs, conversational AI (text and voice), agent-based systems, and MLOps.\nMLOps & Productionization:\nContribute to the development and maintenance of our MLOps infrastructure for training, deploying, monitoring, and iterating on AI models and agents in production.\nEnsure AI systems are scalable, reliable, and maintainable.\nBack-end Development:\nSolid understanding of back-end development principles and experience building or integrating with APIs (e.g., RESTful services) to connect AI models and agents with broader application systems.\nFamiliarity with database technologies (SQL and/or NoSQL) and practical experience in how AI systems interact with data storage and retrieval for training, inference, and logging.\nWhat You'll Bring:\nEducation: Bachelor's or Master's degree in Computer Science, Artificial Intelligence, Machine Learning, or a related field.\nExperience:\n5+ years of hands-on experience in AI/ML engineering, with a significant focus on Natural Language Processing (NLP) and conversational AI.\nDemonstrable experience developing and deploying AI-powered telephonic chatbots or Interactive Voice Response (IVR) systems, including integration with STT/TTS technologies.\nProven experience fine-tuning and deploying Large Language Models (LLMs) for specific tasks and domains. Strong understanding of model architectures, training techniques, and evaluation metrics.\nDemonstrable experience designing and building AI agents or systems that exhibit autonomous behavior, decision-making, and/or tool usage.\nProficiency in Python and common AI/ML frameworks (e.g., TensorFlow, PyTorch, Hugging Face Transformers, LangChain, LlamaIndex).\nExperience with cloud platforms (GCP preferred, AWS/Azure acceptable) and their AI/ML services (e.g., Vertex AI, SageMaker, cloud telephony APIs).\nSolid understanding of MLOps principles and experience with tools for model deployment, monitoring, and CI/CD for ML.\nSkills:\nStrong analytical and problem-solving skills.\nExcellent communication and collaboration abilities.\nAbility to translate complex technical concepts to non-technical stakeholders.\nProactive, self-starter with a passion for building impactful AI solutions.\nNice to Haves:\nExperience working in the healthcare or social care domain.\nFamiliarity with data privacy and security considerations in regulated environments (e.g., HIPAA).\nExperience with specific telephony platforms or APIs (e.g., Twilio, Vonage, Google Dialogflow CX).\nContributions to open-source AI/ML projects.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Conversational Ai', 'Python', 'TTS', 'NLP', 'GCP', 'SST', 'Artificial Intelligence', 'Natural Language Processing', 'Chatbot Development', 'Dialogflow']",2025-06-13 06:17:31
Full Stack Engineer,Axtria,4 - 9 years,Not Disclosed,"['Noida', 'Hyderabad', 'Bengaluru']","Axtria: -Axtria is a global provider of award-winning cloud software and data analytics to the life sciences industry. Axtria enables life sciences organizations to transform the product commercialization journey and deliver much-improved healthcare outcomes for patients worldwide. We are acutely aware that our work impacts millions of people and are incredibly passionate about the improvement we can bring to patients lives.\nOur focus is on delivering solutions that help pharmaceutical, medical device, and diagnostics companies complete the journey from data to insights to action and get superior returns from their investments. As a participant of the United Nations Global Compact, Axtria is committed to aligning strategies and operations with universal principles on human rights, labor, environment, and anti-corruption and taking actions that advance societal goals.\nOur people are our core strength, and they make us proud of our work; this has helped us grow exponentially and make tremendous strides toward developing great products. It is for the grit, thinking of an entrepreneur, and a family-like environment where each member is valued and treasured that we are growing rapidly.\nFor more information, visit www.axtria.com.\nJob Title: - Full Stack Experts ( Open across levels – Senior Associate to Associate Director)\n\nJob Location: -Gurgaon/Bangalore/Pune/Hyderabad\nJob Responsibilities: -\nBe an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on UI\nSoftware development experience in REACT, JavaScript/TypeScript, python is needed as frontend and backend for UI based applications\nBe a part of large delivery teams working on advanced projects when expert assistance is required.\nDeliver advanced Data Science capabilities to businesses in a meaningful manner through successful proof-of-concept solutions, and later smoothly transition the proof-of-concept into production.\nCreate Technical documents, develop, test, and deploy data analytics processes using REACT, Python, SQL on Azure/AWS platforms\nCan interact with client on GenAI related capabilities and use cases\nQualification: -\n- B-Tech or BE in Computer Science / Computer Applications from Tier 1-2 college\nOR\n- Master’s degree in Machine Learning / Statistics / Econometrics, or related discipline from Tier 1-2 college\nMust have Skills: -\nRequire 3-15 years of experience to develop, test, and deploy React, JavaScript, Python based applications on Azure/AWS platforms\nMust have experience in building production grade application with frontend & backend technologies like HTML, JavaScript, REACT, Python etc.\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.\nBasic knowledge on concepts of Generative AI / LLMs / GPT will be beneficial\nSkills that give you an edge: -\nStrong analytical skills to solve and model complex business requirements are a plus. With life sciences or pharma background.\nWe will provide– (Employee Value Proposition)\nOffer an inclusive environment that encourages diverse perspectives and ideas\nDeliver challenging and unique opportunities to contribute to the success of a transforming organization\nOpportunity to work on technical challenges that may impact across geographies\nVast opportunities for self-development: online Axtria Institute, knowledge sharing opportunities globally, learning opportunities through external certifications\nSponsored Tech Talks & Hackathons\nPossibility to relocate to any Axtria office for short and long-term projects\nBenefit package:\n-Health benefits\n-Retirement benefits\n-Paid time off\n-Flexible Benefits\n-Hybrid /FT Office/Remote\nAxtria is an equal-opportunity employer that values diversity and inclusiveness in the workplace.\nWho we are\nAxtria 14 years journey\nAxtria, Great Place to Work\nLife at Axtria\nAxtria Diversity",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['React.Js', 'Python', 'AWS']",2025-06-13 06:17:33
Engineer/Senior Engineer/Lead Engineer/Principal Engineer - Wastewater,Aecom,2 - 7 years,Not Disclosed,['Bengaluru'],"Engineer/Senior Engineer/Lead Engineer/Principal Engineer - Wastewater Modelling.\nProvide technical mentorship to junior modellers, review their work, and deliver training sessions on modelling concepts.\nCapable for leading the project, coordinating with Lead Offices, mentoring juniors and ensuring quality checks\nComplete design/hydraulic modellingactivities as per specified standards for different regional projects.",,,,"['hydraulic modelling', 'wastewater modelling', 'project management', 'program management', 'water quality modelling']",2025-06-13 06:17:35
Product Safety Engineer 4 - System Safety,Lam Research,5 - 10 years,Not Disclosed,['Bengaluru'],"Primary Responsibilities:\nPerforms safety risk assessments on equipment, assembly and component designs to comply with SEMI Guidelines, UL Standards, CE and other industry standards. Having proficiency in the SEMI2 , UL and CE standards.\nEnsures changes and upgrades of the equipment continue to meet safety guidelines and standards, and generate necessary safety notices & bulletins\nReviews Engineering Changes and Problem Reports from safety standpoint to recommend preventive actions for safety hazards\nCollaborates with India and US-based Mechanical, Electrical and Product Engineers for safety and design changes\nWorks with cross functional team and peers across the globe to select, test, implement and roll-out important design processes and BKMs\nSupports SEMI and other 3 rd party safety evaluation\nLeads & supports the design reviews and have ability to signoff designs from safety aspects\nDevelops test procedures to evaluate component safety per safety guidelines\nEligibility Criteria:\nYears of Experience: Minimum 5 years of experience and up to 12 years in product safety or Industrial Safety\nEducational qualification: Bachelor / Master s in Mechanical / Electrical / Chemical/Industrial / Industrial Safety Engineering\nMandatory Skills required to performing the job:\nProficient in equipment and personnel safety requirements to evaluate:\nStructural, vacuum and pressure loads, ergonomic, thermal, flammability, service, component layout/spacing, gas cabinet containment/exhaust, labeling, manuals/procedures, electrical/interlocks, hoists/lifts, chemistry and other considerations\nExperience with capital equipment or similar industry in the role of systems safety Engineer, for e.g., chemical process, medical instrumentation, automotive, aircraft, etc.\nExcellent communication and presentation skills, with a track record of success in cross functional, multi-national work environment.\nDemonstrated leadership skill in decision making and problem solving in absence of complete or accurate data.\nKnowledge in read and understand the Electrical Schematics, Interconnects, System Layouts, Mechanical drawings, Plumbing and Instrumentation Diagram (P&ID s), Service and Maintenance procedures\nKnowledge in Safety testing methods (Electrical testing/Industrial Hygienic/Ergonomic evaluation)\nDesirable Skills:\nExperience in semiconductor industry\nKnowledge of SEMI, UL or equivalent safety standards\nProficient with Microsoft Office applications\nExperience in working with cross-disciplinary teams (including mechanical, electro-mechanical, software, process, manufacturing engineers) for major projects.",Industry Type: Electronic Components / Semiconductors,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Semiconductor', 'Industrial safety', 'Manufacturing process', 'electromechanical', 'thermal', 'Instrumentation', 'MS Office', 'Automotive', 'Electricals', 'Testing']",2025-06-13 06:17:37
Full Stack Engineer- Manager,Axtria,5 - 10 years,20-35 Lacs P.A.,"['Noida', 'Pune', 'Bengaluru']","Job description:\nAxtria: -Axtria is a global provider of award-winning cloud software and data analytics to the life sciences industry. Axtria enables life sciences organizations to transform the product commercialization journey and deliver much-improved healthcare outcomes for patients worldwide. We are acutely aware that our work impacts millions of people and are incredibly passionate about the improvement we can bring to patients lives.\nOur focus is on delivering solutions that help pharmaceutical, medical device, and diagnostics companies complete the journey from data to insights to action and get superior returns from their investments. As a participant of the United Nations Global Compact, Axtria is committed to aligning strategies and operations with universal principles on human rights, labor, environment, and anti-corruption and taking actions that advance societal goals.\nOur people are our core strength, and they make us proud of our work; this has helped us grow exponentially and make tremendous strides toward developing great products. It is for the grit, thinking of an entrepreneur, and a family-like environment where each member is valued and treasured that we are growing rapidly.\nFor more information, visit www.axtria.com.\n\nJob Title: - Full Stack Experts ( Open across levels Senior Associate to Associate Director)\nJob Location: -Gurgaon/Bangalore/Pune/Hyderabad\n\nJob Responsibilities: -\nBe an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on UI\nSoftware development experience in REACT, JavaScript/TypeScript, python is needed as frontend and backend for UI based applications\nBe a part of large delivery teams working on advanced projects when expert assistance is required.\nDeliver advanced Data Science capabilities to businesses in a meaningful manner through successful proof-of-concept solutions, and later smoothly transition the proof-of-concept into production.\nCreate Technical documents, develop, test, and deploy data analytics processes using REACT, Python, SQL on Azure/AWS platforms\nCan interact with client on GenAI related capabilities and use cases\n\nQualification: -\n- B-Tech or BE in Computer Science / Computer Applications from Tier 1-2 college\nOR\n- Masters degree in Machine Learning / Statistics / Econometrics, or related discipline from Tier 1-2 college\n\nMust have Skills: -\nRequire 3-15 years of experience to develop, test, and deploy React, JavaScript, Python based applications on Azure/AWS platforms\nMust have experience in building production grade application with frontend & backend technologies like HTML, JavaScript, REACT, Python etc.\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.\nBasic knowledge on concepts of Generative AI / LLMs / GPT will be beneficial\nSkills that give you an edge: -\nStrong analytical skills to solve and model complex business requirements are a plus. With life sciences or pharma background.\nWe will provide (Employee Value Proposition)\nOffer an inclusive environment that encourages diverse perspectives and ideas\nDeliver challenging and unique opportunities to contribute to the success of a transforming organization\nOpportunity to work on technical challenges that may impact across geographies\nVast opportunities for self-development: online Axtria Institute, knowledge sharing opportunities globally, learning opportunities through external certifications\nSponsored Tech Talks & Hackathons\nPossibility to relocate to any Axtria office for short and long-term projects\nBenefit package:\n-Health benefits\n-Retirement benefits\n-Paid time off\n-Flexible Benefits\n-Hybrid /FT Office/Remote\nAxtria is an equal-opportunity employer that values diversity and inclusiveness in the workplace.\nWho we are\nAxtria 14 years journey\nAxtria, Great Place to Work\nLife at Axtria\nAxtria Diversity",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Javascript', 'React.Js', 'Python', 'AWS', 'SQL']",2025-06-13 06:17:38
S&C Global Network - AI - Life Sciences -Data Science Consultant,Accenture,4 - 9 years,Not Disclosed,['Gurugram'],"Job Title -\n\n\n\nS&C Global Network - AI - Healthcare Analytics - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nBangalore/Gurgaon\n\n\n\nMust-have skills:R,Phython,SQL,Spark,Tableau ,Power BI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPotential to Co-create with leaders in strategy, industry experts, enterprise function practitioners, and business intelligence professionals to shape and recommend innovative solutions that leverage emerging technologies.\nAbility to embed responsible business into everythingfrom how you service your clients to how you operate as a responsible professional.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nOpportunity to thrive in a culture that is committed to accelerating equality for all. Engage in boundaryless collaboration across the entire organization.\n\n\n\n\nWhat you would do in this role\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nProvide Subject matter expertise in various sub-segments of the LS industry.\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\nCo-lead proposals, and business development efforts and coordinate with other colleagues to create consensus-driven deliverables.\nExecute a transformational change plan aligned with the clients business strategy and context for change. Engage stakeholders in the change journey and build commitment to change.\nMake presentations wherever required to a known audience or client on functional aspects of his or her domain.\n\n\n\nWho are we looking for\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\nProven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\nExcellent understanding of Pharma data sets commercial, clinical, RWE (Real World Evidence) & EMR (Electronic medical records)\nLeverage ones hands on experience of working across one or more of these areas such as real-world evidence data, R&D clinical data, digital marketing data.\nHands-on experience with handling Datasets like Komodo, RAVE, IQVIA, Truven, Optum etc.\nHands-on experience in building and deployment of Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\nProficiency in Programming languages such as R, Python, SQL, Spark, etc.\nAbility to work with large data sets and present findings/insights to key stakeholders; Data management using databases like SQL.\nExperience with any of the cloud platforms like AWS, Azure, or Google Cloud for deploying and scaling language models.\nExperience with any of the Data Visualization tools like Tableau, Power BI, Qlikview, Spotfire is good to have.\nExcellent analytical and problem-solving skills, with a data-driven mindset.\nProficient in Excel, MS Word, PowerPoint, etc.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\nQualification\n\n\n\nExperience:\n\n\n\n4-8 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'sql', 'tableau', 'r', 'spark', 'spotfire', 'power bi', 'microsoft azure', 'time series', 'emr', 'machine learning', 'data engineering', 'artificial intelligence', 'qlikview', 'data science', 'gcp', 'predictive modeling', 'segmentation', 'life sciences', 'data visualization', 'aws', 'statistics']",2025-06-13 06:17:40
S&C GN - Data&AI - Life Sciences - Consultant,Accenture,4 - 9 years,Not Disclosed,['Gurugram'],"Management Level:Ind&Func AI Decision Science Consultant\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nExcellent understanding of Pharma data sets commercial, clinical, Leverage ones hands on experience of working across one or more of these areas such as real-world evidence data, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProgramming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI.\n\n\n\n\nJob\n\n\nSummary\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions. Provide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nKey Responsibilities\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nProvide Subject matter expertise in various sub-segments of the LS industry.\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\n\n\n\n\n\nAdditional Information\nProficient in Excel, MS Word, PowerPoint, etc.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\nQualification\n\n\n\nExperience:Proven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'data engineering', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 06:17:41
S&C GN - Data&AI - Life Sciences - Analyst,Accenture,2 - 7 years,Not Disclosed,['Gurugram'],"Management Level:Ind & Func AI Decision Science Analyst\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nLife Sciences/Pharma/Healthcare projects and delivering successful outcomes, commercial, clinical, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProficiency in Programming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI\n\n\n\nExperience:Proven experience (2+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\n\n\n\nJob\n\n\nSummary\n\nWe are seeking an experienced and visionary - Accenture S&C Global Network - Data & AI practice help our clients grow their business in entirely new ways. Analytics enables our clients to achieve high performance through insights from data - insights that inform better decisions and strengthen customer relationships. From strategy to execution, Accenture works with organizations to develop analytic capabilities - from accessing and reporting on data to predictive modelling - to outperform the competition.\n\n\n\nKey Responsibilities\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nWork on variety of projects in Data Modeling, Data Engineering, Data Visualization, Data Science etc.,\nAcquire new skills that have utility across industry groups.\n\n\n\n\n\nAdditional Information\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\n\nQualification\n\n\n\nExperience:Proven experience (2+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'presentation skills', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 06:17:43
S&C Global Network - AI - CMT - Consultant Data Science,Accenture,8 - 10 years,Not Disclosed,['Hyderabad'],"Job Title -\n\n\n\nS&C Global Network - AI - CMT - Consultant Data Science\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nHyderabad, HDC2A\n\n\n\nMust-have skills:CMT Analytics\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nResponsibilities/Authorities\n\nA Telco domain experienced and data science consultant is responsible to help the clients with designing & delivering AI solutions. He/she should be strong in Telco domain, AI fundamentals and should have good hands-on experience working with the following:\nAbility to work with large data sets and present conclusions to key stakeholders; Data management using SQL.\nPropose solutions to the client based on gap analysis for the existing Telco platforms that can generate long term & sustainable value to the client.\nGather business requirements from client stakeholders via interactions like interviews and workshops with all stakeholders Track down and read all previous information on the problem or issue in question. Explore obvious and known avenues thoroughly. Ask a series of probing questions to get to the root of a problem.\nAbility to understand the as-is process; understand issues with the processes which can be resolved either through Data & AI or process solutions and design detail level to-be state\nUnderstand customer needs and identify/translate them to business requirements (business requirement definition), business process flows and functional requirements and be able to inform the best approach to the problem.\nAdopt a clear and systematic approach to complex issues (i.e. A leads to B leads to C). Analyze relationships between several parts of a problem or situation. Anticipate obstacles and identify a critical path for a project.\nIndependently able to deliver products and services that empower clients to implement effective solutions. Makes specific changes and improvements to processes or own work to achieve more. Work with other team members and make deliberate efforts to keep others up to date.\nEstablish a consistent and collaborative presence with clients and act as the primary point of contact for assigned clients; escalate, track, and solve client issues.\nPartner with clients to understand end clients business goals, marketing objectives, and competitive constraints.\nStorytelling Crunch the data & numbers to craft a story to be presented to senior client stakeholders.\nShould be able to travel or relocate to Japan as per business requirement.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nDemonstrated experience in solving real-world data problems through Data & AI\nDirect onsite experience (i.e., experience of facing client inside client offices in India or abroad) is mandatory. Please note we are looking for client facing roles.\nProficiency with data mining, mathematics, and statistical analysis\nAdvanced pattern recognition and predictive modeling experience; knowledge of Advanced analytical fields in text mining, Image recognition, video analytics, IoT etc.\nExecution level understanding of econometric/statistical modeling packages\nTraditional techniques like Linear/logistic regression, multivariate statistical analysis, time series techniques, fixed/Random effect modelling.\nMachine learning techniques like - Random Forest, Gradient Boosting, XG boost, decision trees, clustering etc.\nKnowledge of Deep learning modeling techniques like RNN, CNN etc.\nExperience using digital & statistical modeling software (one or more) Python, R, PySpark, SQL, BigQuery, Vertex AI\nProficient in Excel, MS word, Power point, and corporate soft skills\nKnowledge of Dashboard creation platforms Excel, tableau, Power BI etc.\nExcellent written and oral communication skills with ability to clearly communicate ideas and results to non-technical stakeholders.\nStrong analytical, problem-solving skills and good communication skills\nSelf-Starter with ability to work independently across multiple projects and set priorities\nStrong team player Proactive and solution oriented, able to guide junior team members.\nExecution knowledge of optimization techniques is a good-to-have\nExact optimization Linear, Non-linear optimization techniques\nEvolutionary optimization Both population and search-based algorithms\nCloud platform Certification, experience in Computer Vision are good-to-haves\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:\n\n\n\n8-10Years\n\n\n\n\nEducational Qualification:\n\n\n\nAny Degree",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'mathematics', 'data mining', 'telco', 'statistics', 'cnn', 'pyspark', 'power bi', 'time series', 'random forest', 'artificial intelligence', 'sql', 'deep learning', 'tableau', 'r', 'rnn', 'data science', 'predictive modeling', 'statistical modeling', 'text mining', 'bigquery', 'logistic regression']",2025-06-13 06:17:44
Data Consultant_Strategy & Consulting,Capgemini,8 - 10 years,Not Disclosed,['Pune'],"Capgemini Invent \n\nCapgemini Invent is the digital innovation, consulting and transformation brand of the Capgemini Group, a global business line that combines market leading expertise in strategy, technology, data science and creative design, to help CxOs envision and build whats next for their businesses.\n\n Your Role \n Client Engagement & Advisory: \nPartner with clients to understand their business goals, CX challenges, and data needs.\nProvide strategic recommendations on how to leverage data to enhance customer engagement, satisfaction, and retention.\n Data Strategy Development: \nDesign comprehensive data strategies tailored to client objectives, including customer segmentation, predictive modelling, and personalization.\nDefine KPIs and metrics to measure the success of CX initiatives.\n Data Analysis & Insights: \nAnalyze structured and unstructured data to derive actionable insights.\nLeverage advanced analytics techniques, including machine learning and AI, to identify trends and opportunities.\n Solution Design & Implementation: \nCollaborate with technical teams to design and implement data platforms, dashboards, and analytics solutions.\nDrive the implementation of customer data platforms (CDPs), marketing automation tools, and analytics systems.\n Stakeholder Management: \nAct as a trusted advisor to business stakeholders.\nPresent findings, recommendations, and reports in a clear, compelling manner\n\n\n Your Profile \n8-10 years of experience in data analytics, data strategy, or consulting roles, with a focus on customer experience, marketing, or business transformation.\nMinimum of 5-8 years of experience in omni-channel marketing, data analytics, or marketing automation implementation, with at least 2 years in a consulting role.\n3+ years of experience in marketing orchestration, preferably with hands-on experience using marketing automation platforms (e.g., Salesforce Marketing Cloud, Adobe Campaign, Braze, Marketo, etc.) and leveraging customer data for personalized, cross-channel campaigns.\nBachelors degree in data science, Business Analytics, Computer Science, Marketing, or related fields. Masters degree preferable.\n Technical\n\nSkills:\n Proficiency in analytics tools (e.g., Python, R, SQL).\nFamiliarity with visualization tools (e.g., Tableau, Power BI).\nExperience implementing CDP tools (Salesforce CDP, Adobe CDP, Segment etc)\nKnowledge of cloud platforms (e.g., AWS, Azure, Google Cloud) is a plus.\nData Analytics\nData modelling\nDigital Marketing\nDigital Technology Consulting\nData-Driven Marketing\nProven track record of contributing to client engagements and delivering data-driven solutions.\nStrong communication, problem-solving, and interpersonal skills.\n\n\n What you will love about working here \nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.\n\n\n About Capgemini \n\nCapgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of 22.5 billion",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'analytics tool', 'channel marketing', 'marketing automation', 'marketing', 'digital marketing', 'python', 'power bi', 'microsoft azure', 'adobe', 'omni', 'cdp', 'sql', 'salesforce', 'tableau', 'r', 'salesforce marketing cloud', 'data modeling', 'gcp', 'adobe campaign', 'aws']",2025-06-13 06:17:46
Data Insights & Visualization Practition,Accenture,3 - 5 years,Not Disclosed,['Hyderabad'],"Project Role :Data Insights & Visualization Practition\n\n\n\n\n\nProject Role Description :Create interactive interfaces that enable humans to understand, interpret, and communicate complex data and insights. Wrangle, analyze, and prepare data to ensure delivery of relevant, consistent, timely, and actionable insights. Leverage modern business intelligence, storytelling, and web-based visualization tools to create interactive dashboards, reports and emerging VIS/BI artifacts. Use and customize (Gen)AI and AI-powered VIS/BI capabilities to enable a dialog with data.\n\n\n\nMust have skills :Data Analytics\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n2 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education:**Position\n\n\nSummary:**The Data Visualization Specialist will transform complex datasets into clear, actionable visualizations that support decision-making.**Key Responsibilities:**- Design and develop interactive dashboards and reports.- Collaborate with analysts and stakeholders to gather visualization requirements.- Ensure data visualizations are accurate, intuitive, and impactful.- Stay updated on best practices in data visualization.- Create visually compelling dashboards and reports to communicate insights.- Work closely with stakeholders to understand visualization requirements.- Ensure consistency in visual design and adherence to branding guidelines.- Optimize visualizations for performance and scalability.- Train end-users on interpreting and utilizing visual analytics tools.**\nQualifications:**- Bachelor's degree in Data Science, Computer Science, or a related field.- 3-5 years of experience in data visualization.- Proficiency in Power BI, Tableau, or similar tools.- Strong design sense and attention to detail.- Excellent communication and collaboration skills.\nAdditional Information:- The candidate should have minimum 2 years of experience in Data Analytics.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'power bi', 'business intelligence', 'tableau', 'data visualization', 'python', 'data analysis', 'bi', 'data warehousing', 'business analysis', 'business analytics', 'machine learning', 'dashboards', 'sql server', 'sql', 'r', 'data science', 'data modeling', 'advanced excel', 'etl', 'ssis']",2025-06-13 06:17:48
Data Management Practitioner,Accenture,12 - 17 years,Not Disclosed,['Kolkata'],"Project Role :Data Management Practitioner\n\n\n\n\n\nProject Role Description :Maintain the quality and compliance of an organizations data assets. Design and implement data strategies, ensuring data integrity and enforcing governance policies. Establish protocols to handle data, safeguard sensitive information, and optimize data usage within the organization. Design and advise on data quality rules and set up effective data compliance policies.\n\n\n\nMust have skills :Data Architecture Principles\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :any graduate\n\n\nSummary:As a Data Management Practitioner, you will be responsible for maintaining the quality and compliance of an organization's data assets. Your role involves designing and implementing data strategies, ensuring data integrity, enforcing governance policies, and optimizing data usage within the organization.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Design and advise on data quality rules- Set up effective data compliance policies- Ensure data integrity and enforce governance policies- Optimize data usage within the organization\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Architecture Principles- Strong understanding of data management best practices- Experience in designing and implementing data strategies- Knowledge of data governance and compliance policies- Ability to optimize data usage for organizational benefit\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Architecture Principles- This position is based at our Kolkata office- A degree in any graduate is required\n\nQualification\n\nany graduate",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data management', 'data architecture', 'sql', 'data architecture principles', 'hive', 'data analysis', 'oracle', 'data warehousing', 'machine learning', 'business intelligence', 'sql server', 'plsql', 'tableau', 'data science', 'data modeling', 'hadoop', 'sqoop', 'etl', 'etl development']",2025-06-13 06:17:50
Data Techology Senior Associate,MSCI Services,4 - 8 years,Not Disclosed,['Pune'],"As data engineers, we build scalable systems to process data in various formats and volumes, ranging from megabytes to terabytes. Our systems perform quality checks, match data across various sources, and release it in multiple formats. We leverage the latest technologies, sources, and tools to process the data. Some of the exciting technologies we work with include Snowflake, Databricks, and Apache Spark.\nYour skills and experience that will help you excel\nCore Java, Spring Boot, Apache Spark, Spring Batch, Python. Exposure to sql databases like Oracle, Mysql, Microsoft Sql is a must. Any experience / knowledge / certification on Cloud technology preferrably Microsoft Azure or Google cloud platform is good to have. Exposures to non sql databases like Neo4j or Document database is again good to have.\n  What we offer you\nTransparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall we'llbeing.\nFlexible working arrangements, advanced technology, and collaborative workspaces.\nA culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\nA global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for our clients.\nGlobal Orientation program to kickstart your journey, followe'd by access to our Learning@MSCI platform, LinkedIn Learning Pro and tailored learning opportunities for ongoing skills development.\nMulti-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\nWe actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women s Leadership Forum.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['CVS', 'Core Java', 'Bloomberg', 'spring batch', 'MySQL', 'Oracle', 'Analytics', 'Downstream', 'Python', 'Recruitment']",2025-06-13 06:17:51
Senior Data Research Analyst,Morningstar,3 - 8 years,Not Disclosed,['Mumbai'],"As a Senior Data Research Analyst , you will be responsible for acquiring and validating portfolio holdings data from various vendor sources. Your core responsibilities will involve standardizing this data into agreed formats using internal collection tools and resolving exceptions through thorough validation processes.\nWorking within the Portfolio Data Team, your role will focus on ensuring the accuracy and completeness of portfolio information, which is critical for downstream analytics and reporting. You will collaborate closely with leadership and cross-functional teams to support strategic goals, enhance operational performance, and contribute to the achievement of key KPIs.\nShift: UK/AU/US\nRoles Responsibilities:\nActively collect managed investment data using Morningstar collection systems, and ensure data timelines, completeness and accuracy to meet business goals.\nManage relationships between Morningstar and Asset Management companies, insurance companies and other data vendors.\nPartner with quality assurance, products, and technical departments to resolve clients data issues timely and effectively.\nParticipate in the initiatives focused on consolidating global data collection platforms and supporting database integration projects.\nEstablish and achieve the set Objectives Key Results (OKRs) with the direction of team lead.\nMonitor, analyze and execute summary reports including an investigation of potential data error to continuously improve data collection and quality assurance process using Lean Six Sigma tools.\nActively discover and raise issues in work (including system, process, and collection methodology) and propose enhancement suggestions to further improve system functionality, process efficiency and data quality.\nParticipate in data and process related projects such as industry/market research, market expansion, process certification, new product development support, etc.\nFacilitate cross-team projects to implement approved solutions based on priority and impact.\nDemonstrate a high sense of ownership of the process, understand roles responsibilities by acting as a process trainer and mentor\nRequirements:\n> 3 years experience in finance domain, with emphasis on collection systems and methodologies, senior data research analyst role or above.\nFund Portfolio experience would be preferred\nGood command in MS Office (Excel, PowerPoint etc.); advanced users preferred. SQL, Macro or Python and machine learning will be a plus.\nShould be critical thinker and should possess good communication skill.\nShould be equipped with understanding of data competencies like data content expertise, data analysis etc.\nStrong analytical, problem-solving capabilities, and excellent communication written as well as verbal reporting skills.\nShould be a good team player with good learning ability and equipped with self-motivation in an independent, fast-paced work environment.\nAbility to exercise control over the planned activities like training / mentoring new hires, doing quality checks etc.\nAble to work under tight deadlines and handle pressure during peak seasons.\nGood project management skills with proven track record of working on and delivering projects independently.\nRemote team working experience is a plus.\nFlexibility to work in shifts.\nMorningstar is an equal opportunity employer",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Manager Quality Assurance', 'Analytical', 'Data Research Analyst', 'Data collection', 'Market research', 'Asset management', 'Operations', 'Analytics', 'SQL']",2025-06-13 06:17:53
Head of Modelling & Data Science,Merkle Science,6 - 10 years,Not Disclosed,['Coimbatore'],"The purpose of this role is to set the strategic direction for the team, taking ownership of the overall Insights and analysis discipline in the market and liaising with other channels to ensure an integrated response to people-based marketing objectives.\nJob Description:\nJob Description Summary\nWe are seeking an experienced Advanced Analytics Senior Manager to join our dynamic team. This role focuses on leveraging advanced analytics techniques, including machine learning algorithms, Generative AI (GenAI), and large language models (LLMs), to drive data-driven decision-making within the retail/CPG domain. The ideal candidate will possess a strong quantitative background, a passion for transforming complex data into actionable insights, an extensive experience into leading mid-to-large sized teams, and the ability to create, assess and implement RFPs, POCs, approaches and frameworks.\nKey Responsibilities:\nDevelop, implement, and maintain advanced analytical models using machine learning algorithms and GenAI applications\nUtilize various advanced analytics techniques to uncover trends, patterns, and insights from large and complex datasets.\nCollaborate with cross-functional teams to identify business needs and deliver data-driven solutions.\nCreate visually compelling dashboards and reports to present findings to stakeholders.\nContinuously evaluate and improve existing analytics methodologies and models to enhance accuracy and performance.\nStay abreast of industry trends and advancements in analytics and machine learning to drive innovation within the team.\nMentor junior team members and contribute to knowledge sharing within the organization.\nBasic Qualifications:\nBachelor s or Master s degree in Data Science, Business Analytics, Mathematics, Statistics, or a related field.\n10+ years of experience in advanced analytics, data science, machine learning, Generative AI or a related field.\nStrong experience with quantitative modeling, predictive analytics, text analytics, and forecasting methodologies\nProficiency in SQL (or Google BigQuery), Python, visualization tools like Tableau/PowerBI\nFamiliarity with the Retail/CPG/Tech industry and experience with product, transaction, and customer-level data.\nExcellent communication skills, both verbal and written, with the ability to convey complex concepts to non-technical stakeholders.\nStrong analytical and problem-solving skills, with an inquisitive mindset.\nDesired Skills:\nProficient in the following advanced analytics techniques:\nDescriptive Analytics: Statistical analysis, data visualization.\nPredictive Analytics: Regression analysis, time series forecasting, classification techniques, market mix modeling\nPrescriptive Analytics: Optimization, simulation modeling.\nText Analytics: Natural Language Processing (NLP), sentiment analysis.\nExtensive knowledge of machine learning techniques, including:\nSupervised Learning: Linear regression, logistic regression, decision trees, support vector machines, random forests, gradient boosting machines among others\nUnsupervised Learning: K-means clustering, hierarchical clustering, principal component analysis (PCA), anomaly detection among others\nReinforcement Learning: Q-learning, deep Q-networks, etc.\nExperience with Generative AI and large language models (LLMs) for text generation, summarization, and conversational agents.\nResearching, loading and application of the best LLMs (GPT, Gemini, LLAMA, etc.) for various objectives\nHyper parameter tuning\nPrompt Engineering\nEmbedding & Vectorization\nFine tuning\nProficiency in data visualization tools such as Tableau or Power BI.\nStrong skills in data management, structuring, and harmonization to support analytical needs.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Logistic regression', 'Data management', 'Business analytics', 'Analytical', 'Machine learning', 'linear regression', 'Regression analysis', 'Forecasting', 'SQL', 'Python']",2025-06-13 06:17:55
Head of Modelling & Data Science,Merkle B2b,8 - 13 years,Not Disclosed,['Coimbatore'],"The purpose of this role is to set the strategic direction for the team, taking ownership of the overall Insights and analysis discipline in the market and liaising with other channels to ensure an integrated response to people-based marketing objectives.\nJob Description:\nJob Description Summary\nWe are seeking an experienced Advanced Analytics Senior Manager to join our dynamic team. This role focuses on leveraging advanced analytics techniques, including machine learning algorithms, Generative AI (GenAI), and large language models (LLMs), to drive data-driven decision-making within the retail/CPG domain. The ideal candidate will possess a strong quantitative background, a passion for transforming complex data into actionable insights, an extensive experience into leading mid-to-large sized teams, and the ability to create, assess and implement RFPs, POCs, approaches and frameworks.\nKey Responsibilities:\nDevelop, implement, and maintain advanced analytical models using machine learning algorithms and GenAI applications\nUtilize various advanced analytics techniques to uncover trends, patterns, and insights from large and complex datasets.\nCollaborate with cross-functional teams to identify business needs and deliver data-driven solutions.\nCreate visually compelling dashboards and reports to present findings to stakeholders.\nContinuously evaluate and improve existing analytics methodologies and models to enhance accuracy and performance.\nStay abreast of industry trends and advancements in analytics and machine learning to drive innovation within the team.\nMentor junior team members and contribute to knowledge sharing within the organization.\nBasic Qualifications:\nBachelor s or Master s degree in Data Science, Business Analytics, Mathematics, Statistics, or a related field.\n10+ years of experience in advanced analytics, data science, machine learning, Generative AI or a related field.\nStrong experience with quantitative modeling, predictive analytics, text analytics, and forecasting methodologies\nProficiency in SQL (or Google BigQuery), Python, visualization tools like Tableau/PowerBI\nFamiliarity with the Retail/CPG/Tech industry and experience with product, transaction, and customer-level data.\nExcellent communication skills, both verbal and written, with the ability to convey complex concepts to non-technical stakeholders.\nStrong analytical and problem-solving skills, with an inquisitive mindset.\nDesired Skills:\nProficient in the following advanced analytics techniques:\nDescriptive Analytics: Statistical analysis, data visualization.\nPredictive Analytics: Regression analysis, time series forecasting, classification techniques, market mix modeling\nPrescriptive Analytics: Optimization, simulation modeling.\nText Analytics: Natural Language Processing (NLP), sentiment analysis.\nExtensive knowledge of machine learning techniques, including:\nSupervised Learning: Linear regression, logistic regression, decision trees, support vector machines, random forests, gradient boosting machines among others\nUnsupervised Learning: K-means clustering, hierarchical clustering, principal component analysis (PCA), anomaly detection among others\nReinforcement Learning: Q-learning, deep Q-networks, etc.\nExperience with Generative AI and large language models (LLMs) for text generation, summarization, and conversational agents.\nResearching, loading and application of the best LLMs (GPT, Gemini, LLAMA, etc.) for various objectives\nHyper parameter tuning\nPrompt Engineering\nEmbedding & Vectorization\nFine tuning\nProficiency in data visualization tools such as Tableau or Power BI.\nStrong skills in data management, structuring, and harmonization to support analytical needs.\nLocation:\nCoimbatore\nBrand:\nMerkle\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Logistic regression', 'Data management', 'Business analytics', 'Analytical', 'Machine learning', 'linear regression', 'Regression analysis', 'Forecasting', 'SQL', 'Python']",2025-06-13 06:17:56
"Staff Engr - Data Science(Advanced OOPS,Python,PySpark,Databricks)",The TJX Companies Inc,5 - 10 years,Not Disclosed,['Hyderabad'],"TJX Companies\nAt TJX Companies, every day brings new opportunities for growth, exploration, and achievement. You ll be part of our vibrant team that embraces diversity, fosters collaboration, and prioritizes your development. Whether you re working in our four global Home Offices, Distribution Centers or Retail Stores TJ Maxx, Marshalls, Homegoods, Homesense, Sierra, Winners, and TK Maxx, you ll find abundant opportunities to learn, thrive, and make an impact. Come join our TJX family a Fortune 100 company and the world s leading off-price retailer.\nJob Description:\nAbout TJX:\nAt TJX, is a Fortune 100 company that operates off-price retailers of apparel and home fashions. TJX India - Hyderabad is the IT home office in the global technology organization of off-price apparel and home fashion retailer TJX, established to deliver innovative solutions that help transform operations globally. At TJX, we strive to build a workplace where our Associates contributions are welcomed and are embedded in our purpose to provide excellent value to our customers every day. At TJX India, we take a long-term view of your career. We have a high-performance culture that rewards Associates with career growth opportunities, preferred assignments, and upward career advancement. We take well-being very seriously and are committed to offering a great work-life balance for all our Associates.\nWhat you ll discover\nInclusive culture and career growth opportunities\nA truly Global IT Organization that collaborates across North America, Europe, Asia and Australia, click here to learn more\nChallenging, collaborative, and team-based environment\nWhat you ll do\nThe Global Supply Chain - Logistics Team is responsible for managing various supply chain logistics related solutions within TJX IT. The organization delivers capabilities that enrich the customer experience and provide business value. We seek a motivated, talented Staff E ngineer with good understanding of cloud base, database and BI concepts to help architect enterprise reporting solutions across global buying, planning and allocations .\nWhat you ll need\nThe Global Supply Chain - Logistics Team thrives on strong relationships with our business partners and working diligently to address their needs which supports TJX growth and operational stability. On this tightly knit and fast-paced solution delivery team you will be constantly challenged to stretch and think outside the box .\nYou will be working with product teams, architecture and business partners to strategically plan and deliver the product features by connecting the technical and business worlds. You will need to break down complex problems into steps that drive product development while keeping product quality and security as the priority. You will be responsible for most architecture, design and technical decisions within the assigned scope.\nKey Responsibilities:\nDesign, develop, test and deploy AI solutions using Azure AI services to meet business requirements , working collaboratively with architects and other engineers.\nTrain, fine-tune, and evaluate AI models, including large language models (LLMs), ensuring they meet performance criteria and integrate seamlessly into new or existing solutions.\nDevelop and integrate APIs to enable smooth interaction between AI models and other applications, facilitating efficient model serving.\nCollaborate effectively with cross-functional teams, including data scientists, software engineers, and business stakeholders, to deliver comprehensive AI solutions.\nOptimize AI and ML model performance through techniques such as hyperparameter tuning and model compression to enhance efficiency and effectiveness.\nMonitor and maintain AI systems, providing technical support and troubleshooting to ensure continuous operation and reliability.\nCreate comprehensive documentation for AI solutions, including design documents, user guides, and operational procedures, to support development and maintenance.\nStay updated with the latest advancements in AI, machine learning, and cloud technologies, demonstrating a commitment to continuous learning and improvement.\nDesign, code, deploy, and support software components, working collaboratively with AI architects and engineers to build impactful systems and services.\nLead medium complex initiatives, prioritizing and assigning tasks, providing guidance, and resolving issues to ensure successful project delivery.\nMinimum Qualifications\nBachelors degree in computer science, engineering, or related field\n8 + years of experience in data /software engineering, design, implementation and architecture.\nAt least 5+ years of hands-on experience in developing AI/ML solutions, with a focus on deploying them in a cloud environment .\nDeep understanding of AI and ML algorithms with focus on Operations Research / Optimization knowledge ( preferably M etaheuristics / Genetic Algorithms) .\nStrong programming skills in Python with advanced OOPS concepts.\nGood understanding of structured, semi structured, and unstructured data, Data modelling, Data analysis, ETL and ELT .\nProficiency with Databricks & PySpark .\nExperience with MLOps practices including CI/CD for machine learning models.\nKnowledge of security best practices for deploying AI solutions, including data encryption and access control.\nKnowledge of ethical considerations in AI, including bias detection and mitigation strategies.\nThis role operates in an Agile/Scrum environment and requires a solid understanding of the full software lifecycle, including functional requirement gathering, design and development, testing of software applications, and documenting requirements and technical specifications.\nFully Owns Epics with decreasing guidance. Takes initiative through identifying gaps and opportunities.\nStrong communication and influence skills. Solid team leadership with mentorship skills\nAbility to understand the work environment and competing priorities in conjunction with developing/meeting project goals .\nShows a positive, open-minded, and can-do attitude .\nExperience in the following technologies:\nAdvanced Python programming ( OOPS)\nOperations Research / Optimization knowledge ( preferably M etaheuristics / Genetic Algorithms)\nDatabricks with Pyspark\nAzure / Cloud knowledge\nGithub / version control\nFunctional knowledge on Supply Chain / Logistics is preferred.\nIn addition to our open door policy and supportive work environment, we also strive to provide a competitive salary and benefits package. TJX considers all applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, gender identity and expression, marital or military status, or based on any individuals status in any group or class protected by applicable federal, state, or local law. TJX also provides reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act and applicable state and local law.\nAddress:\nSalarpuria Sattva Knowledge City, Inorbit Road\nLocation:\nAPAC Home Office Hyderabad IN",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Data analysis', 'Operations research', 'Architecture', 'OOPS', 'Cloud', 'Machine learning', 'Technical support', 'Python', 'Logistics']",2025-06-13 06:17:58
Data Engineer,Atyeti,2 - 4 years,Not Disclosed,['Pune'],"Role & responsibilities\n\nDevelop and Maintain Data Pipelines: Design, develop, and manage scalable ETL pipelines to process large datasets using PySpark, Databricks, and other big data technologies.\nData Integration and Transformation: Work with various structured and unstructured data sources to build efficient data workflows and integrate them into a central data warehouse.\nCollaborate with Data Scientists & Analysts: Work closely with the data science and business intelligence teams to ensure the right data is available for advanced analytics, machine learning, and reporting.",,,,"['Azure Synapse', 'Pyspark', 'ETL', 'Python']",2025-06-13 06:17:59
"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon",One of the largest insurance providers.,5 - 10 years,Not Disclosed,['Gurugram'],"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon\n\nSummary: An excellent opportunity for someone having a minimum of five years of experience with expertise in building data pipelines. A person must have experience in Python, Pyspark and AWS.\n\nLocation- Gurgaon (Hybrid)\n\nYour Future Employer- One of the largest insurance providers.\n\nResponsibilities-\nTo design, develop, and maintain large-scale data pipelines that can handle large datasets from multiple sources.\nReal-time data replication and batch processing of data using distributed computing platforms like Spark, Kafka, etc.\nTo optimize the performance of data processing jobs and ensure system scalability and reliability.\nTo collaborate with DevOps teams to manage infrastructure, including cloud environments like AWS.\nTo collaborate with data scientists, analysts, and business stakeholders to develop tools and platforms that enable advanced analytics and reporting.\n\nRequirements-\nHands-on experience with AWS services such as S3, DMS, Lambda, EMR, Glue, Redshift, RDS (Postgres) Athena, Kinesics, etc.\nExpertise in data modeling and knowledge of modern file and table formats.\nProficiency in programming languages such as Python, PySpark, and SQL/PLSQL for implementing data pipelines and ETL processes.\nExperience data architecting or deploying Cloud/Virtualization solutions (Like Data Lake, EDW, Mart ) in the enterprise.\nCloud/hybrid cloud (preferably AWS) solution for data strategy for Data lake, BI and Analytics.\nWhat is in for you-\nA stimulating working environment with equal employment opportunities.\nGrowing of skills while working with industry leaders and top brands.\nA meritocratic culture with great career progression.\n\nReach us- If you feel that you are the right fit for the role please share your updated CV at randhawa.harmeen@crescendogroup.in\n\nDisclaimer- Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Pipeline', 'AWS', 'Data Ingestion', 'Data Engineering', 'Data Processing']",2025-06-13 06:18:01
Gcp Data Engineer,Estuate Software,6 - 11 years,Not Disclosed,['Hyderabad'],"Design, build,\nJob Title: Data Engineer / Integration Engineer\nJob Summary:\nWe are seeking a highly skilled Data Engineer / Integration Engineer to join our team. The ideal candidate will have expertise in Python, workflow orchestration, cloud platforms (GCP/Google BigQuery), big data frameworks (Apache Spark or similar), API integration, and Oracle EBS. The role involves designing, developing, and maintaining scalable data pipelines, integrating various systems, and ensuring data quality and consistency across platforms. Knowledge of Ascend.io is a plus.\nKey Responsibilities:\nDesign, build, and maintain scalable data pipelines and workflows.\nDevelop and optimize ETL/ELT processes using Python and workflow automation tools.\nImplement and manage data integration between various systems, including APIs and Oracle EBS.\nWork with Google Cloud Platform (GCP) or Google BigQuery (GBQ) for data storage, processing, and analytics.\nUtilize Apache Spark or similar big data frameworks for efficient data processing.\nDevelop robust API integrations for seamless data exchange between applications.\nEnsure data accuracy, consistency, and security across all systems.\nMonitor and troubleshoot data pipelines, identifying and resolving performance issues.\nCollaborate with data analysts, engineers, and business teams to align data solutions with business goals.\nDocument data workflows, processes, and best practices for future reference.\nRequired Skills & Qualifications:\nStrong proficiency in Python for data engineering and workflow automation.\nExperience with workflow orchestration tools (e.g., Apache Airflow, Prefect, or similar).\nHands-on experience with Google Cloud Platform (GCP) or Google BigQuery (GBQ).\nExpertise in big data processing frameworks, such as Apache Spark.\nExperience with API integrations (REST, SOAP, GraphQL) and handling structured/unstructured data.\nStrong problem-solving skills and ability to optimize data pipelines for performance.\nExperience working in an agile environment with CI/CD processes.\nStrong communication and collaboration skills.\nPreferred Skills & Nice-to-Have:\nExperience with Ascend.io platform for data pipeline automation.\nKnowledge of SQL and NoSQL databases.\nFamiliarity with Docker and Kubernetes for containerized workloads.\nExposure to machine learning workflows is a plus.\nWhy Join Us?\nOpportunity to work on cutting-edge data engineering projects.\nCollaborative and dynamic work environment.\nCompetitive compensation and benefits.\nProfessional growth opportunities with exposure to the latest technologies.\n\nHow to Apply:\nInterested candidates can apply by sending their resume to 8892751405 / deekshith.naidu@estuate.com or through Naukri",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'GCP', 'Bigquery', 'SQL']",2025-06-13 06:18:02
Data Engineer,Luxoft,5 - 8 years,Not Disclosed,['Pune'],"Project description\nAre you passionate about leveraging the latest technologies for strategic changeDo you enjoy problem solving in clever waysAre you organized enough to drive change across complex data systemsIf so, you could be the right person for this role.\nAs an experienced data engineer, you will join a global data analytics team in our Group Chief Technology Officer / Enterprise Architecture organization supporting our strategic initiatives which ranges from portfolio health to integration.\n\nResponsibilities\n\nHelp Group Enterprise Architecture team to develop our suite of EA tools and workbenches\n\nWork in the development team to support the development of portfolio health insights\n\nBuild data applications from cloud infrastructure to visualization layer\n\nProduce clear and commented code\n\nProduce clear and comprehensive documentation\n\nPlay an active role with technology support teams and ensure deliverables are completed or escalated on time\n\nProvide support on any related presentations, communications, and trainings\n\nBe a team player, working across the organization with skills to indirectly manage and influence\n\nBe a self-starter willing to inform and educate others\n\nSkills\nMust have\n\nB.Sc./M.Sc. degree in computing or similar\n\n5-8+ years' experience as a Data Engineer, ideally in a large corporate environment\n\nIn-depth knowledge of SQL and data modelling/data processing\n\nStrong experience working with Microsoft Azure\n\nExperience with visualisation tools like PowerBI (or Tableau, QlikView or similar)\n\nExperience working with Git, JIRA, GitLab\n\nStrong flair for data analytics\n\nStrong flair for IT architecture and IT architecture metrics\n\nExcellent stakeholder interaction and communication skills\n\nUnderstanding of performance implications when making design decisions to deliver performant and maintainable software.\n\nExcellent end-to-end SDLC process understanding.\n\nProven track record of delivering complex data apps on tight timelines\n\nFluent in English both written and spoken.\n\nPassionate about development with focus on data and cloud\n\nAnalytical and logical, with strong problem solving skills\n\nA team player, comfortable with taking the lead on complex tasks\n\nAn excellent communicator who is adept in, handling ambiguity and communicating with both technical and non-technical audiences\n\nComfortable with working in cross-functional global teams to effect change\n\nPassionate about learning and developing your hard and soft professional skills\n\nNice to have\n\nExperience working in the financial industry\n\nExperience in complex metrics design and reporting\n\nExperience in using artificial intelligence for data analytics\n\nOther\n\nLanguages\n\nEnglishC1 Advanced\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'data processing', 'microsoft azure', 'sql', 'data modeling', 'screening', 'it architecture', 'hiring', 'power bi', 'hrsd', 'knowledge of sql', 'data engineering', 'artificial intelligence', 'sourcing', 'qlikview', 'talent acquisition', 'tableau', 'git', 'recruitment', 'gitlab', 'sdlc', 'jira']",2025-06-13 06:18:04
Data Visualization Engineer,Maimsd Technology,4 - 7 years,Not Disclosed,['Pune'],"Experience : 4 - 7 Yrs\n\nEmployment Type : Full Time, Permanent\n\nWorking mode : Regular\n\nNotice Period : Immediate - 15 Day\n\nAbout the Role :\n\nWe are seeking a skilled Data Visualization Engineer to join our team and transform raw data into actionable insights. You will play a crucial role in designing, developing, and maintaining interactive dashboards and reports using Power BI, Power Apps, Power Query, and Power Automate.\n\nResponsibilities :\n\n- Data Visualization : Create compelling and informative dashboards and reports using Power BI, effectively communicating complex data to stakeholders.\n\n- Data Integration : Import data from various sources (e.g., databases, spreadsheets, APIs) using Power Query and ensure data quality and consistency.\n\n- Data Modeling : Develop robust data models in Power BI to support complex analysis and reporting requirements.\n\n- Power Apps Development : Create custom applications using Power Apps to enable data-driven decision-making and automate workflows.\n\n- Power Automate Integration : Automate repetitive tasks and workflows using Power Automate to improve efficiency and reduce errors.\n\n- Cloud Data Analytics : Leverage cloud-based data analytics platforms to process and analyze large datasets.\n\n- Collaboration : Work closely with data analysts, data scientists, and business users to understand their requirements and deliver effective visualizations.\n\nQualifications :\n\nExperience : 4-7 years of experience in data visualization and business intelligence.\n\nTechnical Skills :\n\n- Proficiency in Power BI, Power Apps, Power Query, and Power Automate.\n\n- Strong understanding of data modeling and ETL processes.\n\n- Experience working with cloud-based data analytics platforms.\n\n- Familiarity with SQL and other data query languages.\n\nSoft Skills :\n\n- Excellent communication and interpersonal skills.\n\n- Strong problem-solving and analytical abilities.\n\n- Attention to detail and ability to deliver high-quality work",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Visualization', 'Reporting Analytics', 'Power BI', 'Dashboard Design', 'Power Automate', 'Data Modeling', 'ETL', 'Power Query M', 'Data Integration', 'SQL']",2025-06-13 06:18:06
Data Engineer,Mobile Programming,5 - 10 years,Not Disclosed,['Mumbai'],"Candidate Skill:Technical Skills - Data Engineering | ETL | SQL | Python | AWS | Azure | Google Cloud | Hadoop | Spark | Kafka | Data Warehousing | Data Modeling | NoSQL | Data Quality\nWe are looking for an experienced Data Engineer to join our team in Mumbai. As a Data Engineer, you will be responsible for designing, building, and maintaining efficient data pipelines that transform raw data into actionable insights. You will work closely with data scientists and analysts to ensure that data is accessible, reliable, and optimized for analysis. Your role will also involve handling large datasets, ensuring data quality, and implementing data processing frameworks.\nKey Responsibilities:Design and build scalable data pipelines for processing, transforming, and integrating large datasets.Develop and maintain ETL processes to extract, transform, and load data from multiple sources into the data warehouse.Collaborate with data scientists and analysts to ensure that the data is optimized for analysis and modeling.Ensure the quality, integrity, and security of data throughout its lifecycle.\nWork with cloud-based technologies for data storage and processing (AWS, Azure, GCP).Implement data processing frameworks for efficient handling of structured and unstructured data.Troubleshoot and resolve issues related to data pipelines and workflows.Automate data integration processes and ensure data consistency and accuracy across systems.\nRequired Skills:\n5+ years of experience in Data Engineering with hands-on experience in data pipeline development.Strong expertise in ETL processes, data integration, and data warehousing.Proficiency in SQL, Python, and other programming languages for data manipulation.Experience with cloud technologies such as AWS, Azure, or Google Cloud.Knowledge of big data technologies like Hadoop, Spark, or Kafka is a plus.Strong understanding of data modeling, data quality, and data governance.\nFamiliarity with NoSQL databases (e.g., MongoDB, Cassandra) and relational databases.Strong analytical and problem-solving skills with the ability to work with large, complex datasets.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure', 'Hadoop', 'Kafka', 'SQL', 'Data Quality', 'NoSQL', 'GCP', 'Spark', 'Data Warehousing', 'Data Modeling', 'ETL', 'AWS', 'Python']",2025-06-13 06:18:08
Data Warehouse Engineer (SSIS),Tek Experts,9 - 10 years,Not Disclosed,['New Delhi'],"We re searching for a Data Warehouse Engineer with a proven track record of developing Business Intelligence solutions to drive key outcomes. The role is responsible for different layers of the data hierarchy - including database design, data collection and storage techniques, to a deep understanding of data transformation tools and methodologies, the provisioning and managing of analytical databases, and building infrastructures that bring machine learning capabilities into production. The role will operate within the Business Analytics unit under the Business Analytics Global Manager.\nAt TeKnowledge , your work makes an impact from day one. We partner with organizations to deliver AI-First Expert Technology Services that\ndrive meaningful impact in AI, Customer Experience, and Cybersecurity. We turn complexity into clarity and potential into progress in a place where people lead and tech empowers.\nYou ll be part of a diverse and inclusive team where trust, teamwork, and shared success fuel everything we do. We push boundaries, using advanced technologies to solve complex challenges for clients around the world.\nHere, your work drives real change, and your ideas help shape the future of technology. We invest in you with top-tier training, mentorship, and career development ensuring you stay ahead in an ever-evolving world.\nWhy You ll Enjoy It Here:\nBe Part of Something Big A growing company where your contributions matter.\nMake an Immediate Impact Support groundbreaking technologies with real-world results.\nWork on Cutting-Edge Tech AI, cybersecurity, and next-gen digital solutions.\nThrive in an Inclusive Team A culture built on trust, collaboration, and respect.\nWe Care Integrity, empathy, and purpose guide every decision.\nWe re looking for innovators, problem-solvers, and experts ready to drive change and grow with us.\nWe Are TeKnowledge. Where People Lead and Tech Empowers.\n\nResponsibilities\n\nEnd to end development of data models, including gathering the data sources, developing ETL processes, and developing front end data model solutions for our business to help our users make smarter decisions.\nFull responsibility on our DWH including infrastructure, data modeling, audit logging, etc.\nBuilding automated validation processes to ensure data integrity.\nOptimizing processes run-time and solving problems in a scalable manner.\nTaking full ownership of designing, building and deployment of data products.\nCollaborating with Backend, Data Science, Data Analysis, and Product teams.\n\nQualifications\n\nBachelor s degree in Information Systems, Industrial Engineering, or equivalent experience is required.\nProfessional fluency in English is essential, both written and spoken.\nThree or more years of experience in BI solutions development (DWH, ETL) as well as in SQL and ETL tools like SSIS is required.\nExperience with building automated validation processes to ensure data integrity is considered an advantage.\nExperience in data modeling, working ETL tools and methodology, as well as with BI reporting tools is required.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Backend', 'Front end', 'Database design', 'Business analytics', 'Machine learning', 'HTML', 'SSIS', 'Business intelligence', 'SQL']",2025-06-13 06:18:09
Senior Data Engineer,ANS Group,2 - 6 years,Not Disclosed,['Ahmedabad'],"ANS Group is looking for Senior Data Engineer\nThe job responsibilities of a Senior Data Engineer may include:1\n\nDesigning and implementing scalable and reliable data pipelines, data models, and data infrastructure for processing large and complex datasets\n\n2\n\nDeveloping and maintaining databases, data warehouses, and data lakes that store and manage the organization's data\n\n3\n\nDeveloping and implementing data integration and ETL (Extract, Transform, Load) processes to ensure that data flows smoothly and accurately between different systems and data sources\n\n4\n\nEnsuring data quality, consistency, and accuracy through data profiling, cleansing, and validation\n\n5\n\nBuilding and maintaining data processing and analytics systems that support business intelligence, machine learning, and other data-driven applications\n\n6\n\nOptimizing the performance and scalability of data systems and infrastructure to ensure that they can handle the organization's growing data needs\n\nTo be a successful Senior Data Engineer, one must have in-depth knowledge of database architecture, data modeling, data integration, and ETL processes\n\nThey should also be proficient in programming languages such as Python, Java, or SQL and have experience working with big data technologies like Hadoop, Spark, and NoSQL databases\n\nStrong communication and leadership skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data warehousing', 'data pipeline', 'machine learning', 'data engineering', 'sql', 'nosql', 'database design', 'data quality', 'java', 'data modeling', 'spark', 'leadership', 'hadoop', 'etl', 'data integration', 'programming', 'data lake', 'etl process', 'communication skills', 'data profiling']",2025-06-13 06:18:11
Senior Data Engineer,Ensemble Health Partners,5 - 9 years,Not Disclosed,[],"Company Overview:\nAs Ensemble Health Partners Company, we're at the forefront of innovation, leveraging cutting-edge technology to drive meaningful impact in the Revenue Cycle Management landscape. Our future-forward technology combines tightly integrated data ingestion, workflow automation and business intelligence solutions on a modern cloud architecture. We have the second-largest share in the RCM space in the US Market with 10000+ professionals working in the organization. With 10 Technology Patents in our name, we believe the best results come from a combination of skilled and experienced team, proven and repeatable processes, and modern and flexible technologies. As a leading player in the industry, we offer an environment that fosters growth, creativity, and collaboration, where your expertise will be valued, and your contributions will make a difference.\n\nRole & responsibilities :\nExperience : 5-9 Years\nLocation : remote/wfh\n\nPosition Summary :\nDesign and maintain scalable data pipelines, manage ETL processes and data warehouses, ensure data quality and governance, collaborate with cross-functional teams, support machine learning deployment, lead projects, mentor juniors, work with big data and cloud technologies, and bring expertise in Spark, Databricks, Streaming/Reactive/Event-driven systems, Agentic programming, and LLM application development.\n\nRequired Skills :\nSpark, Databricks, Streaming/Reactive /Event driven, Agentic programming & LLM Application Experience\n5+ years of coding experience with Microsoft SQL.\n3+ years working with big data technologies including but not limited to Databricks, Apache Spark, Python, Microsoft Azure (Data Factory, Dataflows, Azure Functions, Azure Service Bus) with a willingness and ability to learn new ones\nExcellent understanding of engineering fundamentals: testing automation, code reviews, telemetry, iterative delivery and DevOps\nExperience with polyglot storage architectures including relational, columnar, key-value, graph or equivalent\nExperience with Delta tables as well as Parquet files stored in ADLS\nExperience delivering applications using componentized and distributed architectures using event driven patterns\nDemonstrated ability to communicate effectively to both technical and non-technical, globally distributed audiences\nSolid foundations in formal architecture, design patterns and best practices\nExperience working with healthcare datasets\n\nWhy Join US?\nWe adapt emerging technologies to practical uses to deliver concrete solutions that bring maximum impact to providers bottom line. We currently have 10 Technology Patents in our name.\nWe offer you a great organization to work for, where you will get to do best work of your career and grow with the team that is shaping the future of Revenue Cycle Management.\nWe have our strong focus on Learning and development. We have the best Industry standard professional development policies to support the learning goals of our associates.\nWe have flexible/ remote working/ working from home options\nBenefits\nHealth Benefits and Insurance Coverage for family and parents. Accidental Insurance for the associate.\nCompliant with all Labor Laws- Maternity benefits, Paternity Leaves.\nCompany Swags- Welcome Packages, Work Anniversary Kits\nExclusive Referral Policy\nProfessional Development Program and Reimbursements.\nRemote work flexibility to work from home.\nPlease share your resume on yash.arora@ensemblehp.com with current ctc, expected ctc, notice period.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Functions', 'Azure Databricks']",2025-06-13 06:18:12
Data Engineer,Infoobjects Inc.,3 - 6 years,Not Disclosed,['Jaipur'],"Role & responsibilities:\nDesign, develop, and maintain robust ETL/ELT pipelines to ingest and process data from multiple sources.\nBuild and maintain scalable and reliable data warehouses, data lakes, and data marts.\nCollaborate with data scientists, analysts, and business stakeholders to understand data needs and deliver solutions.\nEnsure data quality, integrity, and security across all data systems.\nOptimize data pipeline performance and troubleshoot issues in a timely manner.\nImplement data governance and best practices in data management.\nAutomate data validation, monitoring, and reporting processes.\n\n\n\nPreferred candidate profile:\nBachelor's or Masters degree in Computer Science, Engineering, Information Systems, or related field.\nProven experience (X+ years) as a Data Engineer or similar role.\nStrong programming skills in Python, Java, or Scala.\nProficiency with SQL and working knowledge of relational databases (e.g., PostgreSQL, MySQL).\nHands-on experience with big data technologies (e.g., Spark, Hadoop).\nFamiliarity with cloud platforms such as AWS, GCP, or Azure (e.g., S3, Redshift, BigQuery, Data Factory).\nExperience with orchestration tools like Airflow or Prefect.\nKnowledge of data modeling, warehousing, and architecture design principles.\nStrong problem-solving skills and attention to detail.\n\nPerks and benefits\nFree Meals\nPF and Gratuity\nMedical and Term Insurance",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Kafka', 'AWS', 'Python', 'Pyspark', 'Java', 'Postgresql', 'Hadoop', 'Spark', 'ETL', 'SQL']",2025-06-13 06:18:14
Senior Azure Data Engineer,Cloud Angles Digital Transformation,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Data Engineer with expertise in leveraging Data Lake architecture and the Azure cloud platform to develop, deploy, and optimise data-driven solutions. . You will play a pivotal role in transforming raw data into actionable insights, supporting strategic decision-making across the organisation.\nResponsibilities\nDesign and implement scalable data science solutions using Azure Data Lake, Azure Data Bricks, Azure Data Factory and related Azure services.\nDevelop, train, and deploy machine learning models to address business challenges.\nCollaborate with data engineering teams to optimise data pipelines and ensure seamless data integration within Azure cloud infrastructure.\nConduct exploratory data analysis (EDA) to identify trends, patterns, and insights.\nBuild predictive and prescriptive models to support decision-making processes.\nExpertise in developing end-to-end Machine learning lifecycle utilizing crisp-DM which includes of data collection, cleansing, visualization, preprocessing, model development, model validation and model retraining\nProficient in building and implementing RAG systems that enhance the accuracy and relevance of model outputs by integrating retrieval mechanisms with generative models.\nEnsure data security, compliance, and governance within the Azure cloud ecosystem.\nMonitor and optimise model performance and scalability in production environments.\nPrepare clear and concise documentation for developed models and workflows.\nSkills Required:\nGood experience in using Pyspark, Python, MLops (Optional), ML flow (Optional), Azure Data Lake Storage. Unity Catalog\nWorked and utilized data from various RDBMS like MYSQL, SQL Server, Postgres and NoSQL databases like MongoDB, Cassandra, Redis and graph DB like Neo4j, Grakn.\nProven experience as a Data Engineer with a strong focus on Azure cloud platform and Data Lake architecture.\nProficiency in Python, Pyspark,\nHands-on experience with Azure services such as Azure Data Lake, Azure Synapse Analytics, Azure Machine Learning, Azure Databricks, and Azure Functions.\nStrong knowledge of SQL and experience in querying large datasets from Data Lakes.\nFamiliarity with data engineering tools and frameworks for data ingestion and transformation in Azure.\nExperience with version control systems (e.g., Git) and CI/CD pipelines for machine learning projects.\nExcellent problem-solving skills and the ability to work collaboratively in a team environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Data Engineering', 'Azure Databricks', 'Pyspark', 'Azure Data Lake', 'Python']",2025-06-13 06:18:15
Walk In Drive II (June 13 Friday) II B2B Tele Sales II Noida,Info Edge,0 - 4 years,1-5 Lacs P.A.,['Noida'],"About Info Edge\nInfoEdges mission is to create world-class platforms that transform lives by continuously innovating. Our products and services are built keeping our customers in mind. We always delight our customers by delivering superior value through enhanced offerings on the internet and other platforms. Through our continuous investment across various businesses, especially in cutting-edge technology, machine learning and artificial intelligence (AI), we have built a robust system that constantly increases our predictive powers on customer behaviour, and optimizes and improves our systems. Our various teams tirelessly work together to solve problems, innovate, and create something to empower our customers.",,,,"['B2B Sales', 'Virtual Sales', 'Sales Process', 'Direct Sales', 'Crm Tool', 'Client Engagement', 'Business Development', 'Salesforce CRM', 'sales', 'Salesforce']",2025-06-13 06:18:17
Corporate Sales Executive,Info Edge,0 - 2 years,Not Disclosed,['Ahmedabad'],"About Info Edge India Ltd\nInfo Edge is Indias leading consumer internet company known for its strong brands in recruitment (naukri.com, naukrigulf.com, iimjobs.com, firstnaukri.com), real estate (99acres.com), matrimony (Jeevansathi.com) and education (shiksha.com).Starting with a classified recruitment online business, naukri.com, the Company has grown and diversified rapidly, setting benchmarks as a pioneer for others to follow either through setting up of in-house brands or through the route of strategic investments and acquisitions. Zomato.com, policybazaar.com & Happily Unmarried Marketing Private Limited are our investee companies to name a few out of many. With years of experience in the domain, strong cash flow generation, and a diversified business portfolio, Info Edge is one of the very few profitable pure play internet companies in the country.These are exciting times for Info Edge as we continue to grow in our businesses and scale newer heights. We are investing across various businesses, especially in cutting-edge technology, machine learning, and artificial intelligence (AI) to increase our predictive powers on customer behavior and continuously optimize and improve our systems.At Info Edge, people are our core competitive advantage and we will continue doing all that is needed to attract and retain the best available talent. Driven by innovation, an experienced and talented leadership team, and a strong entrepreneurial orientation, we pride ourselves on having a culture that promotes meritocracy. Our numerous milestones can largely be credited to an incredibly smart team working in an environment that encourages creativity and going the extra mile to develop products that people love to use and add value to our clients.About BU: Naukri.comNaukri.com, online recruitment classifieds, is a significant player and a market leader in Indias well-established business space. The recruitment space provides all the job seekers with advisory services and caters to different elements of the job listing, employer branding, resume short-listing, career site management, and campus recruitment. With over 67 Million resumes searches daily, Naukri.com has 5 Million job listings, 59 Thousand+ more unique clients and 4.9 Million recruiters connect with the job seekers via emails.The platform, in the online recruitment space, continues to reinforce its established leadership position in India which has given it a competitive edge in the market.",,,,"['B2B Corporate Sales', 'New Client Acquisition', 'Corporate Sales']",2025-06-13 06:18:18
Data Science Lead,Protiviti India,9 - 14 years,25-40 Lacs P.A.,['Mumbai (All Areas)'],"Role & responsibilities\n8+ year bachelors or master’s degree from reputed University with concentration on finance, economics or other quantitative field such as statistics or engineering.\nManage multiple client engagements in Financial Services locally in India\nActively drive pre-sales, sales activities primarily for FS clients locally in Data Science Domain\nUnderstand client requirements in detail and create technical & commercial proposal\nDrive client conversations specifically for business development activities",,,,"['Data Science', 'Natural Language Processing', 'Presales', 'Machine Learning', 'AWS', 'GCP', 'Cloud Platform', 'Python']",2025-06-13 06:18:20
AI ML Engineer,Globallogic,5 - 10 years,Not Disclosed,['Pune'],"reliability\nRole Summary: The AI/ML Engineers will be responsible for developing, testing, and deploying machine learning models and AI algorithms that can be integrated into the product suite. The ideal candidate will have expertise in integrating large language models (LLMs) with external knowledge bases, fine-tuning model architectures for specific tasks, and optimizing retrieval strategies to improve the accuracy and relevance of generated content.\nKey Responsibilities:",,,,"['Artificial Intelligence', 'Machine Learning', 'R', 'Python', 'Tensorflow', 'Natural Language Processing', 'Scikit-Learn', 'SQL', 'Pytorch', 'Huggingface', 'Large Language Model', 'Opencv', 'Keras', 'Spacy']",2025-06-13 06:18:22
AI / ML Engineer,Accenture,15 - 25 years,Not Disclosed,['Pune'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Large Language Models\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n15 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI/ML Engineer, you will develop applications and systems utilizing AI tools, Cloud AI services, and GenAI models. Your role involves creating cloud or on-prem application pipelines with production-ready quality, incorporating deep learning, neural networks, chatbots, and image processing.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead the implementation of large language models in AI applications.- Research and apply cutting-edge AI techniques to enhance system performance.- Contribute to the development and deployment of AI solutions across various domains.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Large Language Models.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Large Language Models.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['machine learning', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'image processing', 'algorithms', 'chatbot', 'python', 'natural language processing', 'power bi', 'artificial intelligence', 'sql', 'deep learning', 'data science', 'text mining', 'data visualization', 'ml']",2025-06-13 06:18:23
AI / ML Engineer,Accenture,12 - 15 years,Not Disclosed,['Navi Mumbai'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Large Language Models\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI / ML Engineer, you will engage in the development of applications and systems that leverage artificial intelligence tools and cloud AI services. Your typical day will involve designing and implementing production-ready solutions, ensuring that they meet quality standards. You will work with various AI models, including generative AI, and may also explore deep learning, neural networks, chatbots, and image processing technologies. Collaboration with cross-functional teams will be essential to integrate these solutions effectively into existing systems and workflows.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate knowledge sharing and training sessions to enhance team capabilities.- Monitor project progress and ensure alignment with strategic goals.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Large Language Models.- Strong understanding of cloud-based AI services and deployment strategies.- Experience with deep learning frameworks such as TensorFlow or PyTorch.- Familiarity with natural language processing techniques and tools.- Ability to design and implement scalable AI solutions.\nAdditional Information:- The candidate should have minimum 12 years of experience in Large Language Models.- This position is based at our Mumbai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['natural language processing', 'deployment process', 'artificial intelligence', 'tensorflow', 'pytorch', 'image processing', 'chatbot', 'kubernetes', 'python', 'cmmi', 'machine learning', 'apache tomcat', 'change management', 'sql', 'gradle', 'java', 'incident management', 'jenkins', 'html', 'ml', 'itil', 'jira']",2025-06-13 06:18:25
AI / ML Engineer,Accenture,12 - 15 years,Not Disclosed,['Kolkata'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Large Language Models\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI / ML Engineer, you will engage in the development of applications and systems that leverage artificial intelligence tools and cloud AI services. Your typical day will involve designing and implementing production-ready solutions, ensuring that they meet quality standards. You will work with generative AI models and may also explore various advanced technologies such as deep learning, neural networks, chatbots, and image processing to enhance the functionality and efficiency of the systems you create. Collaboration with cross-functional teams will be essential to ensure that the solutions are robust and scalable, addressing the needs of diverse stakeholders.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate knowledge sharing and mentoring within the team to enhance overall performance.- Continuously evaluate and improve existing processes and systems to drive efficiency and innovation.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Large Language Models.- Experience with cloud-based AI services and deployment strategies.- Strong understanding of deep learning frameworks such as TensorFlow or PyTorch.- Familiarity with natural language processing techniques and tools.- Ability to design and implement scalable AI solutions that meet production standards.\nAdditional Information:- The candidate should have minimum 12 years of experience in Large Language Models.- This position is based at our Kolkata office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['deep learning frameworks', 'natural language processing', 'artificial intelligence', 'tensorflow', 'pytorch', 'image processing', 'chatbot', 'python', 'cnn', 'neural networks', 'numpy', 'machine learning', 'sql', 'pandas', 'deep learning', 'rnn', 'data science', 'keras', 'text mining', 'ml']",2025-06-13 06:18:27
AI / ML Engineer,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Large Language Models\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI/ML Engineer, you will develop applications and systems utilizing AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production-ready quality. You will apply GenAI models as part of the solution, including deep learning, neural networks, chatbots, and image processing.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead the implementation of AI/ML models.- Conduct research on emerging AI technologies.- Optimize AI algorithms for performance and scalability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Large Language Models.- Strong understanding of natural language processing techniques.- Experience with cloud AI services like AWS SageMaker or Google AI Platform.- Knowledge of deep learning frameworks such as TensorFlow or PyTorch.- Hands-on experience in developing AI applications.- Familiarity with deploying AI models in production environments.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Large Language Models.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['natural language processing', 'aws sagemaker', 'artificial intelligence', 'tensorflow', 'pytorch', 'image processing', 'algorithms', 'chatbot', 'python', 'scikit-learn', 'google', 'numpy', 'machine learning', 'pandas', 'deep learning', 'java', 'data science', 'ai platform', 'keras', 'aws', 'ml']",2025-06-13 06:18:28
AI / ML Engineer,Accenture,15 - 25 years,Not Disclosed,['Kolkata'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Large Language Models\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n15 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI/ML Engineer, you will develop applications and systems utilizing AI tools, Cloud AI services, and GenAI models. Your role involves creating cloud or on-prem application pipelines with production-ready quality, incorporating deep learning, neural networks, chatbots, and image processing.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead the implementation of large language models in AI applications.- Research and apply cutting-edge AI techniques to enhance system performance.- Contribute to the development and deployment of AI solutions across various domains.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Large Language Models.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Large Language Models.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['machine learning', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'image processing', 'algorithms', 'chatbot', 'python', 'natural language processing', 'power bi', 'artificial intelligence', 'sql', 'deep learning', 'data science', 'text mining', 'data visualization', 'ml']",2025-06-13 06:18:30
AI / ML Engineer,Accenture,15 - 25 years,Not Disclosed,['Gurugram'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Large Language Models\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n15 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI / ML Engineer, you will engage in the development of applications and systems that leverage artificial intelligence tools and cloud AI services. Your typical day will involve designing and implementing production-ready solutions, ensuring that they meet quality standards. You will work with generative AI models and may also explore various domains such as deep learning, neural networks, chatbots, and image processing, contributing to innovative projects that push the boundaries of technology.\nRoles & Responsibilities:- Expected to be a Subject Matter Expert with deep knowledge and experience.- Should have influencing and advisory skills.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Mentor junior professionals to foster their growth and development.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Large Language Models.- Experience with cloud-based AI services and deployment strategies.- Strong understanding of deep learning frameworks such as TensorFlow or PyTorch.- Familiarity with natural language processing techniques and tools.- Ability to design and implement scalable AI solutions.\nAdditional Information:- The candidate should have minimum 15 years of experience in Large Language Models.- This position is based at our Gurugram office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['deep learning frameworks', 'natural language processing', 'artificial intelligence', 'tensorflow', 'pytorch', 'image processing', 'chatbot', 'python', 'cnn', 'neural networks', 'numpy', 'machine learning', 'sql', 'pandas', 'deep learning', 'rnn', 'data science', 'keras', 'text mining', 'ml']",2025-06-13 06:18:32
AI / ML Engineer,Accenture,12 - 15 years,Not Disclosed,['Pune'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Large Language Models\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI / ML Engineer, you will engage in the development of applications and systems that leverage artificial intelligence tools and cloud AI services. Your typical day will involve designing and implementing production-ready solutions, ensuring that they meet quality standards. You will work with various AI models, including generative AI, and may also explore deep learning, neural networks, chatbots, and image processing technologies. Collaboration with cross-functional teams will be essential as you contribute to innovative projects that push the boundaries of AI capabilities.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate knowledge sharing and mentoring within the team to enhance overall performance.- Continuously evaluate and improve existing processes and methodologies to optimize team efficiency.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Large Language Models.- Strong understanding of cloud-based AI services and their integration.- Experience with deep learning frameworks such as TensorFlow or PyTorch.- Familiarity with natural language processing techniques and tools.- Ability to design and implement scalable AI solutions.\nAdditional Information:- The candidate should have minimum 12 years of experience in Large Language Models.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['natural language processing', 'artificial intelligence', 'tensorflow', 'pytorch', 'ml', 'image processing', 'chatbot', 'hlookup', 'lookup', 'python', 'conditional formatting', 'sumif', 'pivot table', 'vlookup', 'machine learning', 'deep learning', 'advanced excel', 'computer vision', 'keras', 'countif']",2025-06-13 06:18:34
AI / ML Engineer,Accenture,15 - 25 years,Not Disclosed,['Kolkata'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Large Language Models\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n15 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI / ML Engineer, you will engage in the development of applications and systems that leverage artificial intelligence tools and cloud AI services. Your typical day will involve designing and implementing production-ready solutions, ensuring that they meet quality standards. You will work with generative AI models and may also explore various domains such as deep learning, neural networks, chatbots, and image processing, contributing to innovative projects that push the boundaries of technology.\nRoles & Responsibilities:- Expected to be a Subject Matter Expert with deep knowledge and experience.- Should have influencing and advisory skills.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Mentor junior professionals to foster their growth and development.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Large Language Models.- Strong understanding of cloud computing platforms and services.- Experience with deep learning frameworks such as TensorFlow or PyTorch.- Familiarity with natural language processing techniques and tools.- Ability to design and implement scalable AI solutions.\nAdditional Information:- The candidate should have minimum 15 years of experience in Large Language Models.- This position is based at our Kolkata office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['artificial intelligence', 'tensorflow', 'pytorch', 'cloud computing platform', 'cloud computing', 'image processing', 'chatbot', 'python', 'natural language processing', 'neural networks', 'machine learning', 'deep learning', 'data science', 'computer vision', 'keras', 'opencv', 'ml']",2025-06-13 06:18:35
AI / ML Engineer,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :AI / ML Engineer\n\n\n\n\n\nProject Role Description :Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\n\n\nMust have skills :Large Language Models\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n18 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an AI/ML Engineer, you will develop applications and systems utilizing AI tools, Cloud AI services, and GenAI models. Your role involves creating cloud or on-prem application pipelines with production-ready quality, incorporating deep learning, neural networks, chatbots, and image processing.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead the implementation of large language models in AI applications.- Research and apply cutting-edge AI techniques to enhance system performance.- Contribute to the development and deployment of AI solutions across various domains.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Large Language Models.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Large Language Models.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['machine learning', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'image processing', 'algorithms', 'chatbot', 'python', 'natural language processing', 'power bi', 'artificial intelligence', 'sql', 'deep learning', 'data science', 'text mining', 'data visualization', 'ml']",2025-06-13 06:18:37
Software Engineer II ( Java Fullstack),JPMorgan Chase Bank,3 - 10 years,Not Disclosed,['Hyderabad'],"You re ready to gain the skills and experience needed to grow within your role and advance your career and we have the perfect software engineering opportunity for you.\nAs a Software Engineer II at JPMorgan Chase within the Consumer anc Community Banking , you are part of an agile team that works to enhance, design, and deliver the software components of the firm s state-of-the-art technology products in a secure, stable, and scalable way. As an emerging member of a software engineering team, you execute software solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role.\nJob responsibilities\nExecutes standard software solutions, design, development, and technical troubleshooting\nWrites secure and high-quality code using the syntax of at least one programming language with limited guidance\nDesigns, develops, codes, and troubleshoots with consideration of upstream and downstream systems and technical implications\nApplies knowledge of tools within the Software Development Life Cycle toolchain to improve the value realized by automation\nApplies technical troubleshooting to break down solutions and solve technical problems of basic complexity\nGathers, analyzes, and draws conclusions from large, diverse data sets to identify problems and contribute to decision-making in service of secure, stable application development\nLearns and applies system processes, methodologies, and skills for the development of secure, stable code and systems\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 2+ years applied experience\nHands-on practical experience in system design, application development, testing, and operational stability\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nDemonstrable ability to code in one or more languages\nExperience across the whole Software Development Life Cycle\nExpereince in Java, J2EE, Fullstack, React JS, AWS.\nExposure to agile methodologies such as CI/CD, Application Resiliency, and Security\nEmerging knowledge of software applications and technical processes within a technical discipline (e. g. , cloud, artificial intelligence, machine learning, mobile, etc. )\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Artificial Intelligence', 'Debugging', 'Machine learning', 'Agile', 'System design', 'Application development', 'Troubleshooting', 'Downstream']",2025-06-13 06:18:39
Speech Engineer,BUSINESSNEXT,2 - 5 years,Not Disclosed,['Noida'],"What would you do?\nSystem Design: Architect and design end-to-end speech processing pipelines, from data acquisition to model deployment. Ensure systems are scalable, efficient, and maintainable.\nAdvanced Modeling: Develop and implement advanced machine learning models for speech recognition, speaker diarization, and related tasks. Utilize state-of-the-art techniques such as deep learning, transfer learning, and ensemble methods.\nResearch and Development: Conduct research to explore new methodologies and tools in the field of speech processing. Publish findings and present at industry conferences.",,,,"['Tensorflow', 'Pytorch', 'Stt', 'Speech Recognition', 'transfer learning', 'deep speech', 'Natural Language Processing', 'model deployment', 'Data Acquisition', 'Machine Learning', 'Deep Learning', 'Sts', 'speech processing', 'whisper', 'text to speech']",2025-06-13 06:18:41
Software Development Engineer,Accenture,7 - 12 years,Not Disclosed,['Mumbai'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :Microsoft SQL Server Analysis Services (SSAS)\n\n\n\n\nGood to have skills :Microsoft SQL Server Integration Services (SSIS), Microsoft SQL Server Reporting ServicesMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Software Development Engineer, you will analyze, design, code, and test multiple components of application code across one or more clients. You will perform maintenance, enhancements, and/or development work. Your typical day will involve analyzing requirements, designing solutions, writing code, and conducting testing to ensure the quality of the application. You will collaborate with team members, participate in code reviews, and contribute to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Analyze requirements and design solutions.- Write code and conduct testing to ensure the quality of the application.- Participate in code reviews.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft SQL Server Analysis Services (SSAS), Microsoft SQL Server Integration Services (SSIS), Microsoft SQL Server Reporting Services.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Microsoft SQL Server Analysis Services (SSAS).- This position is based in Mumbai.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ms sql server administration', 'sql server', 'machine learning algorithms', 'statistics', 'data munging', 'algorithms', 'python', 'software development', 'ssas', 'power bi', 'machine learning', 'sql', 'tableau', 'solution design', 'code review', 'data visualization', 'ssis', 'requirement analysis']",2025-06-13 06:18:42
Software Development Engineer,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :Sitecore Commerce\n\n\n\n\nGood to have skills :ASP.NET MVCMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact- Manage the team and ensure successful project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Sitecore Commerce, ASP.NET MVC- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Sitecore Commerce- This position is based at our Chennai office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sitecore', 'net mvc', 'machine learning algorithms', 'asp', 'statistics', 'algorithms', 'python', 'software development', 'power bi', 'machine learning', 'javascript', 'jquery', 'sql', 'asp.net core mvc', 'tableau', 'django', 'java', 'asp.net', 'project delivery', 'html', 'mysql', 'mvc', 'mongodb', 'data munging']",2025-06-13 06:18:44
Technology Support Engineer,Accenture,3 - 8 years,Not Disclosed,['Mumbai'],"Project Role :Technology Support Engineer\n\n\n\n\n\nProject Role Description :Resolve incidents and problems across multiple business system components and ensure operational stability. Create and implement Requests for Change (RFC) and update knowledge base articles to support effective troubleshooting. Collaborate with vendors and help service management teams with issue analysis and resolution.\n\n\n\nMust have skills :Qlik Sense\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Technology Support Engineer, you will be responsible for resolving incidents and problems across multiple business system components, ensuring operational stability. You will create and implement Requests for Change (RFC) and update knowledge base articles to support effective troubleshooting. Collaborating with vendors and assisting service management teams with issue analysis and resolution will be part of your daily tasks.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Proactively identify and resolve technical issues.- Assist in the implementation of system upgrades and enhancements.- Provide technical support to end-users and stakeholders.- Document and maintain system configurations and processes.- Conduct regular system audits to ensure data integrity and security.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Qlik Sense.- Strong understanding of data visualization tools.- Experience with statistical analysis and machine learning algorithms.- Hands-on experience in implementing various machine learning algorithms.- Solid grasp of data munging techniques.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Qlik Sense.- This position is based at our Mumbai office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['machine learning', 'qlikview', 'data integration tools', 'data visualization', 'data munging', 'python', 'data analysis', 'natural language processing', 'spotfire', 'data warehousing', 'power bi', 'sql', 'technical support', 'tableau', 'r', 'java', 'data science', 'troubleshooting', 'etl', 'statistics']",2025-06-13 06:18:46
Technology Support Engineer,Accenture,3 - 8 years,Not Disclosed,['Mumbai'],"Project Role :Technology Support Engineer\n\n\n\n\n\nProject Role Description :Resolve incidents and problems across multiple business system components and ensure operational stability. Create and implement Requests for Change (RFC) and update knowledge base articles to support effective troubleshooting. Collaborate with vendors and help service management teams with issue analysis and resolution.\n\n\n\nMust have skills :Qlik Sense\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Technology Support Engineer, you will be responsible for resolving incidents and problems across multiple business system components, ensuring operational stability. You will create and implement Requests for Change (RFC) and update knowledge base articles to support effective troubleshooting. Collaborating with vendors and assisting service management teams with issue analysis and resolution will be part of your daily tasks.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Proactively identify and resolve technical issues.- Assist in the implementation of system upgrades and enhancements.- Provide technical support to end-users and stakeholders.- Document and maintain accurate records of technical procedures.- Stay updated on industry trends and best practices.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Qlik Sense.- Strong understanding of data visualization tools.- Experience with statistical analysis and machine learning algorithms.- Hands-on experience in implementing various machine learning algorithms.- Solid grasp of data munging techniques.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Qlik Sense.- This position is based at our Mumbai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['machine learning', 'qlikview', 'data integration tools', 'data visualization', 'data munging', 'python', 'data analysis', 'natural language processing', 'spotfire', 'data warehousing', 'power bi', 'sql', 'technical support', 'tableau', 'r', 'java', 'data science', 'troubleshooting', 'etl', 'statistics']",2025-06-13 06:18:48
"Senior Staff Engineer, Generative-AI",Nagarro,10 - 15 years,Not Disclosed,[],"Total experience 10+ Years.\nDeep understanding of Generative AI fundamentals and transformer-based architectures.\nStrong experience in Cloud Architecture (eg, AWS, Azure, GCP) for deploying scalable AI systems.\nProven experience with BERT, GPT, LLaMA, and similar LLMs.\nStrong hands-on experience in prompt engineering and RAG techniques.\nExperience in fine-tuning and deploying models using frameworks like Hugging Face Transformers, LangChain, or equivalent.\nFamiliarity with multi-agent AI systems and collaborative model workflows.\nProficient in Python and machine learning libraries (eg, PyTorch, TensorFlow).\nStrong problem-solving skills and a passion for AI innovation and ethical development.\nExperience integrating models into enterprise platforms and APIs.\nUnderstanding of ML Ops practices and CI/CD pipelines for AI deployment.\nBackground in Natural Language Processing (NLP) and Knowledge Engineering.\nExcellent communication skills, with the ability to articulate solutions effectively.\nRESPONSIBILITIES:\nUnderstanding the client s business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements.\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the client s requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc, and ensure that all relevant best practices are followe'd.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements.\n\n\nBachelor s or master s degree in computer science, Information Technology, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Project implementation', 'Architecture', 'GCP', 'Technical design', 'Machine learning', 'Natural language processing', 'High level design', 'Information technology', 'Python']",2025-06-13 06:18:49
"Staff Engineer, Generative-AI",Nagarro,7 - 12 years,Not Disclosed,[],"Total experience 7+ Years.\nDeep understanding of Generative AI fundamentals and transformer-based architectures.\nStrong experience in Cloud Architecture (eg, AWS, Azure, GCP) for deploying scalable AI systems.\nProven experience with BERT, GPT, LLaMA, and similar LLMs.\nStrong hands-on experience in prompt engineering and RAG techniques.\nExperience in fine-tuning and deploying models using frameworks like Hugging Face Transformers, LangChain, or equivalent.\nFamiliarity with multi-agent AI systems and collaborative model workflows.\nProficient in Python and machine learning libraries (eg, PyTorch, TensorFlow).\nStrong problem-solving skills and a passion for AI innovation and ethical development.\nExperience integrating models into enterprise platforms and APIs.\nUnderstanding of ML Ops practices and CI/CD pipelines for AI deployment.\nBackground in Natural Language Processing (NLP) and Knowledge Engineering.\nExcellent communication skills, with the ability to articulate solutions effectively.\nRESPONSIBILITIES:\nUnderstanding the client s business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements.\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the client s requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc, and ensure that all relevant best practices are followe'd.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements.\n\n\nBachelor s or master s degree in computer science, Information Technology, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Project implementation', 'Architecture', 'GCP', 'Technical design', 'Machine learning', 'Natural language processing', 'High level design', 'Information technology', 'Python']",2025-06-13 06:18:51
"Director, Group AI Platform Engineering",Hsbc,6 - 11 years,Not Disclosed,['Pune'],"Some careers have more impact than others.\nIf you re looking for further opportunities to develop your career, take the next step in fulfilling your potential right here at HSBC.\nHSBC is one of the largest banking and financial services organizations in the world, with operations in 62 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realize their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Director, Group AI Platform Engineering\nDepartment : Emerging Technology\nThe Opportunity:\nIn this fantastic role, you will join a growing team to lead an experienced group of engineers in designing, developing and maintaining key components of our Group AI Platform for scaled consumption across our Group Businesses and Functions with high availability, flexibility and resilience.\nWithin the Group Emerging Technology, Innovation and Ventures (ETIV) team, this is a critical role to ensure a best-in-class AI platform is built to scale AI across the bank and improve the time to market for AI use case delivery.\nLocation: This role can be based in Guangzhou, China or Pune, India.\nWhat you ll do:\nDeveloping platform capabilities to span cloud and on-premise infrastructure across AWS, GCP, Azure, and Ali environments.\nBuilding and maintaining automated pipelines for continuous training, testing, and deployment of machine learning models, with integrated Responsible AI controls.\nDesigning and developing platform capabilities including model garden, AI gateway, RAG pipeline, Agent Framework, etc. to enable effective and efficient AI solution delivery\nEnsuring compliance to architecture principle, technical standards, security requirements and software development life cycle controls\nSolving complex and challenging issues to ensure successful deliveries of AI platform with high quality and resilience of platform services in production\nEstablishing engineering best practices, including CI/CD pipeline, testing frameworks, and monitoring for AI platform and applications, and ensure continuous improvement of engineering practices\nOverseeing the integration of AI systems with enterprise data sources, APIs, and existing technology infrastructure\nGuiding a team of engineers in building robust, scalable, and maintainable AI software solutions\nImplementing observability, reliability, and performance optimizations for AI systems in production\nCollaborating with AI engineers and customers to translate AI capabilities into production-ready systems\nRequirements\nWhat you will need to succeed in the role:\nTechnical:\nDesign and development of AI solutions\nDesign and build of large, scalable platforms\nDesign, development and maintenance of AI platforms\nTraining, fine-tuning, testing and evaluating AI and GenAI-based solutions\nCloud platforms such as GCP, Azure, and Ali.\nContainerization technologies (Docker, Kubernetes, OpenShift) preferred\nProgramming languages such as Java and Python\nCI/CD tools (Jenkins, GitHub Actions)\nAutomation testing, and observability\nAgile Methodology\nBehavioral Skills:\nTeam Management\nEngagement\nOutcome Orientated\nCognitive Skills:\nLogic and reasoning\nCollaboration\nQuantitative\nSelective attention",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation testing', 'Team management', 'Cloud', 'Software development life cycle', 'Manager Technology', 'Programming', 'Agile methodology', 'Monitoring', 'Financial services', 'Python']",2025-06-13 06:18:53
"Director, Common AI Capabilities Engineering",Hsbc,7 - 11 years,Not Disclosed,['Pune'],"Some careers have more impact than others.\nIf you re looking for further opportunities to develop your career, take the next step in fulfilling your potential right here at HSBC.\nHSBC is one of the largest banking and financial services organizations in the world, with operations in 62 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realize their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Director, Common AI Capabilities Engineering\nDepartment: Emerging Technology\nLocation: This role can be based in Guangzhou, China or Pune, India.\nThe Opportunity:\nIn this fantastic new role, you will join a growing team to lead an experienced group of engineers in designing, developing, and maintaining key capabilities supporting our Group AI offering (e.g., Speech transcription, translation, knowledge management) for scaled consumption across our Group Businesses and Functions.\nWithin the Group Emerging Technology, Innovation and Ventures (ETIV) team, this is a critical role to ensure a best-in-class AI platform is built to scale AI across the bank and improve the time to market for AI use case delivery.\nWhat you ll do:\nApplying quality software engineering practices throughout the software development lifecycle.\nManaging and supporting a global team of ML Software engineers.\nEngaging with firm-wide AI/ML teams to shape future roadmaps that align with Group objectives.\nBuilding and operating highly sophisticated machine learning infrastructure.\nIntegrating AI capabilities into our Group AI Platform which spans cloud and on-premise infrastructure across AWS, GCP, Azure, and Ali environments.\nRequirements\nWhat you will need to succeed in the role:\nTechnical:\nSoftware and ML engineering\nAPIs and microservices design and build\nCloud platforms such as AWS, GCP, Azure, and Ali\nContainerization technologies (Docker, Kubernetes, OpenShift)\nCI/CD tools (Jenkins, GitHub Actions)\nData science tools (e.g. PyTorch, Jax, Numpy)\nProgramming languages such as C++ and Python\nAgile Methodology\nBehavioral Skills:\nTeam Management\nEngagement\nEthical\nOutcome Orientated\nCognitive Skills:\nLogic and reasoning\nCollaboration\nQuantitative\nSelective attention",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['C++', 'Team management', 'Transcription', 'data science', 'Machine learning', 'Cloud', 'Software development life cycle', 'Programming', 'Agile methodology', 'Financial services']",2025-06-13 06:18:55
Support Engineer,Amazon,2 - 7 years,Not Disclosed,['Hyderabad'],"We support a fast-paced environment where each day brings new challenges and opportunities.\nAs a Support Engineer, you will play a pivotal role in ensuring the stability, compliance, and operational excellence of our enterprise Data Warehouse (DW) environment.\nIn this role, you will be responsible for monitoring and maintaining production data pipelines, proactively identifying and resolving issues that impact data quality, availability, or timeliness.\nYou ll collaborate closely with data engineers and cross-functional teams to troubleshoot incidents, implement scalable solutions, and enhance the overall resilience of our data infrastructure.\nA key aspect of this role involves supporting our data compliance and governance initiatives, ensuring systems align with internal policies and external regulatory standards such as GDPR.\nYou will help enforce access controls, manage data retention policies, and support audit readiness through strong logging and monitoring practices.\nYou ll also lead efforts to automate manual support processes, improving team efficiency and reducing operational risk.\nAdditionally, you will be responsible for maintaining clear, up-to-date documentation and runbooks for operational procedures and issue resolution, promoting consistency and knowledge sharing across the team.\nWe re looking for a self-motivated, quick-learning team player with a strong sense of ownership and a can-do attitude, someone who thrives in a dynamic, high-impact environment and is eager to make meaningful contributions to our data operations.\n2+ years of software development, or 2+ years of technical support experience\nExperience scripting in modern program languages\nExperience troubleshooting and debugging technical systems Knowledge of web services, distributed systems, and web application development\nExperience troubleshooting maintaining hardware software RAID\nExperience with REST web services, XML, JSON",,,,"['XML', 'Debugging', 'Machine learning', 'Agile', 'JSON', 'Distribution system', 'Technical support', 'Monitoring', 'Product support', 'Auditing']",2025-06-13 06:18:56
Support Engineer,Amazon,2 - 7 years,Not Disclosed,['Hyderabad'],"We support a fast-paced environment where each day brings new challenges and opportunities.\nAs a Support Engineer, you will play a pivotal role in ensuring the stability, compliance, and operational excellence of our enterprise Data Warehouse (DW) environment.\nIn this role, you will be responsible for monitoring and maintaining production data pipelines, proactively identifying and resolving issues that impact data quality, availability, or timeliness.\nYou ll collaborate closely with data engineers and cross-functional teams to troubleshoot incidents, implement scalable solutions, and enhance the overall resilience of our data infrastructure.\nA key aspect of this role involves supporting our data compliance and governance initiatives, ensuring systems align with internal policies and external regulatory standards such as GDPR.\nYou will help enforce access controls, manage data retention policies, and support audit readiness through strong logging and monitoring practices.\nYou ll also lead efforts to automate manual support processes, improving team efficiency and reducing operational risk.\nAdditionally, you will be responsible for maintaining clear, up-to-date documentation and runbooks for operational procedures and issue resolution, promoting consistency and knowledge sharing across the team.\nWe re looking for a self-motivated, quick-learning team player with a strong sense of ownership and a can-do attitude, someone who thrives in a dynamic, high-impact environment and is eager to make meaningful contributions to our data operations.\n2+ years of software development, or 2+ years of technical support experience\nBachelors degree in engineering or equivalent\nExperience troubleshooting and debugging technical systems\nExperience scripting in modern program languages\nExperience with SQL databases (querying and analyzing) Good to have experience with AWS technologies stack including Redshift, RDS, S3, EMR or similar solutions build around Hive/Spark etc\nGood to have experience with reporting tools like Tableau, OBIEE or other BI packages.\nKnowledge of software engineering best practices across the development lifecycle is a plus",,,,"['Debugging', 'Machine learning', 'Agile', 'Data quality', 'Technical support', 'Monitoring', 'Product support', 'Reporting tools', 'SQL', 'Auditing']",2025-06-13 06:18:58
Senior ETL Engineer/Consultant Specialist,Hsbc,3 - 6 years,Not Disclosed,['Hyderabad'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Consultant Specialist\nIn this role you will be\nDesign and Develop ETL Processes: Lead the design and implementation of ETL processes using all kinds of batch/streaming tools to extract, transform, and load data from various sources into GCP.\nCollaborate with stakeholders to gather requirements and ensure that ETL solutions meet business needs.\nData Pipeline Optimization: Optimize data pipelines for performance, scalability, and reliability, ensuring efficient data processing workflows.\nMonitor and troubleshoot ETL processes, proactively addressing issues and bottlenecks.\nData Integration and Management: I ntegrate data from diverse sources, including databases, APIs, and flat files, ensuring data quality and consistency.\nManage and maintain data storage solutions in GCP (e. g. , BigQuery, Cloud Storage) to support analytics and reporting.\nGCP Dataflow Development: Write Apache Beam based Dataflow Job for data extraction, transformation, and analysis, ensuring optimal performance and accuracy.\nCollaborate with data analysts and data scientists to prepare data for analysis and reporting.\nAutomation and Monitoring: Implement automation for ETL workflows using tools like Apache Airflow or Cloud Composer, enhancing efficiency and reducing manual intervention.\nSet up monitoring and alerting mechanisms to ensure the health of data pipelines and compliance with SLAs.\nData Governance and Security: Apply best practices for data governance, ensuring compliance with industry regulations (e. g. , GDPR, HIPAA) and internal policies.\nCollaborate with security teams to implement data protection measures and address vulnerabilities.\nDocumentation and Knowledge Sharing: Document ETL processes, data models, and architecture to facilitate knowledge sharing and onboarding of new team members.\nConduct training sessions and workshops to share expertise and promote best practices within the team.\n\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nEducation: Bachelor s degree in Computer Science, Information Systems, or a related field.\nExperience: Minimum of 5 years of industry experience in data engineering or ETL development, with a strong focus on Data Stage and GCP.\nProven experience in designing and managing ETL solutions, including data modeling, data warehousing, and SQL development.\nTechnical Skills: Strong knowledge of GCP services (e. g. , BigQuery, Dataflow, Cloud Storage, Pub/Sub) and their application in data engineering.\nExperience of cloud-based solutions, especially in GCP, cloud certified candidate is preferred.\nExperience and knowledge of Bigdata data processing in batch mode and streaming mode, proficient in Bigdata eco systems, e. g. Hadoop, HBase, Hive, MapReduce, Kafka, Flink, Spark, etc.\nFamiliarity with Java Python for data manipulation on Cloud/Bigdata platform.\nAnalytical Skills: Strong problem-solving skills with a keen attention to detail.\nAbility to analyze complex data sets and derive meaningful insights.\nBenefits: Competitive salary and comprehensive benefits package.\nOpportunity to work in a dynamic and collaborative environment on cutting-edge data projects.\nProfessional development opportunities to enhance your skills and advance your career.\nIf you are a passionate data engineer with expertise in ETL processes and a desire to make a significant impact within our organization, we encourage you to apply for this exciting opportunity!",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data modeling', 'HIPAA', 'Data quality', 'Apache', 'Monitoring', 'Analytics', 'Financial services', 'Python']",2025-06-13 06:19:00
Cloud Support Engineer / Senior Software Engineer,Hsbc,2 - 5 years,Not Disclosed,['Pune'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Senior Software Engineer\nIn this role, you will:\nProvide three tier (L1, L2, L3) support to all applications and provide assistance to all end users.\nProactively identify any issues in production via automated monitoring, history of production issues and trends.\nMaintain schedule jobs and perform troubleshoot on processes.\nAnalyze all vendor applications and provide operational support.\nDocument all production applications and resolve all application issues and answer all requests.\nMonitor all performance metrics for various production systems and identify root cause for all technical issues and recommend solutions.\nAnalyze all applications and recommend necessary upgrades and patches and perform troubleshoot on all issues.\nMaintain effective relationships with various system administrators and development teams.\nParticipate in periodic meetings and maintain all applications for productions and plan appropriate various strategies.\nPublishing GCP cost Dashboards, Alerting and monitoring\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nKnowledge of Incident Management Problem Management is mandatory.\nProduction support ticketing knowledge is an advantage (Remedy, Jira, SQL Assistant, Blade logic, Splunk, MuleSoft, App Dynamics, GitHUB Knowledge/ Websphere etc. )\nExcellent communication skills in both Oral and Written communication.\nExcellent understanding of machine learning setup in Google architecture and google Analytics products\nShould have experience working in agile and devops environment using team collaboration tools such as Confluence, JIRA.\nProgramming skills and hands-on experience in Python desirable\nProficiency in working with cloud based native data stores/databases\nKnowledge on design patterns for GCP third party tools setup and native tools usage\nExperience in publishing GCP cost Dashboards, Alerting and monitoring",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Ticketing', 'operational support', 'Publishing', 'Google Analytics', 'Production support', 'Problem management', 'Incident management', 'Financial services', 'SQL', 'Remedy']",2025-06-13 06:19:01
Senior Software Engineer,Morningstar,4 - 8 years,Not Disclosed,"['Thane', 'Navi Mumbai', 'Mumbai (All Areas)']","Position Title: Senior Software Engineer\nThe Role: In this role, you will collaborate with tech manager, scrum Master, data analysts, and developers to build technology solutions for Morningstar`s Equity applications. You should have strong understanding of Core Java, JSP, Spring, SQL, Javascript, Component based architectures and create scalable, flexible technical solutions. You would support existing systems, study their enterprise complexities and develop/implement better systems with modern software development practices. Developing good understanding of existing systems on other platforms and its database is a beginning step.\n\nResponsibilities:\n-Design & develop web and enterprise solutions to be flexible, scalable & extensible.\n-Improve complex data flow, data structures and db design to move to next platform.\n-Enforce good agile practices like test driven development, Continuous Integration.\n-Hands-on development will be an integral part of the responsibilities.\n-Develop areas of continuous and automated deployment.\n-Introduce and follow good development practices, innovative frameworks and technology solutions that help business move faster.\n-Follow best practices like estimation, planning, reporting and improvement brought to processes in every day work.\n\nRequirements:\nThese are the most important skills, qualities, etc. that wed like for this role.\n-4-6 Years of experience -Full Stack Developer\n-Hands-on in Java, Adv Java, Spring Framework, JavaScript\n-Hands-on on AWS components\n- Demonstrated familiarity with AI-powered assistants (e.g., GitHub Copilot, ChatGPT) for code generation, debugging, and/or other technical tasks.\n-Very Strong knowledge of databases and hands on MS SQL/MySQL/PostgreSQL or NoSQL DB DynamoDB/MongoDB.\n-Experience with building REST based APIs.\n-Knowledge of any JavaScript framework like Angular JS (version >2)/ NodeJS/VueJS etc.\n-Be aware of activity in the open source world.\nContributing back to open source is a big plus.\n-Participation in full life-cycle system development with teams of developers, analysts, and testers.\n-Completed Bachelors degree in Engineering\n-Experience with modern development practices in areas of Product design, Requirement Analysis, Test Driven Development, Automation & Unit Testing, in a product development environment.\n-Excellent listening, written and verbal communication skills.\nGood to Have: AWS Hands-on experience\nExposure to Capital Market domain preferred (Indexes, Equity etc.) Machine Learning knowledge.\n\nMorningstar India is an equal opportunity employer",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java Fullstack', 'Java', 'Vue.Js', 'Java Spring Boot', 'Javascript', 'React.Js', 'AWS', 'Angular']",2025-06-13 06:19:03
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Microsoft Azure Analytics Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact- Manage the team and ensure successful project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Azure Analytics Services- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Microsoft Azure Analytics Services- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['analytics services', 'azure analytics', 'microsoft azure', 'machine learning algorithms', 'statistics', 'python', 'team management', 'ssas', 'power bi', 'machine learning', 'sql server', 'sql', 'data quality', 'tableau', 'ssrs', 'project delivery', 'data visualization', 'ssis', 'data munging']",2025-06-13 06:19:05
Application Lead,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :PySpark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for overseeing the application development process and ensuring successful project delivery.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Lead the design and development of applications.- Act as the primary point of contact for application-related queries.- Collaborate with team members to ensure project success.- Provide technical guidance and mentorship to junior team members.- Stay updated on industry trends and best practices.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in PySpark.- Strong understanding of big data processing and analytics.- Experience with data processing frameworks like Apache Spark.- Hands-on experience in building scalable data pipelines.- Knowledge of cloud platforms for data processing.- Experience in performance tuning and optimization.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in PySpark.- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'spark', 'big data', 'performance tuning', 'data processing', 'hive', 'scala', 'sql', 'plsql', 'apache', 'java', 'linux', 'mysql', 'hadoop', 'python', 'project management', 'oracle', 'microsoft azure', 'machine learning', 'sql server', 'application development', 'project delivery', 'sqoop', 'agile', 'aws', 'unix']",2025-06-13 06:19:07
Application Lead,Accenture,7 - 12 years,Not Disclosed,['Bhubaneswar'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Microsoft Azure Databricks\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. You will collaborate with teams and contribute to key decisions, providing solutions to problems for your immediate team and across multiple teams. Your typical day will involve designing and implementing applications, troubleshooting issues, and ensuring the applications meet the required specifications and standards.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Design and implement applications based on business process and application requirements.- Troubleshoot and resolve issues in applications.- Ensure applications meet the required specifications and standards.- Collaborate with cross-functional teams to gather requirements and provide technical expertise.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Azure Databricks.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Microsoft Azure Databricks.- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'microsoft azure', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'power bi', 'machine learning', 'sql', 'data quality', 'tableau', 'data science', 'predictive modeling', 'troubleshooting', 'text mining', 'data visualization']",2025-06-13 06:19:08
Application Lead,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP FI S/4HANA Central Finance\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact- Manage the team and ensure successful project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP FI S/4HANA Central Finance- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in SAP FI S/4HANA Central Finance- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap fi', 'central finance', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'sap', 'team management', 'power bi', 'machine learning', 'sql', 'data quality', 'tableau', 'data science', 'predictive modeling', 'project delivery', 'text mining', 'data visualization']",2025-06-13 06:19:10
Application Lead,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP CO Product Cost Controlling, SAP CO Management Accounting\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact for the project- Manage the team and ensure successful project delivery- Collaborate with multiple teams to make key decisions- Provide solutions to problems for the immediate team and across multiple teams\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP CO Product Cost Controlling, SAP CO Management Accounting- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in SAP CO Product Cost Controlling- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap co management', 'management accounting', 'sap co product cost controlling', 'machine learning algorithms', 'statistics', 'sap', 'team management', 'power bi', 'machine learning', 'sap co', 'data quality', 'tableau', 'recruitment', 'project delivery', 'data visualization', 'data munging']",2025-06-13 06:19:12
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :PySpark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As an Application Lead, you will be responsible for leading the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve working with PySpark and collaborating with cross-functional teams to deliver high-quality solutions.\nRoles & Responsibilities:- Lead the design, development, and deployment of PySpark-based applications, ensuring high-quality solutions are delivered on time and within budget.- Collaborate with cross-functional teams, including business analysts, data scientists, and software developers, to ensure that applications meet business requirements and are scalable and maintainable.- Act as the primary point of contact for all application-related issues, providing technical guidance and support to team members and stakeholders.- Ensure that applications are designed and developed in accordance with industry best practices, including coding standards, testing methodologies, and deployment processes.- Stay up-to-date with the latest trends and technologies in PySpark and related fields, and apply this knowledge to improve the quality and efficiency of application development.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nStrong experience in PySpark.- Good To Have\n\n\n\n\nSkills:\nExperience with other big data technologies such as Hadoop, Hive, and Spark.- Solid understanding of software development principles, including object-oriented programming, design patterns, and agile methodologies.- Experience with database technologies such as SQL and NoSQL.- Experience with cloud platforms such as AWS or Azure.- Strong problem-solving and analytical skills, with the ability to troubleshoot complex issues and provide effective solutions.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in PySpark.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering high-quality software solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'software development', 'pyspark', 'spark', 'hadoop', 'visualforce', 'sfdc', 'ado.net', 'microsoft azure', 'triggers', 'sql', 'nosql', 'application development', 'apex', 'salesforce', 'sales force development', 'salesforce crm', 'data loader', 'design patterns', 'agile', 'aws', 'agile methodology']",2025-06-13 06:19:14
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Ahmedabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Microsoft Azure Machine Learning\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for overseeing the entire application development process and ensuring its successful implementation. This role requires strong leadership skills and the ability to collaborate with cross-functional teams to deliver high-quality solutions.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the effort to design, build, and configure applications.- Act as the primary point of contact for application-related matters.- Oversee the entire application development process.- Ensure successful implementation of applications.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Azure Machine Learning.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Microsoft Azure Machine Learning.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure machine learning', 'microsoft azure', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'power bi', 'machine learning', 'application development', 'sql', 'data quality', 'tableau', 'r', 'data science', 'predictive modeling', 'data visualization', 'logistic regression']",2025-06-13 06:19:16
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Apache Spark\n\n\n\n\nGood to have skills :Oracle Procedural Language Extensions to SQL (PLSQL), AWS ArchitectureMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :Mandatory 15 years of Fulltime qualification\n\n\nSummary:As an Application Lead, you will be responsible for designing, building, and configuring applications using Apache Spark. Your typical day will involve leading the effort to design and build applications, acting as the primary point of contact, and utilizing your expertise in Apache Spark to deliver impactful solutions.\nRoles & Responsibilities:- Lead the effort to design, build, and configure applications using Apache Spark.- Act as the primary point of contact for the project, collaborating with cross-functional teams to ensure successful delivery of applications.- Utilize your expertise in Apache Spark to develop and deploy advanced data processing pipelines, including experience with Spark Streaming and Spark SQL.- Design and implement scalable and fault-tolerant solutions using Apache Spark, ensuring high performance and reliability.- Stay updated with the latest advancements in Apache Spark and related technologies, integrating innovative approaches for sustained competitive advantage.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExpertise in Apache Spark.- Good To Have\n\n\n\n\nSkills:\nExperience with Oracle Procedural Language Extensions to SQL (PLSQL) and AWS Architecture.- Strong understanding of distributed computing principles and experience with distributed data processing frameworks.- Experience with data processing pipelines, including Spark Streaming and Spark SQL.- Experience designing and implementing scalable and fault-tolerant solutions using Apache Spark.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Apache Spark.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Gurugram office.\n\nQualification\n\nMandatory 15 years of Fulltime qualification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['distributed computing', 'sql', 'plsql', 'spark', 'data munging', 'python', 'oracle', 'data analysis', 'data analytics', 'natural language processing', 'machine learning', 'data quality', 'r', 'apache', 'data science', 'oracle procedural language extensions to sql', 'aws', 'logistic regression']",2025-06-13 06:19:17
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Microsoft Power Pages\n\n\n\n\nGood to have skills :Microsoft Dynamics CRM TechnicalMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact for the project- Manage the team and ensure successful project delivery- Collaborate with multiple teams to make key decisions- Provide solutions to problems for the immediate team and across multiple teams\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Power Pages- Good To Have\n\n\n\n\nSkills:\nExperience with Microsoft Dynamics CRM Technical- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Microsoft Power Pages- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['tableau', 'data visualization', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'team management', 'ms dynamics crm', 'power bi', 'machine learning', 'sql', 'data quality', 'data science', 'predictive modeling', 'project delivery', 'text mining']",2025-06-13 06:19:19
Software Development Lead,Accenture,7 - 12 years,Not Disclosed,['Bhubaneswar'],"Project Role :Software Development Lead\n\n\n\n\n\nProject Role Description :Develop and configure software systems either end-to-end or for a specific stage of product lifecycle. Apply knowledge of technologies, applications, methodologies, processes and tools to support a client, project or entity.\n\n\n\nMust have skills :SAP ABAP Development for HANA\n\n\n\n\nGood to have skills :SAP Business Workflow Management Design & DevelopmentMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Software Development Lead, you will develop and configure software systems either end-to-end or for a specific stage of the product lifecycle. You will apply your knowledge of technologies, applications, methodologies, processes, and tools to support a client, project, or entity. Your typical day will involve leading a team of developers, collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams. You will also engage with stakeholders, manage the team's performance, and ensure successful project delivery.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead a team of developers- Collaborate with stakeholders to understand project requirements- Manage the team's performance and ensure successful project delivery-Workflows - Traditional ABAP workflows, Flexible workflows and SAP Build process automation\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP ABAP Development for HANA, CDS views, ODATA, RAP- Good To Have\n\n\n\n\nSkills:\nExperience with SAP Business Workflow Management Design & Development- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in SAP ABAP Development for HANA- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['rap', 'cds views', 'odata', 'sap abap development', 'sap hana', 'algorithms', 'python', 'software development', 'power bi', 'machine learning', 'sql', 'sap business workflow', 'tableau', 'project delivery', 'workflow management', 'machine learning algorithms', 'sap abap', 'statistics', 'data munging']",2025-06-13 06:19:20
Software Development Lead,Accenture,12 - 17 years,Not Disclosed,['Ahmedabad'],"Project Role :Software Development Lead\n\n\n\n\n\nProject Role Description :Develop and configure software systems either end-to-end or for a specific stage of product lifecycle. Apply knowledge of technologies, applications, methodologies, processes and tools to support a client, project or entity.\n\n\n\nMust have skills :Data Science\n\n\n\n\nGood to have skills :AWS ArchitectureMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :Should be a Graduate\n\n\nSummary:As a Software Development Lead with 12 years of experience in Data Science, you will be responsible for developing and configuring software systems either end-to-end or for a specific stage of the product lifecycle. You will apply your knowledge of technologies, applications, methodologies, processes, and tools to support a client, project, or entity. Your typical day will involve working with Data Science and AWS Architecture to deliver impactful data-driven solutions.\nRoles & Responsibilities:- Lead the development and configuration of software systems either end-to-end or for a specific stage of the product lifecycle.- Apply knowledge of technologies, applications, methodologies, processes, and tools to support a client, project, or entity.- Collaborate with cross-functional teams to deliver impactful data-driven solutions.- Utilize Data Science and AWS Architecture to develop and configure software systems.-working with terabytes of text, images, and other types of data to solve real-world problems through Gen AI.-proficient in designing and developing advanced Generative AI based solutions to solve diverse customer problems\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nStrong experience in Data Science.-Must to have Python and AWS SageMaker- Good To Have\n\n\n\n\nSkills:\nAWS Architecture, AWS Bedrock- Solid grasp of technologies, applications, methodologies, processes, and tools to support a client, project, or entity.- Experience in collaborating with cross-functional teams.- Experience in delivering impactful data-driven solutions.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Science.- The ideal candidate will possess a strong educational background in statistics, mathematics, computer science, or a related field.- This position is based at our Bengaluru office.\n\nQualification\n\nShould be a Graduate",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'software development', 'data science', 'computer science', 'aws', 'c#', 'c++', 'project management', 'c', 'natural language processing', 'presentation skills', 'microsoft azure', 'teaching', 'machine learning', 'javascript', 'sql server', 'sql', 'java', 'html', 'mysql', 'agile']",2025-06-13 06:19:22
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Apache Spark\n\n\n\n\nGood to have skills :PySparkMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various teams to ensure project milestones are met, facilitating discussions to address challenges, and guiding your team in implementing effective solutions. You will also engage in strategic planning sessions to align project goals with organizational objectives, ensuring that all stakeholders are informed and involved in the development process. Your role will require a balance of technical expertise and leadership skills to drive the project forward successfully.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge.- Facilitate regular team meetings to discuss progress and address any roadblocks.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Apache Spark.- Good To Have\n\n\n\n\nSkills:\nExperience with PySpark.- Strong understanding of distributed computing principles.- Experience with data processing frameworks and tools.- Familiarity with cloud platforms and services related to big data.\nAdditional Information:- The candidate should have minimum 5 years of experience in Apache Spark.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'distributed computing', 'apache', 'spark', 'big data', 'algorithms', 'kubernetes', 'c++', 'sql', 'docker', 'spring', 'java', 'j2ee', 'data structures', 'hadoop', 'cloud computing', 'c#', 'rest', 'python', 'c', 'oracle', 'machine learning', 'kafka', 'aws', 'unix']",2025-06-13 06:19:24
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Microsoft Azure Analytics Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact- Manage the team and ensure successful project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Azure Analytics Services- Good To Have\n\n\n\n\nSkills:\nExperience with cloud platforms such as AWS or Google Cloud- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Microsoft Azure Analytics Services- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['analytics services', 'azure analytics', 'microsoft azure', 'machine learning algorithms', 'statistics', 'team management', 'ssas', 'power bi', 'machine learning', 'sql server', 'sql', 'data quality', 'tableau', 'gcp', 'ssrs', 'project delivery', 'data visualization', 'aws', 'ssis', 'msbi', 'data munging']",2025-06-13 06:19:26
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Apache Spark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact- Manage the team and ensure successful project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Apache Spark- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Apache Spark- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['tableau', 'spark', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'team management', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'apache', 'data science', 'predictive modeling', 'project delivery', 'text mining', 'data visualization']",2025-06-13 06:19:28
Python Developer Lead {ENG - Infosys @ Pan India - G },Infosys,4 - 9 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Responsibilities A day in the life of an Infoscion • As part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain. • You will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements. • You will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers. • You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nTechnical and Professional Requirements: • Primary skills:Process->Testing processes->Test Automation Process,Technology->Machine Learning->Python\n\nPreferred Skills: Process->Testing processes->Test Automation Process\nTechnology->Machine Learning->Python\n\nAdditional Responsibilities: • Knowledge of design principles and fundamentals of architecture • Understanding of performance engineering • Knowledge of quality processes and estimation techniques • Basic understanding of project domain • Ability to translate functional / nonfunctional requirements to systems requirements • Ability to design and code complex programs • Ability to write test cases and scenarios based on the specifications • Good understanding of SDLC and agile methodologies • Awareness of latest technologies and trends • Logical thinking and problem solving skills along with an ability to collaborate\n\nEducational RequirementsMCA,MSc,MTech,Bachelor of Engineering,BCA,BE,BSc,BTech Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Django Framework', 'Python Development', 'Python']",2025-06-13 06:19:29
Senior Lead business execution consultant,Wells Fargo,7 - 12 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Senior Lead business execution consultant\n\nIn this role, you will:\nAct as a Business Execution advisor to leadership to drive performance and initiatives, and develop and implement information delivery or presentations to key stakeholders and senior management",,,,"['Business execution', 'Business Implementation', 'Data Engineering', 'NLP', 'generative AI', 'Data Mining', 'machine learning', 'Strategic Planning', 'agentic AI']",2025-06-13 06:19:31
Snowflake Data Engineer,Prudent Globaltech Solutions,7 - 12 years,16-27.5 Lacs P.A.,['Hyderabad( Madhapur )'],"Job Description Data Engineer\nWe are seeking a highly skilled Data Engineer with extensive experience in Snowflake, Data Build Tool (dbt), Snaplogic, SQL Server, PostgreSQL, Azure Data Factory, and other ETL tools. The ideal candidate will have a strong ability to optimize SQL queries and a good working knowledge of Python. A positive attitude and excellent teamwork skills are essential.\n\nRole & responsibilities\nData Pipeline Development: Design, develop, and maintain scalable data pipelines using Snowflake, DBT, Snaplogic, and ETL tools.",,,,"['Snowflake', 'Azure Data Factory', 'ADF', 'Data Engineer', 'ETL', 'SQL']",2025-06-13 06:19:32
Lead AI/ML Engineer,Okta,7 - 12 years,Not Disclosed,['Bengaluru'],"Okta is seeking a highly skilled Full-Stack Engineer with deep expertise in AWS Bedrock, generative AI, and modern software development to join our fast-moving team at the intersection of developer experience, machine learning, and enterprise software. As part of the Okta Business Technology (BT) team, you will build cutting-edge tools that make AI development intuitive, collaborative, and scalable. If you're passionate about building next-generation AI applications and empowering developers through innovative platforms, this is the role for youJob Duties and Responsibilities Design and develop full-stack applications that integrate seamlessly with Amazon Bedrock AI agents. Build scalable, production-grade AI/ML solutions using AWS Bedrock and the AWS Agent Development Kit. Implement back-end services and APIs to interact with foundation models for tasks such as automated sourcing, content generation, and prompt orchestration. Create intuitive and performant front-end interfaces using Angular that connect with GenAI capabilities. Ensure seamless integration of LLMs and foundation models into the broader application architecture. Explore and rapidly prototype with the latest LLMs and GenAI tools to iterate on new capabilities and features. Build sophisticated AI workflows using knowledge bases, guardrails, and prompt chaining/flows. Deploy and maintain enterprise-ready GenAI applications at scale. Collaborate with business analysts to understand customer needs and use cases, and work with the team to design, develop POCs to test, implement & support solutions Foster strong relationships with teammates, customers, and vendors to facilitate effective communication and collaboration throughout the project lifecycle Perform in-depth analysis of requirements, ensuring compatibility and adherence to established standards and best practicesRequired\n\nSkills:\n7+ years of robust experience with hands-on development & design experience 3+ years of experience in one or more of the following areasDeep Learning, LLMs, NLP, Speech, Conversational AI, AI Infrastructure, Fine-tuning, and optimizations of PyTorch models. Software development experience in languages like Python (must have) and one from the optional (Go, Rust, and C/C++). Experience with at least one LLM such as Llama, GPT, Claude, Falcon, Gemini, etc. Expertise in AWS Bedrock and the AWS Agent Development Kit is mandatory Hands-on experience with Python libraries including boto3, NumPy, Pandas, TensorFlow or PyTorch, and Hugging Face Transformers Solid understanding of the AWS ecosystem (e.g., CloudWatch, Step Functions, Kinesis, Lambda) Familiarity with full software development lifecycle, including version control, CI/CD, code reviews, automated testing, and production monitoring Knowledge about the ERP, HR technology and Legal business processes is an advantageEducation and Certifications Bachelors degree in Computer Science or a related field, or equivalent practical experience AWS certifications (e.g., AWS Certified Solutions Architect, Machine Learning Specialty) are a plus Experience working with large-scale generative AI or LLM-based applications Knowledge of secure application development and data privacy practices in AI/ML workloads""This role requires in-person onboarding and travel to our Bengaluru, IN office during the first week of employment.""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'software development', 'natural language processing', 'deep learning', 'aws', 'erp', 'c++', 'c', 'numpy', 'machine learning', 'kinesis', 'artificial intelligence', 'pandas', 'tensorflow', 'hr software', 'lambda expressions', 'speech', 'infrastructure', 'pytorch', 'amazon cloudwatch', 'ml']",2025-06-13 06:19:34
Lead AI Engineer,Smart Software Testing Solutions,8 - 12 years,Not Disclosed,['Bengaluru'],"Opkey, we are disrupting the space of ERP transformation testing by building an AI-powered No Code Testing platform for Enterprise business applications (like Oracle Fusion Cloud, SAP S4Hana, SAP, Workday, Salesforce, and the likes). Opkey is a fast-growing VC-backed continuous end-to-end test automation software company headquartered in Dublin, California, with additional offices in Pittsburgh (opened in 2022), NYC (opened in 2022), & India (Noida & Bangalore). With the test automation market growing 20% annually, its estimated to reach $50 billion by 2026. Trusted by 250+ enterprise customers, including GAP, Pfizer, and KPMG.\n\n\nWe are seeking a highly skilled Lead AI Engineer with 8+ years of experience, preferably from SaaS and product-based companies, to drive our AI initiatives from ideation to deployment. You will work closely with cross-functional teams to design, develop, and scale innovative AI solutions that power our next-generation platforms.\n\n\nKey Responsibilities:\nArchitect, build, and deploy AI/ML models for SaaS products at scale.\nLead the end-to-end lifecycle of AI projects from data exploration, model development, validation, and deployment, to monitoring and maintenance.\nCollaborate with Product Management, Engineering, and Design teams to integrate AI capabilities into product offerings.\nImplement and optimize Retrieval-Augmented Generation (RAG) systems, Large Language Models (LLMs), and other emerging AI/ML techniques.\nDefine and uphold best practices in AI model development, MLOps, and scalable deployment.\nMentor and guide a team of AI/ML engineers, setting technical direction and fostering a culture of innovation and excellence.\nPartner with stakeholders to define AI strategies aligned with overall technology roadmaps and business objectives.\nStay abreast of advancements in AI and contribute thought leadership internally and externally.\n\n\nRequired Skills & Experience:\n10-12 years of experience in AI/ML engineering, with a strong record of working in SaaS and product-based environments.\nExpertise in Machine Learning, Deep Learning, Natural Language Processing (NLP), Computer Vision, and/or Generative AI.\nHands-on experience with frameworks like TensorFlow, PyTorch, Hugging Face Transformers, etc.\nSolid experience in designing scalable AI architectures and deploying models in production environments (AWS, GCP, Azure, etc.).\nStrong programming skills in Python; familiarity with other languages like Java, Go, or Scala is a plus.\nDeep understanding of MLOps, CI/CD for machine learning pipelines, containerization (Docker, Kubernetes).\nExperience with LLM fine-tuning, prompt engineering, vector databases (e.g., Pinecone, FAISS) is highly desirable.\n[Non-negotiable Requisite] Has experience is having trained a SLM (or Medium Language Model), for a particular vertical. This includes pre-training and fine-tuning.\nStrong problem-solving skills and ability to navigate ambiguous technical challenges.\nExcellent communication, leadership, and stakeholder management skills.\n\n\nPreferred Qualifications:\nMaster s or Ph.D. in Computer Science, Machine Learning, Data Science, or a related field.\nExperience in Go-to-Market Strategy for AI-powered products.\nExperience integrating AI into customer-facing SaaS products with measurable outcomes.\nContributions to open-source AI projects or published research papers",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'Computer vision', 'ERP', 'SAP', 'Machine learning', 'Open source', 'Monitoring', 'Python', 'Salesforce']",2025-06-13 06:19:36
Lead Cloud Engineer,Uplers,8 - 12 years,30-35 Lacs P.A.,['Bengaluru'],"Lead Cloud Engineer\n\nExperience: 5 - 7 Years Exp.\nSalary : INR 25-35 Lacs per annum\nPreferred Notice Period: Within 30 Days\nShift: 09:00AM to 06:00PM IST\nOpportunity Type: Onsite (Bengaluru)\nPlacement Type: Contractual\nContract Duration: Full-Time, Indefinite Period\n\n(*Note: This is a requirement for one of Uplers' Clients)\n\nMust have skills required :\nNodejs, React/Vue, Team management, SQL, RESTful API development, Javascript (es6+), Cloud-based Application Architecture, Ci/Cd Pipelines\nGood to have skills :\nAWS CloudFormation, CDK/CDKTF, EJS, Event driven architecture, Microservices Architecture, Pug, Terraform\n\nUK's Top Electronic Security Products Company (One of Uplers' Clients) is Looking for:\nEngineering Lead (Cloud focused) who is passionate about their work, eager to learn and grow, and who is committed to delivering exceptional results. If you are a team player, with a positive attitude and a desire to make a difference, then we want to hear from you.\n\nRole Overview Description\nThe position of Lead Cloud Engineer would join the Cloud team in the R&D department to accelerate our exciting product roadmap of Cloud-connected systems and digital services which form the core of our development projects. This roadmap includes migrating to a microservices architecture as well as adding new features and functionality.\n\nResponsibilities:\nDesign and implement new features for the Cloud services, building on existing functionality to deliver greater value to our customers.\nLead the implementation of architectural improvements to ensure performance, scalability, and maintainability.\nDevelop RESTful APIs using NodeJS and related frameworks (e.g., Express).\n¢ Investigate and resolve system defects and performance bottlenecks.\n¢ Assist and mentor cloud team members, providing technical guidance and code reviews.\n¢ Understand and translate business vision into actionable technical roadmaps, identifying opportunities for optimization, cost savings, and feature enhancements.\n¢ Provide technical leadership and support to the broader development team.\n¢ Work in a Scrum-based Agile environment, delivering high-quality outputs in regular sprint cycles.\n¢ Support CI/CD deployments into development and production environments.\n\nSkills & Experience you will bring to Team:\n¢ Proven 8+ Years of hands-on experience in developing and maintaining scalable, cloud-based applications using NodeJS and modern backend frameworks.\n¢ Strong experience with functional and object-oriented JavaScript, including ES6 features\n¢ Strong proficiency with SQL, with the ability to write and optimize large, complex SQL queries\n¢ Excellent analytical and debugging skills\n¢ Experience in reviewing application requirements and interface designs, including wireframing.\n¢ Ability to identify web-based user interactions and define smooth user experience flows\n¢ Proven experience in developing and implementing responsive UI components using frameworks such as React or Vue.\n¢ Working knowledge and experience with Scrum and Agile methodologies.\n¢ A track record of mentoring junior developers and fostering a collaborative culture.\n\nAdvantage to Have:\n¢ Experience with Infrastructure as Code tools such as Terraform, AWS CloudFormation, or CDK/CDKTF.\n¢ An understanding of fundamental design principles behind scalable applications, especially Event Driven Architecture and microservices.\n¢Hands-on experience in deploying and monitoring applications in AWS or other cloud infrastructure environments.\n¢ A good understanding of server-side templating languages such as Pug, EJS, or similar.\n\nHow to apply for this opportunity:\nEasy 3-Step Process:\n1. Click On Apply! And Register or log in on our portal\n2. Upload updated Resume & Complete the Screening Form\n3. Increase your chances to get shortlisted & meet the client for the Interview!\n\nAbout Our Client:\nOur electronic security products and services have been protecting millions of people and properties around the world for over 35 years. Our pioneering ethos has allowed us to lead the way with ground-breaking digital and innovative security solutions, providing reliable and trusted protection while offering increasingly connected, intelligent and dynamic solutions to continue to meet the worlds current & future security challenges.\n\nAbout Uplers:\nUplers is the #1 hiring platform for SaaS companies, designed to help you hire top product and engineering talent quickly and efficiently. Our end-to-end AI-powered platform combines artificial intelligence with human expertise to connect you with the best engineering talent from India.\nWith over 1M deeply vetted professionals, Uplers streamlines the hiring process, reducing lengthy screening times and ensuring you find the perfect fit. Companies like GitLab, Twilio, TripAdvisor, and AirBnB trust Uplers to scale their tech and digital teams effectively and cost-efficiently.\nExperience a simpler, faster, and more reliable hiring process with Uplers today.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Cloud Architecture', 'Cloud Infrastructure', 'Nodejs', 'React/Vue', 'Ci/Cd Pipelines', 'Team management', 'RESTful API development', 'SQL']",2025-06-13 06:19:37
Artificial Intelligence Intern,Ergode It Services,6 months duration,"10,000/month",['Kolkata'],"Role & responsibilities\n\nBuild and maintain automated workflows using no-code tools like n8n\nWork with APIs and webhooks to integrate various systems and tools\nCollaborate with the team to iterate on workflow designs based on feedback and evolving needs\nTroubleshoot and optimize existing automation pipelines\nDocument processes and contribute to internal knowledge sharing\n\n\nPreferred candidate profile\nComfortable working with no-code platforms, especially n8n\nBasic understanding of APIs, webhooks, and integration logic\nStrong problem-solving mindset and attention to detail\nEagerness to learn quickly and iterate based on feedback\nAbility to work independently and manage tasks responsibly",Industry Type: Internet (E-Commerce),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Ai Platform', 'Ai Algorithms', 'Ai Techniques', 'N8N', 'no code', 'Artificial Intelligence', 'Ai Solutions', 'API']",2025-06-13 06:19:39
Artificial Intelligence Intern,Ergode It Services,6 months duration,"10,000/month",['Siliguri'],"Role & responsibilities\n\nBuild and maintain automated workflows using no-code tools like n8n\nWork with APIs and webhooks to integrate various systems and tools\nCollaborate with the team to iterate on workflow designs based on feedback and evolving needs\nTroubleshoot and optimize existing automation pipelines\nDocument processes and contribute to internal knowledge sharing\n\n\nPreferred candidate profile\nComfortable working with no-code platforms, especially n8n\nBasic understanding of APIs, webhooks, and integration logic\nStrong problem-solving mindset and attention to detail\nEagerness to learn quickly and iterate based on feedback\nAbility to work independently and manage tasks responsibly",Industry Type: Internet (E-Commerce),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Ai Platform', 'Ai Algorithms', 'Ai Techniques', 'N8N', 'no code', 'Artificial Intelligence', 'Ai Solutions', 'API']",2025-06-13 06:19:41
Lead ML Ops Engineer with GCP,TVS Next,8 - 10 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['hive', 'kubernetes', 'data pipeline', 'sql', 'docker', 'tensorflow', 'java', 'product management', 'gcp', 'spark', 'pytorch', 'bigquery', 'hadoop', 'big data', 'programming', 'hbase', 'ml', 'cloud sql', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'machine learning', 'data engineering', 'ops', 'mapreduce', 'kafka', 'cloud storage', 'hdfs', 'bigtable', 'aws']",2025-06-13 06:19:43
Manager - Referral Hiring - Decision Science / Data Science,Axtria,10 - 15 years,Not Disclosed,['Gurugram'],"Job Responsibilities Craft winning proposals to grow the Data Science Practice. Develop and operationalize scalable processes to deliver on large & complex client engagements. Ensure profitable delivery and great customer experience design the end to end solution, put together the right team and deliver as per established processes. Build an A team hire the required skills sets and nurture them in a supporting environment to develop strong delivery teams for the Data Science Practice. Train and mentor staff and establish best practices and ways of working to enhance data science capabilities at Axtria. Operationalize an eco-system for continuous learning & development. Write white papers, collaborate with academia and participate in relevant forums to continuously upgrade self knowledge & establish Axtrias thought leadership in this space. Research, develop, evaluate, and optimize newly emerging algorithms and technologies for relevant use cases in pharma commercial & clinical space.EducationBachelor Equivalent - OtherPG Diploma in ManagementWork ExperienceData Scientist6-15 years of relevant experience in advanced statistical and mathematical models and predictive modeling using Python. Experience in the data science space prior relevant experience in Artificial intelligence and machine Learning algorithms for developing scalable models supervised and unsupervised techniques like NLP and deep Learning Algorithms. Ability to build scalable models using Python, R-Studio, R Shiny, PySpark, Keras, and TensorFlow. ML Ops Engineering6-15 years of experience with MLOps Frameworks like Kubeflow, MLFlow, Data Robot, Airflow, etc., experience with Docker and Kubernetes, OpenShift. Prior experience in end-to-end automated ecosystems including, but not limited to, building data pipelines, developing & deploying scalable models, orchestration, scheduling, automation, and ML operations. Ability to design and implement cloud solutions and ability to build MLOps pipelines on cloud solutions (AWS, MS Azure, or GCP).Gen AI 6-15 years develop, test, and deploy Python based applications on Azure/AWS platforms. Must have basic knowledge on concepts of Generative AI / LLMs / GPT. Deep understanding of architecture and work experience on Web Technologies. Python, SQL hands-on experience. Expertise in any popular python web frameworks e.g. flask, Django etc. Familiarity with frontend technologies like HTML, JavaScript, REACT.IOS Developer 6+ years of experience in IOS mobile development and 5+ in React Native. Strong background working with React Native tools. Fullstack Developer 6-15 years experience to develop, test and deploy React, Javascript, Python based applications on Azure/AWS platforms. Must have experience working on frontend and backend technologies. Experience on React and Python is a must.Technical CompetenciesML Data ScienceML OpsML engineeringPharmaPythonPySparkAWS Data PipelineAWS EMRAIMLNLPBehavioural CompetenciesOwnershipTeamwork & LeadershipTalent ManagementCultural FitMotivation to Learn and GrowProject Management",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'gpm', 'react native', 'react.js', 'python web framework', 'software testing', 'ios mobile development', 'microsoft azure', 'aws stack', 'machine learning', 'javascript', 'deep learning', 'tensorflow', 'predictive modeling', 'mathematical modeling', 'keras', 'advanced statistical']",2025-06-13 06:19:45
Azure Data Architect,Technip Energies,5 - 10 years,Not Disclosed,['Noida'],"Develop RESTful APIs using Azure APIM\nDevelop integration workflow using LogicApp, synpase and service bus.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation using Azure Data Factory and synapse pipelines.\nCollaborate closely with Product Owners to understand data pipeline requirements and design effective data workflows.\nTranslate business requirements into technical specifications for data pipelines.\nCreate and maintain data storage solutions using Azure Cosmos DB and Azure Data Lake Storage",,,,"['Service level', 'Talent acquisition', 'data security', 'Analytical', 'TPS', 'Data Architect', 'Agile', 'Workflow', 'Project delivery', 'Analytics']",2025-06-13 06:19:46
Artificial Intelligence Architect,Emerson,10 - 20 years,Not Disclosed,['Pune'],"Role & responsibilities\nDesign robust and scalable AI/ML architectures that support the development and deployment of machine learning models and AI solutions.\nDevelop and guide the implementation of end-to-end AI/ML solutions, including model development, data processing, and system integration.\nEvaluate and recommend the latest AI/ML technologies, frameworks, and tools to enhance system capabilities and performance.\nCollaborate with software engineers and other development teams to integrate AI/ML solutions into existing systems and applications. Ensure seamless operation and performance.\nWork with cross-functional teams, including developers, data scientists, machine learning engineers, and business stakeholders, to understand requirements and design solutions that align with business objectives.\n\nPreferred candidate profile\nBachelors degree in computer science, Data Science, Statistics, or a related field or a master's degree or higher is preferred.\nMore than 3 years of experience in designing and implementing AI/ML architectures, with a proven track record of successful projects.\nExtensive experience with machine learning frameworks (e.g., Go, TensorFlow, PyTorch), programming languages C#, .Net, NodeJS and data processing tools.\nStrong understanding of system architecture principles, including distributed systems, microservices, and cloud computing.\nExperience with Microsoft Azure cloud services and their AI/ML offerings\nExperience with event-handling systems such as Kafka\nExperience with big data technologies and data engineering practices.\nExcellent verbal and written communication skills, with the ability to convey complex technical concepts to non-technical stakeholders.",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Ml', 'Python', 'Tensorflow', 'Pytorch', 'Architecture', 'Artificial Intelligence', '.Net', 'Machine Learning', 'Scikit-Learn']",2025-06-13 06:19:48
Senior Artificial Intelligence Engineer - NLP,Rosemallow Technologies,3 - 5 years,Not Disclosed,['Pune'],"Key Responsibilities :\n\nWe are seeking a highly skilled and motivated AI Engineer to join our team. In this role, you will leverage your expertise in AI technologies and the Microsoft ecosystem to design, build, and deploy intelligent agents and automation solutions that enhance business processes and deliver value to our clients.\n\nCandidates must have extensive experience in the Microsoft environment. You will collaborate with cross-functional teams to create innovative solutions using Microsoft tools and platforms.\n\nResponsibilities :\n\nAgent Development :\n\n- Design, implement, and optimize AI agents using Microsoft Azure Framework and related technologies.\n\n- Develop custom AI solutions leveraging Power Automate, Azure OpenAI, and other Microsoft tools.\n\nSolution Integration :\n\n- Deploy AI solutions within client environments, ensuring scalability and seamless integration with existing systems.\n\n- Work with stakeholders to identify automation opportunities and tailor solutions to business needs.\n\nAI Algorithm and Model Implementation :\n\n- Design and implement machine learning algorithms, focusing on natural language processing (NLP) and conversational AI.\n\n- Perform data preprocessing, feature engineering, and model training to create high-performing solutions.\n\nCollaboration and Support :\n\n- Collaborate with cross-functional teams, including software engineers, data scientists, and product managers, to deliver integrated solutions.\n\n- Provide technical guidance and support to ensure the successful adoption and use of AI-driven tools.\n\nContinuous Improvement :\n\n- Stay updated on advancements in AI, machine learning, and Microsofts AI technologies.\n\n- Contribute to knowledge sharing by conducting training sessions and documenting best practices.\n\nPreferred Skills :\n\n- Strong knowledge of Azure OpenAI, Azure AI Search Index and open source libraries such as LangChain and LlamaIndex.\n\n- Proficiency in Python and its AI/ML libraries (e.g., TensorFlow, PyTorch, scikit-learn).\n\n- Familiarity with building and managing cloud-based solutions, preferably on Microsoft Azure.\n\n- Understanding of conversational AI technologies and chatbot frameworks.\n\n- Experience with data analysis tools and techniques to uncover insights and optimize models.\n\nRequirements :\n\n- Bachelors or Masters degree in Computer Science, Data Science, or a related field.\n\n- Proven experience in developing and deploying AI/ML models in real-world applications.\n\n- Strong programming skills, especially in Python, and familiarity with version control systems like Git.\n\n- Extensive experience in the Microsoft environment and related technologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Tensorflow', 'Solution Integration', 'NLP', 'PyTorch', 'Conversational AI', 'OpenAI', 'Machine Learning', 'Python']",2025-06-13 06:19:49
"Director, Software Engineering - Treasury Management System (TMS)",Mastercard,6 - 11 years,Not Disclosed,['Pune'],"The Treasury Services Program provides vital systems and services to the Mastercard Treasury and Finance team in support of global payments/customer funds movement, treasury operations, liquidity, foreign exchange, risk management, and capital management. We are directly responsible for moving billions of dollars in domestic and cross border currencies across the world each day between our customers. We are an agile development organization with teams distributed globally across technology hubs in the U.S., India, and Ireland.\n\nAbout the Role\nSoftware Engineers at Mastercard design and code artificial intelligence, cloud, and machine learning platforms that provide mission-critical insights to many of the world s leading organizations and governments. As a Software Engineering Director, you will deliver these products and solutions with speed and agility as part of a small team. This will involve developing high-performing, highly scalable software solutions and products for some of the world s top brands. Specific tasks vary depending on the project and the business unit that you join in.\nAll staff at Mastercard are expected to demonstrate Mastercard Way cultural values every day - own it, simplify it, sense of urgency, thoughtful risk-taking, unlock potential, and be inclusive - with a relentless focus on our customers.\nAs a Director, Software Engineer at Mastercard, you are expected to perform the following general responsibilities:\nFormally supervise and coach multiple teams of engineers and managers to build, enhance, and support multiple applications/services in the delivery of internal or market-facing Products, Platforms, or Product bundles\nProvide strategic thinking and leadership related to a wide range of applications and systems, or software-development methodologies\nEnsure objectives and development plans are established at the start of the year and reviewe'd continuously throughout the year\nSpeak as one management voice and regularly hold staff meetings with all levels of staff to brief on organization, department, and People & Capabilities (HR) updates\nHold people and teams accountable and effectively delegate responsibilities down to the team\nRecruit and hire the right talent, always bringing in someone better than at least half the individuals in the role\nContinuously engage and improve teams performance by conducting recurring 1-1 meetings, knowing your people, managing career development, and understanding who is at risk\nProvide and facilitate timely feedback, coaching in the moment, and mentoring for staff at all levels\nManage and optimize budgets, forecasting, and cost allocation while delivering on business needs\nContinuously build a strong network across the company for collaboration on technical and business solutions\nEmulate and drive Mastercard Way behaviors through their behavior, recognitions, coaching, and employee engagement\n\nAll About You\nStrategic and experienced IT professional with a successful track record in managing and implementing Treasury Management Systems\nSolid experience in working with Treasury business that is managing liquidity, optimizing cash flows, working capital and overseeing risk management\nIdeal candidate will bring deep technical knowledge of treasury systems and cross-border flows\nKnowledge of in-house banking, netting and payment flows\nUnderstanding of how current regulatory and compliance environments impact treasury functions\nTechnical expertise in event driven architectures, design patterns, microservices and containerized deployments in private and public clouds (IE Azure, AWS, GCP)\nExperience in leading and coaching teams to perform software planning and estimation for large scale complex programs\nProficient in implementing COTS (Custom Off The Shelf) packages within complex, highly secure environments with multiple integrations\nBachelors degree in software engineering, computer science, information technology or related discipline preferred, or equivalent work experience; Payment industry knowledge strongly preferred but not required",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Career development', 'Treasury management', 'GCP', 'Information security', 'Employee engagement', 'Risk management', 'Business solutions', 'Information technology', 'Forecasting']",2025-06-13 06:19:51
"Director, Software Engineering",Dynamic Yield,4 - 11 years,Not Disclosed,['Pune'],"Our Purpose\nTitle and Summary\nDirector, Software Engineering (Treasury Management System (TMS) Experience like FIS Quantum & ION Wallstreet Suite)\nDirector, Software Engineering, Treasury Management\nWho is Mastercard?\nWe work to connect and power an inclusive, digital economy that benefits everyone, everywhere, by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships, and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team - one that makes better decisions, drives innovation, and delivers better business results.\nTechnology at Mastercard\nWhat we create today will define tomorrow. Revolutionary technologies that reshape the digital economy to be more connected and inclusive than ever before. Safer, faster, more sustainable.\nAnd we need the best people to do it. Technologists who are energized by the challenges of a truly global network. With the talent and vision to create the critical systems and products that power global commerce and connect people everywhere to the vital goods and services they need every day.\nWorking at Mastercard means being part of a unique culture. Inclusive and diverse, a rich collaboration of ideas and perspectives. A place that celebrates your strengths, values your experiences, and offers you the flexibility to shape a career across disciplines and continents. And the opportunity to work alongside experts and leaders at every level of the business, improving what exists, and inventing what s next.\nTreasury Services Program\nThe Treasury Services Program provides vital systems and services to the Mastercard Treasury and Finance team in support of global payments/customer funds movement, treasury operations, liquidity, foreign exchange, risk management, and capital management. We are directly responsible for moving billions of dollars in domestic and cross border currencies across the world each day between our customers. We are an agile development organization with teams distributed globally across technology hubs in the U.S., India, and Ireland.\n\nAbout the Role\nSoftware Engineers at Mastercard design and code artificial intelligence, cloud, and machine learning platforms that provide mission-critical insights to many of the world s leading organizations and governments. As a Software Engineering Director, you will deliver these products and solutions with speed and agility as part of a small team. This will involve developing high-performing, highly scalable software solutions and products for some of the world s top brands. Specific tasks vary depending on the project and the business unit that you join in.\nAll staff at Mastercard are expected to demonstrate Mastercard Way cultural values every day - own it, simplify it, sense of urgency, thoughtful risk-taking, unlock potential, and be inclusive - with a relentless focus on our customers.\nAs a Director, Software Engineer at Mastercard, you are expected to perform the following general responsibilities:\nFormally supervise and coach multiple teams of engineers and managers to build, enhance, and support multiple applications/services in the delivery of internal or market-facing Products, Platforms, or Product bundles\nProvide strategic thinking and leadership related to a wide range of applications and systems, or software-development methodologies\nEnsure objectives and development plans are established at the start of the year and reviewed continuously throughout the year\nSpeak as one management voice and regularly hold staff meetings with all levels of staff to brief on organization, department, and People & Capabilities (HR) updates\nHold people and teams accountable and effectively delegate responsibilities down to the team\nRecruit and hire the right talent, always bringing in someone better than at least half the individuals in the role\nContinuously engage and improve teams performance by conducting recurring 1-1 meetings, knowing your people, managing career development, and understanding who is at risk\nProvide and facilitate timely feedback, coaching in the moment, and mentoring for staff at all levels\nManage and optimize budgets, forecasting, and cost allocation while delivering on business needs\nContinuously build a strong network across the company for collaboration on technical and business solutions\nEmulate and drive Mastercard Way behaviors through their behavior, recognitions, coaching, and employee engagement\n\nAll About You\nStrategic and experienced IT professional with a successful track record in managing and implementing Treasury Management Systems\nSolid experience in working with Treasury business that is managing liquidity, optimizing cash flows, working capital and overseeing risk management\nIdeal candidate will bring deep technical knowledge of treasury systems and cross-border flows\nKnowledge of in-house banking, netting and payment flows\nUnderstanding of how current regulatory and compliance environments impact treasury functions\nTechnical expertise in event driven architectures, design patterns, microservices and containerized deployments in private and public clouds (IE Azure, AWS, GCP)\nExperience in leading and coaching teams to perform software planning and estimation for large scale complex programs\nProficient in implementing COTS (Custom Off The Shelf) packages within complex, highly secure environments with multiple integrations\nBachelors degree in software engineering, computer science, information technology or related discipline preferred, or equivalent work experience; Payment industry knowledge strongly preferred but not required",Industry Type: Software Product,Department: Strategic & Top Management,"Employment Type: Full Time, Permanent","['Computer science', 'Career development', 'Treasury management', 'GCP', 'Information security', 'Employee engagement', 'Risk management', 'Business solutions', 'Information technology', 'Forecasting']",2025-06-13 06:19:53
Artificial Intelligence Engineer,Infosys,5 - 10 years,5-10 Lacs P.A.,['Pune'],"Our client INFOSYS is looking for Artificial Intelligence Engineer position with 5+ years of experience in Pune location. CONTRACT TO HIRE AND WORK FROM OFFICE\n\nJob Description :\nMandatory Skills : Python + LLMs + AI + Azure Certified.\nole Definition:\nThis is a specialized role for an AI Software Engineers design, build, and deploy scalable AI models and systems. They work with machine learning frameworks, cloud platforms, and data engineering tools to create and optimize AI solutions.\nSkills:\nProficient:\nLanguages/Framework: Fast API, Azure UI Search API (React)\nCloud: Azure Cloud Basics (Azure DevOps)\nGitlab: Gitlab Pipeline\nAnsible and REX: Rex Deployment\nData Science: Prompt Engineering + Modern Testing\nData pipeline development\nUnderstanding of AI/ML algorithms and their applications\nMLOps frameworks\nKnowledge of cloud platforms (Azure ML especially)\nModel deployment process\no             Data pipeline monitoring\n              Expert: (in addition to proficient skills)\no             Languages/Framework: Azure Open AI\no             Data Science: Open AI GPT Family of models 4o/4/3, Embeddings + Vector Search\no             Databases and ETL: Azure Storage Account, Postgresql, Cosmos\no             Experience with ML frameworks (TensorFlow, PyTorch, Scikit-learn)\no             Knowledge of cloud platforms (AWS SageMaker, Google AI Platform)\no             Expertise in data preprocessing, feature engineering, and model evaluation\no             Understanding of software engineering principles (version control, CI/CD, containerization)\no             Familiarity with distributed computing and big data tools (Spark, Hadoop)\no             Ability to optimize models for performance and scalability\no             Experience with Azure AI Search",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Temporary/Contractual","['llm', 'Python', 'Artificial Intelligence']",2025-06-13 06:19:54
Artificial Intelligence Engineer,Sightspectrum,6 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai']","AI Engineer/ Artificial Intaligence Engineer\n\nShould Experticed as AI Engineer/ Artificial Intaligence Engineer Previously\n\nExperience in :Python, API\n\nWork From Office\n\nJob Location : Chennai/Hyderabad\n\nyears of Experience : 6 to 8 years,\n\nShould Experticed as AI Engineer Previously",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'API', 'Python']",2025-06-13 06:19:56
"Principal Engineer, Performance Modelling",Renesas Electronics India Pvt. Ltd.,7 - 11 years,Not Disclosed,['Noida'],"We are seeking a highly skilled and experienced Engineers to join our team. The ideal candidate will be responsible for performance verification of System-on-Chip (SoC) designs using Platform Architect and Emulation Platform. This role involves working closely with SoC architects and cross-functional teams to optimize SoC performance and ensure the successful delivery of high-quality products.\nKey Responsibilities:\nDevelop and maintain performance models for SoC designs using Synopsys Platform Architect or Emulation Platform\nCollaborate with architecture, design, software and verification teams to define performance requirements and ensure alignment with overall system goals.\nAnalyze and optimize system performance, including DDR, CPU, GPU, Interconnects and high-speed interface like PCIe, UCIe etc\nIdentify performance bottlenecks and propose solutions to improve system efficiency.\nConduct performance simulations and provide detailed analysis and reports.\nMentor and guide junior engineers in performance modelling and analysis techniques and best practices.\nStay updated with the latest advancements in SoC performance modelling and industry solutions.\n\nQualifications\n\nBachelors or Masters degree in Electrical Engineering, Computer Engineering, or a related field.\n15+ years of experience in SoC performance modelling and analysis.\nProficiency in using Platform Architect or Emulation platform for performance modelling and analysis\nStrong understanding of SoC architecture, including CPU, GPU, DDR and interconnect subsystems. Any knowledge of NPUs and AI accelerators would be an added advantage.\nExperience with performance simulation tools and methodologies.\nExcellent problem-solving skills and attention to detail.\nStrong communication and collaboration skills.\nAbility to work independently and as part of a team.\n\nCompany Description\n\nRenesas is one of the top global semiconductor companies in the world. We strive to develop a safer, healthier, greener, and smarter world, and our goal is to make every endpoint intelligent by offering product solutions in the automotive, industrial, infrastructure and IoT markets. Our robust product portfolio includes world leading MCUs, SoCs, Analog and power products, plus Winning Combination solutions that curate these complementary products. We are a key supplier to the world s leading manufacturers of electronics you rely on every day; you may not see our products, but they are all around you.\n\n\n\nRenesas employs roughly 21, 000 people in more than 30 countries worldwide. As a global team, our employees actively embody the Renesas Culture, our guiding principles based on five key elements: Transparent, Agile, Global, Innovative, and Entrepreneurial. Renesas believes in, and has a commitment to, diversity and inclusion, with initiatives and a leadership team dedicated to its resources and values. At Renesas, we want to build a sustainable future where technology helps make our lives easier. Join us and build your future by being part of what s next in electronics and the world.",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Architect', 'Semiconductor', 'Simulation', 'Analog', 'SOC', 'Diversity and Inclusion', 'Agile', 'PCIE', 'Emulators', 'Automotive']",2025-06-13 06:19:58
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Nagpur'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Scientist', 'Operations Research', 'Pandas', 'Numpy', 'Scikit-Learn', 'Python', 'SQL']",2025-06-13 06:20:00
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Delhi / NCR'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Scientist', 'Operations Research', 'Pandas', 'Numpy', 'Scikit-Learn', 'Python', 'SQL']",2025-06-13 06:20:01
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Ahmedabad'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Scientist', 'Operations Research', 'Pandas', 'Numpy', 'Scikit-Learn', 'Python', 'SQL']",2025-06-13 06:20:03
Data Governance Lead,BP INCORPORATE INTERNATIONAL.,8 - 13 years,Not Disclosed,['Pune'],"Grade H - Office/ CoreResponsible for coordinating activities of a team to support the delivery of a range of business activities, driving consistency in terms of commercial deliverables and requirements, ensuring full compliance with all relevant standards and agreements, and providing business oversight and operational assurance.\nEntity:\nFinance\n\nBusiness Support Group\n\nWe re gearing up for the future. At bp our goal for CP is to deliver the future of mobility, energy and services for our customers by innovating with new business models and service platforms. CP will become a hub, housing our midstream, lubricants, aviation, sale of chemicals, mobility and convenience, marketing and our next-generation businesses making it a highly coordinated and interconnected organization. And with safety being our core value, our dedication to safe and reliable operations will never change.\nGlobal Business Services (GBS) is a coordinated part of bp, driving operational perfection and business solutions across the globe. GBS continues to evolve and plays a vital role in delivering business solutions that result in excellent outcomes for bp. We are a transformational engine continuing our focus on detailed customer experiences, digital and innovation !\nKEY ACCOUNTABILITIES\nData governance framework: Demonstrate deep understanding of the data governance framework and play a key SME role supporting the Data Governance manager in crafting processes for consistent implementation.\nProven knowledge of data governance concepts around data definition and catalog, data ownership, data lineage, data policies and controls, data monitoring and data governance forums\nExperience of working on data management tools such as Alation and MDG\nData definition and data glossary: Partner with the business and program team teams to detail business data glossary for assigned domain by assembling data definitions, data-standards, data lineage, data quality rules and critical metrics.\nEnsure the data glossary always remains up to date by following an exacting change governance.\nData ownership and stewardship: Ensure smooth onboarding for data owners and data stewards by providing them vital trainings to carry out their role effectively. Engage with them on a periodic basis to provide progress updates and to seek support to eliminate impediments if any.\nExtensive knowledge on Customer master and Material master Data by understanding integration with upstream and downstream legacy systems.\nDemonstrate deep understanding of the data governance framework and play a key SME role supporting the Data Governance manager in crafting processes for consistent implementation.\nData Policies: Ensure alignment to policies related to data privacy, data lifecycle management and data quality management for the assigned data asset.\nPartnership and Communication: Build a rapport with business collaborators, technology team, program team and wider digital solution and transformation team to find opportunities and areas to make a difference through the implementation of data governance framework!\nEXPERIENCE AND JOB REQUIREMENTS\n8+ years of experience predominantly in data related disciplines such as data governance, data quality and data cleansing in oil and gas or financial services domain.\nExpert knowledge of data governance concepts around data definition and catalog, data ownership, data lineage, data policies and controls, data monitoring and data governance forums.\nDeep knowledge of SAP ERP and associated data structures\nMust have been part of large, multi-year transformational change across multiple geographies across multiple data domains.\nComfortable to harmonise with senior partners and chair meetings/trainings related to data governance.\nExperience of working on data management tools such as Alation and MDG\nSoft skills: Active listening, communication and teamwork, presentation, Problem solving, Customer management.\nOther: Project management. Domain knowledge [Procurement, Finance, Customer], Critical thinking, Storytelling.\nAwareness of standard processes and new technologies in data management, data analytics space.\nDESIRABLE CRITERIA\nGood understanding of data visualization platforms such as Power BI, Tableau or QlikView\nExposure to data analytics, machine learning, artificial intelligence\n\nTravel Requirement\nUp to 10% travel should be expected with this role\n\nRelocation Assistance:\nThis role is eligible for relocation within country\n\nRemote Type:\nThis position is a hybrid of office/remote working\n\nSkills:\nAgility core practices, Analytical Thinking, Commercial Acumen, Communication, Creativity and Innovation, Data Analysis, Decision Making, Digital fluency, Integration, Managing strategic partnerships, Research and insights, Risk Management, Stakeholder Engagement, Stakeholder Management, Sustainability awareness and action",Industry Type: Oil & Gas,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Procurement', 'Data analysis', 'Data management', 'Aviation', 'Project management', 'Analytical', 'Risk management', 'Operations', 'Financial services', 'Monitoring']",2025-06-13 06:20:05
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Pune'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure ML', 'AWS SageMaker', 'Pandas', 'CPLEX', 'Pyomo', 'Databricks', 'Gurobi', 'NumPy', 'Python', 'SQL', 'PuLP']",2025-06-13 06:20:06
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Mumbai'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure ML', 'AWS SageMaker', 'Pandas', 'CPLEX', 'Pyomo', 'Databricks', 'Gurobi', 'NumPy', 'Python', 'SQL', 'PuLP']",2025-06-13 06:20:08
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Indore'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure ML', 'AWS SageMaker', 'Pandas', 'CPLEX', 'Pyomo', 'Databricks', 'Gurobi', 'NumPy', 'Python', 'SQL', 'PuLP']",2025-06-13 06:20:09
Lead Data Analyst - Power BI,Bot Consulting,5 - 8 years,Not Disclosed,['Jaipur'],"We are seeking an experienced and proactive Lead Data Analyst \\u2013 Power BI to lead the development of scalable analytics solutions and guide our growing data team in Jaipur. The ideal candidate will bring strong expertise in Power BI, SQL, Python, and experience with cloud data platforms such as Snowflake. You will be responsible for designing data models, leading dashboard/reporting initiatives, mentoring junior analysts, and enabling business stakeholders to make data-driven decisions.\nRoles & Responsibilities:\nLead the development and enhancement of interactive Power BI dashboards, reports, and data visualizations tailored to business requirements.\nArchitect and optimize data models in Power BI for performance and scalability (including DAX and Power Query transformations).\nBuild and manage robust end-to-end data pipelines, including data extraction, transformation, and loading (ETL/ELT).\nCollaborate with cross-functional stakeholders to translate business needs into technical solutions and actionable insights.\nPerform advanced data analysis using SQL and Python to uncover trends, patterns, and opportunities.\nEnsure data governance, quality, and consistency across all reporting assets and data platforms.\nMentor junior analysts and contribute to best practices in reporting, documentation, and code review.\nAct as a bridge between business and engineering teams, ensuring alignment and impact from analytics projects.\nWork with cloud data warehouses such as Snowflake or similar platforms for scalable analytics.\nSkills & Qualifications\n6+ years of experience in Data Analytics, Business Intelligence, or Data Engineering roles.\nProven expertise in Power BI, including dashboard development, DAX, data modeling, and Power Query.\nAdvanced proficiency in SQL and ability to work with large, complex datasets.\nProgramming experience in Python for data manipulation, automation, or machine learning (preferred).\nStrong understanding of ETL/ELT concepts, data warehousing, and modern cloud data platforms (Snowflake preferred).\nBachelors or Masters degree in Computer Science, Data Science, Engineering, or a related field.\nExcellent analytical thinking, problem-solving, and attention to detail.\nStrong communication skills and the ability to present data insights to non-technical stakeholders.\nPreferred Qualifications\nHands-on experience with Snowflake, Redshift, or BigQuery.\nFamiliarity with Airflow, DBT, or other orchestration tools.\nPower BI Certification (eg, PL-300: Microsoft Power BI Data Analyst).\nExperience with Agile methodologies and managing sprint-based BI deliverables.\nExposure to version control (Git) and CI/CD practices in data analytics projects.\nSigns You May Be a Great Fit\nImpact: Play a pivotal role in shaping a rapidly growing venture studio.\nCulture: Thrive in a collaborative, innovative environment that values creativity and ownership.\nGrowth: Access professional development opportunities and mentorship.\nBenefits: Competitive salary, health/we'llness packages, and flexible work options",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['IT services', 'Data analysis', 'Automation', 'Data modeling', 'Analytical', 'microsoft', 'Business intelligence', 'Analytics', 'SQL', 'Data extraction']",2025-06-13 06:20:11
Data Science Analyst,Cigna Medical Group,2 - 4 years,Not Disclosed,['Bengaluru'],"Key responsibilities\nDeliver quality analytics, from data preparation, data analysis, data exploration, data quality assessment, data manipulation, method selection, design & application, insights generation and visualisation\nDevelop and implement basic machine learning models and algorithms under the guidance of senior data scientists to extract insights and solve business problems\nProactive learning and acquisition of key analytical, technical and commercial skills and business knowledge to become a proficient Analyst working under the supervision of the senior/lead data science analysts.\nKPIs: Timeliness, accuracy, manager and client feedback (Internal and external as required)\nCollaborate with internal stakeholders and demonstrate the ability to transform client questions and problems into analytical solutions\nActive team member in providing the required support to help business understand and optimise use of analytical products and / or solutions\nBuild industry knowledge on the advancements in the field of analytics, data science and GenAI\nComply with the IM Cigna and CHSI Policies, procedures and processes, and continuously demonstrate Cigna Data and Analytics culture.\nKey activities\nWorking in a team to support end-to-end analytical projects\nLiaising with stakeholders to determine objectives / scope of upcoming projects\nData exploration, cleansing and manipulation\nDetermining appropriate type of analysis and undertaking analysis/modelling\nExtracting insights\nClear presentation of insights via spreadsheets, PowerPoint presentations, self-service analytical visualisation tools\nParticipate in client meetings\nOngoing stakeholder interaction (internal and external as required) on project progress\nContribute to the Feedback process (between stakeholders and the team) to ensure continuous improvement with team\nParticipate and contribute in learning forums such as Analytics Community and sharing knowledge with wider team\nExperience and education required\n2-4+ years experience in a technical analytics environment, carrying out data analytics and data science/AI projects and initiatives\nTertiary qualifications in engineering, mathematics, actuarial studies, statistics, physics, or a related discipline\nKnowledge of technical analytics discipline, including data preparation and foundational analytics concepts\nExperience with successfully managing both internal and external stakeholders, delivering against projects, tasks and activities in a dynamic deadline driven environment\nCommercial acumen to understand business needs and be able to suggest the commercial impacts of different analytics solutions or approaches\nCoding and modelling experience in SQL / R / Python and / or Cloud data platforms e.g. AWS\nExperience in visualization and data management tools is an added advantage\nExperience in GenAI/ LLMs is an added advantage\nExperience working with complex datasets\nAttention to detail and self driven continuous learning\nParticipation in external data hackathons and competitions will be an added advantage",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Data management', 'Coding', 'Analytical', 'Healthcare', 'Actuarial', 'Data quality', 'Continuous improvement', 'SQL', 'Python']",2025-06-13 06:20:13
Clinical Data Manager,Eclinical Solutions,7 - 12 years,Not Disclosed,['Bengaluru'],"Role & responsibilities:\nDefine project specifications for Data Management services, including Protocol Conversion, Database Build, CRF Design and Data Review and Data Reconciliation tools.\nUnderstand external data collection, its integration into the clinical trial, and the management and reconciliation processes required to ensure its accuracy and relevance.\nExecute data cleaning strategies to accelerate the time to achieve subject data cleanliness and ensure high-quality, timely deliverables.\nPerform holistic data review and trending analysis via reporting and elluminate analytics to proactively identify issue, risks and develop mitigation strategies.\nUtilize artificial intelligence (AI) and machine learning (ML) for anomaly and outlier detection to enhance the efficiency and quality of trial data.\nMonitor and interpret key performance indicators (KPIs), metrics, dashboards, Clinical Trial Operational Analytics (CTOA), and reports to provide actionable recommendations to study lead(s)/project manager.\nPerform Query Management\nDefine specifications and collaborate with technical team on configuration of centralized data management platform, elluminate Data Central for data cleaning strategy and oversight activities.\nPrepare and maintain data management documentation (e.g., DMP, CCGs, Help Text, DVS) and update throughout trial lifecycle.\nReview and ensure the quality control of team-developed deliverables, covering eCRFs, study documents, program/report specifications, outputs, and elluminate Data Central with analytics modules. Actively evaluate and contribute to enhance processes to increase efficiency and effectiveness.\nCollaborate and work as a team to ensure the deliverables are completed on time with high quality.\nEnsure compliance with eClinical Solutions/industry quality standards, regulations, guidelines, and procedures\n\nTechnical Skills:\n\nKnowledge of ICH/GCP guidelines, 21 CFR Part 11 and clinical trial methodology\nProficient with EDC and Clinical Data Management Systems\nExperience with Cloud SaaS platforms (preferred)Experience with data reporting tools such as Qlik, JReview, Spotfire preferred.\nExperience with RBQM methodology preferred.\nExposure to CDISC guidelines and standards\n\nPreferred candidate profile:\n\n5+ years’ experience in Clinical Data Management preferred.\nBachelor’s degree in a health-related field or equivalent experience preferred.\nCCDM Certification preferred.",Industry Type: Biotechnology,Department: Healthcare & Life Sciences,"Employment Type: Full Time, Permanent","['Study Startup', 'Clinical Data Management', 'Data Validation', 'Medidata Rave', 'EDC', 'CRF', 'Etmf', 'Sae Reconciliation', 'Cdm', 'Close Out']",2025-06-13 06:20:15
Digital Marketing Data Science Manager,Abinbev Gcc Services,6 - 10 years,Not Disclosed,['Bengaluru'],"AB InBev GCC was incorporated in 2014 as a strategic partner for Anheuser-Busch InBev. The center leverages the power of data and analytics to drive growth for critical business functions such as operations, finance, people, and technology. The teams are transforming Operations through Tech and Analytics.\nDo You Dream Big?\nWe Need You.\n\nJob Description",,,,"['NLP', 'Python', 'SQL', 'R', 'Power Bi', 'MFDL', 'Azure Cloud']",2025-06-13 06:20:16
Sr. Data Science,Disa Consulting Services,7 - 9 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Title: SR DATA SCIENCE\nExperience: 7 - 9 Years\nNotice Period: Immediate to 15 days\nWork Mode: Remote, Hybrid\nLocation: Hyderabad & Chennai\n\nRequirements:\nExperience designing and developing automated pipelines that utilize some combination of RAG pipelines, automated generation of prompts to LLMs, and/or multi-agent Agentic inference engines.\nAll skills / experience of above listed data scientist roles plus:\nArchitected and designed end to end pipelines that utilize LLMs and Autonomous Agents.\nDesigned and implemented methods for assuring quality and governing the output of Gen AI or Agentic solutions.\nDesigned and developed APIs exposing services of services that consume LLMs.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Restfull Api', 'Azure Cloud', 'LLM', 'Machine Learning', 'SQL', 'Python']",2025-06-13 06:20:18
Data Science Consultant,Clifyx Technology,2 - 5 years,Not Disclosed,['Bengaluru'],Clifyx Technology. is looking for Data Science Consultant to join our dynamic team and embark on a rewarding career journey Develops data models and performs statistical analysis\n\nProvides insights and recommendations for business strategy\n\nCollaborates with stakeholders to identify data needs\n\nImplements machine learning models and dashboards,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'Architecture', 'AWS', 'Python', 'microservices']",2025-06-13 06:20:19
AI ML Technical Lead,Globallogic,8 - 13 years,Not Disclosed,['Pune'],"Role Summary: The Technical Lead/Manager will be responsible for leading the AI CoE team, driving the implementation of AI-powered solutions for the organization. They will coordinate across multiple teams, work closely with stakeholders, and oversee the technical aspects of AI/ML projects, ensuring the effective application of generative AI technologies to the product suite.\nKey Responsibilities:\nLead the AI CoE team, ensuring alignment with business objectives.\nOversee the design, development, and deployment of AI/ML models for financial use cases.",,,,"['Artificial Intelligence', 'Machine Learning', 'SQL', 'R', 'Python', 'Tensorflow', 'Natural Language Processing', 'Neural Networks', 'Scikit-Learn', 'Deep Learning', 'Pytorch', 'Data Science', 'Image Processing', 'Aiml', 'Keras', 'Computer Vision']",2025-06-13 06:20:21
Application Lead,Accenture,15 - 25 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Automation Advisory\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n15 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for ensuring the successful delivery of projects and providing technical guidance to the team. Your typical day will involve collaborating with stakeholders, analyzing requirements, designing solutions, and overseeing the development and implementation of applications. You will also be involved in troubleshooting and resolving technical issues, as well as providing guidance and support to junior team members.\nRoles & Responsibilities:- As an AI Delivery Lead, youll play a critical role in managing the successful delivery of AI and machine learning (ML) projects.\nYoull collaborate with cross-functional teams, ensuring the timely and effective implementation of AI solutions.\nYour responsibilities will span project planning, risk assessment, stakeholder engagement, and overseeing the full lifecycle of AI solution development.- Expected to be a SME with deep knowledge and experience.- Should have Influencing and Advisory skills.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Collaborate with stakeholders to understand project requirements and objectives.- Analyze user needs and develop technical solutions to meet those needs.- Design, build, and configure applications according to project requirements.- Act as the primary point of contact for technical queries and issues.- Provide technical guidance and support to the development team.- Ensure the successful delivery of projects within the defined timeline and budget.- Troubleshoot and resolve technical issues that arise during the development and implementation process.- Conduct code reviews and ensure adherence to coding standards and best practices.- Stay up-to-date with industry trends and advancements in technology.- Mentor and provide guidance to junior team members to support their professional growth.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Automation Advisory.- Strong understanding of software development lifecycle and methodologies.- Experience in designing and developing applications using [PRIMARY ].- Knowledge of programming languages such as Java, C#, or Python.- Familiarity with database management systems and SQL.- Experience in implementing automation solutions and frameworks.- Excellent problem-solving and analytical skills.- Good To Have\n\n\n\n\nSkills:\nExperience with Agile development methodologies.- Knowledge of cloud platforms such as AWS or Azure.- Experience with DevOps practices and tools.- Understanding of cybersecurity principles and best practices.\nAdditional Information:- The candidate should have a minimum of 15 years of experience in Automation Advisory.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'sql', 'software development life cycle', 'java', 'c#', 'software testing', 'microsoft azure', 'artificial intelligence', 'javascript', 'application development', 'database management', 'solution design', 'devops', 'troubleshooting', 'html', 'agile', 'aws']",2025-06-13 06:20:23
Application Lead,Accenture,12 - 17 years,Not Disclosed,['Noida'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP for Utilities Cust Financial Mgt FICA\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :Graduate\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems that apply across multiple teams. With your expertise and leadership, you will contribute to the success of the project and drive innovation in application development.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Expected to provide solutions to problems that apply across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact- Manage the team and ensure successful project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP for Utilities Cust Financial Mgt FICA- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 12 years of experience in SAP for Utilities Cust Financial Mgt FICA- This position is based in Noida- A Graduate degree is required\n\nQualification\n\nGraduate",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'fica', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'team management', 'power bi', 'machine learning', 'application development', 'sql', 'data quality', 'tableau', 'data science', 'predictive modeling', 'project delivery', 'text mining', 'data visualization']",2025-06-13 06:20:25
Application Lead,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :PySpark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring the successful delivery of projects. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact- Manage the team and ensure successful project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in PySpark- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in PySpark- This position is based at our Hyderabad office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'team management', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'data science', 'predictive modeling', 'project delivery', 'text mining', 'data visualization']",2025-06-13 06:20:27
Application Lead,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :PySpark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact for the project- Manage the team and ensure successful project delivery- Collaborate with multiple teams to make key decisions- Provide solutions to problems for the immediate team and across multiple teams\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in PySpark- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in PySpark- This position is based at our Pune office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'team management', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'data science', 'predictive modeling', 'project delivery', 'text mining', 'data visualization']",2025-06-13 06:20:29
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :PySpark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact for the project- Manage the team and ensure successful project delivery- Collaborate with multiple teams to make key decisions- Provide solutions to problems for the immediate team and across multiple teams\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in PySpark- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 5 years of experience in PySpark- This position is based at our Pune office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'team management', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'data science', 'predictive modeling', 'project delivery', 'text mining', 'data visualization']",2025-06-13 06:20:31
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :PySpark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact- Manage the team and ensure successful project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in PySpark- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 5 years of experience in PySpark- This position is based at our Hyderabad office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'team management', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'data science', 'predictive modeling', 'project delivery', 'text mining', 'data visualization']",2025-06-13 06:20:33
Application Lead,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP FI CO Finance\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact for the project- Manage the team and ensure successful project delivery- Collaborate with multiple teams to make key decisions- Provide solutions to problems for the immediate team and across multiple teams\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP FI CO Finance- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in SAP FI CO Finance- This position is based at our Hyderabad office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['tableau', 'sap fico', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'sap', 'team management', 'sap fi', 'power bi', 'machine learning', 'sql', 'data quality', 'data science', 'predictive modeling', 'project delivery', 'text mining', 'data visualization']",2025-06-13 06:20:35
Application Lead,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP Analytics Cloud Planning\n\n\n\n\nGood to have skills :SAP Analytics Cloud DevelopmentMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your day will involve overseeing the application development process and ensuring successful project delivery.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the application development process- Ensure successful project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Analytics Cloud Planning- Good To Have\n\n\n\n\nSkills:\nExperience with SAP Analytics Cloud Development- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in SAP Analytics Cloud Planning- This position is based at our Pune office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap analytics cloud', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'sap', 'cloud development', 'power bi', 'machine learning', 'application development', 'sql', 'data quality', 'data science', 'predictive modeling', 'project delivery', 'data visualization']",2025-06-13 06:20:36
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Data Analytics\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n2 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various stakeholders to gather requirements, overseeing the development process, and ensuring that the applications meet the specified needs. You will also engage in problem-solving discussions with your team, providing guidance and support to ensure successful project outcomes. Your role will require you to stay updated on industry trends and best practices to enhance application performance and user experience.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Facilitate knowledge sharing sessions to enhance team capabilities.- Mentor junior team members to foster their professional growth.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Analytics.- Strong analytical skills to interpret complex data sets.- Experience with data visualization tools to present findings effectively.- Ability to develop and implement data-driven strategies.- Familiarity with statistical analysis techniques to derive insights.\nAdditional Information:- The candidate should have minimum 2 years of experience in Data Analytics.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data analytics', 'sql', 'ssrs', 'data visualization', 'c#', 'data analysis', 'power bi', 'business analysis', 'machine learning', 'business intelligence', 'sql server', 'plsql', 'tableau', 'r', 'data science', 'incident management', 'advanced excel', 'ssis', 'unix']",2025-06-13 06:20:38
Application Lead,Accenture,7 - 12 years,Not Disclosed,['Coimbatore'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Data Analytics\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\nProject Role :Application Lead\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact. Must have skills :Retail, CPG, Supply chain, marketing data Analytics\nGood to have skills :NAMinimum 7.5 year(s) of experience is required\nEducational Qualification :Application Lead\n\n\nSummary:Candidate needs to have an outside-in view of how Retail/Consumer goods services industry is evolving to support clients in their transformation journey .As an Application Lead who is experienced in Retail and Consumer goods Data Analytics , you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for overseeing the entire application development process and ensuring its successful implementation. Your role will involve collaborating with cross-functional teams, making key decisions, and providing solutions to problems. With your expertise in Data Retail and Consumer Goods Analytics, you will contribute to the development of innovative and efficient applications.\nRoles & Responsibilities:- Expected to be proficient in either of Retail, CPG, Supply chain, marketing data Analytics and or data engineering.\nExpected to be an SME, collaborate, and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute to key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the effort to design, build, and configure applications.- Act as the primary point of contact for application-related matters.- Oversee the entire application development process.- Ensure successful implementation of applications.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Retail and Consumer goods and services Analytics.- Strong understanding of statistical analysis and machine learning algorithms.-Extracting, cleaning, and manipulating large datasets from various sources required for delivery- Utilizing advanced data processing techniques and data visualization tools to present the reporting in a clear and compelling manner- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Retail and Consumer goods and services Data Analytics.- This position is based at our Pune office.- An Application Lead education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['retail', 'marketing', 'cpg', 'consumer goods', 'statistics', 'python', 'data analytics', 'analytics services', 'supply chain', 'machine learning', 'data engineering', 'application development', 'sql', 'data quality', 'data visualization', 'machine learning algorithms', 'data munging']",2025-06-13 06:20:40
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Pune'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Oracle Siebel Configuration\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems that apply across multiple teams. With your expertise in Oracle Siebel Configuration, you will play a crucial role in driving the success of the project.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Expected to provide solutions to problems that apply across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact- Manage the team and ensure successful project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Oracle Siebel Configuration- Good To Have\n\n\n\n\nSkills:\nExperience with Agile Project Management- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Oracle Siebel Configuration- This position is based at our Pune office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['oracle e-business suite', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'oracle', 'team management', 'power bi', 'machine learning', 'sql', 'data quality', 'data science', 'predictive modeling', 'project delivery', 'text mining', 'data visualization', 'agile']",2025-06-13 06:20:42
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Chennai'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Oracle Hyperion Planning\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :BE/M-TECH\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute to key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact- Manage the team and ensure successful project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Oracle Hyperion Planning- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Oracle Hyperion Planning- This position is based at our Chennai office- A BE/M-TECH is required\n\nQualification\n\nBE/M-TECH",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['tableau', 'oracle hyperion planning', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'oracle', 'team management', 'power bi', 'machine learning', 'oracle hyperion', 'sql', 'data quality', 'predictive modeling', 'project delivery', 'data visualization']",2025-06-13 06:20:43
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Chennai'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :PySpark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact- Manage the team and ensure successful project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in PySpark- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 5 years of experience in PySpark- This position is based at our Chennai office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'team management', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'data science', 'predictive modeling', 'project delivery', 'text mining', 'data visualization']",2025-06-13 06:20:45
Application Lead,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Data Analytics\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :Application Lead\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for overseeing the entire application development process and ensuring its successful implementation. Your role will involve collaborating with cross-functional teams, making key decisions, and providing solutions to problems. With your expertise in Data Analytics, you will contribute to the development of innovative and efficient applications.\nRoles & Responsibilities:- Expected to be an SME, collaborate, and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute to key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the effort to design, build, and configure applications.- Act as the primary point of contact for application-related matters.- Oversee the entire application development process.- Ensure successful implementation of applications.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Analytics.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Data Analytics.- This position is based at our Pune office.- An Application Lead education is required.\n\nQualification\n\nApplication Lead",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'data visualization', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'application development', 'sql', 'data quality', 'tableau', 'r', 'data science', 'predictive modeling', 'text mining']",2025-06-13 06:20:47
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Gurugram'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP S/4HANA Group Reporting\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems that apply across multiple teams. With your expertise and leadership, you will contribute to the success of the project and drive innovation.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Expected to provide solutions to problems that apply across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact for the project- Manage the team and ensure successful project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP S/4HANA Group Reporting- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 12 years of experience in SAP S/4HANA Group Reporting- This position is based at our Gurugram office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap s hana', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'team management', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'data science', 'predictive modeling', 'project delivery', 'text mining', 'data visualization']",2025-06-13 06:20:48
Application Lead,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :IFS Solutions\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for overseeing the entire application development process and ensuring its successful implementation.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the effort to design, build, and configure applications.- Act as the primary point of contact for application-related matters.- Oversee the entire application development process.- Ensure successful implementation of applications.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in IFS Solutions.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in IFS Solutions.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['tableau', 'ifs', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'application development', 'sql', 'data quality', 'data science', 'predictive modeling', 'text mining', 'data visualization']",2025-06-13 06:20:50
Business Function Implementation Lead,Accenture,7 - 12 years,Not Disclosed,['Chennai'],"Project Role :Business Function Implementation Lead\n\n\n\n\n\nProject Role Description :Plan and lead the implementation of all activities for a specific business function to improve performance for the business function end to end. Ensure alignment with business requirements including process analysis, design/re-design and/or organization structure definition.\n\n\n\nMust have skills :SAP CO Product Cost Controlling\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Function Implementation Lead, you will plan and lead the implementation of all activities for a specific business function to improve performance for the business function end to end. You will ensure alignment with business requirements including process analysis, design/re-design, and/or organization structure definition. Your typical day will involve strategizing and executing plans to optimize the performance of the business function, analyzing processes, and identifying areas for improvement. You will collaborate with cross-functional teams and provide guidance to achieve business goals.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the implementation of all activities for a specific business function.- Ensure alignment with business requirements including process analysis, design/re-design, and/or organization structure definition.- Strategize and execute plans to optimize the performance of the business function.- Analyze processes and identify areas for improvement.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP CO Product Cost Controlling.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in SAP CO Product Cost Controlling.- This position is based at our Chennai office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['tableau', 'sap co product cost controlling', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'business leadership', 'power bi', 'machine learning', 'sap co', 'sql', 'data science', 'predictive modeling', 'text mining', 'data visualization']",2025-06-13 06:20:52
Software Development Lead,Accenture,5 - 10 years,Not Disclosed,['Gurugram'],"Project Role :Software Development Lead\n\n\n\n\n\nProject Role Description :Develop and configure software systems either end-to-end or for a specific stage of product lifecycle. Apply knowledge of technologies, applications, methodologies, processes and tools to support a client, project or entity.\n\n\n\nMust have skills :Campaign Analytics & Reporting\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Software Development Lead, you will be responsible for developing and configuring software systems, applying knowledge of technologies, methodologies, and tools to support projects or clients in Gurugram.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead and mentor junior professionals- Drive innovation and continuous improvement\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Campaign Analytics & Reporting- Strong understanding of data analysis techniques- Experience with data visualization tools such as Tableau or Power BI- Hands-on experience in implementing various analytics algorithms- Solid grasp of data munging techniques\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Campaign Analytics & Reporting- This position is based at our Gurugram office- A 15 years full time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analysis', 'power bi', 'tableau', 'campaign analytics', 'data munging', 'algorithms', 'python', 'software development', 'natural language processing', 'predictive analytics', 'machine learning', 'sql', 'r', 'data science', 'predictive modeling', 'logistic regression', 'statistics']",2025-06-13 06:20:53
Technology Delivery Lead,Accenture,15 - 25 years,Not Disclosed,['Ahmedabad'],"Project Role :Technology Delivery Lead\n\n\n\n\n\nProject Role Description :Manages the delivery of large, complex technology projects using appropriate frameworks and collaborating with sponsors to manage scope and risk. Drives profitability and continued success by managing service quality and cost and leading delivery. Proactively support sales through innovative solutions and delivery excellence.\n\n\n\nMust have skills :SAP FI S/4HANA Accounting\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n15 year(s) of experience is required\n\n\n\n\nEducational Qualification :Any Degree\n\n\nSummary:As a Technology Delivery Lead, you will manage the delivery of large, complex technology projects using appropriate frameworks and collaborating with sponsors to manage scope and risk. You will drive profitability and continued success by managing service quality and cost and leading delivery. Additionally, you will proactively support sales through innovative solutions and delivery excellence.\nRoles & Responsibilities:- Expected to be a SME with deep knowledge and experience.- Should have Influencing and Advisory skills.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Manage the delivery of large, complex technology projects using appropriate frameworks.- Collaborate with sponsors to manage scope and risk.- Drive profitability and continued success by managing service quality and cost.- Lead delivery and ensure delivery excellence.- Proactively support sales through innovative solutions.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP FI S/4HANA Accounting.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 15 years of experience in SAP FI S/4HANA Accounting.- This position is based at our Hyderabad office.- A Any Degree is required.\n\nQualification\n\nAny Degree",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap fi', 'accounting', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analytics', 'natural language processing', 'delivery leadership', 'power bi', 'machine learning', 'sales', 'sql', 'tableau', 'r', 'data science', 'predictive modeling', 'text mining']",2025-06-13 06:20:55
Delivery Lead Manager,Accenture,13 - 18 years,Not Disclosed,['Navi Mumbai'],"Skill required: Record To Report - Account Management\n\n\n\n\nDesignation: Delivery Lead Manager\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:13 to 18 years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nYou will be aligned with our Finance Operations vertical and will be helping us in determiningfinancial outcomes by collecting operational data/reports, whilst conducting analysis and reconciling transactions.The team aligns Finance with Business Strategy in order to maximize operational efficiency andeffectiveness by harnessing the power of robotics to accelerate transaction processing, with AI toprovide analysis and commentary and machine learning deployed for matching and reconciliationsThis team is responsible for leading the transformation agenda for our clients by helping themimprove finance function performance within the context of their organizations strategies. The teamleads growth and mining in existing F&A accounts for industry segments. Procure to Pay (PTP), Record to Report (RTR), Order to cash (OTC), Finance Processes (FPNA),Tax and Treasury towers to provide best in the class industry view, support existing engagements,diagnostic, due diligence, client co-design workshops, transformation engagement, drive existingclient portfolios. The team works on Industry Digital F&A technologies & leading tools, automation,AI, and Analytics.\n\n\n\n\nWhat are we looking for\nRecord To ReportAbility to work well in a teamCommitment to qualityWritten and verbal communicationAbility to manage multiple stakeholdersAbility to meet deadlines\n\n\n\nRoles and Responsibilities: In this role you are required to identify and assess complex problems for area of responsibility The person would create solutions in situations in which analysis requires an in-depth evaluation of variable factors Requires adherence to strategic direction set by senior management when establishing near-term goals Interaction of the individual is with senior management at a client and/or within Accenture, involving matters that may require acceptance of an alternate approach Some latitude in decision-making in involved you will act independently to determine methods and procedures on new assignments Decisions individual at this role makes have a major day to day impact on area of responsibility The person manages large - medium sized teams and/or work efforts (if in an individual contributor role) at a client or within Accenture Please note that this role may require you to work in rotational shifts\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['due diligence', 'business strategy', 'account management', 'delivery management', 'record to report', 'project management', 'program management', 'sales', 'pmp', 'portfolio', 'relationship management', 'wealth management', 'mutual funds', 'service delivery management', 'finance']",2025-06-13 06:20:57
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Mumbai'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Salesforce CRM Analytics\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various teams to ensure that application requirements are met, overseeing the development process, and providing guidance to team members. You will also engage in problem-solving activities, ensuring that the applications are aligned with business objectives and user needs, while maintaining a focus on quality and efficiency throughout the project lifecycle.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and ensure timely delivery of milestones.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Salesforce CRM Analytics.- Strong understanding of data integration techniques and tools.- Experience with application design and architecture principles.- Ability to analyze business requirements and translate them into technical specifications.- Familiarity with agile methodologies and project management practices.\nAdditional Information:- The candidate should have minimum 5 years of experience in Salesforce CRM Analytics.- This position is based in Mumbai.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['application design', 'crm analytics', 'salesforce crm', 'data integration', 'agile methodology', 'python', 'data analysis', 'sas', 'predictive analytics', 'machine learning', 'sql', 'salesforce', 'r', 'predictive modeling', 'statistical modeling', 'agile', 'logistic regression', 'statistics']",2025-06-13 06:20:59
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Kolkata'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP Sales and Distribution (SD)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute to key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact for the project- Manage the team and ensure successful project delivery- Collaborate with multiple teams to make key decisions- Provide solutions to problems for the immediate team and across multiple teams\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Sales and Distribution (SD)- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 5 years of experience in SAP Sales and Distribution (SD)- This position is based in Mumbai- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['distribution', 'sap presales', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'sap', 'team management', 'power bi', 'machine learning', 'sales', 'sql', 'data quality', 'tableau', 'data science', 'predictive modeling', 'project delivery', 'text mining', 'data visualization']",2025-06-13 06:21:00
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :PySpark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As an Application Lead, you will be responsible for leading the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve working with PySpark and collaborating with cross-functional teams to deliver high-quality solutions.\nRoles & Responsibilities:- Lead the design, development, and deployment of PySpark-based applications, ensuring high-quality solutions are delivered on time and within budget.- Collaborate with cross-functional teams, including business analysts, data scientists, and software developers, to ensure that applications meet business requirements and are scalable and maintainable.- Act as the primary point of contact for all application-related issues, providing technical guidance and support to team members and stakeholders.- Ensure that applications are designed and developed in accordance with industry best practices, including coding standards, testing methodologies, and deployment processes.- Stay up-to-date with the latest trends and technologies in PySpark and related fields, and apply this knowledge to improve the quality and efficiency of application development.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nStrong experience in PySpark.- Good To Have\n\n\n\n\nSkills:\nExperience with other big data technologies such as Hadoop, Hive, and Spark.- Solid understanding of software development principles, including object-oriented programming, design patterns, and agile methodologies.- Experience with database technologies such as SQL and NoSQL.- Experience with cloud platforms such as AWS or Azure.- Strong problem-solving and analytical skills, with the ability to troubleshoot complex issues and provide effective solutions.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in PySpark.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering high-quality software solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'software development', 'pyspark', 'spark', 'hadoop', 'visualforce', 'sfdc', 'ado.net', 'microsoft azure', 'triggers', 'sql', 'nosql', 'application development', 'apex', 'salesforce', 'sales force development', 'salesforce crm', 'data loader', 'design patterns', 'agile', 'aws', 'agile methodology']",2025-06-13 06:21:02
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :ACI Universal Payment BASE24-Classic\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact- Manage the team and ensure successful project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in ACI Universal Payment BASE24-Classic- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 5 years of experience in ACI Universal Payment BASE24-Classic- This position is based in Pune- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['aci', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'team management', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'data science', 'predictive modeling', 'project delivery', 'text mining', 'data visualization']",2025-06-13 06:21:04
Application Lead,Accenture,7 - 12 years,Not Disclosed,['Navi Mumbai'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :SAP Sales and Distribution (SD)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for managing the team and ensuring successful project delivery. Your typical day will involve collaborating with multiple teams, making key decisions, and providing solutions to problems for your immediate team and across multiple teams.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute to key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the effort to design, build, and configure applications- Act as the primary point of contact for the project- Manage the team and ensure successful project delivery- Collaborate with multiple teams to make key decisions- Provide solutions to problems for the immediate team and across multiple teams\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Sales and Distribution (SD)- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in SAP Sales and Distribution (SD)- This position is based in Mumbai- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['distribution', 'sap presales', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'sap', 'team management', 'power bi', 'machine learning', 'sales', 'sql', 'data quality', 'tableau', 'data science', 'predictive modeling', 'project delivery', 'text mining', 'data visualization']",2025-06-13 06:21:05
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :MicroStrategy Business Intelligence\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will be dedicated to delivering innovative financial solutions for BANK OF AMERICA's '9940151521 - FS NA AO - BOFA Test LLP' project, leveraging MicroStrategy Business Intelligence expertise.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the development and implementation of BI solutions.- Design and implement effective business intelligence solutions.- Ensure effective collaboration with cross-functional teams.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in MicroStrategy Business Intelligence.- Strong understanding of data visualization tools such as Tableau or Power BI.- Hands-on experience in implementing various machine learning algorithms.- Solid grasp of data munging techniques for data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in MicroStrategy Business Intelligence.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['business intelligence', 'microstrategy', 'tableau', 'data integration tools', 'data munging', 'python', 'data analytics', 'natural language processing', 'power bi', 'data warehousing', 'machine learning', 'sql', 'plsql', 'data quality', 'r', 'java', 'data science', 'etl tool', 'data visualization', 'etl']",2025-06-13 06:21:07
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :PySpark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your day will involve overseeing the application development process and ensuring successful project delivery.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the application development process- Ensure successful project delivery- Mentor and guide team members\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in PySpark- Strong understanding of big data processing- Experience with data pipelines and ETL processes- Hands-on experience in building scalable applications- Knowledge of cloud platforms and services\nAdditional Information:- The candidate should have a minimum of 5 years of experience in PySpark- This position is based at our Hyderabad office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data processing', 'pyspark', 'application development', 'big data', 'etl', 'hive', 'python', 'project management', 'oracle', 'data analysis', 'data warehousing', 'microsoft azure', 'machine learning', 'sql server', 'sql', 'plsql', 'java', 'spark', 'project delivery', 'hadoop', 'sqoop', 'aws', 'data entry', 'unix']",2025-06-13 06:21:09
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Chennai'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :PySpark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will oversee the application development process and ensure successful project delivery.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the application development process- Ensure timely project delivery- Provide technical guidance and mentorship to team members\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in PySpark- Strong understanding of big data processing- Experience with data processing frameworks like Apache Spark- Hands-on experience in designing and implementing scalable data pipelines- Solid grasp of data manipulation and transformation techniques\nAdditional Information:- The candidate should have a minimum of 5 years of experience in PySpark- This position is based at our Chennai office- An Engineering graduate preferably Computer Science graduate with 15 years of full-time education is required\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data manipulation', 'data processing', 'pyspark', 'spark', 'big data', 'c#', 'python', 'entity framework', 'ado.net', 'machine learning', 'autocad', 'sql server', 'javascript', 'application development', 'sql', 'tableau', 'apache', 'solid works', 'asp.net', 'project delivery', 'data visualization']",2025-06-13 06:21:10
Application Lead,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Apache Spark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various stakeholders to gather requirements, overseeing the development process, and ensuring that the applications meet the specified needs. You will also engage in problem-solving discussions with your team, providing guidance and support to ensure successful project outcomes. Your role will require you to stay updated with the latest technologies and methodologies to enhance application performance and user experience.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Facilitate knowledge sharing sessions to enhance team capabilities.- Mentor junior team members to foster their professional growth.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Apache Spark.- Strong understanding of distributed computing principles.- Experience with data processing frameworks and tools.- Familiarity with cloud platforms and services.- Ability to optimize application performance and scalability.\nAdditional Information:- The candidate should have minimum 3 years of experience in Apache Spark.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'distributed computing', 'apache', 'java', 'spark', 'c#', 'algorithms', 'c++', 'c', 'oracle', 'machine learning', 'sql server', 'sql', 'spring', 'incident management', 'application support', 'j2ee', 'data structures', 'hadoop', 'big data', 'aws', 'cloud computing', 'unix']",2025-06-13 06:21:12
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :PySpark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :A Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will oversee the application development process and ensure successful project delivery.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the application development process- Ensure timely project delivery- Provide technical guidance and support to the team\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in PySpark- Strong understanding of big data processing- Experience with cloud platforms like AWS or Azure- Hands-on experience in designing and implementing scalable applications- Knowledge of data modeling and database management\nAdditional Information:- The candidate should have a minimum of 5 years of experience in PySpark- This position is based at our Pune office- A Engineering graduate preferably Computer Science graduate 15 years of full time education is required\n\nQualification\n\nA Engineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data processing', 'pyspark', 'microsoft azure', 'big data', 'aws', 'hive', 'c#', 'python', 'oracle', 'data warehousing', 'machine learning', 'sql server', 'application development', 'sql', 'plsql', 'database management', 'java', 'data modeling', 'spark', 'project delivery', 'hadoop', 'sqoop', 'unix']",2025-06-13 06:21:14
"Manager III, Quality , QA Lead",Amazon,10 - 15 years,Not Disclosed,['Chennai'],"Amazon india is an inventive research and development company that designs and engineers high-profile devices like Echo, FireTV, tablets, e-readers, which have changed the daily lives of millions of users. Most advanced products need cutting-edge testing to ensure they reach the customers with the best quality. This is the mission of every Software QA engineer at Amazon Lab126/India .\n\nAt Amazon, we not only test the products, we treat the testing activities on par with design and invention of a system. We use state of the art technologies and methodologies to ensure the most efficient and most comprehensive testing. Invention of new test methodologies and innovation is the essence of testing in our team. Work hard. Have fun. Make history.\n\nRole Requirements:\nYou are expected to have industry-leading technical abilities that enable you to significantly improve product quality. You should have a combination of:\nSolid in-depth knowledge of Quality Assurance Concepts\nExcellent solid understanding of the operating system software, as well as knowledge of object-oriented design principles.\nUnderstanding of 802.11 standards\nTotal 10+ years of experience in wireless testing in BT, or Zigbee\nYou will spend efforts studying and designing the new methodologies and techniques to facilitate testing, including the state-of-art Machine Learning and Deep Learning methods.\n\nBasic qualifications\nExperience in manual testing and automated testing\n6+ year of experience working as a Quality Assurance Engineer\nBachelor s degree in Computer Science, Computer Engineering or similar technical field.\n6+ years of experience in automation development and testing using Python, experience in automating the test cases for embedded software.\n\nPreferred qualifications\nProduct experience with wireless consumer electronics\nDomain expertise in Wi-Fi or , Bluetooth or Zigbee\nHands-on experience in Bluetooth certification and Wi-Fi certification\nHands-on experience in Bluetooth profiles and features (A2DP, AVRCP, MAP, HID and HFP) testing\nSolid understanding of QA tool and environments for wi-Fi , BT packet capture\nExperience writing automation code for (Wi-Fi,Bluetooth, Zigbee) based systems\nFamiliarity with WiFi 802.11 a/b/g/n/ac\nShows creativity and initiative to improve product coverage and effectiveness\nKnowledge of industry standard test automation tools and experience developing product test harnesses\nExperience with Android, iOS or other mobile application development or testing\nStrong experience in Python, Java, C, and C++\nExperience with open source tools and resources\nExperience working closely with development and business teams.\n\n\nAs a Software Quality Assurance Engineer, you will join the team of hands-on, passionate and seasoned SQA professionals developing innovative consumer-centric testing solutions. In this role, you will:\nDevelop test plans and test cases\nDevelop Automation test harness and automate tests using internal and open source tools infrastructure\nFind, isolate, document, regress, and track bugs through to a resolution\nInterpret and report testing results, and be a vocal proponent for quality in every phase of the development process\nEngage with an experienced cross-disciplinary staff to conceive, design and develop innovative consumer products.\nNeed to have hunger of continuously searching for better and more efficient test solutions, and an instinct for continuous invention and innovation.\nHave the opportunity to propose improvements to our existing processes and automated tools in order to improve the team s speed, quality and efficiency.\nBe responsive, flexible and able to succeed within an open collaborative peer environment. You will need to be able to work efficiently and effectively in a fun, fast-paced dynamic team environment.\n\nA day in the life\nLead the team with hands on exp in technology\n\nAbout the team\nConnectivity team works in WIFI , BT , ZIGBEE technology on all Amazon products 1+ years of quality assurance engineering experience\n3+ years of quality assurance teams management experience\nExperience managing manual testers\nExperience managing automation testers\nExperience testing web technologies and back-end services\nExperience identifying and reviewing test plans, test cases and testing results with a strong QA background Experience preparing quality metrics and effectively engaging with stakeholders to set and drive quality goals\nExperience transforming QA programs from manual to automation",,,,"['Wireless', 'C++', 'SQA', 'Bluetooth', 'Manual testing', 'Test cases', 'Open source', 'Android', 'Python', 'Embedded software']",2025-06-13 06:21:16
Team Lead,Startek,1 - 5 years,Not Disclosed,['Noida'],"STARTEK is looking for Team Lead to join our dynamic team and embark on a rewarding career journey A Team Lead is a professional who is responsible for leading, guiding, and supervising a team of employees to achieve specific goals and objectives\n\nSome of the key responsibilities of a Team Lead include:1\n\nProviding direction, guidance, and support to team members to help them achieve their individual and team goals\n\n2\n\nManaging team schedules, delegating tasks, and ensuring that deadlines are met\n\n3\n\nMentoring, coaching, and providing feedback to team members to help them grow and develop their skills\n\n4\n\nIdentifying and resolving conflicts and obstacles that may impact team performance\n\n5\n\nEnsuring that team members have the necessary resources and support to perform their job effectively\n\nThe ideal candidate for this role should have strong leadership, communication, and interpersonal skills",Industry Type: Oil & Gas,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Wireless', 'Business transformation', 'orchestration', 'Artificial Intelligence', 'Customer support', 'Customer experience', 'Management', 'Troubleshooting', 'Customer engagement', 'digital transformation']",2025-06-13 06:21:18
Architect (AI and Cloud),Rakuten Symphony,10 - 18 years,Not Disclosed,['Bengaluru( Kadubeesanahalli )'],"Why should you choose us? Rakuten Symphony is reimagining telecom, changing supply chain norms and disrupting outmoded thinking that threatens the industrys pursuit of rapid innovation and growth. Based on proven modern infrastructure practices, its open interface platforms make it possible to launch and operate advanced mobile services in a fraction of the time and cost of conventional approaches, with no compromise to network quality or security. Rakuten Symphony has operations in Japan, the United States, Singapore, India, South Korea, Europe, and the Middle East Africa region. For more information, visit: https://symphony.rakuten.com\n\nBuilding on the technology Rakuten used to launch Japans newest mobile network, we are taking our mobile offering global. To support our ambitions to provide an innovative cloud-native telco platform for our customers, Rakuten Symphony is looking to recruit and develop top talent from around the globe. We are looking for individuals to join our team across all functional areas of our business from sales to engineering, support functions to product development. Lets build the future of mobile telecommunications together!\n\nAbout Rakuten Rakuten Group, Inc. (TSE: 4755) is a global leader in internet services that empower individuals, communities, businesses and society. Founded in Tokyo in 1997 as an online marketplace, Rakuten has expanded to offer services in ecommerce, fintech, digital content and communications to approximately 1.5 billion members around the world. The Rakuten Group has over 27,000 employees, and operations in 30 countries and regions. For more information visit https://global.rakuten.com/corp/\n\nJob Summary:\nThe AI Architect is a senior technical leader responsible for designing and implementing the overall AI infrastructure and architecture for the organization. This role will define the technical vision for AI initiatives, select appropriate technologies and platforms, and ensure that AI systems are scalable, reliable, secure, and aligned with business requirements. The AI Architect will work closely with CTO Office, product manager, engineering manager, data scientists, machine learning engineers, and other stakeholders to build a robust and efficient AI ecosystem.\n\nMandatory Skills:\nCloud Computing Platforms (AWS, Azure, GCP).\nAI/ML Frameworks (TensorFlow, PyTorch, scikit-learn) .\nData Engineering Tools (Spark, Hadoop, Kafka).\nMicroservices Architecture.\nAI/ML as a service Deployment.\nDevOps Principles (CI/CD/CT).\nStrong understanding of AI/ML algorithms and techniques\n\nRoles & Responsibilities:\nDefine the overall AI architecture and infrastructure strategy for the organization.\nSelect appropriate technologies and platforms for AI development and deployment. • Design scalable, reliable, and secure AI systems.\nDevelop and maintain architectural blueprints and documentation.\nProvide technical leadership and guidance to tech lead, engineering manager, data scientists, machine learning engineers, and other stakeholders.\nEnsure that AI systems are aligned with business requirements and industry best practices. Evaluate new AI technologies and trends.\nCollaborate with security and compliance teams to ensure that AI systems meet regulatory requirements.\nCollaborate with CTO Office to ensure the AI strategy implemented aligned with overall business unit strategy.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Architect', 'Artificial Intelligence', 'Cloud', 'Microservice Based Architecture', 'Tensorflow', 'Pytorch', 'Ai', 'Machine Learning', 'Solution Architect', 'Scikit-Learn', 'Technical Architecture']",2025-06-13 06:21:20
S&C Global Network - AI - Auto & Industrial - Analyst,Accenture,1 - 3 years,Not Disclosed,['Bengaluru'],"Management Level:Ind & Func AI Decision Science Analyst\n\n\n\nLocation:Bengaluru (Bangalore), Gurugram (Gurgaon), Hyderabad, Chennai.\n\n\n\nMust-have skills:Programming languages -Python/R, Generative AI, Large Language Models (LLMs), ML libraries such as Scikit-learn, TensorFlow, Torch, Lang Chain, or OpenAI API, RAG Applications.\n\n\n\n\nGood to have skills:Big data technologies such as Spark or Hadoop,AI model explainability(XAI),bias detection and AI ethics. Familiarity with Edge AI and deploying models on embedded devices for industrial automation. Experience with Reinforcement Learning (RL) and AI-driven optimization techniques.\n\n\n\nJob\n\n\nSummary\n\nWe are looking for a Data Scientist / AI Specialist with 1-3 years of experience to join our team and work on client projects in the Automotive & Industrial sectors. This role will involve leveraging traditional Machine Learning (ML), Generative AI (GenAI), Agentic AI, and Autonomous AI Systems to drive innovation, optimize processes, and enhance decision-making in complex industrial environments.\n\nPrior experience in the Auto/Industrial industry is a plus, but we welcome candidates from any domain with a strong analytical mindset and a passion for applying AI to real-world business challenges.\n\n\n\n\nRoles & Responsibilities:\nDevelop, deploy and monitor AI/ML models in production environments & enterprise systems, including predictive analytics, anomaly detection, and process optimization for clients.\nWork with Generative AI models (e.g., GPT, Stable Diffusion, DALLE) for applications such as content generation, automated documentation, code synthesis, and intelligent assistants.\nImplement Agentic AI systems, including AI-powered automation, self-learning agents, and decision-support systems for industrial applications.\nDesign and build Autonomous AI solutions for tasks like predictive maintenance, supply chain optimization, and robotic process automation (RPA).\nWork with structured and unstructured data from various sources, including IoT sensors, manufacturing logs, and customer interactions.\nOptimize and fine-tune LLMs (Large Language Models) for specific business applications, ensuring ethical and explainable AI use.\nUtilize MOps and AI orchestration tools to streamline model deployment, monitoring, and retraining cycles.\nCollaborate with cross-functional teams, including engineers, business analysts, and domain experts, to align AI solutions with business objectives.\nStay updated with cutting-edge AI research in Generative AI, Autonomous AI, and Multi-Agent Systems.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n1-3 years of experience in Data Science, Machine Learning, or AI-related roles.\nProficiency in Python (preferred) or R, and experience with ML libraries such as Scikit-learn, TensorFlow, Torch, Lang Chain, or OpenAI API.\nStrong understanding of Generative AI, Large Language Models (LLMs), and their practical applications.\nHands-on experience in fine-tuning and deploying foundation models (e.g., OpenAI, Llama, Claude, Gemini, etc.).\nExperience with Vector Databases (e.g., FAISS, Chroma, Weaviate, Pinecone) for retrieval-augmented generation (RAG) applications.\nKnowledge of Autonomous AI Agents (e.g., AutoGPT, BabyAGI) and multi-agent orchestration frameworks.\nExperience working with SQL and NoSQL databases.\nFamiliarity with cloud platforms (AWS, Azure, or GCP) for AI/ML model deployment.\nStrong problem-solving and analytical thinking abilities.\nAbility to communicate complex AI concepts to technical and non-technical stakeholders.\nBonus:Experience in Automotive, Industrial, or Manufacturing AI applications (e.g., predictive maintenance, quality inspection, digital twins).\n\n\n\n\nAdditional Information:\nBachelor/Masters degree in Statistics/Economics/ Mathematics/ Computer Science or related disciplines with an excellent academic record /MBA from top-tier universities.\nExcellent Communication and Interpersonal Skills.\n\n\n\n\n\nAbout Our Company | Accenture\n\n\n\n\n\nQualification\n\n\n\nExperience:Minimum 1-3 years of relevant Data Science, Machine Learning or AI-related roles., Exposure to Industrial & Automotive Firms or Professional Services.\n\n\n\n\nEducational Qualification: Bachelor/Masters degree in Statistics/Economics/ Mathematics/ Computer Science or related disciplines with an excellent academic record or MBA from top-tier universities.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'sql', 'tensorflow', 'data science', 'image processing', 'c++', 'scikit-learn', 'microsoft azure', 'artificial intelligence', 'nosql', 'deep learning', 'r', 'spark', 'gcp', 'computer vision', 'network analysis', 'hadoop', 'api', 'big data', 'aws', 'opencv']",2025-06-13 06:21:21
S&C Global Network - AI - Auto & Industrial - Consultant,Accenture,3 - 6 years,Not Disclosed,['Gurugram'],"Management Level:Ind & Func AI Decision Science Consultant\n\n\n\nLocation:Bengaluru (Bangalore), Gurugram (Gurgaon), Hyderabad, Chennai.\n\n\n\nMust-have skills:Programming languages -Python/R, Generative AI, Large Language Models (LLMs), ML libraries such as Scikit-learn, TensorFlow, Torch, Lang Chain, or OpenAI API, RAG Applications.\n\n\n\n\nGood to have skills:Big data technologies such as Spark or Hadoop,AI model explainability(XAI),bias detection and AI ethics. Familiarity with Edge AI and deploying models on embedded devices for industrial automation. Experience with Reinforcement Learning (RL) and AI-driven optimization techniques.\n\n\n\nJob\n\n\nSummary\n\nWe are looking for a Data Scientist / AI Specialist with 3-6 years of experience to join our team and work on client projects in the Automotive & Industrial sectors. This role will involve leveraging traditional Machine Learning (ML), Generative AI (GenAI), Agentic AI, and Autonomous AI Systems to drive innovation, optimize processes, and enhance decision-making in complex industrial environments.\n\nPrior experience in the Auto/Industrial industry is a plus, but we welcome candidates from any domain with a strong analytical mindset and a passion for applying AI to real-world business challenges.\n\n\n\n\nRoles & Responsibilities:\nDevelop, deploy and monitor AI/ML models in production environments & enterprise systems, including predictive analytics, anomaly detection, and process optimization for clients.\nWork with Generative AI models (e.g., GPT, Stable Diffusion, DALLE) for applications such as content generation, automated documentation, code synthesis, and intelligent assistants.\nImplement Agentic AI systems, including AI-powered automation, self-learning agents, and decision-support systems for industrial applications.\nDesign and build Autonomous AI solutions for tasks like predictive maintenance, supply chain optimization, and robotic process automation (RPA).\nWork with structured and unstructured data from various sources, including IoT sensors, manufacturing logs, and customer interactions.\nOptimize and fine-tune LLMs (Large Language Models) for specific business applications, ensuring ethical and explainable AI use.\nUtilize MOps and AI orchestration tools to streamline model deployment, monitoring, and retraining cycles.\nCollaborate with cross-functional teams, including engineers, business analysts, and domain experts, to align AI solutions with business objectives.\nStay updated with cutting-edge AI research in Generative AI, Autonomous AI, and Multi-Agent Systems.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n3-6 years of experience in Data Science, Machine Learning, or AI-related roles.\nProficiency in Python (preferred) or R, and experience with ML libraries such as Scikit-learn, TensorFlow, Torch, Lang Chain, or OpenAI API.\nStrong understanding of Generative AI, Large Language Models (LLMs), and their practical applications.\nHands-on experience in fine-tuning and deploying foundation models (e.g., OpenAI, Llama, Claude, Gemini, etc.).\nExperience with Vector Databases (e.g., FAISS, Chroma, Weaviate, Pinecone) for retrieval-augmented generation (RAG) applications.\nKnowledge of Autonomous AI Agents (e.g., AutoGPT, BabyAGI) and multi-agent orchestration frameworks.\nExperience working with SQL and NoSQL databases.\nFamiliarity with cloud platforms (AWS, Azure, or GCP) for AI/ML model deployment.\nStrong problem-solving and analytical thinking abilities.\nAbility to communicate complex AI concepts to technical and non-technical stakeholders.\nBonus:Experience in Automotive, Industrial, or Manufacturing AI applications (e.g., predictive maintenance, quality inspection, digital twins).\n\n\n\n\nAdditional Information:\nBachelor/Masters degree in Statistics/Economics/ Mathematics/ Computer Science or related disciplines with an excellent academic record /MBA from top-tier universities.\nExcellent Communication and Interpersonal Skills.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:Minimum 3-6 years of relevant Data Science, Machine Learning or AI-related roles., Exposure to Industrial & Automotive Firms or Professional Services.\n\n\n\n\nEducational Qualification: Bachelor/Master degree in Statistics/Economics/ Mathematics/ Computer Science or related disciplines with an excellent academic record or MBA from top-tier universities.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'sql', 'tensorflow', 'data science', 'scikit-learn', 'microsoft azure', 'communication and interpersonal skills', 'artificial intelligence', 'nosql', 'r', 'spark', 'gcp', 'supply chain optimization', 'hadoop', 'api', 'robotic process automation', 'big data', 'aws']",2025-06-13 06:21:23
S&C Global Network - AI - Auto & Industrial - Consultant,Accenture,3 - 6 years,Not Disclosed,['Bengaluru'],"Management Level:Ind & Func AI Decision Science Consultant\n\n\n\nLocation:Bengaluru (Bangalore), Gurugram (Gurgaon), Hyderabad, Chennai.\n\n\n\nMust-have skills:Programming languages -Python/R, Generative AI, Large Language Models (LLMs), ML libraries such as Scikit-learn, TensorFlow, Torch, Lang Chain, or OpenAI API, RAG Applications.\n\n\n\n\nGood to have skills:Big data technologies such as Spark or Hadoop,AI model explainability(XAI),bias detection and AI ethics. Familiarity with Edge AI and deploying models on embedded devices for industrial automation. Experience with Reinforcement Learning (RL) and AI-driven optimization techniques.\n\n\n\nJob\n\n\nSummary\n\nWe are looking for a Data Scientist / AI Specialist with 3-6 years of experience to join our team and work on client projects in the Automotive & Industrial sectors. This role will involve leveraging traditional Machine Learning (ML), Generative AI (GenAI), Agentic AI, and Autonomous AI Systems to drive innovation, optimize processes, and enhance decision-making in complex industrial environments.\n\nPrior experience in the Auto/Industrial industry is a plus, but we welcome candidates from any domain with a strong analytical mindset and a passion for applying AI to real-world business challenges.\n\n\n\n\nRoles & Responsibilities:\nDevelop, deploy and monitor AI/ML models in production environments & enterprise systems, including predictive analytics, anomaly detection, and process optimization for clients.\nWork with Generative AI models (e.g., GPT, Stable Diffusion, DALLE) for applications such as content generation, automated documentation, code synthesis, and intelligent assistants.\nImplement Agentic AI systems, including AI-powered automation, self-learning agents, and decision-support systems for industrial applications.\nDesign and build Autonomous AI solutions for tasks like predictive maintenance, supply chain optimization, and robotic process automation (RPA).\nWork with structured and unstructured data from various sources, including IoT sensors, manufacturing logs, and customer interactions.\nOptimize and fine-tune LLMs (Large Language Models) for specific business applications, ensuring ethical and explainable AI use.\nUtilize MOps and AI orchestration tools to streamline model deployment, monitoring, and retraining cycles.\nCollaborate with cross-functional teams, including engineers, business analysts, and domain experts, to align AI solutions with business objectives.\nStay updated with cutting-edge AI research in Generative AI, Autonomous AI, and Multi-Agent Systems.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n3-6 years of experience in Data Science, Machine Learning, or AI-related roles.\nProficiency in Python (preferred) or R, and experience with ML libraries such as Scikit-learn, TensorFlow, Torch, Lang Chain, or OpenAI API.\nStrong understanding of Generative AI, Large Language Models (LLMs), and their practical applications.\nHands-on experience in fine-tuning and deploying foundation models (e.g., OpenAI, Llama, Claude, Gemini, etc.).\nExperience with Vector Databases (e.g., FAISS, Chroma, Weaviate, Pinecone) for retrieval-augmented generation (RAG) applications.\nKnowledge of Autonomous AI Agents (e.g., AutoGPT, BabyAGI) and multi-agent orchestration frameworks.\nExperience working with SQL and NoSQL databases.\nFamiliarity with cloud platforms (AWS, Azure, or GCP) for AI/ML model deployment.\nStrong problem-solving and analytical thinking abilities.\nAbility to communicate complex AI concepts to technical and non-technical stakeholders.\nBonus:Experience in Automotive, Industrial, or Manufacturing AI applications (e.g., predictive maintenance, quality inspection, digital twins).\n\n\n\n\nAdditional Information:\nBachelor/Masters degree in Statistics/Economics/ Mathematics/ Computer Science or related disciplines with an excellent academic record /MBA from top-tier universities.\nExcellent Communication and Interpersonal Skills.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:Minimum 3-6 years of relevant Data Science, Machine Learning or AI-related roles., Exposure to Industrial & Automotive Firms or Professional Services.\n\n\n\n\nEducational Qualification: Bachelor/Master degree in Statistics/Economics/ Mathematics/ Computer Science or related disciplines with an excellent academic record or MBA from top-tier universities.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'sql', 'tensorflow', 'data science', 'scikit-learn', 'microsoft azure', 'communication and interpersonal skills', 'artificial intelligence', 'nosql', 'r', 'spark', 'gcp', 'supply chain optimization', 'hadoop', 'api', 'robotic process automation', 'big data', 'aws']",2025-06-13 06:21:25
S&C Global Network - AI - Healthcare Analytics - Senior Analyst,Accenture,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nS&C Global Network - AI - Healthcare Analytics - Senior Analyst\n\n\n\nManagement Level:\n\n\n\n10-Senior Analyst\n\n\n\nLocation:\n\n\n\nGurgaon/Bangalore/Mumbai\n\n\n\nMust-have skills:Phython, Spark,SQL, Tableau, Power BI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nConduct data wrangling and analysis on healthcare claims, provider datasets, and publicly available health data.\nDevelop predictive models using data science and AI techniques to address client needs.\nUtilize natural language processing (NLP) capabilities to extract insights from unstructured data sources.\nCollaborate with cross-functional teams to implement analytics solutions effectively.\nTranslate complex data findings into clear, concise, and actionable strategies.\n\n\n\nWhat you would do in this role\nWork with Managers to get Client's business requirements and deliver Analytics driven solution.\nDuties and Responsibilities\nSr. Data Scientist responsible for generating actionable recommendations well-supported by quantitative analysis to help our clients address their ongoing problems.\nPresent analytic findings & opportunities for improvement to senior management and summarize key findings, and aid in the dissemination of metrics throughout the organization.\nBuild knowledge base and disseminate information on applications of variety of analytical techniques.\nDevelop statistical models and delivery of analytic offerings and solutions in health domain areas.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nExperience in resolving complex data issues in creative and effective ways.\nStrong people management skills both at client site and an offshore environment\nExcellent communication skills and ability to interact with all levels of end users, stakeholders, and technical resources.\nAdept with using Statistical (like forecasting/modeling, optimization models), Machine Learning (GBM, Decision Trees etc.), AI techniques (Deep Learning)\nGood exposure to consulting experience/basic understanding of business problems and suggesting solutions.\n\n\nTechnical\n\n\n\n\nSkills:\nProficient in data handling suites PYTHON, Spark, SQL, or similar packages\nExcellent written and oral communication skills with ability to clearly communicate ideas and results to non-technical businesspeople.\nStrong aptitude, ability, motivation, and interest in placing quantitative analysis in the context of health care business for providers / payers/Public health systems\nExperience working with cloud providers (e.g. AWS, Azure, GCP)\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:\n\n\n\n3-5 Years in Healthcare Analytics\n\n\n\n\nEducational Qualification:\n\n\n\nBachelor's / masters degree in computer science, statistics, applied mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'power bi', 'sql', 'tableau', 'spark', 'switching', 'advanced analytics', 'network engineering', 'data analytics', 'data analysis', 'microsoft azure', 'networking', 'machine learning', 'data science', 'gcp', 'healthcare analytics', 'network analysis', 'data handling', 'aws', 'ccna']",2025-06-13 06:21:27
S&C Global Network - AI - Supply Chain Analytics - Consultant,Accenture,5 - 6 years,Not Disclosed,['Pune'],"Job Title -\n\n\n\nS&C Global Network - AI - Supply Chain Analytics - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nBengaluru/Mumbai/Hyderabad/Gurugram\n\n\n\nMust-have skills:Supply Chain Management (SCM)\n\n\n\n\nGood to have skills:Knowledge of emerging technologies, cloud computing, and cybersecurity best practices.\n\n\n\nJob\n\n\nSummary:\n\nCandidate should have good understanding of statistics/analytical/optimization methods and approaches. The ideal candidate should be able to bring in meaningful data driven insights supporting with statistical concepts and apply the same in wider Supply Chain area. The candidate is expected to use data science skills to solve clients business problem in the supply chain area. The candidate should have functional skill set and Supply Chain domain knowledge with ability to apply data science skill set to solve supply chain problems. Additionally, the role would require contributing towards asset development initiatives.\n\n\n\n\nRoles & Responsibilities:\n\nDevelop and execute technology transformation strategies, oversee implementation projects, and optimize digital capabilities for business efficiency.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\nExperience and Education5-6 years of experience in Machine Learning and/or Optimization Techniques. Masters degree in technology or engineering or quantitative field (e.g. MSc in Statistics and Operations Research, M Tech in Industrial Engineering, Applied Math/Statistics, Computer Science) Certifications in any one or two of the areas will be an added advantage:Python, AI/ML, Optimization, Simulation, any of the cloud platforms (Azure/ GCP/ AWS). Mandatory\n\n\n\n\nSkills:\nMust haveProficiency in data modeling developed through client projects. Extensive use of data-driven techniques including exploratory data analysis and data pre-processing to solve business problems. Proficient in using Python/PySpark programming for data manipulation, data visualization, and machine learning models with good hands-on experience. Proficiency in any one of the Cloud solutions Azure, GCP or AWS Proficiency in SQL for data preparation, manipulation, and descriptive analysis Proficient in supply chain domain Excellent written and oral communication skills\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:\n\n\n\n8-10Years\n\n\n\n\nEducational Qualification:\n\n\n\nAny Degree",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'sql', 'gcp', 'aws', 'supply chain', 'data manipulation', 'microsoft azure', 'machine learning', 'data preparation', 'artificial intelligence', 'descriptive analysis', 'data modeling', 'cloud applications', 'supply chain management', 'data visualization', 'cloud computing']",2025-06-13 06:21:28
S&C Global Network - AI - Conversational AI - Senior Manager,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Job Title:S&C GN - Data Science Senior Manager\n\n\n\nLocation:Bengaluru, Gurugram, Mumbai, Kolkata, Pune, Chennai, Hyderabad\n\n\n\nMust have skills:Machine Learning, Deep Learning, Gen AI, NLP, Python, R, AI, SQL\n\n\n\n\nGood to have skills:\nExperience in Graph Neural Networks (GNNs) for complex relational data modeling.\nExpertise in LLM advancements, including fine-tuning, prompt engineering, and efficient deployment of Large Language Models.\nA track record of scientific publications or intellectual property generation demonstrating expertise in AI and data science.\n\nWe are an innovative Data Science team specializing in the Contact Center domain, dedicated to transforming customer interactions through data-driven intelligence. As part of the Marketing Analytics team, we focus on enhancing contact center operations by leveraging machine learning, natural language processing (NLP), and advanced analytics.\n\nOur work involves analyzing customer conversations, predicting user behavior, and enhancing self-service capabilities to reduce call transfers and improve satisfaction. We build conversational AI models, real-time recommendation systems, and predictive analytics solutions that drive meaningful business impact. By integrating AI-driven insights with marketing strategies, we enable proactive customer engagement, personalized experiences, and operational efficiencies across various touchpoints.\n\nIf you're passionate about applying data science to solve real-world customer experience challenges, this is the team for you!\n\n\n\n\nProfessional & Technical Skills:As a Senior Manager in our Data Science team, you will play a key role in leveraging Generative AI and Machine Learning to enhance customer experiences and optimize contact center operations. Your responsibilities will include:\nDriving the strategy and execution of AI/ML initiatives to enhance customer interactions, automate responses, and improve self-service capabilities.\nLeading the development and deployment of scalable ML solutions for intent classification, sentiment analysis, and predictive analytics.\nDesigning and implementing AI-driven customer segmentation and behavioral analysis frameworks for personalized recommendations and targeted engagement.\nOverseeing the optimization of conversational AI workflows by enhancing intent recognition, response generation, and real-time decision-making.\nSpearheading the analysis of large-scale customer interaction data to extract actionable insights and refine business strategies.\nPartnering with engineering, product, and marketing teams to integrate AI-driven solutions into enterprise platforms.\n\nThis role offers the opportunity to work on cutting-edge AI and data science techniques, driving real-world impact in the contact center space.\n\n\n\nLeadership & Business Impact:\nLeading multiple AI/ML-driven projects from conceptualization to deployment, ensuring alignment with business objectives and client needs.\nEngaging with senior stakeholders and C-suite executives to translate AI insights into actionable business strategies.\nManaging high-performing data science teams, mentoring talent, and fostering a culture of innovation and excellence.\nDriving business development efforts, including identifying new opportunities, leading client presentations, and contributing to proposals and thought leadership.\nEstablishing and strengthening partnerships with key clients, ensuring long-term collaboration and success.\nShaping the organization's AI roadmap by staying ahead of industry trends, evaluating emerging technologies, and driving continuous improvement.\n\nThis position is ideal for an experienced AI leader passionate about leveraging advanced analytics and machine learning to solve complex business problems and drive meaningful impact.\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience &\nEducational Qualification:\nPhD with 10+ years of relevant experience, OR\nMasters degree with 12+ years of experience, OR\nBachelors degree with 15+ years of experience",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['artificial intelligence', 'deep learning', 'python', 'natural language processing', 'machine learning', 'hlookup', 'neural networks', 'vlookup', 'dbms', 'mss', 'sql', 'gen', 'jcl', 'java', 'data modeling', 'gcp', 'spark', 'advanced excel', 'bigquery', 'mgw', 'macros', 'data analysis', 'pivot table', 'hlr', 'r', 'cobol']",2025-06-13 06:21:30
S&C Global Network - AI - Conversational AI - Senior Manager,Accenture,5 - 10 years,Not Disclosed,['Gurugram'],"Job Title:S&C GN - Data Science Senior Manager\n\n\n\nLocation:Bengaluru, Gurugram, Mumbai, Kolkata, Pune, Chennai, Hyderabad\n\n\n\nMust have skills:Machine Learning, Deep Learning, Gen AI, NLP, Python, R, AI, SQL\n\n\n\n\nGood to have skills:\nExperience in Graph Neural Networks (GNNs) for complex relational data modeling.\nExpertise in LLM advancements, including fine-tuning, prompt engineering, and efficient deployment of Large Language Models.\nA track record of scientific publications or intellectual property generation demonstrating expertise in AI and data science.\n\nWe are an innovative Data Science team specializing in the Contact Center domain, dedicated to transforming customer interactions through data-driven intelligence. As part of the Marketing Analytics team, we focus on enhancing contact center operations by leveraging machine learning, natural language processing (NLP), and advanced analytics.\n\nOur work involves analyzing customer conversations, predicting user behavior, and enhancing self-service capabilities to reduce call transfers and improve satisfaction. We build conversational AI models, real-time recommendation systems, and predictive analytics solutions that drive meaningful business impact. By integrating AI-driven insights with marketing strategies, we enable proactive customer engagement, personalized experiences, and operational efficiencies across various touchpoints.\n\nIf you're passionate about applying data science to solve real-world customer experience challenges, this is the team for you!\n\n\n\n\nProfessional & Technical Skills:As a Senior Manager in our Data Science team, you will play a key role in leveraging Generative AI and Machine Learning to enhance customer experiences and optimize contact center operations. Your responsibilities will include:\nDriving the strategy and execution of AI/ML initiatives to enhance customer interactions, automate responses, and improve self-service capabilities.\nLeading the development and deployment of scalable ML solutions for intent classification, sentiment analysis, and predictive analytics.\nDesigning and implementing AI-driven customer segmentation and behavioral analysis frameworks for personalized recommendations and targeted engagement.\nOverseeing the optimization of conversational AI workflows by enhancing intent recognition, response generation, and real-time decision-making.\nSpearheading the analysis of large-scale customer interaction data to extract actionable insights and refine business strategies.\nPartnering with engineering, product, and marketing teams to integrate AI-driven solutions into enterprise platforms.\n\nThis role offers the opportunity to work on cutting-edge AI and data science techniques, driving real-world impact in the contact center space.\n\n\n\nLeadership & Business Impact:\nLeading multiple AI/ML-driven projects from conceptualization to deployment, ensuring alignment with business objectives and client needs.\nEngaging with senior stakeholders and C-suite executives to translate AI insights into actionable business strategies.\nManaging high-performing data science teams, mentoring talent, and fostering a culture of innovation and excellence.\nDriving business development efforts, including identifying new opportunities, leading client presentations, and contributing to proposals and thought leadership.\nEstablishing and strengthening partnerships with key clients, ensuring long-term collaboration and success.\nShaping the organization's AI roadmap by staying ahead of industry trends, evaluating emerging technologies, and driving continuous improvement.\n\nThis position is ideal for an experienced AI leader passionate about leveraging advanced analytics and machine learning to solve complex business problems and drive meaningful impact.\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience &\nEducational Qualification:\nPhD with 10+ years of relevant experience, OR\nMasters degree with 12+ years of experience, OR\nBachelors degree with 15+ years of experience",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['artificial intelligence', 'deep learning', 'python', 'natural language processing', 'machine learning', 'hlookup', 'neural networks', 'vlookup', 'dbms', 'mss', 'sql', 'gen', 'jcl', 'java', 'data modeling', 'gcp', 'spark', 'advanced excel', 'bigquery', 'mgw', 'macros', 'data analysis', 'pivot table', 'hlr', 'r', 'cobol']",2025-06-13 06:21:32
India Market Unit - Industry X - IMT - Sr. Manager,Accenture,10 - 15 years,Not Disclosed,['Bengaluru'],"Work Location:Delhi, Mumbai|\n\n\n\nYears of experience:10+ years\n\n\n\n| Entity:India Business |\n\n\n\nLevel:Senior Manager\n\n\n\nAccenture Industry X\n\nIndustry X combines Accentures powerful digital capabilities with deep engineering, manufacturing, andinfrastructure & capital projects expertise. Working across multiple industries, we offer the broadest suite of services for digitizing R&D, engineering, construction, factory floors, and plant operations; speeding up the transformation of hardware into software-enabled products; reinventinginfrastructure & capital projects processes and resulting infrastructure; driving operational safety, sustainability and productivity at scale.\n\nUsing data and technologies such as AR/VR, cloud, AI, 5G, robotics and digital twins, we work with market leaders to build resilient and agile businesses that adapt their engineering,infrastructure & capital projects, manufacturing operations in the face of change.\n\n\n\nKey responsibilities:\nDrive client value creation through participating in Industry X(Digital Transformation) related projected deliveries as well as support in end-to-end sales lifecycle (deal origination to sales)\nConduct in-depth assessments of client manufacturing operations to identify areas ripe for Industry X interventions\nStrong understanding of digital/AI use-cases and impact in manufacturing operations\nAnalyze client needs and translate them into actionable roadmaps incorporating relevant Industry X technologies such as IoT, Big Data, Cloud, AI, and Machine Learning.\nDevelop and present compelling business cases highlighting the potential return on investment (ROI) associated with Industry X initiatives.\nManage and lead client engagements, ensuring successful project execution within budget and timelines.\nPartner with internal and external stakeholders, including technology vendors and system integrators, to deliver seamless project implementation.\nStay abreast of the latest Industry X trends and technologies, continuously expanding your knowledge base to provide clients with cutting-edge insights.\nEstablish self as a trusted advisor with the client C-suite through an in-depth understanding of the client as well as industry\nBuild networks at Accenture to be able to bring the best of Accenture to the client\n\n\n\n\nRequired experience:\nMinimum 10+ years of experience in management consulting, with a strong track record in Resources industries such as:\nAutomotive (Auto parts, Cars, Trucks)\nElectronics & Semiconductors (Integrated circuits, Printed circuit boards)\nMachinery & Equipment (Industrial machinery, Construction equipment)\nProven experience in successfully leading and managing complex client engagements within the discrete manufacturing domain.\nIn-depth understanding of Industry X concepts, technologies (e.g., Industrial IoT, Predictive Maintenance, Digital Twins), and their application in discrete manufacturing operations.\nStrong understanding of lean manufacturing principles and their integration with Industry X solutions.\nExperience working with Manufacturing Execution Systems (MES) and other relevant industry-specific software is a plus.\n\n\nQualification\n\n\n\nEducational qualifications\nBachelor's degree in Engineering, Business Administration, or a related field, with a focus in industrial engineering, manufacturing engineering, or a similar discipline.\n\n\n\n\nWhats in-store for you\n\n\n\n\nLearn and grow continuously:Build new skills, grow existing skills, develop new areas of expertise within functional, technical or industry areas of the business with Accentures unmatched 24/7 expert-curated learning boards, webinars and classroom-style training programs\n\n\n\n\nInnovate:Get access to resources that will allow you to leverage the latest technologies and bring innovation to life with the worlds most recognizable companies\n\n\n\n\nThrive and advance:Grow your career as far as your ambitions take you.\n\n\n\n\nTruly human:Bring your whole self to a company that aims to be the most diverse in the world and delivers real-time performance feedback based on your strengths, not stats",Industry Type: IT Services & Consulting,Department: Strategic & Top Management,"Employment Type: Full Time, Permanent","['management consulting', 'machine learning', 'artificial intelligence', 'lean manufacturing', 'big data', 'auto parts', 'manufacturing execution system', 'sales', 'manufacturing engineering', 'iot', 'business administration', 'manufacturing', 'agile', 'digital transformation']",2025-06-13 06:21:34
S&C Global Network - AI - Healthcare Analytics - Consultant,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nS&C Global Network - AI - Healthcare Analytics - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nGurgaon/Bangalore/Mumbai\n\n\n\nMust-have skills:Phython, Spark,SQL, Tableau, Power BI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nConduct data wrangling and analysis on healthcare claims, provider datasets, and publicly available health data.\nDevelop predictive models using data science and AI techniques to address client needs.\nUtilize natural language processing (NLP) capabilities to extract insights from unstructured data sources.\nCollaborate with cross-functional teams to implement analytics solutions effectively.\nTranslate complex data findings into clear, concise, and actionable strategies.\n\n\n\nWhat you would do in this role\nWork with Managers to get Client's business requirements and deliver Analytics driven solution.\nDuties and Responsibilities\nSr. Data Scientist responsible for generating actionable recommendations well-supported by quantitative analysis to help our clients address their ongoing problems.\nPresent analytic findings & opportunities for improvement to senior management and summarize key findings, and aid in the dissemination of metrics throughout the organization.\nBuild knowledge base and disseminate information on applications of variety of analytical techniques.\nDevelop statistical models and delivery of analytic offerings and solutions in health domain areas.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:\n\n\n\n4-8 Years in Healthcare Analytics\n\n\n\n\nEducational Qualification:\n\n\n\nBachelor's / masters degree in computer science, statistics, applied mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'power bi', 'sql', 'tableau', 'spark', 'natural language processing', 'business process optimization', 'artificial intelligence', 'strategic advisory services', 'strategic initiatives', 'stakeholder management', 'business transformation', 'healthcare analytics']",2025-06-13 06:21:36
S&C Global Network - AI - Conversational AI - Consultant,Accenture,3 - 5 years,Not Disclosed,['Gurugram'],"Job Title - S&C GN -\n\n\n\nData Science Consultant\n\n\n\nLocation:Bengaluru, Gurugram, Mumbai, Kolkata, Pune, Chennai, Hyderabad\n\n\n\nMust have skills:Machine Learning, Deep Learning, Gen AI, NLP, Python, R, AI, SQL\n\n\n\n\nGood to have skills:\nExperience in Graph Neural Networks (GNNs) for complex relational data modelling.\nExpertise in LLM Optimization, including fine-tuning, prompt engineering, and efficient deployment of Large Language Models.\nA track record of scientific publications or intellectual property generation demonstrating expertise in AI and data science.\n\nWe are an innovative Data Science team specializing in the Contact Center domain, dedicated to transforming customer interactions through data-driven intelligence. As part of the Marketing Analytics team, we focus on optimizing contact center operations by leveraging machine learning, natural language processing (NLP), and advanced analytics.\n\nOur work involves analyzing customer conversations, predicting user behavior, and enhancing self-service capabilities to reduce call transfers and improve satisfaction. We build conversational AI models, real-time recommendation systems, and predictive analytics solutions that drive meaningful business impact. By integrating AI-driven insights with marketing strategies, we enable proactive customer engagement, personalized experiences, and operational efficiencies across various touchpoints.\n\nIf you're passionate about applying data science to solve real-world customer experience challenges, this is the team for you!\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\nAs part of our Data Science team, you will play a key role in leveraging\n\n\n\nGenerative AI and\n\n\n\nMachine Learning to enhance customer experiences and optimize contact center operations. Your responsibilities will include:\n\n\n\n\nDeveloping and fine-tuning Generative AI models to improve customer interactions, automate responses, and enhance self-service capabilities.\n\n\n\n\nBuilding and deploying ML models for various use cases, including\n\n\n\nintent classification, sentiment analysis, and predictive analytics.\n\n\n\n\nCustomer segmentation and behavioral analysis to identify patterns, personalize recommendations, and drive targeted engagement strategies.\n\n\n\n\nOptimizing conversational AI workflows by improving\n\n\n\nintent recognition, response generation, and real-time decision-making.\n\n\n\n\nAnalyzing large-scale customer interaction data to extract actionable insights and refine business strategies.\n\n\n\n\nCollaborating with cross-functional teams including engineering, product, and marketing to integrate AI-driven solutions into existing platforms.\n\nThis role offers the opportunity to work on\n\n\n\ncutting-edge AI and data science techniques, driving real-world impact in the contact center space.\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience &\nEducational Qualification:\nPhD with 2+ years of relevant experience, OR.\nMasters degree with 3+ years of experience, OR\nBachelors degree with 4+ years of experience.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'natural language processing', 'machine learning', 'artificial intelligence', 'deep learning', 'neural networks', 'predictive analytics', 'sql', 'gen', 'customer segmentation', 'contact center operations', 'r', 'data modeling', 'intellectual property', 'marketing analytics']",2025-06-13 06:21:38
S&C Global Network - AI - Supply Chain Analytics-Analyst,Accenture,2 - 3 years,Not Disclosed,['Bengaluru'],"Entity:- Accenture Strategy & Consulting\n\n\n\nTeam:- Strategy & Consulting Global Network\n\n\n\nPractice:- Supply Chian Analytics\n\n\n\nTitle:- Ind & Func AI Decision Science Analyst\n\n\n\nJob location:- Bangalore/Gurgaon/Hyderabad/ Mumbai\n\n\n\nExplore an Exciting Career at Accenture\n\nDo you believe in creating an impactAre you a problem solver who enjoys working on transformative strategies for global clientsAre you passionate about being part of an inclusive, diverse, and collaborative culture\n\n\n\nThen, this is the right place for you! Welcome to a host of exciting global opportunities in Accenture Strategy and Consulting\n\nCandidate should have good understanding of statistics/time series forecasting/Optimization methods and approaches. The candidate should be able to bring in meaningful data driven insights supporting with statistical concepts and apply the same in wider Supply Chain area. The candidate is expected to use data science skills to solve clients business problem in the supply chain area. Additionally, the role would require contributing towards asset development initiatives.\n\n\n\nExperience and Education:\n2-3 years of experience in Machine Learning, Time series forecasting or Optimization Techniques.\nMasters degree in technology or engineering or quantitative field (e.g. MSc in Statistics and Operations Research, M.Tech. in Industrial Engineering, Applied Math/Statistics, Computer Science, MBA in Operations).\nCertifications in any one or two of the areas will be an added advantage:Python, AI/ML, Optimization, Simulation, any of the cloud platforms (Azure/ GCP/ AWS).\n\nQualification\n\n\n\nMandatory\n\n\n\n\nSkills:\n\n\nMust have:\nProficiency in data modeling developed through client projects.\nExtensive use of data-driven techniques including exploratory data analysis and data pre-processing to solve business problems.\nProficient in using Python/PySpark programming for data manipulation, data visualization, and machine learning models with good hands-on experience.\nProficiency in any one of the Cloud solutions Azure, GCP or AWS\nProficiency in SQL for data preparation, manipulation, and descriptive analysis\nProficient in supply chain domain\nExcellent written and oral communication skills\n\nGood to have:\nExperience on Simulation and Optimization\nVisualization packages like Tableau/ Power BI\nExposure to tools like BY/ Anaplan/ o9/ Kinaxis /SAP IBP\nExposure to Client interaction\nExposure to business platforms (o9/Kinaxis/BY)\n\nAbout Accenture:\nwww.accenture.com\n\nAbout Accenture Strategy & Consulting:\n\nAccenture Strategy shapes our clients future, combining deep business insight with the understanding of how technology will impact industry and business models. Our focus on issues such as digital disruption, redefining competitiveness, operating and business models as well as the workforce of the future helps our clients find future value and growth in a digital world. Today, digital is changing the way organizations engage with their employees, business partners, customers and communities. This is our unique differentiator. To bring this global perspective to our clients, Accenture Strategy's services include those provided by our Capability Network a distributed management consulting organization that provides management consulting and strategy expertise across the client lifecycle. Our Capability Network teams complement our in-country teams to deliver cutting-edge expertise and measurable value to clients all around the world.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'sql', 'gcp', 'aws', 'time series analysis', 'supply chain', 'data manipulation', 'microsoft azure', 'power bi', 'machine learning', 'data preparation', 'artificial intelligence', 'tableau', 'descriptive analysis', 'data modeling', 'cloud applications', 'data visualization', 'ml']",2025-06-13 06:21:40
I&F Decision Sci Practitioner Specialist,Accenture,7 - 11 years,Not Disclosed,['Bengaluru'],"Skill required: Delivery - HR Analytics\n\n\n\n\nDesignation: I&F Decision Sci Practitioner Specialist\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:7 to 11 years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nData & AIA set of tasks to provide insights about the effectiveness of HR processes, procedures and policies, help make data-driven decisions based on the information collected and help HR to move from operational to tactical or strategic partner.\n\n\n\n\nWhat are we looking for\nHR Analytics Text Analytics Python Adaptable and flexible Ability to work well in a team Commitment to quality Agility for quick learning Prioritization of workload\n\n\n\nRoles and Responsibilities: In this role you are required to do analysis and solving of moderately complex problems May create new solutions, leveraging and, where needed, adapting existing methods and procedures The person would require understanding of the strategic direction set by senior management as it relates to team goals Primary upward interaction is with direct supervisor May interact with peers and/or management levels at a client and/or within Accenture Guidance would be provided when determining methods and procedures on new assignments Decisions made by you will often impact the team in which they reside Individual would manage small teams and/or work efforts (if in an individual contributor role) at a client or within Accenture\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['python', 'service operations', 'machine learning', 'text analytics', 'hr analytics', 'data analytics', 'natural language processing', 'neural networks', 'artificial intelligence', 'sql', 'deep learning', 'data science', 'predictive modeling', 'statistical modeling', 'text mining']",2025-06-13 06:21:41
Technology Expert - Voice and Contact Centre,Nestle,9 - 14 years,Not Disclosed,['Bengaluru'],"Position Summary\nThe Technology Expert for Voice will be responsible for designing, implementing, and maintaining advanced voice communication systems based on Microsoft Teams. This global position involves collaborating with regional teams on specific voice topics, projects, and operations. The role includes troubleshooting and resolving complex technical issues, ensuring optimal performance of voice networks, and enhancing telephony infrastructure. The expert will also provide technical documentation and support Teams Voice technology, focusing on the integration and optimization of Microsoft Teams voice features and services.\nProvides deep specialist guidance and support in their specific specialties (e.g. artificial intelligence, robotic process automation, machine learning, etc., based on subject expertise)\nSupports the integration of solutions with various technologies\nSupports and influences the technical activity (design, build, testing) in significant or complex innovation initiatives, accountable to the Innovation or Product Manager for the delivery and quality of technical deliverables\nS/He should be self-motivated to find opportunities and take initiatives.\nA day in the life ....\nMakes recommendations on how to improve the effectiveness, efficiency and delivery of services through the use of (emerging) technology and technical methods and methodologies\nProvides the product team with technical expertise, advising on best technology solutions, translating the business vision into technical vision\nParticipates in the development of and/or review of standards, documentation and methods of working in the relevant area of expertise\nPerforms preliminary studies, general systems specifications and detailed systems specifications\nConsiders a broad range of options and applies sound judgment to develop solutions within their specific specialism\nEnsures solution operational readiness: defines global procedures and processes for operations in the area of expertise\nEnsures IT Customer and Consumer requirements are met and service quality maintained when introducing new products. Considers the cost effectiveness of proposed solutions\nSupports and influences the partners to drive improvements to meet the objectives.\nContinuous improvement in effectiveness and efficiency of operational support for internally or by third party managed product or platform groups within in scope\nEnsure sustainable alignment in the end-to-end service delivery in the stream with the Global model, including services delivered internally and by suppliers\nAct as single-point-of-contact (SPoC) for operational related escalations\nBuilds and maintains relationships with product team leaders to develop a clear understanding of product strategy, product value delivery planning and of business needs\nProvides timely updates and information to IT Consumers/stakeholders on platform issues and maintenance windows\nFosters collaboration from his/her operations team in project teams for small-scale business process improvement projects and/or Business Requirements\nCoach, develop and mentor the team to strive for their full potential.\nDefine and cascade SMART (specific, measurable, achievable, realistic, timely). objectives to help deliver overarching Nestle IT objectives,\nEnsure partner delivery quality services as per the contractual framework and service levels.\nWhat Will Make You Successful\nBachelor's degree in computer science, system analysis or a related study, or equivalent experience\n9+ years of experience in the relevant area of expertise\nExperience of successfully leading technical evaluations\nExperience of resolving technical issues, including those involving 3rd parties\nPreferably certification in corresponding technical fields\nExperience with effective communication at different levels in the organization and in English\nExperience working in a global environment and with virtual teams\nUnderstanding and knowledge of IT standards and controls\nExcellence in the technology domain of specialism\nUnderstanding and knowledge of system development life cycle methodologies (such as agile software development, rapid prototyping and DevOps, project management principles)\nUnderstanding and knowledge of IT standards and controls\nExcellent analytical and technical skills\nAbility to understand the financial impact of technology alternatives\nAbility to quickly comprehend the functions and capabilities of existing, new and emerging technologies that enable and drive new business designs and models\nKnowledge of financial models and budgeting",Industry Type: Consumer Electronics & Appliances,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['system development lifecycle methodologies', 'product strategy', 'DevOps', 'project management principles', 'financial models', 'rapid prototyping', 'agile software development', 'budgeting']",2025-06-13 06:21:43
Sr. Manager,Amazon,10 - 15 years,Not Disclosed,['Bengaluru'],"Gift Cards mission is to drive Amazon s flywheel by inspiring people around the globe to gift or transact with confidence. We are looking for Sr. Manager, Data science, to transform how millions of customers discover, purchase, and share the perfect gift through data science innovation at Amazon Gift Cards. This leader leads a multi-disciplinary team driving innovation and operational excellence.\n\nThis role is pivotal to our mission, combining strategic vision, hands-on leadership, and scientific expertise. You will manage a diverse team of data scientists, business intelligence and data engineers, working collaboratively to deliver high-impact scientific solutions and scalable systems to improve customer experience and drive business growth. The role focuses on identifying and solving ambiguous, high-value problems using state-of-the-art methodologies in machine learning (ML) as well as developing self-service data platforms for internal analytics.\n\n\nStrategic Leadership: Define and execute the scientific vision for Gift cards, aligning with the broader business goals. Drive cross-\norganizational initiatives, identifying opportunities to leverage data science and advanced analytics to solve critical business challenges. Partner with senior leaders to prioritize programs and align resources with strategic objectives.\nTeam Development: Hire, develop, and mentor a diverse team of scientists and engineers, fostering a culture of innovation, learning and excellence.\nScience and Innovation: Design and implement state-of-the-art ML models, analytics systems, and scalable architectures to support e-commerce. Explore novel scientific approaches and contribute to advancements in areas such as sales automation, product recommendation, pricing optimization, and customer segmentation. Stay abreast of industry developments to inspire and implement\nstate-of-the-art solutions, particularly in GenAI.\nStakeholder Engagement: Represent scientific and technical initiatives to senior leadership, articulating trade-offs and driving consensus. Collaborate across geographies and functions to increase penetration of Payment Products, and leverage data to recommend deprecations or new ways of operating. Together, improving customer experience and operational scalability.\nOperational Excellence: Develop the data foundation and metrics to enhance operational transparency and decision-making.\nManage day-to-day technical operations, ensuring quality and efficiency across data pipelines, tools, and models. Masters in Computer Science or a related field.\n10+ years experience in machine learning, AI, or a related field.\n7+ years of leading and managing science teams that deliver large impact.\nProven track record of leading and managing science teams that deliver large impact.\nExpert knowledge of machine learning algorithms and techniques.\nFamiliarity with large-scale data processing and storage systems.\nExcellent communication and interpersonal skills.\nAbility to work in a fast-paced, collaborative environment PhD in Computer Science or a related field.\nWorked with product teams for building vision/strategy for a space.\n2+ years of hands-on experience as a Senior Scientist",,,,"['Computer science', 'Stakeholder Engagement', 'customer segmentation', 'Technical operations', 'Penetration', 'Operational excellence', 'Machine learning', 'Data processing', 'Strategic leadership', 'Business intelligence']",2025-06-13 06:21:45
Senior - Internal Audit,KPMG India,2 - 5 years,Not Disclosed,['Bengaluru'],"Skills and Expertise: The consultant should have expertise in the following:\n1. Designing, developing, and deploying AI-driven solutions using Python.\n2. Developing and deploying Generative AI models for tasks such as text generation, image synthesis, or data augmentation.\n3. Designing workflows to fine-tune Large Language Models (LLMs) like GPT or BERT for specific use cases.\n4. Experience with Natural Language Processing (NLP) techniques and libraries such as spaCy, NLTK, or Hugging Face Transformers.\n5. Proficiency in computer vision techniques and libraries such as OpenCV, TensorFlow, or PyTorch.\n6. Implementing and optimizing AI algorithms for performance and scalability.\n7. Knowledge of reinforcement learning and its applications.\n8. Developing custom API endpoints for model deployment and real-time data processing. Ensuring API security and scalability for handling large volumes of requests efficiently.\n9. Integrating third-party APIs to enhance AI functionality (e.g., speech-to-text, language translation, or image recognition).\n10. Utilizing deep learning frameworks such as PyTorch or TensorFlow for building neural networks.\n11. Strong understanding of AI/ML concepts, including frameworks like scikit-learn and TensorFlow. Experience with Machine Learning libraries for advanced modeling.\n12. Experience with AI model deployment and serving using tools like Docker, Kubernetes, or cloud services (AWS, Azure, GCP).\n13. Collaborating with data scientists and analysts to integrate machine learning models into production.\n14. Hands-on experience with Python, knowledge of Data Analytics to proficiently write and optimize scripts.\n15. Writing complex SQL queries to extract, transform, and analyze data from relational/non-relational databases.\n16. Proficiency in version control tools such as Git.\n17. Knowledge of data visualization tools like Power BI for creating dashboards and reports (good to have).\n18. Exposure to ServiceNow for IT service management and automation (good to have).\n19. Familiarity with Microsoft Azure cloud services for AI and data processing (good to have).\n20. Good communication and presentation skills.\n.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer vision', 'Automation', 'Manager Internal Audit', 'Version control', 'GCP', 'Neural networks', 'Machine learning', 'Data processing', 'Natural language processing', 'Python']",2025-06-13 06:21:47
Google Cloud Platform (GCP) Architect,Tech Mahindra,9 - 14 years,Not Disclosed,['Bengaluru'],"Job Title: Google Cloud Platform (GCP) Architect - Vertex AI Generative AIResponsibilities:Design, implement, and maintain highly scalable cloud-based solutions using GCP services, with a focus on Vertex AI for building and deploying AI models\nCreate and maintain architectures leveraging GCP products such as BigQuery, Kubernetes Engine (GKE), Cloud Functions, Pub/Sub, and others\nWork with teams to define strategies for implementing machine learning (ML) workflows, optimizing model performance, and managing data pipelines\nUtilize Vertex AI to build end-to-end ML workflows, from data collection and preprocessing to model training, deployment, and monitoring\nLeverage Generative AI models and techniques to deliver cutting-edge AI solutions, particularly in content generation, data augmentation, or natural language processing\nExperience on Agentic AI FrameworksImplement best practices in deploying and scaling machine learning models in the cloud\nPartner with cross-functional teams (data scientists, engineers, product managers) to understand business requirements and deliver technical solutions\nDrive continuous improvement and innovation in cloud-native AI solutions\nSkills Experience: 10+ Years of IT experienceRequired:Deep Knowledge of GCP Ecosystem:Strong proficiency with GCP services including but not limited to:Vertex AI for end-to-end AI/ML lifecycle management\nBigQuery for large-scale data processing\nGoogle Kubernetes Engine (GKE) for container orchestration\nCloud Functions, Cloud Pub/Sub, Cloud Storage, and others\nExperience designing, deploying, and optimizing GCP-based architectures in production environments\nExpertise in Vertex AI:Experience using Vertex AI for building, training, and deploying machine learning models at scale\nProficiency with Vertex AI Workbench, Pipelines, and Model Monitoring\nFamiliarity with AutoML and custom model training on Vertex AI\nExperience with Generative AI:Hands-on experience with Generative AI techniques and models, such as GPT (Generative Pre-trained Transformer), GANs (Generative Adversarial Networks), or other advanced natural language models\nFamiliarity with applying Generative AI in real-world scenarios, such as content generation, AI-driven chatbots, synthetic data generation, or AI-enhanced user experiences\nExperience with Agentic AI:Experience of Agentic AI Framework (e\ng\nGoogle Agentspace)Strong Programming and Scripting Skills:Proficiency in Python (required for machine learning tasks), along with experience in ML libraries like TensorFlow, PyTorch, or Scikit-learn\nFamiliarity with SQL for querying data in BigQuery\nCloud Security and Governance:Strong understanding of cloud security practices (IAM roles, service accounts, encryption)\nExperience with GCP security tools (e\ng\n, Cloud Identity, Security Command Center)\nCertifications:Google Cloud Professional Cloud Architect or Google Cloud Professional Data Engineer certification\nVertex AI or AI/ML-related certifications\nEducation:Masters degree/PhD in Computer Science Data Science/AI ML or a related technical field or equivalent practical experience",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'orchestration', 'Machine learning', 'Data collection', 'Data processing', 'Natural language processing', 'Continuous improvement', 'Monitoring', 'SQL', 'Python']",2025-06-13 06:21:48
Engineering Lead python and Gen AI Mumabi Andheri West with startup,Intelli Search Services,12 - 18 years,40-70 Lacs P.A.,['Mumbai (All Areas)'],"Job Title: Engineering Lead\nLocation: Mumbai Andheri West\n\nJob Overview:\nAs the Engineering Lead, you will be responsible for leading the end-to-end development of our Generative AI products, ensuring scalability, performance, and innovation. You will drive engineering excellence, collaborate with cross-functional teams, and build a high-performing engineering culture. This role requires deep expertise in AI/ML, Python Coding ,distributed systems, cloud architectures, and modern software engineering practices. Needs hands on coding experience\n\nKey Responsibilities:\n1. Technical Leadership & Strategy: Define and execute the technology roadmap for Generative AI products, ensuring alignment with business goals.\n2. AI/ML Product Development: Lead the development of AI-powered products, optimizing models for performance, scalability, and real-world application.\n3. Engineering Excellence: Establish best practices in software development, DevOps, MLOps, and cloud-native architectures.\n4. Team Leadership & Scaling: Recruit, mentor, and manage a high-performing engineering team, fostering a culture of innovation and collaboration.\n5. Cross-Functional Collaboration: Work closely with Product, Data Science, and Business teams to translate AI research into real-world applications.\n6. Scalability & Performance Optimization: Architect and optimize distributed systems, ensuring efficient deployment of AI models across cloud and edge environments.\n7. Security & Compliance: Implement best practices for AI ethics, data security, and compliance with industry regulations.\n\nQualifications & Skills:\n12+ years of experience in software engineering, with at least 3 years in leadership roles within AI-driven product companies.\nStrong expertise in Generative AI, Deep Learning, NLP, Computer Vision, and model deployment.\nExperience with ML frameworks and cloud platforms (AWS, GCP, Azure).\nProven ability to scale AI/ML infrastructure and optimize models for performance and cost-efficiency.\nDeep understanding of distributed systems, cloud-native architectures, and microservices.\nHands-on experience with MLOps, CI/CD, and DevOps practices.\nStrong problem-solving, strategic thinking, and stakeholder management skills.\nAbility to attract, develop, and retain top engineering talent in a competitive market.\n\nWhy Join Us?\nLead a team at the cutting edge of Generative AI innovation.\nWork on scalable, high-impact AI products shaping the future of technology.\nOpportunity to work with a truly entrepreneurial style culture that fosters imagination, innovation and teamwork.\nOpportunity to work with a collaborative and high-calibre team.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Generative Ai', 'Artificial Intelligence', 'Machine Learning', 'Coding', 'Python', 'development', 'Devops', 'cloud', 'NLP', 'infrastructure', 'CI/CD', 'Deploying Models', 'Computer Vision']",2025-06-13 06:21:50
AI/ML,Larsen & Toubro (L&T),2 - 4 years,Not Disclosed,"['Chennai', 'Bengaluru']","Experience Required\n\n2 to 4 years of experience in AI/ML model development, deployment, and optimization. Hands-on experience in building machine learning pipelines and working with large datasets\n\nDomain Experience (Functional)\nExperience in domains such as natural language processing (NLP), computer vision, predictive analytics, or recommendation systems. Exposure to industry-specific AI applications (e.g., healthcare, finance, retail, manufacturing) is a plus.\n\nQualification\nBachelors or Masters degree in Computer Science, Artificial Intelligence, Data Science, Mathematics, or a related field\n\nRoles & Responsibilities\nDesign, develop, and deploy machine learning and deep learning models.\nCollaborate with data engineers and domain experts to collect, clean, and preprocess data.\nConduct experiments, evaluate model performance, and iterate for improvement.\nIntegrate AI models into production systems and monitor their performance.\nStay updated with the latest research and advancements in AI/ML.\nDocument model development processes and contribute to knowledge sharing.\n\nTechnical Skills\n\nProficient in Python and core ML libraries: TensorFlow, PyTorch, Scikit-learn.\nStrong with Pandas, NumPy for data handling.\nSolid grasp of ML algorithms, statistics, and model evaluation.\nFamiliar with cloud platforms (AWS/Azure/GCP).\nExperience with Git and basic CI/CD for model deployment",Industry Type: Engineering & Construction,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Npl', 'Aiml', 'Tensorflow', 'Ci/Cd', 'Machine Learning', 'Deep Learning', 'Scikit-Learn', 'Numpy', 'Pytorch', 'GCP', 'Pandas', 'Microsoft Azure', 'AWS', 'Python']",2025-06-13 06:21:52
Tcs Direct Walkin_ AI ML Engineer_Bangalore_14th Jun,Tata Consultancy Services,4 - 9 years,Not Disclosed,['Bengaluru'],Roles & Responsibilities:\n\n•Experiment and Develop: You will drive the end to end machine learning project lifecycle using best practices and well managed software delivery\n•Lead the design and development of virtual assistants using Conversational AI platforms\n•Leads efforts to foster innovative ideas for developing high impact solutions.\n•Helps in design and develop advanced analytic solutions across unctional areas as per requirement/opportunities.\n•Participate in discussions with business stakeholders to identify business challenges that can be solved with AI/ML.\n•Use LLM technologies for projects and provide technical assistance for others regarding LLM and GenAI usage.\n•Support and advise teams on best practices regarding LLM usage and,,,,"['Generative Ai', 'Artificial Intelligence', 'Natural Language Processing', 'Machine Learning', 'Python', 'Llp']",2025-06-13 06:21:53
Full Stack Performance Analyst,IBM,5 - 10 years,Not Disclosed,['Bengaluru'],"As a Full Stack Performance Analyst your responsibilities would be\n\n1. Study workloads characteristics on IBM Power and x86\n\n2. Executing & measuring performance of various PowerVM (Hypervisor) functions & features\n\n3. Using various performance tools to analyze performance & identify bottlenecks / opportunities for improving PowerVM (Hypervisor) stack/functions performance\n\n4. Provide tuning & performance optimizations suggestion to improve performance\n\n5. Working on client performance issues\n\n\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\n5-15 years of overall IT experience.\n3+ years of experience as a System Performance Analyst\n5+ Experience with OS internals, hands on debugging\nExperience doing Performance Analysis, Performance Tuning & Performance Optimization\nGood Knowledge & Experience in using Performance Monitoring Tools like vmstat, netstat, iostat, nmon, topas etc\nGood Knowledge & Experience in C/C++ programming\nGood understanding of Hypervisor & Virtualization concepts\nGood understanding of Virtual IO concepts\nGood understanding of System Architecutre\nGood understanding of Operating System concepts\nGood communication & presentation skills.\n\n\n\n\nPreferred technical and professional experience\n\n\n\n\nDemonstrated application of machine-learning or AI technologies to data analysis\nAgile/ Scrum methodology experience",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analysis', 'os internals', 'ai techniques', 'scrum', 'agile', 'load runner', 'c++', 'c', 'performance tuning', 'performance testing', 'hp performance center', 'presentation skills', 'dynatrace', 'neoload', 'nmon', 'jmeter', 'hypervisor', 'performance center', 'debugging', 'performance analysis']",2025-06-13 06:21:55
Software Developer: Python & JavaScript,IBM,2 - 7 years,Not Disclosed,['Bengaluru'],"As a Software Developer, you will be responsible for designing, developing, and deploying software solutions aimed at application modernization using Generative AI technologies. This role requires strong programming skills, problem-solving ability, and experience with AI-powered development.\nKey Responsibilities:\n\nDevelop and deploy modernized applications leveraging Generative AI technologies.\n\nWrite clean, scalable, and maintainable code using Python and JavaScript (Node.js, TypeScript); experience with FastAPI is a plus.\n\nUtilize IBM Cloud services and AI/ML technologies, particularly watsonx, for innovative software solutions.\n\nWork with developer tools such as GitHub, VSCode, and CI/CD pipelines to optimize development processes.\n\nDesign, implement, and maintain relational and NoSQL databases like Db2, PostgreSQL, and MongoDB.\n\nDevelop, manage, and optimize containerized applications using Docker, Kubernetes, and OpenShift.\n\nImplement microservices and serverless architectures for scalable and efficient application development.\n\nTroubleshoot and resolve software issues with strong analytical and problem-solving skills.\n\n\n\n\nRequired education\nBachelor's Degree\n\nRequired technical and professional expertise\n2+ years of experience in software development with expertise in application modernization.\nProficiency in Python and JavaScript (Node.js, TypeScript) frameworks; FastAPI experience is beneficial.\nStrong understanding of IBM Cloud services and AI/ML technologies, including watsonx.\nHands-on experience with DevOps tools like GitHub, VSCode, and CI/CD pipelines.\nSolid knowledge of database management (Db2, PostgreSQL, MongoDB).\nPractical experience working with Docker, Kubernetes, and OpenShift for containerization.\nKnowledge of microservices and serverless architectures for scalable solutions.\nStrong analytical and problem-solving capabilities, with a strategic approach to software development.\n\n\nPreferred technical and professional experience\nExperience working with FastAPI for lightweight application development.\nFamiliarity with security best practices in cloud-based AI applications.\nUnderstanding of AI-driven automation and optimization techniques.\nAbility to design and maintain secure and high-performance distributed systems.\nStrong collaboration skills for cross-functional team projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'cloud services', 'artificial intelligence', 'javascript', 'database management', 'continuous integration', 'kubernetes', 'github', 'software development', 'ibm cloud', 'openshift', 'ci/cd', 'docker', 'nosql', 'microservices', 'node.js', 'postgresql', 'devops', 'typescript', 'mongodb']",2025-06-13 06:21:57
Software Development Manager - OCP on IBM Z,IBM,10 - 15 years,Not Disclosed,['Bengaluru'],"Join theIBM Systems Development Lab, a global center of innovation and collaboration, where we develop industry-leading technologies that power the future of enterprise computing. As part of one of IBM’s most diverse portfolios of hardware and software technologies, you’ll lead a team that is shaping the future of Linux on IBM Z (s390x) systems—driving automation, modernization, and seamless integration with hybrid cloud and AI platforms.\n\nWe are seeking ahighly motivated and experienced Engineering Managerto lead the development of innovative solutions that simplify the installation, configuration, and testing of Linux on Z systems. This role combines technical leadership, people management, and strategic delivery to ensure the success of a critical product in IBM’s hybrid cloud ecosystem.\nKey ResponsibilitiesTeam Leadership & People Management\nManage, mentor, and support a team of software developers, fostering a culture of growth, collaboration, and innovation.\nConduct regular 1:1s and career development conversations to support individual growth and team engagement.\nPromote a positive, inclusive, and high-performance team environment.\nOversee performance management, including goal setting, feedback, and performance reviews.\nDrive employee engagement, skill development, and retention of top engineering talent.\nProject & Product Delivery\nCollaborate with product managers and stakeholders to define project scope, requirements, and timelines.\nTranslate business requirements into technical epics and stories, and drive end-to-end development.\nEnsure timely delivery of high-quality software through effective planning, prioritization, and execution.\nCoordinate with QA, DevOps, and other cross-functional teams to ensure smooth delivery and validation.\nProactively manage product releases by identifying and mitigating risks and removing obstacles.\nTechnical Leadership\nAlign OpenShift Container Platform (OCP) development initiatives with enterprise architecture and business goals.\nProvide technical guidance on architecture, design, and implementation.\nEnsure adherence to coding standards, security protocols, and development methodologies.\nParticipate in code reviews, design discussions, and technical decision-making.\nStay current with emerging technologies and industry trends to inform technical strategy.\nCustomer & Stakeholder Engagement\nAct as a point of contact for technical escalations and customer issues.\nEnsure alignment between engineering efforts and business goals.\nCommunicate project status, risks, and outcomes to stakeholders clearly and effectively.\nCollaborate with Red Hat, open-source communities, and global IBM teams to drive innovation and integration.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n10+ years of experience in IT, with a focus on systems product R&D and cloud technologies\nProven experience managing technical teams, including hiring, mentoring, and performance management\nStrong problem-solving and conflict resolution skills\nDeep understanding of Linux/Unix operating systems\nHands-on experience with container technologies (Docker, Kubernetes) and Red Hat OpenShift (OCP)\nProficiency in programming languages such as Python, C++, Go, or Java\nExperience with automation and scripting (Shell, Python, Perl, Ansible)\nFamiliarity with CI/CD tools and frameworks (e.g., Jenkins, Robot)\nWorking knowledge of Agile, DevOps, and Design Thinking methodologies\nExperience with project and defect tracking tools (e.g., Jira, GitHub, RTC, RQM)\nExcellent written and verbal communication skills in English\nAbility to work effectively in global, cross-functional teams\n\n\nPreferred technical and professional experience\nFamiliarity with IBM Z (s390x) architecture\nExposure to AI/ML technologies and their integration into enterprise systems\nPassion for open-source collaboration and community engagement\nExperience aligning technical initiatives with enterprise architecture and business goals",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'conflict resolution', 'devops', 'linux', 'unix operating system', 'continuous integration', 'kubernetes', 'c++', 'github', 'redhat openshift', 'golang', 'ci/cd', 'artificial intelligence', 'docker', 'ansible', 'java', 'jenkins', 'shell scripting', 'perl', 'agile', 'unix', 'jira']",2025-06-13 06:21:59
Technical Support Professional,IBM,5 - 10 years,Not Disclosed,['Bengaluru'],"Your Role and Responsibilities\nAs a Technical Support Professional, you should have experience in a customer-facing leadership capacity. This role necessitates exceptional customer relationship management skills along with a solid technical grasp of the product/s they will support.\n\nThe Technical Support Professional is expected to adeptly manage conflicting priorities, thrive under pressure, and autonomously navigate tasks with minimal active guidance. The successful applicant should possess a comprehensive understanding of IBM support, development, and service processes and deliveries. Knowledge of other IBM business procedures and professional training in mediation or conflict resolution would be advantageous.\n\nYour primary responsibilities include:\n\nDirect Problem-Solving Experience:Previous experience in addressing client issues is valuable, along with a demonstrated ability to effectively resolve problems.\n\nStrong Communication\n\nSkills:\nAbility to communicate clearly with both internal and external clients through spoken and written channels.\n\nBusiness Networking ExperienceIn-depth experience and understanding of the IBM and/or OEM support organizations, facilitating effective networking and collaboration.\n\nExcellent Coordination, Leadership & Organizational\n\nSkills:\nExceptional coordination and organizational abilities, capable of leading diverse teams and multitasking within a team-based business network environment. Proficiency in project management is beneficial.\n\nExcellence in Client Service & Client Satisfaction:Personal commitment to pursuing client satisfaction and continuous improvement in the delivery of client problem resolution.\n\nLanguage\n\nSkills:\nProficiency in English is required, with fluency in multiple languages considered advantageous.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\nBachelor's Degree\n\nExperience5+ years\n\nBasic knowledge in Operating system administration (Windows, Linux)\n\nBasic knowledge in database administration (DB2, Oracle, MS SQL)\n\nEnglishFluent in speaking and writing\n\nAnalytical thinking, structured problem-solving techniques\n\nStrong positive customer service attitude with sensitivity to client satisfaction.\n\nMust be a self-starter, quick learner, and enjoy working in a challenging, fast paced environment.\n\nStrong analytical and troubleshooting skills, including problem recreation, analyzing logs and traces, debugging complex issues to determine a course of action and recommend solutions.\n\n\nPreferred technical and professional experience\n\nMaster's Degree in Information Technology\n\nKnowledge with OpenShift\n\nKnowledge with Apache Flink and Kafka\n\nKnowledge with Kibana\n\nKnowledge with Containerization and Kubernetes\n\nKnowledge with scripting (including Python, JavaScript)\n\nKnowledge with products of IBM's Digital Business Automation Product Family\n\nKnowledge with Process/Data Mining\n\nKnowledge with Containerization\n\nBasic knowledge of process/data mining\n\nBasic knowledge of LDAP\n\nBasic knowledge of AI technologies\n\nFluent in speaking and writing in English\n\nExperience in Technical Support is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['project management', 'client servicing', 'networking', 'troubleshooting', 'crm', 'kubernetes', 'python', 'oracle', 'data mining', 'openshift', 'dbms', 'apache flink', 'javascript', 'sql server', 'artificial intelligence', 'conflict resolution', 'technical support', 'ldap', 'kafka', 'linux', 'kibana']",2025-06-13 06:22:01
Business Analyst,Accenture,5 - 10 years,Not Disclosed,['Navi Mumbai'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Property and Casualty Insurance\n\n\n\n\nGood to have skills :Business AnalysisMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. You will assess the current state, identify customer requirements, and define the future state and/or business solution. Additionally, you will research, gather, and synthesize information to contribute to the success of the organization.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Conduct thorough analysis of business processes and systems.- Identify areas for improvement and propose solutions.- Collaborate with stakeholders to gather and document business requirements.- Create and maintain project documentation.- Assist in the development and execution of test plans.- Conduct user acceptance testing and provide feedback.- Support the implementation of new processes and systems.- Provide training and support to end-users.- Stay up-to-date with industry trends and best practices.- Assist in the development and implementation of change management strategies.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Property and Casualty Insurance, Business Analysis.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.- Strong understanding of business process modeling and analysis.- Experience in conducting stakeholder interviews and workshops.- Ability to translate business requirements into functional specifications.- Familiarity with data analysis and visualization tools.- Ability to work effectively in a cross-functional team environment.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Property and Casualty Insurance and Business Analysis.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business process modeling', 'property and casualty insurance', 'business analysis', 'machine learning algorithms', 'statistics', 'data analysis', 'bi', 'power bi', 'machine learning', 'data cleansing', 'business requirement analysis', 'tableau', 'data visualization', 'data munging']",2025-06-13 06:22:03
Product Owner,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Product Owner\n\n\n\n\n\nProject Role Description :Drives the vision for the product by being the voice of the customer, following a human-centered design approach. Shapes and manages the product roadmap and product backlog and ensures the product team consistently deliver on the clients needs and wants. Validates and tests ideas through recurrent feedback loops to ensure knowledge discovery informs timely direction changes.\n\n\n\nMust have skills :Data Analytics\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Product Owner, you will drive the vision for the product by being the voice of the customer, following a human-centered design approach. You will shape and manage the product roadmap and product backlog, ensuring the product team consistently delivers on the clients' needs and wants. You will validate and test ideas through recurrent feedback loops to ensure knowledge discovery informs timely direction changes.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Lead the product vision and strategy.- Collaborate with stakeholders to define product requirements.- Prioritize and manage the product backlog.- Facilitate communication within the product team and with external stakeholders.- Analyze market trends and competition to inform product decisions.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Analytics.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Data Analytics.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'data visualization', 'machine learning algorithms', 'statistics', 'data munging', 'data validation', 'bi', 'power bi', 'business analysis', 'product roadmap', 'user stories', 'machine learning', 'data cleansing', 'data quality', 'tableau', 'product vision', 'agile']",2025-06-13 06:22:04
S&C Global Network - AI - Software & Platform - Gen AI - Specialist,Accenture,2 - 6 years,Not Disclosed,['Bengaluru'],"About AccentureCombining unmatched experience and specialized skills across more than 40 industries,we offer Strategy and Consulting,Technology and Operations Services, and Accenture Song - all powered by the world's largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us atwww.accenture.comEntity:- Accenture Strategy & ConsultingPractice:- Strategy & Consulting Global NetworkTeam:CMT S&PTitle:- Data Science ConsultantJob location:- Hyderabad/BangaloreAbout the Team:-The team is focused on driving Data & AI based solutions for SaaS and PaaS clients for Accenture. The team collaborates actively with onsite counterparts to help identify opportunities for growth as well as drives client deliveries from offshore.Qualification\nWHATS IN IT FOR YOUAs part of our Analytics practice, you will join a worldwide network of over 20,000 smart and driven colleagues experienced in leading statistical tools, methods, and applications. From data to analytics and insights to actions, our forward-thinking consultants provide analytically informed, issue-based insights at scale to help our clients improve outcomes and achieve high performance. Accenture will continually invest in your learning and growth. You'll work with experts in SaaS & PaaS and Accenture will support you in growing your own tech stack and certifications. In Applied intelligence you will understands the importance of sound analytical decision-making, relationship of tasks to the overall project, and executes projects in the context of a business performance improvement initiative.What you would do in this role Gathering business requirements to create high level business solution framework aligning with business objectives and goals. Monitor project progress able to plan project plan, proactively identify risks, and develop mitigation strategies. Work closely with project leads, engineers, and business analysts to develop AI solutions. Develop & test AI algorithms and techniques tailored to solve specific business problems. Present and communicate solutions and project updates to internal & external stakeholders. Foster positive client relationships by ensuring alignment between project deliverables and client expectations. Adopt a clear and systematic approach to complex issues. Analyze relationships between several parts of a problem or situation. Anticipate obstacles and identify a critical path for a project Mentor and guide a team of AI professionals, cultivating a culture of innovation, collaboration, and excellence. Conduct comprehensive market research and stay updated on the latest advancements and trends in AI technologies. Foster the professional development of team members through continuous learning opportunities.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['professional development', 'performance improvement', 'artificial intelligence', 'saas', 'paas', 'service operations', 'data analysis', 'statistical tools', 'presentation skills', 'analytics data', 'teaching', 'market research', 'sql', 'curriculum development', 'r', 'client relationship']",2025-06-13 06:22:06
S&C Global Network - AI - Financial Services Analytics - Consultant,Accenture,8 - 13 years,Not Disclosed,['Mumbai'],"Entity:- Accenture Strategy & Consulting\n\n\n\nTeam:- Global Network - Data & AI\n\n\n\nPractice:- Banking & Financial Services Analytics\n\n\n\nTitle:- Consultant Level 9\n\n\n\nJob location:- Bengaluru, Gurugram, Mumbai\n\n\n\nAbout S&C - Global Network Data & AI:- Accenture Global Network - Data & AI practice help our clients grow their business in entirely new ways. Analytics enables our clients to achieve high performance through insights from data - insights that inform better decisions and strengthen customer relationships. From strategy to execution, Accenture works with organizations to develop analytic capabilities - from accessing and reporting on data to predictive modelling to outperform the competition.\n\n\n\nRole Overview:- As an experienced financial services professional, your market-leading industry, management and technology expertise will provide critical solutions that answer unparalleled strategic, operational, technology and sourcing demands. And, in doing so, you'll improve the future of the global financial services industry.\n\nAccenture serves the world's leading financial services organisations across three industry groups\nCentral Banks:Monetary authorities, reserve banks across globe\nBanking :Retail and commercial banks and diversified financial institutions.\nCapital Markets :Investment banks, broker/dealers, asset & wealth management firms, depositories, exchanges, clearing & settlement organisations.\n\n\n\nWhats in it for you\nYoull be part of a diverse, vibrant, global Accenture data science community, continually pushing the boundaries of analytical capabilities\nGet to work with top financial clients globally\nBuild new skills, grow existing skills, develop new areas of expertise within functional and technical areas of business\nGet access to resources that will allow you to leverage the latest technologiesand bring innovation to life with the worlds most recognizable companies\n\n\n\n\nWhat you would do in this role\nWork closely with clients to understand their business goals and challenges.\nDevelop data strategies aligned with business objectives and industry best practices.\nCollaborate with business stakeholders to understand analytical requirements and deliver actionable insights\nIdentification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights\nDevelop and implement predictive models to assess and manage financial risks, leveraging advanced data science techniques.\nBuild predictive models to assess and maximize Customer Lifetime Value, enabling targeted retention and acquisition strategies.\nHelp in responding to RFPs and design POVs\nWorking across client teams to develop and architect Generative AI solutions using ML and GenAI\nImplement data security measures to safeguard sensitive financial information and ensure adherence to industry standards.\n\nQualification\n\n\n\nWho we are looking for\n3-8+ years of experience in Data Scienece preferably with financial services clients\nBachelors or masters degree in a relevant field Computer Science / Statistics / Data Science / Econometrics / Economics / Engineering from reputed institute or MBA from Tier1 colleges\nProficiency in programming languages such as Python, R, Scala,pyspark\nPossess strong analytical skills to derive meaningful insights from complex data sets\nExperience with data visualization tools (Tableau, Power BI, etc.)\nKnowledge of Data Science and Machine Learning concepts and algorithms such as clustering, regression, classification, forecasting, hyperparameters optimization, NLP, computer vision, speech processing; understanding of ML model lifecycle would be an asset\nExperience in Supervisory analytics (like Network Analytics, IFRS9 / Basel, Risk Analytics, Balance of Payments Analytics, Licensing analytics, AML etc) is a plus\nStrong domain experience in one of the following:\nCentral banks (Monetary / Regulatory / Compliance / BASEL),\nCommercial Banking,\nAsset & Wealth management\nProven experience in one of data engineering, data governance, data science roles\nExperience in Generative AI or Central / Supervisory banking is a plus.\nExperience with any Cloud Technologies (MS Azure, GCP, AWS)\nFamiliarity with Deep Learning concepts & tools (H2O, TensorFlow, PyTorch, Keras, Theano, etc.) is a plus.\nExcellent communication and client-facing skills\nAbility to work independently and collaboratively in a team\nProject management skills and the ability to manage multiple tasks concurrently\nStrong written and oral communication skills\n\n\n\n\nAccenture is an equal opportunities employer and welcomes applications from all sections of society and does not discriminate on grounds of race, religion or belief, ethnic or national origin, disability, age, citizenship, marital, domestic or civil partnership status, sexual orientation, gender identity, or any other basis as protected by applicable law.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'commercial banking', 'tensorflow', 'wealth management', 'asset', 'ms azure', 'management skills', 'scala', 'investment banking', 'pyspark', 'power bi', 'machine learning', 'capital market', 'financial services', 'deep learning', 'r', 'tableau', 'gcp', 'pytorch', 'keras', 'aws']",2025-06-13 06:22:08
S&C Global Network - AI - CFO & EV - Risk Analytics - Consultant,Accenture,2 - 6 years,Not Disclosed,['Bengaluru'],"Management Level: Ind & Func AI Decision Science Consultant\n\n\n\nLocation: Gurgaon, Mumbai, Bangalore\n\n\n\nMust-have skills: Risk Analytics, Model Development, Validation, and Auditing, Performance Evaluation, Monitoring, Governance, Statistical Techniques:Linear Regression, Logistic Regression, GLM, GBM, XGBoost, Time Series (ARMA/ARIMA), Programming Languages:SAS, R, Python, Spark, Scala, Tools:Tableau, PowerBI, Regulatory Knowledge:Basel/CCAR/DFAST/CECL/IFRS9, Risk Reporting and Dashboard Solutions\n\n\n\n\nGood to have skills: Advanced Data Science Techniques, AML, Operational Risk Modelling, Cloud Platform Experience (AWS/Azure/GCP), Machine Learning Interpretability and Bias Algorithms\n\n\n\nJob\n\n\nSummary\n\nWe are seeking a highly skilled Ind & Func AI Decision Science Consultant to join the Accenture Strategy & Consulting team in the Global Network Data & AI practice. You will be responsible for risk model development, validation, and auditing activities, ensuring performance evaluation, monitoring, governance, and documentation. This role offers opportunities to work with top financial clients globally, utilizing cutting-edge technologies to drive business capabilities and foster innovation.\n\n\n\n\nRoles & Responsibilities:\n\n\n\nEngagement Execution\nWork independently/with minimal supervision in client engagements that may involve model development, validation, governance, strategy, transformation, implementation, and end-to-end delivery of risk solutions for Accentures clients.\nAbility to manage workstreams of small projects, overseeing the quality of deliverables for junior team members.\nDemonstrated ability to manage day-to-day interactions with client stakeholders.\n\n\n\nPractice Enablement\nGuide junior team members.\nSupport development of the practice by driving innovations and initiatives.\nDevelop thought leadership and disseminate information around current and emerging trends in Risk.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n2-6 years of relevant Risk Analytics experience at one or more Financial Services firms or Professional Services/Risk Advisory with significant exposure to\n\n\n\n\nCredit Risk: PD/LGD/EAD Models, CCAR/DFAST Loss Forecasting, Revenue Forecasting Models, IFRS9/CECL Loss Forecasting across Retail and Commercial portfolios.\n\n\n\n\nCredit Acquisition/Behavior: Modeling, Credit Policies, Limit Management, Acquisition Frauds, Collections Agent Matching/Channel Allocations across Retail and Commercial portfolios.\n\n\n\n\nRegulatory Capital and Economic Capital Models\n\n\n\n\nLiquidity Risk: Liquidity Models, Stress Testing Models, Basel Liquidity Reporting Standards\n\n\n\n\nAnti-Money Laundering (AML): AML Scenarios/Alerts, Network Analysis\n\n\n\n\nOperational Risk: AMA Modeling, Operational Risk Reporting\n\n\n\n\nModeling Techniques: Linear Regression, Logistic Regression, GLM, GBM, XGBoost, CatBoost, Neural Networks, Time Series (ARMA/ARIMA), ML Interpretability and Bias Algorithms\n\n\n\n\nProgramming Languages & Tools: SAS, R, Python, Spark, Scala, Tableau, QlikView, PowerBI, SAS VA\nStrong understanding of Risk functions and their application in client discussions and project implementation.\n\n\n\n\nAdditional Information:\nMasters degree in a quantitative discipline (mathematics, statistics, economics, financial engineering, operations research) or MBA from top-tier universities.\nIndustry Certifications:FRM, PRM, CFA preferred.\nExcellent Communication and Interpersonal Skills.\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 2-6 years of relevant Risk Analytics experience, Exposure to Financial Services firms or Professional Services/Risk Advisory\n\n\n\n\nEducational Qualification: Masters degree in a quantitative discipline (mathematics, statistics, economics, financial engineering, operations research) or MBA from top-tier universities, Industry certifications such as FRM, PRM, CFA preferred",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'sas', 'linear regression', 'r', 'logistic regression', 'data validation', 'statistical techniques', 'scala', 'performance evaluation', 'time series', 'glm', 'microsoft azure', 'power bi', 'auditing', 'va', 'qlikview', 'tableau', 'spark', 'gcp', 'model development', 'xgboost', 'aws', 'risk analytics']",2025-06-13 06:22:10
S&C Global Network - AI - Responsible AI - Specialist,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Entity:- Accenture Strategy & Consulting\n\n\n\nTeam:- Strategy & Consulting Global Network\n\n\n\nPractice:- Responsible AI COE\n\n\n\nTitle:- Responsible AI Specialist/ Sr. Analyst\n\n\n\nJob location:- Bangalore/Gurgaon/Pune/ Hyderabad/Chennai/Mumbai\n\nThe rapid development of AI is creating new opportunities to improve the lives of people around the world, from business to healthcare to education. As a result, it is also raising new questions about the best way to build fairness, interpretability, privacy and security into these systems.\n\nThe Data and AI revolution is changing everything. Its everywhere transforming how we work and play. Join Accenture and help transform leading companies and communities around the world.\n\nAccenture is driving these exciting changes and bringing them to life across 40 industries in more than 120 countries. The sheer scale of our capabilities and client engagements and the way we collaborate with the ecosystem, operate, and deliver value provides an unparalleled opportunity to grow and advance.\n\nAccentures S&C Global Network Data & AI team covers the range of skills, from Strategy, Data Science, Data Architecture, AI Engineering and Visual Insights. When combined with our broader Strategy and Consulting practice, we bring a unique ability to drive end to end business change through the application of Data and AI.\n\nAt the forefront of the industry, youll help make our Responsible AI vision a reality for clients looking to better serve their customers and operate always-on enterprises. Were not just focused on increasing revenues our technologies and innovations are making millions of lives easier and more comfortable. But above all, were doing this responsibly and inclusively to make sure AI technology is used equitably and in a way that is both ethically and technically sound.\n\nJoin us and become an integral part of our global Responsible AI team with the credibility, expertise and insight clients depend on. There will never be a typical day at Accenture, but thats why people love it here. You will be working with famous brands and household names no worrying about how to explain what you do to your family again!\n\nWe are looking for experienced and motivated individuals who will be a part of the\n\n\n\nResponsible AI\n\n\n\nCentre of Excellence (COE) within Accenture to join our multi-disciplinary Responsible AI team. We are committed to help people build AI products/solutions/services in a responsible and trustworthy manner.\n\n\n\nThe ideal candidate should have a strong client-facing consulting background in data science/analytics with an ability to pick up new technologies very quickly. He/she will be passionate about understanding the impact of AI systems on people and society and will have a track record in using tools to undertake assessments such as\n\n\n\nFairness/Bias, Explainability, Model Validation and Robustness, to assess model behaviour through a Responsibility lens.\n\nBeing a part of Accenture will help you grow both professionally and personally as you help shape our thinking and approaches to Responsible AI, working alongside world-class academics, industry leaders and practitioners. Responsible AI is a key strategic priority for Accenture and were looking for the very best in the field to help us meet our ambitious goals in this space.Qualification\n\n\n\nResponsibilities:\n\nAs a client-facing in Responsible AI, you will consult with Accentures clients on how todesign & develop reliable, effective user-centered AI systems in adherence to general best practices for software systems, together with practices that address responsibility considerations unique to AI & machine learning. You will also be expected to contribute to research on how AI systems can be designed holistically with fairness, interpretability, privacy, security, safety, and robustness built in by design.\n\nAs part of our team, you will:\nBe a subject matter expert on technical aspects of Responsible AI & Data\nCollaborate with colleagues to develop best practices, frameworks, tools for scalable implementation of Responsible AI in enterprises\nConduct research on Responsible AI policies, principles, issues, risk identification, risk remediation, regulatory requirements, latest trends etc.\nBring a strong conceptual understanding of Responsible AI, principles & tools with experience of using these tools in close collaboration with internal and external stakeholders/clients\nEvaluate and implement technical best practices and tools for fairness, explainability, transparency, accountability, and other relevant aspects of Responsible AI\nDevelop a clear understanding of clients business issues to adopt the best approach to implement the Responsible AI Framework\nEstablish a consistent and collaborative presence by partnering with clients to understand the wider business goals, objectives & competitive constraints\nProvide thought leadership by publishing in public forums/conferences/blogs on Responsible AI products, research or developments\nLeading diverse and well-qualified RAI team.\n\n\n\n\nSkillset :\n\n\n\n\nEducation:- PhD / Masters / Bachelors degree in Statistics / Economics / Mathematics /Computer Science / Physics or related disciplines from Premier Colleges in India or Abroad. Specialization in Data Science.\n\n\n\nMust Have\n3 10 years of Hands-on Data science experience in solving real life complex business problems\nMinimum 1 years experience in enhancing AI systems to meet Responsible AI principles - Explainability, Fairness, Accountability, etc.\nPassionate about understanding the impact of AI systems on people and society\nHands-on experience of using techniques such as data bias testing (e.g. for under-represented groups, proxy variables, recall bias, skew etc), Explainability (e.g. SHAP values, LIME), sensitivity testing, repeatability and similar to understand model limitations.\nDemonstrated experience in writing reports that summarize analysis / assessments into simple and concise actionable points\nStrong conceptual knowledge and practical experience in the Development, Validation, and Deployment of ML/AL models such as:\nSupervised Learning - regression, classification techniques\nUnsupervised Learning clustering techniques\nRecommender Systems\nReinforcement Learning\nDeep Learning Sequence models (RNN, GRU, LSTM etc.), CNN, GAN etc\nEconometric models\nExploratory Data analysis, Hypothesis testing etc.\nComfortable in ingestion of technical whitepapers, legal policies, government regulations etc. in relation into Responsible AI and work with Academic partners to convert them into practice\nAbility to learn and develop new methods, strategies and frameworks to proactively identify potential loopholes.\nComfortable with ambiguity, believe in first principles and have the skill to transform broad ideas into action plans\nExcellent written and verbal communication skills with ability to clearly communicate ideas and results to both technical and non-technical business audience, such as senior leaders\nGood time management skills to manage day-to-day work progress and ensure timely and high-quality deliverables\nSelf-motivated with ability to work independently across multiple projects and set priorities and Strong analytical bent of mind.\n\n\n\n\nGood to have\nCloud Certifications (Azure / AWS / GCP)\nKnowledge of AWS SageMaker Clarify / Azure Responsible ML and Fairlearn SDK / GCP AI Explanations\nExperience in Chatbot Analytics, Web Crawling\nExperience in MLOps tools like MLflow or Kubeflow\nKnowledge of cybersecurity, vulnerability assessment, risk remediation etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['network data', 'data science', 'chatbot', 'aws', 'web crawling', 'switching', 'eigrp', 'load balancing', 'networking', 'bgp', 'ccnp', 'f5', 'routing', 'vlan', 'mpls', 'network security', 'ccnp routing', 'network administration', 'ospf', 'stp', 'sdwan', 'firewall', 'cisco routers', 'hsrp', 'ccna']",2025-06-13 06:22:12
S&C Global Network - AI - Responsible AI - Sr Analyst,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Entity:- Accenture Strategy & Consulting\n\n\n\nTeam:- Strategy & Consulting Global Network\n\n\n\nPractice:- Responsible AI COE\n\n\n\nTitle:- Responsible AI Specialist/ Sr. Analyst\n\n\n\nJob location:- Bangalore/Gurgaon/Pune/ Hyderabad/Chennai/Mumbai\n\nThe rapid development of AI is creating new opportunities to improve the lives of people around the world, from business to healthcare to education. As a result, it is also raising new questions about the best way to build fairness, interpretability, privacy and security into these systems.\n\nThe Data and AI revolution is changing everything. Its everywhere transforming how we work and play. Join Accenture and help transform leading companies and communities around the world.\n\nAccenture is driving these exciting changes and bringing them to life across 40 industries in more than 120 countries. The sheer scale of our capabilities and client engagements and the way we collaborate with the ecosystem, operate, and deliver value provides an unparalleled opportunity to grow and advance.\n\nAccentures S&C Global Network Data & AI team covers the range of skills, from Strategy, Data Science, Data Architecture, AI Engineering and Visual Insights. When combined with our broader Strategy and Consulting practice, we bring a unique ability to drive end to end business change through the application of Data and AI.\n\nAt the forefront of the industry, youll help make our Responsible AI vision a reality for clients looking to better serve their customers and operate always-on enterprises. Were not just focused on increasing revenues our technologies and innovations are making millions of lives easier and more comfortable. But above all, were doing this responsibly and inclusively to make sure AI technology is used equitably and in a way that is both ethically and technically sound.\n\nJoin us and become an integral part of our global Responsible AI team with the credibility, expertise and insight clients depend on. There will never be a typical day at Accenture, but thats why people love it here. You will be working with famous brands and household names no worrying about how to explain what you do to your family again!\n\nWe are looking for experienced and motivated individuals who will be a part of the\n\n\n\nResponsible AI\n\n\n\nCentre of Excellence (COE) within Accenture to join our multi-disciplinary Responsible AI team. We are committed to help people build AI products/solutions/services in a responsible and trustworthy manner.\n\n\n\nThe ideal candidate should have a strong client-facing consulting background in data science/analytics with an ability to pick up new technologies very quickly. He/she will be passionate about understanding the impact of AI systems on people and society and will have a track record in using tools to undertake assessments such as\n\n\n\nFairness/Bias, Explainability, Model Validation and Robustness, to assess model behaviour through a Responsibility lens.\n\nBeing a part of Accenture will help you grow both professionally and personally as you help shape our thinking and approaches to Responsible AI, working alongside world-class academics, industry leaders and practitioners. Responsible AI is a key strategic priority for Accenture and were looking for the very best in the field to help us meet our ambitious goals in this space.\n\n\n\nResponsibilities:\n\nAs a client-facing in Responsible AI, you will consult with Accentures clients on how todesign & develop reliable, effective user-centered AI systems in adherence to general best practices for software systems, together with practices that address responsibility considerations unique to AI & machine learning. You will also be expected to contribute to research on how AI systems can be designed holistically with fairness, interpretability, privacy, security, safety, and robustness built in by design.\n\nAs part of our team, you will:\nBe a subject matter expert on technical aspects of Responsible AI & Data\nCollaborate with colleagues to develop best practices, frameworks, tools for scalable implementation of Responsible AI in enterprises\nConduct research on Responsible AI policies, principles, issues, risk identification, risk remediation, regulatory requirements, latest trends etc.\nBring a strong conceptual understanding of Responsible AI, principles & tools with experience of using these tools in close collaboration with internal and external stakeholders/clients\nEvaluate and implement technical best practices and tools for fairness, explainability, transparency, accountability, and other relevant aspects of Responsible AI\nDevelop a clear understanding of clients business issues to adopt the best approach to implement the Responsible AI Framework\nEstablish a consistent and collaborative presence by partnering with clients to understand the wider business goals, objectives & competitive constraints\nProvide thought leadership by publishing in public forums/conferences/blogs on Responsible AI products, research or developments\nLeading diverse and well-qualified RAI team.\n\n\nQualification\n\n\n\nSkillset :\n\n\n\n\nEducation:- PhD / Masters / Bachelors degree in Statistics / Economics / Mathematics /Computer Science / Physics or related disciplines from Premier Colleges in India or Abroad. Specialization in Data Science.\n\n\n\nMust Have\n3 10 years of Hands-on Data science experience in solving real life complex business problems\nMinimum 1 years experience in enhancing AI systems to meet Responsible AI principles - Explainability, Fairness, Accountability, etc.\nPassionate about understanding the impact of AI systems on people and society\nHands-on experience of using techniques such as data bias testing (e.g. for under-represented groups, proxy variables, recall bias, skew etc), Explainability (e.g. SHAP values, LIME), sensitivity testing, repeatability and similar to understand model limitations.\nDemonstrated experience in writing reports that summarize analysis / assessments into simple and concise actionable points\nStrong conceptual knowledge and practical experience in the Development, Validation, and Deployment of ML/AL models such as:\nSupervised Learning - regression, classification techniques\nUnsupervised Learning clustering techniques\nRecommender Systems\nReinforcement Learning\nDeep Learning Sequence models (RNN, GRU, LSTM etc.), CNN, GAN etc\nEconometric models\nExploratory Data analysis, Hypothesis testing etc.\nComfortable in ingestion of technical whitepapers, legal policies, government regulations etc. in relation into Responsible AI and work with Academic partners to convert them into practice\nAbility to learn and develop new methods, strategies and frameworks to proactively identify potential loopholes.\nComfortable with ambiguity, believe in first principles and have the skill to transform broad ideas into action plans\nExcellent written and verbal communication skills with ability to clearly communicate ideas and results to both technical and non-technical business audience, such as senior leaders\nGood time management skills to manage day-to-day work progress and ensure timely and high-quality deliverables\nSelf-motivated with ability to work independently across multiple projects and set priorities and Strong analytical bent of mind.\n\n\n\n\nGood to have\nCloud Certifications (Azure / AWS / GCP)\nKnowledge of AWS SageMaker Clarify / Azure Responsible ML and Fairlearn SDK / GCP AI Explanations\nExperience in Chatbot Analytics, Web Crawling\nExperience in MLOps tools like MLflow or Kubeflow\nKnowledge of cybersecurity, vulnerability assessment, risk remediation etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'chatbot', 'network analysis', 'aws', 'web crawling', 'switching', 'eigrp', 'network engineering', 'load balancing', 'networking', 'bgp', 'network data', 'ccnp', 'f5', 'routing', 'vlan', 'mpls', 'cisco', 'network security', 'network administration', 'ospf', 'sdwan', 'firewall', 'cisco routers', 'ccna']",2025-06-13 06:22:14
I&F Decision Sci Practitioner Specialist,Accenture,7 - 11 years,Not Disclosed,['Bengaluru'],"Skill required: Delivery - Sales Reporting\n\n\n\n\nDesignation: I&F Decision Sci Practitioner Specialist\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:7 to 11 years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nData & AIDesign, develop and provide reports of exports and representations of pipeline data, sales results and other relevant data points. Assess pipeline status, and sales performance, identify trends and analyze root causes.\n\n\n\n\nWhat are we looking for\nSales Operations Domain SQL Python Machine Learning Data Analytics Microsoft Excel Prioritization of workload Adaptable and flexible Ability to work well in a team Ability to handle disputes Ability to meet deadlines Team Mentor Power BI\n\n\n\nRoles and Responsibilities: In this role you are required to do analysis and solving of moderately complex problems May create new solutions, leveraging and, where needed, adapting existing methods and procedures The person would require understanding of the strategic direction set by senior management as it relates to team goals Primary upward interaction is with direct supervisor May interact with peers and/or management levels at a client and/or within Accenture Guidance would be provided when determining methods and procedures on new assignments Decisions made by you will often impact the team in which they reside Individual would manage small teams and/or work efforts (if in an individual contributor role) at a client or within Accenture\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data analytics', 'machine learning', 'sales', 'sql', 'project management', 'service operations', 'bi', 'power bi', 'data warehousing', 'business development', 'root cause analysis', 'business intelligence', 'sql server', 'sales operations', 'tableau', 'etl']",2025-06-13 06:22:15
S&C Global Network - AI - Supply Chain Analytics - Consultant,Accenture,8 - 10 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nS&C Global Network - AI - Supply Chain Analytics - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nBengaluru, BDC7C\n\n\n\nMust-have skills:Supply Chain Analytics\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\nMandatory\n\n\n\n\nSkills:\n\n\nMust have:\nProficiency in data modeling developed through client projects.\nExtensive use of data-driven techniques including exploratory data analysis and data pre-processing to solve business problems.\nProficient in using Python/PySpark programming for data manipulation, data visualization, and machine learning models with good hands-on experience.\nProficiency in any one of the Cloud solutions Azure, GCP or AWS\nProficiency in SQL for data preparation, manipulation, and descriptive analysis\nProficient in supply chain domain\nExcellent written and oral communication skills\n\nGood to have:\nExperience on Simulation and Optimization\nVisualization packages like Tableau/ Power BI\nExposure to tools like BY/ Anaplan/ o9/ Kinaxis /SAP IBP\nExposure to Client interaction\nExposure to business platforms (o9/Kinaxis/BY)\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:\n\n\n\n8-10Years\n\n\n\n\nEducational Qualification:\n\n\n\nAny Degree",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'sql', 'gcp', 'aws', 'anaplan', 'supply chain', 'data manipulation', 'microsoft azure', 'power bi', 'machine learning', 'supply', 'data preparation', 'tableau', 'descriptive analysis', 'data modeling', 'sap ibp', 'cloud applications', 'supply chain consulting', 'data visualization']",2025-06-13 06:22:17
Analytics and Modeling Senior Manager,Accenture,16 - 25 years,Not Disclosed,['Bengaluru'],"Skill required: Analytics Support - Predictive Modeling and Analytics\n\n\n\n\nDesignation: Analytics and Modeling Senior Manager\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:16 to 25 years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nAt Accenture, we believe your career is about what you want to be and who you want to be. Its about bringing your skills, your curiosity, and your best true self to your work. Here, youll match your ingenuity with the latest technology to do incredible things. Together, we can create positive, long-lasting change.We are:Sales Excellence at Accenture. We empower our people to compete, win and grow. We develop everything they need to build and mature their client portfolios, optimize their deals and enable their sales talent, all driven by sales intelligence.You are:Someone who bases stories on evidence with a knack for transforming data into insights. You re a rigorous critical thinker, testing your hypotheses. You have grit:dead ends do not deter you. You re a unique combination of proactiveness, curiosity, & business acumen. When the truth runs contrary to prevailing narratives, you arent afraid to push back. You share the how s and so whats making complex trends understandable. Your understanding of the world is iterative:you re comfortable saying I dont know. You are not a lonely rider - you re part of an Analytics community exchanging knowledge.\n\n\n\n\nWhat are we looking for\nHeres what you need:Undergraduate degree or equivalentEnglish language fluency (oral and written) A Minimum of 8 years of Analytics work experience in Sales, Pricing, Finance, or related fieldAdvanced skills in Microsoft Excel and PowerPointIntermediate skills in business analytics tools (Qlik, Tableau, or PowerBI) Experience in analytics in Sales, Pricing, Finance or a related field Experience working with senior leadership and creating executive-level briefings Experience managing and motivating a virtual team in a global environmentExtra Credit if you have:Understanding of Accenture sales process & systems, business modelExcellent oral & written communications skillsAccuracy & attention to detail; ability to prioritize own workload and manage downstream stakeholders and resourcesQuality assurance/thoroughnessAbility to work autonomously as a self-starter (see the need before it is known)Ability to work effectively in a remote, virtual, global environmentYou may also need:A home office or work area that is suitable for productive remote work, per local guidelines. This includes a safe, ergonomic workspace and a high-speed internet connection that is stable and secure.\n\n\n\nRoles and Responsibilities: The work:You will be a trusted advisor for leadership and enable them with business insights to drive profitable sales growth. Your responsibilities will include:Primary responsibilities includeLead an Analytics team that provides insights on our sales performance to stakeholdersPartner with other Sales Excellence people and with internal clients to confirm business priorities, define success criteria, and measure outcomes.Help evolve Sales Excellence tools and champion their use by leadership and account teams.\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['downstream', 'power bi', 'sales', 'tableau', 'predictive modeling', 'python', 'service operations', 'data analytics', 'qlikview development', 'business analytics', 'sales process', 'machine learning', 'qlikview', 'sql', 'assurance', 'qlik nprinting', 'data science', 'data visualization', 'pricing']",2025-06-13 06:22:19
S&C Global Network - AI - Supply Chain Analytics - Consultant,Accenture,8 - 10 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nS&C Global Network - AI - Supply Chain Analytics - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nBengaluru, BDC7C\n\n\n\nMust-have skills:Supply Chain Analytics\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\nCandidate should have good understanding of statistics/analytical/optimization methods and approaches. The ideal candidate should be able to bring in meaningful data driven insights supporting with statistical concepts and apply the same in wider Supply Chain area. The candidate is expected to use data science skills to solve clients business problem in the supply chain area. The candidate should have functional skill set and Supply Chain domain knowledge with ability to apply data science skill set to solve supply chain problems. Additionally, the role would require contributing towards asset development initiatives.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\nMandatory\n\n\n\n\nSkills:\n\n\nMust have:\nProficiency in data modeling developed through client projects.\nExtensive use of data-driven techniques including exploratory data analysis and data pre-processing to solve business problems.\nProficient in using Python/PySpark programming for data manipulation, data visualization, and machine learning models with good hands-on experience.\nProficiency in any one of the Cloud solutions Azure, GCP or AWS\nProficiency in SQL for data preparation, manipulation, and descriptive analysis\nProficient in supply chain domain\nExcellent written and oral communication skills\n\nGood to have:\nExperience on Simulation and Optimization\nVisualization packages like Tableau/ Power BI\nExposure to tools like BY/ Anaplan/ o9/ Kinaxis /SAP IBP\nExposure to Client interaction\nExposure to business platforms (o9/Kinaxis/BY)\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:\n\n\n\n8-10Years\n\n\n\n\nEducational Qualification:\n\n\n\nAny Degree",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'sql', 'gcp', 'aws', 'anaplan', 'supply chain', 'data manipulation', 'microsoft azure', 'power bi', 'machine learning', 'data preparation', 'tableau', 'descriptive analysis', 'data modeling', 'sap ibp', 'cloud applications', 'stakeholder management', 'data visualization']",2025-06-13 06:22:21
S&C Global Network - AI - CFO & EV - Risk Analytics - Manager,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Management Level:07- I&F Decision Sci Practitioner Manager\n\n\n\nLocation:Mumbai\n\n\n\nMust-have skills:Risk Analytics, Model Development, Validation, and Auditing, Performance Evaluation, Monitoring, Governance, Statistical Techniques:Linear Regression, Logistic Regression, GLM, GBM, XGBoost, CatBoost, Neural Networks, Programming Languages:SAS, R, Python, Spark, Scala, Tools:Tableau, QlikView, PowerBI, SAS VA, Regulatory Knowledge:Basel/CCAR/DFAST/CECL/IFRS9, Risk Reporting and Dashboard Solutions\n\n\n\n\nGood to have skills:Advanced Data Science Techniques, AML, Operational Risk Modelling, Cloud Platform Experience (AWS/Azure/GCP), Machine Learning Interpretability and Bias Algorithms\n\n\n\nJob\n\n\nSummary\n\nWe are seeking a highly skilled\n\n\n\nI&F Decision Sci Practitioner Manager to join the\n\n\n\nAccenture Strategy & Consulting team in the\n\n\n\nGlobal Network Data & AI practice. You will be responsible for leading risk model development, validation, and auditing activities, ensuring performance evaluation, monitoring, governance, and documentation. This role also provides opportunities to work with top financial clients globally, utilizing cutting-edge technologies to drive business capabilities and foster innovation.\n\n\n\n\nRoles & Responsibilities:\n\n\n\nEngagement Execution\nLead the team in the development, validation, governance, strategy, transformation, implementation, and end-to-end delivery of risk solutions for clients.\nManage workstreams for large and small projects, overseeing the quality of deliverables for junior team members.\nDevelop and frame Proof of Concept for key clients where applicable.\n\n\n\nPractice Enablement\nMentor, guide, and counsel analysts and consultants.\nSupport the development of the practice by driving innovations and initiatives.\nSupport efforts of sales team to identify and win potential opportunities by assisting with RFPs, RFI. Assist in designing POVs, GTM collateral.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n\n\n\n7-12 years of relevant\n\n\n\nRisk Analytics experience at one or more\n\n\n\nFinancial Services firms or\n\n\n\nProfessional Services / Risk Advisory with significant exposure to\n\n\n\n\nCredit Risk:PD/LGD/EAD Models, CCAR/DFAST Loss Forecasting, Revenue Forecasting Models, IFRS9/CECL Loss Forecasting across Retail and Commercial portfolios.\n\n\n\n\nCredit Acquisition/Behavior:Modeling, Credit Policies, Limit Management, Acquisition Frauds, Collections Agent Matching/Channel Allocations across Retail and Commercial portfolios.\n\n\n\n\nRegulatory Capital and Economic Capital Models\n\n\n\n\nLiquidity Risk:Liquidity Models, Stress Testing Models, Basel Liquidity Reporting Standards\n\n\n\n\nAnti-Money Laundering (AML):AML Scenarios/Alerts, Network Analysis\n\n\n\n\nOperational Risk:AMA Modeling, Operational Risk Reporting\n\n\n\n\nModeling Techniques:Linear Regression, Logistic Regression, GLM, GBM, XGBoost, CatBoost, Neural Networks, Time Series (ARMA/ARIMA), ML Interpretability and Bias Algorithms\n\n\n\n\nProgramming Languages & Tools:SAS, R, Python, Spark, Scala, Tableau, QlikView, PowerBI, SAS VA\nStrong understanding of\n\n\n\nRisk functions and their application in client discussions and project implementation.\n\n\n\n\n\nAdditional Information:\n\n\n\n\nMasters Degree in a quantitative discipline (mathematics, statistics, economics, financial engineering, operations research) or MBA from top-tier universities\n\n\n\n\nIndustry Certifications:FRM, PRM, CFA preferred\n\n\n\n\nExcellent Communication and Interpersonal Skills\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:Minimum 7-12 years of relevant Risk Analytics experience, Exposure to Financial Services firms or Professional Services/Risk Advisory\n\n\n\n\nEducational Qualification:Masters degree in a quantitative discipline (mathematics, statistics, economics, financial engineering, operations research) or MBA from top-tier universities, Industry certifications such as FRM, PRM, CFA preferred",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'sas', 'linear regression', 'r', 'logistic regression', 'data validation', 'statistical techniques', 'scala', 'neural networks', 'performance evaluation', 'glm', 'microsoft azure', 'power bi', 'auditing', 'qlikview', 'tableau', 'spark', 'gcp', 'model development', 'xgboost', 'aws', 'risk analytics']",2025-06-13 06:22:23
Internal Audit Infosec Senior Analyst,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Title - IT Audit Senior Analyst\n\n\n\nManagement Level:ML10\n\n\n\nLocation:Bangalore\n\n\n\nMust have skills:IT Audit experience, Understanding of Security Standards like ISO27001, PCI DSS, HIPAA, NIST 800-53\n\n\n\n\nGood to have skills:Possession of a one or more of these professional certifications (ISO27001 Lead Auditor, CISA, CISSP, CIA, CCSK, AWS Cloud Practitioner, Azure Fundamentals) is preferred.\n\n\n\nJob\n\n\nSummary:\n\n\n\n\nRoles & Responsibilities:\nParticipate in execution of the risk-based audit plan, reporting results to Accenture Leadership and the Audit Committee of the Board of Directors\nConduct a wide-ranging scope of audits with an emphasis on assessing emerging areas of risk including cyber security, artificial intelligence, cloud computing, robotic process automation, and the Internet of Things.\nThrough advisory services, work with our business partners to help them proactively identify and manage risk in new technologies, new go-to-market offerings, and critical corporate initiatives.\nShape the future of the Accenture Internal Audit through involvement in departmental initiatives that enable us to become more efficient and effective in everything we do.\nEnsure your technical skill set and business acumen stay current and relevant through participation in our robust training program.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nExperience conducting IT external and internal audits or assessments, preferably for a global organization.\nStrong IT knowledge in infrastructure technologies (networking, data centers and hosting, virtualization, cloud etc.), application development and support, and emerging technologies.\nExperience leveraging predictive models and custom analytics in audit planning and execution is preferred.\nTechnical knowledge and familiarity with control requirements in areas including ERP applications, Windows and Unix operating systems, cyber security, and vendor management.\nStrong verbal and written communication skills and proficiency with the English language.\nDemonstrated analytical thinking, teamwork, and collaboration skills.\nPossession of a relevant professional certification (CISA, CISSP, CIA, CPA, CCSK) is preferred.\nAbility to adopt flexible work hours to collaborate with global teams and travel (up to 20%).\n\n\n\n\nAdditional Information:\n\n\n\nWe Are:\n\nAccenture is helping transform leading organizations and communities around the world. Choose Accenture and make delivering innovative work part of your extraordinary career. Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. Accenture is consistently recognized onFORTUNEs 100 Best Companies to Work Forand DiversityIncs Top 50 Companies for Diversitylists.\nThe Internal Audit Department provides the Audit Committee of the Board of Directors with an independent and objective assessment of the reliability and integrity of financial and select operating information, the effectiveness and efficiency of Accenture plc and its consolidated subsidiaries (the Company) systems and internal controls, and compliance with the Companys policies and procedures. Internal Audit Services also provides advisory services designed to add value and improve the Companys operations through bringing a systematic, disciplined approach to evaluate and improve the effectiveness of risk management, controls, operations, and governance processes.\n\n\n\nYou Are:\n\nAn agile, highly-motivated, innovative thinker with a background in audit, risk, or compliance looking to join a fast-paced, global internal audit organization that has embraced transformative capabilities including advanced analytics, dynamic risk assessment processes, and automation to retain its role as a trusted advisor to the business.\n\n\n\nWhy Should I Join the Accenture Team\n\nYou are looking for an internal audit role that provides you with exposure to senior levels of leadership, enables you to work with emerging technologies, provides opportunities for international travel and flexible work arrangements (work from home), requires little to no SOX testing, and offers a competitive salary and benefits package.\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:\nMinimum 2 years experience in IT auditing, testing IT General controls and information security controls, or related technical role focusing on security compliance activities\nStrong IT knowledge in infrastructure technologies (networking, data centers and hosting, virtualization, cloud etc.), application development and support, and emerging technologies.\n\n\n\n\n\nEducational Qualification:Undergraduate degree in Computer Science, Information Systems, Accounting, Business Administration, or Finance. MBA, Masters in Engineering.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['information security', 'it audit', 'infrastructure setup', 'security controls', 'it general controls', 'risk management', 'erp', 'software testing', 'hipaa', 'internal control', 'auditing', 'accounting', 'internal audit', 'nist', 'predictive modeling', 'cisa', 'pci dss', 'audit planning', 'sox', 'unix']",2025-06-13 06:22:25
GN - SONG - MT - Gen AI Brand & Creative-Manager,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Title - Gen AI Brand and Creative - Manager - GN SONG\n\nManagement 07 - Manager\n\nLocation:Delhi/ Mumbai/Bangalore/Gurgaon/Hyderabad\n\nMust have skills:Generative AI\nGood to have skills:Creative delivery, brand transformation,\n\n\n\nJob\n\n\nSummary: The Strategy & Consulting Global Network Song (S&C GN Song) practice works with clients across their marketing, sales and services and customer engagement functions. Our services help our clients become living businesses by optimizing their marketing, sales, and customer service strategy, thereby driving cost reduction, enhancing revenue, improving customer satisfaction, and impacting front end business metrics in a positive manner.\n\nWe are seeking a strategic and creatively driven Manager with deep agency experience to lead brand and creative transformation programs powered by Generative AI. This role is ideal for a brand strategist and innovation leader with a strong foundation in brand architecture, creative operations, and content production.\n\nYou will drive the use of GenAI and other emerging technologies to design high-impact, insight-led campaigns and reimagine content supply chains at global scale. As a trusted advisor to brand, marketing, and creative leaders, you will connect brand strategy with AI-enabled execution to deliver measurable business value and build long standing relationships.\n\n\n\n\nRoles & Responsibilities:\nDrive Business by shaping creative-led GenAI transformation agendas, influencing senior stakeholders, and identifying opportunities for consultative solutioning.\nTranslate brand strategy into scalable creative operations, aligning vision with execution across channels and markets.\nLead strategic assessments of current marketing capabilities and define GenAI-led reinvention roadmaps, future operating models, future-fit capabilities and process designs.\nDesign and operationalize GenAI-enabled global-to-local content supply chains, enabling asset creation, localization, and personalization at scale.\nDevelop brand positioning frameworks, creative territories, and visual storytelling constructs across markets and segments.\nLead and mentor cross-functional teams of consultants, strategists, and technologists working on marketing reinvention.\nLead multi-format content production (video, audio, CGI, animation, static) across ATL, BTL, owned, paid, and social/community channels.\nCollaborate with media, brand, studio, and technology teams to ensure creative alignment and executional excellence.\nEstablish business value cases linking GenAI-driven creative innovation to KPIs such as engagement, efficiency, and reach.\nChampion the use of insights and intelligence to inform GenAI creative use cases and storytelling strategies.\nMentor content production teams and agency partners to ensure brand consistency and campaign impact.\nEngage clients in workshops and solution development, translating strategic narratives into actionable creative programs.\n\nProfessional & Technical\n\n\n\n\nSkills:\nMBA from a Tier 1 institute with a focus in Marketing, Strategy, or Technology.\n7-13 years of experience in creative delivery, brand transformation, or marketing operations preferably with deep agency background.\nMinimum 4 years of recent experience in GenAI, creative technology, agentic AI, Marketing OS experience, or marketing innovation with tangible portfolio impact.\nDemonstrated experience in designing or running GenAI-enabled content supply chains or creative ops platforms.\nExcellent team management skills to ensure collaborative and incremental work, growth of individuals and team success.\nHands-on experience with Agentic solutioning and architecture design, LLMs and Generative AI frameworks (OpenAI, Anthropic, etc.)\nStrong understanding of brand architecture, positioning, creative strategy, and content systems.\nExperience in managing content production across multiple formats and channels.\nKnowledge of creative technologies and GenAI platforms (e.g., Adobe Firefly, Runway, DALLE, LLMs).\nAbility to connect creative excellence with business impact through structured value case development.\nExcellent project management, communication, and cross-functional collaboration skills.\nExperience supporting client-facing workstreams and delivering solutions in consultative, fast-paced environments.\n\n\n\n\nAdditional Information: Accenture Strategy shapes our clients future, combining deep business insight with the understanding of how technology will impact industry and business models. Our focus on issues such as digital disruption, redefining competitiveness, operating and business models as well as the workforce of the future helps our clients find future value and growth in a digital world. Today, digital is changing the way organizations engage with their employees, business partners, customers and communities. This is our unique differentiator. To bring this global perspective to our clients, Accenture Strategy's services include those provided by our Global Network a distributed management consulting organization that provides management consulting and strategy expertise across the client lifecycle. Our Global Network teams complement our in-country teams to deliver cutting-edge expertise and measurable value to clients all around the world.\n\nAbout Our Company | AccentureQualification\nExperience:Minimum 7 13 years of experience is required\nEducational Qualification:MBA",Industry Type: IT Services & Consulting,Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['project management', 'team management', 'supply chain', 'brand architecture', 'creative strategy', 'digital marketing', 'artificial intelligence', 'sales', 'brand strategy', 'animation', 'marketing', 'campaigns', 'portfolio', 'atl', 'btl', 'marketing operations', 'brand positioning']",2025-06-13 06:22:26
I&F Decision Sci Practitioner Specialist,Accenture,7 - 11 years,Not Disclosed,['Bengaluru'],"Skill required: Delivery - Customer Insight & Marketing Analytics\n\n\n\n\nDesignation: I&F Decision Sci Practitioner Specialist\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:7 to 11 years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nData & AIProcess by which data from customer behavior is used to help make key business decisions via market segmentation and predictive analytics. This information is used by businesses for direct marketing, site selection, and customer relationship management.\n\n\n\n\nWhat are we looking for\nData Science Adaptable and flexible Agility for quick learning Ability to work well in a team Commitment to quality Results orientation\n\n\n\nRoles and Responsibilities: In this role you are required to do analysis and solving of moderately complex problems May create new solutions, leveraging and, where needed, adapting existing methods and procedures The person would require understanding of the strategic direction set by senior management as it relates to team goals Primary upward interaction is with direct supervisor May interact with peers and/or management levels at a client and/or within Accenture Guidance would be provided when determining methods and procedures on new assignments Decisions made by you will often impact the team in which they reside Individual would manage small teams and/or work efforts (if in an individual contributor role) at a client or within Accenture\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['service operations', 'predictive analytics', 'sql', 'data science', 'marketing analytics', 'python', 'data analysis', 'data analytics', 'power bi', 'business analysis', 'business analytics', 'machine learning', 'business intelligence', 'tableau', 'r', 'advanced excel', 'data visualization']",2025-06-13 06:22:28
Ind & Func AI Decision Science Manager,Accenture,12 - 14 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nInd & Func AI Decision Science Manager\n\n\n\nManagement Level:\n\n\n\n7-Manager\n\n\n\nLocation:\n\n\n\nBengaluru, BDC7C\n\n\n\nMust-have skills:Risk Analytics\n\n\n\n\nGood to have skills:Experience in financial modeling, valuation techniques, and deal structuring.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nAccenture CFO & EV team under Data & AI team has comprehensive suite of capabilities in Risk, Fraud, Financial crime, and Finance. Within risk realm, our focus revolves around the model development, model validation, and auditing of models. Additionally, our work extends to ongoing performance evaluation, vigilant monitoring, meticulous governance, and thorough documentation of models.\nGet to work with top financial clients globally\nAccess resources enabling you to utilize cutting-edge technologies, fostering innovation with the worlds most recognizable companies.\nAccenture will continually invest in your learning and growth and will support you in expanding your knowledge. Youll be part of a diverse and vibrant team collaborating with talented individuals from various backgrounds and disciplines continually pushing the boundaries of business capabilities, fostering an environment of innovation.\n\n\n\nWhat you would do in this role\n\nEngagement Execution\nWork independently/with minimal supervision in client engagements that may involve model development, validation, governance, strategy, transformation, implementation and end-to-end delivery of risk solutions for Accentures clients.\nAbility to manage workstream of small projects with responsibilities of managing quality of deliverables for junior team members.\nDemonstrated ability of managing day to day interactions with the Client stakeholders\n\no Practice Enablement\n\no Guide junior team members.\n\no Support development of the Practice by driving innovations, initiatives.\n\no Develop thought capital and disseminate information around current and emerging trends in Risk.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nDevelopment, validation, and audit of:\n\nCredit Risk- PD/LGD/EAD Models, CCAR/DFAST Loss Forecasting and Revenue Forecasting Models, IFRS9/CECL Loss Forecasting Models across Retail and Commercial portfolios\nCredit Acquisition/Behavior/Collections/Recovery Modeling and Strategies, Credit Policies, Limit Management, Acquisition Frauds, Collections Agent Matching/Channel Allocations across Retail and Commercial portfolios\nRegulatory Capital and Economic Capital Models\nLiquidity Risk Liquidity models, stress testing models, Basel Liquidity reporting standards\n\no Anti Money Laundering AML scenarios/alerts, Network Analysis\n\no Operational risk AMA modeling, operational risk reporting\nConceptual understanding of Basel/CCAR/DFAST/CECL/IFRS9 and other risk regulations\nExperience in conceptualizing and creating risk reporting and dashboarding solutions.\nExperience in modeling with statistical techniques such as linear regression, logistic regression, GLM, GBM, XGBoost, CatBoost, Neural Networks, Time series ARMA/ARIMA, ML interpretability and bias algorithms etc.\nPrograming Languages - SAS, R, Python, Spark, Scala etc., Tools such as Tableau, QlikView, PowerBI, SAS VA etc.\nStrong understanding of Risk function and ability to apply them in client discussions and project implementation.\n\nAcademic :\nMasters degree in a quantitative discipline mathematics, statistics, economics, financial engineering, operations research or related field or MBA from top-tier universities.\nStrong academic credentials and publications, if applicable.\nIndustry certifications such as FRM, PRM, CFA preferred.\nExcellent communication and interpersonal skills.\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:\n\n\n\n12-14Years\n\n\n\n\nEducational Qualification:\n\n\n\nAny Degree",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'scala', 'tableau', 'spark', 'risk analytics', 'risk management', 'apqp', 'sas', 'mathematics', 'power bi', 'va', 'machine learning', 'qlikview', 'sql', 'r', 'data science', 'predictive modeling', 'financial modelling', 'statistical modeling', 'ppap', 'data visualization', 'logistic regression']",2025-06-13 06:22:30
S&C Global Network - AI - CDP - Analyst,Accenture,8 - 10 years,Not Disclosed,['Gurugram'],"Job Title -\n\n\n\nS&C Global Network - AI - RTCDP - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nBengaluru\n\n\n\nMust-have skills:Web Analytics Tools\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\nTechnical\n\n\n\n\nSkills:\n\nAny CDP platforms experience e.g., Lytics CDP platform developer, or/and\nSegment CDP platform developer, or/and\nAdobe Experience Platform (Real time CDP) developer, or/and\nCustom CDP developer on any cloud\nGA4/GA360, or/and Adobe Analytics\nGoogle Tag Manager, and/or Adobe Launch, and/or any Tag Manager Tool\nGoogle Ads, DV360, Campaign Manager, Facebook Ads Manager, The Trading desk etc.\nDeep Cloud experiecne (GCP, AWS, Azure)\nAdvance level Python, SQL, Shell Scripting experience\nData Migration, DevOps, MLOps, Terraform Script programmer\n\n\n\nSoft\n\n\n\n\nSkills:\n\nStrong problem solving skills\nGood team player\nAttention to details\nGood communication skills\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\nWHATS IN IT FOR YOU\n\nAs part of our Analytics practice, you will join a worldwide network of over 20k+ smart and driven colleagues experienced in leading AI/ML/Statistical tools, methods and applications. From data to analytics and insights to actions, our forward-thinking consultants provide analytically-informed, issue-based insights at scale to help our clients improve outcomes and achieve high performance.\n\n\n\nWhat you would do in this role\n\nA Consultant/Manager for Customer Data Platforms serves as the day-to-day marketing technology point of contact and helps our clients get value out of their investment into a Customer Data Platform (CDP) by developing a strategic roadmap focused on personalized activation. You will be working with a multidisciplinary team of Solution Architects, Data Engineers, Data Scientists, and Digital Marketers.\n\n\n\nKey Duties and Responsibilities:\nBe a platform expert in one or more leading CDP solutions. Developer level expertise on Lytics, Segment, Adobe Experience Platform, Amperity, Tealium, Treasure Data etc. Including custom build CDPs\nDeep developer level expertise for real time even tracking for web analytics e.g., Google Tag Manager, Adobe Launch etc.\nProvide deep domain expertise in our clients business and broad knowledge of digital marketing together with a Marketing Strategist industry\nDeep expert level knowledge of GA360/GA4, Adobe Analytics, Google Ads, DV360, Campaign Manager, Facebook Ads Manager, The Trading desk etc.\nAssess and audit the current state of a clients marketing technology stack (MarTech) including data infrastructure, ad platforms and data security policies together with a solutions architect.\nConduct stakeholder interviews and gather business requirements\nTranslate business requirements into BRDs, CDP customer analytics use cases, structure technical solution\nPrioritize CDP use cases together with the client.\nCreate a strategic CDP roadmap focused on data driven marketing activation.\nWork with the Solution Architect to strategize, architect, and document a scalable CDP implementation, tailored to the clients needs.\nProvide hands-on support and platform training for our clients.\nData processing, data engineer and data schema/models expertise for CDPs to work on data models, unification logic etc.\nWork with Business Analysts, Data Architects, Technical Architects, DBAs to achieve project objectives - delivery dates, quality objectives etc.\nBusiness intelligence expertise for insights, actionable recommendations.\nProject management expertise for sprint planning\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:\n\n\n\n8 to 10 Years\n\n\n\n\nEducational Qualification:\n\n\n\nB.Com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['digital marketing', 'display video', 'google ads', 'campaign management', 'facebook ads manager', 'trading', 'python', 'adobe analytics', 'adobe', 'microsoft azure', 'sql', 'gcp', 'web analytics', 'devops', 'segmentation', 'web analytics tools', 'shell scripting', 'network analysis', 'aws', 'seo']",2025-06-13 06:22:31
S&C Global Network - AI - Supply Chain Analytics-Analyst,Accenture,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nS&C Global Network - AI - Supply Chain Analytics - Analyst\n\n\n\nManagement Level:\n\n\n\n11-Analyst\n\n\n\nLocation:\n\n\n\nHyderabad, HDC2A\n\n\n\nMust-have skills:Data Analytics\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\nCandidate should have good understanding of statistics/time series forecasting/Optimization methods and approaches. The candidate should be able to bring in meaningful data driven insights supporting with statistical concepts and apply the same in wider Supply Chain area. The candidate is expected to use data science skills to solve clients business problem in the supply chain area. Additionally, the role would require contributing towards asset development initiatives.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\nMandatory\n\n\n\n\nSkills:\n\n\nMust have:\nProficiency in data modeling developed through client projects.\nExtensive use of data-driven techniques including exploratory data analysis and data pre-processing to solve business problems.\nProficient in using Python/PySpark programming for data manipulation, data visualization, and machine learning models with good hands-on experience.\nProficiency in any one of the Cloud solutions Azure, GCP or AWS\nProficiency in SQL for data preparation, manipulation, and descriptive analysis\nProficient in supply chain domain\nExcellent written and oral communication skills\n\nGood to have:\nExperience on Simulation and Optimization\nVisualization packages like Tableau/ Power BI\nExposure to tools like BY/ Anaplan/ o9/ Kinaxis /SAP IBP\nExposure to Client interaction\nExposure to business platforms (o9/Kinaxis/BY)\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:\n\n\n\n3-5Years\n\n\n\n\nEducational Qualification:\n\n\n\nAny Degree",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'sql', 'gcp', 'aws', 'anaplan', 'data analytics', 'sap', 'supply chain', 'data manipulation', 'microsoft azure', 'power bi', 'machine learning', 'supply', 'data preparation', 'tableau', 'descriptive analysis', 'data modeling', 'sap ibp', 'cloud applications', 'ibp', 'data visualization']",2025-06-13 06:22:33
Solution Advisory Senior Manager (CL6) - Health Provider,Accenture,14 - 16 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nSolution Advisory Senior Manager (CL6) - Health Provider\n\n\n\nManagement Level:\n\n\n\n6-Senior Manager\n\n\n\nLocation:\n\n\n\nBengaluru, BDC10A\n\n\n\nMust-have skills:Health Services\n\n\n\n\nGood to have skills:Knowledge of emerging technologies, cloud computing, and cybersecurity best practices.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nDevelop and execute technology transformation strategies, oversee implementation projects, and optimize digital capabilities for business efficiency.\n\n\n\nWe are:\n\nAccentures Strategy & Consulting Global Network is a network of over 9,000 strategy and management consulting specialists connecting across industries and functions to support clients and partners from business development through sales to delivery. Specialty areas entail Industry Strategy, CFO & Enterprise Value, Technology Strategy & Advisory, Talent & Organization, Supply Chain & Operations, Industry X, AI, and Song.\n\nSolution and Innovation (S&I) Advisory. The S&I Advisory team is a specialty sales advisory service within Accentures Strategy & Consulting Global Network. We are specialists aligned to Accentures priority business offerings with expertise in how to go-to-market and best position Accenture to win consulting work. We are a sales lab that works closely with sales leadership, equipping them with relevant market insights, customized sales messages, and curated sales assets to originate, sell, and win.\n\n\n\nYou are:\n\nYou are an experienced Senior Consulting Leader who will seed & lead a multi-industry team focused on providing solution architecting support. You will play a critical part in shaping deals, developing strategic proposals, and preparing client-facing materials, enabling our client teams in selling Healthcare Provider consulting work.\n\n\n\nThe work:\n\nAs a Solution & Innovation Advisory Senior Manager, you provide high touch sales support to our Accenture Health Practice\n\nAccenture Health Practice:Our 4000+ practitioners across the globe, help our clients make a meaningful impact on patients lives through New Science, novel medical technologies and better collaboration.\n\nThis work will include but is not limited to the following:\n\n\n\nPlanning and Targeting\n\nSupport annual planning to outline sales targets, target clients, and enhancement of sales assets.\n\nConduct ongoing research to identify which companies to target over next 6-12 months.\n\nSupport sales / pipeline reviews with leaders.\n\nPrepare for and support first conversations with potential clients.\n\n\n\nProposals / Orals\n\nDefine proposal response requirements and best sales messaging approach.\n\nBring the latest knowledge and best of content to each opportunity (e.g., industry trends benchmarking, competitive insights, etc.).\n\nHelp shape the overall solution (approach, team, pricing, differentiators, etc.) to best fit the deal requirements.\n\nPrepare and help lead orals with innovating ways of selling to help differentiate Accenture.\n\n\n\nContent Management / Continuous Improvement\n\nSupport development of go-to-market approach / assets.\n\nMaintain global repository of sales assets (e.g., proposals, stage 0 decks credentials, etc.).\n\nConduct reviews (loss / delivery) with client teams to understand how we can improve sales and harvest deliverables.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:\n\n\n\n14-16Years\n\n\n\n\nEducational Qualification:\n\n\n\nAny Degree",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['management consulting', 'cyber security', 'business development', 'cloud computing', 'artificial intelligence', 'emerging technologies', 'project management', 'strategic initiatives', 'business transformation', 'business analysis', 'solution architecting', 'corporate strategy']",2025-06-13 06:22:35
Application Developer,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Tableau\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. You will play a crucial role in developing innovative solutions to enhance business operations and user experience.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to analyze business requirements and translate them into technical solutions.- Develop and maintain high-quality software design and architecture.- Implement best practices for software development and ensure code quality.- Conduct code reviews and provide constructive feedback to team members.- Stay updated with the latest technologies and trends in application development.Tableau Platform AdministrationTableau User Management Tableau Site AdministrationTableau Server Management\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Tableau.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.Teamplayer Problem Solving Good Communication Skills Strong understanding on Tableau BI ToolsExperience in problem solving Tableau Platform Issues when upgrading and addressing user issuesTableau Administration Good to Have Skills :Installations, Configurations, administration, Migrations, Performance tuning, load balancing - Working with Databases such as PostgreSQL, Oracle\nAdditional Information:- The candidate should have a minimum of 2 years of experience in Tableau.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['bi', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'algorithms', 'python', 'oracle', 'software development', 'performance tuning', 'software design', 'power bi', 'machine learning', 'application development', 'sql', 'data quality', 'postgresql', 'code review']",2025-06-13 06:22:37
Application Developer,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Tableau\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. You will play a crucial role in the development and implementation of various applications.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Collaborate with cross-functional teams to define, design, and ship new features.- Develop high-quality software design and architecture.- Identify, prioritize, and execute tasks in the software development life cycle.- Conduct software analysis, programming, testing, and debugging.- Create technical documentation for reference and reporting.Tableau Platform AdministrationTableau User Management Tableau Site AdministrationTableau Server Management\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Tableau.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.Teamplayer Problem Solving Good Communication Skills Strong understanding on Tableau BI ToolsExperience in problem solving Tableau Platform Issues when upgrading and addressing user issuesTableau Administration Good to Have Skills :Installations, Configurations, administration, Migrations, Performance tuning, load balancing - Working with Databases such as PostgreSQL, Oracle\nAdditional Information:- The candidate should have a minimum of 2 years of experience in Tableau.- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['bi', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'oracle', 'performance tuning', 'software testing', 'software design', 'power bi', 'machine learning', 'application development', 'software development life cycle', 'postgresql', 'software analysis', 'debugging']",2025-06-13 06:22:38
Application Developer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP ABAP Development for HANA\n\n\n\n\nGood to have skills :SAP ABAP DevelopmentMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. You will be responsible for ensuring the smooth functioning of applications and providing solutions to work-related problems. A typical day in this role involves collaborating with the team, analyzing business requirements, and developing efficient applications to meet those requirements. You will also actively participate in team discussions and contribute to providing solutions to work-related problems. This role requires a strong understanding of SAP ABAP Development for HANA and proficiency in SAP ABAP Development.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Collaborate with the team to analyze business requirements.- Design and build efficient applications to meet business process and application requirements.- Configure applications to ensure smooth functioning.- Troubleshoot and debug applications to identify and resolve issues.- Provide technical support and guidance to end-users.- Stay updated with the latest industry trends and technologies.- Continuously enhance skills and knowledge in SAP ABAP Development for HANA.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP ABAP Development for HANA, SAP ABAP Development.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in SAP ABAP Development for HANA.- This position is based at our Bengaluru office.- A 15 years full-time education is required.- Should support in the US timings (Afternoon and Night India time)\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap abap development', 'sap hana', 'machine learning algorithms', 'statistics', 'data munging', 'algorithms', 'python', 'sap', 'power bi', 'machine learning', 'application development', 'sql', 'business requirement analysis', 'data quality', 'tableau', 'troubleshooting', 'sap abap', 'abap']",2025-06-13 06:22:40
Application Developer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP ABAP Development for HANA\n\n\n\n\nGood to have skills :SAP FI CO Finance, BASISMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. Your typical day will involve collaborating with the team to understand the business needs, designing and developing applications using SAP ABAP Development for HANA, and ensuring the applications meet the required standards and specifications. You will also be responsible for troubleshooting and resolving any application issues that arise, as well as providing support and guidance to junior team members.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with the team to understand business needs and requirements.- Design and develop applications using SAP ABAP Development for HANA.- Ensure applications meet the required standards and specifications.- Troubleshoot and resolve any application issues that arise.- Provide support and guidance to junior team members.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP ABAP Development for HANA.- Good To Have\n\n\n\n\nSkills:\nExperience with SAP FI CO Finance.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in SAP ABAP Development for HANA.- This position is based at our Bengaluru office.- A 15 years full time education is required.-Should support in the Japan timings (Early Morning India time) and should have Support experience\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap abap development', 'sap hana', 'machine learning algorithms', 'statistics', 'data munging', 'fico', 'algorithms', 'python', 'sap', 'power bi', 'machine learning', 'application development', 'sql', 'data quality', 'tableau', 'sap fico', 'troubleshooting', 'data visualization', 'sap abap', 'abap']",2025-06-13 06:22:42
Business Process Architect,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Project Role :Business Process Architect\n\n\n\n\n\nProject Role Description :Analyze and design new business processes to create the documentation that guides the implementation of new processes and technologies. Partner with the business to define product requirements and use cases to meet process and functional requirements. Participate in user and task analysis to represent business needs.\n\n\n\nMust have skills :Guidewire Digital Portals\n\n\n\n\nGood to have skills :React.jsMinimum\n\n\n\n2 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Process Architect, you will analyze and design new business processes to create the documentation that guides the implementation of new processes and technologies. Partner with the business to define product requirements and use cases to meet process and functional requirements. Participate in user and task analysis to represent business needs.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Analyze and design new business processes to create documentation for implementation.- Partner with the business to define product requirements and use cases.- Participate in user and task analysis to represent business needs.- Collaborate with cross-functional teams to ensure successful process implementation.- Identify areas for process improvement and recommend solutions.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Guidewire Digital Portals.- Good To Have\n\n\n\n\nSkills:\nExperience with React.js.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 2 years of experience in Guidewire Digital Portals.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['guidewire', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'react.js', 'data quality', 'r', 'data science', 'predictive modeling', 'text mining', 'data visualization', 'logistic regression']",2025-06-13 06:22:44
Application Developer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Syniti ADM for SAP\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :SYNITI\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. You will collaborate with teams to ensure seamless integration and functionality.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead and mentor junior professionals- Conduct regular knowledge sharing sessions- Stay updated on industry trends and best practices\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Syniti ADM for SAP- Strong understanding of data migration processes- Experience in SAP data management and governance- Knowledge of SAP data models and structures- Hands-on experience in SAP data quality management\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Syniti ADM for SAP- This position is based at our Bengaluru office- A SYNITI education is required\n\nQualification\n\nSYNITI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'data management', 'data migration', 'adm', 'data quality management', 'python', 'data analysis', 'data analytics', 'data warehousing', 'business analysis', 'machine learning', 'business intelligence', 'application development', 'sql', 'plsql', 'tableau', 'data modeling', 'data visualization']",2025-06-13 06:22:46
Application Developer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Data Engineering\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will be involved in designing, building, and configuring applications to meet business process and application requirements. Your typical day will revolve around creating innovative solutions to address various business needs and ensuring seamless application functionality.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the development and implementation of data engineering solutions- Optimize and maintain data pipelines for optimal performance- Collaborate with data scientists and analysts to understand data requirements\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Engineering- Strong understanding of ETL processes- Experience with cloud-based data platforms such as AWS or Azure- Knowledge of data modeling and database design- Experience with big data technologies like Hadoop or Spark\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Data Engineering- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data engineering', 'spark', 'hadoop', 'aws', 'etl process', 'python', 'oracle', 'datastage', 'microsoft azure', 'data warehousing', 'sql server', 'application development', 'sql', 'plsql', 'database design', 'java', 'unix shell scripting', 'data modeling', 'leadership development', 'etl', 'informatica', 'unix']",2025-06-13 06:22:48
Associate Director - AI & Privacy Compliance,Flipkart,3 - 7 years,Not Disclosed,['Bengaluru'],"Skills Required :\nArtificial Intelligence, Data Privacy, Legal Research, Stakeholder Management, Techno Functional\nLocation :\nBangalore,Karnataka\nEducation/Qualification :\n10+ years of experience in AI governance, data privacy, compliance, or risk management, preferably in a technology-driven organization. Experience working in industries such as e-commerce, fintech, healthcare with exposure to AI/ML use cases\nDesirable Skills :\nArtificial Intelligence, Data Privacy, Legal Research, Stakeholder Management, Regulatory Compliance",Industry Type: Courier / Logistics,Department: Risk Management & Compliance,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Legal Research', 'Techno Functional', 'Data Privacy', 'Stakeholder Management']",2025-06-13 06:22:50
Application Developer,Accenture,15 - 20 years,Not Disclosed,['Indore'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :SAP Generative AI\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. You will be responsible for ensuring that the applications are aligned with the needs of the organization and contribute to its overall success. Your typical day will involve collaborating with cross-functional teams, analyzing business requirements, and developing innovative solutions to address complex challenges.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Collaborate with stakeholders to understand business requirements and translate them into technical specifications.- Design, develop, and test applications based on the defined requirements.- Troubleshoot and debug issues in the application code.- Ensure the applications are scalable, secure, and performant.- Conduct code reviews and provide constructive feedback to team members.- Stay updated with the latest industry trends and technologies to continuously improve the applications.- Document the application design, architecture, and technical specifications.- Collaborate with cross-functional teams to integrate applications with other systems.- Provide technical guidance and mentorship to junior team members.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Generative AI.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on experience implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in SAP Generative AI.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'algorithms', 'application design', 'power bi', 'machine learning', 'application development', 'business requirement analysis', 'data quality', 'code review', 'technical specifications', 'data visualization']",2025-06-13 06:22:51
Application Developer,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :PySpark, Apache Spark, Talend ETLMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. You will collaborate with teams, make team decisions, and provide solutions to problems. Engaging with multiple teams, you will contribute to key decisions and provide solutions for your immediate team and across multiple teams. In this role, you will have the opportunity to showcase your creativity and technical expertise in designing and building applications.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Design, build, and configure applications to meet business process and application requirements- Collaborate with cross-functional teams to gather and define application requirements- Develop and implement software solutions using the Databricks Unified Data Analytics Platform- Perform code reviews and ensure adherence to coding standards- Troubleshoot and debug applications to identify and resolve issues- Optimize application performance and ensure scalability- Document technical specifications and user manuals for applications- Stay updated with emerging technologies and industry trends- Train and mentor junior developers to enhance their technical skills\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform- Good To Have\n\n\n\n\nSkills:\nExperience with PySpark, Apache Spark, Talend ETL- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform- This position is based at our Bengaluru office- A 15 years full time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'spark', 'machine learning algorithms', 'statistics', 'data munging', 'talend', 'pyspark', 'power bi', 'machine learning', 'application development', 'data bricks', 'technical design', 'data quality', 'tableau', 'apache', 'troubleshooting', 'code review', 'etl']",2025-06-13 06:22:53
Application Developer,Accenture,5 - 10 years,Not Disclosed,['Bhubaneswar'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :PySpark, Apache Spark, Talend ETLMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. You will collaborate with teams, make team decisions, and provide solutions to problems. Engaging with multiple teams, you will contribute to key decisions and provide solutions for your immediate team and across multiple teams. In this role, you will have the opportunity to showcase your creativity and technical expertise in designing and building applications.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Design, build, and configure applications to meet business process and application requirements- Collaborate with cross-functional teams to gather and define application requirements- Develop and implement software solutions using the Databricks Unified Data Analytics Platform- Perform code reviews and ensure adherence to coding standards- Troubleshoot and debug applications to identify and resolve issues- Optimize application performance and ensure scalability- Document technical specifications and user manuals for applications- Stay updated with emerging technologies and industry trends- Train and mentor junior developers to enhance their technical skills\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform- Good To Have\n\n\n\n\nSkills:\nExperience with PySpark, Apache Spark, Talend ETL- Strong understanding of statistical analysis and machine learning algorithms- Experience with data visualization tools such as Tableau or Power BI- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform- This position is based at our Bengaluru office- A 15 years full time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'spark', 'machine learning algorithms', 'statistics', 'data munging', 'talend', 'pyspark', 'power bi', 'machine learning', 'application development', 'data bricks', 'technical design', 'data quality', 'tableau', 'apache', 'troubleshooting', 'code review', 'etl']",2025-06-13 06:22:55
Application Developer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Java Full Stack Development\n\n\n\n\nGood to have skills :Spring BootMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. You will be responsible for creating efficient and scalable applications that align with the organization's goals and objectives. Your typical day will involve collaborating with cross-functional teams, analyzing business requirements, and developing innovative solutions to meet customer needs. You will also be involved in troubleshooting and resolving application issues to ensure smooth operations and user satisfaction.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Develop and maintain high-quality software code for applications.- Participate in the entire application lifecycle, from concept and design to testing and deployment.- Collaborate with cross-functional teams to gather and analyze business requirements.- Design and implement efficient and scalable solutions that meet business needs.- Troubleshoot and resolve application issues to ensure smooth operations and user satisfaction.\nProfessional & Technical\n\n\n\n\nSkills:\nMust have --Experience with REST concepts-Experience with XML and JSON data formats-Experience of large-team development in integrated environments (eg:Intellij )Good To Have\n\n\n\n\nSkills:\n-Experience with Test Driven Development (TDD) and unit testing frameworks-Agile program experience with continuous delivery approach-Microservices Experience with Java Full Stack Development- Strong understanding of statistical analysis and machine learning algorithms\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Java Full Stack Development.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['rest', 'java', 'full stack', 'machine learning algorithms', 'statistics', 'algorithms', 'unit testing', 'machine learning', 'microservices', 'intellij idea', 'application development', 'spring', 'continuous delivery', 'business requirement analysis', 'tdd', 'xml', 'json', 'troubleshooting', 'agile']",2025-06-13 06:22:57
Technology Educator,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Technology Educator\n\n\n\n\n\nProject Role Description :Instrumental in keeping technology talent market relevant, by upskilling and cross skilling them. Primarily responsible for delivering foundational training in technology, delivery, professional development and industry content. Contribute to course content development and creation of questions for certification and assessments.\n\n\n\nMust have skills :Machine Learning\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Technology Educator, you will play a crucial role in keeping technology talent market relevant by upskilling and cross-skilling them. Your responsibilities will include delivering foundational training in technology, delivery, professional development, and industry content, as well as contributing to course content development and creating questions for certification and assessments.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Develop innovative training methodologies.- Conduct assessments and evaluations.- Stay updated with the latest industry trends.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Machine Learning.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Machine Learning.- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Teaching & Training,"Employment Type: Full Time, Permanent","['machine learning', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'professional development', 'sql', 'assessment', 'content development', 'data science', 'predictive modeling', 'text mining', 'data visualization']",2025-06-13 06:22:58
Analytics Manager,Flipkart,8 - 12 years,Not Disclosed,['Bengaluru'],"Skills Required :\nStrong background in statistical modelling and experience with machine learning / data mining tools such as R, Python, SQL, Spark, SAS, Excel. High expertise in implementing machine learning and AI models\nEducation/Qualification :\nBachelors in Engineering, Computer Science, Math, Statistics, or related discipline from a reputed institute or an MBA from a reputed institute\nDesirable Skills :",,,,"['Analytics', 'R', 'Excel', 'Power BI', 'SAS', 'Qlikview', 'Tableau', 'Datastudio', 'Spark', 'Python', 'SQL']",2025-06-13 06:23:00
Eqs Automation SPG Strats - Associate,Goldman Sachs,3 - 8 years,Not Disclosed,['Bengaluru'],"We are a team of desk strategists who work to transform the SPG business within Equities by automating the key decisions taken every day. Our team has a wide remit including automatic quoting, optimizing hedging decisions and developing algorithms to trade cash and derivatives on venues around the world. We also deploy statistical analysis techniques and mathematical models to enhance the decision making process, with the overall aim of improving business performance while working closely with traders and sales people on the trading floor.\n  Role Responsibilities\nAutomate pricing of derivative products, providing fast and accurate prices in response to quote requests from our clients.\nImplement automated hedging algorithms, and build frameworks to manage risk centrally across asset classes.\nPerform systematic and quantitative analysis of franchise flows and market data, driving business decisions and the design of our automation platform.\nWork closely with sales and trading, support our automated pricing and trading systems.\nBe involved with all stages of the software development life cycle with a range of technologies, and collaborate closely with engineering teams who support the underlying infrastructure and frameworks,\nBasic Qualifications\nExcellent academic record in a relevant quantitative field such as physics, mathematics, statistics, engineering or computer science.\nStrong programming skills in an object oriented or functional paradigm such as C++, Java or Python.\nAt least 3 years experience.\nSelf-starter with strong self-management skills, ability to manage multiple priorities and work in a high-pressure environment.\nExcellent written and verbal communication skills.\nPreferred Qualifications\nPrevious quantitative or technical role working on or with a trading desk (irrespective of asset class).\nKnowledge of building automated trading systems in a live trading environment.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'C++', 'Analytical', 'Machine learning', 'market data', 'Investment banking', 'Investment management', 'Financial services', 'Python']",2025-06-13 06:23:02
AI Product Specialist,Capco,3 - 5 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","We are seeking a detail-oriented and strategic AI Product Specialist to support the implementation and governance of AI solutions within our HR ecosystem, primarily leveraging SAP SuccessFactors. This role is instrumental in coordinating cross-functional teams, overseeing AI feature testing, ensuring responsible AI practices, and tracking performance metrics to support data-driven HR decisions.\nKey Responsibilities:\nAI Implementation Integration:\nLead and support the rollout of AI-driven features and functionalities within SAP SuccessFactors including virtual agents, predictive analytics, and intelligent recommendations.\nCollaborate with HR, IT, and vendor teams to integrate AI capabilities aligned with business needs.\nEvaluate and advise on agentic AI capabilities and their potential integration into HR workflows.\nResponsible AI Governance\nEnsure compliance with internal and external AI governance standards (e.g., Responsible AI frameworks, country compliance).\nReview and document RAI risks, mitigation actions, and audit trails throughout the development lifecycle.\nTesting Quality Assurance\nDesign and oversee test cases for AI features, including validation of outcomes across diverse user scenarios.\nMonitor for bias, accuracy, performance, and user acceptance of AI outputs.\nCross-functional Coordination\nCollaborate with product managers, data scientists, HR stakeholders and compliance teams to ensure successful end to end implementation.\nManage communication between stakeholders to ensure AI product milestones are met.\nMetrics Reporting\nDefine and track KPIs to measure the performance and adoption of AI features.\nSupport the development and visualization of AI dashboards to provide insights on usage, fairness, and efficiency. Continuous Improvement\nServe as a liaison between business stakeholders and technical team to ensure AI implementations meet both user expectations and regulatory requirements.\nGather user feedback and partner with vendors to enhance AI functionality and user experience.\nStay current on emerging AI trends in HR tech and recommend innovations that align with business goals.",Industry Type: Banking,Department: Consulting,"Employment Type: Full Time, Permanent","['Usage', 'SAP', 'Manager Quality Assurance', 'Management consulting', 'Cross functional coordination', 'HR', 'Test cases', 'Continuous improvement', 'Financial services', 'Auditing']",2025-06-13 06:23:03
Embedded Software Architect,Thales,8 - 15 years,Not Disclosed,['Bengaluru'],"Location: Bangalore, India\nIn fast changing markets, customers worldwide rely on Thales. Thales is a business where brilliant people from all over the world come together to share ideas and inspire each other. In aerospace, transportation, defence, security and space, our architects design innovative solutions that make our tomorrows possible.\nTechnical skills - essential:\nExperience- 8yrs-15yrs\nSolid hands-on experience of Linux internals and device drivers\nSolid hands-on knowledge of porting Linux based RTOSes on ARM / RISCV architectures\nGood understanding of hypervisors / hardware virtualization\nExperienced in technical documentation as per industrial standards (any specification like DO, ISO specifications should be ok\nExcellent programming skills in C, C++, python, with good knowledge of data structures, pointers thread and memory management\nSolid understanding of programming safety levels - thread safety, memory safety.\nUnderstanding of machine learning concepts, familiarity with AI tools, prompts etc.\nResponsibilities:\nAs a senior technical lead, you will be responsible for defining new directions in the development of full stacks targeted for ARM / RISCV architectures.\nAlong with the individual contributions on fostering growth of the embedded software domain, you will mentor on-going projects and help the team on technical troubleshooting and arriving at risk mitigation plans for existing deliverables or milestones.\nYou will present the software activities of the team at important forums within Thales and outside and often collaborate with the Head, Thales Research and Technology India in strategy build-up for the team on this domain.\nYou will have to act as a solid interface for the team in front of Thales engineering and business development teams.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['C++', 'ISO', 'Linux', 'Aerospace', 'Machine learning', 'Data structures', 'Troubleshooting', 'Embedded software', 'Python', 'Technical documentation']",2025-06-13 06:23:05
Lead Software Engineer,New Relic One,6 - 11 years,Not Disclosed,['Hyderabad'],"Your opportunity\nAt New Relic, we provide businesses with a state-of-the-art observability platform, leveraging advanced technologies to deliver real-time insights into the performance of software applications and infrastructure. We enable organizations to monitor, analyze, and optimize their systems to achieve enhanced reliability, performance, and user experience. New Relic is a leader in the industry and has been on the forefront of developing cutting edge AI/ML solutions to revolutionize observability.\nWe are seeking an experienced and dynamic Lead Backend Engineer (Python) to join our AI/ML team. You will develop scalable web services and APIs using Python and its extended ecosystem for our Agentic Platform which will be the nucleus of AI driven workflows at New Relic. Your responsibilities will include ideating, implementing and owning the low level design of the services and leading the rest of the team.\nWhat youll do\nDrive the design, development, and enhancement of core features and functionalities of our AI platform with micro-services architecture and deliver scalable, secure and reliable solutions\nBe proactive in identifying and addressing performance bottlenecks, applying optimizations, and maintaining the stability and availability of our platform\nBuild thoughtful, high-quality code that is easy to read and maintain\nCollaborate with your team, external contributors, and others to help solve problems. Write and share proposals to improve team processes and approaches.\nThis role requires\nBachelor s degree in Computer Science discipline or related field\n6+ years of experience as a Software Engineer working with Python, developing production grade applications\nDemonstrated experience in designing, developing, and maintaining large-scale cloud platforms with a strong understanding of scalable distributed systems and microservices architecture\nProficiency in back-end frameworks such as Flask/FastAPI; Pydantic for robust models; asyncio, aiohttp libraries for asynchronous request handling; Decorators for abstraction; Pytest for testing\nCompetency in using Python threading and multiprocessing modules for parallel task execution. Knowledge of Coroutines. Understand the GIL and its implications on concurrency\nExperience in building secure infrastructure having simulated race condition attacks, injection attacks; leading teams through real incident management situations with strong debugging skills\nDemonstrated experience in working with both Relational and NoSQL DBs; message queueing systems (SQS/Kafka/RabbitMQ)\nUp to date with cloud technologies - AWS/Azure/GCP, Serverless, Docker, Kubernetes, CI/CD pipelines among others\nBonus points if you have\nMasters in Computer Science discipline\nExposure to Machine Learning and GenAI technologies\nExperience with Authentication/Authorization services etc.\nCommunication protocol - gRPC\nGraphQL API working knowledge\n\n\nPlease note that visa sponsorship is not available for this position.\n\nFostering a diverse, welcoming and inclusive environment is important to us. We work hard to make everyone feel comfortable bringing their best, most authentic selves to work every day. We celebrate our talented Relics different backgrounds and abilities, and recognize the different paths they took to reach us - including nontraditional ones. Their experiences and perspectives inspire us to make our products and company the best they can be. We re looking for people who feel connected to our mission and values, not just candidates who check off all the boxes.\nIf you require a reasonable accommodation to complete any part of the application or recruiting process, please reach out to resume@newrelic.com .\nWe believe in empowering all Relics to achieve professional and business success through a flexible workforce model. This model allows us to work in a variety of workplaces that best support our success, including fully office-based, fully remote, or hybrid.\nOur hiring process\n\nIn compliance with applicable law, all persons hired will be required to verify identity and eligibility to work and to complete employment eligibility verification. Note: Our stewardship of the data of thousands of customers means that a criminal background check is required to join New Relic.\n\nWe will consider qualified applicants with arrest and conviction records based on individual circumstances and in accordance with applicable law including, but not limited to, the San Francisco Fair Chance Ordinance .\n\nHeadhunters and recruitment agencies may not submit resumes/CVs through this website or directly to managers. New Relic does not accept unsolicited headhunter and agency resumes, and will not pay fees to any third-party agency or company that does not have a signed agreement with New Relic.\nCandidates are evaluated based on qualifications, regardless of race, religion, ethnicity, national origin, sex, sexual orientation, gender expression or identity, age, disability, neurodiversity, veteran or marital status, political viewpoint, or other legally protected characteristics.\nReview our Applicant Privacy Notice at https: / / newrelic.com / termsandconditions / applicant-privacy-policy",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'SAN', 'CVS', 'Backend', 'NoSQL', 'Debugging', 'Machine learning', 'Incident management', 'Distribution system', 'Python']",2025-06-13 06:23:07
Engagement Manager,Fractal Analytics,10 - 15 years,Not Disclosed,['Bengaluru'],"Brief About Fractal\nFractal Analytics is Leading Fortune 500 companies leverage Big Data, analytics and technology to drive smarter, faster and more accurate decisions in every aspect of their business.\nFortune 500 companies recognize analytics is a competitive advantage to understand s and make better decisions. We deliver insight, innovation and impact to them through predictive analytics and visual storytelling.\nBrief About The Role\nAn Engagement Manager has complete ownership and accountability for successful delivery of client projects and enabling growth for a particular client account. Own client outcomes across engagements, typically managing multiple projects in parallel and cross-sell new consulting work. An engagement Manager plays a pivotal role in leading and executing large global programs for various clients and bring thought leadership to build a roadmap/analytics strategy . Their roles and responsibilities include the following:",,,,"['advanced analytics', 'Staffing', 'Analytical', 'Manager Program Management', 'Machine learning', 'Business planning', 'SOW', 'Management', 'Forecasting', 'Operations']",2025-06-13 06:23:08
"Distinguished, Technical Program Manager",Walmart,8 - 9 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team:\nThe Enterprise People Technology team supports the successful deployment and adoption of new People technology across the enterprise. As a Fortune #1 company, our work impacts millions of associates globally. We strive to continuously improve people technology and products to help managers and associates so they can focus on what matters most - supporting our customers and members. People Technology is one of the major segments of Walmart Global Techs Enterprise Business Services, which is invested in building a compact, robust organization that includes service operations and technology solutions for Finance, People, and the Associate Digital Experience.\nWhat youll do:\nThe TPM for Payroll modernization .\nWork collaboratively and cross-functionally to achieve enterprise objectives and day-to-day excellence\nCoordinate and manage the development of large-scale, cross-functional projects in a multi-disciplinary environment spanning cutting-edge technologies such as big data, machine learning, AI, real-time analytics, and high-volume, low-latency, high-availability services to achieve these goals.\nWork with stakeholders to understand business needs, gather and define technical requirements and collaborate with engineering teams to design technical solutions\nDefine goals, stage gates, critical delivery paths, and key deliverables for large-scale programs that incorporate multiple programs with dependencies across multiple areas of work, timelines, and resource constraints.\nWork closely with architects , engineering, product, design and business leaders to ensure the successful completion of strategic programs\nYou will also establish a resource capacity allocation process and tool to transform the planning process. Your problem-solving and decision-making skills will be crucial in navigating any setbacks or obstacles that Align program deliverables with stakeholder expectations and ensure transparency throughout the project lifecycle\nProactively identify and mitigate risks, resolve issues, manage dependencies, and drive fast and effective decision making\nAct with humility, honesty, and transparency, while also embracing change with curiosity, a growth mindset, and fostering an environment of learning, innovation, and intelligent risk-taking.\nYou will manage large software development projects and work closely with architects and engineering ; product leaders to ensure successful project completion\nYou will have a deep understanding of software architecture and contribute to architectural decision-making processes\nBuild strong relationships with stakeholders across Walmart Global Tech, EBS and Transactional Systems\nDefine goals and deliverables for large-scale programs, and efficiently utilizing engineering resources will be crucial. Problem-solving and decision-making skills will be essential in navigating setbacks and obstacles\nEmbrace change with curiosity, a growth mindset, and fostering an environment of learning, innovation, and intelligent risk-taking\nWhat youll bring:\n8+ years of experience managing large enterprise Engineering or Product programs across multiple teams and disciplines\nExceptional ability to collaborate tasks and responsibilities to team members, ensuring efficient project execution while maintaining accountability and fostering a collaborative and empowering work environment\nExperienced in system/platform design concepts, architecture, technical design, services, APIs, and technologies\nDeep understanding of software architecture, software design principles and contribute to architectural decision-making processes\nEfficient engineering resource utilization and delivery will be a priority, and you will need to simplify and clarify priorities and create plans with appropriate milestones for advance planning and executive buy-in.\nBuild positive relationships and collaborate with product managers to understand target personas and business process needs, which you will then translate into features and user stories that engineers will leverage to iterate products quickly\nStrong sense of ownership and accountability, with a love for data and solving complex problems.\nExpertise in change ; risk management methodologies, project management tools, techniques, project tracking tools, dashboards, and reports\nStrong communicator in both verbal and written forms and are equally adept in communicating upwards, outwards, and downward\nDeep understanding of JIRA, JIRA Align, Big Picture, etc. would be advantageous\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in computer science, information technology, engineering, or related area and 7 years experience in engineering, engineering program management, technical program management, product management, or related area. Option 2: 9 years experience in engineering, engineering program management, technical program management, product management, or related area.\nPreferred Qualifications...",Industry Type: Retail,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'Payroll', 'Software design', 'Networking', 'Project management', 'Risk management', 'JIRA', 'Information technology', 'Analytics']",2025-06-13 06:23:10
Software Development Manager,Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"As part of the AWS Solutions organization, we have a vision to provide business applications, leveraging Amazon s unique experience and expertise, that are used by millions of companies worldwide to manage day-to-day operations. We will accomplish this by accelerating our customers businesses through delivery of intuitive and differentiated technology solutions that solve enduring business challenges. We blend vision with curiosity and Amazon s real-world experience to build opinionated, turnkey solutions. Where customers prefer to buy over build, we become their trusted partner with solutions that are no-brainers to buy and easy to use.\n\nJust Walk Out builds technology enables a new kind of stores with no lines and no checkout you just grab and go! Customers simply use the Amazon Go app to enter the store, take what they want from our selection of fresh, delicious meals and grocery essentials, and go!\n\nOur checkout-free shopping experience is made possible by our Just Walk Out Technology, which automatically detects when products are taken from or returned to the shelves and keeps track of them in a virtual cart. When you re done shopping, you can just leave the store. Shortly after, we ll charge your Amazon account and send you a receipt. Our Just Walk Out Technology uses a variety of technologies including vision, sensor fusion, and advanced machine learning. Innovation is part of our DNA! Our goal is to be Earths most customer centric company and we are just getting started. We need people who want to join an ambitious program that continues to push the state of the art in vision, machine learning, distributed systems and hardware.\n\nAs a Software Development Manager, you will own delivery of cross functional initiatives to build new features and improve Just Walk Out experience. You will manage a team of 6-10 software development engineers to build and enhance the shopping experience in Just Walk out stores.\n\n\nWe are looking for an individual that thrives by creating high-bar engineering culture. You ll be chartered with building software that deliver critical capabilities to our AWS Just Walk out business enabling bar raising customer experience, embed operational knowledge and encode best practices into everything we do.\nWe have a team culture that encourages innovation and we expect team members and management alike to take high degree of ownership for their program vision and execution of ideas. Beyond a strong engineering, data storage/modeling, visualization, and front-end background, a successful candidate will have experience successfully leading teams, developing people, and building a scalable infrastructure. We are looking for someone that is a self-starter, someone that approaches complex business questions with data and curiosity, and a person that dives below the surface to identify the root cause and ""so what"" rather than just superficial trends\n\nAbout the team\nDiverse Experiences\nAmazon values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn t followed a traditional path, or includes alternative experiences, don t let it stop you from applying.\n\nWhy AWS\nAmazon Web Services (AWS) is the world s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating that s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses.\nWork/Life Balance\nWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there s nothing we can t achieve.\nInclusive Team Culture\nAWS values curiosity and connection. Our employee-led and company-sponsored affinity groups promote inclusion and empower our people to take pride in what makes us unique. Our inclusion events foster stronger, more collaborative teams. Our continual innovation is fueled by the bold ideas, fresh perspectives, and passionate voices our teams bring to everything we do.\n\nMentorship and Career Growth\nWe re continuously raising our performance bar as we strive to become Earth s Best Employer. That s why you ll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional.\n\nThe Retail Platform Technology team in AWS Just Walk out, builds the platform and architecture that offers retail capabilities to the customers to run their Just walk out stores. This spans across various domains such as Order Management, in-store pricing intelligence, shopper receipts and refunds, and the merchant experience capabilities that allow merchants to seamlessly integrate with Amazon to power their stores with Just Walk out technology. 2+ years of engineering team management experience\n7+ years of engineering experience\n8+ years of leading the definition and development of multi tier web services experience\nKnowledge of engineering practices and patterns for the full software/hardware/networks development life cycle, including coding standards, code reviews, source control management, build processes, testing, certification, and livesite operations\nExperience partnering with product or program management teams Experience in communicating with users, other technical teams, and senior leadership to collect requirements, describe software product features, technical designs, and product strategy\nExperience in recruiting, hiring, mentoring/coaching and managing teams of Software Engineers to improve their skills, and make them more effective, product software engineers\nExperience designing or architecting (design patterns, reliability and scaling) of new and existing systems",,,,"['Order management', 'Cloud computing', 'Team management', 'Front end', 'Coding', 'Machine learning', 'Technology solutions', 'Business applications', 'Distribution system', 'Operations']",2025-06-13 06:23:12
Software Development Manager,Amazon,7 - 12 years,Not Disclosed,['Bengaluru'],"Do you want to shape the future of how Alexa-enabled devices and the Alexa Cloud functionDo you want to set up, lead, grow, and be part of a team that builds services used by millions of customersIf you said yes to any or all of those, come join the Alexa Connected Devices! The team builds best-in-class tier-1 services that enable connectivity between Alexa-enabled devices and the Alexa Cloud.\n\nWe are seeking a Software Development Manager to lead the development of tier-1 services for the Alexa Cloud.\n\n\nYou will set goals, define, develop and realize the roadmap, identify resource requirements and hire against it, develop the careers of engineers, deliver high-quality software that exceeds customer expectations, write docs to communicate vision, project status and operational reviews, and maintaining operational excellence of the system you own.\n\nA day in the life\nYou will be a member of the leadership team and the technical leader responsible for service development and operations. You will collaborate with stakeholders from Alexa devices and several Alexa Cloud teams to define roadmaps, plan for new requirements, make prioritization trade-offs, and track dependencies. You will brainstorm new ideas with principal engineers to address requirements, solve problems, and scale services. Additionally, you will conduct sprint planning and daily scrum meetings to track deliverables. Furthermore, you will review operational and ticket dashboards to identify and prioritize customer issue resolutions. You will also coach and mentor your team members, plan their careers, and participate in hiring new team members. And, you will author documents to communicate vision and project updates.\n\nAbout the team\nThe mission of the Alexa Connected Devices team is to deliver low-latency, low-cost, highly reliable, and highly scalable connectivity between Alexa-enabled devices and the Alexa Cloud. The changes we implement aim to improve the speed and reliability experienced by end customers when interacting with Alexa devices, and to simplify the experience for developers and the rest of the Alexa ecosystem by abstracting the connection management from them. The team handles tens of billions of transactions per day. This role is highly visible, involving participation in the inception phase, development of new features, and operation of a tier-1 Cloud Services. The ideal candidate is excited about the incredible opportunities that Alexa and the Cloud represent and is passionate about delivering high-quality services in a hyper-growth environment. 7+ years of engineering experience\n3+ years of engineering team management experience\n5+ years of leading the definition and development of multi tier web services experience\nKnowledge of engineering practices and patterns for the full software/hardware/networks development life cycle, including coding standards, code reviews, source control management, build processes, testing, certification, and livesite operations\nExperience partnering with product or program management teams\nExperience designing or architecting (design patterns, reliability and scaling) of new and existing systems\nExperience in recruiting, hiring, mentoring/coaching and managing teams of Software Engineers to improve their skills, and make them more effective, product software engineers Experience in communicating with users, other technical teams, and senior leadership to collect requirements, describe software product features, technical designs, and product strategy\nExperience with the scrum methodology",,,,"['Team management', 'Operational excellence', 'Web services', 'Coding', 'Cloud Services', 'Manager Program Management', 'Scrum', 'Architecting', 'Product strategy', 'Recruitment']",2025-06-13 06:23:13
Business Analyst II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazons vision is to be the earths most customer centric company . The ambition of the Amazon payments business is to transform payments in India . The payments data team provides data and insights to the partner verticals so as to enable them to take decisions. This a senior role. The role requires team-work and building a business perspective. The individual will have to interact extensively with other members within the team, across business lines work in partnership with Machine Learning and Central Customer Analytic teams. There will be a steep learning curve, adding a fair amount of business skills to the individual.\n\n\nAbility to do exploratory analysis: Fetch data from systems and analyze trends\nIdentify consumer segments that need to be actioned to drive adoption of Amazon Pay\nDeveloping India specific customer segmentation models to improve the efficiency of marketing and financing product roadmap inputs .\nEstablishing mechanisms for cross functional teams to consume customer insights to improve engagement along the customer life cycle\nAbility to work with regulated(RBI, FIU, etc) bodies across IN\nAbility to identify and deepdive at the most granular level and flag anomalies\nWork with cross functional teams like Category, Product, Operations, etc 3+ years of tax, finance or a related analytical field experience\n5+ years of Excel (including VBA, pivot tables, array functions, power pivots, etc.) and data visualization tools such as Tableau experience\n3+ years of business or financial analysis experience\nExperience defining requirements and using data and metrics to draw business insights\nExperience making business recommendations and influencing stakeholders\nExperience with Excel -Advanced SQL / data mining skills -Extensive experience in reports/dashboards automation Experience with statistical analysis (R/python) -Experience working with data visualization tools and creating data visualization concepts (Tableau preferred, D3 is a bonus) E commerce experience is preferred",,,,"['customer segmentation', 'Automation', 'Statistical analysis', 'Financial analysis', 'Analytical', 'Machine learning', 'data visualization', 'Data mining', 'SQL', 'Python']",2025-06-13 06:23:15
Survey Programmer,Iqvia Biotech,4 - 8 years,Not Disclosed,['Bengaluru'],"Provides high quality, on-time input to client projects in the life sciences field. Assignments range in complexity from basic analysis and problem solving to assisting in the development of more complex solutions. May serve as project leader for small teams or work streams.\n\nEssential Functions\nDevelop online survey using effective survey programming tools, viz. Decipher, Confirmit, Sawtooth etc.\nAssist in complex custom scripts using jQuery/ JavaScript\nAssists with the review and analysis of client requirements or problems and assists in the development of proposals and client solutions.\nAssists in the development of detailed documentation and specifications.\nPerforms quantitative or qualitative analyses to assist in the identification of client issues and the development of client specific solutions.\nAssists in the design/structure and completion of presentations that are appropriate to the characteristics or needs of the audience.\nDevelops, and may present, complete client deliverables within known/identified frameworks and methodologies.\nProactively develops a basic knowledge of consulting methodologies and the life sciences market through the delivery of consulting engagements and participation in formal and informal learning opportunities.\nEngagement based responsibilities are assigned and managed by Senior Consultants, Engagement Managers or Principals.\nStrong analytical and problem-solving skills with experience in data interpretation\nAbility to work in a fast-paced environment and manage multiple projects simultaneously\nQualifications\nBachelors Degree required\n4-8 years of related experience required\nWorks willingly and effectively with others in and across the organization to accomplish team goals.\nKnowledge of data processing/ analysis tools, viz. SPSS, Wincross is a good to have skill.\nKnowledge and understanding of the fundamental processes of business, their interaction, and the impact of external/internal influences on decision making, growth and decline.\nKnowledge of consulting methods, tools and techniques, related to one s functional area.\nKnowledge of current events and developments within an industry and major competitors.\nEffective time & team management skills.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team management', 'jQuery', 'Analytical', 'Consulting', 'Javascript', 'Healthcare', 'Data processing', 'Clinical research', 'Life sciences', 'SPSS']",2025-06-13 06:23:17
Survey Programmer,Iqvia Biotech,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Overview\nProvides high quality, on-time input to client projects in the life sciences field. Assignments range in complexity from basic analysis and problem solving to assisting in the development of more complex solutions. May serve as project leader for small teams or work streams.\n\nEssential Functions\nDevelop online survey using effective survey programming tools, viz. Decipher, Confirmit, Sawtooth etc.\nAssist in complex custom scripts using jQuery/ JavaScript\nAssists with the review and analysis of client requirements or problems and assists in the development of proposals and client solutions.\nAssists in the development of detailed documentation and specifications.\nPerforms quantitative or qualitative analyses to assist in the identification of client issues and the development of client specific solutions.\nAssists in the design/structure and completion of presentations that are appropriate to the characteristics or needs of the audience.\nDevelops, and may present, complete client deliverables within known/identified frameworks and methodologies.\nProactively develops a basic knowledge of consulting methodologies and the life sciences market through the delivery of consulting engagements and participation in formal and informal learning opportunities.\nEngagement based responsibilities are assigned and managed by Senior Consultants, Engagement Managers or Principals.\nStrong analytical and problem-solving skills with experience in data interpretation\nAbility to work in a fast-paced environment and manage multiple projects simultaneously\nQualifications\nBachelors Degree required\n4-8 years of related experience required\nWorks willingly and effectively with others in and across the organization to accomplish team goals.\nKnowledge of data processing/ analysis tools, viz. SPSS, Wincross is a good to have skill.\nKnowledge and understanding of the fundamental processes of business, their interaction, and the impact of external/internal influences on decision making, growth and decline.\nKnowledge of consulting methods, tools and techniques, related to one s functional area.\nKnowledge of current events and developments within an industry and major competitors.\nEffective time & team management skills.\n. We create intelligent connections to accelerate the development and commercialization of innovative medical treatments to help improve patient outcomes and population health worldwide . Learn more at https://jobs.iqvia.com",Industry Type: Medical Devices & Equipment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team management', 'jQuery', 'Analytical', 'Consulting', 'Javascript', 'Healthcare', 'Data processing', 'Clinical research', 'Life sciences', 'SPSS']",2025-06-13 06:23:19
IN Associate Internal Audit Internal Audit Services,PwC Service Delivery Center,1 - 6 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nRisk\nManagement Level\nAssociate\n& Summary\n.\n\n\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary Exciting Career Opportunity in Risk Consulting .\nResponsible to ensure timely delivery and quality of findings during the internal audit assignments\nLead of Internal Audit / risk & controls / risk assessment engagements\nCoordination with all levels of stakeholders both client and internal. Establishing strong professional relationships with external clients and internal team members. Supporting the partners in driving select client relationships of the firm.\nResponsible for taking ownership of assignments / work allocated and being proactive for ensuring success of the assignment allocation\nResponsible for endtoend delivery including supervising and reviewing engagement teams.\nReview the working papers of subordinates, ensure proactive and regular updates on the project to both internal and external stakeholders with an endeavor to create a nosurprise working culture\nSupport in internal risk clearance activities , if any\nCollaborating with other service lines within the firm for internal opportunities.\nMandatory skill sets\nHighly skilled in Project management\nExcellent in Internal Audit with sector experience Retail/FMCG/Manufacturing. Meticulous and having eye for details.\nQuick learner and ability to work under minimum supervision\nSavvy/ handson in MS office excel, power point etc.\nPreferred skill sets\nExperience of working for clients across various sectors and solutions in audit and consulting firms\nCandidates should preferably have experience in working with the compliance / internal audit/risk management function/operations department\nStrong knowledge of processes and systems in their respective area of operations\nShould have the ability to multitask and manage multiple projects\nStrong project management capabilities and experience in managing a team\nStrong interpersonal skills and wellspoken\nSolution oriented and smart working individual\nStrong problemsolving skills paired with the ability to develop creative and efficient solutions o Ability to manage client expectations through effective communication, technical knowledge, and responsiveness\nAbility to multitask effectively\nAbility to develop and build a client base\nHigh on integrity and a selfdriven/proactive work attitude to deliver results within tight deadlines and in demanding situations\nStrong presentation and negotiation skills\nExcellent written, and verbal communication with presentation and team management skills\nLead Internal Audit/ Process Audit concepts & methodology\nCOSO Framework\nProcesses, Subprocesses, and Activities as well as their relationship\nSarbanes Oxley Act (SOX)\nInternal control concepts (e.g., Preventive Controls; Detective Controls; Antifraud Controls; etc.)\nYears of experience required\n1 + years\nEducation qualification\nCA with relevant postqual experience of 1+ years\nMBA/ACCA or a bachelor s degree with postqual experience of 2+ years .\nEducation\nDegrees/Field of Study required Master of Business Administration, Chartered Accountant Diploma\nDegrees/Field of Study preferred\nRequired Skills\nInternal Auditing\nAccepting Feedback, Accepting Feedback, Accounting and Financial Reporting Standards, Active Listening, Artificial Intelligence (AI) Platform, Auditing, Auditing Methodologies, Business Process Improvement, Communication, Compliance Auditing, Corporate Governance, Data Analysis and Interpretation, Data Ingestion, Data Modeling, Data Quality, Data Security, Data Transformation, Data Visualization, Emotional Regulation, Empathy, Financial Accounting, Financial Audit, Financial Reporting, Financial Statement Analysis, Generally Accepted Accounting Principles (GAAP) {+ 19 more}\nNo",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['Data analysis', 'Manager Internal Audit', 'Team management', 'Financial reporting', 'Data modeling', 'Project management', 'Consulting', 'Outsourcing', 'FMCG', 'Risk management']",2025-06-13 06:23:21
"IT Service Management Senior Analyst, Quality and Laboratory Systems",AstraZeneca India Pvt. Ltd,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Title: IT Service Management Senior Analyst, Quality and Laboratory Systems\nWork Location: Manyata Tech Park, Bangalore, India\nWork Schedule: Three days on-site, two days remote (3/2)\nCareer Level: D3 - Individual Contributor position.\n\nIntroduction to role\nAre you ready to make a difference in the world of rare diseases? At Alexion AstraZeneca Rare Disease, we are committed to transforming lives by innovating and delivering meaningful value to patients and families. Our diverse team is our strength, driving breakthroughs that impact lives. Join us in a role where your work will directly contribute to the advancement of life-changing medicines.\n\nAccountabilities\nIn this pivotal role, you will be responsible for IT support and service administration for Laboratory and Quality IT systems within Alexions Quality and Manufacturing Science and Analytical Technology (MSAT) functions. As a key contact for these functions, you will oversee BAU service management, IT support, upgrades, and minor enhancements, collaborating with external partners and vendors. Your efforts will drive operational efficiency through lean digital solutions and processes.\n\nEssential Skills/Experience\nBachelors degree in Computer Science, Information Technology, Science, Engineering or a related field.\n8+ years of experience in IT service management within the GxP Quality and Laboratory Systems Domain\nStrong knowledge of ITIL principles and IT service management best practices (e.g. ITIL 4 certification)\nExperience (1-3 years) of Stakeholder Relationship and Communication Management\nDirect experience working with the following systems:\nDocument Management Systems (e.g. Veeva Vault QualityDocs, ZenQMS)\nQuality Management Systems (e.g. Veeva Vault QMS, Sparta TrackWise QMS)\nValidation Lifecycle Management Systems (e.g. ValGenesis and Kneat Gx)\nComputer Systems Validation and GAMP 5 experience\nProficiency in quality management processes and regulatory requirements.\nExperience of Administering On-Premises Infrastructure and Cloud Hosting\nExperience with Administration of Segregated Lab Networks and IT Endpoint Asset Management\n\nDesirable Skills/Experience\nMaster s Degree in Computer Science, Information Technology, Science, Engineering or a related field.\nCloud Certification (e.g. AWS, Azure)\nSix Sigma or Lean certification\nKnowledge of emerging technologies like AI/ML in regulated environments\nDemonstrable leadership in managing multiple system and data initiatives\nCustomer focused with high enthusiasm and energy\nExcellent communication, analytical, and decision-making skills\nSelf-starter with the ability to work independently\nExperience working with global, multi-cultural teams\n\n\nAt AstraZenecas Alexion division, we champion diversity and foster an energizing culture where new ideas thrive. Our commitment to inclusion ensures that life-changing innovations can come from anywhere. We celebrate each others successes and take pride in giving back to our communities. Here, your career is more than just a path; its a journey to making a difference where it truly counts.\n\nReady to make a difference? Apply now to join our team!\n\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, sex, gender, gender expression, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n11-Jun-2025\n17-Jun-2025\nAlexion is proud to be an Equal Employment Opportunity and Affirmative Action employer. We are committed to fostering a culture of belonging where every single person can belong because of their uniqueness. The Company will not make decisions about employment, training, compensation, promotion, and other terms and conditions of employment based on race, color, religion, creed or lack thereof, sex, sexual orientation, age, ancestry, national origin, ethnicity, citizenship status, marital status, pregnancy, (including childbirth, breastfeeding, or related medical conditions), parental status (including adoption or surrogacy), military status, protected veteran status, disability, medical condition, gender identity or expression, genetic information, mental illness or other characteristics protected by law. Alexion provides reasonable accommodations to meet the needs of candidates and employees. To begin an interactive dialogue with Alexion regarding an accommodation, please contact accommodations@Alexion.com . Alexion participates in E-Verify.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Management systems', 'Analytical', 'ITSM', 'Asset management', 'Document management', 'Operations', 'Information technology', 'Six sigma', 'Quality management']",2025-06-13 06:23:23
Deloitte - Senior Consultant - Databricks,Deloitte,6 - 10 years,Not Disclosed,"['Pune', 'Bengaluru', 'Delhi / NCR']","What impact will you make?\nEvery day, your work will make an impact that matters, while you thrive in a dynamic culture of inclusion, collaboration and high performance. As the undisputed leader in professional services, Deloitte is where you will find unrivaled opportunities to succeed and realize your full potential\n\nThe Team\nDeloittes AI&D practice can help you uncover and unlock the value buried deep inside vast amounts of data. Our global network provides strategic guidance and implementation services to help companies manage data from disparate sources and convert it into accurate, actionable information that can support fact-driven decision-making and generate an insight-driven advantage. Our practice addresses the continuum of opportunities in business intelligence & visualization, data management, performance management and next-generation analytics and technologies, including big data, cloud, cognitive and machine learning.\n\nWork youll do\nLocation: Bangalore/Mumbai/Pune/Delhi/Chennai/Hyderabad/Kolkata\n\nRoles: Databricks Data Engineering Senior Consultant\nWe are seeking highly skilled Databricks Data Engineers to join our data modernization team. You will play a pivotal role in designing, developing, and maintaining robust data solutions on the Databricks platform. Your experience in data engineering, along with a deep understanding of Databricks, will be instrumental in building solutions to drive data-driven decision-making across a variety of customers.\nMandatory Skills: Databricks, Spark, Python / SQL\n\nResponsibilities\n•        Design, develop, and optimize data workflows and notebooks using Databricks to ingest, transform, and load data from various sources into the data lake.\n•        Build and maintain scalable and efficient data processing workflows using Spark (PySpark or Spark SQL) by following coding standards and best practices.\n•        Collaborate with technical and business stakeholders to understand data requirements and translate them into technical solutions.\n•        Develop data models and schemas to support reporting and analytics needs.\n•        Ensure data quality, integrity, and security by implementing appropriate checks and controls.\n•        Monitor and optimize data processing performance, identifying, and resolving bottlenecks.\n•        Stay up to date with the latest advancements in data engineering and Databricks technologies.\nQualifications\n•        Bachelors or masters degree in any field\n•        6-10 years of experience in designing, implementing, and maintaining data solutions on Databricks\n•        Experience with at least one of the popular cloud platforms – Azure, AWS or GCP\n•        Experience with ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) processes\n•        Knowledge of data warehousing and data modelling concepts\n•        Experience with Python or SQL\n•        Experience with Delta Lake\n•        Understanding of DevOps principles and practices\n•        Excellent problem-solving and troubleshooting skills\n•        Strong communication and teamwork skills\n\nYour role as a leader\nAt Deloitte India, we believe in the importance of leadership at all levels. We expect our people to embrace and live our purpose by challenging themselves to identify issues that are most important for our clients, our people, and for society and make an impact that matters.\nIn addition to living our purpose, Senior Consultant across our organization:\nDevelop high-performing people and teams through challenging and meaningful opportunities\nDeliver exceptional client service; maximize results and drive high performance from people while fostering collaboration across businesses and borders\nInfluence clients, teams, and individuals positively, leading by example and establishing confident relationships with increasingly senior people\nUnderstand key objectives for clients and Deloitte; align people to objectives and set priorities and direction.\nActs as a role model, embracing and living our purpose and values, and recognizing others for the impact they make\n\nHow you will grow\nAt Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there is always room to learn. We offer opportunities to help build excellent skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Centre.\n\nBenefits\nAt Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.\n\nOur purpose\nDeloitte is led by a purpose: To make an impact that matters. Every day, Deloitte people are making a real impact in the places they live and work. We pride ourselves on doing not only what is good for clients, but also what is good for our people and the\nCommunities in which we live and work—always striving to be an organization that is held up as a role model of quality, integrity, and positive change. Learn more about Deloitte's impact on the world\n\nRecruiter tips\nWe want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you are applying to. Check out recruiting tips from Deloitte professionals.",Industry Type: Management Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Aws Databricks', 'Data Bricks', 'Python', 'SQL', 'Pyspark']",2025-06-13 06:23:24
Quantitative Analytics Manager,Wells Fargo,4 - 8 years,Not Disclosed,['Bengaluru'],"In this role, you will:\nManage a team responsible for the creation and implementation of low to moderate complex financial areas\nMitigate operational risk and compute capital requirements\nDetermine scope and prioritization of work in consultation with experienced management\nParticipate in the development of strategy, policies, procedures, and organizational controls with model users, developers, validators, and technology",,,,"['Quantitative Analytics', 'strategy Planning', 'marketing', 'Git', 'GitHub', 'talent development', 'credit risk analysis']",2025-06-13 06:23:26
ML Engineer (Advanced AI/ML + Python),HR Solutions,9 - 14 years,20-25 Lacs P.A.,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Dear Candidate,\n\nWe have an urgent opening with one of Multinational Company for Below Locations.\n\nInterested candidate can share the resume on Deepaksharma@thehrsolutions.in OR WhatsApp on 8882505093\n\nExperience : 9+ Years\nProfile : ML Engineer (Advanced AI/ML + Python)\nLocations : Hyderabad, Bangalore, Mumbai, Kolkata\nNotice period : Only immediate joiners OR Already serving.\nJob Description - ML Engineer (Advanced AI/ML + Python)\n\nThis role leverages machine learning & advanced AI services to generate business insights\n8+ Years of relevant experience in Machine learning & Deep learning (GenAI-optional) experience\nExperience in deploying NLP, CV & Deep learning algorithms\nExperience in leading teams and managing the end-to-end deliverables. Exposure to MLOps implementations\nLeadership abilities to guide and mentor junior team members.\nDesign and implement scalable and reliable ML pipelines for model training, evaluation and deployment\nAdvanced degree in Data Science, Statistics, Computer Science, or a related field.\nProficiency in Python, Git, Github,Pyspark, Pytorch, , SQL and strong expertise of AI/ML algorithms is essential with knowledge of AWS cloud ML services.\nStrong analytical skills & communication skills. Ability to interpret complex data sets.\n\nResponsibilities-\n\nLead the development and implementation of predictive models and machine learning algorithms.\nDesign and implement scalable and reliable ML pipelines for model training, evaluation and deployment\nCollaborate with CMA CGM cross-functional teams to understand business needs and devise possible solutions.\nPresent and visualize data insights to stakeholders in a comprehensible manner.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Machine Learning', 'Python', 'MLops', 'AI/ML', 'Natural Language Processing', 'AIML', 'Deep Learning']",2025-06-13 06:23:28
Senior AI Engineer,Reltio,5 - 10 years,Not Disclosed,['Bengaluru'],"Job Title: Senior AI Engineer\nLocation: Bengaluru, India - (Hybrid)\nAt Reltio , we believe data should fuel business success. Reltio s AI-powered data unification and management capabilities encompassing entity resolution, multi-domain master data management (MDM), and data products transform siloed data from disparate sources into unified, trusted, and interoperable data. Reltio Data Cloud delivers interoperable data where and when its needed, empowering data and analytics leaders with unparalleled business responsiveness. Leading enterprise brands across multiple industries around the globe rely on our award-winning data unification and cloud-native MDM capabilities to improve efficiency, manage risk and drive growth.\nAt Reltio, our values guide everything we do. With an unyielding commitment to prioritizing our Customer First , we strive to ensure their success. We embrace our differences and are Better Together as One Reltio. We are always looking to Simplify and Share our knowledge when we collaborate to remove obstacles for each other. We hold ourselves accountable for our actions and outcomes and strive for excellence. We Own It . Every day, we innovate and evolve, so that today is Always Better Than Yesterday . If you share and embody these values, we invite you to join our team at Reltio and contribute to our mission of excellence.\nReltio has earned numerous awards and top rankings for our technology, our culture and our people. Reltio was founded on a distributed workforce and offers flexible work arrangements to help our people manage their personal and professional lives. If you re ready to work on unrivaled technology where your desire to be part of a collaborative team is met with a laser-focused mission to enable digital transformation with connected data, let s talk!\nJob Summary:\nAs a Senior AI Engineer at Reltio, you will be a core part of the team responsible for building intelligent systems that enhance data quality, automate decision-making, and drive entity resolution at scale. You will work with cross-functional teams to design and deploy advanced AI/ML solutions that are production-ready, scalable, and embedded into our flagship data platform.\nThis is a high-impact engineering role with exposure to cutting-edge problems in entity resolution, deduplication, identity stitching, record linking, and metadata enrichment .\nJob Duties and Responsibilities:\nDesign, implement, and optimize state-of-the-art AI/ML models for solving real-world data management challenges such as entity resolution, classification, similarity matching, and anomaly detection.\nWork with structured, semi-structured, and unstructured data to extract signals and engineer intelligent features for large-scale ML pipelines.\nDevelop scalable ML workflows using Spark, MLlib, PyTorch, TensorFlow, or MLFlow , with seamless integration into production systems.\nTranslate business needs into technical design and collaborate with data scientists, product managers, and platform engineers to operationalize models.\nContinuously monitor and improve model performance using feedback loops, A/B testing, drift detection, and retraining strategies.\nConduct deep dives into customer data challenges and apply innovative machine learning algorithms to address accuracy, speed, and bias.\nActively contribute to research and experimentation efforts, staying updated with latest AI trends in graph learning, NLP, probabilistic modeling , etc.\nDocument designs and present outcomes to both technical and non-technical stakeholders , fostering transparency and knowledge sharing.\nSkills You Must Have:\nBachelor s or Master s degree in Computer Science, Machine Learning, Artificial Intelligence , or related field. PhD is a plus.\n5+ years of hands-on experience in developing and deploying machine learning models in production environments.\nProficiency in Python (NumPy, scikit-learn, pandas, PyTorch/TensorFlow) and experience with large-scale data processing tools ( Spark, Kafka, Airflow ).\nStrong understanding of ML fundamentals , including classification, clustering, feature selection, hyperparameter tuning, and evaluation metrics.\nDemonstrated experience working with entity resolution, identity graphs, or data deduplication .\nFamiliarity with containerized environments (Docker, Kubernetes) and cloud platforms (AWS, GCP, Azure)\nStrong debugging, analytical, and communication skills with a focus on delivery and impact.\nAttention to detail, ability to work independently, and a passion for staying updated with the latest advancements in the field of data science\nSkills Good to Have:\nExperience with knowledge graphs, graph-based ML, or embedding techniques .\nExposure to deep learning applications in data quality, record matching, or information retrieval .\nExperience building explainable AI solutions in regulated domains.\nPrior work in SaaS, B2B enterprise platforms , or data infrastructure companies .\nWhy Join Reltio*\nHealth Wellness:\nComprehensive Group medical insurance, including your parent,s with additional top-up options.\nAccidental Insurance\nLife insurance\nFree online unlimited doctor consultations\nAn Employee Assistance Program (EAP)\nWork-Life Balance:\n36 annual leaves, which include Sick leaves - 18, Earned Leaves - 18\n26 weeks of maternity leave, 15 days of paternity leave\nVery unique to Reltio - 01 week of additional off as recharge week every year globally\nSupport for home office setup:\nHome office setup allowance.\nStay Connected, Work Flexibly: Mobile Internet Reimbursement\nNo need to pack a lunch we ve got you covered with a free meal. And many more ..",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'metadata', 'Data management', 'Analytical', 'Debugging', 'Machine learning', 'Data processing', 'Data quality', 'Analytics', 'Python']",2025-06-13 06:23:29
AI Engineer,Onebyzero,1 - 4 years,Not Disclosed,['Bengaluru'],"Job_Description"":""\nAs an AI Engineer specializing in Machine Learning and Natural Language Processing, you will lead the development and deployment of state-of-the-art AI solutions and products for a diverse client base.\n\nWhat You\\u2019ll do\nDesign, develop, and operationalize advanced NLP models such as summarization, question-answering, intent recognition, dialog/conversational AI, semantic search, named entity recognition, knowledge discovery, document understanding, and text classification.\nWork with a wide range of datasets from various sources including websites, wikis, enterprise applications, document stores, file systems, conversation platforms, social media, and databases.\nEmploy leading-edge algorithms and models from TensorFlow, PyTorch , and Hugging Face, and engage with next-gen LLM frameworks like Langchain and Guardrails.\nUtilize modern MLOps practices to evaluate, manage, and deploy models efficiently and effectively in production environments.\nDevelop and refine tools and processes to improve model performance and reproducibility across multiple customer engagements.\nBuild and maintain robust, scalable solutions using cloud infrastructure such as AWS and Databricks to deploy LLM-powered systems.\nCreate evaluation datasets, conduct rigorous model testing to ensure they meet high standards of accuracy and usability, and present findings and models effectively using platforms like Jupyter Notebooks.\n\n\nRequirements\nBachelors degree in Computer Science , Information Technology, or a related field. A Masters degree or relevant certification would be a plus.\nStrong experience with web frameworks like ReactJS, NextJS or Vue.js\nStrong programming skills in languages such as Python, Bash.\nExcellent analytical and problem-solving skills, and attention to detail.\nExceptional communication skills, with the ability to explain complex technical concepts to non-technical stakeholders.\n\n\n\nBenefits\nWhat We Offer\n\nAn opportunity to be part of an agile, highly proficient and experienced AI/ML team\nAn opportunity to work on challenging data science and machine learning problems with customers and seeing your work deployed in action\nA fast-paced software development environment that uses the latest open-source tools across the development stack\n\nBenefits\n\nWe provide a competitive salary and benefits package, a vibrant work environment, and numerous opportunities for professional growth. Youll have the opportunity to work with a team of industry experts on exciting projects that transform businesses and create significant value. Join us to revolutionize the way companies leverage technology for digital transformation.\n\nOnebyZero is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.\n\n\n"",""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Enterprise applications', 'Social media', 'Analytical', 'Machine learning', 'Deployment', 'Natural language processing', 'Open source', 'Information technology', 'Python']",2025-06-13 06:23:31
Sr Software Engineer (Full Stack),Johnson Controls,4 - 7 years,Not Disclosed,['Pune'],"Job Title-Senior Software Engineer (AI Engineering)\nPosting Title- Senior Software Engineer (Full Stack - AI Data Engineering)\nJob Code/Job Profile/Job Level- 172\nPreferred Locations-\nIndia (Pune)\nIntroduction\nThe future is being built today and Johnson Controls is making that future more productive, more secure and more sustainable. We are harnessing the power of cloud, AI/ML and Data analytics, the Internet of Things (IoT), and user design thinking to deliver on the promise of intelligent buildings and smart cities that connect communities in ways that make people s lives and the world better.",,,,"['Computer science', 'Object oriented design', 'Machine learning', 'Javascript', 'Engineering Manager', 'HTML', 'Scrum', 'Open source', 'Monitoring', 'Python']",2025-06-13 06:23:33
Technical Systems Engineer - AI/NVIDIA GPU/HPC Clusters,Cisco,5 - 10 years,Not Disclosed,['Bengaluru'],"Position - Technical Systems Engineer - AI/NVIDIA GPU/HPC Clusters (Hybrid Cloud, Virtualisation and Container technologies) + Python/GoLang\nWhat You Will Do\nCisco IT is building, developing, and expanding our artificial intelligence platform, which will empower the business to fundamentally change the world. You will be a critical member of the Cloud Infrastructure and Platform Services (CIPS) organization building and managing the internal AI platform at Cisco.\nYou will provide leadership in the design and implementation of GPU compute cluster that runs demanding deep learning, high performance computing, and computationally intensive workloads. You will be responsible for AI hardware analysis, design, procurement, and support. You will be an expert in identifying architectural changes and/or completely innovative approaches for our artificial intelligence platform.\nTechnical hand-on role in building and supporting NVIDIA based artificial intelligence platforms.\nPlan, build and install/upgrade new systems that support NVIDIA DGX hardware and software.\nAutomate configuration management, software updates, and maintenance and monitoring of GPU system availability using modern DevOps tools (Ansible, Gitlab, etc.)\nLead the advancement of artificial intelligence platforms and practices.\nAdminister Linux systems, ranging from powerful GPU enabled servers to general-purpose compute systems.\nCollaborate closely with internal Cisco Business Units, application teams and cross-functional technical domains.\nCreate written technical designs, documents, and presentations.\nStay up to date with AI industry advancements and cutting-edge technologies.\nAccelerate the delivery of AI capabilities across our portfolio.\nDesign new tools to monitor alerts that will help discover failures or issues before our customers.\nMaintain services once they are live by measuring and monitoring availability, latency, and overall system health.\n\n\nWho Youll Work With\nWhen you work with us, you will work as part of a hardware and software engineering team that designs and develops Hybrid-Cloud compute platforms and capabilities that are crucial to keeping Ciscos critical business applications and processes available.\nWho You Are\nYou are an experienced technical leader in artificial intelligence, machine learning, data analytics, software engineering, and managing complex integrated systems. An excellent collaborator who can partner, lead, teach, and communicate advanced technical concepts. A talented and passionate engineer comfortable working in high-pressure, large scale enterprise environments.\nOur Minimum Requirements include:\nYou have a BA, BS, or MS in CS, EE, CE or equivalent experience.\n5+ years of previous experience deploying and administrating HPC clusters.\nFamiliar with GPU resource scheduling managers (Slurm (preferred), RunAI, etc.).\nProficient in Hybrid Cloud, Virtualization, and Container technologies.\nDeep understanding of operating systems, computer networks, and high-performance applications\nFamiliar with project tracking tools (e.g. Jira), Git (any Version Control systems), and CI/CD systems (e.g. GitLab, GitHub Actions, Jenkins).\nProficient in general purpose programming languages (Python, GoLang, C/C++) and development platforms and technologies (GIT, JIRA, Jenkins, etc.).\nExperience with Agile and DevOps operating models.\nHard-working dedication to provide quality in support for your customers.\nEstablished record of leading technical initiatives and delivering results.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'data analytics', 'GitHub', 'System Engineering', 'software engineering', 'machine learning', 'GoLang', 'artificial intelligence', 'GitLab', 'Hybrid Cloud']",2025-06-13 06:23:35
Software Engineer - Networking | Automation | AI,Cisco,8 - 13 years,Not Disclosed,['Bengaluru'],"Your Impact\nAs a System Test Engineer, your responsibilities will include:\nDesigning and developing tools to test the functionality of Cisco products, ensuring quality and stability from a system/solution perspective that simulates customer environments with IIoT deployments\nCreating documentation (Cisco Validated Profiles) for various end-to-end solutions.\nAddressing customer issues related to Industrial IoT (IIoT) products, reproducing problems in-house for root cause analysis, and driving resolution.\nCollaborating with local and remote teams to review Product Requirements Documents, Software Functional and Design Specifications, and other relevant documentation to extract test requirements and provide feedback for testability and system automation design/execution.\nEnhancing test efficiency and effectiveness by analyzing customer-found issues and applying appropriate testing approaches, technologies, tools, and innovations.\nLeveraging Machine Learning (MI) and Artificial Intelligence (AI) techniques to improve test automation and manual testing processes.\nSupporting the test team in expanding their knowledge of testing technologies and networking by staying updated with the latest industry developments.\nContributing to the departments knowledge retention by systematically archiving meaningful documentation and equipment.""\n\n\nMinimum Qualifications\nYou are proficient with writing quality code (automation test code)\nYou have deeper networking knowledge with the hands-on experience on routers/switches.\nHave hands on experience with scripting languages like Python\nYou leverage AI/ML tools for network infrastructure support and to drive productivity improvements.\nYou have experience in conducting Performance and Scale testing of newer platforms and features\nYou have 8+ years of experience in Software Quality Assurance with a focus on test automation with hands on experience in building and maintaining test suites.\n\n\nPreferred Qualifications\nYou leverage AI/ML tools for network infrastructure support and to drive productivity improvements.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Engineering', 'automation test code', 'Software Quality Assurance', 'AI/ML tools', 'network infrastructure support']",2025-06-13 06:23:37
AI-ML Engineer,Sumeru Digital Solutions,2 - 7 years,Not Disclosed,['Bengaluru'],"Collaborate with cross-functional teams to achieve strategic outcomes\nApply subject expertise to support operations, planning, and decision-making\nUtilize tools, analytics, or platforms relevant to the job domain\nEnsure compliance with policies while improving efficiency and outcomes\n",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['image processing', 'python', 'c++', 'data analysis', 'c', 'natural language processing', 'neural networks', 'aiml', 'machine learning', 'artificial intelligence', 'sql', 'deep learning', 'tensorflow', 'java', 'data science', 'computer vision', 'linux', 'pytorch', 'keras', 'opencv', 'ml']",2025-06-13 06:23:39
Senior Software Engineer - Backend (Python),Uplers,5 - 7 years,25-35 Lacs P.A.,['Bengaluru'],"Senior Software Engineer - Backend (Python)\n\nExperience: 5 - 7 Years Exp.\nSalary : INR 25-35 Lacs per annum\nPreferred Notice Period: Within 30 Days\nShift: 09:00AM to 06:00PM IST\nOpportunity Type: Onsite (Bengaluru)\nPlacement Type: Contractual\nContract Duration: Full-Time, Indefinite Period\n\n(*Note: This is a requirement for one of Uplers' Clients)\n\nMust have skills required :\nAdvanced python, FastAPI, Api & microservices architecture, Cloud infrastructure (aws), Docker/Kubernetes, Database management (PostgreSQL/ MySQL/ MongoDB/ Redis), Integration with ML/Video Systems, Flask/ Django\nGood to have skills :\nAsynchronous programming, security best practices, Stream Processing & Messaging, Domain Knowledge in AI/ computer vision\n\nRadius AI (One of Uplers' Clients) is Looking for:\nSenior Software Engineer - Backend (Python) who is passionate about their work, eager to learn and grow, and who is committed to delivering exceptional results. If you are a team player, with a positive attitude and a desire to make a difference, then we want to hear from you.\n\nRole Overview Description\nRadiusAI is looking for a Senior Software Engineer (Python) to build and optimize the backend infrastructure that drives our real-time AI products. This is a hands-on role ideal for an engineer who has a deep understanding of backend architecture, API design, and distributed systems and can scale systems to support intensive machine learning and video processing workloads. You will be a key part of a cross-functional team building robust, scalable, and secure platforms for AI deployment.\n\nKey Responsibilities\nDesign and implement backend services, APIs, and data pipelines to support AI and CV platforms.\nBuild scalable microservices and RESTful APIs using Python (FastAPI, Flask, or Django).\nIntegrate with computer vision systems and ML inference engines via APIs or streaming data interfaces.\nOptimize system performance for real-time or near-real-time processing, especially in video-based environments.\nWork with cloud services (AWS, GCP, or Azure) for deployment, scaling, and observability.\nImplement robust logging, monitoring, and alerting across backend services.\nCollaborate closely with ML engineers, DevOps, and frontend teams to deliver full-stack features.\nOwn the entire software development lifecycle: architecture, development, testing, deployment, and maintenance.\nWrite clean, testable, scalable, and maintainable code.\nParticipate in code reviews, mentoring, and setting engineering best practices.\n\nRequired Qualifications\n5+ years of experience in backend development, with Python as the primary language.\nStrong experience with Python web frameworks such as FastAPI, Django, or Flask.\nExpertise in designing and building RESTful APIs and microservices architectures.\\\nSolid understanding of software architecture, design patterns, and scalability principles.\nExperience working with databases (PostgreSQL, MySQL, MongoDB, Redis, etc.).\nProficient with Docker, Kubernetes and experience containerizing applications for local and cloud deployment.\nHands-on experience working with cloud platforms.\nExperience integrating with machine learning models and handling high-throughput data (image/video or time-series is a plus).\\\nFamiliarity with CI/CD practices, Git, unit testing, and agile methodologies.\\\nStrong problem-solving skills and a collaborative mindset.\n\nPreferred Qualifications\nExperience with asynchronous programming (e.g., asyncio, aiohttp, FastAPI with async).\nFamiliarity with message queues and stream processing (Kafka, RabbitMQ, Redis Streams, etc.).\nExposure to real-time data processing systems, especially involving video or IoT sensor data.\nKnowledge of security best practices in backend systems (authentication, authorization, rate limiting).\nPrior experience in computer vision or AI-focused products is a strong plus.\nContributions to open-source Python projects or backend infrastructure tooling.\n\nInterview rounds\n1st - Technical screening\n2nd - Live coding\n3rd - Technical & cultural discussion\n\nHow to apply for this opportunity:\nEasy 3-Step Process:\n1. Click On Apply! And Register or log in on our portal\n2. Upload updated Resume & Complete the Screening Form\n3. Increase your chances to get shortlisted & meet the client for the Interview!\n\nAbout Our Client:\nRadiusAI is a pioneering computer vision analytics company revolutionizing retail operations with advanced, human-centric AI solutions. We offer the world's most advanced VisionAI checkout and we provide real-time data to improve operational efficiency across the entire retail industry, focusing on enterprise-level customers and secure edge integration\n\nAbout Uplers:\nUplers is the #1 hiring platform for SaaS companies, designed to help you hire top product and engineering talent quickly and efficiently. Our end-to-end AI-powered platform combines artificial intelligence with human expertise to connect you with the best engineering talent from India.\nWith over 1M deeply vetted professionals, Uplers streamlines the hiring process, reducing lengthy screening times and ensuring you find the perfect fit. Companies like GitLab, Twilio, TripAdvisor, and AirBnB trust Uplers to scale their tech and digital teams effectively and cost-efficiently.\nExperience a simpler, faster, and more reliable hiring process with Uplers today.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Fast Api', 'Microservices', 'Python', 'Api & microservices architecture', 'Advanced python', 'Flask/ Django']",2025-06-13 06:23:40
AI/ML Engineer,Our Client is a leading global end to en...,4 - 7 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Our client is Leading IT Services and Consulting organization\n\nLocation: Bangalore/ Hyderabad/Chennai\n\nAI/ML Engineer\nMandatory Skills: Python, Machine, AI/ML\n\nNotice period: Immediate 30 Days",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Aiml', 'Machine Learning', 'Deep Learning', 'Python', 'Natural Language Processing']",2025-06-13 06:23:42
"Software Engineer(.NET, Azure, C#)",Renewable Energy Equipment Manufacturing,3 - 5 years,7-12 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nDesigning and delivering Azure API and associated data platform solutions\nProvision of data in a secure and reliable manner\nTroubleshoot and resolve issues in our dev, test and production environments.\nComfort with frequent, incremental code testing and deployment\nDelivering and presenting Proofs of Concept variants to prospective customers.\nRequirements Analysis and contribute in solution architecture design.\nDocumentation of solutions (e.g. data models, configurations, and setup).\nWorking with data developers to ensure high quality access to and supply of specified data.\nEnsuring that platforms and data solutions can be deployed and operated in a highly repeatable and predictable manner through interaction and collaboration with DevOps specialists.\nDeal with other stakeholders/ end users in the software development lifecycle\nQualifications\n2 to 7 years of hands-on experience of designing and delivering distributed cloud solutions using Microsoft Azure\nVery strong, in-depth, and demonstrable hands-on experience with the large numbers of the following technologies:\nMicrosoft Azure PaaS and SaaS solution development technologies including Azure Functions, Logic Apps, .NET, JavaScript, Python etc.\nMicrosoft Azure App Service Fabric, App Service Environment, Microsoft Azure API Management platform technologies\nJSON, REST and data based APIs and high scale performant service facades\nMicrosoft Azure Identity Management and Security technologies including custom SAML 2.0 providers\nMicrosoft Visual Studio Team System\nAzure Service Bus and Azure Notifications Hub\nAzure Artificial Intelligence and Machine Learning platforms Microsoft Azure Machine Learning, Azure Cognitive Services – would be a plus to have\nMicrosoft Azure Operational and Monitoring tools\nFamiliarity with CosmosDB, Cassandra, Mongo DB or similar technologies would additionally be very useful\nFamiliarity with any of the following would be distinct advantage: Azure Data Analytics platform (Cortana Intelligence Platform) including Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Search, Azure Databricks and Open Source technologies such Apache Spark, Atlas, Hadoop, NoSQL, Kafka, Solr\nExperience with best practice design principles and approaches for a range of application styles and technologies to help guide and steer decisions.\nExperience working with structured and unstructured data including imaging & geospatial data.\nExperience of working in highly dynamic teams using agile methodologies often under demanding timescales.\nExperience of motivating and managing team performance in delivering to agreed timelines",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['C#', 'WebAPi', 'Azure Cloud', '.Net', 'Sql', 'oops concept']",2025-06-13 06:23:43
AI/ML Engineering,Manufacturing company,1 - 2 years,Not Disclosed,['Bengaluru'],"Role & responsibilities :\n\n1. Data Handling\nCollect and clean data from machines, sensors, and production lines.\n\n2. Model Building\nSupport basic AI/ML model development for quality checks, defect detection, and maintenance prediction.\n\n3. PoC\nAssist in small projects to test AI ideas in real manufacturing settings.\n\n4. Collaboration\nWork with engineers and IT teams to understand problems and implement solutions.\n\nGood to know:\n\nInterest in manufacturing and automation.\n\nPreferred candidate profile\n\nInterested candidate kindly share me your CV at jeevabvr@gmail.com",Industry Type: Automobile (Automobile Dealers),"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Temporary/Contractual","['Predictive Maintenance', 'Artificial Intelligence', 'Machine Learning', 'Manufacturing Engineering', 'Robotics', 'Industrial Equipment', 'Computer Vision', 'Python']",2025-06-13 06:23:45
Senior Software Engineer | C Programming and Routing Protocols,Cisco,9 - 14 years,Not Disclosed,['Bengaluru'],"Meet the Team\nCiscos Distributed Systems Engineering (DSE) team builds the foundational software that powers the worlds largest, most demanding data centers . The PI-Common team is at the core if this, as we build the routing protocols and infra, that are the heart of data centers. As AI/ML workloads explode and cloud-scale infrastructure becomes the norm, the industry is transformingand Cisco is leading the way.\n\nOur NX-OS operating system, running on industry-leading Cisco Nexus switches , delivers the performance, programmability, and reliability needed for modern data center networks. At DSE, we combine open-source innovation, deep systems engineering, and hardware-software co-design to build platforms that scale globally and evolve rapidly.\n\nWere looking for passionate engineers who thrive on solving complex problems, building distributed systems, and working hands-on with real-world infrastructure.\n\nYour Impact\nYoull be part of a high-performing team driving innovation across Ciscos data center portfolio , which includes Nexus 9000 series switches , NX-OS , and technologies like Routing Protocols, Routing Infra, Virtualization, VXLAN EVPN , segment routing , and telemetry . Youll engage closely with senior engineers, architects, and open-source communities, influencing product direction and technical strategy.\n\nMinimum Qualifications\nB.E./B.Tech in Computer Science, ECE, or a related field\n59 years of software development experience\nStrong proficiency in C programming in multi-threaded, embedded environments\nIn-depth understanding of routing protocols such as OSPF , ISIS , and BGP\nSolid grasp of networking fundamentals (TCP/IP, multicast, switching, forwarding)\nExperience working with or developing for NX-OS or similar network operating systems\nStrong understanding of OS internals , IPC , memory management , and HA systems\nFamiliarity with YANG , SNMP , or model-driven telemetry\nExperience in scripting with Python\nStrong problem-solving and debugging skills\nExcellent communication and collaboration skills\n\nPreferred Qualifications\nExposure to data center protocols such as Routing, VXLAN , EVPN , or segment routing\nExposure to open-source tools, network automation, or configuration frameworks\nPrior experience building infrastructure at scale\n\nWhy Cisco\n\nAt Cisco, we dont just build productswe build the internet . Nearly every data packet in the world touches Cisco hardware or software. Our data center platforms serve as the backbone for hyperscalers, enterprises, and mission-critical services globally.\n\nWe combine the stability of a global brand with a culture of innovation . Youll work on meaningful technology, grow with the best minds in the industry, and help create the next wave of networking solutions AI-ready, cloud-native, and highly programmable .\n\nWe support individuality, diversity, and continuous learning. And yes, we give backevery Cisco employee gets dedicated days each year for community impact.\n\nReady to engineer the futureApply now and join the PI-Common team at Cisco DSE.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['networking', 'memory management', 'routing protocols', 'debugging', 'multithreading', 'tcp', 'switching', 'network automation', 'python', 'software development', 'ip', 'bgp', 'nx', 'snmp', 'ospf', 'artificial intelligence', 'os internals', 'computer science', 'software engineering', 'protocols', 'ipc', 'ml']",2025-06-13 06:23:46
Senior QA Automation Engineer,Luxoft,6 - 11 years,Not Disclosed,['Bengaluru'],"Project description\nWe are looking for a highly skilled and experienced Senior QA Automation Engineer to lead testing initiatives for complex financial systems. The ideal candidate will have deep expertise in test automation, framework design, and domain knowledge in Murex. Familiarity with Xceptor is a strong advantage.\n\nResponsibilities\n\nLead the design and implementation of robust, scalable test automation frameworks.\n\nDevelop and maintain automated test scripts for functional, regression, and integration testing.\n\nCollaborate with cross-functional teams including developers, BAs, and DevOps to ensure high-quality releases.\n\nDrive test strategy, planning, and execution for Murex-related projects.\n\nMentor junior QA team members and enforce best practices in test automation and quality assurance.\n\nParticipate in code reviews, CI/CD pipeline integration, and test data management.\n\nSupport UAT and production validation efforts.\n\nSkills\nMust have\n\n6+ years of experience in software testing with at least 4+ years in automation.\n\nStrong hands-on experience with test automation tools such as Selenium, TestNG, Cucumber, RestAssured, or similar.\n\nProficiency in Java/Python or other scripting languages used in automation.\n\nExperience in building and maintaining custom automation frameworks.\n\nSolid understanding of Murex architecture, trade lifecycle, Trade insertion, E2E deal flow (FO, BO, Confo and settlements).\n\nStrong knowledge of SQL and database validation.\n\nExperience with CI/CD tools like Jenkins, Git, Maven, or similar.\n\nNice to have\n\nWorking knowledge of Xceptor for data transformation and reconciliation.\n\nExperience with performance testing tools (e.g., JMeter, LoadRunner) is a plus.\n\nExposure to cloud platforms (AWS, Azure) and using Machine Learning skills\n\nISTQB Advanced Level or equivalent certification.\n\nStrong leadership and mentoring capabilities.\n\nExcellent communication and stakeholder management skills.\n\nAnalytical mindset with a proactive approach to problem-solving.\n\nOther\n\nLanguages\n\nEnglishB2 Upper Intermediate\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'automation testing', 'sql', 'java', 'test automation framework', 'continuous integration', 'software testing', 'cucumber', 'murex', 'framework design', 'trade life cycle', 'jmeter', 'rest assured', 'trade', 'selenium', 'stakeholder management', 'jenkins', 'e2e', 'testng', 'scripting languages']",2025-06-13 06:23:48
Solution Engineer - AI Infrastructure,Cisco,10 - 15 years,Not Disclosed,['Bengaluru'],"What Youll Do\nWe are seeking a Solutions Engineer - Artificial Intelligence (AI) Practitioner to join our dynamic sales team. As an SE (AI Practitioner), you will consult and drive the adoption of our AI solutions across various industries. You will identify potential clients, understand their specific needs, and provide tailored AI solutions that enhance their business operations. This role requires a deep understanding of AI technologies, real world deployment experience and expert capability to relay business and technical concepts to a diverse audience.\nWho Youll Work With\nThe Cloud and AI Infrastructure team is responsible for helping customers change their business using Cisco technology in the data center and on cloud. The team works with customers, partners and engineering teams to take customer problems and turn them into business advantage.\nWho You Are\nYoure energized by the fast-paced landscape in IT and how AI is changing the world.You love technology and thrive in solving complex problems. You are an amazing presenter and can translate complex technical scenarios into a simple easy to understand message. You inspire those around to use technology in innovative ways and use a hands-on methods to demonstrate your ideas.\nMinimum Qualifications:\n10+ years of technology consulting experience (preferably in systems, software, data, analytics and AI).\nGood understanding of programming/scripting languages\nAbility to provide detailed and consumable documentation and standard methodologies for deployment around application acceleration, automation/management efficiencies, enterprise, and AI/ML solutions.\nBe able to develop and showcase real world examples of how AI technology can help businesses thrive and solve problems.\nExcellent presentation skills ability to value-sell and deliver engaging workshops to both technical and non-technical audiences on AI and/or infrastructure topics.\nPreferred Qualifications:\nBachelor's Degree in Computer Science, Computer Engineering, Electrical Engineering, or related field. Advanced degree in Data Science is a plus.\nExperience with AI relevant infrastructure, including Networking (InfiniBand and RoCE), Storage (FC, IP and scale out) and AI accelerators (GPUs etc).\nIn-depth understanding of AI models, including but not limited to GPT, Llama, Resnet or similar.\nExperienced with data storage and management (SQL, NOSQL, Vector, BigQuery etc) and AI/ML frameworks (scikit-learn, TensorFlow, PyTorch, Jupyter, NIMS and NVAIE etc).\nExpertise in training and fine-tuning AI models on premise or in cloud environments.\nFamiliarity with containerization (k8 etc).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'BigQuery', 'PyTorch', 'scikit-learn', 'NIMS', 'Jupyter', 'NOSQL', 'Vector', 'SQL', 'TensorFlow']",2025-06-13 06:23:50
Software Engineer,NatWest Markets,6 - 11 years,Not Disclosed,['Bengaluru'],"This is an opportunity for a driven Software Engineer to take on an exciting new career challenge\nDay-to-day, you'll be engineering and maintaining innovative, customer centric, high performance, secure and robust solutions\nIt s a chance to hone your existing technical skills and advance your career while building a wide network of stakeholders\nwe're offering this role as associate level\nWhat you'll do\nIn your new role, you'll be working within a feature team to engineer software, scripts and tools, as we'll as liaising with other engineers, architects and business analysts across the platform.\nyou'll also be:\nProducing complex and critical software rapidly and of high quality which adds value to the business\nWorking in permanent teams who are responsible for the full life cycle, from initial development, through enhancement and maintenance to replacement or decommissioning\nCollaborating to optimise our software engineering capability\nDesigning, producing, testing and implementing our working software solutions\nWorking across the life cycle, from requirements analysis and design, through coding to testing, deployment and operations\nThe skills you'll need\nTo take on this role, you'll need a background in software engineering, software design, and architecture, and an understanding of how your area of expertise supports our customers. you'll need minimum 6 years of experience in build & deployment of Bigdata applications using Pyspark, SparkSQL, SparkStreaming in Python.\nyou'll also need:\nProven experience as a Full Stack Developer, particularly in data-driven applications. Design and implement interactive UIs with React or Angular.\nEnsure responsiveness for optimal user experience across devices. Develop server-side applications using Node.js or Python. Manage and optimise SQL and NoSQL databases for efficient data processing. Integrate APIs and data sources for analytical data ingestion.\nCollaborate with data analysts to implement machine learning models. Create and maintain APIs for system communication. Keep API documentation clear and accessible.\nUse Git for version control and work with cross-functional teams. Engage in Agile methodologies, including sprint planning and daily stand-ups.\nWrite automated tests to ensure code quality. Conduct code reviews to uphold coding standards.\nStay updated on industry trends to enhance application functionality. Strong skills in front-end technologies (HTML, CSS, JavaScript) and back-end languages (Node.js, Python).\nFamiliarity with cloud services (AWS, GCP) and containerisation tools (Docker). Good to have Quantexa Certification\nAlign to the organisation s architectural direction and principles, and make sure that the applications and services being built adhere to them.\nLiaise with lead engineers, architects, business analysts and other key stakeholders to understand the objectives and requirements",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software design', 'Front end', 'Version control', 'GIT', 'Coding', 'Machine learning', 'Agile', 'HTML', 'SQL', 'Python']",2025-06-13 06:23:51
Staff Business Applications Engineer,Tekion Corp,8 - 13 years,Not Disclosed,['Bengaluru'],"We are seeking a NetSuite Techno-Functional Lead to architect and own technical solutions across our ERP ecosystem. You will serve as a subject matter expert across Finance, Operations, and cross-functional business systems, leading hands-on development, process optimization, reporting, and integration efforts. This role is ideal for someone who thrives in a fast-paced SaaS environment and is excited to solve complex problems through smart automation and scalable solutions.\n  Key Responsibilities",,,,"['Procurement', 'System testing', 'ERP', 'Financial reporting', 'Machine learning', 'User acceptance testing', 'Automotive', 'Reporting tools', 'SQL', 'Data extraction']",2025-06-13 06:23:53
Director of Software Engineering,Toast,15 - 20 years,Not Disclosed,['Bengaluru'],"Foster a positive, inclusive, and respectful team environment that encourages innovation, collaboration, and professional growth, aligning with Toast values.\nCollaborating with our Global AI data team leaders, define and drive the technical vision, strategy, and roadmap for the AI Data team, aligning with Toast s business goals.\nStay at the forefront of the latest innovations in AI, machine learning, and data technologies.\nProvide hands-on technical direction and articulate strong, well-reasoned opinions on architecture, design, and technology choices.\nOversee the end-to-end lifecycle of AI and data projects, from ideation and design through development, deployment, and maintenance.\nImplement and manage robust project management processes to ensure timely delivery of high-quality solutions.\nCollaborate closely with product leaders, business stakeholders, and other engineering teams to understand requirements, define priorities, and deliver impactful solutions.\nDrive continuous improvement in engineering culture and hygiene, processes, tools, and technologies used by the team.\nDo you have the right\ningredients*\n( Qualifications):\nBachelors in Computer Science, Computer Engineering, Data Science, AI, or a closely related field. A Masters degree is a plus.\n15+ years of experience in software engineering, with at least 5+ years in a leadership role overseeing AI/ML/Data engineering teams.\nProficiency in programming languages commonly used in AI/Data/Software engineering.\nDeep experience with tools and best practices for developing model deployment pipelines\nExperience with microservice-based architecture, preferably with AWS tooling (SageMaker, DynamoDB, Athena, etc.)\nStrong competency with the following languages (Java/Kotlin, Python), ML frameworks (scikit-learn, Tensorflow, PyTorch) and distributed computing frameworks (Spark, Ray, Dask)\nExperience in software engineering best practices and tools including object-oriented programming, test-driven development, CI/CD, git, shell scripting, task orchestration (Airflow)\nA proven track record of shipping machine learning solution in production environments at scale\nExceptional skills in communication, presentation, and interpersonal abilities.\nStrong problem-solving and analytical skills.\nHigh degree of emotional intelligence and empathy.\nBonus:\nPassionate about the restaurant and hospitality industry.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Hospitality', 'orchestration', 'GIT', 'Project management', 'Machine learning', 'Shell scripting', 'test driven development', 'Continuous improvement', 'Python']",2025-06-13 06:23:54
Software Engineering Senior Analyst,ManipalCigna Health Insurance,2 - 6 years,Not Disclosed,['Hyderabad'],"Software Engineering Senior Analyst - HIH - Evernorth\nAbout Evernorth:\nEvernorth Health Services, a division of The Cigna Group (NYSE: CI), creates pharmacy, care, and benefits solutions to improve health and increase vitality. We relentlessly innovate to make the prediction, prevention, and treatment of illness and disease more accessible to millions of people.\n\nPosition Summary:\nCigna, a leading Health Services company, is looking for Data Engineers in our Engineering Enablement Office (EEO) organization. The Data Engineer is responsible for the delivery of test data business need starting from understanding the data requirements to manufacturing test data for a work initiative. This role requires you to be fluent in some of the critical technologies with proficiency in others and have a hunger to learn on the job and add value to the business. Critical attributes of being a Data Engineer, among others, is Ownership Accountability. In addition to Delivery, the Data Engineer should have an automation first and continuous improvement mindset.\nDescription Responsibilities:\nThe Data Engineer will be responsible for determining the best approach to create test data. It includes account, enrolment, claims, and provider setup applications. This team member must have the ability to engage in test data requirements analysis with Integration Solution Manager (ISM) and Quality Engineer (QE) teams. This team player will also be responsible to collaborate with ISM QE team to explore opportunities to automate test data setup/mining processes.\nResponsible for test data creation (data manufacturing)\nUnderstands various back end and front-end architecture components required for job executions.\nDetermines priorities for test data creation , validation triage.\nUnderstand test data mapping with test scenarios.\nManage test data catalog and self-service mining tools.\nManage data cleanup activities, renewal identification and planning.\nDeveloping subject matter expertise and building knowledge of supported applications\nAnalyzing and communicating test data challenges and risks effectively to identify practical solutions.\nCompleting work governed by best practices, standards and processes and continuously learning about Agile to effectively integrate best practices into delivery activities.\nBe fluent in particular areas and have proficiency in many areas.\nHave a passion to learn.\nTake ownership and accountability.\nUnderstands when to automate and when not to.\nHave a desire to simplify.\nBe entrepreneurial / business minded.\nHave a quality mindset, not just code quality but also to ensure ongoing data quality by monitoring data to identify problems before they have business impact.\nTake risks and champion new ideas.\nExperience Required:\n4+ years being part of Agile teams\n2+ years of experience in a Test data account, enrollment, claims, and provider setup in healthcare domain\n2+ years of experience in Healthcare\nExperience Desired:\n3-6 years of IT experience in similar role\nAbility to analyse, interpret, and organize large amounts of data.\nProblem-solving, and analytical skills\nTime Management skills\nCigna application flow and business knowledge\nEducation and Training Required:\nKnowledge and/or experience with Health care information domains is a plus.\nComputer science - Good to have\nPrimary Skills:\nAdvance SQL knowledge\nJira (Sprint/Kanban)\nConfluence for documentation\nProgramming Logic and Algorithms\nVBA (Excel Macros)\nBuild and maintain integrations with data sources and APIs\nStrong knowledge of database systems, data modeling techniques, and SQL proficiency\nAutomation Skills\nEfficient at least in one Programming language (Python/Java) or Scripting language (JavaScript)\nExpertise at least in one Automation Framework [Robot Framework /Behave / Pytest Framework/ BDD Cucumber / TestNG / Cypress\nGood Exposure on integrating the Test Automation Reporting with CI Tools (Jenkins/Azure)\nHands on experience on the platforms [ GUI Automation Non-GUI Automation [ DB / API / MQ ( Kafka)\nTableau data catalogue and Dashboard (Good to have)\nProficiency with ETL tools commonly used in data engineering (like Databricks)\nAutomation in Cloud (experience a plus)\nAI Machine Learning (experience a plus)\nAdditional Skills:\nExcellent troubleshooting skills\nStrong communication skills\nWork in an agile CI/CD environment (Jenkins experience a plus)\nFamiliarity with cloud platforms and services (like AWS, Azure)",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data modeling', 'Javascript', 'Agile', 'Healthcare', 'Troubleshooting', 'Macros', 'Monitoring', 'SQL', 'Python']",2025-06-13 06:23:56
Senior Software Engineer - Frontend,Poppulo,3 - 6 years,Not Disclosed,['Bengaluru'],"Introduction:\n\nAre you searching for an opportunity to play a key role in driving the dramatic growth of a highly successful software company?\n\nAt Poppulo, we re working on what s next in communications and workplace technology. As a pioneer in this industry, we understand that meaningfully reaching every employee is hard. And so is managing office space in a hybrid world. And so is improving the customer and guest experience. We exist to make each of these things easier. We exist to bring harmony to our customers.\n\nAnd we do that at enterprise scale. Our omnichannel employee communications, customer communications, and workplace experience platform is trusted by over 6,000 organizations today, reaching more t\\han 35M employees and delivering content to 500,000+ digital signs.\n\nWe know there s no such thing as a perfect"" candidate - we re all a work in progress and are growing new skills and capabilities all the time. We encourage you to apply for a position with Poppulo even if you don t meet 100% of the requirements. We believe in fostering an environment where there is a diversity of perspectives, in hopes that we can all thrive.\n\nThe Opportunity\n\nWe are seeking a talented Senior Software Engineer to join our Digital Signage Cloud Team, focused on frontend development. Our cloud-based digital signage platform empowers customers globally to manage and deliver multimedia content seamlessly across various displays. You will lead frontend development efforts, utilizing React, TypeScript, and Redux to create responsive, intuitive, and accessible user experiences. Backend experience with NodeJS and AWS serverless services is beneficial. A keen interest or practical experience in Artificial Intelligence, particularly Large Language Models (LLMs), generative AI, or agentic coding tools, is highly desirable.\n\nKey Responsibilities:\n\n\nDesign, develop, and maintain intuitive and responsive frontend applications using React, Redux, and TypeScript.\n\nCollaborate with backend developers, architects, and product managers to ensure cohesive and seamless integration.\n\nEnsure accessibility and responsiveness across diverse devices and platforms.\n\nImplement frontend performance optimizations and maintain high-quality standards for UX/UI.\n\nUtilize best practices in modern frontend development, including modular architecture, testing, and CI/CD.\n\nMentor junior developers and conduct code reviews, promoting strong coding standards.\n\nEngage actively in agile development practices, contributing to sprint planning, stand-ups, and retrospectives.\n\nExplore opportunities to integrate AI and generative AI capabilities into the frontend experience.\n\n\nSkill & Experience Requirements:\n\n\nBachelors degree in computer science or related discipline; masters preferred.\n\n6+ years in software development, primarily frontend.\n\nProven track record in developing and optimizing large-scale frontend applications.\n\nExperience with cloud-based software platforms and SaaS solutions.\n\nDemonstrable experience or strong interest in AI integration and generative technologies.\n\nExpertise in frontend development (React, Redux, TypeScript).\n\nSolid understanding of web technologies (HTML5, CSS3, responsive design principles).\n\nFamiliarity with modern UI frameworks (Material UI, Tailwind CSS, or similar).\n\nExperience with frontend testing frameworks (Jest, React Testing Library, Cypress).\n\nKnowledge of AWS cloud services and integration with serverless backend architectures (beneficial).\n\nKnowledge of WCAG standards and accessibility best practices.\n\nInterest or experience in AI technologies, especially generative AI tools (LLMs, Cursor, Windsurf).\n\n\nWho We Are\n\nWe are a values-driven organization that encourages our employees to bring their authentic selves to work every day and empowers everyone to make a tangible impact on our products, clients, and culture. We offer a dynamic environment with driven, fun, and flexible individuals who thrive on challenge and responsibility. This is an opportunity to contribute to our culture and join a company that s on the move.\n\nWe live the Poppulo values each day, as they are key to everything we do.\n\n\nBring Your Best Self\nWe show up authentically, are self-aware and always strive to be better.\n\nSee it. Own it. Solve it.\nWe proactively innovate and solve for our customers and each other. We set an example with high standards for our work. We foster a culture of learning, acknowledging our successes and our failures.\n\nTogether We re Better\nWe value and celebrate our diversity. We learn from others, respecting their expertise, and focus on building trust. Thats what makes us a team.\n\n\nNamed a Great Place to Work in 2015, 2016, 2017, 2018, 2019, 2020, and 2021, we are a fast-growing global technology company, with offices in Ireland, the US, and the UK.\n\nPoppulo is an equal opportunity employer.\n\nNamed a Great Place to Work in 2015, 2016, 2017, 2018, 2019, 2020, and 2021, we are a fast-growing global technology company, with offices in Ireland, the US, and the UK.\nPoppulo is an equal opportunity employer.\nWe are committed to protecting your privacy. For details on how we collect, use, and protect your personal information, please refer to our Job Applicant Privacy Policy.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Backend', 'Front end', 'Coding', 'Cloud Services', 'Artificial Intelligence', 'Manager Technology', 'AWS', 'CSS3', 'Testing']",2025-06-13 06:23:58
AI/ML Engineer,Aptos Labs,1 - 5 years,Not Disclosed,['Bengaluru'],"Making a career change is a big decision. Why consider Aptos?\nBecome a part of a team that is passionate about creating and delivering cutting-edge solutions for retailers worldwide. At our company, we re dedicated to supporting your career aspirations and helping you exceed your goals. You ll benefit from industry-leading training, global development opportunities, and the chance to collaborate within a diverse culture across our offices in nine countries. Our inclusive culture reflects our purpose: to make a difference for every colleague, every client, every day .\nAs a leading provider of Unified Commerce solutions for retail, our technology empowers top retail brands by optimizing product management, promotions, merchandising, and store operations. With the global shift toward our cloud-native, microservices architecture, opportunities for career growth have never been more exciting. Today, more than 100,000 retail stores in fashion, grocery, footwear, general merchandise, discount, and sporting goods rely on our solutions to generate nearly $2 trillion in annual revenue.",,,,"['Product management', 'Computer science', 'orchestration', 'GCP', 'Analytical', 'MongoDB', 'Forecasting', 'Analytics', 'SQL', 'Python']",2025-06-13 06:24:00
Senior Engineer II - System Engineer,Alphawave Semi,5 - 8 years,Not Disclosed,"['Pune', 'Bengaluru']","The Opportunity\n\nWere looking for the Wavemakers of tomorrow.\nSystem Engineer\nWe are looking for a System Modelling Engineer to lead our work on developing high-level models of complex systems for simulation, verification and Emulation purposes. You will be responsible for creating, testing, and maintaining C/C++/SystemC models of hardware and software components, as well as integrating them into larger system models. You will also collaborate with other engineers and stakeholders to ensure the accuracy and quality of the models and their alignment with the specifications and requirements. You will also mentor and guide junior engineers and contribute to the improvement of the modelling and simulation processes and tools.\nWhat Youll do:\nCreate virtual models of our connectivity IPs using SystemC/C++ and C libraries\nTest and debug the models using SystemC simulation tools and frameworks\nIntegrate models into emulation and verification environments to enable early software development as well perform functional simulations.\nWork on Industry Emulation platforms such as Zebu, Palladium, Veloce\nIntegrate the models into virtual platforms and verify their functionality and performance\nMaintain and update the models according to the changes in the design and requirements\nDocument the models and their interfaces and provide support for their usage and integration\nCollaborate with other engineers and stakeholders to ensure the consistency and quality of the models and their alignment with the specifications and requirements\nContribute to the improvement of the modelling and simulation processes and tools and propose new solutions and best practices.\nWhat Youll need:\nBachelors degree or higher in Computer Science, Computer Engineering, Electrical Engineering, or related field\n5-8 years of experience in SystemC/C++/C modelling and simulation\nExposure to Emulation tools such Palladium, Zebu, Veloce.\nFamiliar with SystemC simulation tools and frameworks, such as TLM2.0, and UVM-SystemC\nKnowledge of hardware and software design and verification methodologies and standards\nExperience in modelling and simulating complex systems, such as embedded systems, SoCs, or ASICs\nExcellent communication and teamwork skills\nStrong leadership and mentoring skills\nAttention to detail and quality-oriented mindset\nWe have a flexible work environment to support and help employees thrive in personal and professional capacities\nAs part of our commitment to the well-being and satisfaction of our employees, we have designed a comprehensive benefits package that includes:\nCompetitive Compensation Package\nRestricted Stock Units (RSUs)\nProvisions to pursue advanced education from Premium Institute, eLearning content providers\nMedical Insurance and a cohort of Wellness Benefits\nEducational Assistance\nAdvance Loan Assistance\nOffice lunch & Snacks Facility\nEqual Employment Opportunity Statement\nAlphawave Semi is an equal opportunity employer, welcoming all applicants regardless of age, gender, race, disability, or other protected characteristics. We value diversity and provide accommodations during the recruitment process.",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Wireless', 'Computer science', 'C++', 'Software design', 'Simulation', 'Networking', 'Product innovation', 'Artificial Intelligence', 'Data communication', 'UVM']",2025-06-13 06:24:01
"Senior Software Engineer, Supply Chain and Retail Technology",Wayfair,12 - 17 years,Not Disclosed,['Bengaluru'],"Candidates for this position are preferred to be based in Bangalore, India and will be expected to\ncomply with their teams hybrid work schedule requirements.\nWe are looking for a passionate Backend Software Engineer to join the Fulfilment Optimisation team. This team builds the platforms that determine how customer orders are fulfilled, optimising for Wayfair profitability and customer delight. A big part of our work revolves around enhancing and scaling customer-facing platforms that provide fulfillment information on our websites, starting at the top of the customer funnel on the search pages all the way through orders being delivered. Throughout this customer journey, we are responsible for maintaining an accurate representation of our dynamic supply chain, determining how different products will fit into boxes, predicting how these boxes will flow through warehouses and trucks, and ultimately surfacing the information our customers need to inform their decision and the details our suppliers and carriers require to successfully execute on the promises made to our customers. We do all of this in milliseconds, thousands of times per second.\nWhat You ll Do:\nPartner with your business stakeholders to provide them with transparency, data, and resources to make informed decisions\nBe a technical leader within and across the teams you work with.\nDrive high impact architectural decisions and hands-on development, including inception, design, execution, and delivery following good design and coding practices.\nObsessively focus on production readiness for the team including testing, monitoring, deployment, documentation and proactive troubleshooting.\nIdentify risks and gaps in technical approaches and propose solutions to meet team and project goals.\nCreate proposals and action plans to garner support across the organization.\nInfluence and contribute to the team s strategy and roadmap.\nTenacity for learning - curious, and constantly pushing the boundary of what is possible.\nWe Are a Match Because You Have:\nA Bachelor s Degree in Computer Science or a related engineering discipline\nAt least 12 years of experience in a senior engineer or technical lead role.\nShould have mentored 10-12 people.\nExperience developing and designing scalable distributed systems with deep understanding of architectural and design patterns, object oriented design, modern\nprogram languages.\nExcellent communication skills and ability to work effectively with engineers, product managers, data scientists, analysts and business stakeholders.\nPassion for mentoring and leading peer engineers.\nExperience designing APIs and micro services.\nExperience working on cloud technologies specifically GCP is a plus.\nDeep understanding of data processing and data pipelines.\nCommon open source platforms, tools and framework, eg: Kafka, Kubernetes, Containerization, Java microservices, GraphQL APIs, Aerospike etc\nDesigning and developing recommendation systems and productionalizing ML models for real time decisions, large-scale data processing and event-driven systems and technologies is a plus.\n.",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Computer science', 'Object oriented design', 'Backend', 'Coding', 'Troubleshooting', 'Open source', 'Distribution system', 'Monitoring', 'Logistics']",2025-06-13 06:24:03
Senior Software Engineer - IT,Infobahn Softworld,5 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Description:\n\nACCOUNTABILITIES: Designs, codes, tests, debugs and documents software according to Dell s systems quality standards, policies and procedures. Analyzes business needs and creates software solutions. Responsible for preparing design documentation. Prepares test data for unit, string and parallel testing. Evaluates and recommends software and hardware solutions to meet user needs. Resolves customer issues with software solutions and responds to suggestions for improvements and enhancements. Works with business and development teams to clarify requirements to ensure testability. Drafts, revises, and maintains test plans, test cases, and automated test scripts. Executes test procedures according to software requirements specifications Logs defects and makes recommendations to address defects. Retests software corrections to ensure problems are resolved. Documents evolution of testing procedures for future replication. May conduct performance and scalability testing. RESPONSIBILITIES: Leads small to moderate budget projects; may perform in project leadership role and/or may supervise the activities of lower level personnel. Provides resolutions to a diverse range of complex problems. Executes schedules, costs and documentation to ensure assigned projects come to successful conclusion. May assist in training, assigning and checking the work of less experienced developers. Performs estimation efforts on projects and tracks progress. Drafts and revises test plans and scripts with consideration to end-to-end system flows. Executes test scripts according to application requirements documentation. Logs defects, identifies course of action and performs preliminary root cause analysis. Analyzes and communicates test results to project team.\nDescription Comments\nAdditional Details\nDescription Comments : Skills: Python, PySpark and SQL 5 years of experience in Spark, Scala, PySpark for big data processing Proficiency in Python programming for data manipulation and analysis. Experience with Python libraries such as Pandas, NumPy. Knowledge of Spark architecture and components (RDDs, DataFrames, Spark SQL). Strong knowledge of SQL for querying databases. Experience with database systems like Lakehouse, PostgreSQL, Teradata, SQL Server. Ability to write complex SQL queries for data extraction and transformation. Strong analytical skills to interpret data and provide insights. Ability to troubleshoot and resolve data-related issues. Strong problem-solving skills to address data-related challenges Effective communication skills to collaborate with cross-functional teams.Role/Responsibilities: Work on development activities along with lead activities Coordinate with the Product Manager (PdM) and Development Architect (Dev Architect) and handle deliverables independently Collaborate with other teams to understand data requirements and deliver solutions. Design, develop, and maintain scalable data pipelines using Python and PySpark. Utilize PySpark and Spark scripting for data processing and analysis Implement ETL (Extract, Transform, Load) processes to ensure data is accurately processed and stored. Develop and maintain Power BI reports and dashboards. Optimize data pipelines for performance and reliability. Integrate data from various sources into centralized data repositories. Ensure data quality and consistency across different data sets. Analyze large data sets to identify trends, patterns, and insights. Optimize PySpark applications for better performance and scalability. Continuously improve data processing workflows and infrastructure.\nNot to Exceed Rate : (No Value)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Root cause analysis', 'Test scripts', 'data manipulation', 'Postgresql', 'SCALA', 'Data processing', 'Data quality', 'Test cases', 'Python', 'Data extraction']",2025-06-13 06:24:05
Computer Vision Engineer,Zen Technologies,1 - 3 years,Not Disclosed,['Hyderabad'],"\\u200b\nJobTitle: ComputerVision Engineer\nLocation: Gachibowli\nExperience: 1-3 years\nEmployment Type: Full-time\nAbout the Role\nWe arelooking for a passionate and skilled Computer Vision Engineer to joinour cutting-edge simulation and defense technology team. This role isinstrumental in building next-generation simulation environments thatincorporate AI, computer vision, and immersive sensory realism for militarytraining and war-gaming applications.\nKey Strategic Objectives\nDevelop modular AI/ML-enhanced simulation blocks to support complex virtual training scenarios.\nBuild real-time Computer Vision/Image Processing capabilities for enhanced situational awareness.\nImprove sensory realism \\u2014including vision, sound, and motion\\u2014to deliver truly immersive experiences.\nEnable adaptive learning and behavioral modeling for intelligent virtual entities.\nContribute to the realization of a Military War Room / Gaming Command Centre with dynamic, data-driven capabilities.\nKey Responsibilities\nDesign and implement AI/ML models tailored for simulation use-cases.\nIntegrate image processing and computer vision techniques into real-time simulation pipelines.\nCollaborate with domain experts to translate training and tactical requirements into virtual environments.\nOptimize system performance for real-time execution and immersive responsiveness.\nPrototype and deploy modules involving visual perception, motion prediction, and behavioral logic.\nRequirements\nEssential:\nStrong mathematical foundation in Linear Algebra , Calculus , Probability , and Optimization .\nProficiency in Python and/or C/C++ for simulation and ML development.\nDemonstrated understanding of AI/ML algorithms and computer vision techniques.\nDesirable:\nExperience with OpenCV , YOLO , SLAM , depth estimation techniques.\nFamiliarity with NVIDIA NVAPI , CUDA programming, or real-time graphics APIs.\nBackground in developing simulations or real-time systems in gaming, defense, or robotics domains.\nWhat We Offer\nOpportunity to work on impactful, mission-critical defense simulation projects.\nA collaborative team of engineers, data scientists, and defense experts.\nContinuous learning and access to the latest technologies in AI, ML, and simulation.\n\\u200b",Industry Type: Aviation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Graphics', 'Training', 'Computer vision', 'C++', 'Prototype', 'Simulation', 'Image processing', 'Gaming', 'Robotics', 'Python']",2025-06-13 06:24:06
Senior Software Engineer,Movate Technologies,3 - 5 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\nJob Description: Salesforce Senior Engineer\nWe are seeking a highly skilled and experienced Salesforce Senior Engineer to join our dynamic team. The ideal candidate will be responsible for designing, developing, and implementing advanced Salesforce solutions, ensuring seamless integration with other enterprise systems. This role requires a deep understanding of Salesforce architecture, best practices, and the ability to lead complex projects.\nKey Responsibilities",,,,"['Integration', 'Artificial Intelligence', 'Salesforce.com', 'Data Cloud', 'Apex', 'Agentforce', 'Einstein Analytics', 'SFDX', 'Lwc', 'Experience Cloud', 'Salesforce']",2025-06-13 06:24:08
Software Engineer,NetApp,12 - 22 years,Not Disclosed,['Bengaluru'],"Job Summary\nAs a Principal Engineer in NetApp’s Cloud Service organization, you will lead and execute our most challenging and complex projects. You will be responsible for decomposing complex problems into simple solutions, understanding system interdependencies and limitations, cloud architectures, performance, scalability, enterprise system architecture, engineering best practices, and testing methods.\n \nYou'll be partnering with our Distinguished Engineers and other leaders in the cloud storage services team, writing/reviewing code, mentoring and developing others, influencing multiple NetApp teams, and making critical technical decisions. You will be crucial in helping NetApp stay ahead of the curve and deliver innovative solutions to our customers.",,,,"['c#', 'kubernetes', 'fullstack development', 'faas', 'software', 'golang', 'scale', 'distributed architecture', 'microservices', 'nosql', 'cloud', 'cloud architecture', 'saas applications', 'saas', 'paas', 'full stack', 'relational db', 'software engineering', 'object', 'communication skills', 'system engineering', 'architecture']",2025-06-13 06:24:10
Gen AI Engineer ( immediate To 15 days NP only),Sais It Services,5 - 10 years,15-27.5 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities :\nStrong problem solving skills\nGenerative AI ( Langchain, Open AI)\nAdvance level of python and javascript\nPython libraries like FastAPI , pytest\nGit\nReact JS",,,,"['Gen AI', 'Fast Api', 'Python', 'Basic Linux commands', 'Azure/GCP', 'Advance level of python and javascript']",2025-06-13 06:24:11
Job | AI/ML Engineer | mokSa Technologies,Moksa Technologies,5 - 10 years,Not Disclosed,"['Bengaluru', 'Delhi / NCR', 'Mumbai (All Areas)']","Role & responsibilities\nA results-driven AI/ML Engineer with a strong foundation in machine learning, deep learning, and data science. Skilled in designing, developing, and deploying intelligent solutions that solve real-world problems across domains such as customer experience, natural language processing, and predictive analytics. Proficient in Python, TensorFlow, PyTorch, and cloud platforms (AWS, GCP, Azure). Experienced in building scalable models, fine-tuning large language models (LLMs), and applying AI to optimize user engagement and business outcomes.\nDesign and implement machine learning models for classification, regression, clustering, and NLP tasks.\nBuild data pipelines for training and inference using structured and unstructured data.\nFine-tune pre-trained models (e.g., BERT, GPT, LLaMA, Whisper, RNNoise) for domain-specific tasks.\nDevelop APIs and interfaces to serve AI/ML models at scale (e.g., using FastAPI, Flask, TensorFlow Serving).\nCollaborate with data engineers to ensure data availability, quality, and consistency.\nConduct A/B testing, model validation, and monitor model performance in production.\nResearch and integrate state-of-the-art ML and deep learning techniques.\nWork with cloud platforms (AWS/GCP/Azure) and MLOps tools for model deployment and lifecycle management.\nFamiliarity with the customer experience domain is a strong advantage\n5-10 years of experience.\n\nInterested candidates are requested to share their updated CV at deepak.singh@moksatechnologies.com along with the following details:\nTotal Experience:\nCurrent CTC:\nExpected CTC:\nCurrent Location:\nNotice Period:",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Natural Language Processing', 'Tensorflow', 'Pytorch', 'Neural Networks', 'Bert', 'Deep Learning', 'Python']",2025-06-13 06:24:13
Senior AI/ML Engineer (Generative AI Expertise),Vee Healthtek,5 - 10 years,Not Disclosed,"['Salem', 'Chennai', 'Bengaluru']","Note : 5 days work from Office. Atleast one round must be F2F.\n\nJob Summary: We are seeking a highly skilled Senior AI/ML Engineer with extensive experience in Generative AI to lead the design, implementation, and optimization of advanced AI/ML solutions. The ideal candidate will serve as a subject matter expert (SME), mentor a team of AI/ML specialists and collaborating with cross-functional teams to deliver impactful solutions.\n\nMust-Have Skills:",,,,"['Pytorch', 'Tensorflow', 'Generative Ai', 'Pandas', 'Numpy', 'Keras']",2025-06-13 06:24:15
Java (Backend) Engineer,Service based Top B2C/B2B MNC in Analyti...,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Job Title: Java (Backend) Engineer\nLocation: Bangalore & Chennai\nWork Mode: Hybrid (Work from Client Office)\nJob Description:\nWe are looking for a Java Backend Engineer to join our dynamic team. The ideal candidate will have solid experience in back-end development using Java, Microservices architecture, and Spring Boot framework. You will be responsible for developing robust server-side logic, building scalable APIs, and ensuring application performance and responsiveness.\nMust-Have Skills:\nStrong experience in Java programming (Back-end Server & SDK Development)\nHands-on experience with Spring Boot, Microservices, and RESTful APIs\nFamiliar with tools such as Git, Maven, and Jenkins\nSolid understanding of system reliability, availability, scalability, and performance\nBasic working knowledge of SQL\nGood to Have:\nExposure to database design and optimization\nKnowledge of front-end basics and mobile-responsive design\nRoles and Responsibilities:\nControl all technical, functional, and visual aspects of software in development\nDesign and develop robust, scalable server-side architecture\nDevelop and maintain well-functioning databases and back-end logic\nWrite, test, and maintain REST APIs\nTroubleshoot, debug, and upgrade applications\nOptimize software for performance and responsiveness\nCollaborate with frontend developers, analysts, and data scientists\nImplement security and data protection best practices\nCreate and maintain technical documentation\nEducation:\nBachelors degree in Computer Science, Software Engineering, MIS, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spring Boot', 'Microservices', 'Restfull Api', 'Msql', 'Orcale']",2025-06-13 06:24:16
Legal - Intern,Coditas Technologies,0 years,Not Disclosed,['Pune( Viman Nagar )'],"Company Introduction\n\nCoditas is an offshore product development organization comprised of passionate engineers, design thinkers, data scientists, cloud professionals, and other top industry professionals. We offer services spanning the entire length and breadth of software development, including cutting-edge technologies such as Artificial Intelligence, Machine Learning, and Generative AI. With over 200 clients worldwide, we are partners with multi-billion dollar and Fortune 500 clients such as JPMorgan Chase, BCG, KPMG, Reliance, HDFC, IDFC, SunPharma, and many more. Coditas has experienced fast-paced growth thanks to an engineering-driven culture and steadfast philosophies around writing clean code, designing intuitive user experiences, and letting the work speak for itself.\n\nRoles and Responsibilities\n\nDocument Review: Conduct thorough reviews of legal documents to ensure accuracy, compliance,\nAgreements Abstract: Summarize and extract key terms and conditions from contracts and agreements for easy reference and analysis\nData Maintenance: Maintain and update legal databases and records to ensure data integrity and accessibility for ongoing legal processes.\nDrafting of Agreements: Prepare and draft various legal agreements, ensuring they are legally sound and tailored to meet specific business needs and requirements.\nData Protection Compliance: Assist in reviewing, updating, and drafting privacy notices, data processing agreements, and internal policies to ensure alignment with DPDPA and GDPR requirements.\n\nTechnical Skills\n\nAnalytical and research skills\nSearching for information online and offline\nCreating and delivering presentations\nUnderstanding of Data Protection Laws: Basic understanding of data privacy principles under the DPDPA (India) and GDPR (EU); prior academic exposure or certifications would be an advantage\nExperience Range: 0 to 1 years\nDegree: Completed Five or Three years of Law Degree from a government-recognized University in Law.\nGood communication skills written and verbal.\nA positive attitude toward your job in general.\nKnowledge of MS Office, Google Office suites\nAptitude for numbers and strong problem-solving and analytical skills\nAbility to work effectively with people at all levels of the organization\n\n\nA full-time employment opportunity may be extended upon successful completion of the six-month internship, subject to satisfactory performance.",Industry Type: IT Services & Consulting,Department: Legal & Regulatory,"Employment Type: Full Time, Permanent","['Agreement reviewing', 'Data Privacy', 'Gdpr', 'Drafting', 'Legal Documentation', 'Contract Drafting', 'DPDPA', 'Contract Review', 'Alternative Dispute Resolution', 'Drafting Agreements']",2025-06-13 06:24:18
Fullstack Python Developer,Fia Technology Services,0 - 3 years,4-7 Lacs P.A.,['Gurugram'],"Role & responsibilities :\nDesign, develop, test, and maintain Python-based applications.\nBuild RESTful APIs and integrate with third-party services.\nWork with databases (e.g., PostgreSQL, MongoDB, MySQL) for data modeling and queries.\nWrite clean, maintainable, and well-documented code.\nCollaborate with frontend developers, DevOps, and QA teams.\nParticipate in code reviews and technical discussions.",,,,"['Python', 'Rabbit MQ', 'GIT', 'Postgresql', 'Django', 'MySQL', 'Kafka', 'Mysql Database', 'MongoDB', 'Flask']",2025-06-13 06:24:19
News & Video Content Creator,Lexiconn Content Services,0 - 2 years,Not Disclosed,[],"LexiConn is seeking an experienced and highly skilled Content & Video Ops Writer who will contribute in curating content for our client in secondment which is an artificial intelligence based product company that delivers personalized content to the lock screens of smartphones.\n\nWhat will you be doing\n\nWrite and edit high-quality news content across platforms with a focus on videos.\nWork on new-age content creation tools for content and video.\nIdeate on fresh content campaigns on a daily basis.\nPlan and execute marquee campaigns and be flexible around specific content needs.\nWork and develop cross-content campaigns across formats - video, text, multimedia.\nInnovate content deliverables and work on intersections of content and commerce.\nDevelop new ways of delivering video content to highly active users.\nWork to enhance the quality of our content within tight timelines.\nWork on AI-supported tools and an evolving CMS with skill and agility\nSimplify communication and rework copy (as needed) for varying demographics.\nEdit copy for varying lengths and adhere to various platform-specific guidelines.\nEnsure alignment of content to brand requirements and tonality.\nWhat is expected out of you\n\nHave a deep knowledge of content and news categories\nHave an experience of 0.5-2 years working in a newsroom.\nBe comfortable across categories - hard news, entertainment, lifestyle, etc\nHave a keen grasp on content needs for the Indian audience.\nKnow how to write and what language to use for news and personalised content.\nTake ownership and deliver specific content campaigns, bespoke to the needs of the content partner.\nBe snappy with your copy.\nKnow what image will best describe your story.\nAmenable to work on any given day, including weekends and holidays (rotational shifts), to address the news cycle we'll.\nFit in 365-day operations and ready to work in varying shifts. (5-day working week for candidates on shift basis)\nOutstanding in your ability to use language and grammar we'll, without being verbose, for a new-age content platform.\nPassionate about working in a fast-paced environment, where role dynamics, expectations and work nature can evolve almost on a quarterly basis.\nComfortable with using basic video and image editing software to process basic multimedia work.\nFantastic at time management and deeply committed to meeting deadlines.\nA team-player with exceptional interpersonal skills, who is always willing to get your hands dirty, and get the job done.\nWork deeply with cross-functional teams to deliver the best results.\nBenefits\nExposure to AI-first content platforms\nLearning and growth",Industry Type: Advertising & Marketing,"Department: Content, Editorial & Journalism","Employment Type: Full Time, Permanent","['Rework', 'Basic', 'Image editing', 'Usage', 'Interpersonal skills', 'Time management', 'Artificial Intelligence', 'Focus', 'CMS', 'Commerce']",2025-06-13 06:24:21
Sports Writer,Lexiconn Content Services,0 - 2 years,Not Disclosed,[],"LexiConn is seeking an experienced and highly skilled Sports & Ops Writer who will contribute in curating content for our client in secondment which is an artificial intelligence based product company that delivers personalized content to the lock screens of smartphones.\n\nWhat will you be doing\nWrite and edit high-quality multimedia sports and news content across platforms.\nWork on new-age content creation tools.\nIdeate on fresh sports content and content campaigns on a daily basis.\nPlan and execute marquee sports events like IPL, World Cups etc\nWork and develop cross-content campaigns across formats - video, text, multimedia.\nInnovate content deliverables and work on intersections of content and commerce.\nDevelop new ways of delivering content to highly active users.\nWork to enhance the quality of content within tight timelines.\nWork on AI-supported tools and an evolving CMS with skill and agility.\nSimplify communication and rework copy (as needed) for varying demographics.\nEdit copy for varying lengths and adhere to various platform-specific guidelines.\nEnsure alignment of content to brand requirements and tonality.\nMaintain accuracy amidst fast-paced sports operations.\nWhat is expected out of you\nHave a deep knowledge of sports and news content categories.\nHave an experience of 0.5-2 years working in a newsroom.\nHave a keen grasp on cricket, football, tennis and Indian sports.\nStays updated about the current news and sports developments.\nwe'll-versed with where to look for content and aware about the who\\u2019s who.\nKnow how to write for news and sports and can be snappy with copy.\nKnow what image will best describe your story.\nAmenable to work on any given day, including weekends and holidays, to address the news cycle we'll.\nFit in 365-day operations and ready to work in varying shifts. [team members are required to work only 5 out of 7 days]\nOutstanding ability to use language and grammar we'll, without being verbose, for a new-age content platform.\nPassionate about working in a fast-paced environment, where role dynamics, expectations and work nature can evolve almost on a quarterly basis.\nComfortable with using basic video and image editing software to process basic multimedia work.\nFantastic at time management and deeply committed to meeting deadlines.\nA team-player with exceptional interpersonal skills, who is always willing to get their hands dirty, and get the job done.\nWork deeply with cross-functional teams to deliver the best results.\nBenefits\nExposure to AI-first content platforms\nLearning and growth",Industry Type: Advertising & Marketing,"Department: Content, Editorial & Journalism","Employment Type: Full Time, Permanent","['Rework', 'Basic', 'Image editing', 'Interpersonal skills', 'Usage', 'Time management', 'Artificial Intelligence', 'CMS', 'Commerce']",2025-06-13 06:24:22
AI/ML Intern,Zuddl,0 - 1 years,Not Disclosed,['Hyderabad'],"Zuddl is a modular platform for events and webinars that helps event marketers plan and execute events that drive growth. Event teams from global organizations like Microsoft, Google, ServiceNow, Zylo, Postman, TransPerfect and the United Nations trust Zuddl. Our modular approach to event management lets B2B marketers and conferences organizers decide which components they need to build the perfect event and scale their event program. Zuddl is an outcome-oriented platform with a focus on flexibility, and is more partner, less vendor..\n\nFUNDING\nZuddl being a part Y-Combinator 2020 batch has raised $13.35 million in Series A funding led by Alpha Wave Incubation and Qualcomm Ventures with participation from our existing investors GrowX ventures and Waveform Ventures.\n\nWhat youll Do\n\nPrototype LLM-powered features using frameworks like LangChain, OpenAI Agents SDK to power content automation and intelligent workflows.\nBuild and optimize Retrieval Augmented Generation (RAG) systems : document ingestion, chunking, embedding with vector DBs, and LLM integration.\nWork with vector databases to implement similarity search for use cases like intelligent Q&A, content recommendation, and context-aware responses.\nExperiment with prompt engineering and fine-tuning techniques\nDeploy LLM-based microservices and agents using Docker, K8s and CI/CD best practices.\nAnalyze model metrics , document findings, and suggest improvements based on quantitative evaluations.\nCollaborate across functions including product, design, and engineering to align AI features with business needs and enhance user impact.\n\n\nRequirement\n\nStrong Python programming skills .\nHands-on with LLMs experience building, fine-tuning, or applying large language models.\nFamiliarity with agentic AI frameworks , such as LangChain or OpenAI Agents SDK (or any relevant tool).\nUnderstanding of RAG architectures and prior implementation in projects or prototypes.\nExperience with vector databases like FAISS, Opensearch etc.\nPortfolio of LLM-based projects , demonstrated via GitHub, notebooks, or other coding samples.\n\nGood to Have\n\nCapability to build full stack web applications .\nData analytics skills data manipulation (Pandas/SQL), visualization (Matplotlib/Seaborn/Tableau), and statistical analysis. Worked with PostgreSQL, Metabase or relevant tools/databases.\nStrong ML fundamentals : regression, classification, clustering, deep learning techniques.\nExperience building recommender systems or hybrid ML solutions.\nExperience with deep learning frameworks : PyTorch, TensorFlow (or any relevant tool).\nExposure to MLOps/DevOps tooling : Docker, Kubernetes, MLflow, Kubeflow (or any relevant tool).\n\nWhy You Want To Work Here\nOpportunity to convert to a Full-Time Role, based on performance and organisational requirements after the end of the internship tenure.\nA culture built on trust, transparency, and integrity\nGround floor opportunity at a fast-growing series A startup\nCompetitive Stipend\nWork on AI-first features in an event-tech startup with global customers\nThrive in a remote-first, empowering culture fueled by ownership and trust",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Web technologies', 'Coding', 'Postgresql', 'Product design', 'SDK', 'microsoft', 'Internship', 'SQL', 'Python']",2025-06-13 06:24:24
Trainee Research Associate,Jubilant Biosys,0 - 2 years,Not Disclosed,['Noida'],"Jubilant Biosys is looking for Trainee Research Associate to join our dynamic team and embark on a rewarding career journey.\n\nAs a Research Associate, you will be responsible for conducting research and analyzing data to support various research projects. Conduct research studies and analyze data using a variety of research methods and tools. Analyze and interpret data using statistical software and other analytical tools. Prepare reports and presentations summarizing research findings and conclusions.",Industry Type: Clinical Research / Contract Research,Department: Other,"Employment Type: Full Time, Permanent","['python', 'data interpretation', 'data analysis', 'sas', 'stata', 'predictive analytics', 'dissolution', 'spss', 'machine learning', 'minitab', 'research', 'hplc', 'excel', 'r', 'clinical research', 'predictive modeling', 'statistical modeling', 'statistical software', 'logistic regression', 'statistics']",2025-06-13 06:24:26
Category Intern,Purplle.com,0 - 1 years,Not Disclosed,['Mumbai'],"In Stock Excellence The Key Accounts Intern is accountable for efficient online availability of the portfolio on their respective brands. This requires her/him to work on demand forecast, customer ordering, fulfillment and go live with the sourcing team.\nExecution Rigor He/ She will be responsible for configuration of offer and its compliance, media assets and content upload across the assortment\nBudgeting She/he must have the ability to work with data comfortably to forecast and budget for optimized brand investments on the respective platforms Financial Hygiene They will be responsible for maintaining collection efficiency, customer claims and other financial metrics for the account\nNegotiations and Operations He/She must have the ability to negotiate both strategically and tactically in an agile fast paced channel\nWorking with collaborative teams The role requires working closely with Shopper Marketing, Customer Service, Demand Planning, Business Finance and Capability Teams to achieve joint ambitions for sustainable customer top-line\nEducational Qualification, Experience Skills:\nGraduate/Masters degree\nExperience in e-commerce set-up will be preferred.\nTechnical aptitude and agility to learn web-based tools\nLooking for talent with - owners mindset, passion and agility\nMicrosoft Excel proficiency will be an add-on.",Industry Type: Beauty & Personal Care,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['insurance', 'python', 'investment banking', 'investment', 'accounting', 'machine learning', 'financial services', 'autocad', 'sales', 'excel', 'financial operations', 'credit cards', 'relationship management', 'pro-e', 'mutual funds', 'wealth management', 'finance', 'communication skills']",2025-06-13 06:24:28
Applied ML Engineer (Stock Broking / BFSI Domain),A Client of TEAM RECRUITERS,4 - 7 years,15-25 Lacs P.A.,['Mumbai( Prabhadevi )'],"• Build ML models, Predictive analytics, Intelligent rule engines & Operational optimization\n• Search & recommendation model, Design engineering pipelines across data types\n• Build reproducible ML pipelines integrated into production systems\n\nRequired Candidate profile\n• Python (Pandas, NumPy, Scikit-Learn, PyTorch/TensorFlow–basic working level)\n• Classification, Regression, Anomaly Detection, Ranking models, Forecasting\n• Windowing, lags, rolling stats, trend",Industry Type: Financial Services (Broking),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Tensorflow', 'Machine Learning', 'Ml', 'Python', 'Pytorch', 'Scikit-Learn']",2025-06-13 06:24:29
AI/ML Engineer/Architect -,Choice Consultants,6 - 10 years,20-35 Lacs P.A.,"['New Delhi', 'Bengaluru']","Automotive, Business Relationship Management, Collaborative Leadership, Communication, Computer Vision, AI, Machine Learning, Team Leadership, Technological Innovation, Proposal",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['C/C++', 'AI/ML', 'PYTHON', 'COMPUTER VISION', 'AIML', 'Medical Imaging', 'DEEP LEARING']",2025-06-13 06:24:31
"Sr. QA Engineer, AI",Conga,5 - 8 years,Not Disclosed,"['Pune', 'Ahmedabad', 'Bengaluru']","Job Title: Sr. QA Engineer\nLocations: Ahmedabad/ Bangalore/ Pune\nReports to: Manager, Quality Engineering\n\nA quick snapshot\n\nAs Senior QA Engineer your responsibilities will include designing and implementing tests, debugging, and defining corrective actions. You will also review system requirements and track quality assurance metrics. These tests entail other tasks such as developing and running new tests and reporting their results to stakeholders, who will collaborate to fix program bugs or problems. You will mentor juniors, collect daily updates, and circulate to managers/ higher forums making this role more important in the system.\n\nWhy its a big deal\n\nA Senior QA Engineer role has significance in the Testing Center of Excellence (TCoE) team at Conga, managing the production of test documents, the creation of test procedures, and ensuring high-quality products. Your expertise in agile methodology, and automation tools, will help in accelerating a continuous enhancement of our product features is a truly Big Deal in Conga Way. Your extensive contribution to scrum teams in the implementation of automation footprints with a Sprint/Release will bring a high-quality impact on Congas products. Your collaboration with cross-functional teams ensures the smooth running of the QA department and ultimately customer satisfaction.\n\nAre you the person were looking for?\n\nProven success in testing (Automation and Manual).Your experiences will include at least 5 years in test case planning, assessments, script development, and maintenance. You have hands-on experience with automation tools and frameworks and developing automation scripts.\n\nSelenium and API. You have expertise with automation tools such as Selenium web driver, frameworks, and developing automation scripts using Java. Strong hands-on experience with API approach using Rest Assured or any such client. Hands-on with test management software such as qTest, JIRA, Jmeter, Load Runner.\n\nAI Technology. You have experience in Large Language model, machine learning experience, AI Git knowledge for Advance Automation as well as familiar with AI Microsoft CoPilot. Candidate should be aware with attorney use cases for variety of documents\n\nAgile Methodology. You are proficient with Agile and a collaborative cross-functional approach to building awesome software. You are comfortable working with teams and collaborating on best practices across multiple Agile teams. You constantly seek opinions and solicit feedback to create the best work possible. You dont know any other way. Its a team effort and you completely appreciate that. Strong experience in software testing lifecycle (STLC) and knowledge of software development lifecycle (SDLC).\n\nEducation. A bachelors degree in engineering or equivalent.\n\nHere’s what will give you an edge\n\nStrong attention to detail. The Conga revenue lifecycle management solution showcases a wide variety of use cases, across multiple regions and languages. As a senior QA paying attention to the smallest details can help identify bugs that others might miss.\n\nStrong testing skills and logic based thinking is your forte. This is an absolute must. Your proven ability to analyze and apply logical thinking to determine the root cause of an issue is fundamental to success in this role. You can easily understand how systems interact/integrate with each other and as well as how changes in one application will affect others.\n\nInitiative. As a Senior QA, we need to own and initiate multiple things to make the quality better. Functional aspects, Non-functional aspects, Broader thinking, Integration approach, Reuse approach in Automation, Performance, Security, Database testing, and a lot more.\n\nAwareness. This role should be aware of the company vision, Goals, and Requirements, and work towards that direction to deliver quality so participation in multiple forums makes it more vital.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['copilot', 'Rest Assured', 'Playwright', 'Selenium With Java']",2025-06-13 06:24:32
Hiring FCT Mentor( Education Loan Team Leader)-Shiksha.com,Info Edge,3 - 7 years,Not Disclosed,['Noida'],"About Info Edge\nInfoEdges mission is to create world-class platforms that transform lives by continuously innovating. Our products and services are built keeping our customers in mind. We always delight our customers by delivering superior value through enhanced offerings on the internet and other platforms. Through our continuous investment across various businesses, especially in cutting-edge technology, machine learning and artificial intelligence (AI), we have built a robust system that constantly increases our predictive powers on customer behaviour, and optimizes and improves our systems. Our various teams tirelessly work together to solve problems, innovate, and create something to empower our customers.",,,,"['Education Loan', 'Client Management', 'Team Handling', 'sales orientation', 'account manager', 'overseas education', 'study abroad', 'loan counsellor']",2025-06-13 06:24:34
"Principal Engineer, Director",NatWest Markets,15 - 20 years,Not Disclosed,['Gurugram'],"This is a challenging role that will see you design and engineer software with the customer or user experience as the primary objective\nWith your software development background, you'll be working with architects to help define major components of the business-wide target architecture and roadmap\nyou'll gain valuable senior stakeholder exposure as we'll as the opportunity to hone your technical talents and leadership skills\nwe're offering this role at director level\nWhat you'll do\nAs a Principal Engineer, you'll be creating great customer outcomes via engineering and innovative solutions to existing and new challenges, and technology designs which are innovative, customer centric, high performance, secure and robust.\nyou'll be leading the more significant, complex and technically challenging assignments, coordinating multiple feature teams, making sure that their technical journeys support realisation of the targets, and deliver the values of the relevant metrics published to our investors.\nyou'll also be:\nDefining, creating and providing oversight and governance of engineering and design solutions with a focus on end-to-end automation, simplification, resilience, security, performance, scalability and reusability\nWorking within a platform or feature team along with software engineers to design and engineer complex software, scripts and tools to enable the delivery of bank platforms, applications and services, acting as a point of contact for solution design considerations\nDefining and developing architecture models and roadmaps of application and software components to meet business and technical requirements, driving common usability across products and domains\nInfluencing the development of strategies and architecture at domain and enterprise levels, identifying transformational opportunities for the businesses and technology areas\nThe skills you'll need\nyou'll come with significant experience in software engineering, software or database design and architecture, as we'll as experience of developing software within a DevOps and Agile framework.\nAlong with an expert understanding of the latest market trends, technologies and tools, you'll bring significant and demonstrable experience of implementing programming best practice, especially around scalability, automation, virtualisation, optimisation, availability and performance.\nyou'll also need:\nProven proficiency in Java, .net, Python, Angular\nA strong background in cloud platform such as AWS\nA background of working with AI/ML frameworks such as TensorFlow, PyTorch, Scikit-learn, MLflow\nSignificant and demonstrable experience of test-driven development and using automated test frameworks, mocking and stubbing and unit testing tools\nThe ability to rapidly and effectively understand and translate product and business requirements into technical solution",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Architecture', 'Scalability', 'Testing tools', 'Database design', 'Agile', 'test driven development', 'Unit testing', 'Principal', 'Python']",2025-06-13 06:24:36
Principal Engineer - Applications Development,Oliver Wyman,5 - 9 years,Not Disclosed,['Gurugram'],"Company: Mercer\nDescription:\nSupports and assists senior developers in developing codes, tests and debugs of software programs and enhancements to meet project plan goals.\nWorks under guidance from senior software developers.\nAssists with repairing coding problems.\nProvides technical support to internal clients on existing problems, escalates as appropriate.\nMonitors trends and results from technical support calls to advises senior team members on performance of existing systems.",Industry Type: Management Consulting,Department: Other,"Employment Type: Full Time, Permanent","['algorithms', 'css', 'python', 'project management', 'software development', 'software testing', 'hibernate', 'javascript', 'application development', 'sql', 'plsql', 'spring', 'coding', 'angular', 'spring boot', 'requirement gathering', 'node.js', 'java', 'html', 'data structures', 'angularjs', 'jira']",2025-06-13 06:24:38
"Principal Support Engineer, Critical Support Team",QAD,8 - 9 years,Not Disclosed,['Mumbai'],"The Critical Support Team (CST) is a specialized group within the organization dedicated to addressing the most complex, high-impact, and recurring customer issues. Acting as a problem-solving hub, the CST is responsible for thoroughly investigating escalated cases and recurring incidents to deliver sustainable, long-term solutions.\nThe Principal Engineer, Critical Support Team (CST) serves as a senior technical authority and problem-solving leader within the organization. This role is dedicated to addressing the most complex, high-impact, and recurring customer issues, driving systemic improvements to enhance product reliability and customer satisfaction.\nAs a member of the Critical Support Team, the Principal Engineer leads the investigation and resolution of escalated cases, performing in-depth root cause analysis (RCA) to uncover and resolve systemic gaps in products and processes. This role goes beyond troubleshooting by collaborating closely with Subject Matter Experts (SMEs), Engineering, Product Management, and cross-functional teams to implement long-term solutions and advocate for product and process enhancements.\nThe Principal Engineer is instrumental in mentoring team members, developing best practices, and contributing to the improvement of support operations and tools. With a focus on proactive issue prevention, this role identifies trends and implements strategies to minimize recurring issues while enhancing operational efficiency.\nThis position requires exceptional technical expertise, strong problem-solving skills, and a customer-centric mindset. Given the global nature of the team, the Principal Engineer must demonstrate flexibility, adaptability, and resilience to deliver consistent results in a dynamic, fast-paced environment.\nGiven the global nature of our support operations, CST Engineers must demonstrate flexibility by participating in shift rotations, weekend shifts, and on-call schedules. This ensures uninterrupted support for critical cases and consistent resolution of urgent customer needs across different time zones. Adaptability, resilience, and the ability to thrive under pressure are essential for success in this role.\n\nTechnical Support and Troubleshooting:\nPerform advanced troubleshooting for complex technical issues, including system errors, database optimization, performance tuning, and application debugging.\nTake ownership of escalated cases, ensuring efficient root cause analysis and long-term resolution.\nCollaborate with cross-functional teams to identify and resolve technical gaps, ensuring alignment with customer needs and expectations.\nIncident Handling:\nManage incoming escalations and critical incidents through established ticketing systems, ensuring accurate prioritization and categorization.\nDrive incident resolution by engaging with relevant teams, maintaining clear communication, and adhering to established Service Level Agreements (SLAs).\nProactively monitor and analyze incident patterns to identify recurring issues and recommend preventive measures.\nEnsure clear and consistent escalation protocols for unresolved or systemic issues, partnering with Engineering and Product teams as needed.\nDocumentation and Knowledge Sharing:\nMaintain detailed documentation of all troubleshooting steps, resolutions, and root cause findings for escalated cases.\nDevelop comprehensive playbooks, guides, and FAQs to support both internal teams and customers in addressing similar issues effectively.\nContribute to the organizational knowledge base by documenting verified solutions and sharing insights from critical incidents.\nAssist in training and mentoring frontline support teams to improve their ability to handle complex or recurring issues.\nCustomer Engagement and Communication:\nCommunicate effectively with customers and partners to provide regular updates on issue resolution progress.\nExplain technical concepts clearly and concisely, ensuring that both technical and non-technical stakeholders understand the issue and the resolution.\nFoster trust and transparency with customers by maintaining a proactive approach and ensuring their concerns are addressed promptly.\nRoot Cause Analysis and Product Improvement\nInvestigate recurring issues to identify root causes and systemic gaps, collaborating with Engineering for sustainable fixes.\nPrepare in-depth RCA reports with actionable insights, outlining long-term preventive measures.\nAct as a customer advocate during product discussions, providing feedback on recurring pain points to influence future product enhancements.\nPartner with Product Management to incorporate RCA findings into development roadmaps and ensure alignment with customer expectations.\nTraining and Development:\nAssist in developing and delivering training materials and sessions to enhance the skills and technical knowledge of team members.\nMentor junior team members, providing guidance on complex cases and fostering a collaborative and growth-oriented team environment.\nStay updated on the latest developments in QAD and Progress products, engaging in continuous learning to improve personal expertise.\nShift Coverage and Flexibility:\nParticipate in shift rotations, weekend shifts, and on-call coverage to provide uninterrupted global support.\nAdapt to varying scheduling requirements to ensure timely assistance for customers in different time zones.\nAny Other Duties as Assigned:\nFulfill additional responsibilities as needed to support organizational goals and priorities, ensuring alignment with the overall mission of delivering high-quality support.\n\n\nEducation:\nA Bachelor s Degree in Information Technology, Computer Science, or a related field. Equivalent experience will be considered.\nExperience:\n8+ years",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Performance tuning', 'C++', 'SAP', 'Linux', 'Debugging', 'Windows', 'Troubleshooting', 'Information technology', 'Technical support', 'SQL']",2025-06-13 06:24:40
Snowflake - Senior Technical Lead,Sopra Steria,2 - 11 years,Not Disclosed,['Noida'],"Position: Snowflake - Senior Technical Lead\nExperience: 8-11 years\nLocation: Noida/ Bangalore\nEducation: B.E./ B.Tech./ MCA\nPrimary Skills: Snowflake, Snowpipe, SQL, Data Modelling, DV 2.0, Data Quality, AWS, Snowflake Security\nGood to have Skills: Snowpark, Data Build Tool, Finance Domain",,,,"['Performance tuning', 'Schema', 'HIPAA', 'Javascript', 'Data quality', 'Informatica', 'Analytics', 'SQL', 'Python', 'Auditing']",2025-06-13 06:24:41
"Principal Engineer, VP",NatWest Markets,12 - 16 years,Not Disclosed,['Gurugram'],"Join us as a Principal Engineer\nThis is an exciting and challenging opportunity to work in a collaborative, agile and forward thinking team environment\nWith your software development background, you ll be delivering software components to enable the delivery of platforms, applications and services for the bank\nAs well as developing your technical talents, youll have the opportunity to build project and leadership skills which will open up a range of exciting career options\nWere offering this role at vice president level\nWhat youll do\nAs a Principal Engineer, you ll be driving development software and tools to accomplish project and departmental objectives by converting functional and non-functional requirements into suitable designs. You ll play a leading role in planning, developing and deploying high performance robust and resilient systems for the bank, and will develop your leadership skills as you manage the technical delivery of one or more software engineering teams.\nYou ll also gain a distinguished leadership status in the software engineering community as you lead the wider participation in internal and industry wide events, conferences and other activities.\nYou ll also be:\nDesigning and developing high performance and high availability applications, using proven frameworks and technologies\nMaking sure that the bank s systems follow excellent architectural and engineering principles, and are fit for purpose\nMonitoring the technical progress against plans while safeguarding functionality, scalability and performance, and providing progress updates to stakeholders\nDesigning and developing reusable libraries and APIs for use across the bank\nWriting unit and integration tests within automated test environments to ensure code quality\nThe skills youll need\nYou ll need at least 12 years of experience in software engineering, software or database design and architecture, as well as significant experience in delivering solutions for eFI Trading system .\nAlong with development experience in large scale, high volume, multi-threaded and highly distributed systems , you ll bring an excellent understanding of implementing programming best practice, especially around scalability, availability and performance.\nYou ll also need:\nExperience in backend development in ultra-low latency sensitive application\nExperience in Java, Multithreading, Concurrency packages and REST services\nExperience of test-driven development and using automated test frameworks, mocking and stubbing and unit testing tools\nU nderstanding of Java coding best practices, data structures, algorithms and design Patterns\nExperience of supporting, modifying and maintaining systems and code developed by teams other than your own",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Backend', 'Multithreading', 'Testing tools', 'Coding', 'Database design', 'Agile', 'Data structures', 'Unit testing', 'Distribution system', 'Monitoring']",2025-06-13 06:24:43
Principal Engineer- Project Purchasing,Burns and Mc Donnells Engineering India,7 - 10 years,Not Disclosed,['Mumbai'],"About us:\nBurns & McDonnell  is a leading player in the Engineering, Procurement, and Construction (EPC) industry, delivering innovative solutions to clients  across multiple industries like Chemicals,Oil & Gas, Transmission & Distribution, Power among other verticals.We are proud to be part of a global network with our US parent company. With a track record of successful projects across various industries, we are committed to innovation, sustainability, and client satisfaction. As we continue to grow, we are seeking an experienced Purchasing  Engineers to join our team.\nPosition Overview :\nAs a Principal Purchasing Engineer at Burns & McDonnell, your role will be pivotal in the successful execution of work share projects between our consultancy firm and our US parent company.You will support US procurement manager and Buyers Purchasing on project to ensure seamless integration between the two entities, adhering to the highest standards of efficiency and quality.\nKey Responsibilities:\n1.Provide procurement support to Operations in relation to purchasing Project Procurement items like Pressure vessels, Rotary Items, E& I  items, etc. in a timely manner as assigned with supervision and support from the Business Support Manager.\n2.Responsible for RFP compilation ,Quality review of  RFP with Engineering, Floating Enquiry, Bid evaluation, Bid Tabulation, Purchase Recommendation and post order PO management.\n3.Coordiation with International supplier for bid clarification and with US counterpart to update the status of purchasing\n4.Receive and Check Supplier Invoice, Tag to proper Project , Process through Oracle OnBase application for further Projects approval and  for final processing by Finance. Complete Tracking to be followed until Invoice is processed and release to supplier.\n5.Coordinate with Procurement leadership team & project Management team  to provide Monthly status on Purchasing.\nKey Technical Deliverables:\n1.Approved Manufacturer list\n2.Prepare Request For Quotations (RFQs) and evaluate responses\n3.Negotiate with suppliers on all matters relating to terms and conditions, improved pricing of quotes received and delivery options that may be more economic and timely.\n4.Tabulate Commercial Bid Evaluation\n5.Issue Purchase Recommendation\n4.Coordinate on contractual, commercial, taxation, insurance, and legal issues with relevant internal stakeholders.\n6.Raise/Revise Purchase Orders (POs) and resolve queries as require.\n7.Prepare & Maintain Purchase reports.\n-\n1.11 to 12  years  Procurement experience, preferably in an  EPC environment in Oil and Gas, Transmission & Distribution industry.\n2. Good understanding of PO contact terms and condition, logistics, Supplier Qualification.\n3. Experience in contract formulation activities, systems and processes\n4. Good communication skills, both oral and written.\n5. Computer Knowledge and operating skills on MS office.",Industry Type: Engineering & Construction,Department: Procurement & Supply Chain,"Employment Type: Full Time, Permanent","['execution', 'project management', 'engineering purchase', 'purchase', 'purchase order', 'engineering', 'environment', 'purchase management', 'operations', 'vendor development', 'procurement', 'project procurement', 'writing', 'epc', 'construction', 'communication skills', 'ms office', 'project purchase']",2025-06-13 06:24:44
Software Developer Intern,Cubeion Software,6 months duration,"5,000/month",[],"Role & responsibilities\nCollaborate with the team to design, develop and test real world AI features.\n\nPreferred candidate profile\nBasic knowledge of software development, Python.\nStrong willingness to learn, build and collaborate.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Development', 'Python', 'Artificial Intelligence', 'Javascript', 'Machine Learning']",2025-06-13 06:24:46
Walkin Drive II Corporate Sales B2B II 99acres_Bangalore,Info Edge,1 - 5 years,Not Disclosed,['Bengaluru'],"Walkin Drive on 13th and 14th June\nRole: Corporate Sales (B2B)\nExperience: 1- 5 years\nSkill: B2B Sales, New client acquisition\n\nAbout Info Edge:\nInfoEdges mission is to create world-class platforms that transform lives by continuously innovating. Our products and services are built keeping our customers in mind. We always delight our customers by delivering superior value through enhanced offerings on the internet and other platforms. Through our continuous investment across various businesses, especially in cutting-edge technology, machine learning and artificial intelligence (AI), we have built a robust system that constantly increases our predictive powers on customer behaviour, and optimizes and improves our systems. Our various teams tirelessly work together to solve problems, innovate, and create something to empower our customers.",,,,"['B2B Sales', 'Client Acquisition', 'Field Sales', 'Key Account Management', 'Business Development', 'Fresher Hiring']",2025-06-13 06:24:47
Walk-In (17th June) Client Servicing & Human Resource | Naukri Campus,Info Edge,1 - 5 years,4-5 Lacs P.A.,['Bengaluru'],"Join our team! We are currently hiring for the role of Client Servicing Professional at Naukri Campus (formerly Firstnaukri.com)\n\nIf you are interested, Attend your interview in Mega Walk-In drive, we will be happy to meet you :)\n\nInterview Date - 17th June 2025 (Tuesday)\nInterview Time - 10:00AM - 4:00PM",,,,"['Human Resource', 'Customer Service', 'Client Acquisition', 'Hiring', 'Client Interaction', 'Recruitment', 'Client Coordination', 'Sales', 'Client Servicing', 'Client Retention', 'Campus Hiring', 'Business Development', 'Fresher Hiring', 'Campus Recruitment', 'Post Sales', 'Client Handling']",2025-06-13 06:24:49
STEM / Robotics Trainer,Mindsightz Education,0 - 2 years,1.5-4 Lacs P.A.,"['Tiruppur', 'Coimbatore', 'Erode']","The position is responsible for implementing innovative, challenging, and engaging Science, Technology, Engineering and Math hands-on time tested curriculum; providing STEM training for students and participate in special Robotic events at schools\n\nRequired Candidate profile\nOral and written communication skills\nPassion for teaching\nTraining delivery and presentation skills\nThe deployment of hands-on learning technologies\nWork effectively with partner",Industry Type: Education / Training,Department: Teaching & Training,"Employment Type: Full Time, Permanent","['Training', 'Teaching', 'computer programming', 'Curriculum development', 'SCRATCH', 'STEM Robotics', 'Arduino', 'Artificial intelligence', 'Robotics', 'STEM']",2025-06-13 06:24:51
Gen AI Experts,Axtria,5 - 10 years,Not Disclosed,"['Noida', 'Hyderabad', 'Bengaluru']","Axtria: -Axtria is a global provider of award-winning cloud software and data analytics to the life sciences industry. Axtria enables life sciences organizations to transform the product commercialization journey and deliver much-improved healthcare outcomes for patients worldwide. We are acutely aware that our work impacts millions of people and are incredibly passionate about the improvement we can bring to patients lives.\nOur focus is on delivering solutions that help pharmaceutical, medical device, and diagnostics companies complete the journey from data to insights to action and get superior returns from their investments. As a participant of the United Nations Global Compact, Axtria is committed to aligning strategies and operations with universal principles on human rights, labor, environment, and anti-corruption and taking actions that advance societal goals.\nOur people are our core strength, and they make us proud of our work; this has helped us grow exponentially and make tremendous strides toward developing great products. It is for the grit, thinking of an entrepreneur, and a family-like environment where each member is valued and treasured that we are growing rapidly. For more information, visit www.axtria.com.\n\n\nJob Title: - Gen AI Experts ( Open across levels – Senior Associate to Associate Director)\n\nJob Location: -Gurgaon/Bangalore/Pune/Hyderabad\n\nJob Responsibilities: -\nBe an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT\nSoftware development experience in python is needed as backend for UI based applications\nBe a part of large delivery teams working on advanced projects when expert assistance is required.\nDeliver advanced Data Science capabilities to businesses in a meaningful manner through successful proof-of-concept solutions, and later smoothly transition the proof-of-concept into production.\nCreate Technical documents, develop, test, and deploy data analytics processes using Python, SQL on Azure/AWS platforms\nCan interact with client on GenAI related capabilities and use cases\n\nQualification: -\n- B-Tech or BE in Computer Science / Computer Applications from Tier 1-2 college\nOR\n- Master’s degree in Machine Learning / Statistics / Econometrics, or related discipline from Tier 1-2 college\n\nMust have Skills: -\nRequire 3-15 years experience to develop, test, and deploy Python based applications on Azure/AWS platforms\nMust have basic knowledge on concepts of Generative AI / LLMs / GPT\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.\nFamiliarity with frontend technologies like HTML, JavaScript, REACT\nSkills that give you an edge: -\nStrong analytical skills to solve and model complex business requirements are a plus. With life sciences or pharma background.\nWe will provide– (Employee Value Proposition)\nOffer an inclusive environment that encourages diverse perspectives and ideas\nDeliver challenging and unique opportunities to contribute to the success of a transforming organization\nOpportunity to work on technical challenges that may impact across geographies\nVast opportunities for self-development: online Axtria Institute, knowledge sharing opportunities globally, learning opportunities through external certifications\nSponsored Tech Talks & Hackathons\nPossibility to relocate to any Axtria office for short and long-term projects\nBenefit package:\n-Health benefits\n-Retirement benefits\n-Paid time off\n-Flexible Benefits\n-Hybrid /FT Office/Remote\nAxtria is an equal-opportunity employer that values diversity and inclusiveness in the workplace.\nWho we are\nAxtria 14 years journey\nAxtria, Great Place to Work\nLife at Axtria\nAxtria Diversity",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'Model Building', 'Python', 'SQL', 'Modeling Tools']",2025-06-13 06:24:53
Sr. Consultant - Infra Job,Yash Technologies,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking forward to hire PowerShell Professionals in the following areas :\nExperience\n5-8 Years\nWe are seeking a seasoned Exchange Administrator to oversee the deployment, maintenance, and optimization of our email and messaging systems. Your expertise will ensure seamless and secure communication across the organization.\nResponsibilities:\nManage on-premises and cloud-based Exchange environments.\nMonitor and troubleshoot mail flow, connectivity, and performance issues.\nImplement security measures and ensure compliance with organizational standards.\nPerform migrations, upgrades, and patches for Exchange systems.\nCollaborate with IT teams to ensure integrated messaging solutions.\nTrain and mentor team members on Exchange administration.",,,,"['SMTP', 'Business transformation', 'Powershell', 'Analytical', 'POP3', 'Exchange administration', 'Cloud', 'Agile', 'Biztalk', 'Oracle']",2025-06-13 06:24:55
Fellow Software System Design Eng.,"Advanced Micro Devices, Inc",2 - 6 years,Not Disclosed,['Bengaluru'],"THE ROLE:\nWe are looking for a Fellow to join AMD s ROCm GPU communications team to enhance AMD s future offerings for GPU networking solutions. You will lead vision, architecture and design of GPU networking and collective communication software. You will identify and evaluate new technologies, innovations, and strategic partners to solve problems in modern data centers. You will mentor top-tier talent and drive high-priority strategic initiatives by collaborating with senior executive leadership in this role.\nTHE PERSON:\nYou are an experienced technical leader that is well versed in state-of-the-art technologies employed in modern data centers with large scale deployments of GPUs for AI/ML and HPC workloads. You are passionate about solving networking and communication problems to scale performance to meet the demands of future workloads.\nKEY RESPONSIBILITIES:\nProvide deep technical leadership and guidance for GPU communication technologies, define the technical vision and direction for the GPU communication software stack.\nEngage with executives and key stakeholders to provide insight into industry trends and recommend strategic initiatives. Influence the future direction of the company s technical portfolio.\nRepresent AMD in leadership positions at industry organizations and standards bodies.\nEngage with clients and industry partners to deeply understand technical needs, ensuring their satisfaction with tailored solutions that leverage your experience in strategic customer engagements and architectural wins.\nCollaborate with hardware and software architects, system engineers and business teams in identifying requirements and building roadmaps for future products.\nMentor engineers and technical leaders, fostering a culture of innovation and excellence. Help develop the next generation of leaders through coaching, training, and feedback.\nPREFERRED EXPERIENCE:\nExperience architecting and developing communication software solutions for accelerators using RDMA and accelerator-to-accelerator fabrics (eg. Infinity Fabric, UALink), from low-level device drivers and OS internals up through applications and AI/ML frameworks\nDeep expertise with distributed programming models (MPI, SHMEM), and the implementation and optimization of collective communication algorithms\nDeep expertise with RoCE, RDMA, and network topologies\nExperience with system software development in C/C++, and GPU software development and parallel programing\nAnalytical and performance analysis skills\nEffective communication and problem-solving skills\nProven history of communication software thought leadership, backed with patents, publications, and participation in industry standards bodies\nACADEMIC CREDENTIALS:\nBachelor s degree in Computer Science, Computer Engineering, Electrical Engineering, or related\nAdvanced degrees, such as Master s or Ph. D. are preferred\n\n\n\nBenefits offered are described:\nAMD benefits at a glance .",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Selection process', 'C++', 'Networking', 'Analytical', 'Artificial Intelligence', 'System design', 'System software development', 'Gaming', 'Recruitment']",2025-06-13 06:24:56
Scala/Spark Developer - Chennai,Photon,8 - 9 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Develop, test, and deploy data processing applications using Apache Spark and Scala.\nOptimize and tune Spark applications for better performance on large-scale data sets.\nWork with the Cloudera Hadoop ecosystem (e.g., HDFS, Hive, Impala, HBase, Kafka) to build data pipelines and storage solutions.\nCollaborate with data scientists, business analysts, and other developers to understand data requirements and deliver solutions.",,,,"['cloudera', 'Version control', 'spark', 'Postgresql', 'Data processing', 'Application development', 'data integrity', 'Apache', 'Analytics', 'SQL']",2025-06-13 06:24:58
CE4 WIRELESS RB,Zensar,5 - 8 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","d Skills (Good to have as value add to this role) o Python and API o Ekahau Pro/ sidekick usage o ISE deployment and configuration experience Education &/ Additional Cortication s o Typically requires BSc Computer Science or equivalent plus 5-8 years of relevant work experience. Advanced degree strongly preferred. o Cisco certifications (CCNP, CCIE etc.) would be highly desirable.\nd Skills (Good to have as value add to this role) o Python and API o Ekahau Pro/ sidekick usage o ISE deployment and configuration experience Education &/ Additional Cortication s o Typically requires BSc Computer Science or equivalent plus 5-8 years of relevant work experience. Advanced degree strongly preferred. o Cisco certifications (CCNP, CCIE etc.) would be highly desirable.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Wireless', 'Computer science', 'Usage', 'Ccie', 'Deployment', 'cisco', 'CCNP', 'Python']",2025-06-13 06:25:00
Logistics Analyst 4,Lam Research,8 - 12 years,Not Disclosed,['Bengaluru'],"Logistics Analyst, Program Lead\nThe Logistics Analyst will be the point of contact for all SAP TMS system implementation, Training internal team and enhancement as part of Digital transformation\nPrimary Job Responsibilities:\nOperations Support\nRouting guide management\nEnsure booking of shipments for respective Logistics Service Providers (LSP)\nTrack & tracing and exception handling : BN4L exception management\nAbility to quickly react to unforeseen events and communicate with stakeholders as needed\nFreight Rate tender & Freight audit\nBN4L exception management\nFollow SOPs (Standard Operations Procedures) with great attention to details\nSAP TMS Administration & Troubleshooting\nUser management (user set up, onboarding and ongoing support)\nWork with core technical team and Training internal teams on new SAP TMS tools\nMaster data maintenance as needed\nTMS troubleshooting and communication between the user base and TMS BSA/service provider regarding system performance and outages\nSupport standardization and documentation of processes (SOP creation) as needed\nAnalytics\nReport generation and analysis turning data into actionable insights (improving transportation provider selection, route optimization, identifying cost reduction opportunities, etc.)\nGain insight over carrier performance to evaluate trends and pursue advantageous alternatives\nThe Group You ll Be A Part Of\nThe Global Operations Group brings information systems, facilities, supply chain, logistics, and high-volume manufacturing together to drive the engine of our global business operations. We help Lam deliver industry-leading solutions with speed and efficiency, while actively supporting the resilient and profitable growth of Lams business.\nThe Impact You ll Make\nAs a Logistics Analyst at Lam, youll orchestrate and streamline material flow, ensuring efficient supply chain operations and maintaining optimal inventory levels. Your role encompasses a broad set of responsibilities, including supply chain services, inventory control, and ensuring critical parts availability through enterprise warehouse and inventory systems. Your skilled analysis will support production planning and volume studies. Your expertise will be pivotal in optimizing Lams logistics plans for seamless operations.\nWhat You ll Do\nWho We re Looking For\nMinimum 8-12 years working experience in any of the following areas: Global Logistics Project/Program mgt, Global Transportation, SAP TMS & Trade operations in global environment\nPreferred Qualifications\nOur Commitment\nWe believe it is important for every person to feel valued, included, and empowered to achieve their full potential. By bringing unique individuals and viewpoints together, we achieve extraordinary results.\nLam Research (""Lam"" or the ""Company"") is an equal opportunity employer. Lam is committed to and reaffirms support of equal opportunity in employment and non-discrimination in employment policies, practices and procedures on the basis of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex (including pregnancy, childbirth and related medical conditions), gender, gender identity, gender expression, age, sexual orientation, or military and veteran status or any other category protected by applicable federal, state, or local laws. It is the Companys intention to comply with all applicable laws and regulations. Company policy prohibits unlawful discrimination against applicants or employees.\nLam offers a variety of work location models based on the needs of each role. Our hybrid roles combine the benefits of on-site collaboration with colleagues and the flexibility to work remotely and fall into two categories - On-site Flex and Virtual Flex. On-site Flex you ll work 3+ days per week on-site at a Lam or customer/supplier location, with the opportunity to work remotely for the balance of the week. Virtual Flex you ll work 1-2 days per week on-site at a Lam or customer/supplier location, and remotely the rest of the time.",Industry Type: Electronic Components / Semiconductors,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAP', 'Production planning', 'Cost reduction', 'Logistics Analyst', 'Inventory control', 'Analytics', 'Freight', 'Auditing', 'Logistics', 'Business operations']",2025-06-13 06:25:02
Big Data Developer,Techstar Group,7 - 10 years,Not Disclosed,['Hyderabad'],"Responsibilities of the Candidate :\n\n- Be responsible for the design and development of big data solutions. Partner with domain experts, product managers, analysts, and data scientists to develop Big Data pipelines in Hadoop\n\n- Be responsible for moving all legacy workloads to a cloud platform\n\n- Work with data scientists to build Client pipelines using heterogeneous sources and provide engineering services for data PySpark science applications\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- Define needs around maintainability, testability, performance, security, quality, and usability for the data platform\n\n- Drive implementation, consistent patterns, reusable components, and coding standards for data engineering processes\n\n- Convert SAS-based pipelines into languages like PySpark, and Scala to execute on Hadoop and non-Hadoop ecosystems\n\n- Tune Big data applications on Hadoop and non-Hadoop platforms for optimal performance\n\n- Apply an in-depth understanding of how data analytics collectively integrate within the sub-function as well as coordinate and contribute to the objectives of the entire function.\n\n- Produce a detailed analysis of issues where the best course of action is not evident from the information available, but actions must be recommended/taken.\n\n- Assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients, and assets, by driving compliance with applicable laws, rules, and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct, and business practices, and escalating, managing and reporting control issues with transparency\n\nRequirements :\n\n- 6+ years of total IT experience\n\n- 3+ years of experience with Hadoop (Cloudera)/big data technologies\n\n- Knowledge of the Hadoop ecosystem and Big Data technologies Hands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Pig, Impala, Spark, Kafka, Kudu, Solr)\n\n- Experience in designing and developing Data Pipelines for Data Ingestion or Transformation using Java Scala or Python.\n\n- Experience with Spark programming (Pyspark, Scala, or Java)\n\n- Hands-on experience with Python/Pyspark/Scala and basic libraries for machine learning is required.\n\n- Proficient in programming in Java or Python with prior Apache Beam/Spark experience a plus.\n\n- Hand on experience in CI/CD, Scheduling and Scripting\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- System level understanding - Data structures, algorithms, distributed storage & compute\n\n- Can-do attitude on solving complex business problems, good interpersonal and teamwork skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Hive', 'Data Engineering', 'Data Pipeline', 'PySpark', 'Hadoop', 'Kafka', 'HDFS', 'Spark', 'Python']",2025-06-13 06:25:03
Data Architect,Exavalu,7 - 12 years,Not Disclosed,[],"We are seeking an experienced Azure Data Architect to lead the design, implementation, and optimization of scalable, secure, and cost-effective cloud data solutions on Microsoft Azure. The ideal candidate will have deep expertise in data architecture, cloud computing, and modern data platforms, with a proven ability to align technology strategies with business goals.\nKey Responsibilities:\nDesign and implement modern data platform solutions using Azure services including Azure Data Lake, Azure Synapse Analytics, Azure Data Factory, and Azure Databricks.\nDefine data architecture frameworks, standards, and principles.\nLead data modernization and cloud migration initiatives.\nBuild secure and scalable data pipelines to ingest, transform, and store large datasets.\nCollaborate with data engineers, data scientists, and business stakeholders to understand data requirements and provide architectural guidance.\nOptimize performance, security, and cost of data platforms.\nEvaluate and recommend new Azure services and tools based on evolving project needs.\nEnsure data governance, data quality, and compliance standards are met.\nRequirements Required Qualifications:\nBachelors or Masters degree in Computer Science, Information Systems, or a related field.\n7+ years of experience in data architecture or data engineering roles.\n3+ years of hands-on experience with Azure data services.\nStrong knowledge of relational and non-relational databases (SQL Server, Cosmos DB, etc).\nProficiency with data modeling, data warehousing, and ETL processes.\nFamiliarity with big data and analytics tools like Databricks, Spark, and Synapse.\nExperience with scripting and programming languages (Python, SQL, PowerShell).\nExcellent understanding of security, identity, and governance in Azure.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Cloud computing', 'Architecture', 'Data modeling', 'Data quality', 'Cosmos', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-13 06:25:05
Data Science Trainee,Lanxess,1 - 3 years,Not Disclosed,['Thane'],"Contract Type: Regular 12 months\n\n\n\n\nIf the chemistry is right, we can make a difference at LANXESS: speed up sports, make beverages last longer, add more color to leisure time and much more.\n\nAs a leading specialty chemicals group, we develop and produce chemical intermediates, additives, specialty chemicals and high-tech plastics. With more than 13,000 employees. Be part of it!",,,,"['Training', 'deep learning', 'C', 'data science', 'Finance', 'Machine learning', 'Javascript', 'Packaging', 'SQL', 'Python']",2025-06-13 06:25:07
Data Science (SSE) | FINJO I766,Omni Recruit,3 - 8 years,Not Disclosed,['Mumbai (All Areas)'],"Python Developer\nWork from Office\nLocation : Airoli , Navi Mumbai\n& :\n3.55 years of relevant experience in Python\nMinimum 3.5 years in Python programming\nAt least 1+ year in machine learning and natural language processing (NLP)\nMinimum 1.5 years with LLMs and GenAI\nAt least 2 years of experience with any database\n1+ year of experience deploying ML models/Python applications on Azure or AWS",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Machine Learning', 'Python']",2025-06-13 06:25:08
Data Solutions Architect (Immediate Joiner),Expro,7 - 12 years,Not Disclosed,['Hyderabad( HITEC City )'],"Overall Purpose of the Job\nWe are seeking an experienced and innovative Data Solutions Architect to join our team. The ideal candidate will have a strong background in designing and implementing data systems, ensuring seamless integration and scalability of data solutions, and leading the design of data architectures that meet business needs. This role requires proficiency in leveraging the Azure IoT Framework to enable IoT-driven data solutions. As a Data Solutions Architect, you will work closely with cross-functional teams to create robust, secure, and scalable data solutions that empower the business to leverage data for strategic decision-making.\n\nRole & responsibilities\nDesign and implement scalable, secure, and high-performance data architectures that meet business requirements.\nLead the integration of Azure IoT Framework into the architecture, enabling real-time data ingestion, processing, and analysis from IoT devices.\nCollaborate with stakeholders (business, IT, and data science teams) to understand data needs and translate them into effective solutions.\nOversee the full data lifecycle, including data collection, transformation, storage, and consumption.\nEvaluate and recommend tools, platforms, and technologies for data storage, processing, and analytics, ensuring that the solutions are aligned with business goals.\nDefine data integration strategies, ensuring that data from disparate sources, including IoT devices and sensors, can be ingested, processed, and unified effectively.\nCreate and enforce data governance practices, ensuring compliance with data privacy, security, and regulatory requirements.\nLead and mentor teams in the development and implementation of data solutions, ensuring adherence to architectural best practices and design patterns.\nEnsure the scalability, reliability, and performance of data systems to handle increasing volumes of data, including large datasets from IoT sources.\nStay updated on emerging trends and technologies in data management, cloud platforms, Azure IoT, and data engineering.\n\nRequired Skills & Qualifications:\n\nBachelors or masters degree in computer science, Data Engineering, Information Technology, or a related field.\nProven experience (typically 5+ years) in designing, implementing, and managing large-scale data architectures.\nExpertise in Azure IoT Framework, including services such as Azure IoT Hub, Azure Stream Analytics, and Azure Digital Twins.\nStrong understanding of data management, ETL processes, and database design (e.g., SQL, NoSQL, data lakes, and data warehouses).\nHands-on experience with modern data technologies and tools such as Hadoop, Spark, Kafka, and ETL frameworks.\nProficiency with data governance, security, and privacy standards.\nFamiliarity with machine learning, AI integration, and analytics platforms is a plus.\nStrong leadership skills with the ability to mentor and guide technical teams.\nExcellent problem-solving abilities and a deep understanding of system architecture and distributed systems.\nExcellent communication skills, both written and verbal, with the ability to translate complex technical concepts into business-friendly terms.\n\nPreferred Skills:\nExperience with cloud-native data platforms and services (e.g., Azure Synapse Analytics, Azure Databricks).\nKnowledge of data visualization and business intelligence tools (e.g., Tableau, Power BI, Looker).\nExperience with DevOps and CI/CD practices for data pipelines and data-driven applications.\nFamiliarity with microservices architectures and APIs.\nCertifications in Azure technologies (e.g., Microsoft Certified: Azure Solutions Architect Expert, Microsoft Certified: Azure IoT Developer).\nExperience with edge computing and real-time data processing for IoT solutions.\nRequired Immediate joiner.",Industry Type: Oil & Gas,Department: Other,"Employment Type: Full Time, Permanent","['Datafactory', 'Azure Event Hub', 'SQL Server', 'Data Bricks', 'Azure IoT Hub', 'Sap Data Services']",2025-06-13 06:25:10
Data Architect,Calibo,12 - 16 years,Not Disclosed,[],"About the Role:\n\nWe are looking for a highly skilled Data Engineering Architect with strong Data Engineering pipeline implementation experience to serve as the lead Solution/Technical Architect and Subject Matter Expert for customer experience data solutions across multiple data sources. The ideal candidate will collaborate with the Enterprise Architect and the client IT team to establish and implement strategic initiatives.\n\nResponsibilities and Technical Skills:\n12+ years of relevant experience in designing and Architecting ETL, ELT, Reverse ETL, Data Management or Data Integration, Data Warehouse, Data Lake, and Data Migration.\nMust have expertise in building complex ETL pipelines and large Data Processing, Data Quality and Data security\nExperience in delivering quality work on time with multiple, competing priorities.\nExcellent troubleshooting and problem-solving skills must be able to consistently identify critical elements, variables and alternatives to develop solutions.\nExperience in identifying, analyzing and translating business requirements into conceptual, logical and physical data models in complex, multi-application environments.\nExperience with Agile and Scaled Agile Frameworks.\nExperience in identifying and documenting data integration issues, and challenges such as duplicate data, non-conformed data, and unclean data. Multiple platform development experience.\nStrong experience in performance tuning of ETL processes using Data Platforms\nMust have experience in handling Data formats like Delta Tables, Parquet files, Iceberg etc.\nExperience in Cloud technologies such as AWS/Azure or Google Cloud.\nApache Spark design and development experience using Scala, Java, Python or Data Frames with Resilient Distributed Datasets (RDDs).\nDevelopment experience in databases like Oracle, AWS Redshift, AWS RDS, Postgres Databricks and/or Snowflake.\nHands-on professional work experience with Python is highly desired.\nExperience in Hadoop ecosystem tools for real-time or batch data ingestion.\nStrong communication and teamwork skills to interface with development team members, business analysts, and project management. Excellent analytical skills.\nIdentification of data sources, internal and external, and defining a plan for data management as per business data strategy.\nCollaborating with cross-functional teams for the smooth functioning of the enterprise data system.\nManaging end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nPlanning and execution of big data solutions using Databricks, Big Data, Hadoop, Big Query, Snowflake, MongoDB, DynamoDB, PostgreSQL and SQL Server\nHands-on experience in defining and implementing various Machine Learning models for different business needs.\nIntegrating technical functionality, ensuring data accessibility, accuracy, and security.\nProgramming / Scripting Languages like Python / Java / Go, Microservices\nMachine Learning / AI tools like Scikit-learn / TensorFlow / PyTorch",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud', 'ETL', 'AWS', 'Data Handling', 'Spark']",2025-06-13 06:25:11
Product Manager - Data Management,Reuters,12 - 15 years,Not Disclosed,"['Mumbai', 'Hyderabad']","We are seeking an experienced Product Manager-Data Management to lead the development and adoption of our 3rd party data platforms, including D&B and other similar platforms. The successful candidate will be responsible for driving the integration and utilization of 3rd party data across marketing campaigns, improving data quality and accuracy, and expanding the use cases and applications for 3rd party data.\n  About the Role\nDevelop and execute a comprehensive strategy for 3rd party data platform adoption and expansion across the organization, with a focus on driving business outcomes and improving marketing effectiveness.\nCollaborate with marketing teams to integrate 3rd party data into their campaigns and workflows and provide training and support to ensure effective use of the data.\nDevelop and showcase compelling use cases that demonstrate the value of 3rd party data in improving marketing effectiveness and measure the success of these use cases through metrics such as adoption rate, data quality, and marketing ROI.\nDevelop and maintain a roadmap for 3rd party data platform adoption and expansion across the organization, with a focus on expanding use cases and applications for 3rd party data and developing new data-driven products and services.\nMonitor and measure the effectiveness of 3rd party data in driving business outcomes, and adjust the adoption strategy accordingly\nWork with cross-functional teams to ensure data quality and governance, and develop and maintain relationships with 3rd party data vendors to ensure seamless data integration and delivery.\nDrive the development of new data-driven products and services that leverage 3rd party data, and collaborate with stakeholders to prioritize and develop these products and services.\nShift Timings: 2 PM to 11 PM (IST).\nWork from office for 2 days in a week (Mandatory).\nAbout You\n12+ years of experience in data management, product management, or a related field.\nBachelors or Masters degree in Computer Science, Data Science, Information Technology, or a related field.\nExperience with data management tools such as data warehousing, ETL (Extract, Transform, Load), data governance, and data quality.\nUnderstanding of the Marketing domain and data platforms such as Treasure Data, Salesforce, Eloqua, 6Sense, Alteryx, Tableau and Snowflake within a MarTech stack.\nExperience with machine learning and AI frameworks (eg, TensorFlow, PyTorch).\nExpertise in SQL and Alteryx.\nExperience with data integration tools and technologies such as APIs, data pipelines, and data virtualization.\nExperience with data quality and validation tools and techniques such as data profiling, data cleansing, and data validation.\nStrong understanding of data modeling concepts, data architecture, and data governance.\nExcellent communication and collaboration skills.\nAbility to drive adoption and expansion of D&B data across the organization.\nCertifications in data management, data governance, or data science is nice to have.\nExperience with cloud-based data platforms (eg, AWS, GCP, Azure) nice to have.\nKnowledge of machine learning and AI concepts, including supervised and unsupervised learning, neural networks, and deep learning nice to have.\nWhat s in it For You\nHybrid Work Model: we've adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected.\nFlexibility & Work-Life Balance: Flex My Way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. This builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance.\nCareer Development and Growth: By fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow s challenges and deliver real-world solutions. Our Grow My Way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an AI-enabled future.\nIndustry Competitive Benefits: We offer comprehensive benefit plans to include flexible vacation, two company-wide Mental Health Days off, access to the Headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial we'llbeing.\nCulture: Globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. We live by our values: Obsess over our Customers, Compete to Win, Challenge (Y)our Thinking, Act Fast / Learn Fast, and Stronger Together.\nSocial Impact: Make an impact in your community with our Social Impact Institute. We offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and Environmental, Social, and Governance (ESG) initiatives.\nMaking a Real-World Impact: We are one of the few companies globally that helps its customers pursue justice, truth, and transparency. Together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.",Industry Type: Internet,Department: Product Management,"Employment Type: Full Time, Permanent","['Computer science', 'data cleansing', 'Data validation', 'Data management', 'Data modeling', 'Consulting', 'Data quality', 'Information technology', 'SQL', 'Salesforce']",2025-06-13 06:25:13
Director of Data Governance and Operations,Sailpoint Technologies,10 - 12 years,Not Disclosed,['Pune'],"SailPoint is the leader in identity security for the cloud enterprise. Our identity security solutions secure and enable thousands of companies worldwide, giving our customers unmatched visibility into the entirety of their digital workforce, ensuring workers have the right access to do their job - no more, no less.\nBuilt on a foundation of AI and ML, our Identity Security Cloud Platform delivers the right level of access to the right identities and resources at the right time matching the scale, velocity, and changing needs of today s cloud-oriented, modern enterprise.\nAbout the role:\nWe are seeking a dynamic and experienced Director of Data Governance and Operations to lead our data governance and data quality initiatives . Reporting to the VP of IT, this leadership role will be pivotal in driving data-driven decision-making across our Go-To-Market (GTM) operations and Finance organizations. The ideal candidate will possess a deep passion for data governance, a proven track record of developing and executing successful data quality strategies, and a strong understanding of how data fuels AI innovation.\nEssential Duties and Responsibilities\nDevelop and champion a comprehensive data strategy aligned with SailPoint s overall business objectives , with a particular focus on GTM and Finance.\nEstablish and maintain a robust data governance framework, including policies, standards, and procedures, to ensure data quality, accuracy, and compliance.\nPartner with GTM and Finance leadership to identify data needs, develop data solutions, and drive the adoption of data-driven insights to improve performance.\nLead efforts to monitor and improve data quality across key systems and data sources, implementing data cleansing and validation processes.\nEnsure data readiness for AI initiatives, collaborating with Enterprise Applications and D ata E ngineering teams to prepare and structure data for AI model development and deployment.\nSalesforce Expertise: Leverage in-depth knowledge of Salesforce to optimize data management, reporting, and analytics within the platform.\nBuild, mentor, and manage a high-performing team of data professionals, fostering a culture of collaboration, innovation, and continuous improvement.\nCollaborate effectively with stakeholders across IT, GTM, Finance, and other departments to ensure alignment on data strategy and priorities.\nStay abreast of emerging trends in data management, AI, and analytics, and identify opportunities to leverage new technologies to enhance SailPoint s data capabilities.\nRequired Qualifications\n15 + years of experience in data management, data governance, or data strategy roles, with increasing levels of responsibility.\nProven track record of developing and executing successful data strategies for GTM operations and Finance.\nStrong experience with Master Data Management (MDM)\nDeep understanding of data governance principles , practices , tools, and technologies.\nIn-depth knowledge of Salesforce data model, reporting, and analytics capabilities.\nExperience with Salesforce administration.\nStrong understanding of AI and machine learning concepts, and the importance of data readiness for AI.\nExcellent leadership, communication, and interpersonal skills.\nAbility to influence and collaborate effectively with stakeholders at all levels of the organization.\nExperience building and managing high-performing teams.\nStrong analytical and problem-solving skills.\nSalesforce certifications are highly desirable.\nBachelor s degree in a relevant field (e.g., Computer Science, Data Science, Business Analytics).\nWhat success looks like\n30-Day Plan (Orientation and Integration):\nObjective: Establish foundational knowledge and integrate into the team.\nTraining Learning:\nIntr oduction to SailPoint Enterprise Systems and Data Platform\nReview company policies, procedures, and compliance requirements\nRoles Responsibilities:\nUnderstand job expectations, key performance indicators (KPIs), and deliverables.\nReview IT team structure and individual roles\nReview GTM and Finance team structure and roles and responsibilities of key stakeholders\nRelationship Building:\nMeet team members, key stakeholders, and cross-department colleagues.\nParticipate in onboarding activities and one-on-one meetings with IT leaders .\nImmediate Contributions:\nAssess and compile problem statement on data governance and data quality\nAssess and compile data flow diagrams for the key GTM and Finance metrics\nAssist with data requests to get familiar with SailPoint internal data needs , e.g. ACV/ARR\nInitiate recruiting of data analyst\n60-Day Plan ( Data Leadership and Strategic Thinking )\nObjective: Expertise on SailPoint business data and define strategy for data governance and data quality\nEstablish structure and recurring cadence with key business stakeholders and IT system owners on data governance\nDefine SailPoint Master Data and process to ensure data integrity\nEstablish metrics to measure data quality\nRecommend industry best practice for data governance and data quality\nDe fine roadmap\nData Governance and MDM initiatives\nData repository and metadata for AI\nProject Involvement:\nSailPoint ARR/ACV Reporting\nData found ation for AI\nTeam Building and Relationship Building :\nComplete recruiting of new data analyst\nDemonstrate ability to engage business and IT stakeholders and partnership on key data initiatives\n90 -180 Day Plan ( Execution on Roadmap )\nObjective: Establish leadership and make impact to key data initiatives\nData Quality\nComplete the top 2 initiatives for data quality for Salesforce\nData Governance and MDM\nPublish the master data definition and framework\nCollaborate with business stakeholders and IT system owners to implement system validation and business operations to ensure data integrity\nAI\nCollaborate with IT system owners and Data Engineering team to build metadata for AI consumption\nDeliver data for AI model training and AI search\nValidate the AI output to ensure accuracy",Industry Type: IT Services & Consulting,Department: Strategic & Top Management,"Employment Type: Full Time, Permanent","['data cleansing', 'Enterprise applications', 'Business analytics', 'Analytical', 'data governance', 'Data Analyst', 'Data quality', 'Continuous improvement', 'Business operations', 'Salesforce']",2025-06-13 06:25:15
"AVP, Business Intelligence Developer",Synchrony,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Description:\nRole Title: AVP, Business Intelligence Developer (L11)\nCompany Overview:\nSynchrony (NYSE: SYF) is a premier consumer financial services company delivering one of the industry s most complete digitally enabled product suites. Our experience, expertise and scale encompass a broad spectrum of industries including digital, health and wellness, retail, telecommunications, home, auto, outdoors, pet and more.\nWe have recently been ranked #2 among India s Best Companies to Work for by Great Place to Work. We were among the Top 50 India s Best Workplaces in Building a Culture of Innovation by All by GPTW and Top 25 among Best Workplaces in BFSI by GPTW. We have also been recognized by AmbitionBox Employee Choice Awards among the Top 20 Mid-Sized Companies, ranked #3 among Top Rated Companies for Women, and Top-Rated Financial Services Companies.\nSynchrony celebrates ~51% women diversity, 105+ people with disabilities, and ~50 veterans and veteran family members.\nWe offer Flexibility and Choice for all employees and provide best-in-class employee benefits and programs that cater to work-life integration and overall well-being.\nWe provide career advancement and upskilling opportunities, focusing on Advancing Diverse Talent to take up leadership roles.\nOrganizational Overview :\nThe Business Intelligence and Reporting team is a part of the Technology and Operations team. We are an Enterprise Operation team supporting primarily collections and quality. We provide reporting that is insightful, timely and actionable. We help drive agent and business performance while improving customer satisfaction.\nRole Summary/Purpose:\nAs a Business Intelligence (BI) Developer, you will be a dynamic and highly motivated individual with extensive experience working with internal stakeholders in acquiring and refining business requirements for business intelligence solutions in the financial industry. You will design, develop, and implement Natural Language Processing (NLP) solutions that will lead to increased efficiency and profitability across Operations. You will work in the Apache Spark framework and its machine learning library (Spark ML) to build scalable NLP pipelines using Python and PySpark.\nEssential Responsibilities:\nAs a Business Intelligence Developer, you will identify insights and opportunities for increased efficiency and growth across functions in operations.\nYou will build and analyze metrics that can be leveraged for business performance measurement, management, QA enhancement and improvement and derive insights about behavior of customers and agents\nProvide advanced analytics and experimentation support, enabling data-driven solutions by applying statistical techniques to solve business problems\nDemonstrate subject matter expertise and technical leadership in data extraction and manipulation, dash-boarding, visualization, and analytical/statistical/data mining techniques\nDeliver user-friendly and informative dashboards and recurring reports, iterating and prototyping to meet business needs\nResponsible for creating, optimizing and automating Spark NLP pipelines to process large volumes of text data efficiently\nExtract relevant features from text data to improve the performance of NLP models\nAssess the accuracy of NLP models and identifying areas for improvement through metrics like precision, recall, and F1-score\nConnecting Spark NLP pipelines with other data processing systems and applications, including data warehouses, databases, and visualization tools.\nIdentify new innovative opportunities to drive performance improvement, efficiency, and cost savings\nRequired Skills/Knowledge:\nBachelor s degree (or foreign equivalent) in any discipline and minimum 5+ years of Business Intelligence experience within financial services, banking, or retail finance, or in lieu of degree, 7+ years Machine Learning experience within financial services, banking, or retail finance\nMinimum of 3+ years of experience in Natural Language Processing\nMinimum 2+ years of Tableau, Power BI or other data visualization tool\nDesired Characteristics:\nStrong background in ETL pipeline development in Anaconda Enterprise and proficiency in Python and SQL\nExperience working with Apache Spark framework and machine learning library (Spark ML) to build scalable NLP pipelines\nExtensive Python/PySpark ETL pipeline development experience\nStrong data visualization experience with Tableau utilizing data visualization best practices and performance optimization\nEffective communication skills with stakeholders, business analysts, architects and other developers\nEligibility Criteria:\nBachelor s degree (or foreign equivalent) in any discipline and minimum 5+ years of Business Intelligence experience within financial services, banking, or retail finance, or in lieu of degree, 7+ years Machine Learning experience within financial services, banking, or retail finance\nWork Timings:\nThis role qualifies for Enhanced Flexibility and Choice offered in Synchrony India and will require the incumbent to be available between 06:00 AM Eastern Time - 11:30 AM Eastern Time (timings are anchored to US Eastern hours and will adjust twice a year locally). This window is for meetings with India and US teams. The remaining hours will be flexible for the employee to choose. Exceptions may apply periodically due to business needs. Please discuss this with the hiring manager for more details.\nFor Internal Applicants :\nUnderstand the criteria or mandatory skills required for the role, before applying\nInform your manager and HRM before applying for any role on Workday\nEnsure that your professional profile is updated (fields such as education, prior experience, other skills) and it is mandatory to upload your updated resume (Word or PDF format)\nMust not be any corrective action plan (Formal/Final Formal, LPP)\nL9+ Employees who have completed 18 months in the organization and 12 months in current role and level are only eligible.\nL09+ Employees can apply\nGrade/Level: 11\nJob Family Group:\nInformation Technology",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['PDF', 'Bfsi', 'Analytical', 'Data processing', 'Business intelligence', 'Data mining', 'Information technology', 'Financial services', 'SQL', 'Data extraction']",2025-06-13 06:25:16
Data Architect,Armakuni,8 - 12 years,Not Disclosed,['Ahmedabad'],"DataArchitecture Design: Develop and maintain a comprehensive data architecture strategy that aligns with the business objectives and technology landscape.\nDataModeling:Createand managelogical, physical, and conceptual data models to support various business applications and analytics. DatabaseDesign: Design and implement database solutions, including data warehouses, data lakes, and operational databases.\nDataIntegration: Oversee the integration of data from disparate sources into unified, accessible systems using ETL/ELT processes. DataGovernance:Implementand enforce data governance policies and procedures to ensure data quality, consistency, and security.\nTechnologyEvaluation: Evaluate and recommend data management tools, technologies, and best practices to improve data infrastructure and processes.\nCollaboration: Work closely with data engineers, data scientists, business analysts, and other stakeholders to understand data requirements and deliver effective solutions. Trusted by the world s leading brands\nDocumentation:Createand maintain documentation related to data architecture, data flows, data dictionaries, and system interfaces. PerformanceTuning: Optimize database performance through tuning, indexing, and query optimization.\nSecurity: Ensure data security and privacy by implementing best practices for data encryption, access controls, and compliance with relevant regulations (e.g., GDPR, CCPA)\n\nRequirements:\nHelpingproject teams withsolutions architecture,troubleshooting, and technical implementation assistance.\nExperiencewithbig data technologies (e.g., Hadoop, Spark, Kafka, Airflow).\nExpertisewithcloud platforms (e.g., AWS, Azure, Google Cloud) and their data services.\nKnowledgeofdataintegration tools (e.g., Informatica, Talend, FiveTran, Meltano).\nUnderstandingofdatawarehousing concepts and tools (e.g., Snowflake, Redshift, Synapse, BigQuery). Experiencewithdata governanceframeworks and tools.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['query optimization', 'Data management', 'data security', 'Postgresql', 'MySQL', 'Data quality', 'Informatica', 'Troubleshooting', 'Analytics', 'Data architecture']",2025-06-13 06:25:18
Data Techology Senior Associate,MSCI Services,2 - 8 years,Not Disclosed,['Pune'],"The Data Technology team at MSCI is responsible for meeting the data requirements across various business areas, including Index, Analytics, and Sustainability. Our team collates data from multiple sources such as vendors (e.g., Bloomberg, Reuters), website acquisitions, and web scraping (e.g., financial news sites, company websites, exchange websites, filings). This data can be in structured or semi-structured formats. We normalize the data, perform quality checks, assign internal identifiers, and release it to downstream applications.\nYour Key Responsibilities\nAs data engineers, we build scalable systems to process data in various formats and volumes, ranging from megabytes to terabytes. Our systems perform quality checks, match data across various sources, and release it in multiple formats. We leverage the latest technologies, sources, and tools to process the data. Some of the exciting technologies we work with include Snowflake, Databricks, and Apache Spark.\nYour skills and experience that will help you excel\nCore Java, Spring Boot, Apache Spark, Spring Batch, Python. Exposure to sql databases like Oracle, Mysql, Microsoft Sql is a must. Any experience / knowledge / certification on Cloud technology preferrably Microsoft Azure or Google cloud platform is good to have. Exposures to non sql databases like Neo4j or Document database is again good to have.\nAbout MSCI\nWhat we offer you\nTransparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall wellbeing.\nFlexible working arrangements, advanced technology, and collaborative workspaces.\nA culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\nA global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for for ongoing skills development.\nMulti-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\nWe actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women s Leadership Forum.\n.\nMSCI Inc. is an equal opportunity employer. It is the policy of the firm to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, religion, creed, age, sex, gender, gender identity, sexual orientation, national origin, citizenship, disability, marital and civil partnership/union status, pregnancy (including unlawful discrimination on the basis of a legally protected parental leave), veteran status, or any other characteristic protected by law. MSCI is also committed to working with and providing reasonable accommodations to individuals with disabilities. If you are an individual with a disability and would like to request a reasonable accommodation for . Please note, this e-mail is intended only for individuals who are requesting a reasonable workplace accommodation; it is not intended for other inquiries.\nTo all recruitment agencies\n.\nNote on recruitment scams",Industry Type: NGO / Social Services / Industry Associations,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['CVS', 'Core Java', 'Bloomberg', 'spring batch', 'MySQL', 'Oracle', 'Analytics', 'Downstream', 'Python', 'Recruitment']",2025-06-13 06:25:20
Trainer - AI & Data Analytics,NIIT Foundation (NF),1 - 3 years,Not Disclosed,['Ahmedabad'],"1. Experience in Technical Training/Teaching\n\n2. Strong knowledge of Data Analytics, Machine Learning,Text Mining, Tableau, Power BI, Advance Excel\n\n3. Conducting classes and practical data analytics fortechnical/engineering students\n\n4. Should be able to describe the course content of offeredclasses to students\n\n5. Preference is given to certified trainers.\n\n\nRequirements\n\nRequirements\nEducation Qualification :\n\nMandatory: Graduation is must. Computer Graduate who knowscomputer & hardware well.\nPast Experience/Skills Required (Mandatory)\nCandidate should be strong technical skills/soft skill\nStrong motivation to work in the social sector\nPerson should be comfortable to train students\nAt least one year of experience in Training or Management\nExcellent Communication skills, Presentation skills. People",Industry Type: NGO / Social Services / Industry Associations,Department: Teaching & Training,"Employment Type: Full Time, Permanent","['Text mining', 'Technical training', 'Training', 'tableau', 'Computer hardware', 'Machine learning', 'power bi', 'Data analytics', 'Advanced Excel', 'Management']",2025-06-13 06:25:21
Head of Modelling & Data Science,Aegis Media,5 - 9 years,Not Disclosed,['Coimbatore'],"The purpose of this role is to set the strategic direction for the team, taking ownership of the overall Insights and analysis discipline in the market and liaising with other channels to ensure an integrated response to people-based marketing objectives.\nJob Description:\nJob Description Summary\nWe are seeking an experienced Advanced Analytics Senior Manager to join our dynamic team. This role focuses on leveraging advanced analytics techniques, including machine learning algorithms, Generative AI (GenAI), and large language models (LLMs), to drive data-driven decision-making within the retail/CPG domain. The ideal candidate will possess a strong quantitative background, a passion for transforming complex data into actionable insights, an extensive experience into leading mid-to-large sized teams, and the ability to create, assess and implement RFPs, POCs, approaches and frameworks.\nKey Responsibilities:\nDevelop, implement, and maintain advanced analytical models using machine learning algorithms and GenAI applications\nUtilize various advanced analytics techniques to uncover trends, patterns, and insights from large and complex datasets.\nCollaborate with cross-functional teams to identify business needs and deliver data-driven solutions.\nCreate visually compelling dashboards and reports to present findings to stakeholders.\nContinuously evaluate and improve existing analytics methodologies and models to enhance accuracy and performance.\nStay abreast of industry trends and advancements in analytics and machine learning to drive innovation within the team.\nMentor junior team members and contribute to knowledge sharing within the organization.\nBasic Qualifications:\nBachelor s or Master s degree in Data Science, Business Analytics, Mathematics, Statistics, or a related field.\n10+ years of experience in advanced analytics, data science, machine learning, Generative AI or a related field.\nStrong experience with quantitative modeling, predictive analytics, text analytics, and forecasting methodologies\nProficiency in SQL (or Google BigQuery), Python, visualization tools like Tableau/PowerBI\nFamiliarity with the Retail/CPG/Tech industry and experience with product, transaction, and customer-level data.\nExcellent communication skills, both verbal and written, with the ability to convey complex concepts to non-technical stakeholders.\nStrong analytical and problem-solving skills, with an inquisitive mindset.\nDesired Skills:\nProficient in the following advanced analytics techniques:\nDescriptive Analytics: Statistical analysis, data visualization.\nPredictive Analytics: Regression analysis, time series forecasting, classification techniques, market mix modeling\nPrescriptive Analytics: Optimization, simulation modeling.\nText Analytics: Natural Language Processing (NLP), sentiment analysis.\nExtensive knowledge of machine learning techniques, including:\nSupervised Learning: Linear regression, logistic regression, decision trees, support vector machines, random forests, gradient boosting machines among others\nUnsupervised Learning: K-means clustering, hierarchical clustering, principal component analysis (PCA), anomaly detection among others\nReinforcement Learning: Q-learning, deep Q-networks, etc.\nExperience with Generative AI and large language models (LLMs) for text generation, summarization, and conversational agents.\nResearching, loading and application of the best LLMs (GPT, Gemini, LLAMA, etc.) for various objectives\nHyper parameter tuning\nPrompt Engineering\nEmbedding & Vectorization\nFine tuning\nProficiency in data visualization tools such as Tableau or Power BI.\nStrong skills in data management, structuring, and harmonization to support analytical needs.\nLocation:\nCoimbatore\nBrand:\nMerkle\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Logistic regression', 'Data management', 'Business analytics', 'Analytical', 'Machine learning', 'linear regression', 'Regression analysis', 'Forecasting', 'SQL', 'Python']",2025-06-13 06:25:23
Data Architect Telecom Domain databrick BSS OSS,fast growing Data Driven IT solutions an...,10 - 20 years,45-55 Lacs P.A.,"['Noida', 'Hyderabad', 'Gurugram']","Data Architect Telecom Domain\nTo design comprehensive data architecture and technical solutions specifically for telecommunications industry challenges, leveraging TMforum frameworks and modern data platforms. To work closely with customers, and technology partners to deliver data solutions that address complex telecommunications business requirements including customer experience management, network optimization, revenue assurance, and digital transformation initiatives.\nResponsibilities:\nDesign and articulate enterprise-scale telecom data architectures incorporating TMforum standards and frameworks, including SID (Shared Information/Data Model), TAM (Telecom Application Map), and eTOM (enhanced Telecom Operations Map)\nDevelop comprehensive data models aligned with TMforum guidelines for telecommunications domains such as Customer, Product, Service, Resource, and Partner management\nCreate data architectures that support telecom-specific use cases including customer journey analytics, network performance optimization, fraud detection, and revenue assurance\nDesign solutions leveraging Microsoft Azure and Databricks for telecom data processing and analytics\nConduct technical discovery sessions with telecom clients to understand their OSS/BSS architecture, network analytics needs, customer experience requirements, and digital transformation objectives\nDesign and deliver proof of concepts (POCs) and technical demonstrations showcasing modern data platforms solving real-world telecommunications challenges\nCreate comprehensive architectural diagrams and implementation roadmaps for telecom data ecosystems spanning cloud, on-premises, and hybrid environments\nEvaluate and recommend appropriate big data technologies, cloud platforms, and processing frameworks based on telecom-specific requirements and regulatory compliance needs.\nDesign data governance frameworks compliant with telecom industry standards and regulatory requirements (GDPR, data localization, etc.)\nStay current with the latest advancements in data technologies including cloud services, data processing frameworks, and AI/ML capabilities\nContribute to the development of best practices, reference architectures, and reusable solution components for accelerating proposal development\nQualifications:\nBachelor's or Master's degree in Computer Science, Telecommunications Engineering, Data Science, or a related technical field\n10+ years of experience in data architecture, data engineering, or solution architecture roles with at least 5 years in telecommunications industry\nDeep knowledge of TMforum frameworks including SID (Shared Information/Data Model), eTOM, TAM, and their practical implementation in telecom data architectures\nDemonstrated ability to estimate project efforts, resource requirements, and implementation timelines for complex telecom data initiatives\nHands-on experience building data models and platforms aligned with TMforum standards and telecommunications business processes\nStrong understanding of telecom OSS/BSS systems, network management, customer experience management, and revenue management domains\nHands-on experience with data platforms including Databricks, and Microsoft Azure in telecommunications contexts\nExperience with modern data processing frameworks such as Apache Kafka, Spark and Airflow for real-time telecom data streaming\nProficiency in Azure cloud platform and its respective data services with an understanding of telecom-specific deployment requirements\nKnowledge of system monitoring and observability tools for telecommunications data infrastructure\nExperience implementing automated testing frameworks for telecom data platforms and pipelines\nFamiliarity with telecom data integration patterns, ETL/ELT processes, and data governance practices specific to telecommunications\nExperience designing and implementing data lakes, data warehouses, and machine learning pipelines for telecom use cases\nProficiency in programming languages commonly used in data processing (Python, Scala, SQL) with telecom domain applications\nUnderstanding of telecommunications regulatory requirements and data privacy compliance (GDPR, local data protection laws)\nExcellent communication and presentation skills with ability to explain complex technical concepts to telecom stakeholders\nStrong problem-solving skills and ability to think creatively to address telecommunications industry challenges\nGood to have TMforum certifications or telecommunications industry certifications\nRelevant data platform certifications such as Databricks, Azure Data Engineer are a plus\nWillingness to travel as required\nif you will all or most of the criteria contact bdm@intellisearchonline.net M 9341626895",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Telecom Bss', 'Data Architect', 'Telecom OSS', 'ETOM', 'Data Bricks']",2025-06-13 06:25:24
I&F Decision Science Practitioner Specialist,Accenture,7 - 11 years,Not Disclosed,['Mumbai'],"Skill required: Data Scientist - Data Science\n\n\n\n\nDesignation: Specialist\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:7 - 11 Years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nYou will be aligned with our Insights & Intelligence vertical and help us generate insights by leveraging the latest Artificial Intelligence (AI) and Analytics techniques to deliver value to our clients. You will also help us apply your expertise in building world-class solutions, conquering business problems, addressing technical challenges using AI Platforms and technologies. You will be required to utilize the existing frameworks, standards, patterns to create architectural foundation and services necessary for AI applications that scale from multi-user to enterprise-class and demonstrate yourself as an expert by actively blogging, publishing research papers, and creating awareness in this emerging area. You will be working as a part of Marketing & Customer Analytics team which provides a set of processes that measure, manage and analyze marketing activities in order to provide actionable insights and recommendations to marketing organizations in terms of optimizing ROI & performance efficiency in operations.Customer analytics is a process by which data from customer behavior is used to help make key business decisions via market segmentation and predictive analytics. This information is used by businesses for direct marketing, site selection, and customer relationship management. You should have exposure to digital marketing, A/B testing, MVT, Google Analytics/Site Catalyst. You will be a core member of Accenture Operations global Applied Intelligence group, an energetic, strategic, high-visibility and high-impact team, to innovate and transform the Accenture Operations business using machine learning, advanced analytics to support data-driven decisioning. The objectives of the team include but are not limited toLeading team of data scientists to build and deploy data science models to uncover deeper insights, predict future outcomes, and optimize business processes for clients. Refining and improving data science models based on feedback, new data, and evolving business needs. Analyze available data to identify opportunities for enhancing brand equity, improving retail margins, achieving profitable growth, and expanding market share for clients.\n\n\n\n\nWhat are we looking for\nExtensive experience in leading Data Science and Advanced Analytics delivery teams Strong statistical programming experience - Python, R, SAS, S-plus, MATLAB, STATA or SPSS. Experience working with large data sets and big data tools like Snowflake, AWS, Spark, etc. Solid knowledge in at least one of the following Supervised and Unsupervised Learning, Classification, Regression, Clustering, Neural Networks, Ensemble Modelling (random forest, boosted tree, etc.), Multivariate Statistics, Non-parametric Methods, Reliability Models, Markov Models, Stochastic models, Bayesian Models Experience in atleast one of these business domainsCPG, Retail, Marketing Analytics, Customer Analytics, Digital Marketing, eCommerce, Health, Supply Chain Extensive experience in client engagement and business development Ability to work in a global collaborative team environment\n\n\n\nRoles and Responsibilities: In this role you are required to do analysis and solving of moderately complex problems Typically creates new solutions, leveraging and, where needed, adapting existing methods and procedures The person requires understanding of the strategic direction set by senior management as it relates to team goals Primary upward interaction is with direct supervisor or team leads Generally, interacts with peers and/or management levels at a client and/or within Accenture The person should require minimal guidance when determining methods and procedures on new assignments Decisions often impact the team in which they reside and occasionally impact other teams Individual would manage medium-small sized teams and/or work efforts (if in an individual contributor role) at a client or within Accenture Please note that this role may require you to work in rotational shifts\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['client engagement', 'neural networks', 'business development', 'data science', 'clustering', 'digital marketing', 'matlab', 'snowflake', 'python', 'adobe analytics', 'sas', 'stata', 'spss', 'ensemble', 'unsupervised learning', 'r', 'spark', 'google analytics', 'statistical programming', 'aws']",2025-06-13 06:25:26
Web Developer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Web Developer\n\n\n\n\n\nProject Role Description :Design, build and test web-based applications for various site components and edit site content. Document technical designs and specifications. Research and incorporate updated content for websites.\n\n\n\nMust have skills :SAP BTP Integration Suite\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Web Developer, you will design, build, and test web-based applications for various site components, edit site content, document technical designs and specifications, and research and incorporate updated content for websites. You will have a dynamic and engaging role in creating innovative web solutions.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Develop responsive web applications that are user-friendly and visually appealing.- Collaborate with cross-functional teams to ensure seamless integration of web components.- Implement best practices for web development and adhere to coding standards.- Optimize web applications for maximum speed and scalability.- Stay updated on emerging technologies and trends in web development.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP BTP Integration Suite.- Strong understanding of web development technologies and frameworks.- Experience with front-end technologies such as HTML, CSS, and JavaScript.- Knowledge of RESTful APIs and web services.- Familiarity with version control systems like Git.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in SAP BTP Integration Suite.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['css', 'sap', 'javascript', 'web development', 'html', 'rest', 'emerging technologies', 'project management', 'web services', 'version control', 'enterprise architecture', 'machine learning', 'artificial intelligence', 'git', 'java', 'strategy consulting', 'it strategy', 'digital transformation']",2025-06-13 06:25:28
S&C Global Network - AI - Software & Platform - Gen AI - Consultant,Accenture,5 - 10 years,Not Disclosed,['Hyderabad'],"Entity:- Accenture Strategy & Consulting\n\n\n\nTeam:- Global Network - Data & AI\n\n\n\nPractice:- CMT Software & Platforms\n\n\n\nTitle:- Level 9 - Ind & Func AI Decision Science Consultant\n\n\n\nJob location:- Hyderabad/Bangalore\n\n\n\nAbout S&C - Global Network:- Accenture Global Network - Data & AI practice help our clients grow their business in entirely new ways.Analytics enables our clients to achieve high performance through insights from data - insights that inform better decisions and strengthen customer relationships. From strategy to execution, Accenture works with organizations to develop analytic capabilities - from accessing and reporting on data to predictive modelling - to outperform the competition\n\n\n\nAbout the Software & Platforms Team:-\n\nThe team is focused on driving Data & AI based solutions for SaaS and PaaS clients for Accenture. The team collaborates actively with onsite counterparts to help identify opportunities for growth as well as drives client deliveries from offshore.\n\n\n\nWHATS IN IT FOR YOU\nAs part of our Data & AI practice, you will join a worldwide network of smart and driven colleagues experienced in leading statistical tools, methods, and applications. From data to analytics and insights to actions, our forward-thinking consultants provide analytically informed, issue-based insights at scale to help our clients improve outcomes and achieve high performance.\nAccenture will continually invest in your learning and growth. You'll work with experts in SaaS & PaaS and Accenture will support you in growing your own tech stack and certifications.\nIn Data & AI you will understands the importance of sound analytical decision-making, relationship of tasks to the overall project, and executes projects in the context of a business performance improvement initiative.\n\n\nQualification\n\n\n\nWhat you would do in this role\nGathering business requirements to create high level business solution framework aligning with business objectives and goals.\nMonitor project progress able to plan project plan, proactively identify risks, and develop mitigation strategies.\nWork closely with project leads, engineers, and business analysts to develop AI solutions.\nDevelop & test AI algorithms and techniques tailored to solve specific business problems.\nPresent and communicate solutions and project updates to internal & external stakeholders.\nFoster positive client relationships by ensuring alignment between project deliverables and client expectations.\nAdopt a clear and systematic approach to complex issues. Analyze relationships between several parts of a problem or situation. Anticipate obstacles and identify a critical path for a project\nMentor and guide a team of AI professionals, cultivating a culture of innovation, collaboration, and excellence.\nConduct comprehensive market research and stay updated on the latest advancements and trends in AI technologies.\nFoster the professional development of team members through continuous learning opportunities.\n\n\n\n\nWho are we looking for\nBachelors or masters degree in computer science, engineering, data science, or a related field.\nExperience working for large Software or Platform organizations.\nProven experience (5+ years) in working on AI projects and delivering successful outcomes.\nHands-On exposure to Generative AI frameworks (Azure Open AI, Vertex AI) and implementations & Strong knowledge of AI technologies, including Embedding, prompt engineering, natural language processing, computer vision, etc.\nHands on experience in building and deployment of Statistical Models/Machine Learning including Segmentation & predictive modelling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\nProficiency in statistical packages such as R, Python, Java, SQL, Spark, etc.\nAbility to work with large data sets and present findings / insights to key stakeholders; Data management using databases like SQL\nExperience in training large language models and fine-tuning for specific applications or domains.\nUnderstanding of linguistic concepts, encompassing syntax, semantics, and pragmatics, to enhance language modeling.\nExperience with cloud platforms like AWS, Azure, or Google Cloud for deploying and scaling language models.\nUnderstanding of containerization technologies (e.g., Docker) and orchestration tools (e.g., Kubernetes) for managing and deploying models & exposure of CI/CD pipelines for automated testing and deployment of language models.\nExcellent analytical and problem-solving skills, with a data-driven mindset.\nStrong project management abilities, including planning, resource management, and risk assessment.\nProficient in Excel, MS word, PowerPoint, etc.\nExceptional communication and interpersonal skills to engage effectively with clients and internal stakeholders.\n\n\n\n\nAccenture is an equal opportunities employer and welcomes applications from all sections of society and does not discriminate on grounds of race, religion or belief, ethnic or national origin, disability, age, citizenship, marital, domestic or civil partnership status, sexual orientation, gender identity, or any other basis as protected by applicable law.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'project management', 'ci/cd', 'ai techniques', 'aws', 'kubernetes', 'algorithms', 'python', 'c', 'natural language processing', 'microsoft azure', 'machine learning', 'artificial intelligence', 'docker', 'sql', 'r', 'java', 'data science', 'gcp', 'spark', 'computer vision', 'paas']",2025-06-13 06:25:30
S&C Global Network - AI - Retail - Consultant - AI/Gen AI Retail,Accenture,2 - 6 years,Not Disclosed,['Gurugram'],"Title : S&C Global Network - AI - Retail - Consultant - AI/Gen AI Retail Specialist\n\n\n\n\n\nJob Title - AI/Gen AI Retail Specialist Level 9 SnC GN Data & AI\n\n\n\nManagement Level:09 - Consultant\n\n\n\nLocation:Bangalore / Gurgaon / Mumbai / Chennai / Pune / Hyderabad / Kolkata\n\n\n\nMust have skills:\nStrong understanding of retail industry\nAbility to work with large datasets from different sources (e.g., transactional data, customer data, social media data) and data preprocessing techniques.\nExpertise in supervised and unsupervised learning algorithms\nProficiency in deep learning frameworks like TensorFlow or PyTorch\nAbility to implement NLP techniques for chatbots, recommendation systems, sentiment analysis, and personalized search.\n\n\n\n\nGood to have skills:\nFamiliarity with SQL, NoSQL databases, and big data technologies like Spark for handling large-scale data.\nFamiliarity with transformer-based models such as BERT, GPT, and T5 for understanding and generating human language.\nExperience with Generative Pre-trained Transformers (GPT)\nExperience with Data Visualization Tools\n\n\n\n\nJob\n\n\nSummary: As an AI/Gen AI Retail Specialist, you will play a crucial role in leveraging advanced machine learning, deep learning, and generative AI technologies to transform retail strategies, improve customer experiences, and optimize operations. Your expertise in AI will drive the development of cutting-edge solutions that enhance retail performance across various domains, including merchandising, pricing, customer engagement, and product assortment.\n\n\n\n\nRoles & Responsibilities:\nLeverage Retail Knowledge:Utilize your deep understanding of the retail industry (merchandising, customer behavior, product lifecycle) to design AI solutions that address critical retail business needs.\nDesign, develop, and implement AI algorithms (machine learning, deep learning, etc.) and generative models (e.g., GPT, GANs) tailored to the retail sector.\nFocus on the creation of AI models that can help in wide range of use cases across customer lifecycle, personalized customer experiences, optimize pricing, and product assortment.\nAnalyze and preprocess large datasets from various sources (sales data, customer data, inventory data) to build effective AI models.\nTranslate data insights into actionable strategies for marketing, merchandising, and customer experience teams. For example, identifying cross-selling opportunities or optimizing product assortments.\nProvide regular reports and dashboards on AI model performance, business impact, and KPIs, communicating the results effectively to stakeholders.\nWork closely with cross-functional teams (e.g., product, marketing, business development) to understand the retail business challenges and translate them into AI-driven solutions.\nCommunicate the potential of AI solutions to both technical and non-technical stakeholders, ensuring alignment between technical capabilities and business objectives.\n\n\n\n\nProfessional & Technical Skills:\nExpertise in machine learning, natural language processing (NLP), and generative AI models.\nExperience with platforms like TensorFlow, PyTorch, and OpenAI technologies.\nKnowledge of AI ethics and responsible AI practices.\n\n\n\n\nAdditional Information:\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\nQualification\n\n\n\nExperience:Minimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification:Msc. Statistics, B.Tech. / M.Tech Computer Science, Econometrics, Data Science",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['deep learning frameworks', 'natural language processing', 'machine learning', 'tensorflow', 'pytorch', 'customer engagement', 'business development', 'artificial intelligence', 'retail', 'sql', 'nosql', 'unsupervised learning', 'spark', 'data visualization', 'customer experience']",2025-06-13 06:25:31
