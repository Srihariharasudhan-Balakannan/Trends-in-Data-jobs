[
    {
        "job_title": "Asst./Dy Manager - Data engineer",
        "company_name": "Ajanta Pharma",
        "experience": "5-7 Years",
        "location": "Mumbai",
        "industry": "Manufacturing, Pharmaceutical",
        "job_description": "Ajanta Pharma is looking for Asst./Dy Manager - Data engineer to join our dynamic team and embark on a rewarding career journey\nLiaising with coworkers and clients to elucidate the requirements for each task\nConceptualizing and generating infrastructure that allows big data to be accessed and analyzed\nReformulating existing frameworks to optimize their functioning\nTesting such structures to ensure that they are fit for use\nPreparing raw data for manipulation by data scientists\nDetecting and correcting errors in your work\nEnsuring that your work remains backed up and readily accessible to relevant coworkers\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs",
        "skills": [
            "NoSQL Databases",
            "Cloud Computing",
            "Etl Tools",
            "Data Modeling",
            "Data Warehousing",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Consultant - Sr.Data Engineer (DBT+Snowflake)",
        "company_name": "Genpact",
        "experience": "Fresher",
        "location": "Bengaluru",
        "industry": "IT/Computers - Hardware & Networking",
        "job_description": "Ready to shape the future of work\n\n\n\nAt Genpact, we don&rsquot just adapt to change&mdashwe drive it. AI and digital innovation are redefining industries, and we&rsquore leading the charge. Genpact&rsquos , our industry-first accelerator, is an example of how we&rsquore scaling advanced technology solutions to help global enterprises work smarter, grow faster, and transform at scale. From large-scale models to , our breakthrough solutions tackle companies most complex challenges.\n\n\n\nIf you thrive in a fast-moving, tech-driven environment, love solving real-world problems, and want to be part of a team that&rsquos shaping the future, this is your moment.\n\n\n\nGenpact (NYSE: G) is anadvanced technology services and solutions company that deliverslastingvalue for leading enterprisesglobally.Through ourdeep business knowledge, operational excellence, and cutting-edge solutions - we help companies across industries get ahead and stay ahead.Powered by curiosity, courage, and innovation,our teamsimplementdata, technology, and AItocreate tomorrow, today.Get to know us atand on,,, and.\n\n\n\nInviting applications for the role ofConsultant -Sr.Data Engineer (DBT+Snowflake)\n\n\n\n!\n\n\n\nIn this role, the Sr.Data Engineer is responsible for providing technical direction and lead a group of one or more developer to address a goal.\n\n\n\n\n\n\n\n\n\n\n\n\nJob Description:\n\n\n\n\n\nDevelop, implement, and optimize data pipelines using Snowflake, with a focus on Cortex AI capabilities.\n\n\n\n\n\n\n\nExtract, transform, and load (ETL) data from various sources into Snowflake, ensuring data integrity and accuracy.\n\n\n\n\n\n\n\nImplement Conversational AI solutions using Snowflake Cortex AI to facilitate data interaction through ChatBot agents.\n\n\n\n\n\n\n\nCollaborate with data scientists and AI developers to integrate predictive analytics and AI models into data workflows.\n\n\n\n\n\n\n\nMonitor and troubleshoot data pipelines to resolve data discrepancies and optimize performance.\n\n\n\n\n\n\n\nUtilize Snowflake%27s advanced features, including Snowpark, Streams, and Tasks, to enable data processing and analysis.\n\n\n\n\n\n\n\nDevelop and maintain data documentation, best practices, and data governance protocols.\n\n\n\n\n\n\n\nEnsure data security, privacy, and compliance with organizational and regulatory guidelines.\n\n\n\n\n\n\n\n\n\n\nResponsibilities:\n\n\n\n\n\n. Bachelor&rsquos degree in Computer Science, Data Engineering, or a related field.\n\n\n\n\n\n\n\n. experience in data engineering, with experience working with Snowflake.\n\n\n\n\n\n\n\n. Proven experience in Snowflake Cortex AI, focusing on data extraction, chatbot development, and Conversational AI.\n\n\n\n\n\n\n\n. Strongproficiency in SQL, Python, and data modeling.\n\n\n\n\n\n\n\n. Experience with data integration tools (e.g., Matillion, Talend, Informatica).\n\n\n\n\n\n\n\n. Knowledge of cloud platforms such as AWS, Azure, or GCP.\n\n\n\n\n\n\n\n. Excellent problem-solving skills, with a focus on data quality and performance optimization.\n\n\n\n\n\n\n\n. Strong communication skills and the ability to work effectively in a cross-functional team.\n\n\n\n\n\n\n\nProficiency in using DBT%27s testing and documentation features to ensure the accuracy and reliability of data transformations.\n\n\n\n\n\n\n\nUnderstanding of data lineage and metadata management concepts, and ability to track and document data transformations using DBT%27s lineage capabilities.\n\n\n\n\n\n\n\nUnderstanding of software engineering best practices and ability to apply these principles to DBT development, including version control, code reviews, and automated testing.\n\n\n\n\n\n\n\nShould have experience building data ingestion pipeline.\n\n\n\n\n\n\n\nShould have experience with Snowflake utilities such as SnowSQL, SnowPipe, bulk copy, Snowpark, tables, Tasks, Streams, Time travel, Cloning, Optimizer, Metadata Manager, data sharing, stored procedures and UDFs, Snowsight.\n\n\n\n\n\n\n\nShould have good experience in implementing CDC or SCD type 2\n\n\n\n\n\n\n\nProficiency in working with Airflow or other workflow management tools for scheduling and managing ETL jobs.\n\n\n\n\n\n\n\nGood to have experience in repository tools like Github/Gitlab, Azure repo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQualifications/Minimum qualifications\n\n\n\n\n\nB.E./ Masters in Computer Science, Information technology, or Computer engineering or any equivalent degree with good IT experience and relevant of working experience as a Sr. Data Engineer with DBT+Snowflake skillsets\n\n\n\n\n\n\n\nSkill Matrix:\n\n\n\n\n\n\n\n\n\n\nDBT (Core or Cloud), Snowflake, AWS/Azure, SQL, ETL concepts, Airflow or any orchestration tools, Data Warehousing concepts\n\n\n\n\n\n\n\n\n\n\n\nWhy join Genpact\n\n\n\n\n\nBe a transformation leader - Work at the cutting edge of AI, automation, and digital innovation\n\n\n\n\n\n\n\nMake an impact - Drive change for global enterprises and solve business challenges that matter\n\n\n\n\n\n\n\nAccelerate your career - Get hands-on experience, mentorship, and continuous learning opportunities\n\n\n\n\n\n\n\nWork with the best - Join 140,000+ bold thinkers and problem-solvers who push boundaries every day\n\n\n\n\n\n\n\nThrive in a values-driven culture - Our courage, curiosity, and incisiveness - built on a foundation of integrity and inclusion - allow your ideas to fuel progress\n\n\n\n\n\nCome join the tech shapers and growth makers at Genpact and take your career in the only direction that matters: Up.\n\n\n\nLet&rsquos build tomorrow together.\n\n\n\n\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values respect and integrity, customer focus, and innovation.\nFurthermore, please do note that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such scams include purchasing a %27starter kit,%27 paying to apply, or purchasing equipment or training.",
        "skills": []
    },
    {
        "job_title": "Data Engineer",
        "company_name": "Nimiety Careers",
        "experience": "8-12 Years\nINR 0.5 - 48.5 LPA ",
        "location": "Bengaluru, Chennai, Pune",
        "industry": "Login to check your skill match score",
        "job_description": "Position :Data Engineer\nType:FTE/Lateral\nLocation:Hyderabad(Ind), Bangalore, Pune, Chennai, Ahmedabad, Noida (Encora)\nExperience:08-12Yrs.\nSKILLS\nBig DataSnowflakeData bricksPysparkSqlAWSPysparkt\nProvide the organization's data consumers high quality data sets by data curation, consolidation, and manipulation from a wide variety of large scale (terabyte and growing) sources.\nBuild first-class data products and ETL processes that interact with terabytes of data on leading platforms such as Snowflake and BigQuery.\nPartner with our Analytics, Product, CRM, and Marketing teams.\nBe responsible for the data pipelines SLA and dependency management.\nWrite technical documentation for data solutions, and present at design reviews.\nSolve data pipeline failure events and implement anomaly detection.\nWork with various teams from Data Science, Product owners, to Marketing and software engineers on data solutions and solving technical challenges.\nMentor junior members of the team\nWhat We Seek\nEducation and Work Experience\nBachelor's degree in Computer Science or related field\n8+ Years experience in commercial data engineering or software development.\nTech Experience\nExperience with Big Data technologies such as Snowflake, Databricks, PySpark\nExpert level skills in writing and optimizing complex SQL; advanced data exploration skills with proven record of querying and analyzing large datasets.\nSolid experience developing complex ETL processes from concept to implementation to deployment and operations, including SLA definition, performance measurements and monitoring.\nHands-on knowledge of the modern AWS Data Ecosystem, including AWS S3\nExperience with relational databases such as Postgres, and with programming languages such as Python and/or Java\nKnowledge of cloud data warehouse concepts.\nExperience in building and operating data pipelines and products in compliance with the data mesh philosophy would be beneficial. Demonstrated efficiency in treating data, including data lineage, data quality, data observability and data discoverability.\nCommunication/people skills\nExcellent verbal and written communication skills. Ability to convey key insights from complex analyses in summarized business terms to non-technical stakeholders and also ability to effectively communicate with other technical teams.\nStrong interpersonal skills and the ability to work in a fast-paced and dynamic environment.\nAbility to make progress on projects independently and enthusiasm for solving difficult problem",
        "skills": [
            "snowflake",
            "Big Data",
            "Data Engineer"
        ]
    },
    {
        "job_title": "Senior Data Engineer",
        "company_name": "Adidas",
        "experience": "7-12 Years",
        "location": "Gurugram",
        "industry": "Sporting Goods",
        "job_description": "Data Lakehouse & Data Product Engineer\nPurpose & Overall Relevance for the Organization:\nBe part of a team building a state-of-the-art data lakehouse and create data products which are ingested from heterogeneous source systems.\nOpportunity to work in leading-edge technology like Databricks.\nCollaborate with multiple stakeholders to drive the data roadmap by translating stories into data pipelines.\nLearn and build knowledge around data domains and business processes.\nSet the mindset to lifeblood for adidas data-driven strategy by enabling the data backbone, powering AI and BI solutions that produce unseen insights for the entire company.\nWhat You Bring:\nExpert in SAP data models:\nExpert in SAP ECC and SAP S4HANA data models and processes. SAP process knowledge in sales and distribution (SD) is preferred.\nSAP BW Development Experience:\nStrong background in SAP BW Development, with knowledge of SAP ECC and S4HANA versions and their integration with data warehouse solutions.\nExposure to Spark Architecture:\nFamiliarity with Spark, PySpark, and Python is key, especially the willingness to learn and create well-documented, maintainable, and production-ready solutions.\nStrong ETL Build Knowledge:\nKnowledge of Slowly Changing Dimension (SCD) and heterogeneous data harmonization methods.\nExperienced with SQL:\nFoundation in writing and interpreting SQL queries is essential, with the expectation to handle more complex queries.\nFamiliarity with Data Tools:\nFamiliar with tools such as Databricks and AWS, and have knowledge of different storage formats like Delta, Parquet, Avro, etc.\nExcellent Analytical and Team Leadership Skills:\nConvince with excellent analytical skills, ability to work in a team, as well as lead a team of engineers.\nRequisite Education and Experience / Minimum Qualifications:\nFour-year college or university degree with focus on Business Administration or IT or related areas, or equivalent combination of education and experience.\nProficient spoken and written command of English.\nAt least 7 years of experience in IT.\n5 years of experience in relevant area.\n2 years of experience in team management.\nCOURAGE: Speak up when you see an opportunity; step up when you see a need.\nOWNERSHIP: Pick up the ball. Be proactive, take responsibility and follow-through.\nINNOVATION: Elevate to win. Be curious, test and learn new and better ways of doing things.\nTEAMPLAY: Win together. Work collaboratively and cultivate a shared mindset.\nINTEGRITY: Play by the rules. Hold yourself and others accountable to our company's standards.\nRESPECT: Value all players. Display empathy, be inclusive and show dignity to all.",
        "skills": [
            "SAP BW Development",
            "SAP S4HANA",
            "Pyspark",
            "Spark",
            "Sql",
            "Sap Ecc"
        ]
    },
    {
        "job_title": "Data Engineer",
        "company_name": "Vichara Technologies",
        "experience": "6-12 Years",
        "location": "Delhi, Delhi NCR",
        "industry": "Consulting",
        "job_description": "Key deliverables:\nEnhance and maintain the MDM platform\nMonitor and optimize system performance\nTroubleshoot and resolve technical issues\nSupport production incident resolution\nRole responsibilities:\nDevelop and refactor Python and SQL code\nIntegrate REST APIs within MDM workflows\nUtilize Azure Databricks and ADF for ETL tasks\nWork on Markit EDM or Semarchy-based solutions",
        "skills": [
            "Rest Api",
            "Adf",
            "Azure Databricks",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Engineer 3",
        "company_name": "Vichara Technologies",
        "experience": "7-12 Years",
        "location": "Pune",
        "industry": "Consulting",
        "job_description": "Key deliverables:\nEnhance and maintain the MDM platform to support business needs\nDevelop data pipelines using Snowflake, Python, SQL, and orchestration tools like Airflow\nMonitor and improve system performance and troubleshoot data pipeline issues\nResolve production issues and ensure platform reliability\nRole responsibilities:\nCollaborate with data engineering and analytics teams for scalable solutions\nApply DevOps practices to streamline deployment and automation\nIntegrate cloud-native tools and services (AWS, Azure) with the data platform\nUtilize dbt and version control (Git) for data transformation and management",
        "skills": [
            "Airflow",
            "snowflake",
            "dbt",
            "Python",
            "Sql",
            "AWS"
        ]
    },
    {
        "job_title": "Data Engineer",
        "company_name": "Vichara Technologies",
        "experience": "6-12 Years",
        "location": "Hyderabad",
        "industry": "Consulting",
        "job_description": "Key deliverables:\nEnhance and maintain the MDM platform\nMonitor and optimize system performance\nTroubleshoot and resolve technical issues\nSupport production incident resolution\nRole responsibilities:\nDevelop and refactor Python and SQL code\nIntegrate REST APIs within MDM workflows\nUtilize Azure Databricks and ADF for ETL tasks\nWork on Markit EDM or Semarchy-based solutions",
        "skills": [
            "Rest Api",
            "Adf",
            "Azure Databricks",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Engineer 3",
        "company_name": "Vichara Technologies",
        "experience": "7-12 Years",
        "location": "Delhi, Delhi NCR",
        "industry": "Consulting",
        "job_description": "Key deliverables:\nEnhance and maintain the MDM platform to support business needs\nDevelop data pipelines using Snowflake, Python, SQL, and orchestration tools like Airflow\nMonitor and improve system performance and troubleshoot data pipeline issues\nResolve production issues and ensure platform reliability\nRole responsibilities:\nCollaborate with data engineering and analytics teams for scalable solutions\nApply DevOps practices to streamline deployment and automation\nIntegrate cloud-native tools and services (AWS, Azure) with the data platform\nUtilize dbt and version control (Git) for data transformation and management",
        "skills": [
            "Airflow",
            "snowflake",
            "dbt",
            "Python",
            "Sql",
            "AWS"
        ]
    },
    {
        "job_title": "Data Engineer 3",
        "company_name": "Vichara Technologies",
        "experience": "7-12 Years",
        "location": "Bengaluru",
        "industry": "Consulting",
        "job_description": "Key deliverables:\nEnhance and maintain the MDM platform to support business needs\nDevelop data pipelines using Snowflake, Python, SQL, and orchestration tools like Airflow\nMonitor and improve system performance and troubleshoot data pipeline issues\nResolve production issues and ensure platform reliability\nRole responsibilities:\nCollaborate with data engineering and analytics teams for scalable solutions\nApply DevOps practices to streamline deployment and automation\nIntegrate cloud-native tools and services (AWS, Azure) with the data platform\nUtilize dbt and version control (Git) for data transformation and management",
        "skills": [
            "Airflow",
            "snowflake",
            "dbt",
            "Python",
            "Sql",
            "AWS"
        ]
    },
    {
        "job_title": "Big Data Engineer - Xebia",
        "company_name": "Foray Software Private Limited",
        "experience": "3-7 Years",
        "location": "Hyderabad",
        "industry": "Consulting",
        "job_description": "Role Responsibilities:\nBuild scalable data pipelines and applications from scratch on AWS\nDesign and manage data ingestion processes and workflows\nCollaborate with functional and data science teams to deliver data solutions\nOptimize data storage and organization using HDFS, S3, and Delta Lake\nJob Requirements:\nStrong programming experience in Python or Java\nHands-on experience with AWS services and deployment\nProficiency in HDFS, S3, SQL, and data ingestion tools\nExposure to Delta Lake or Databricks is an added advantage",
        "skills": [
            "HDFS",
            "Data Lake",
            "Python",
            "Sql",
            "AWS"
        ]
    }
]